{"title":"Pumping with Symmetry","abstract":"Re-configurable materials and meta-materials can jump between space symmetry classes during their deformations. Here, we introduce the concept of singular symmetry enhancement, which refers to an abrupt jump to a higher symmetry class accompanied by an un-avoidable reduction in the number of dispersion bands of the excitations of the material. Such phenomenon prompts closings of some of the spectral resonant gaps along singular manifolds in a parameter space. In this work, we demonstrate that these singular manifolds carry topological charges. As a concrete example, we show that a deformation of an acoustic crystal that encircles a $p11g$-symmetric configuration of the cavity resonators results in an adiabatic cycle that carries a Chern number in the bulk and displays Thouless pumping at the edges. The outcome is a very general principle for recognizing or engineering topological adiabatic processes in complex materials and meta-materials.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16401v1","tag":"prompt-eng"}}
{"title":"Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models","abstract":"Large language models (LLMs) have demonstrated impressive performance on various downstream tasks without requiring fine-tuning, including ChatGPT, a chat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having a lower training proportion compared to English, these models also exhibit remarkable capabilities in other languages. In this study, we assess the performance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks: sentiment analysis, translation, transliteration, paraphrasing, part of speech tagging, summarization, and diacritization. Our findings reveal that GPT-4 outperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an extensive analysis of the sentiment analysis task, providing insights into how LLMs achieve exceptional results on a challenging dialectal dataset. Additionally, we introduce a new Python interface https://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks effortlessly.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16322v1","tag":"prompt-eng"}}
{"title":"Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting","abstract":"Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment. However, manual summarization of food effect from extensive drug application review documents is time-consuming, which arouses a need to develop automated methods. Recent advances in large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach, iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the quality of the generated summary. We conduct a series of extensive evaluations, ranging from automated metrics to FDA professionals and even evaluation by GPT-4, on 100 NDA review documents selected over the past five years. We observe that the summary quality is progressively improved throughout the process. Moreover, we find that GPT-4 performs better than ChatGPT, as evaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%). Importantly, all the FDA professionals unanimously rated that 85% of the summaries generated by GPT-4 are factually consistent with the golden reference summary, a finding further supported by GPT-4 rating of 72% consistency. These results strongly suggest a great potential for GPT-4 to draft food effect summaries that could be reviewed by FDA professionals, thereby improving the efficiency of PSG assessment cycle and promoting the generic drug product development.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16275v1","tag":"prompt-eng"}}
{"title":"Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks","abstract":"We assessed the performance of commercial Large Language Models (LLMs) GPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b Phase B, which is focused on answer generation, both models demonstrated competitive abilities with leading systems. Remarkably, they achieved this with simple zero-shot learning, grounded with relevant snippets. Even without relevant snippets, their performance was decent, though not on par with the best systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was able to compete with GPT-4 in the grounded Q&A setting on factoid and list answers. In Task 11b Phase A, focusing on retrieval, query expansion through zero-shot learning improved performance, but the models fell short compared to other systems. The code needed to rerun these experiments is available through GitHub.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16108v1","tag":"prompt-eng"}}
{"title":"Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias","abstract":"Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\\% of the querying cost of ChatGPT associated with the latter. We release the generated dataset and used prompts to facilitate future research. The data and code will be available on \\url{https://github.com/yueyu1030/AttrPrompt}.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.15895v1","tag":"prompt-eng"}}
{"title":"MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning","abstract":"Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of > 20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16413v1","tag":"new-dataset"}}
{"title":"Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift","abstract":"Statistical machine learning methods often face the challenge of limited data available from the population of interest. One remedy is to leverage data from auxiliary source populations, which share some conditional distributions or are linked in other ways with the target domain. Techniques leveraging such \\emph{dataset shift} conditions are known as \\emph{domain adaptation} or \\emph{transfer learning}. Despite extensive literature on dataset shift, limited works address how to efficiently use the auxiliary populations to improve the accuracy of risk evaluation for a given machine learning task in the target population.   In this paper, we study the general problem of efficiently estimating target population risk under various dataset shift conditions, leveraging semiparametric efficiency theory. We consider a general class of dataset shift conditions, which includes three popular conditions -- covariate, label and concept shift -- as special cases. We allow for partially non-overlapping support between the source and target populations. We develop efficient and multiply robust estimators along with a straightforward specification test of these dataset shift conditions. We also derive efficiency bounds for two other dataset shift conditions, posterior drift and location-scale shift. Simulation studies support the efficiency gains due to leveraging plausible dataset shift conditions.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16406v1","tag":"new-dataset"}}
{"title":"Cascaded encoders for fine-tuning ASR models on overlapped speech","abstract":"Multi-talker speech recognition (MT-ASR) has been shown to improve ASR performance on speech containing overlapping utterances from more than one speaker. Multi-talker models have typically been trained from scratch using simulated or actual overlapping speech datasets. On the other hand, the trend in ASR has been to train foundation models using massive datasets collected from a wide variety of task domains. Given the scale of these models and their ability to generalize well across a variety of domains, it makes sense to consider scenarios where a foundation model is augmented with multi-talker capability. This paper presents an MT-ASR model formed by combining a well-trained foundation model with a multi-talker mask model in a cascaded RNN-T encoder configuration. Experimental results show that the cascade configuration provides improved WER on overlapping speech utterances with respect to a baseline multi-talker model without sacrificing performance achievable by the foundation model on non-overlapping utterances.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16398v1","tag":"new-dataset"}}
{"title":"Towards Measuring the Representation of Subjective Global Opinions in Language Models","abstract":"Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16388v1","tag":"new-dataset"}}
{"title":"Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses","abstract":"Graph Neural Networks (GNNs) are emerging as a powerful tool for learning from graph-structured data and performing sophisticated inference tasks in various application domains. Although GNNs have been shown to be effective on modest-sized graphs, training them on large-scale graphs remains a significant challenge due to lack of efficient data access and data movement methods. Existing frameworks for training GNNs use CPUs for graph sampling and feature aggregation, while the training and updating of model weights are executed on GPUs. However, our in-depth profiling shows the CPUs cannot achieve the throughput required to saturate GNN model training throughput, causing gross under-utilization of expensive GPU resources. Furthermore, when the graph and its embeddings do not fit in the CPU memory, the overhead introduced by the operating system, say for handling page-faults, comes in the critical path of execution.   To address these issues, we propose the GPU Initiated Direct Storage Access (GIDS) dataloader, to enable GPU-oriented GNN training for large-scale graphs while efficiently utilizing all hardware resources, such as CPU memory, storage, and GPU memory with a hybrid data placement strategy. By enabling GPU threads to fetch feature vectors directly from storage, GIDS dataloader solves the memory capacity problem for GPU-oriented GNN training. Moreover, GIDS dataloader leverages GPU parallelism to tolerate storage latency and eliminates expensive page-fault overhead. Doing so enables us to design novel optimizations for exploiting locality and increasing effective bandwidth for GNN training. Our evaluation using a single GPU on terabyte-scale GNN datasets shows that GIDS dataloader accelerates the overall DGL GNN training pipeline by up to 392X when compared to the current, state-of-the-art DGL dataloader.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16384v1","tag":"new-dataset"}}
{"title":"A new method for short duration transient detection in radio images: Searching for transient sources in MeerKAT data of NGC 5068","abstract":"Transient surveys are a vital tool in exploring the dynamic universe, with radio transients acting as beacons for explosive and highly energetic astrophysical phenomena. However, performing commensal transient surveys using radio imaging can require a significant amount of computing power, data storage and time. With the instrumentation available to us, and with new and exciting radio interferometers in development, it is essential that we develop efficient methods to probe the radio transient sky. In this paper, we present results from an commensal short duration transient survey, on time scales of 8 seconds, 128 seconds and 1 hour, using data from the MeerKAT radio telescope. The dataset used was obtained as part of a galaxy observing campaign, and we focus on the field of NGC 5068. We present a quick, wide field imaging strategy to enable fast imaging of large datasets, and develop methods to efficiently filter detected transient candidates. No transient candidates were identified on the time scales of 8 seconds, 128 seconds and 1 hour, leading to competitive limits on the transient surface densities of $6.7{\\times}10^{-5}$ deg$^{-1}$, $1.1{\\times}10^{-3}$ deg$^{-1}$, and $3.2{\\times}10^{-2}$ deg$^{-1}$ at sensitivities of 56.4 mJy, 19.2 mJy, and 3.9 mJy for the respective time scales. We find one possible candidate that could be associated with a stellar flare, that was rejected due to strict image quality control. Further short time-scale radio observations of this candidate could give definite results to its origin.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16383v1","tag":"new-dataset"}}
{"title":"Generalized Core Spanner Inexpressibility via Ehrenfeucht-Fra\u00efss\u00e9 Games for FC","abstract":"Despite considerable research on document spanners, little is known about the expressive power of generalized core spanners. In this paper, we use Ehrenfeucht-Fra\\\"iss\\'e games to obtain general inexpressibility lemmas for the logic FC (a finite-model variant of the theory of concatenation). Applying these lemmas give inexpressibility results for FC that we lift to generalized core spanners. In particular, we give several relations that cannot be selected by generalized core spanners, thus demonstrating the effectiveness of the inexpressibility lemmas. As an immediate consequence, we also gain new insights into the expressive power of core spanners.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16364v1","tag":"new-dataset"}}
{"title":"Theater Aid System for the Visually Impaired Through Transfer Learning of Spatio-Temporal Graph Convolution Networks","abstract":"The aim of this research is to recognize human actions performed on stage to aid visually impaired and blind individuals. To achieve this, we have created a theatre human action recognition system that uses skeleton data captured by depth image as input. We collected new samples of human actions in a theatre environment, and then tested the transfer learning technique with three pre-trained Spatio-Temporal Graph Convolution Networks for skeleton-based human action recognition: the spatio-temporal graph convolution network, the two-stream adaptive graph convolution network, and the multi-scale disentangled unified graph convolution network. We selected the NTU-RGBD human action benchmark as the source domain and used our collected dataset as the target domain. We analyzed the transferability of the pre-trained models and proposed two configurations to apply and adapt the transfer learning technique to the diversity between the source and target domains. The use of transfer learning helped to improve the performance of the human action system within the context of theatre. The results indicate that Spatio-Temporal Graph Convolution Networks is positively transferred, and there was an improvement in performance compared to the baseline without transfer learning.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16357v1","tag":"new-dataset"}}
{"title":"A global unstructured, coupled, high-resolution hindcast of waves and storm surge","abstract":"Accurate information on waves and storm surges is essential to understand coastal hazards that are expected to increase in view of global warming and rising sea levels. Despite the recent advancement in development and application of large-scale coastal models, nearshore processes are still not sufficiently resolved due to coarse resolutions, transferring errors to coastal risk assessments and other large-scale applications. Here we developed a 50-year hindcast of waves and storm surges on an unstructured mesh of >650,000 nodes with an unprecedented resolution of 2-4 km at the global coast. Our modelling system is based on the circulation model SCHISM that is fully coupled with the WWM-V (WindWaveModel) and is forced by surface winds, pressure, and ice coverage from the ERA5 reanalysis. Results are compared with observations from satellite altimeters, tidal gauges and buoys, and show good skill for both Sea Surface Height (SSH) and Significant Wave Height (Hs), and a much-improved ability to reproduce the nearshore dynamics compared with previous, lower-resolution studies. Besides SSH, the modelling system also produces a range of other wave-related fields at each node of the mesh with a time step of 3 hours, including the spectral parameters of the first three largest energy peaks. This dataset offers the potential for more accurate global-scale applications on coastal hazard and risk","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16337v1","tag":"new-dataset"}}
{"title":"Representation Learning via Variational Bayesian Networks","abstract":"We present Variational Bayesian Network (VBN) - a novel Bayesian entity representation learning model that utilizes hierarchical and relational side information and is particularly useful for modeling entities in the ``long-tail'', where the data is scarce. VBN provides better modeling for long-tail entities via two complementary mechanisms: First, VBN employs informative hierarchical priors that enable information propagation between entities sharing common ancestors. Additionally, VBN models explicit relations between entities that enforce complementary structure and consistency, guiding the learned representations towards a more meaningful arrangement in space. Second, VBN represents entities by densities (rather than vectors), hence modeling uncertainty that plays a complementary role in coping with data scarcity. Finally, we propose a scalable Variational Bayes optimization algorithm that enables fast approximate Bayesian inference. We evaluate the effectiveness of VBN on linguistic, recommendations, and medical inference tasks. Our findings show that VBN outperforms other existing methods across multiple datasets, and especially in the long-tail.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16326v1","tag":"new-dataset"}}
{"title":"A new method for short duration transient detection in radio images: Searching for transient sources in MeerKAT data of NGC 5068","abstract":"Transient surveys are a vital tool in exploring the dynamic universe, with radio transients acting as beacons for explosive and highly energetic astrophysical phenomena. However, performing commensal transient surveys using radio imaging can require a significant amount of computing power, data storage and time. With the instrumentation available to us, and with new and exciting radio interferometers in development, it is essential that we develop efficient methods to probe the radio transient sky. In this paper, we present results from an commensal short duration transient survey, on time scales of 8 seconds, 128 seconds and 1 hour, using data from the MeerKAT radio telescope. The dataset used was obtained as part of a galaxy observing campaign, and we focus on the field of NGC 5068. We present a quick, wide field imaging strategy to enable fast imaging of large datasets, and develop methods to efficiently filter detected transient candidates. No transient candidates were identified on the time scales of 8 seconds, 128 seconds and 1 hour, leading to competitive limits on the transient surface densities of $6.7{\\times}10^{-5}$ deg$^{-1}$, $1.1{\\times}10^{-3}$ deg$^{-1}$, and $3.2{\\times}10^{-2}$ deg$^{-1}$ at sensitivities of 56.4 mJy, 19.2 mJy, and 3.9 mJy for the respective time scales. We find one possible candidate that could be associated with a stellar flare, that was rejected due to strict image quality control. Further short time-scale radio observations of this candidate could give definite results to its origin.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16383v1","tag":"data-quality"}}
{"title":"SE-PQA: Personalized Community Question Answering","abstract":"Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to filling this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new curated resource to design and evaluate personalized models related to the task of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization remarkably improves the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16261v1","tag":"data-quality"}}
{"title":"The NANOGrav 15-year Data Set: Observations and Timing of 68 Millisecond Pulsars","abstract":"We present observations and timing analyses of 68 millisecond pulsars (MSPs) comprising the 15-year data set of the North American Nanohertz Observatory for Gravitational Waves (NANOGrav). NANOGrav is a pulsar timing array (PTA) experiment that is sensitive to low-frequency gravitational waves. This is NANOGrav's fifth public data release, including both \"narrowband\" and \"wideband\" time-of-arrival (TOA) measurements and corresponding pulsar timing models. We have added 21 MSPs and extended our timing baselines by three years, now spanning nearly 16 years for some of our sources. The data were collected using the Arecibo Observatory, the Green Bank Telescope, and the Very Large Array between frequencies of 327 MHz and 3 GHz, with most sources observed approximately monthly. A number of notable methodological and procedural changes were made compared to our previous data sets. These improve the overall quality of the TOA data set and are part of the transition to new pulsar timing and PTA analysis software packages. For the first time, our data products are accompanied by a full suite of software to reproduce data reduction, analysis, and results. Our timing models include a variety of newly detected astrometric and binary pulsar parameters, including several significant improvements to pulsar mass constraints. We find that the time series of 23 pulsars contain detectable levels of red noise, 10 of which are new measurements. In this data set, we find evidence for a stochastic gravitational-wave background.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16217v1","tag":"data-quality"}}
{"title":"Deterministic End-to-End Transmission to Optimize the Network Efficiency and Quality of Service: A Paradigm Shift in 6G","abstract":"Toward end-to-end mobile service provision with optimized network efficiency and quality of service, tremendous efforts have been devoted in upgrading mobile applications, transport and internet networks, and wireless communication networks for many years. However, the inherent loose coordination between different layers in the end-to-end communication networks leads to unreliable data transmission with uncontrollable packet delay and packet error rate, and a terrible waste of network resources incurred for data re-transmission. In an attempt to shed some lights on how to tackle these challenges, design methodologies and some solutions for deterministic end-to-end transmission for 6G and beyond are presented, which will bring a paradigm shift to the end-to-end wireless communication networks.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16193v1","tag":"data-quality"}}
{"title":"Effective Transfer of Pretrained Large Visual Model for Fabric Defect Segmentation via Specifc Knowledge Injection","abstract":"Fabric defect segmentation is integral to textile quality control. Despite this, the scarcity of high-quality annotated data and the diversity of fabric defects present significant challenges to the application of deep learning in this field. These factors limit the generalization and segmentation performance of existing models, impeding their ability to handle the complexity of diverse fabric types and defects. To overcome these obstacles, this study introduces an innovative method to infuse specialized knowledge of fabric defects into the Segment Anything Model (SAM), a large-scale visual model. By introducing and training a unique set of fabric defect-related parameters, this approach seamlessly integrates domain-specific knowledge into SAM without the need for extensive modifications to the pre-existing model parameters. The revamped SAM model leverages generalized image understanding learned from large-scale natural image datasets while incorporating fabric defect-specific knowledge, ensuring its proficiency in fabric defect segmentation tasks. The experimental results reveal a significant improvement in the model's segmentation performance, attributable to this novel amalgamation of generic and fabric-specific knowledge. When benchmarking against popular existing segmentation models across three datasets, our proposed model demonstrates a substantial leap in performance. Its impressive results in cross-dataset comparisons and few-shot learning experiments further demonstrate its potential for practical applications in textile quality control.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16186v1","tag":"data-quality"}}
{"title":"Neural directional distance field object representation for uni-directional path-traced rendering","abstract":"Faster rendering of synthetic images is a core problem in the field of computer graphics. Rendering algorithms, such as path-tracing is dependent on parameters like size of the image, number of light bounces, number of samples per pixel, all of which, are fixed if one wants to obtain a image of a desired quality. It is also dependent on the size and complexity of the scene being rendered. One of the largest bottleneck in rendering, particularly when the scene is very large, is querying for objects in the path of a given ray in the scene. By changing the data type that represents the objects in the scene, one may reduce render time, however, a different representation of a scene requires the modification of the rendering algorithm. In this paper, (a) we introduce directed distance field, as a functional representation of a object; (b) how the directed distance functions, when stored as a neural network, be optimized and; (c) how such an object can be rendered with a modified path-tracing algorithm.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16142v1","tag":"data-quality"}}
{"title":"More efficient manual review of automatically transcribed tabular data","abstract":"Machine learning methods have proven useful in transcribing historical data. However, results from even highly accurate methods require manual verification and correction. Such manual review can be time-consuming and expensive, therefore the objective of this paper was to make it more efficient. Previously, we used machine learning to transcribe 2.3 million handwritten occupation codes from the Norwegian 1950 census with high accuracy (97%). We manually reviewed the 90,000 (3%) codes with the lowest model confidence. We allocated those 90,000 codes to human reviewers, who used our annotation tool to review the codes. To assess reviewer agreement, some codes were assigned to multiple reviewers. We then analyzed the review results to understand the relationship between accuracy improvements and effort. Additionally, we interviewed the reviewers to improve the workflow. The reviewers corrected 62.8% of the labels and agreed with the model label in 31.9% of cases. About 0.2% of the images could not be assigned a label, while for 5.1% the reviewers were uncertain, or they assigned an invalid label. 9,000 images were independently reviewed by multiple reviewers, resulting in an agreement of 86.43% and disagreement of 8.96%. We learned that our automatic transcription is biased towards the most frequent codes, with a higher degree of misclassification for the lowest frequency codes. Our interview findings show that the reviewers did internal quality control and found our custom tool well-suited. So, only one reviewer is needed, but they should report uncertainty.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16126v1","tag":"data-quality"}}
{"title":"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases","abstract":"Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation.   In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models. We also open-sourced our model and part of the data at https://github.com/PKU-YuanGroup/ChatLaw.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16092v1","tag":"data-quality"}}
{"title":"A Dimensional Structure based Knowledge Distillation Method for Cross-Modal Learning","abstract":"Due to limitations in data quality, some essential visual tasks are difficult to perform independently. Introducing previously unavailable information to transfer informative dark knowledge has been a common way to solve such hard tasks. However, research on why transferred knowledge works has not been extensively explored. To address this issue, in this paper, we discover the correlation between feature discriminability and dimensional structure (DS) by analyzing and observing features extracted from simple and hard tasks. On this basis, we express DS using deep channel-wise correlation and intermediate spatial distribution, and propose a novel cross-modal knowledge distillation (CMKD) method for better supervised cross-modal learning (CML) performance. The proposed method enforces output features to be channel-wise independent and intermediate ones to be uniformly distributed, thereby learning semantically irrelevant features from the hard task to boost its accuracy. This is especially useful in specific applications where the performance gap between dual modalities is relatively large. Furthermore, we collect a real-world CML dataset to promote community development. The dataset contains more than 10,000 paired optical and radar images and is continuously being updated. Experimental results on real-world and benchmark datasets validate the effectiveness of the proposed method.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.15977v1","tag":"data-quality"}}
{"title":"You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting","abstract":"Despite significant advancements in existing models, generating text descriptions from structured data input, known as data-to-text generation, remains a challenging task. In this paper, we propose a novel approach that goes beyond traditional one-shot generation methods by introducing a multi-step process consisting of generation, verification, and correction stages. Our approach, VCP(Verification and Correction Prompting), begins with the model generating an initial output. We then proceed to verify the correctness of different aspects of the generated text. The observations from the verification step are converted into a specialized error-indication prompt, which instructs the model to regenerate the output while considering the identified errors. To enhance the model's correction ability, we have developed a carefully designed training procedure. This procedure enables the model to incorporate feedback from the error-indication prompt, resulting in improved output generation. Through experimental results, we demonstrate that our approach effectively reduces slot error rates while maintaining the overall quality of the generated text.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.15933v1","tag":"data-quality"}}
{"title":"OpenCitations Meta","abstract":"OpenCitations Meta is a new database that contains bibliographic metadata of scholarly publications involved in citations indexed by the OpenCitations infrastructure. It adheres to Open Science principles and provides data under a CC0 license for maximum reuse. The data can be accessed through a SPARQL endpoint, REST APIs, and dumps. OpenCitations Meta serves three important purposes. Firstly, it enables disambiguation of citations between publications described using different identifiers from various sources. For example, it can link publications identified by DOIs in Crossref and PMIDs in PubMed. Secondly, it assigns new globally persistent identifiers (PIDs), known as OpenCitations Meta Identifiers (OMIDs), to bibliographic resources without existing external persistent identifiers like DOIs. Lastly, by hosting the bibliographic metadata internally, OpenCitations Meta improves the speed of metadata retrieval for citing and cited documents. The database is populated through automated data curation, including deduplication, error correction, and metadata enrichment. The data is stored in RDF format following the OpenCitations Data Model, and changes and provenance information are tracked. OpenCitations Meta and its production. OpenCitations Meta currently incorporates data from Crossref, DataCite, and the NIH Open Citation Collection. In terms of semantic publishing datasets, it is currently the first in data volume.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16191v1","tag":"data-quality"}}
{"title":"Federated Generative Learning with Foundation Models","abstract":"Existing federated learning solutions focus on transmitting features, parameters or gadients between clients and server, which suffer from serious low-efficiency and privacy-leakage problems. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning, that transmits prompts associated with distributed training data between clients and server. The informative training data can be synthesized remotely based on received prompts containing little privacy and the foundation generative models. The new framework possesses multiple advantages, including improved communication efficiency, better resilience to distribution shift, substantial performance gains, and enhanced privacy protection, which are verified in extensive experiments on ImageNet and DomainNet datasets.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.16064v1","tag":"data-quality"}}
{"title":"NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data","abstract":"Federated learning (FL), a privacy-preserving distributed machine learning, has been rapidly applied in wireless communication networks. FL enables Internet of Things (IoT) clients to obtain well-trained models while preventing privacy leakage. Person detection can be deployed on edge devices with limited computing power if combined with FL to process the video data directly at the edge. However, due to the different hardware and deployment scenarios of different cameras, the data collected by the camera present non-independent and identically distributed (non-IID), and the global model derived from FL aggregation is less effective. Meanwhile, existing research lacks public data set for real-world FL object detection, which is not conducive to studying the non-IID problem on IoT cameras. Therefore, we open source a non-IID IoT person detection (NIPD) data set, which is collected from five different cameras. To our knowledge, this is the first true device-based non-IID person detection data set. Based on this data set, we explain how to establish a FL experimental platform and provide a benchmark for non-IID person detection. NIPD is expected to promote the application of FL and the security of smart city.","created":"2023-06-28","meta":{"link":"http://arxiv.org/abs/2306.15932v1","tag":"data-quality"}}
