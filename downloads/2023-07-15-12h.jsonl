{"created":"2023-07-13 17:59:47","title":"HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models","abstract":"Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10000x smaller than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io","sentences":["Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities.","However, the process of personalization presents inherent challenges in terms of time and memory requirements.","Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity.","To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person.","By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications.","Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth.","Also our method yields a model that is 10000x smaller than a normal DreamBooth model.","Project page: https://hyperdreambooth.github.io"],"url":"http://arxiv.org/abs/2307.06949v1"}
{"created":"2023-07-13 17:59:35","title":"Self-regulating Prompts: Foundational Model Adaptation without Forgetting","abstract":"Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks. Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP. This leads to the loss of the model's original generalization capability. To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating {prompted} representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch. To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity. PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization. We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods. Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC.","sentences":["Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks.","Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP.","This leads to the loss of the model's original generalization capability.","To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints).","PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating {prompted} representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch.","To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity.","PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization.","We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods.","Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC."],"url":"http://arxiv.org/abs/2307.06948v1"}
{"created":"2023-07-13 17:59:33","title":"Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition","abstract":"Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modulation-based spatio-temporal context modeling and demonstrate our parallel spatial and temporal encoding design to be the optimal choice. Video-FocalNets perform favorably well against the state-of-the-art transformer-based models for video recognition on three large-scale datasets (Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our code/models are released at https://github.com/TalalWasim/Video-FocalNets.","sentences":["Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling.","Video transformer designs are based on self-attention that can model global context at a high computational cost.","In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling.","Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts.","Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency.","Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations.","We extensively explore the design space of focal modulation-based spatio-temporal context modeling and demonstrate our parallel spatial and temporal encoding design to be the optimal choice.","Video-FocalNets perform favorably well against the state-of-the-art transformer-based models for video recognition on three large-scale datasets (Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost.","Our code/models are released at https://github.com/TalalWasim/Video-FocalNets."],"url":"http://arxiv.org/abs/2307.06947v1"}
{"created":"2023-07-13 17:59:21","title":"In-context Autoencoder for Context Compression in a Large Language Model","abstract":"We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM. Our code and data will be released shortly.","sentences":["We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM).","The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes.","We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context.","Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses.","Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts.","The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM.","Our code and data will be released shortly."],"url":"http://arxiv.org/abs/2307.06945v1"}
{"created":"2023-07-13 17:58:32","title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation","abstract":"This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.","sentences":["This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation.","The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.","Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale.","Specifically, we utilize a multi-scale approach to generate video-related descriptions.","Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance.","Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications.","They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research.","These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation."],"url":"http://arxiv.org/abs/2307.06942v1"}
{"created":"2023-07-13 17:57:21","title":"On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations","abstract":"Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantitatively show the difference in explanations at every stage of the connection between the two approaches and corroborate the theoretical findings.","sentences":["Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations.","These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical.","This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations.","After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent.","We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values.","Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances.","Experiments on three datasets quantitatively show the difference in explanations at every stage of the connection between the two approaches and corroborate the theoretical findings."],"url":"http://arxiv.org/abs/2307.06941v1"}
{"created":"2023-07-13 17:57:13","title":"Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation","abstract":"Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering. To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances. We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts. For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure. For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters. The videos are synthesized by following the structural guidance and appearance instruction. To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts. Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines.","sentences":["Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering.","To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances.","We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts.","For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure.","For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters.","The videos are synthesized by following the structural guidance and appearance instruction.","To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts.","Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines."],"url":"http://arxiv.org/abs/2307.06940v1"}
{"created":"2023-07-13 17:53:25","title":"PHOENI2X -- A European Cyber Resilience Framework With Artificial-Intelligence-Assisted Orchestration, Automation and Response Capabilities for Business Continuity and Recovery, Incident Response, and Information Exchange","abstract":"As digital technologies become more pervasive in society and the economy, cybersecurity incidents become more frequent and impactful. According to the NIS and NIS2 Directives, EU Member States and their Operators of Essential Services must establish a minimum baseline set of cybersecurity capabilities and engage in cross-border coordination and cooperation. However, this is only a small step towards European cyber resilience. In this landscape, preparedness, shared situational awareness, and coordinated incident response are essential for effective cyber crisis management and resilience. Motivated by the above, this paper presents PHOENI2X, an EU-funded project aiming to design, develop, and deliver a Cyber Resilience Framework providing Artificial-Intelligence-assisted orchestration, automation and response capabilities for business continuity and recovery, incident response, and information exchange, tailored to the needs of Operators of Essential Services and the EU Member State authorities entrusted with cybersecurity.","sentences":["As digital technologies become more pervasive in society and the economy, cybersecurity incidents become more frequent and impactful.","According to the NIS and NIS2 Directives, EU Member States and their Operators of Essential Services must establish a minimum baseline set of cybersecurity capabilities and engage in cross-border coordination and cooperation.","However, this is only a small step towards European cyber resilience.","In this landscape, preparedness, shared situational awareness, and coordinated incident response are essential for effective cyber crisis management and resilience.","Motivated by the above, this paper presents PHOENI2X, an EU-funded project aiming to design, develop, and deliver a Cyber Resilience Framework providing Artificial-Intelligence-assisted orchestration, automation and response capabilities for business continuity and recovery, incident response, and information exchange, tailored to the needs of Operators of Essential Services and the EU Member State authorities entrusted with cybersecurity."],"url":"http://arxiv.org/abs/2307.06932v1"}
{"created":"2023-07-13 17:51:58","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs","abstract":"Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder. With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by leveraging a pretrained multilingual LLM. To this end, we \\textit{re-align} an image encoder previously tuned to an English LLM to a new, multilingual LLM -- for this, we leverage multilingual data from a mix of vision-and-language tasks, which we obtain by machine-translating high-quality English data to 95 languages. On the IGLUE benchmark, mBLIP yields results competitive with state-of-the-art models. Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared to these very large multilingual vision-language models trained from scratch, we obtain mBLIP by training orders of magnitude fewer parameters on magnitudes less data. We release our model and code at \\url{https://github.com/gregor-ge/mBLIP}.","sentences":["Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most.","Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder.","With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs.","Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora.","In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by leveraging a pretrained multilingual LLM.","To this end, we \\textit{re-align} an image encoder previously tuned to an English LLM to a new, multilingual LLM -- for this, we leverage multilingual data from a mix of vision-and-language tasks, which we obtain by machine-translating high-quality English data to 95 languages.","On the IGLUE benchmark, mBLIP yields results competitive with state-of-the-art models.","Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms PaLI-X (a model with 55B parameters).","Compared to these very large multilingual vision-language models trained from scratch, we obtain mBLIP by training orders of magnitude fewer parameters on magnitudes less data.","We release our model and code at \\url{https://github.com/gregor-ge/mBLIP}."],"url":"http://arxiv.org/abs/2307.06930v1"}
{"created":"2023-07-13 17:51:04","title":"Ill-Typed Programs Don't Evaluate","abstract":"We introduce two-sided type systems, which are a particular kind of sequent calculi for typing formulas. Two-sided type systems allow for hypothetical reasoning over the typing of compound program expressions, and the refutation of typing formulas. By incorporating a type of all values, these type systems support symmetrical notions of well-typing and ill-typing, guaranteeing both that well-typed programs don't go wrong and that ill-typed programs do not evaluate - that is, reach a value. This makes two-sided type systems suitable for incorrectness reasoning in higher-order program verification, which we illustrate through an application to precise data-flow typing in a language with constructors and pattern matching. Finally, we investigate the internalisation of the meta-level negation in the system as a complement operator on types. This motivates an alternative semantics for the typing judgement, which guarantees that ill-typed programs don't evaluate, but in which well-typed programs may yet go wrong.","sentences":["We introduce two-sided type systems, which are a particular kind of sequent calculi for typing formulas.","Two-sided type systems allow for hypothetical reasoning over the typing of compound program expressions, and the refutation of typing formulas.","By incorporating a type of all values, these type systems support symmetrical notions of well-typing and ill-typing, guaranteeing both that well-typed programs don't go wrong and that ill-typed programs do not evaluate - that is, reach a value.","This makes two-sided type systems suitable for incorrectness reasoning in higher-order program verification, which we illustrate through an application to precise data-flow typing in a language with constructors and pattern matching.","Finally, we investigate the internalisation of the meta-level negation in the system as a complement operator on types.","This motivates an alternative semantics for the typing judgement, which guarantees that ill-typed programs don't evaluate, but in which well-typed programs may yet go wrong."],"url":"http://arxiv.org/abs/2307.06928v1"}
{"created":"2023-07-13 17:46:42","title":"Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models","abstract":"Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts. Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times. However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts. In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models. This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods.","sentences":["Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts.","Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times.","However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts.","In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts.","We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens.","Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models.","This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods."],"url":"http://arxiv.org/abs/2307.06925v1"}
{"created":"2023-07-13 17:46:15","title":"DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding","abstract":"Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, provide a good guiding experience, and connect users with their surrounding environment in an intuitive manner.","sentences":["Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them.","Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment.","Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language.","By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations.","Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language.","We conduct a user study with blindfolded participants in an everyday indoor environment.","Our results demonstrate that DRAGON is able to communicate with the user smoothly, provide a good guiding experience, and connect users with their surrounding environment in an intuitive manner."],"url":"http://arxiv.org/abs/2307.06924v1"}
{"created":"2023-07-13 17:43:12","title":"Crucible: Graphical Test Cases for Alloy Models","abstract":"Alloy is a declarative modeling language that is well suited for verifying system designs. Alloy models are automatically analyzed using the Analyzer, a toolset that helps the user understand their system by displaying the consequences of their properties, helping identify any missing or incorrect properties, and exploring the impact of modifications to those properties. To achieve this, the Analyzer invokes off-the-shelf SAT solvers to search for scenarios, which are assignments to the sets and relations of the model such that all executed formulas hold. To help write more accurate software models, Alloy has a unit testing framework, AUnit, which allows users to outline specific scenarios and check if those scenarios are correctly generated or prevented by their model. Unfortunately, AUnit currently only supports textual specifications of scenarios. This paper introduces Crucible, which allows users to graphically create AUnit test cases. In addition, Crucible provides automated guidance to users to ensure they are creating well structured, valuable test cases. As a result, Crucible eases the burden of adopting AUnit and brings AUnit test case creation more in line with how Alloy scenarios are commonly interacted with, which is graphically.","sentences":["Alloy is a declarative modeling language that is well suited for verifying system designs.","Alloy models are automatically analyzed using the Analyzer, a toolset that helps the user understand their system by displaying the consequences of their properties, helping identify any missing or incorrect properties, and exploring the impact of modifications to those properties.","To achieve this, the Analyzer invokes off-the-shelf SAT solvers to search for scenarios, which are assignments to the sets and relations of the model such that all executed formulas hold.","To help write more accurate software models, Alloy has a unit testing framework, AUnit, which allows users to outline specific scenarios and check if those scenarios are correctly generated or prevented by their model.","Unfortunately, AUnit currently only supports textual specifications of scenarios.","This paper introduces Crucible, which allows users to graphically create AUnit test cases.","In addition, Crucible provides automated guidance to users to ensure they are creating well structured, valuable test cases.","As a result, Crucible eases the burden of adopting AUnit and brings AUnit test case creation more in line with how Alloy scenarios are commonly interacted with, which is graphically."],"url":"http://arxiv.org/abs/2307.06922v1"}
{"created":"2023-07-13 17:42:02","title":"Targeting Completeness: Using Closed Forms for Size Bounds of Integer Programs","abstract":"We present a new procedure to infer size bounds for integer programs automatically. Size bounds are important for the deduction of bounds on the runtime complexity or in general, for the resource analysis of programs. We show that our technique is complete (i.e., it always computes finite size bounds) for a subclass of loops, possibly with non-linear arithmetic. Moreover, we present a novel approach to combine and integrate this complete technique into an incomplete approach to infer size and runtime bounds of general integer programs. We prove completeness of our integration for an important subclass of integer programs. We implemented our new algorithm in the automated complexity analysis tool KoAT to evaluate its power, in particular on programs with non-linear arithmetic.","sentences":["We present a new procedure to infer size bounds for integer programs automatically.","Size bounds are important for the deduction of bounds on the runtime complexity or in general, for the resource analysis of programs.","We show that our technique is complete (i.e., it always computes finite size bounds) for a subclass of loops, possibly with non-linear arithmetic.","Moreover, we present a novel approach to combine and integrate this complete technique into an incomplete approach to infer size and runtime bounds of general integer programs.","We prove completeness of our integration for an important subclass of integer programs.","We implemented our new algorithm in the automated complexity analysis tool KoAT to evaluate its power, in particular on programs with non-linear arithmetic."],"url":"http://arxiv.org/abs/2307.06921v1"}
{"created":"2023-07-13 17:40:30","title":"DAXiot: A Decentralized Authentication and Authorization Scheme for Dynamic IoT Networks","abstract":"Federated and decentralized networks supporting frequently changing system participants are a requirement for future Internet of Things (IoT) use cases. IoT devices and networks often lack adequate authentication and authorization mechanisms, resulting in insufficient privacy for entities in such systems. In this work we address both issues by designing a privacy preserving challenge-response style authentication and authorization scheme based on Decentralized Identifiers and Verifiable Credentials. Our solution allows a decentralized permission management of frequently changing network participants and supports authenticated encryption for data confidentiality. We demonstrate our solution in an MQTT 5.0 scenario and evaluate its security, privacy guarantees, and performance.","sentences":["Federated and decentralized networks supporting frequently changing system participants are a requirement for future Internet of Things (IoT) use cases.","IoT devices and networks often lack adequate authentication and authorization mechanisms, resulting in insufficient privacy for entities in such systems.","In this work we address both issues by designing a privacy preserving challenge-response style authentication and authorization scheme based on Decentralized Identifiers and Verifiable Credentials.","Our solution allows a decentralized permission management of frequently changing network participants and supports authenticated encryption for data confidentiality.","We demonstrate our solution in an MQTT 5.0 scenario and evaluate its security, privacy guarantees, and performance."],"url":"http://arxiv.org/abs/2307.06919v1"}
{"created":"2023-07-13 17:31:41","title":"LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT","abstract":"Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. KGs surpass any other form of representation in terms of effectiveness. However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. It also demands a significant amount of work. Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE. In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs.","sentences":["Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines.","KGs surpass any other form of representation in terms of effectiveness.","However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices.","It also demands a significant amount of work.","Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE.","In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs."],"url":"http://arxiv.org/abs/2307.06917v1"}
{"created":"2023-07-13 17:21:54","title":"Uncovering Unique Concept Vectors through Latent Space Decomposition","abstract":"Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors. This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data.","sentences":["Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety.","Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency.","However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts.","To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training.","By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts.","Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand.","Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors.","This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data."],"url":"http://arxiv.org/abs/2307.06913v1"}
{"created":"2023-07-13 17:20:36","title":"BittyBuzz: A Swarm Robotics Runtime for Tiny Systems","abstract":"Swarm robotics is an emerging field of research which is increasingly attracting attention thanks to the advances in robotics and its potential applications. However, despite the enthusiasm surrounding this area of research, software development for swarm robotics is still a tedious task. That fact is partly due to the lack of dedicated solutions, in particular for low-cost systems to be produced in large numbers and that can have important resource constraints. To address this issue, we introduce BittyBuzz, a novel runtime platform: it allows Buzz, a domain-specific language, to run on microcontrollers while maintaining dynamic memory management. BittyBuzz is designed to fit a flash memory as small as 32 kB (with usable space for scripts) and work with as little as 2 kB of RAM. In this work, we introduce the BittyBuzz implementation, its differences from the original Buzz virtual machine, and its advantages for swarm robotics systems. We show that BittyBuzz is successfully integrated with three robotic platforms with minimal memory footprint and conduct experiments to show computation performance of BittyBuzz. Results show that BittyBuzz can be effectively used to implement common swarm behaviors on microcontroller-based systems.","sentences":["Swarm robotics is an emerging field of research which is increasingly attracting attention thanks to the advances in robotics and its potential applications.","However, despite the enthusiasm surrounding this area of research, software development for swarm robotics is still a tedious task.","That fact is partly due to the lack of dedicated solutions, in particular for low-cost systems to be produced in large numbers and that can have important resource constraints.","To address this issue, we introduce BittyBuzz, a novel runtime platform: it allows Buzz, a domain-specific language, to run on microcontrollers while maintaining dynamic memory management.","BittyBuzz is designed to fit a flash memory as small as 32 kB (with usable space for scripts) and work with as little as 2 kB of RAM.","In this work, we introduce the BittyBuzz implementation, its differences from the original Buzz virtual machine, and its advantages for swarm robotics systems.","We show that BittyBuzz is successfully integrated with three robotic platforms with minimal memory footprint and conduct experiments to show computation performance of BittyBuzz.","Results show that BittyBuzz can be effectively used to implement common swarm behaviors on microcontroller-based systems."],"url":"http://arxiv.org/abs/2307.06912v1"}
{"created":"2023-07-13 17:14:38","title":"Generating Benchmarks for Factuality Evaluation of Language Models","abstract":"Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators. We make our data and code publicly available in https://github.com/AI21Labs/factor.","sentences":["Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain.","Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts.","We propose FACTOR:","Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality.","FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements.","We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR.","We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.","We make our data and code publicly available in https://github.com/AI21Labs/factor."],"url":"http://arxiv.org/abs/2307.06908v1"}
{"created":"2023-07-13 17:05:49","title":"Trajectory-Aware Rate Adaptation for Flying Networks","abstract":"Despite the trend towards ubiquitous wireless connectivity, there are scenarios where the communications infrastructure is damaged and wireless coverage is insufficient or does not exist, such as in natural disasters and temporary crowded events. Flying networks, composed of Unmanned Aerial Vehicles (UAV), have emerged as a flexible and cost-effective solution to provide on-demand wireless connectivity in these scenarios. UAVs have the capability to operate virtually everywhere, and the growing payload capacity makes them suitable platforms to carry wireless communications hardware. The state of the art in the field of flying networks is mainly focused on the optimal positioning of the flying nodes, while the wireless link parameters are configured with default values. On the other hand, current link adaptation algorithms are mainly targeting fixed or low mobility scenarios.   We propose a novel rate adaptation approach for flying networks, named Trajectory Aware Rate Adaptation (TARA), which leverages the knowledge of flying nodes' movement to predict future channel conditions and perform rate adaptation accordingly. Simulation results of 100 different trajectories show that our solution increases throughput by up to 53% and achieves an average improvement of 14%, when compared with conventional rate adaptation algorithms such as Minstrel-HT.","sentences":["Despite the trend towards ubiquitous wireless connectivity, there are scenarios where the communications infrastructure is damaged and wireless coverage is insufficient or does not exist, such as in natural disasters and temporary crowded events.","Flying networks, composed of Unmanned Aerial Vehicles (UAV), have emerged as a flexible and cost-effective solution to provide on-demand wireless connectivity in these scenarios.","UAVs have the capability to operate virtually everywhere, and the growing payload capacity makes them suitable platforms to carry wireless communications hardware.","The state of the art in the field of flying networks is mainly focused on the optimal positioning of the flying nodes, while the wireless link parameters are configured with default values.","On the other hand, current link adaptation algorithms are mainly targeting fixed or low mobility scenarios.   ","We propose a novel rate adaptation approach for flying networks, named Trajectory Aware Rate Adaptation (TARA), which leverages the knowledge of flying nodes' movement to predict future channel conditions and perform rate adaptation accordingly.","Simulation results of 100 different trajectories show that our solution increases throughput by up to 53% and achieves an average improvement of 14%, when compared with conventional rate adaptation algorithms such as Minstrel-HT."],"url":"http://arxiv.org/abs/2307.06905v1"}
{"created":"2023-07-13 16:50:38","title":"Words are not Wind -- How Joint Commitment and Reputation Solve Social Dilemmas, without Repeated Interactions or Enforcement by Third Parties","abstract":"Joint commitment was argued to \"make our social world\" (Gilbert, 2014) and to separate us from other primates. 'Joint' entails that neither of us promises anything, unless the other promises as well. When we need to coordinate for the best mutual outcome, any commitment is beneficial. However, when we are tempted to free-ride (i.e. in social dilemmas), commitment serves no obvious purpose. We show that a reputation system, which judges action in social dilemmas only after joint commitment, can prevent free-riding. Keeping commitments builds trust. We can selectively enter joint commitments with trustworthy individuals to ensure their cooperation (since they will now be judged). We simply do not commit to cooperate with those we do not trust, and hence can freely defect without losing the trust of others. This principle might be the reason for pointedly public joint commitments, such as marriage. It is especially relevant to our evolutionary past, in which no mechanisms existed to enforce commitments reliably and impartially (e.g. via a powerful and accountable government). Much research from anthropology, philosophy and psychology made the assumption that past collaborations were mutually beneficial and had little possibilities to free-ride, for which there is little support. Our evolutionary game theory approach proves that this assumption is not necessary, because free-riding could have been dealt with joint commitments and reputation.","sentences":["Joint commitment was argued to \"make our social world\" (Gilbert, 2014) and to separate us from other primates.","'Joint' entails that neither of us promises anything, unless the other promises as well.","When we need to coordinate for the best mutual outcome, any commitment is beneficial.","However, when we are tempted to free-ride (i.e. in social dilemmas), commitment serves no obvious purpose.","We show that a reputation system, which judges action in social dilemmas only after joint commitment, can prevent free-riding.","Keeping commitments builds trust.","We can selectively enter joint commitments with trustworthy individuals to ensure their cooperation (since they will now be judged).","We simply do not commit to cooperate with those we do not trust, and hence can freely defect without losing the trust of others.","This principle might be the reason for pointedly public joint commitments, such as marriage.","It is especially relevant to our evolutionary past, in which no mechanisms existed to enforce commitments reliably and impartially (e.g. via a powerful and accountable government).","Much research from anthropology, philosophy and psychology made the assumption that past collaborations were mutually beneficial and had little possibilities to free-ride, for which there is little support.","Our evolutionary game theory approach proves that this assumption is not necessary, because free-riding could have been dealt with joint commitments and reputation."],"url":"http://arxiv.org/abs/2307.06898v1"}
{"created":"2023-07-13 16:49:07","title":"Proof Systems for the Modal $\u03bc$-Calculus Obtained by Determinizing Automata","abstract":"Automata operating on infinite objects feature prominently in the theory of the modal $\\mu$-calculus. One such application concerns the tableau games introduced by Niwi\\'{n}ski & Walukiewicz, of which the winning condition for infinite plays can be naturally checked by a nondeterministic parity stream automaton. Inspired by work of Jungteerapanich and Stirling we show how determinization constructions of this automaton may be used to directly obtain proof systems for the $\\mu$-calculus. More concretely, we introduce a binary tree construction for determinizing nondeterministic parity stream automata. Using this construction we define the annotated cyclic proof system $\\mathsf{BT}$, where formulas are annotated by tuples of binary strings. Soundness and Completeness of this system follow almost immediately from the correctness of the determinization method.","sentences":["Automata operating on infinite objects feature prominently in the theory of the modal $\\mu$-calculus.","One such application concerns the tableau games introduced by Niwi\\'{n}ski & Walukiewicz, of which the winning condition for infinite plays can be naturally checked by a nondeterministic parity stream automaton.","Inspired by work of Jungteerapanich and Stirling we show how determinization constructions of this automaton may be used to directly obtain proof systems for the $\\mu$-calculus.","More concretely, we introduce a binary tree construction for determinizing nondeterministic parity stream automata.","Using this construction we define the annotated cyclic proof system $\\mathsf{BT}$, where formulas are annotated by tuples of binary strings.","Soundness and Completeness of this system follow almost immediately from the correctness of the determinization method."],"url":"http://arxiv.org/abs/2307.06897v1"}
{"created":"2023-07-13 16:46:26","title":"Automatic Routing System for Intelligent Warehouses","abstract":"Automation of logistic processes is essential to improve productivity and reduce costs. In this context, intelligent warehouses are becoming a key to logistic systems thanks to their ability of optimizing transportation tasks and, consequently, reducing costs. This paper initially presents briefly routing systems applied on intelligent warehouses. Then, we present the approach used to develop our router system. This router system is able to solve traffic jams and collisions, generate conflict-free and optimized paths before sending the final paths to the robotic forklifts. It also verifies the progress of all tasks. When a problem occurs, the router system can change the task priorities, routes, etc. in order to avoid new conflicts. In the routing simulations, each vehicle executes its tasks starting from a predefined initial pose, moving to the desired position. Our algorithm is based on Dijkstra's shortest path and the time window approaches and it was implemented in C language. Computer simulation tests were used to validate the algorithm efficiency under different working conditions. Several simulations were carried out using the Player/Stage Simulator to test the algorithms. Thanks to the simulations, we could solve many faults and refine the algorithms before embedding them in real robots.","sentences":["Automation of logistic processes is essential to improve productivity and reduce costs.","In this context, intelligent warehouses are becoming a key to logistic systems thanks to their ability of optimizing transportation tasks and, consequently, reducing costs.","This paper initially presents briefly routing systems applied on intelligent warehouses.","Then, we present the approach used to develop our router system.","This router system is able to solve traffic jams and collisions, generate conflict-free and optimized paths before sending the final paths to the robotic forklifts.","It also verifies the progress of all tasks.","When a problem occurs, the router system can change the task priorities, routes, etc. in order to avoid new conflicts.","In the routing simulations, each vehicle executes its tasks starting from a predefined initial pose, moving to the desired position.","Our algorithm is based on Dijkstra's shortest path and the time window approaches and it was implemented in C language.","Computer simulation tests were used to validate the algorithm efficiency under different working conditions.","Several simulations were carried out using the Player/Stage Simulator to test the algorithms.","Thanks to the simulations, we could solve many faults and refine the algorithms before embedding them in real robots."],"url":"http://arxiv.org/abs/2307.06893v1"}
{"created":"2023-07-13 16:40:12","title":"Controlling Epidemic Spread Under Immunization Delay Constraints","abstract":"In this paper, we study the problem of minimizing the spread of a viral epidemic when immunization takes a non-negligible amount of time to take into effect. Specifically, our problem is to determine which set of nodes to be vaccinated when vaccines take a random amount of time in order to maximize the total reward, which is the expected number of saved nodes. We first provide a mathematical analysis for the reward function of vaccinating an arbitrary number of nodes when there is a single source of infection. While it is infeasible to obtain the optimal solution analytically due to the combinatorial nature of the problem, we establish that the problem is a monotone submodular maximization problem and develop a greedy algorithm that achieves a $(1\\!-\\!1/e)$-approximation. We further extend the scenario to the ones with multiple infection sources and discuss how the greedy algorithm can be applied systematically for the multiple-source scenarios. We finally present extensive simulation results to demonstrate the superiority of our greedy algorithm over other baseline vaccination strategies.","sentences":["In this paper, we study the problem of minimizing the spread of a viral epidemic when immunization takes a non-negligible amount of time to take into effect.","Specifically, our problem is to determine which set of nodes to be vaccinated when vaccines take a random amount of time in order to maximize the total reward, which is the expected number of saved nodes.","We first provide a mathematical analysis for the reward function of vaccinating an arbitrary number of nodes when there is a single source of infection.","While it is infeasible to obtain the optimal solution analytically due to the combinatorial nature of the problem, we establish that the problem is a monotone submodular maximization problem and develop a greedy algorithm that achieves a $(1\\!-\\!1/e)$-approximation.","We further extend the scenario to the ones with multiple infection sources and discuss how the greedy algorithm can be applied systematically for the multiple-source scenarios.","We finally present extensive simulation results to demonstrate the superiority of our greedy algorithm over other baseline vaccination strategies."],"url":"http://arxiv.org/abs/2307.06889v1"}
{"created":"2023-07-13 16:39:08","title":"Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks","abstract":"Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first results proving feature learning occurs in a multi-task setting with a nonlinear model. We show that when the tasks are binary classification problems with labels depending on only $r$ directions within the ambient $d\\gg r$-dimensional input space, executing a simple gradient-based multitask learning algorithm on a two-layer ReLU neural network learns the ground-truth $r$ directions. In particular, any downstream task on the $r$ ground-truth coordinates can be solved by learning a linear classifier with sample and neuron complexity independent of the ambient dimension $d$, while a random feature model requires exponential complexity in $d$ for such a guarantee.","sentences":["Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs.","Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs.","But in practice, neural networks are increasingly often trained on {\\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings.","In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models.","However, multi-task learning via {\\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious.","In this work, we present the first results proving feature learning occurs in a multi-task setting with a nonlinear model.","We show that when the tasks are binary classification problems with labels depending on only $r$ directions within the ambient $d\\gg r$-dimensional input space, executing a simple gradient-based multitask learning algorithm on a two-layer ReLU neural network learns the ground-truth $r$ directions.","In particular, any downstream task on the $r$ ground-truth coordinates can be solved by learning a linear classifier with sample and neuron complexity independent of the ambient dimension $d$, while a random feature model requires exponential complexity in $d$ for such a guarantee."],"url":"http://arxiv.org/abs/2307.06887v1"}
{"created":"2023-07-13 16:39:01","title":"Min-Max Optimization under Delays","abstract":"Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role. As such, several works have extensively analyzed stochastic optimization with delayed gradients. However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning. Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates. First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\\texttt{EG}) to diverge on simple instances for which \\texttt{EG} guarantees convergence in the absence of delays. Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms. Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\\texttt{GDA}) and \\texttt{EG} with delayed updates continue to guarantee convergence to saddle points for convex-concave and strongly convex-strongly concave settings. Our complexity bounds reveal, in a transparent manner, the slow-down in convergence caused by delays.","sentences":["Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role.","As such, several works have extensively analyzed stochastic optimization with delayed gradients.","However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning.","Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates.","First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\\texttt{EG}) to diverge on simple instances for which \\texttt{EG} guarantees convergence in the absence of delays.","Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms.","Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\\texttt{GDA}) and \\texttt{EG} with delayed updates continue to guarantee convergence to saddle points for convex-concave and strongly convex-strongly concave settings.","Our complexity bounds reveal, in a transparent manner, the slow-down in convergence caused by delays."],"url":"http://arxiv.org/abs/2307.06886v1"}
{"created":"2023-07-13 16:25:04","title":"The complexity of non-stationary reinforcement learning","abstract":"The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\\neq$ NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that just $\\textit{adding}$ a new state-action pair is considerably easier to implement.","sentences":["The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning.","We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\\neq$ NP conjecture.","Recall that the number of states in current applications of reinforcement learning is typically astronomical.","In contrast, we show that just $\\textit{adding}$ a new state-action pair is considerably easier to implement."],"url":"http://arxiv.org/abs/2307.06877v1"}
{"created":"2023-07-13 16:20:43","title":"Target Acquired? Evaluating Target Generation Algorithms for IPv6","abstract":"Internet measurements are a crucial foundation of IPv6-related research. Due to the infeasibility of full address space scans for IPv6 however, those measurements rely on collections of reliably responsive, unbiased addresses, as provided e.g., by the IPv6 Hitlist service. Although used for various use cases, the hitlist provides an unfiltered list of responsive addresses, the hosts behind which can come from a range of different networks and devices, such as web servers, customer-premises equipment (CPE) devices, and Internet infrastructure. In this paper, we demonstrate the importance of tailoring hitlists in accordance with the research goal in question. By using PeeringDB we classify hitlist addresses into six different network categories, uncovering that 42% of hitlist addresses are in ISP networks. Moreover, we show the different behavior of those addresses depending on their respective category, e.g., ISP addresses exhibiting a relatively low lifetime. Furthermore, we analyze different Target Generation Algorithms (TGAs), which are used to increase the coverage of IPv6 measurements by generating new responsive targets for scans. We evaluate their performance under various conditions and find generated addresses to show vastly differing responsiveness levels for different TGAs.","sentences":["Internet measurements are a crucial foundation of IPv6-related research.","Due to the infeasibility of full address space scans for IPv6 however, those measurements rely on collections of reliably responsive, unbiased addresses, as provided e.g., by the IPv6 Hitlist service.","Although used for various use cases, the hitlist provides an unfiltered list of responsive addresses, the hosts behind which can come from a range of different networks and devices, such as web servers, customer-premises equipment (CPE) devices, and Internet infrastructure.","In this paper, we demonstrate the importance of tailoring hitlists in accordance with the research goal in question.","By using PeeringDB we classify hitlist addresses into six different network categories, uncovering that 42% of hitlist addresses are in ISP networks.","Moreover, we show the different behavior of those addresses depending on their respective category, e.g., ISP addresses exhibiting a relatively low lifetime.","Furthermore, we analyze different Target Generation Algorithms (TGAs), which are used to increase the coverage of IPv6 measurements by generating new responsive targets for scans.","We evaluate their performance under various conditions and find generated addresses to show vastly differing responsiveness levels for different TGAs."],"url":"http://arxiv.org/abs/2307.06872v1"}
{"created":"2023-07-13 16:19:25","title":"Identifying Early Help Referrals For Local Authorities With Machine Learning And Bias Analysis","abstract":"Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools. This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support. LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18. The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models. Bias mitigation techniques were applied to improve the fairness of these models. During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced data, incorrectly identifying individuals who most likely did not need an Early Help referral. This paper empirically explores the suitability of data-driven ML models for identifying young people who may require Early Help services and discusses their appropriateness and limitations for this task.","sentences":["Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools.","This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support.","LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18.","The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models.","Bias mitigation techniques were applied to improve the fairness of these models.","During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced data, incorrectly identifying individuals who most likely did not need an Early Help referral.","This paper empirically explores the suitability of data-driven ML models for identifying young people who may require Early Help services and discusses their appropriateness and limitations for this task."],"url":"http://arxiv.org/abs/2307.06871v1"}
{"created":"2023-07-13 16:18:55","title":"Embodied Lifelong Learning for Task and Motion Planning","abstract":"A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.","sentences":["A robot deployed in a home over long stretches of time faces a true lifelong learning problem.","As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant.","We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP).","Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner.","Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state.","Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark."],"url":"http://arxiv.org/abs/2307.06870v1"}
{"created":"2023-07-13 16:16:51","title":"DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering","abstract":"Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability.","sentences":["Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability.","Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets.","Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained.","To deal with these challenges, we propose a simple yet effective metric called DecompEval.","This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability.","To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence.","The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result.","Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability."],"url":"http://arxiv.org/abs/2307.06869v1"}
{"created":"2023-07-13 16:15:08","title":"Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success","abstract":"The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.","sentences":["The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query.","The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query.","They have even been treated as commodities to be bought and sold.","However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret.","In this paper, we present a framework for systematically measuring the success of prompt extraction attacks.","In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability."],"url":"http://arxiv.org/abs/2307.06865v1"}
{"created":"2023-07-13 16:09:53","title":"LVLane: Deep Learning for Lane Detection and Classification in Challenging Conditions","abstract":"Lane detection plays a pivotal role in the field of autonomous vehicles and advanced driving assistant systems (ADAS). Over the years, numerous algorithms have emerged, spanning from rudimentary image processing techniques to sophisticated deep neural networks. The performance of deep learning-based models is highly dependent on the quality of their training data. Consequently, these models often experience a decline in performance when confronted with challenging scenarios such as extreme lighting conditions, partially visible lane markings, and sparse lane markings like Botts' dots. To address this, we present an end-to-end lane detection and classification system based on deep learning methodologies. In our study, we introduce a unique dataset meticulously curated to encompass scenarios that pose significant challenges for state-of-the-art (SOTA) models. Through fine-tuning selected models, we aim to achieve enhanced localization accuracy. Moreover, we propose a CNN-based classification branch, seamlessly integrated with the detector, facilitating the identification of distinct lane types. This architecture enables informed lane-changing decisions and empowers more resilient ADAS capabilities. We also investigate the effect of using mixed precision training and testing on different models and batch sizes. Experimental evaluations conducted on the widely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset demonstrate the effectiveness of our model in accurately detecting and classifying lanes amidst challenging scenarios. Our method achieves state-of-the-art classification results on the TuSimple dataset. The code of the work will be published upon the acceptance of the paper.","sentences":["Lane detection plays a pivotal role in the field of autonomous vehicles and advanced driving assistant systems (ADAS).","Over the years, numerous algorithms have emerged, spanning from rudimentary image processing techniques to sophisticated deep neural networks.","The performance of deep learning-based models is highly dependent on the quality of their training data.","Consequently, these models often experience a decline in performance when confronted with challenging scenarios such as extreme lighting conditions, partially visible lane markings, and sparse lane markings like Botts' dots.","To address this, we present an end-to-end lane detection and classification system based on deep learning methodologies.","In our study, we introduce a unique dataset meticulously curated to encompass scenarios that pose significant challenges for state-of-the-art (SOTA) models.","Through fine-tuning selected models, we aim to achieve enhanced localization accuracy.","Moreover, we propose a CNN-based classification branch, seamlessly integrated with the detector, facilitating the identification of distinct lane types.","This architecture enables informed lane-changing decisions and empowers more resilient ADAS capabilities.","We also investigate the effect of using mixed precision training and testing on different models and batch sizes.","Experimental evaluations conducted on the widely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset demonstrate the effectiveness of our model in accurately detecting and classifying lanes amidst challenging scenarios.","Our method achieves state-of-the-art classification results on the TuSimple dataset.","The code of the work will be published upon the acceptance of the paper."],"url":"http://arxiv.org/abs/2307.06853v1"}
{"created":"2023-07-13 16:08:03","title":"Self-Supervised Learning for Interactive Perception of Surgical Thread for Autonomous Suture Tail-Shortening","abstract":"Accurate 3D sensing of suturing thread is a challenging problem in automated surgical suturing because of the high state-space complexity, thinness and deformability of the thread, and possibility of occlusion by the grippers and tissue. In this work we present a method for tracking surgical thread in 3D which is robust to occlusions and complex thread configurations, and apply it to autonomously perform the surgical suture \"tail-shortening\" task: pulling thread through tissue until a desired \"tail\" length remains exposed. The method utilizes a learned 2D surgical thread detection network to segment suturing thread in RGB images. It then identifies the thread path in 2D and reconstructs the thread in 3D as a NURBS spline by triangulating the detections from two stereo cameras. Once a 3D thread model is initialized, the method tracks the thread across subsequent frames. Experiments suggest the method achieves a 1.33 pixel average reprojection error on challenging single-frame 3D thread reconstructions, and an 0.84 pixel average reprojection error on two tracking sequences. On the tail-shortening task, it accomplishes a 90% success rate across 20 trials. Supplemental materials are available at https://sites.google.com/berkeley.edu/autolab-surgical-thread/ .","sentences":["Accurate 3D sensing of suturing thread is a challenging problem in automated surgical suturing because of the high state-space complexity, thinness and deformability of the thread, and possibility of occlusion by the grippers and tissue.","In this work we present a method for tracking surgical thread in 3D which is robust to occlusions and complex thread configurations, and apply it to autonomously perform the surgical suture \"tail-shortening\" task: pulling thread through tissue until a desired \"tail\" length remains exposed.","The method utilizes a learned 2D surgical thread detection network to segment suturing thread in RGB images.","It then identifies the thread path in 2D and reconstructs the thread in 3D as a NURBS spline by triangulating the detections from two stereo cameras.","Once a 3D thread model is initialized, the method tracks the thread across subsequent frames.","Experiments suggest the method achieves a 1.33 pixel average reprojection error on challenging single-frame 3D thread reconstructions, and an 0.84 pixel average reprojection error on two tracking sequences.","On the tail-shortening task, it accomplishes a 90% success rate across 20 trials.","Supplemental materials are available at https://sites.google.com/berkeley.edu/autolab-surgical-thread/ ."],"url":"http://arxiv.org/abs/2307.06845v1"}
{"created":"2023-07-13 16:02:09","title":"Dynamic Capacity Enhancement using Air Computing: An Earthquake Case","abstract":"Earthquakes are one of the most destructive natural disasters harming life and the infrastructure of cities. After an earthquake, functioning communication and computational capacity are crucial for rescue teams and healthcare of victims. Therefore, an earthquake can be investigated for dynamic capacity enhancement in which additional resources are deployed since the surviving portion of the infrastructure may not meet the demand of the users. In this study, we propose a new computation paradigm, air computing, which is the air vehicle assisted next generation edge computing through different air platforms, in order to enhance the capacity of the areas affected by an earthquake. To this end, we put forward a novel paradigm that presents a dynamic, responsive, and high-resolution computation environment by explaining its corresponding components, air layers, and essential advantages. Moreover, we focus on the unmanned aerial vehicle (UAV) deployment problem and apply three different methods including the emergency method, the load balancing method, and the location selection index (LSI) method in which we take the delay requirements of applications into account. To test and compare their performance in terms of the task success rate, we developed an earthquake scenario in which three towns are affected with different severity. The experimental results showed that each method can be beneficial considering the circumstances, and goal of the rescue.","sentences":["Earthquakes are one of the most destructive natural disasters harming life and the infrastructure of cities.","After an earthquake, functioning communication and computational capacity are crucial for rescue teams and healthcare of victims.","Therefore, an earthquake can be investigated for dynamic capacity enhancement in which additional resources are deployed since the surviving portion of the infrastructure may not meet the demand of the users.","In this study, we propose a new computation paradigm, air computing, which is the air vehicle assisted next generation edge computing through different air platforms, in order to enhance the capacity of the areas affected by an earthquake.","To this end, we put forward a novel paradigm that presents a dynamic, responsive, and high-resolution computation environment by explaining its corresponding components, air layers, and essential advantages.","Moreover, we focus on the unmanned aerial vehicle (UAV) deployment problem and apply three different methods including the emergency method, the load balancing method, and the location selection index (LSI) method in which we take the delay requirements of applications into account.","To test and compare their performance in terms of the task success rate, we developed an earthquake scenario in which three towns are affected with different severity.","The experimental results showed that each method can be beneficial considering the circumstances, and goal of the rescue."],"url":"http://arxiv.org/abs/2307.06838v1"}
{"created":"2023-07-13 15:40:38","title":"A Causal Framework to Unify Common Domain Generalization Approaches","abstract":"Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s). It is a fundamental problem in machine learning and has attracted much attention in recent years. A large number of approaches have been proposed. Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area. In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework. Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method? (2) Why is it expected to improve generalization to new domains theoretically? (3) How are different DG methods related to each other and what are relative advantages and limitations? By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective approaches for this critical problem in machine learning.","sentences":["Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s).","It is a fundamental problem in machine learning and has attracted much attention in recent years.","A large number of approaches have been proposed.","Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area.","In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework.","Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method?","(2) Why is it expected to improve generalization to new domains theoretically?","(3) How are different DG methods related to each other and what are relative advantages and limitations?","By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective approaches for this critical problem in machine learning."],"url":"http://arxiv.org/abs/2307.06825v1"}
{"created":"2023-07-13 15:39:26","title":"TinyMetaFed: Efficient Federated Meta-Learning for TinyML","abstract":"The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational efficiency via online learning, and robustness to client heterogeneity through few-shot learning. The evaluations on three TinyML use cases demonstrate that TinyMetaFed can significantly reduce energy consumption and communication overhead, accelerate convergence, and stabilize the training process.","sentences":["The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers.","The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications.","Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world.","However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations.","We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML.","TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices.","It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational efficiency via online learning, and robustness to client heterogeneity through few-shot learning.","The evaluations on three TinyML use cases demonstrate that TinyMetaFed can significantly reduce energy consumption and communication overhead, accelerate convergence, and stabilize the training process."],"url":"http://arxiv.org/abs/2307.06822v1"}
{"created":"2023-07-13 15:23:24","title":"Spatio-Temporal Calibration for Omni-Directional Vehicle-Mounted","abstract":"We present a solution to the problem of spatio-temporal calibration for event cameras mounted on an onmi-directional vehicle. Different from traditional methods that typically determine the camera's pose with respect to the vehicle's body frame using alignment of trajectories, our approach leverages the kinematic correlation of two sets of linear velocity estimates from event data and wheel odometers, respectively. The overall calibration task consists of estimating the underlying temporal offset between the two heterogeneous sensors, and furthermore, recovering the extrinsic rotation that defines the linear relationship between the two sets of velocity estimates. The first sub-problem is formulated as an optimization one, which looks for the optimal temporal offset that maximizes a correlation measurement invariant to arbitrary linear transformation. Once the temporal offset is compensated, the extrinsic rotation can be worked out with an iterative closed-form solver that incrementally registers associated linear velocity estimates. The proposed algorithm is proved effective on both synthetic data and real data, outperforming traditional methods based on alignment of trajectories.","sentences":["We present a solution to the problem of spatio-temporal calibration for event cameras mounted on an onmi-directional vehicle.","Different from traditional methods that typically determine the camera's pose with respect to the vehicle's body frame using alignment of trajectories, our approach leverages the kinematic correlation of two sets of linear velocity estimates from event data and wheel odometers, respectively.","The overall calibration task consists of estimating the underlying temporal offset between the two heterogeneous sensors, and furthermore, recovering the extrinsic rotation that defines the linear relationship between the two sets of velocity estimates.","The first sub-problem is formulated as an optimization one, which looks for the optimal temporal offset that maximizes a correlation measurement invariant to arbitrary linear transformation.","Once the temporal offset is compensated, the extrinsic rotation can be worked out with an iterative closed-form solver that incrementally registers associated linear velocity estimates.","The proposed algorithm is proved effective on both synthetic data and real data, outperforming traditional methods based on alignment of trajectories."],"url":"http://arxiv.org/abs/2307.06810v1"}
{"created":"2023-07-13 15:13:58","title":"Decomposing Finite Languages","abstract":"The paper completely characterizes the primality of acyclic DFAs, where a DFA $\\mathcal{A}$ is prime if there do not exist DFAs $\\mathcal{A}_1,\\dots,\\mathcal{A}_t$ with $\\mathcal{L}(\\mathcal{A}) = \\bigcap_{i=1}^{t} \\mathcal{L}({\\mathcal{A}_i})$ such that each $\\mathcal{A}_i$ has strictly less states than the minimal DFA recognizing the same language as $\\mathcal{A}$. A regular language is prime if its minimal DFA is prime. Thus, this result also characterizes the primality of finite languages.   Further, the $\\mathsf{NL}$-completeness of the corresponding decision problem $\\mathsf{PrimeDFA}_{\\text{fin}}$ is proven. The paper also characterizes the primality of acyclic DFAs under two different notions of compositionality, union and union-intersection compositionality.   Additionally, the paper introduces the notion of S-primality, where a DFA $\\mathcal{A}$ is S-prime if there do not exist DFAs $\\mathcal{A}_1,\\dots,\\mathcal{A}_t$ with $\\mathcal{L}(\\mathcal{A}) = \\bigcap_{i=1}^{t} \\mathcal{L}(\\mathcal{A}_i)$ such that each $\\mathcal{A}_i$ has strictly less states than $\\mathcal{A}$ itself. It is proven that the problem of deciding S-primality for a given DFA is $\\mathsf{NL}$-hard. To do so, the $\\mathsf{NL}$-completeness of $\\mathsf{2MinimalDFA}$, the basic problem of deciding minimality for a DFA with at most two letters, is proven.","sentences":["The paper completely characterizes the primality of acyclic DFAs, where a DFA $\\mathcal{A}$ is prime if there do not exist DFAs $\\mathcal{A}_1,\\dots,\\mathcal{A}_t$ with $\\mathcal{L}(\\mathcal{A}) = \\bigcap_{i=1}^{t} \\mathcal{L}({\\mathcal{A}_i})$ such that each $\\mathcal{A}_i$ has strictly less states than the minimal DFA recognizing the same language as $\\mathcal{A}$. A regular language is prime if its minimal DFA is prime.","Thus, this result also characterizes the primality of finite languages.   ","Further, the $\\mathsf{NL}$-completeness of the corresponding decision problem $\\mathsf{PrimeDFA}_{\\text{fin}}$ is proven.","The paper also characterizes the primality of acyclic DFAs under two different notions of compositionality, union and union-intersection compositionality.   ","Additionally, the paper introduces the notion of S-primality, where a DFA $\\mathcal{A}$ is S-prime if there do not exist DFAs $\\mathcal{A}_1,\\dots,\\mathcal{A}_t$ with $\\mathcal{L}(\\mathcal{A}) = \\bigcap_{i=1}^{t} \\mathcal{L}(\\mathcal{A}_i)$ such that each $\\mathcal{A}_i$ has strictly less states than $\\mathcal{A}$ itself.","It is proven that the problem of deciding S-primality for a given DFA is $\\mathsf{NL}$-hard.","To do so, the $\\mathsf{NL}$-completeness of $\\mathsf{2MinimalDFA}$",", the basic problem of deciding minimality for a DFA with at most two letters, is proven."],"url":"http://arxiv.org/abs/2307.06802v1"}
{"created":"2023-07-13 15:08:44","title":"Fast and Functional Structured Data Generators Rooted in Out-of-Equilibrium Physics","abstract":"In this study, we address the challenge of using energy-based models to produce high-quality, label-specific data in complex structured datasets, such as population genetics, RNA or protein sequences data. Traditional training methods encounter difficulties due to inefficient Markov chain Monte Carlo mixing, which affects the diversity of synthetic data and increases generation times. To address these issues, we use a novel training algorithm that exploits non-equilibrium effects. This approach, applied on the Restricted Boltzmann Machine, improves the model's ability to correctly classify samples and generate high-quality synthetic data in only a few sampling steps. The effectiveness of this method is demonstrated by its successful application to four different types of data: handwritten digits, mutations of human genomes classified by continental origin, functionally characterized sequences of an enzyme protein family, and homologous RNA sequences from specific taxonomies.","sentences":["In this study, we address the challenge of using energy-based models to produce high-quality, label-specific data in complex structured datasets, such as population genetics, RNA or protein sequences data.","Traditional training methods encounter difficulties due to inefficient Markov chain Monte Carlo mixing, which affects the diversity of synthetic data and increases generation times.","To address these issues, we use a novel training algorithm that exploits non-equilibrium effects.","This approach, applied on the Restricted Boltzmann Machine, improves the model's ability to correctly classify samples and generate high-quality synthetic data in only a few sampling steps.","The effectiveness of this method is demonstrated by its successful application to four different types of data: handwritten digits, mutations of human genomes classified by continental origin, functionally characterized sequences of an enzyme protein family, and homologous RNA sequences from specific taxonomies."],"url":"http://arxiv.org/abs/2307.06797v1"}
{"created":"2023-07-13 15:05:34","title":"Leveraging Vision-Language Foundation Models for Fine-Grained Downstream Tasks","abstract":"Vision-language foundation models such as CLIP have shown impressive zero-shot performance on many tasks and datasets, especially thanks to their free-text inputs. However, they struggle to handle some downstream tasks, such as fine-grained attribute detection and localization. In this paper, we propose a multitask fine-tuning strategy based on a positive/negative prompt formulation to further leverage the capacities of the vision-language foundation models. Using the CLIP architecture as baseline, we show strong improvements on bird fine-grained attribute detection and localization tasks, while also increasing the classification performance on the CUB200-2011 dataset. We provide source code for reproducibility purposes: it is available at https://github.com/FactoDeepLearning/MultitaskVLFM.","sentences":["Vision-language foundation models such as CLIP have shown impressive zero-shot performance on many tasks and datasets, especially thanks to their free-text inputs.","However, they struggle to handle some downstream tasks, such as fine-grained attribute detection and localization.","In this paper, we propose a multitask fine-tuning strategy based on a positive/negative prompt formulation to further leverage the capacities of the vision-language foundation models.","Using the CLIP architecture as baseline, we show strong improvements on bird fine-grained attribute detection and localization tasks, while also increasing the classification performance on the CUB200-2011 dataset.","We provide source code for reproducibility purposes: it is available at https://github.com/FactoDeepLearning/MultitaskVLFM."],"url":"http://arxiv.org/abs/2307.06795v1"}
{"created":"2023-07-13 15:03:48","title":"Negated Complementary Commonsense using Large Language Models","abstract":"Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense.","sentences":["Larger language models, such as GPT-3, have shown to be excellent in many tasks.","However, we demonstrate that out-of-ordinary questions can throw the model off guard.","This work focuses on finding answers to negated complementary questions in commonsense scenarios.","We illustrate how such questions adversely affect the model responses.","We propose a model-agnostic methodology to improve the performance in negated complementary scenarios.","Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions.","The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense."],"url":"http://arxiv.org/abs/2307.06794v1"}
{"created":"2023-07-13 15:02:49","title":"Planar Disjoint Paths, Treewidth, and Kernels","abstract":"In the Planar Disjoint Paths problem, one is given an undirected planar graph with a set of $k$ vertex pairs $(s_i,t_i)$ and the task is to find $k$ pairwise vertex-disjoint paths such that the $i$-th path connects $s_i$ to $t_i$. We study the problem through the lens of kernelization, aiming at efficiently reducing the input size in terms of a parameter. We show that Planar Disjoint Paths does not admit a polynomial kernel when parameterized by $k$ unless coNP $\\subseteq$ NP/poly, resolving an open problem by [Bodlaender, Thomass{\\'e}, Yeo, ESA'09]. Moreover, we rule out the existence of a polynomial Turing kernel unless the WK-hierarchy collapses. Our reduction carries over to the setting of edge-disjoint paths, where the kernelization status remained open even in general graphs.   On the positive side, we present a polynomial kernel for Planar Disjoint Paths parameterized by $k + tw$, where $tw$ denotes the treewidth of the input graph. As a consequence of both our results, we rule out the possibility of a polynomial-time (Turing) treewidth reduction to $tw= k^{O(1)}$ under the same assumptions. To the best of our knowledge, this is the first hardness result of this kind. Finally, combining our kernel with the known techniques [Adler, Kolliopoulos, Krause, Lokshtanov, Saurabh, Thilikos, JCTB'17; Schrijver, SICOMP'94] yields an alternative (and arguably simpler) proof that Planar Disjoint Paths can be solved in time $2^{O(k^2)}\\cdot n^{O(1)}$, matching the result of [Lokshtanov, Misra, Pilipczuk, Saurabh, Zehavi, STOC'20].","sentences":["In the Planar Disjoint Paths problem, one is given an undirected planar graph with a set of $k$ vertex pairs $(s_i,t_i)$ and the task is to find $k$ pairwise vertex-disjoint paths such that the $i$-th path connects $s_i$ to $t_i$. We study the problem through the lens of kernelization, aiming at efficiently reducing the input size in terms of a parameter.","We show that Planar Disjoint Paths does not admit a polynomial kernel when parameterized by $k$ unless coNP $\\subseteq$ NP/poly, resolving an open problem by [Bodlaender, Thomass{\\'e}, Yeo, ESA'09].","Moreover, we rule out the existence of a polynomial Turing kernel unless the WK-hierarchy collapses.","Our reduction carries over to the setting of edge-disjoint paths, where the kernelization status remained open even in general graphs.   ","On the positive side, we present a polynomial kernel for Planar Disjoint Paths parameterized by $k + tw$, where $tw$ denotes the treewidth of the input graph.","As a consequence of both our results, we rule out the possibility of a polynomial-time (Turing) treewidth reduction to $tw= k^{O(1)}$ under the same assumptions.","To the best of our knowledge, this is the first hardness result of this kind.","Finally, combining our kernel with the known techniques [Adler, Kolliopoulos, Krause, Lokshtanov, Saurabh, Thilikos, JCTB'17; Schrijver, SICOMP'94] yields an alternative (and arguably simpler) proof that Planar Disjoint Paths can be solved in time $2^{O(k^2)}\\cdot n^{O(1)}$, matching the result of [Lokshtanov, Misra, Pilipczuk, Saurabh, Zehavi, STOC'20]."],"url":"http://arxiv.org/abs/2307.06792v1"}
{"created":"2023-07-13 14:59:22","title":"scda: A Minimal, Serial-Equivalent Format for Parallel I/O","abstract":"We specify a file-oriented data format suitable for parallel, partition-independent disk I/O. Here, a partition refers to a disjoint and ordered distribution of the data elements between one or more processes. The format is designed such that the file contents are invariant under linear (i. e., unpermuted), parallel repartition of the data prior to writing. The file contents are indistinguishable from writing in serial. In the same vein, the file can be read on any number of processes that agree on any partition of the number of elements stored.   In addition to the format specification we propose an optional convention to implement transparent per-element data compression. The compressed data and metadata is layered inside ordinary format elements. Overall, we pay special attention to both human and machine readability. If pure ASCII data is written, or compressed data is reencoded to ASCII, the entire file including its header and sectioning metadata remains entirely in ASCII. If binary data is written, the metadata stays easy on the human eye.   We refer to this format as scda. Conceptually, it lies one layer below and is oblivious to the definition of variables, the binary representation of numbers, considerations of endianness, and self-describing headers, which may all be specified on top of scda. The main purpose of the format is to abstract any parallelism and provide sufficient structure as a foundation for a generic and flexible archival and checkpoint/restart. A documented reference implementation is available as part of the general-purpose libsc free software library.","sentences":["We specify a file-oriented data format suitable for parallel, partition-independent disk I/O. Here, a partition refers to a disjoint and ordered distribution of the data elements between one or more processes.","The format is designed such that the file contents are invariant under linear (i. e., unpermuted), parallel repartition of the data prior to writing.","The file contents are indistinguishable from writing in serial.","In the same vein, the file can be read on any number of processes that agree on any partition of the number of elements stored.   ","In addition to the format specification we propose an optional convention to implement transparent per-element data compression.","The compressed data and metadata is layered inside ordinary format elements.","Overall, we pay special attention to both human and machine readability.","If pure ASCII data is written, or compressed data is reencoded to ASCII, the entire file including its header and sectioning metadata remains entirely in ASCII.","If binary data is written, the metadata stays easy on the human eye.   ","We refer to this format as scda.","Conceptually, it lies one layer below and is oblivious to the definition of variables, the binary representation of numbers, considerations of endianness, and self-describing headers, which may all be specified on top of scda.","The main purpose of the format is to abstract any parallelism and provide sufficient structure as a foundation for a generic and flexible archival and checkpoint/restart.","A documented reference implementation is available as part of the general-purpose libsc free software library."],"url":"http://arxiv.org/abs/2307.06789v1"}
{"created":"2023-07-13 14:50:38","title":"Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation","abstract":"This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics. A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments. To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm. Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack. A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator. The motion planner divides the crack into multiple nodes which are then explored individually. Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm. If a crack is detected, also the length, width, orientation and number of branches are calculated. This is repeated until all the nodes of the crack are explored.   In order to validate the complete algorithm, various experiments are performed: comparison of exploration of cracks through full scan and motion planning algorithm, implementation of frequency-based features for crack classification and geometry analysis using a combination of vision and tactile data. From the results of the experiments, it is shown that the proposed algorithm is able to detect cracks and improve the results obtained from vision to correctly classify cracks and their geometry with minimal cost thanks to the motion planning algorithm.","sentences":["This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics.","A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments.","To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm.","Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack.","A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator.","The motion planner divides the crack into multiple nodes which are then explored individually.","Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm.","If a crack is detected, also the length, width, orientation and number of branches are calculated.","This is repeated until all the nodes of the crack are explored.   ","In order to validate the complete algorithm, various experiments are performed: comparison of exploration of cracks through full scan and motion planning algorithm, implementation of frequency-based features for crack classification and geometry analysis using a combination of vision and tactile data.","From the results of the experiments, it is shown that the proposed algorithm is able to detect cracks and improve the results obtained from vision to correctly classify cracks and their geometry with minimal cost thanks to the motion planning algorithm."],"url":"http://arxiv.org/abs/2307.06784v1"}
{"created":"2023-07-13 14:38:15","title":"Data Behind the Walls An Advanced Architecture for Data Privacy Management","abstract":"In today's highly connected society, we are constantly asked to provide personal information to retailers, voter surveys, medical professionals, and other data collection efforts. The collected data is stored in large data warehouses. Organisations and statistical agencies share and use this data to facilitate research in public health, economics, sociology, etc. However, this data contains sensitive information about individuals, which can result in identity theft, financial loss, stress and depression, embarrassment, abuse, etc. Therefore, one must ensure rigorous management of individuals' privacy. We propose, an advanced data privacy management architecture composed of three layers. The data management layer consists of de-identification and anonymisation, the access management layer for re-enforcing data access based on the concepts of Role-Based Access Control and the Chinese Wall Security Policy, and the roles layer for regulating different users. The proposed system architecture is validated on healthcare datasets.","sentences":["In today's highly connected society, we are constantly asked to provide personal information to retailers, voter surveys, medical professionals, and other data collection efforts.","The collected data is stored in large data warehouses.","Organisations and statistical agencies share and use this data to facilitate research in public health, economics, sociology, etc.","However, this data contains sensitive information about individuals, which can result in identity theft, financial loss, stress and depression, embarrassment, abuse, etc.","Therefore, one must ensure rigorous management of individuals' privacy.","We propose, an advanced data privacy management architecture composed of three layers.","The data management layer consists of de-identification and anonymisation, the access management layer for re-enforcing data access based on the concepts of Role-Based Access Control and the Chinese Wall Security Policy, and the roles layer for regulating different users.","The proposed system architecture is validated on healthcare datasets."],"url":"http://arxiv.org/abs/2307.06779v1"}
{"created":"2023-07-13 14:34:18","title":"Deciding Conjugacy of a Rational Relation","abstract":"A rational relation is conjugate if every pair of related words are conjugates. It is shown that checking whether a rational relation is conjugate is decidable. For this, we generalise the Lyndon-Sch\\\"utzenberger's theorem from word combinatorics. A consequence of the generalisation is that a set of pairs generated by a sumfree rational expression is conjugate if and only if there is a word witnessing the conjugacy of all the pairs.","sentences":["A rational relation is conjugate if every pair of related words are conjugates.","It is shown that checking whether a rational relation is conjugate is decidable.","For this, we generalise the Lyndon-Sch\\\"utzenberger's theorem from word combinatorics.","A consequence of the generalisation is that a set of pairs generated by a sumfree rational expression is conjugate if and only if there is a word witnessing the conjugacy of all the pairs."],"url":"http://arxiv.org/abs/2307.06777v1"}
{"created":"2023-07-13 14:33:42","title":"Approximation algorithms for the square min-sum bin packing problem","abstract":"In this work, we study the square min-sum bin packing problem (SMSBPP), where a list of square items has to be packed into indexed square bins of dimensions $1 \\times 1$ with no overlap between the areas of the items. The bins are indexed and the cost of packing each item is equal to the index of the bin in which it is placed in. The objective is to minimize the total cost of packing all items, which is equivalent to minimizing the average cost of items. The problem has applications in minimizing the average time of logistic operations such as cutting stock and delivery of products. We prove that classic algorithms for two-dimensional bin packing that order items in non-increasing order of size, such as Next Fit Decreasing Height or Any Fit Decreasing Height heuristics, can have an arbitrarily bad performance for SMSBPP. We, then, present a $\\frac{53}{22}$-approximation and a PTAS for the problem.","sentences":["In this work, we study the square min-sum bin packing problem (SMSBPP), where a list of square items has to be packed into indexed square bins of dimensions $1 \\times 1$ with no overlap between the areas of the items.","The bins are indexed and the cost of packing each item is equal to the index of the bin in which it is placed in.","The objective is to minimize the total cost of packing all items, which is equivalent to minimizing the average cost of items.","The problem has applications in minimizing the average time of logistic operations such as cutting stock and delivery of products.","We prove that classic algorithms for two-dimensional bin packing that order items in non-increasing order of size, such as Next Fit Decreasing Height or Any Fit Decreasing Height heuristics, can have an arbitrarily bad performance for SMSBPP.","We, then, present a $\\frac{53}{22}$-approximation and a PTAS for the problem."],"url":"http://arxiv.org/abs/2307.06776v1"}
{"created":"2023-07-13 14:25:09","title":"Stackelberg Vertex Cover on a Path","abstract":"A Stackelberg Vertex Cover game is played on an undirected graph $\\mathcal{G}$ where some of the vertices are under the control of a \\emph{leader}. The remaining vertices are assigned a fixed weight. The game is played in two stages. First, the leader chooses prices for the vertices under her control. Afterward, the second player, called \\emph{follower}, selects a min weight vertex cover in the resulting weighted graph. That is, the follower selects a subset of vertices $C^*$ such that every edge has at least one endpoint in $C^*$ of minimum weight w.r.t.\\ to the fixed weights, and the prices set by the leader. Stackelberg Vertex Cover (StackVC) describes the leader's optimization problem to select prices in the first stage of the game so as to maximize her revenue, which is the cumulative price of all her (priceable) vertices that are contained in the follower's solution. Previous research showed that StackVC is \\textsf{NP}-hard on bipartite graphs, but solvable in polynomial time in the special case of bipartite graphs, where all priceable vertices belong to the same side of the bipartition. In this paper, we investigate StackVC on paths and present a dynamic program with linear time and space complexity.","sentences":["A Stackelberg Vertex Cover game is played on an undirected graph $\\mathcal{G}$ where some of the vertices are under the control of a \\emph{leader}.","The remaining vertices are assigned a fixed weight.","The game is played in two stages.","First, the leader chooses prices for the vertices under her control.","Afterward, the second player, called \\emph{follower}, selects a min weight vertex cover in the resulting weighted graph.","That is, the follower selects a subset of vertices $C^*$ such that every edge has at least one endpoint in $C^*$ of minimum weight w.r.t.\\ to the fixed weights, and the prices set by the leader.","Stackelberg Vertex Cover (StackVC) describes the leader's optimization problem to select prices in the first stage of the game so as to maximize her revenue, which is the cumulative price of all her (priceable) vertices that are contained in the follower's solution.","Previous research showed that StackVC is \\textsf{NP}-hard on bipartite graphs, but solvable in polynomial time in the special case of bipartite graphs, where all priceable vertices belong to the same side of the bipartition.","In this paper, we investigate StackVC on paths and present a dynamic program with linear time and space complexity."],"url":"http://arxiv.org/abs/2307.06772v1"}
{"created":"2023-07-13 14:11:40","title":"An update on the coin-moving game on the square grid","abstract":"This paper extends the work started in 2002 by Demaine, Demaine and Verill (DDV) on coin-moving puzzles. These puzzles have a long history in the recreational literature, but were first systematically analyzed by DDV, who gave a full characterization of the solvable puzzles on the triangular grid and a partial characterization of the solvable puzzles on the square grid. This article specifically extends the study of the game on the square grid. Notably, DDV's result on puzzles with two \"extra coins\" is shown to be overly broad: this paper provides counterexamples as well as a revised version of this theorem. A new method for solving puzzles with two extra coins is then presented, which covers some cases where the aforementioned theorem does not apply. Puzzles with just one extra coin seem even more complicated, and are only touched upon by DDV. This paper delves deeper, studying a class of such puzzles that may be considered equivalent to a game of \"poking\" coins. Within this class, some cases are considered that are amenable to analysis.","sentences":["This paper extends the work started in 2002 by Demaine, Demaine and Verill (DDV) on coin-moving puzzles.","These puzzles have a long history in the recreational literature, but were first systematically analyzed by DDV, who gave a full characterization of the solvable puzzles on the triangular grid and a partial characterization of the solvable puzzles on the square grid.","This article specifically extends the study of the game on the square grid.","Notably, DDV's result on puzzles with two \"extra coins\" is shown to be overly broad: this paper provides counterexamples as well as a revised version of this theorem.","A new method for solving puzzles with two extra coins is then presented, which covers some cases where the aforementioned theorem does not apply.","Puzzles with just one extra coin seem even more complicated, and are only touched upon by DDV.","This paper delves deeper, studying a class of such puzzles that may be considered equivalent to a game of \"poking\" coins.","Within this class, some cases are considered that are amenable to analysis."],"url":"http://arxiv.org/abs/2307.06767v1"}
{"created":"2023-07-13 14:02:54","title":"Retroactive Parametrized Monitoring","abstract":"In online monitoring, we first synthesize a monitor from a formal specification, which later runs in tandem with the system under study, incrementally receiving its progress and evolving with the system. In offline monitoring the trace is logged as the system progresses to later do post-mortem analysis after the system has finished executing.   In this paper we propose retroactive dynamic parametrization, a technique that allows a monitor to revisit the past log as it progresses, while still executing in an online manner. This feature allows new monitors to be incorporated into a running system and to revisit the past for particular behaviors based on new information discovered. Retroactive parametrization also allows a monitor to lazily ignore events and revisit and process them later, when it discovers that it should have followed those events. We showcase the use of retroactive dynamic parametrization to monitor denial of service attacks on a network using network logs.","sentences":["In online monitoring, we first synthesize a monitor from a formal specification, which later runs in tandem with the system under study, incrementally receiving its progress and evolving with the system.","In offline monitoring the trace is logged as the system progresses to later do post-mortem analysis after the system has finished executing.   ","In this paper we propose retroactive dynamic parametrization, a technique that allows a monitor to revisit the past log as it progresses, while still executing in an online manner.","This feature allows new monitors to be incorporated into a running system and to revisit the past for particular behaviors based on new information discovered.","Retroactive parametrization also allows a monitor to lazily ignore events and revisit and process them later, when it discovers that it should have followed those events.","We showcase the use of retroactive dynamic parametrization to monitor denial of service attacks on a network using network logs."],"url":"http://arxiv.org/abs/2307.06763v1"}
{"created":"2023-07-13 13:59:54","title":"Privacy-Utility Trade-offs in Neural Networks for Medical Population Graphs: Insights from Differential Privacy and Graph Structure","abstract":"We initiate an empirical investigation into differentially private graph neural networks on population graphs from the medical domain by examining privacy-utility trade-offs at different privacy levels on both real-world and synthetic datasets and performing auditing through membership inference attacks. Our findings highlight the potential and the challenges of this specific DP application area. Moreover, we find evidence that the underlying graph structure constitutes a potential factor for larger performance gaps by showing a correlation between the degree of graph homophily and the accuracy of the trained model.","sentences":["We initiate an empirical investigation into differentially private graph neural networks on population graphs from the medical domain by examining privacy-utility trade-offs at different privacy levels on both real-world and synthetic datasets and performing auditing through membership inference attacks.","Our findings highlight the potential and the challenges of this specific DP application area.","Moreover, we find evidence that the underlying graph structure constitutes a potential factor for larger performance gaps by showing a correlation between the degree of graph homophily and the accuracy of the trained model."],"url":"http://arxiv.org/abs/2307.06760v1"}
{"created":"2023-07-13 13:56:27","title":"Layered controller synthesis for dynamic multi-agent systems","abstract":"In this paper we present a layered approach for multi-agent control problem, decomposed into three stages, each building upon the results of the previous one. First, a high-level plan for a coarse abstraction of the system is computed, relying on parametric timed automata augmented with stopwatches as they allow to efficiently model simplified dynamics of such systems. In the second stage, the high-level plan, based on SMT-formulation, mainly handles the combinatorial aspects of the problem, provides a more dynamically accurate solution. These stages are collectively referred to as the SWA-SMT solver. They are correct by construction but lack a crucial feature: they cannot be executed in real time. To overcome this, we use SWA-SMT solutions as the initial training dataset for our last stage, which aims at obtaining a neural network control policy. We use reinforcement learning to train the policy, and show that the initial dataset is crucial for the overall success of the method.","sentences":["In this paper we present a layered approach for multi-agent control problem, decomposed into three stages, each building upon the results of the previous one.","First, a high-level plan for a coarse abstraction of the system is computed, relying on parametric timed automata augmented with stopwatches as they allow to efficiently model simplified dynamics of such systems.","In the second stage, the high-level plan, based on SMT-formulation, mainly handles the combinatorial aspects of the problem, provides a more dynamically accurate solution.","These stages are collectively referred to as the SWA-SMT solver.","They are correct by construction but lack a crucial feature: they cannot be executed in real time.","To overcome this, we use SWA-SMT solutions as the initial training dataset for our last stage, which aims at obtaining a neural network control policy.","We use reinforcement learning to train the policy, and show that the initial dataset is crucial for the overall success of the method."],"url":"http://arxiv.org/abs/2307.06758v1"}
{"created":"2023-07-13 13:52:07","title":"PREFENDER: A Prefetching Defender against Cache Side Channel Attacks as A Pretender","abstract":"Cache side channel attacks are increasingly alarming in modern processors due to the recent emergence of Spectre and Meltdown attacks. A typical attack performs intentional cache access and manipulates cache states to leak secrets by observing the victim's cache access patterns. Different countermeasures have been proposed to defend against both general and transient execution based attacks. Despite their effectiveness, they mostly trade some level of performance for security, or have restricted security scope. In this paper, we seek an approach to enforcing security while maintaining performance. We leverage the insight that attackers need to access cache in order to manipulate and observe cache state changes for information leakage. Specifically, we propose Prefender, a secure prefetcher that learns and predicts attack-related accesses for prefetching the cachelines to simultaneously help security and performance. Our results show that Prefender is effective against several cache side channel attacks while maintaining or even improving performance for SPEC CPU 2006 and 2017 benchmarks.","sentences":["Cache side channel attacks are increasingly alarming in modern processors due to the recent emergence of Spectre and Meltdown attacks.","A typical attack performs intentional cache access and manipulates cache states to leak secrets by observing the victim's cache access patterns.","Different countermeasures have been proposed to defend against both general and transient execution based attacks.","Despite their effectiveness, they mostly trade some level of performance for security, or have restricted security scope.","In this paper, we seek an approach to enforcing security while maintaining performance.","We leverage the insight that attackers need to access cache in order to manipulate and observe cache state changes for information leakage.","Specifically, we propose Prefender, a secure prefetcher that learns and predicts attack-related accesses for prefetching the cachelines to simultaneously help security and performance.","Our results show that Prefender is effective against several cache side channel attacks while maintaining or even improving performance for SPEC CPU 2006 and 2017 benchmarks."],"url":"http://arxiv.org/abs/2307.06756v1"}
{"created":"2023-07-13 13:43:02","title":"Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent","abstract":"The learning of Gaussian Mixture Models (also referred to simply as GMMs) plays an important role in machine learning. Known for their expressiveness and interpretability, Gaussian mixture models have a wide range of applications, from statistics, computer vision to distributional reinforcement learning. However, as of today, few known algorithms can fit or learn these models, some of which include Expectation-Maximization algorithms and Sliced Wasserstein Distance. Even fewer algorithms are compatible with gradient descent, the common learning process for neural networks.   In this paper, we derive a closed formula of two GMMs in the univariate, one-dimensional case, then propose a distance function called Sliced Cram\\'er 2-distance for learning general multivariate GMMs. Our approach has several advantages over many previous methods. First, it has a closed-form expression for the univariate case and is easy to compute and implement using common machine learning libraries (e.g., PyTorch and TensorFlow). Second, it is compatible with gradient descent, which enables us to integrate GMMs with neural networks seamlessly. Third, it can fit a GMM not only to a set of data points, but also to another GMM directly, without sampling from the target model. And fourth, it has some theoretical guarantees like global gradient boundedness and unbiased sampling gradient. These features are especially useful for distributional reinforcement learning and Deep Q Networks, where the goal is to learn a distribution over future rewards. We will also construct a Gaussian Mixture Distributional Deep Q Network as a toy example to demonstrate its effectiveness. Compared with previous models, this model is parameter efficient in terms of representing a distribution and possesses better interpretability.","sentences":["The learning of Gaussian Mixture Models (also referred to simply as GMMs) plays an important role in machine learning.","Known for their expressiveness and interpretability, Gaussian mixture models have a wide range of applications, from statistics, computer vision to distributional reinforcement learning.","However, as of today, few known algorithms can fit or learn these models, some of which include Expectation-Maximization algorithms and Sliced Wasserstein Distance.","Even fewer algorithms are compatible with gradient descent, the common learning process for neural networks.   ","In this paper, we derive a closed formula of two GMMs in the univariate, one-dimensional case, then propose a distance function called Sliced Cram\\'er 2-distance for learning general multivariate GMMs.","Our approach has several advantages over many previous methods.","First, it has a closed-form expression for the univariate case and is easy to compute and implement using common machine learning libraries (e.g., PyTorch and TensorFlow).","Second, it is compatible with gradient descent, which enables us to integrate GMMs with neural networks seamlessly.","Third, it can fit a GMM not only to a set of data points, but also to another GMM directly, without sampling from the target model.","And fourth, it has some theoretical guarantees like global gradient boundedness and unbiased sampling gradient.","These features are especially useful for distributional reinforcement learning and Deep Q Networks, where the goal is to learn a distribution over future rewards.","We will also construct a Gaussian Mixture Distributional Deep Q Network as a toy example to demonstrate its effectiveness.","Compared with previous models, this model is parameter efficient in terms of representing a distribution and possesses better interpretability."],"url":"http://arxiv.org/abs/2307.06753v1"}
{"created":"2023-07-13 13:42:44","title":"(k-2)-linear connected components in hypergraphs of rank k","abstract":"We define a $q$-linear path in a hypergraph $H$ as a sequence $(e_1,\\ldots,e_L)$ of edges of $H$ such that $|e_i \\cap e_{i+1}| \\in [\\![1,q]\\!]$ and $e_i \\cap e_j=\\varnothing$ if $|i-j|>1$. In this paper, we study the connected components associated to these paths when $q=k-2$ where $k$ is the rank of $H$. If $k=3$ then $q=1$ which coincides with the well-known notion of linear path or loose path. We describe the structure of the connected components, using an algorithmic proof which shows that the connected components can be computed in polynomial time. We then mention two consequences of our algorithmic result. The first one is that deciding the winner of the Maker-Breaker game on a hypergraph of rank 3 can be done in polynomial time. The second one is that tractable cases for the NP-complete problem of \"Paths Avoiding Forbidden Pairs\" in a graph can be deduced from the recognition of a special type of line graph of a hypergraph.","sentences":["We define a $q$-linear path in a hypergraph $H$ as a sequence $(e_1,\\ldots,e_L)$ of edges of $H$ such that $|e_i \\cap e_{i+1}| \\in","[\\![1,q]\\!]$ and $e_i \\cap e_j=\\varnothing$ if $|i-j|>1$.","In this paper, we study the connected components associated to these paths when $q=k-2$ where $k$ is the rank of $H$. If $k=3$ then $q=1$ which coincides with the well-known notion of linear path or loose path.","We describe the structure of the connected components, using an algorithmic proof which shows that the connected components can be computed in polynomial time.","We then mention two consequences of our algorithmic result.","The first one is that deciding the winner of the Maker-Breaker game on a hypergraph of rank 3 can be done in polynomial time.","The second one is that tractable cases for the NP-complete problem of \"Paths Avoiding Forbidden Pairs\" in a graph can be deduced from the recognition of a special type of line graph of a hypergraph."],"url":"http://arxiv.org/abs/2307.06752v1"}
{"created":"2023-07-13 13:41:32","title":"Watch Your Pose: Unsupervised Domain Adaption with Pose based Triplet Selection for Gait Recognition","abstract":"Gait Recognition is a computer vision task aiming to identify people by their walking patterns. Existing methods show impressive results on individual datasets but lack the ability to generalize to unseen scenarios. Unsupervised Domain Adaptation (UDA) tries to adapt a model, pre-trained in a supervised manner on a source domain, to an unlabelled target domain. UDA for Gait Recognition is still in its infancy and existing works proposed solutions to limited scenarios. In this paper, we reveal a fundamental phenomenon in adaptation of gait recognition models, in which the target domain is biased to pose-based features rather than identity features, causing a significant performance drop in the identification task. We suggest Gait Orientation-based method for Unsupervised Domain Adaptation (GOUDA) to reduce this bias. To this end, we present a novel Triplet Selection algorithm with a curriculum learning framework, aiming to adapt the embedding space by pushing away samples of similar poses and bringing closer samples of different poses. We provide extensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP, GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL, showing the superiority of our proposed method over prior works.","sentences":["Gait Recognition is a computer vision task aiming to identify people by their walking patterns.","Existing methods show impressive results on individual datasets but lack the ability to generalize to unseen scenarios.","Unsupervised Domain Adaptation (UDA) tries to adapt a model, pre-trained in a supervised manner on a source domain, to an unlabelled target domain.","UDA for Gait Recognition is still in its infancy and existing works proposed solutions to limited scenarios.","In this paper, we reveal a fundamental phenomenon in adaptation of gait recognition models, in which the target domain is biased to pose-based features rather than identity features, causing a significant performance drop in the identification task.","We suggest Gait Orientation-based method for Unsupervised Domain Adaptation (GOUDA) to reduce this bias.","To this end, we present a novel Triplet Selection algorithm with a curriculum learning framework, aiming to adapt the embedding space by pushing away samples of similar poses and bringing closer samples of different poses.","We provide extensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP, GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL, showing the superiority of our proposed method over prior works."],"url":"http://arxiv.org/abs/2307.06751v1"}
{"created":"2023-07-13 13:31:25","title":"Smart Cities and Digital Twins in Lower Austria","abstract":"Smart city solutions require innovative governance approaches together with the smart use of technology, such as digital twins, by city managers and policymakers to manage the big societal challenges. The project Smart Cities aNd Digital Twins in Lower Austria (SCiNDTiLA) extends the state of the art of research in several contributing disciplines and uses the foundations of complexity theory and computational social science methods to develop a digital-twin-based smart city model. The project will also apply a novel transdisciplinary process to conceptualise sustainable smart cities and validate the smart city generic model. The outcomes will be translated into a roadmap highlighting methodologies, guidelines and policy recommendations for tackling societal challenges in smart cities with a focus on rescaling the entire framework to be transferred to regions, smaller towns and non-urban environments, such as rural areas and smart villages, in ways that fit the respective local governance, ethical and operational capacity context.","sentences":["Smart city solutions require innovative governance approaches together with the smart use of technology, such as digital twins, by city managers and policymakers to manage the big societal challenges.","The project Smart Cities aNd Digital Twins in Lower Austria (SCiNDTiLA) extends the state of the art of research in several contributing disciplines and uses the foundations of complexity theory and computational social science methods to develop a digital-twin-based smart city model.","The project will also apply a novel transdisciplinary process to conceptualise sustainable smart cities and validate the smart city generic model.","The outcomes will be translated into a roadmap highlighting methodologies, guidelines and policy recommendations for tackling societal challenges in smart cities with a focus on rescaling the entire framework to be transferred to regions, smaller towns and non-urban environments, such as rural areas and smart villages, in ways that fit the respective local governance, ethical and operational capacity context."],"url":"http://arxiv.org/abs/2307.06743v1"}
{"created":"2023-07-13 13:18:38","title":"Closeness Centralities of Lollipop Graphs","abstract":"Closeness is one of the most studied characteristics of networks. Residual closeness is a very sensitive measure of graphs robustness. Additional closeness is a measure of growth potentials of networks. In this article we calculate the closeness, vertex residual closeness, link residual closeness, and additional closeness of lollipop graphs.","sentences":["Closeness is one of the most studied characteristics of networks.","Residual closeness is a very sensitive measure of graphs robustness.","Additional closeness is a measure of growth potentials of networks.","In this article we calculate the closeness, vertex residual closeness, link residual closeness, and additional closeness of lollipop graphs."],"url":"http://arxiv.org/abs/2307.06738v1"}
{"created":"2023-07-13 13:17:50","title":"Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data","abstract":"Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios. We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and poses. We introduce a new method for synthetic data generation - RePoGen, RarE POses GENerator - with comprehensive control over pose and view to augment the COCO dataset. Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top-view pose estimation and significantly improves performance on the bottom-view dataset. Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodological choices and demonstrate improved performance. The code and the datasets are available on the project website.","sentences":["Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios.","We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and poses.","We introduce a new method for synthetic data generation - RePoGen, RarE POses GENerator - with comprehensive control over pose and view to augment the COCO dataset.","Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top-view pose estimation and significantly improves performance on the bottom-view dataset.","Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodological choices and demonstrate improved performance.","The code and the datasets are available on the project website."],"url":"http://arxiv.org/abs/2307.06737v1"}
{"created":"2023-07-13 13:16:01","title":"MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting","abstract":"Time series forecasting has received wide interest from existing research due to its broad applications and inherent challenging. The research challenge lies in identifying effective patterns in historical series and applying them to future forecasting. Advanced models based on point-wise connected MLP and Transformer architectures have strong fitting power, but their secondary computational complexity limits practicality. Additionally, those structures inherently disrupt the temporal order, reducing the information utilization and making the forecasting process uninterpretable. To solve these problems, this paper proposes a forecasting model, MPR-Net. It first adaptively decomposes multi-scale historical series patterns using convolution operation, then constructs a pattern extension forecasting method based on the prior knowledge of pattern reproduction, and finally reconstructs future patterns into future series using deconvolution operation. By leveraging the temporal dependencies present in the time series, MPR-Net not only achieves linear time complexity, but also makes the forecasting process interpretable. By carrying out sufficient experiments on more than ten real data sets of both short and long term forecasting tasks, MPR-Net achieves the state of the art forecasting performance, as well as good generalization and robustness performance.","sentences":["Time series forecasting has received wide interest from existing research due to its broad applications and inherent challenging.","The research challenge lies in identifying effective patterns in historical series and applying them to future forecasting.","Advanced models based on point-wise connected MLP and Transformer architectures have strong fitting power, but their secondary computational complexity limits practicality.","Additionally, those structures inherently disrupt the temporal order, reducing the information utilization and making the forecasting process uninterpretable.","To solve these problems, this paper proposes a forecasting model, MPR-Net.","It first adaptively decomposes multi-scale historical series patterns using convolution operation, then constructs a pattern extension forecasting method based on the prior knowledge of pattern reproduction, and finally reconstructs future patterns into future series using deconvolution operation.","By leveraging the temporal dependencies present in the time series, MPR-Net not only achieves linear time complexity, but also makes the forecasting process interpretable.","By carrying out sufficient experiments on more than ten real data sets of both short and long term forecasting tasks, MPR-Net achieves the state of the art forecasting performance, as well as good generalization and robustness performance."],"url":"http://arxiv.org/abs/2307.06736v1"}
{"created":"2023-07-13 12:37:25","title":"The Price of Equity with Binary Valuations and Few Agent Types","abstract":"In fair division problems, the notion of price of fairness measures the loss in welfare due to a fairness constraint. Prior work on the price of fairness has focused primarily on envy-freeness up to one good (EF1) as the fairness constraint, and on the utilitarian and egalitarian welfare measures. Our work instead focuses on the price of equitability up to one good (EQ1) (which we term price of equity) and considers the broad class of generalized $p$-mean welfare measures (which includes utilitarian, egalitarian, and Nash welfare as special cases). We derive fine-grained bounds on the price of equity in terms of the number of agent types (i.e., the maximum number of agents with distinct valuations), which allows us to identify scenarios where the existing bounds in terms of the number of agents are overly pessimistic.   Our work focuses on the setting with binary additive valuations, and obtains upper and lower bounds on the price of equity for $p$-mean welfare for all $p \\leqslant 1$. For any fixed $p$, our bounds are tight up to constant factors. A useful insight of our work is to identify the structure of allocations that underlie the upper (respectively, the lower) bounds simultaneously for all $p$-mean welfare measures, thus providing a unified structural understanding of price of fairness in this setting. This structural understanding, in fact, extends to the more general class of binary submodular (or matroid rank) valuations. We also show that, unlike binary additive valuations, for binary submodular valuations the number of agent types does not provide bounds on the price of equity.","sentences":["In fair division problems, the notion of price of fairness measures the loss in welfare due to a fairness constraint.","Prior work on the price of fairness has focused primarily on envy-freeness up to one good (EF1) as the fairness constraint, and on the utilitarian and egalitarian welfare measures.","Our work instead focuses on the price of equitability up to one good (EQ1) (which we term price of equity) and considers the broad class of generalized $p$-mean welfare measures (which includes utilitarian, egalitarian, and Nash welfare as special cases).","We derive fine-grained bounds on the price of equity in terms of the number of agent types (i.e., the maximum number of agents with distinct valuations), which allows us to identify scenarios where the existing bounds in terms of the number of agents are overly pessimistic.   ","Our work focuses on the setting with binary additive valuations, and obtains upper and lower bounds on the price of equity for $p$-mean welfare for all $p \\leqslant 1$. For any fixed $p$, our bounds are tight up to constant factors.","A useful insight of our work is to identify the structure of allocations that underlie the upper (respectively, the lower) bounds simultaneously for all $p$-mean welfare measures, thus providing a unified structural understanding of price of fairness in this setting.","This structural understanding, in fact, extends to the more general class of binary submodular (or matroid rank) valuations.","We also show that, unlike binary additive valuations, for binary submodular valuations the number of agent types does not provide bounds on the price of equity."],"url":"http://arxiv.org/abs/2307.06726v1"}
{"created":"2023-07-13 12:37:14","title":"Multimodal Object Detection in Remote Sensing","abstract":"Object detection in remote sensing is a crucial computer vision task that has seen significant advancements with deep learning techniques. However, most existing works in this area focus on the use of generic object detection and do not leverage the potential of multimodal data fusion. In this paper, we present a comparison of methods for multimodal object detection in remote sensing, survey available multimodal datasets suitable for evaluation, and discuss future directions.","sentences":["Object detection in remote sensing is a crucial computer vision task that has seen significant advancements with deep learning techniques.","However, most existing works in this area focus on the use of generic object detection and do not leverage the potential of multimodal data fusion.","In this paper, we present a comparison of methods for multimodal object detection in remote sensing, survey available multimodal datasets suitable for evaluation, and discuss future directions."],"url":"http://arxiv.org/abs/2307.06724v1"}
{"created":"2023-07-13 12:32:49","title":"Breaking 3-Factor Approximation for Correlation Clustering in Polylogarithmic Rounds","abstract":"In this paper, we study parallel algorithms for the correlation clustering problem, where every pair of two different entities is labeled with similar or dissimilar. The goal is to partition the entities into clusters to minimize the number of disagreements with the labels. Currently, all efficient parallel algorithms have an approximation ratio of at least 3. In comparison with the $1.994+\\epsilon$ ratio achieved by polynomial-time sequential algorithms [CLN22], a significant gap exists.   We propose the first poly-logarithmic depth parallel algorithm that achieves a better approximation ratio than 3. Specifically, our algorithm computes a $(2.4+\\epsilon)$-approximate solution and uses $\\tilde{O}(m^{1.5})$ work. Additionally, it can be translated into a $\\tilde{O}(m^{1.5})$-time sequential algorithm and a poly-logarithmic rounds sublinear-memory MPC algorithm with $\\tilde{O}(m^{1.5})$ total memory.   Our approach is inspired by Awerbuch, Khandekar, and Rao's [AKR12] length-constrained multi-commodity flow algorithm, where we develop an efficient parallel algorithm to solve a truncated correlation clustering linear program of Charikar, Guruswami, and Wirth [CGW05]. Then we show the solution of the truncated linear program can be rounded with a factor of at most 2.4 loss by using the framework of [CMSY15]. Such a rounding framework can then be implemented using parallel pivot-based approaches.","sentences":["In this paper, we study parallel algorithms for the correlation clustering problem, where every pair of two different entities is labeled with similar or dissimilar.","The goal is to partition the entities into clusters to minimize the number of disagreements with the labels.","Currently, all efficient parallel algorithms have an approximation ratio of at least 3.","In comparison with the $1.994+\\epsilon$ ratio achieved by polynomial-time sequential algorithms [CLN22], a significant gap exists.   ","We propose the first poly-logarithmic depth parallel algorithm that achieves a better approximation ratio than 3.","Specifically, our algorithm computes a $(2.4+\\epsilon)$-approximate solution and uses $\\tilde{O}(m^{1.5})$ work.","Additionally, it can be translated into a $\\tilde{O}(m^{1.5})$-time sequential algorithm and a poly-logarithmic rounds sublinear-memory MPC algorithm with $\\tilde{O}(m^{1.5})$ total memory.   ","Our approach is inspired by Awerbuch, Khandekar, and Rao's [AKR12] length-constrained multi-commodity flow algorithm, where we develop an efficient parallel algorithm to solve a truncated correlation clustering linear program of Charikar, Guruswami, and","Wirth [CGW05].","Then we show the solution of the truncated linear program can be rounded with a factor of at most 2.4 loss by using the framework of [CMSY15].","Such a rounding framework can then be implemented using parallel pivot-based approaches."],"url":"http://arxiv.org/abs/2307.06723v1"}
{"created":"2023-07-13 12:29:29","title":"Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative","abstract":"Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog. In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL). In RL-based DPL, dialog policies are updated according to rewards. The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations. One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL). Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse. This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator. Next, based on these analyses, we propose a method that eliminates AL from reward estimation and DPL while retaining its advantages. We evaluate our method using MultiWOZ, a multi-domain task-oriented dialog corpus.","sentences":["Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog.","In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL).","In RL-based DPL, dialog policies are updated according to rewards.","The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations.","One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL).","Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse.","This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator.","Next, based on these analyses, we propose a method that eliminates AL from reward estimation and DPL while retaining its advantages.","We evaluate our method using MultiWOZ, a multi-domain task-oriented dialog corpus."],"url":"http://arxiv.org/abs/2307.06721v1"}
{"created":"2023-07-13 12:26:27","title":"Weakly supervised marine animal detection from remote sensing images using vector-quantized variational autoencoder","abstract":"This paper studies a reconstruction-based approach for weakly-supervised animal detection from aerial images in marine environments. Such an approach leverages an anomaly detection framework that computes metrics directly on the input space, enhancing interpretability and anomaly localization compared to feature embedding methods. Building upon the success of Vector-Quantized Variational Autoencoders in anomaly detection on computer vision datasets, we adapt them to the marine animal detection domain and address the challenge of handling noisy data. To evaluate our approach, we compare it with existing methods in the context of marine animal detection from aerial image data. Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent studies in the literature. Our framework offers improved interpretability and localization of anomalies, providing valuable insights for monitoring marine ecosystems and mitigating the impact of human activities on marine animals.","sentences":["This paper studies a reconstruction-based approach for weakly-supervised animal detection from aerial images in marine environments.","Such an approach leverages an anomaly detection framework that computes metrics directly on the input space, enhancing interpretability and anomaly localization compared to feature embedding methods.","Building upon the success of Vector-Quantized Variational Autoencoders in anomaly detection on computer vision datasets, we adapt them to the marine animal detection domain and address the challenge of handling noisy data.","To evaluate our approach, we compare it with existing methods in the context of marine animal detection from aerial image data.","Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent studies in the literature.","Our framework offers improved interpretability and localization of anomalies, providing valuable insights for monitoring marine ecosystems and mitigating the impact of human activities on marine animals."],"url":"http://arxiv.org/abs/2307.06720v1"}
{"created":"2023-07-13 12:13:38","title":"Experimental Demonstration of 3D Reflected Beamforming at sub6GHz thanks to Varactor Based Reconfigurable Intelligent Surface","abstract":"Reconfigurable intelligent surface (RIS) is a promising solution to boost coverage sustainably by reflecting waves from a transmitter to a receiver and acting as a low-power and passive relay. In this paper, for the first time, we demonstrate experimentally that a reconfigurable intelligent surface designed for sub6GHz, and using varactor technology, can perform three-dimensional reflective beamforming. This result is achieved with a RIS prototype of 984 unit-cells, thanks to a compact control circuit individually addressing and configuring the voltage of each unit-cell, with a distinct voltage. To our knowledge, this prototype configures 17 to 70 times more distinct voltages than in the state-of-the-art. The experimental results in an indoor environment show a 10 dB gain. They also show, for the first time, that producing such a new prototype is feasible with minimal energy footprint and environmental impact, thanks to refurbishing. Indeed, a reflectarray antenna originally designed for three-dimensional beamforming has been turned into a RIS.","sentences":["Reconfigurable intelligent surface (RIS) is a promising solution to boost coverage sustainably by reflecting waves from a transmitter to a receiver and acting as a low-power and passive relay.","In this paper, for the first time, we demonstrate experimentally that a reconfigurable intelligent surface designed for sub6GHz, and using varactor technology, can perform three-dimensional reflective beamforming.","This result is achieved with a RIS prototype of 984 unit-cells, thanks to a compact control circuit individually addressing and configuring the voltage of each unit-cell, with a distinct voltage.","To our knowledge, this prototype configures 17 to 70 times more distinct voltages than in the state-of-the-art.","The experimental results in an indoor environment show a 10 dB gain.","They also show, for the first time, that producing such a new prototype is feasible with minimal energy footprint and environmental impact, thanks to refurbishing.","Indeed, a reflectarray antenna originally designed for three-dimensional beamforming has been turned into a RIS."],"url":"http://arxiv.org/abs/2307.06716v1"}
{"created":"2023-07-13 12:11:46","title":"Asymptotic SEP Analysis and Optimization of Linear-Quantized Precoding in Massive MIMO Systems","abstract":"A promising approach to deal with the high hardware cost and energy consumption of massive MIMO transmitters is to use low-resolution digital-to-analog converters (DACs) at each antenna element. This leads to a transmission scheme where the transmitted signals are restricted to a finite set of voltage levels. This paper is concerned with the analysis and optimization of a low-cost quantized precoding strategy, referred to as linear-quantized precoding, for a downlink massive MIMO system under Rayleigh fading. In linear-quantized precoding, the signals are first processed by a linear precoding matrix and subsequently quantized component-wise by the DAC. In this paper, we analyze both the signal-to-interference-plus-noise ratio (SINR) and the symbol error probability (SEP) performances of such linear-quantized precoding schemes in an asymptotic framework where the number of transmit antennas and the number of users grow large with a fixed ratio. Our results provide a rigorous justification for the heuristic arguments based on the Bussgang decomposition that are commonly used in prior works. Based on the asymptotic analysis, we further derive the optimal precoder within a class of linear-quantized precoders that includes several popular precoders as special cases. Our numerical results demonstrate the excellent accuracy of the asymptotic analysis for finite systems and the optimality of the derived precoder.","sentences":["A promising approach to deal with the high hardware cost and energy consumption of massive MIMO transmitters is to use low-resolution digital-to-analog converters (DACs) at each antenna element.","This leads to a transmission scheme where the transmitted signals are restricted to a finite set of voltage levels.","This paper is concerned with the analysis and optimization of a low-cost quantized precoding strategy, referred to as linear-quantized precoding, for a downlink massive MIMO system under Rayleigh fading.","In linear-quantized precoding, the signals are first processed by a linear precoding matrix and subsequently quantized component-wise by the DAC.","In this paper, we analyze both the signal-to-interference-plus-noise ratio (SINR) and the symbol error probability (SEP) performances of such linear-quantized precoding schemes in an asymptotic framework where the number of transmit antennas and the number of users grow large with a fixed ratio.","Our results provide a rigorous justification for the heuristic arguments based on the Bussgang decomposition that are commonly used in prior works.","Based on the asymptotic analysis, we further derive the optimal precoder within a class of linear-quantized precoders that includes several popular precoders as special cases.","Our numerical results demonstrate the excellent accuracy of the asymptotic analysis for finite systems and the optimality of the derived precoder."],"url":"http://arxiv.org/abs/2307.06714v1"}
{"created":"2023-07-13 12:11:36","title":"Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models","abstract":"A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.","sentences":["A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs).","These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning.","In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries.","The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task.","Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data."],"url":"http://arxiv.org/abs/2307.06713v1"}
{"created":"2023-07-13 12:07:39","title":"GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators","abstract":"A wide variety of generative models for graphs have been proposed. They are used in drug discovery, road networks, neural architecture search, and program synthesis. Generating graphs has theoretical challenges, such as isomorphic representations -- evaluating how well a generative model performs is difficult. Which model to choose depending on the application domain?   We extensively study kernel-based metrics on distributions of graph invariants and manifold-based and kernel-based metrics in graph embedding space. Manifold-based metrics outperform kernel-based metrics in embedding space. We use these metrics to compare GraphRNN and GRAN, two well-known generative models for graphs, and unveil the influence of node orderings. It shows the superiority of GRAN over GraphRNN - further, our proposed adaptation of GraphRNN with a depth-first search ordering is effective for small-sized graphs.   A guideline on good practices regarding dataset selection and node feature initialization is provided. Our work is accompanied by open-source code and reproducible experiments.","sentences":["A wide variety of generative models for graphs have been proposed.","They are used in drug discovery, road networks, neural architecture search, and program synthesis.","Generating graphs has theoretical challenges, such as isomorphic representations -- evaluating how well a generative model performs is difficult.","Which model to choose depending on the application domain?   ","We extensively study kernel-based metrics on distributions of graph invariants and manifold-based and kernel-based metrics in graph embedding space.","Manifold-based metrics outperform kernel-based metrics in embedding space.","We use these metrics to compare GraphRNN and GRAN, two well-known generative models for graphs, and unveil the influence of node orderings.","It shows the superiority of GRAN over GraphRNN - further, our proposed adaptation of GraphRNN with a depth-first search ordering is effective for small-sized graphs.   ","A guideline on good practices regarding dataset selection and node feature initialization is provided.","Our work is accompanied by open-source code and reproducible experiments."],"url":"http://arxiv.org/abs/2307.06709v1"}
{"created":"2023-07-13 12:06:48","title":"To share or not to share: What risks would laypeople accept to give sensitive data to differentially-private NLP systems?","abstract":"Although the NLP community has adopted central differential privacy as a go-to framework for privacy-preserving model training or data sharing, the choice and interpretation of the key parameter, privacy budget $\\varepsilon$ that governs the strength of privacy protection, remains largely arbitrary. We argue that determining the $\\varepsilon$ value should not be solely in the hands of researchers or system developers, but must also take into account the actual people who share their potentially sensitive data. In other words: Would you share your instant messages for $\\varepsilon$ of 10? We address this research gap by designing, implementing, and conducting a behavioral experiment (311 lay participants) to study the behavior of people in uncertain decision-making situations with respect to privacy-threatening situations. Framing the risk perception in terms of two realistic NLP scenarios and using a vignette behavioral study help us determine what $\\varepsilon$ thresholds would lead lay people to be willing to share sensitive textual data - to our knowledge, the first study of its kind.","sentences":["Although the NLP community has adopted central differential privacy as a go-to framework for privacy-preserving model training or data sharing, the choice and interpretation of the key parameter, privacy budget $\\varepsilon$ that governs the strength of privacy protection, remains largely arbitrary.","We argue that determining the $\\varepsilon$ value should not be solely in the hands of researchers or system developers, but must also take into account the actual people who share their potentially sensitive data.","In other words: Would you share your instant messages for $\\varepsilon$ of 10?","We address this research gap by designing, implementing, and conducting a behavioral experiment (311 lay participants) to study the behavior of people in uncertain decision-making situations with respect to privacy-threatening situations.","Framing the risk perception in terms of two realistic NLP scenarios and using a vignette behavioral study help us determine what $\\varepsilon$ thresholds would lead lay people to be willing to share sensitive textual data - to our knowledge, the first study of its kind."],"url":"http://arxiv.org/abs/2307.06708v1"}
{"created":"2023-07-13 12:02:51","title":"Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues","abstract":"Answer selection in open-domain dialogues aims to select an accurate answer from candidates. Recent success of answer selection models hinges on training with large amounts of labeled data. However, collecting large-scale labeled data is labor-intensive and time-consuming. In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm. Specifically, we propose the intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels. We carry out extensive experiments on two benchmark datasets with open-domain dialogues. The experimental results show that ICAST outperforms baselines consistently with 1%, 5% and 10% labeled data. Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data.","sentences":["Answer selection in open-domain dialogues aims to select an accurate answer from candidates.","Recent success of answer selection models hinges on training with large amounts of labeled data.","However, collecting large-scale labeled data is labor-intensive and time-consuming.","In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm.","Specifically, we propose the intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels.","We carry out extensive experiments on two benchmark datasets with open-domain dialogues.","The experimental results show that ICAST outperforms baselines consistently with 1%, 5% and 10% labeled data.","Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data."],"url":"http://arxiv.org/abs/2307.06703v1"}
{"created":"2023-07-13 11:58:27","title":"S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction","abstract":"We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evaluations despite a much smaller model size. Finally, we boost S-HR-VQVAE by proposing a novel training method to jointly estimate the HR-VQVAE and ST-PixelCNN parameters.","sentences":["We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN).","We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE).","By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction.","These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics.","Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evaluations despite a much smaller model size.","Finally, we boost S-HR-VQVAE by proposing a novel training method to jointly estimate the HR-VQVAE and ST-PixelCNN parameters."],"url":"http://arxiv.org/abs/2307.06701v1"}
{"created":"2023-07-13 11:55:03","title":"Parmesan: mathematical concept extraction for education","abstract":"Mathematics is a highly specialized domain with its own unique set of challenges that has seen limited study in natural language processing. However, mathematics is used in a wide variety of fields and multidisciplinary research in many different domains often relies on an understanding of mathematical concepts. To aid researchers coming from other fields, we develop a prototype system for searching for and defining mathematical concepts in context, focusing on the field of category theory. This system, Parmesan, depends on natural language processing components including concept extraction, relation extraction, definition extraction, and entity linking. In developing this system, we show that existing techniques cannot be applied directly to the category theory domain, and suggest hybrid techniques that do perform well, though we expect the system to evolve over time. We also provide two cleaned mathematical corpora that power the prototype system, which are based on journal articles and wiki pages, respectively. The corpora have been annotated with dependency trees, lemmas, and part-of-speech tags.","sentences":["Mathematics is a highly specialized domain with its own unique set of challenges that has seen limited study in natural language processing.","However, mathematics is used in a wide variety of fields and multidisciplinary research in many different domains often relies on an understanding of mathematical concepts.","To aid researchers coming from other fields, we develop a prototype system for searching for and defining mathematical concepts in context, focusing on the field of category theory.","This system, Parmesan, depends on natural language processing components including concept extraction, relation extraction, definition extraction, and entity linking.","In developing this system, we show that existing techniques cannot be applied directly to the category theory domain, and suggest hybrid techniques that do perform well, though we expect the system to evolve over time.","We also provide two cleaned mathematical corpora that power the prototype system, which are based on journal articles and wiki pages, respectively.","The corpora have been annotated with dependency trees, lemmas, and part-of-speech tags."],"url":"http://arxiv.org/abs/2307.06699v1"}
{"created":"2023-07-13 11:54:32","title":"IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation","abstract":"Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development of machine learning models that emphasize semantic understanding.","sentences":["Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations.","A key task in the literature is predicting missing links between entities.","However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure.","Semantics is crucial in several downstream tasks, such as query answering or reasoning.","We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs.","We propose IntelliGraphs, a set of five new Knowledge Graph datasets.","The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference.","We also present the dataset generator that produced the synthetic datasets.","We designed four novel baseline models, which include three models based on traditional KGEs.","We evaluate their expressiveness and show that these models cannot capture the semantics.","We believe this benchmark will encourage the development of machine learning models that emphasize semantic understanding."],"url":"http://arxiv.org/abs/2307.06698v1"}
{"created":"2023-07-13 11:41:38","title":"Towards Traitor Tracing in Black-and-White-Box DNN Watermarking with Tardos-based Codes","abstract":"The growing popularity of Deep Neural Networks, which often require computationally expensive training and access to a vast amount of data, calls for accurate authorship verification methods to deter unlawful dissemination of the models and identify the source of the leak. In DNN watermarking the owner may have access to the full network (white-box) or only be able to extract information from its output to queries (black-box), but a watermarked model may include both approaches in order to gather sufficient evidence to then gain access to the network. Although there has been limited research in white-box watermarking that considers traitor tracing, this problem is yet to be explored in the black-box scenario. In this paper, we propose a black-and-white-box watermarking method that opens the door to collusion-resistant traitor tracing in black-box, exploiting the properties of Tardos codes, and making it possible to identify the source of the leak before access to the model is granted. While experimental results show that the method can successfully identify traitors, even when further attacks have been performed, we also discuss its limitations and open problems for traitor tracing in black-box.","sentences":["The growing popularity of Deep Neural Networks, which often require computationally expensive training and access to a vast amount of data, calls for accurate authorship verification methods to deter unlawful dissemination of the models and identify the source of the leak.","In DNN watermarking the owner may have access to the full network (white-box) or only be able to extract information from its output to queries (black-box), but a watermarked model may include both approaches in order to gather sufficient evidence to then gain access to the network.","Although there has been limited research in white-box watermarking that considers traitor tracing, this problem is yet to be explored in the black-box scenario.","In this paper, we propose a black-and-white-box watermarking method that opens the door to collusion-resistant traitor tracing in black-box, exploiting the properties of Tardos codes, and making it possible to identify the source of the leak before access to the model is granted.","While experimental results show that the method can successfully identify traitors, even when further attacks have been performed, we also discuss its limitations and open problems for traitor tracing in black-box."],"url":"http://arxiv.org/abs/2307.06695v1"}
{"created":"2023-07-13 11:37:47","title":"Ageing Analysis of Embedded SRAM on a Large-Scale Testbed Using Machine Learning","abstract":"Ageing detection and failure prediction are essential in many Internet of Things (IoT) deployments, which operate huge quantities of embedded devices unattended in the field for years. In this paper, we present a large-scale empirical analysis of natural SRAM wear-out using 154 boards from a general-purpose testbed. Starting from SRAM initialization bias, which each node can easily collect at startup, we apply various metrics for feature extraction and experiment with common machine learning methods to predict the age of operation for this node. Our findings indicate that even though ageing impacts are subtle, our indicators can well estimate usage times with an $R^2$ score of 0.77 and a mean error of 24% using regressors, and with an F1 score above 0.6 for classifiers applying a six-months resolution.","sentences":["Ageing detection and failure prediction are essential in many Internet of Things (IoT) deployments, which operate huge quantities of embedded devices unattended in the field for years.","In this paper, we present a large-scale empirical analysis of natural SRAM wear-out using 154 boards from a general-purpose testbed.","Starting from SRAM initialization bias, which each node can easily collect at startup, we apply various metrics for feature extraction and experiment with common machine learning methods to predict the age of operation for this node.","Our findings indicate that even though ageing impacts are subtle, our indicators can well estimate usage times with an $R^2$ score of 0.77 and a mean error of 24% using regressors, and with an F1 score above 0.6 for classifiers applying a six-months resolution."],"url":"http://arxiv.org/abs/2307.06693v1"}
{"created":"2023-07-13 11:28:03","title":"A Local-Time Semantics for Negotiations","abstract":"Negotiations, introduced by Esparza et al., are a model for concurrent systems where computations involving a set of agents are described in terms of their interactions. In many situations, it is natural to impose timing constraints between interactions -- for instance, to limit the time available to enter the PIN after inserting a card into an ATM. To model this, we introduce a real-time aspect to negotiations. In our model of local-timed negotiations, agents have local reference times that evolve independently. Inspired by the model of networks of timed automata, each agent is equipped with a set of local clocks. Similar to timed automata, the outcomes of a negotiation contain guards and resets over the local clocks.   As a new feature, we allow some interactions to force the reference clocks of the participating agents to synchronize. This synchronization constraint allows us to model interesting scenarios. Surprisingly, it also gives unlimited computing power. We show that reachability is undecidable for local-timed negotiations with a mixture of synchronized and unsynchronized interactions. We study restrictions on the use of synchronized interactions that make the problem decidable.","sentences":["Negotiations, introduced by Esparza et al., are a model for concurrent systems where computations involving a set of agents are described in terms of their interactions.","In many situations, it is natural to impose timing constraints between interactions -- for instance, to limit the time available to enter the PIN after inserting a card into an ATM.","To model this, we introduce a real-time aspect to negotiations.","In our model of local-timed negotiations, agents have local reference times that evolve independently.","Inspired by the model of networks of timed automata, each agent is equipped with a set of local clocks.","Similar to timed automata, the outcomes of a negotiation contain guards and resets over the local clocks.   ","As a new feature, we allow some interactions to force the reference clocks of the participating agents to synchronize.","This synchronization constraint allows us to model interesting scenarios.","Surprisingly, it also gives unlimited computing power.","We show that reachability is undecidable for local-timed negotiations with a mixture of synchronized and unsynchronized interactions.","We study restrictions on the use of synchronized interactions that make the problem decidable."],"url":"http://arxiv.org/abs/2307.06691v1"}
{"created":"2023-07-13 11:21:58","title":"YOLIC: An Efficient Method for Object Localization and Classification on Edge Devices","abstract":"In the realm of Tiny AI, we introduce \"You Only Look at Interested Cells\" (YOLIC), an efficient method for object localization and classification on edge devices. Seamlessly blending the strengths of semantic segmentation and object detection, YOLIC offers superior computational efficiency and precision. By adopting Cells of Interest for classification instead of individual pixels, YOLIC encapsulates relevant information, reduces computational load, and enables rough object shape inference. Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configuration that provides information about potential object location, size, and shape. To tackle the issue of single-label classification limitations, a multi-label classification approach is applied to each cell, effectively recognizing overlapping or closely situated objects. This paper presents extensive experiments on multiple datasets, demonstrating that YOLIC achieves detection performance comparable to the state-of-the-art YOLO algorithms while surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU. All resources related to this study, including datasets, cell designer, image annotation tool, and source code, have been made publicly available on our project website at https://kai3316.github.io/yolic.github.io","sentences":["In the realm of Tiny AI, we introduce \"You Only Look at Interested Cells\" (YOLIC), an efficient method for object localization and classification on edge devices.","Seamlessly blending the strengths of semantic segmentation and object detection, YOLIC offers superior computational efficiency and precision.","By adopting Cells of Interest for classification instead of individual pixels, YOLIC encapsulates relevant information, reduces computational load, and enables rough object shape inference.","Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configuration that provides information about potential object location, size, and shape.","To tackle the issue of single-label classification limitations, a multi-label classification approach is applied to each cell, effectively recognizing overlapping or closely situated objects.","This paper presents extensive experiments on multiple datasets, demonstrating that YOLIC achieves detection performance comparable to the state-of-the-art YOLO algorithms while surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU.","All resources related to this study, including datasets, cell designer, image annotation tool, and source code, have been made publicly available on our project website at https://kai3316.github.io/yolic.github.io"],"url":"http://arxiv.org/abs/2307.06689v1"}
{"created":"2023-07-13 11:20:18","title":"Aeolus Ocean -- A simulation environment for the autonomous COLREG-compliant navigation of Unmanned Surface Vehicles using Deep Reinforcement Learning and Maritime Object Detection","abstract":"Heading towards navigational autonomy in unmanned surface vehicles (USVs) in the maritime sector can fundamentally lead towards safer waters as well as reduced operating costs, while also providing a range of exciting new capabilities for oceanic research, exploration and monitoring. However, achieving such a goal is challenging. USV control systems must, safely and reliably, be able to adhere to the international regulations for preventing collisions at sea (COLREGs) in encounters with other vessels as they navigate to a given waypoint while being affected by realistic weather conditions, either during the day or at night. To deal with the multitude of possible scenarios, it is critical to have a virtual environment that is able to replicate the realistic operating conditions USVs will encounter, before they can be implemented in the real world. Such \"digital twins\" form the foundations upon which Deep Reinforcement Learning (DRL) and Computer Vision (CV) algorithms can be used to develop and guide USV control systems. In this paper we describe the novel development of a COLREG-compliant DRL-based collision avoidant navigational system with CV-based awareness in a realistic ocean simulation environment. The performance of the trained autonomous Agents resulting from this approach is evaluated in several successful navigations to set waypoints in both open sea and coastal encounters with other vessels. A binary executable version of the simulator with trained agents is available at https://github.com/aavek/Aeolus-Ocean","sentences":["Heading towards navigational autonomy in unmanned surface vehicles (USVs) in the maritime sector can fundamentally lead towards safer waters as well as reduced operating costs, while also providing a range of exciting new capabilities for oceanic research, exploration and monitoring.","However, achieving such a goal is challenging.","USV control systems must, safely and reliably, be able to adhere to the international regulations for preventing collisions at sea (COLREGs) in encounters with other vessels as they navigate to a given waypoint while being affected by realistic weather conditions, either during the day or at night.","To deal with the multitude of possible scenarios, it is critical to have a virtual environment that is able to replicate the realistic operating conditions USVs will encounter, before they can be implemented in the real world.","Such \"digital twins\" form the foundations upon which Deep Reinforcement Learning (DRL) and Computer Vision (CV) algorithms can be used to develop and guide USV control systems.","In this paper we describe the novel development of a COLREG-compliant DRL-based collision avoidant navigational system with CV-based awareness in a realistic ocean simulation environment.","The performance of the trained autonomous Agents resulting from this approach is evaluated in several successful navigations to set waypoints in both open sea and coastal encounters with other vessels.","A binary executable version of the simulator with trained agents is available at https://github.com/aavek/Aeolus-Ocean"],"url":"http://arxiv.org/abs/2307.06688v1"}
{"created":"2023-07-13 11:14:46","title":"Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities","abstract":"In recent years, ubiquitous semantic Metaverse has been studied to revolutionize immersive cyber-virtual experiences for augmented reality (AR) and virtual reality (VR) users, which leverages advanced semantic understanding and representation to enable seamless, context-aware interactions within mixed-reality environments. This survey focuses on the intelligence and spatio-temporal characteristics of four fundamental system components in ubiquitous semantic Metaverse, i.e., artificial intelligence (AI), spatio-temporal data representation (STDR), semantic Internet of Things (SIoT), and semantic-enhanced digital twin (SDT). We thoroughly survey the representative techniques of the four fundamental system components that enable intelligent, personalized, and context-aware interactions with typical use cases of the ubiquitous semantic Metaverse, such as remote education, work and collaboration, entertainment and socialization, healthcare, and e-commerce marketing. Furthermore, we outline the opportunities for constructing the future ubiquitous semantic Metaverse, including scalability and interoperability, privacy and security, performance measurement and standardization, as well as ethical considerations and responsible AI. Addressing those challenges is important for creating a robust, secure, and ethically sound system environment that offers engaging immersive experiences for the users and AR/VR applications.","sentences":["In recent years, ubiquitous semantic Metaverse has been studied to revolutionize immersive cyber-virtual experiences for augmented reality (AR) and virtual reality (VR) users, which leverages advanced semantic understanding and representation to enable seamless, context-aware interactions within mixed-reality environments.","This survey focuses on the intelligence and spatio-temporal characteristics of four fundamental system components in ubiquitous semantic Metaverse, i.e., artificial intelligence (AI), spatio-temporal data representation (STDR), semantic Internet of Things (SIoT), and semantic-enhanced digital twin (SDT).","We thoroughly survey the representative techniques of the four fundamental system components that enable intelligent, personalized, and context-aware interactions with typical use cases of the ubiquitous semantic Metaverse, such as remote education, work and collaboration, entertainment and socialization, healthcare, and e-commerce marketing.","Furthermore, we outline the opportunities for constructing the future ubiquitous semantic Metaverse, including scalability and interoperability, privacy and security, performance measurement and standardization, as well as ethical considerations and responsible AI.","Addressing those challenges is important for creating a robust, secure, and ethically sound system environment that offers engaging immersive experiences for the users and AR/VR applications."],"url":"http://arxiv.org/abs/2307.06687v1"}
{"created":"2023-07-13 10:49:02","title":"Overcoming the Mental Set Effect in Programming Problem Solving","abstract":"This paper adopts a cognitive psychology perspective to investigate the recurring mistakes in code resulting from the mental set (Einstellung) effect. The Einstellung effect is the tendency to approach problem-solving with a preconceived mindset, often overlooking better solutions that may be available. This effect can significantly impact creative thinking, as the development of patterns of thought can hinder the emergence of novel and creative ideas. Our study aims to test the Einstellung effect and the two mechanisms of its overcoming in the field of programming. The first intervention was the change of the color scheme of the code editor to the less habitual one. The second intervention was a combination of instruction to \"forget the previous solutions and tasks\" and the change in the color scheme. During the experiment, participants were given two sets of four programming tasks. Each task had two possible solutions: one using suboptimal code dictated by the mental set, and the other using a less familiar but more efficient and recommended methodology. Between the sets, participants either received no treatment or one of two interventions aimed at helping them overcome the mental set. The results of our experiment suggest that the tested techniques were insufficient to support overcoming the mental set, which we attribute to the specificity of the programming domain. The study contributes to the existing literature by providing insights into creativity support during problem-solving in software development and offering a framework for experimental research in this field.","sentences":["This paper adopts a cognitive psychology perspective to investigate the recurring mistakes in code resulting from the mental set (Einstellung) effect.","The Einstellung effect is the tendency to approach problem-solving with a preconceived mindset, often overlooking better solutions that may be available.","This effect can significantly impact creative thinking, as the development of patterns of thought can hinder the emergence of novel and creative ideas.","Our study aims to test the Einstellung effect and the two mechanisms of its overcoming in the field of programming.","The first intervention was the change of the color scheme of the code editor to the less habitual one.","The second intervention was a combination of instruction to \"forget the previous solutions and tasks\" and the change in the color scheme.","During the experiment, participants were given two sets of four programming tasks.","Each task had two possible solutions: one using suboptimal code dictated by the mental set, and the other using a less familiar but more efficient and recommended methodology.","Between the sets, participants either received no treatment or one of two interventions aimed at helping them overcome the mental set.","The results of our experiment suggest that the tested techniques were insufficient to support overcoming the mental set, which we attribute to the specificity of the programming domain.","The study contributes to the existing literature by providing insights into creativity support during problem-solving in software development and offering a framework for experimental research in this field."],"url":"http://arxiv.org/abs/2307.06673v1"}
{"created":"2023-07-13 10:25:30","title":"Uncovering the Deceptions: An Analysis on Audio Spoofing Detection and Future Prospects","abstract":"Audio has become an increasingly crucial biometric modality due to its ability to provide an intuitive way for humans to interact with machines. It is currently being used for a range of applications, including person authentication to banking to virtual assistants. Research has shown that these systems are also susceptible to spoofing and attacks. Therefore, protecting audio processing systems against fraudulent activities, such as identity theft, financial fraud, and spreading misinformation, is of paramount importance. This paper reviews the current state-of-the-art techniques for detecting audio spoofing and discusses the current challenges along with open research problems. The paper further highlights the importance of considering the ethical and privacy implications of audio spoofing detection systems. Lastly, the work aims to accentuate the need for building more robust and generalizable methods, the integration of automatic speaker verification and countermeasure systems, and better evaluation protocols.","sentences":["Audio has become an increasingly crucial biometric modality due to its ability to provide an intuitive way for humans to interact with machines.","It is currently being used for a range of applications, including person authentication to banking to virtual assistants.","Research has shown that these systems are also susceptible to spoofing and attacks.","Therefore, protecting audio processing systems against fraudulent activities, such as identity theft, financial fraud, and spreading misinformation, is of paramount importance.","This paper reviews the current state-of-the-art techniques for detecting audio spoofing and discusses the current challenges along with open research problems.","The paper further highlights the importance of considering the ethical and privacy implications of audio spoofing detection systems.","Lastly, the work aims to accentuate the need for building more robust and generalizable methods, the integration of automatic speaker verification and countermeasure systems, and better evaluation protocols."],"url":"http://arxiv.org/abs/2307.06669v1"}
{"created":"2023-07-13 10:19:48","title":"DGCNet: An Efficient 3D-Densenet based on Dynamic Group Convolution for Hyperspectral Remote Sensing Image Classification","abstract":"Deep neural networks face many problems in the field of hyperspectral image classification, lack of effective utilization of spatial spectral information, gradient disappearance and overfitting as the model depth increases. In order to accelerate the deployment of the model on edge devices with strict latency requirements and limited computing power, we introduce a lightweight model based on the improved 3D-Densenet model and designs DGCNet. It improves the disadvantage of group convolution. Referring to the idea of dynamic network, dynamic group convolution(DGC) is designed on 3d convolution kernel. DGC introduces small feature selectors for each grouping to dynamically decide which part of the input channel to connect based on the activations of all input channels. Multiple groups can capture different and complementary visual and semantic features of input images, allowing convolution neural network(CNN) to learn rich features. 3D convolution extracts high-dimensional and redundant hyperspectral data, and there is also a lot of redundant information between convolution kernels. DGC module allows 3D-Densenet to select channel information with richer semantic features and discard inactive regions. The 3D-CNN passing through the DGC module can be regarded as a pruned network. DGC not only allows 3D-CNN to complete sufficient feature extraction, but also takes into account the requirements of speed and calculation amount. The inference speed and accuracy have been improved, with outstanding performance on the IN, Pavia and KSC datasets, ahead of the mainstream hyperspectral image classification methods.","sentences":["Deep neural networks face many problems in the field of hyperspectral image classification, lack of effective utilization of spatial spectral information, gradient disappearance and overfitting as the model depth increases.","In order to accelerate the deployment of the model on edge devices with strict latency requirements and limited computing power, we introduce a lightweight model based on the improved 3D-Densenet model and designs DGCNet.","It improves the disadvantage of group convolution.","Referring to the idea of dynamic network, dynamic group convolution(DGC) is designed on 3d convolution kernel.","DGC introduces small feature selectors for each grouping to dynamically decide which part of the input channel to connect based on the activations of all input channels.","Multiple groups can capture different and complementary visual and semantic features of input images, allowing convolution neural network(CNN) to learn rich features.","3D convolution extracts high-dimensional and redundant hyperspectral data, and there is also a lot of redundant information between convolution kernels.","DGC module allows 3D-Densenet to select channel information with richer semantic features and discard inactive regions.","The 3D-CNN passing through the DGC module can be regarded as a pruned network.","DGC not only allows 3D-CNN to complete sufficient feature extraction, but also takes into account the requirements of speed and calculation amount.","The inference speed and accuracy have been improved, with outstanding performance on the IN, Pavia and KSC datasets, ahead of the mainstream hyperspectral image classification methods."],"url":"http://arxiv.org/abs/2307.06667v1"}
{"created":"2023-07-13 10:19:04","title":"Transformer-based end-to-end classification of variable-length volumetric data","abstract":"The automatic classification of 3D medical data is memory-intensive. Also, variations in the number of slices between samples is common. Naive solutions such as subsampling can solve these problems, but at the cost of potentially eliminating relevant diagnosis information. Transformers have shown promising performance for sequential data analysis. However, their application for long-sequences is data, computationally, and memory demanding. In this paper, we propose an end-to-end Transformer-based framework that allows to classify volumetric data of variable length in an efficient fashion. Particularly, by randomizing the input slice-wise resolution during training, we enhance the capacity of the learnable positional embedding assigned to each volume slice. Consequently, the accumulated positional information in each positional embedding can be generalized to the neighbouring slices, even for high resolution volumes at the test time. By doing so, the model will be more robust to variable volume length and amenable to different computational budgets. We evaluated the proposed approach in retinal OCT volume classification and achieved 21.96% average improvement in balanced accuracy on a 9-class diagnostic task, compared to state-of-the-art video transformers. Our findings show that varying the slice-wise resolution of the input during training results in more informative volume representation as compared to training with fixed number of slices per volume. Our code is available at: https://github.com/marziehoghbaie/VLFAT.","sentences":["The automatic classification of 3D medical data is memory-intensive.","Also, variations in the number of slices between samples is common.","Naive solutions such as subsampling can solve these problems, but at the cost of potentially eliminating relevant diagnosis information.","Transformers have shown promising performance for sequential data analysis.","However, their application for long-sequences is data, computationally, and memory demanding.","In this paper, we propose an end-to-end Transformer-based framework that allows to classify volumetric data of variable length in an efficient fashion.","Particularly, by randomizing the input slice-wise resolution during training, we enhance the capacity of the learnable positional embedding assigned to each volume slice.","Consequently, the accumulated positional information in each positional embedding can be generalized to the neighbouring slices, even for high resolution volumes at the test time.","By doing so, the model will be more robust to variable volume length and amenable to different computational budgets.","We evaluated the proposed approach in retinal OCT volume classification and achieved 21.96% average improvement in balanced accuracy on a 9-class diagnostic task, compared to state-of-the-art video transformers.","Our findings show that varying the slice-wise resolution of the input during training results in more informative volume representation as compared to training with fixed number of slices per volume.","Our code is available at: https://github.com/marziehoghbaie/VLFAT."],"url":"http://arxiv.org/abs/2307.06666v1"}
{"created":"2023-07-13 10:03:40","title":"A Comprehensive Analysis of Blockchain Applications for Securing Computer Vision Systems","abstract":"Blockchain (BC) and Computer Vision (CV) are the two emerging fields with the potential to transform various sectors.The ability of BC can help in offering decentralized and secure data storage, while CV allows machines to learn and understand visual data. This integration of the two technologies holds massive promise for developing innovative applications that can provide solutions to the challenges in various sectors such as supply chain management, healthcare, smart cities, and defense. This review explores a comprehensive analysis of the integration of BC and CV by examining their combination and potential applications. It also provides a detailed analysis of the fundamental concepts of both technologies, highlighting their strengths and limitations. This paper also explores current research efforts that make use of the benefits offered by this combination. The effort includes how BC can be used as an added layer of security in CV systems and also ensure data integrity, enabling decentralized image and video analytics using BC. The challenges and open issues associated with this integration are also identified, and appropriate potential future directions are also proposed.","sentences":["Blockchain (BC) and Computer Vision (CV) are the two emerging fields with the potential to transform various sectors.","The ability of BC can help in offering decentralized and secure data storage, while CV allows machines to learn and understand visual data.","This integration of the two technologies holds massive promise for developing innovative applications that can provide solutions to the challenges in various sectors such as supply chain management, healthcare, smart cities, and defense.","This review explores a comprehensive analysis of the integration of BC and CV by examining their combination and potential applications.","It also provides a detailed analysis of the fundamental concepts of both technologies, highlighting their strengths and limitations.","This paper also explores current research efforts that make use of the benefits offered by this combination.","The effort includes how BC can be used as an added layer of security in CV systems and also ensure data integrity, enabling decentralized image and video analytics using BC.","The challenges and open issues associated with this integration are also identified, and appropriate potential future directions are also proposed."],"url":"http://arxiv.org/abs/2307.06659v1"}
{"created":"2023-07-13 10:01:25","title":"Pair Programming Practiced in Hybrid Work","abstract":"Pair programming (PP) has been a widespread practice for decades and is known for facilitating knowledge exchange and improving the quality of software. Many agilists advocated the importance of collocation, face-to-face interaction, and physical artifacts incorporated in the shared workspace when pairing. After a long period of forced work-from-home, many knowledge workers prefer to work remotely two or three days per week, which is affecting practices such as PP. In this revelatory single-case study, we aimed to understand how PP is practiced during hybrid work when team members alternate between on-site days and working from home. We collected qualitative and quantitative data through 11 semi-structured interviews, observations, feedback sessions, and self-reported surveys. The interviewees were members of an agile software development team in a Norwegian fintech company. The results presented in this paper indicate that PP can be practiced through on-site, remote, and mixed sessions, where the mixed mode seems to be the least advantageous. The findings highlight the importance of adapting the work environment to suit individual work mode preferences when it comes to PP. In the future, we will build on these findings to explore PP in other teams and organizations practicing hybrid work.","sentences":["Pair programming (PP) has been a widespread practice for decades and is known for facilitating knowledge exchange and improving the quality of software.","Many agilists advocated the importance of collocation, face-to-face interaction, and physical artifacts incorporated in the shared workspace when pairing.","After a long period of forced work-from-home, many knowledge workers prefer to work remotely two or three days per week, which is affecting practices such as PP.","In this revelatory single-case study, we aimed to understand how PP is practiced during hybrid work when team members alternate between on-site days and working from home.","We collected qualitative and quantitative data through 11 semi-structured interviews, observations, feedback sessions, and self-reported surveys.","The interviewees were members of an agile software development team in a Norwegian fintech company.","The results presented in this paper indicate that PP can be practiced through on-site, remote, and mixed sessions, where the mixed mode seems to be the least advantageous.","The findings highlight the importance of adapting the work environment to suit individual work mode preferences when it comes to PP.","In the future, we will build on these findings to explore PP in other teams and organizations practicing hybrid work."],"url":"http://arxiv.org/abs/2307.06658v1"}
{"created":"2023-07-13 10:01:21","title":"Downlink Precoding for Cell-free FBMC/OQAM Systems With Asynchronous Reception","abstract":"In this work, an efficient precoding design scheme is proposed for downlink cell-free distributed massive multiple-input multiple-output (DM-MIMO) filter bank multi-carrier (FBMC) systems with asynchronous reception and highly frequency selectivity. The proposed scheme includes a multiple interpolation structure to eliminate the impact of response difference we recently discovered, which has better performance in highly frequency-selective channels. Besides, we also consider the phase shift in asynchronous reception and introduce a phase compensation in the design process. The phase compensation also benefits from the multiple interpolation structure and better adapts to asynchronous reception. Based on the proposed scheme, we theoretically analyze its ergodic achievable rate performance and derive a closed-form expression. Simulation results show that the derived expression can accurately characterize the rate performance, and FBMC with the proposed scheme outperforms orthogonal frequency-division multiplexing (OFDM) in the asynchronous scenario.","sentences":["In this work, an efficient precoding design scheme is proposed for downlink cell-free distributed massive multiple-input multiple-output (DM-MIMO) filter bank multi-carrier (FBMC) systems with asynchronous reception and highly frequency selectivity.","The proposed scheme includes a multiple interpolation structure to eliminate the impact of response difference we recently discovered, which has better performance in highly frequency-selective channels.","Besides, we also consider the phase shift in asynchronous reception and introduce a phase compensation in the design process.","The phase compensation also benefits from the multiple interpolation structure and better adapts to asynchronous reception.","Based on the proposed scheme, we theoretically analyze its ergodic achievable rate performance and derive a closed-form expression.","Simulation results show that the derived expression can accurately characterize the rate performance, and FBMC with the proposed scheme outperforms orthogonal frequency-division multiplexing (OFDM) in the asynchronous scenario."],"url":"http://arxiv.org/abs/2307.06657v1"}
{"created":"2023-07-13 09:51:09","title":"Packing squares independently","abstract":"Given a set of squares and a strip of bounded width and infinite height, we consider a square strip packaging problem, which we call the square independent packing problem (SIPP), to minimize the strip height so that all the squares are packed into independent cells separated by horizontal and vertical partitions. For the SIPP, we first investigate efficient solution representations and propose a compact representation that reduces the search space from $\\Omega(n!)$ to $O(2^n)$, with $n$ the number of given squares, while guaranteeing that there exists a solution representation that corresponds to an optimal solution. Based on the solution representation, we show that the problem is NP-hard, and then we propose a fully polynomial-time approximation scheme (FPTAS) to solve it. We also propose three mathematical programming formulations based on different solution representations and confirm the performance of these algorithms through computational experiments. Finally, we discuss several extensions that are relevant to practical applications.","sentences":["Given a set of squares and a strip of bounded width and infinite height, we consider a square strip packaging problem, which we call the square independent packing problem (SIPP), to minimize the strip height so that all the squares are packed into independent cells separated by horizontal and vertical partitions.","For the SIPP, we first investigate efficient solution representations and propose a compact representation that reduces the search space from $\\Omega(n!)$ to $O(2^n)$, with $n$ the number of given squares, while guaranteeing that there exists a solution representation that corresponds to an optimal solution.","Based on the solution representation, we show that the problem is NP-hard, and then we propose a fully polynomial-time approximation scheme (FPTAS) to solve it.","We also propose three mathematical programming formulations based on different solution representations and confirm the performance of these algorithms through computational experiments.","Finally, we discuss several extensions that are relevant to practical applications."],"url":"http://arxiv.org/abs/2307.06654v1"}
{"created":"2023-07-13 09:23:21","title":"DeepIPCv2: LiDAR-powered Robust Environmental Perception and Navigational Control for Autonomous Vehicle","abstract":"We present DeepIPCv2, an autonomous driving model that perceives the environment using a LiDAR sensor for more robust drivability, especially when driving under poor illumination conditions. DeepIPCv2 takes a set of LiDAR point clouds for its main perception input. As point clouds are not affected by illumination changes, they can provide a clear observation of the surroundings no matter what the condition is. This results in a better scene understanding and stable features provided by the perception module to support the controller module in estimating navigational control properly. To evaluate its performance, we conduct several tests by deploying the model to predict a set of driving records and perform real automated driving under three different conditions. We also conduct ablation and comparative studies with some recent models to justify its performance. Based on the experimental results, DeepIPCv2 shows a robust performance by achieving the best drivability in all conditions. Codes are available at https://github.com/oskarnatan/DeepIPCv2","sentences":["We present DeepIPCv2, an autonomous driving model that perceives the environment using a LiDAR sensor for more robust drivability, especially when driving under poor illumination conditions.","DeepIPCv2 takes a set of LiDAR point clouds for its main perception input.","As point clouds are not affected by illumination changes, they can provide a clear observation of the surroundings no matter what the condition is.","This results in a better scene understanding and stable features provided by the perception module to support the controller module in estimating navigational control properly.","To evaluate its performance, we conduct several tests by deploying the model to predict a set of driving records and perform real automated driving under three different conditions.","We also conduct ablation and comparative studies with some recent models to justify its performance.","Based on the experimental results, DeepIPCv2 shows a robust performance by achieving the best drivability in all conditions.","Codes are available at https://github.com/oskarnatan/DeepIPCv2"],"url":"http://arxiv.org/abs/2307.06647v1"}
{"created":"2023-07-13 09:21:39","title":"Multivariate Time Series characterization and forecasting of VoIP traffic in real mobile networks","abstract":"Predicting the behavior of real-time traffic (e.g., VoIP) in mobility scenarios could help the operators to better plan their network infrastructures and to optimize the allocation of resources. Accordingly, in this work the authors propose a forecasting analysis of crucial QoS/QoE descriptors (some of which neglected in the technical literature) of VoIP traffic in a real mobile environment. The problem is formulated in terms of a multivariate time series analysis. Such a formalization allows to discover and model the temporal relationships among various descriptors and to forecast their behaviors for future periods. Techniques such as Vector Autoregressive models and machine learning (deep-based and tree-based) approaches are employed and compared in terms of performance and time complexity, by reframing the multivariate time series problem into a supervised learning one. Moreover, a series of auxiliary analyses (stationarity, orthogonal impulse responses, etc.) are performed to discover the analytical structure of the time series and to provide deep insights about their relationships. The whole theoretical analysis has an experimental counterpart since a set of trials across a real-world LTE-Advanced environment has been performed to collect, post-process and analyze about 600,000 voice packets, organized per flow and differentiated per codec.","sentences":["Predicting the behavior of real-time traffic (e.g., VoIP) in mobility scenarios could help the operators to better plan their network infrastructures and to optimize the allocation of resources.","Accordingly, in this work the authors propose a forecasting analysis of crucial QoS/QoE descriptors (some of which neglected in the technical literature) of VoIP traffic in a real mobile environment.","The problem is formulated in terms of a multivariate time series analysis.","Such a formalization allows to discover and model the temporal relationships among various descriptors and to forecast their behaviors for future periods.","Techniques such as Vector Autoregressive models and machine learning (deep-based and tree-based) approaches are employed and compared in terms of performance and time complexity, by reframing the multivariate time series problem into a supervised learning one.","Moreover, a series of auxiliary analyses (stationarity, orthogonal impulse responses, etc.) are performed to discover the analytical structure of the time series and to provide deep insights about their relationships.","The whole theoretical analysis has an experimental counterpart since a set of trials across a real-world LTE-Advanced environment has been performed to collect, post-process and analyze about 600,000 voice packets, organized per flow and differentiated per codec."],"url":"http://arxiv.org/abs/2307.06645v1"}
{"created":"2023-07-13 09:19:54","title":"Estimating Temporal Trends using Indirect Surveys","abstract":"Indirect surveys, in which respondents provide information about other people they know, have been proposed for scenarios where privacy is important or where the population to be surveyed is hard to reach. As an example, during various stages of the COVID-19 pandemic surveys, including indirect surveys, have been used to estimate the number of cases or the level of vaccination. The Network Scale-up Method (NSUM) is the classical approach to developing such estimates but was designed with discrete, time-limited indirect surveys in mind. Further, it requires asking for or estimating the number of individuals in each respondent's network. In recent years, surveys are being increasingly deployed online and collecting data continuously (e.g., COVID-19 surveys on Facebook during much of the pandemic). Conventional NSUM can be applied to these scenarios by analyzing the data independently during each time interval, but this misses the opportunity of leveraging the temporal dimension. Understanding the advantage of simply smoothing NSUM results to various degrees is not trivial. We propose to use the responses from indirect surveys collected over time and develop analytical tools (i) to prove that indirect surveys can be used to provide better estimates for the size of the hidden population compared to direct surveys, and (ii) to identify appropriate aggregations over time to further improve the estimates. We demonstrate through simulations that our approach outperforms traditional NSUM and direct surveying methods to estimate the size of a time-varying hidden population. We also demonstrate the superiority of our approach on an existing indirect survey dataset on COVID-19 confirmed cases.","sentences":["Indirect surveys, in which respondents provide information about other people they know, have been proposed for scenarios where privacy is important or where the population to be surveyed is hard to reach.","As an example, during various stages of the COVID-19 pandemic surveys, including indirect surveys, have been used to estimate the number of cases or the level of vaccination.","The Network Scale-up Method (NSUM) is the classical approach to developing such estimates but was designed with discrete, time-limited indirect surveys in mind.","Further, it requires asking for or estimating the number of individuals in each respondent's network.","In recent years, surveys are being increasingly deployed online and collecting data continuously (e.g., COVID-19 surveys on Facebook during much of the pandemic).","Conventional NSUM can be applied to these scenarios by analyzing the data independently during each time interval, but this misses the opportunity of leveraging the temporal dimension.","Understanding the advantage of simply smoothing NSUM results to various degrees is not trivial.","We propose to use the responses from indirect surveys collected over time and develop analytical tools (i) to prove that indirect surveys can be used to provide better estimates for the size of the hidden population compared to direct surveys, and (ii) to identify appropriate aggregations over time to further improve the estimates.","We demonstrate through simulations that our approach outperforms traditional NSUM and direct surveying methods to estimate the size of a time-varying hidden population.","We also demonstrate the superiority of our approach on an existing indirect survey dataset on COVID-19 confirmed cases."],"url":"http://arxiv.org/abs/2307.06643v1"}
{"created":"2023-07-13 09:14:48","title":"Discovering How Agents Learn Using Few Data","abstract":"Decentralized learning algorithms are an essential tool for designing multi-agent systems, as they enable agents to autonomously learn from their experience and past interactions. In this work, we propose a theoretical and algorithmic framework for real-time identification of the learning dynamics that govern agent behavior using a short burst of a single system trajectory. Our method identifies agent dynamics through polynomial regression, where we compensate for limited data by incorporating side-information constraints that capture fundamental assumptions or expectations about agent behavior. These constraints are enforced computationally using sum-of-squares optimization, leading to a hierarchy of increasingly better approximations of the true agent dynamics. Extensive experiments demonstrated that our approach, using only 5 samples from a short run of a single trajectory, accurately recovers the true dynamics across various benchmarks, including equilibrium selection and prediction of chaotic systems up to 10 Lyapunov times. These findings suggest that our approach has significant potential to support effective policy and decision-making in strategic multi-agent systems.","sentences":["Decentralized learning algorithms are an essential tool for designing multi-agent systems, as they enable agents to autonomously learn from their experience and past interactions.","In this work, we propose a theoretical and algorithmic framework for real-time identification of the learning dynamics that govern agent behavior using a short burst of a single system trajectory.","Our method identifies agent dynamics through polynomial regression, where we compensate for limited data by incorporating side-information constraints that capture fundamental assumptions or expectations about agent behavior.","These constraints are enforced computationally using sum-of-squares optimization, leading to a hierarchy of increasingly better approximations of the true agent dynamics.","Extensive experiments demonstrated that our approach, using only 5 samples from a short run of a single trajectory, accurately recovers the true dynamics across various benchmarks, including equilibrium selection and prediction of chaotic systems up to 10 Lyapunov times.","These findings suggest that our approach has significant potential to support effective policy and decision-making in strategic multi-agent systems."],"url":"http://arxiv.org/abs/2307.06640v1"}
{"created":"2023-07-13 09:06:22","title":"Towards a 6G embedding sustainability","abstract":"From its conception, 6G is being designed with a particular focus on sustainability. The general philosophy of the H2020 Hexa-X project work on sustainability in 6G is based on two principles: to reduce direct negative life cycle impacts of 6G systems as much as possible (Sustainable 6G) and to analyze use cases that maximize positive environmental, social, and economic effects in other sectors of society (6G for Sustainability or its enablement effect). To apply this philosophy, Hexa-X is designing 6G with three sustainability objectives in mind: to enable the reduction of emissions in 6G-powered sectors of society, to reduce the total cost of ownership and to improve energy efficiency. This paper describes these objectives, their associated KPIs and quantitative targets, and the levers to reach them. Furthermore, to maximize the positive effects of 6G through the enablement effect, a link between 6G and the United Nations' Sustainable Development Goals (UN SDGs) framework is proposed and illustrated by Hexa-X use case families.","sentences":["From its conception, 6G is being designed with a particular focus on sustainability.","The general philosophy of the H2020 Hexa-X project work on sustainability in 6G is based on two principles: to reduce direct negative life cycle impacts of 6G systems as much as possible (Sustainable 6G) and to analyze use cases that maximize positive environmental, social, and economic effects in other sectors of society (6G for Sustainability or its enablement effect).","To apply this philosophy, Hexa-X is designing 6G with three sustainability objectives in mind: to enable the reduction of emissions in 6G-powered sectors of society, to reduce the total cost of ownership and to improve energy efficiency.","This paper describes these objectives, their associated KPIs and quantitative targets, and the levers to reach them.","Furthermore, to maximize the positive effects of 6G through the enablement effect, a link between 6G and the United Nations' Sustainable Development Goals (UN SDGs) framework is proposed and illustrated by Hexa-X use case families."],"url":"http://arxiv.org/abs/2307.06636v1"}
{"created":"2023-07-13 09:04:36","title":"Making local algorithms efficiently self-stabilizing in arbitrary asynchronous environments","abstract":"This paper deals with the trade-off between time, workload, and versatility in self-stabilization, a general and lightweight fault-tolerant concept in distributed computing.In this context, we propose a transformer that provides an asynchronous silent self-stabilizing version Trans(AlgI) of any terminating synchronous algorithm AlgI. The transformed algorithm Trans(AlgI) works under the distributed unfair daemon and is efficient both in moves and rounds.Our transformer allows to easily obtain fully-polynomial silent self-stabilizing solutions that are also asymptotically optimal in rounds.We illustrate the efficiency and versatility of our transformer with several efficient (i.e., fully-polynomial) silent self-stabilizing instances solving major distributed computing problems, namely vertex coloring, Breadth-First Search (BFS) spanning tree construction, k-clustering, and leader election.","sentences":["This paper deals with the trade-off between time, workload, and versatility in self-stabilization, a general and lightweight fault-tolerant concept in distributed computing.","In this context, we propose a transformer that provides an asynchronous silent self-stabilizing version Trans(AlgI) of any terminating synchronous algorithm AlgI.","The transformed algorithm Trans(AlgI) works under the distributed unfair daemon and is efficient both in moves and rounds.","Our transformer allows to easily obtain fully-polynomial silent self-stabilizing solutions that are also asymptotically optimal in rounds.","We illustrate the efficiency and versatility of our transformer with several efficient (i.e., fully-polynomial) silent self-stabilizing instances solving major distributed computing problems, namely vertex coloring, Breadth-First Search (BFS) spanning tree construction, k-clustering, and leader election."],"url":"http://arxiv.org/abs/2307.06635v1"}
{"created":"2023-07-13 08:59:39","title":"FF-LINS: A Consistent Frame-to-Frame Solid-State-LiDAR-Inertial State Estimator","abstract":"Most of the existing LiDAR-inertial navigation systems are based on frame-to-map registrations, leading to inconsistency in state estimation. The newest solid-state LiDAR with a non-repetitive scanning pattern makes it possible to achieve a consistent LiDAR-inertial estimator by employing a frame-to-frame data association. In this letter, we propose a robust and consistent frame-to-frame LiDAR-inertial navigation system (FF-LINS) for solid-state LiDARs. With the INS-centric LiDAR frame processing, the keyframe point-cloud map is built using the accumulated point clouds to construct the frame-to-frame data association. The LiDAR frame-to-frame and the inertial measurement unit (IMU) preintegration measurements are tightly integrated using the factor graph optimization, with online calibration of the LiDAR-IMU extrinsic and time-delay parameters. The experiments on the public and private datasets demonstrate that the proposed FF-LINS achieves superior accuracy and robustness than the state-of-the-art systems. Besides, the LiDAR-IMU extrinsic and time-delay parameters are estimated effectively, and the online calibration notably improves the pose accuracy. The proposed FF-LINS and the employed datasets are open-sourced on GitHub (https://github.com/i2Nav-WHU/FF-LINS).","sentences":["Most of the existing LiDAR-inertial navigation systems are based on frame-to-map registrations, leading to inconsistency in state estimation.","The newest solid-state LiDAR with a non-repetitive scanning pattern makes it possible to achieve a consistent LiDAR-inertial estimator by employing a frame-to-frame data association.","In this letter, we propose a robust and consistent frame-to-frame LiDAR-inertial navigation system (FF-LINS) for solid-state LiDARs.","With the INS-centric LiDAR frame processing, the keyframe point-cloud map is built using the accumulated point clouds to construct the frame-to-frame data association.","The LiDAR frame-to-frame and the inertial measurement unit (IMU) preintegration measurements are tightly integrated using the factor graph optimization, with online calibration of the LiDAR-IMU extrinsic and time-delay parameters.","The experiments on the public and private datasets demonstrate that the proposed FF-LINS achieves superior accuracy and robustness than the state-of-the-art systems.","Besides, the LiDAR-IMU extrinsic and time-delay parameters are estimated effectively, and the online calibration notably improves the pose accuracy.","The proposed FF-LINS and the employed datasets are open-sourced on GitHub (https://github.com/i2Nav-WHU/FF-LINS)."],"url":"http://arxiv.org/abs/2307.06632v1"}
{"created":"2023-07-13 08:56:50","title":"Frameless Graph Knowledge Distillation","abstract":"Knowledge distillation (KD) has shown great potential for transferring knowledge from a complex teacher model to a simple student model in which the heavy learning task can be accomplished efficiently and without losing too much prediction accuracy. Recently, many attempts have been made by applying the KD mechanism to the graph representation learning models such as graph neural networks (GNNs) to accelerate the model's inference speed via student models. However, many existing KD-based GNNs utilize MLP as a universal approximator in the student model to imitate the teacher model's process without considering the graph knowledge from the teacher model. In this work, we provide a KD-based framework on multi-scaled GNNs, known as graph framelet, and prove that by adequately utilizing the graph knowledge in a multi-scaled manner provided by graph framelet decomposition, the student model is capable of adapting both homophilic and heterophilic graphs and has the potential of alleviating the over-squashing issue with a simple yet effectively graph surgery. Furthermore, we show how the graph knowledge supplied by the teacher is learned and digested by the student model via both algebra and geometry. Comprehensive experiments show that our proposed model can generate learning accuracy identical to or even surpass the teacher model while maintaining the high speed of inference.","sentences":["Knowledge distillation (KD) has shown great potential for transferring knowledge from a complex teacher model to a simple student model in which the heavy learning task can be accomplished efficiently and without losing too much prediction accuracy.","Recently, many attempts have been made by applying the KD mechanism to the graph representation learning models such as graph neural networks (GNNs) to accelerate the model's inference speed via student models.","However, many existing KD-based GNNs utilize MLP as a universal approximator in the student model to imitate the teacher model's process without considering the graph knowledge from the teacher model.","In this work, we provide a KD-based framework on multi-scaled GNNs, known as graph framelet, and prove that by adequately utilizing the graph knowledge in a multi-scaled manner provided by graph framelet decomposition, the student model is capable of adapting both homophilic and heterophilic graphs and has the potential of alleviating the over-squashing issue with a simple yet effectively graph surgery.","Furthermore, we show how the graph knowledge supplied by the teacher is learned and digested by the student model via both algebra and geometry.","Comprehensive experiments show that our proposed model can generate learning accuracy identical to or even surpass the teacher model while maintaining the high speed of inference."],"url":"http://arxiv.org/abs/2307.06631v1"}
{"created":"2023-07-13 08:56:20","title":"Image Transformation Sequence Retrieval with General Reinforcement Learning","abstract":"In this work, the novel Image Transformation Sequence Retrieval (ITSR) task is presented, in which a model must retrieve the sequence of transformations between two given images that act as source and target, respectively. Given certain characteristics of the challenge such as the multiplicity of a correct sequence or the correlation between consecutive steps of the process, we propose a solution to ITSR using a general model-based Reinforcement Learning such as Monte Carlo Tree Search (MCTS), which is combined with a deep neural network. Our experiments provide a benchmark in both synthetic and real domains, where the proposed approach is compared with supervised training. The results report that a model trained with MCTS is able to outperform its supervised counterpart in both the simplest and the most complex cases. Our work draws interesting conclusions about the nature of ITSR and its associated challenges.","sentences":["In this work, the novel Image Transformation Sequence Retrieval (ITSR) task is presented, in which a model must retrieve the sequence of transformations between two given images that act as source and target, respectively.","Given certain characteristics of the challenge such as the multiplicity of a correct sequence or the correlation between consecutive steps of the process, we propose a solution to ITSR using a general model-based Reinforcement Learning such as Monte Carlo Tree Search (MCTS), which is combined with a deep neural network.","Our experiments provide a benchmark in both synthetic and real domains, where the proposed approach is compared with supervised training.","The results report that a model trained with MCTS is able to outperform its supervised counterpart in both the simplest and the most complex cases.","Our work draws interesting conclusions about the nature of ITSR and its associated challenges."],"url":"http://arxiv.org/abs/2307.06630v1"}
{"created":"2023-07-13 08:46:19","title":"Fast and Practical Quantum-Inspired Classical Algorithms for Solving Linear Systems","abstract":"We propose fast and practical quantum-inspired classical algorithms for solving linear systems. Specifically, given sampling and query access to a matrix $A\\in\\mathbb{R}^{m\\times n}$ and a vector $b\\in\\mathbb{R}^m$, we propose classical algorithms that produce a data structure for the solution $x\\in\\mathbb{R}^{n}$ of the linear system $Ax=b$ with the ability to sample and query its entries. The resulting $x$ satisfies $\\|x-A^{+}b\\|\\leq\\epsilon\\|A^{+}b\\|$, where $\\|\\cdot\\|$ is the spectral norm and $A^+$ is the Moore-Penrose inverse of $A$. Our algorithm has time complexity $\\widetilde{O}(\\kappa_F^4/\\kappa\\epsilon^2)$ in the general case, where $\\kappa_{F} =\\|A\\|_F\\|A^+\\|$ and $\\kappa=\\|A\\|\\|A^+\\|$ are condition numbers. Compared to the prior state-of-the-art result [Shao and Montanaro, arXiv:2103.10309v2], our algorithm achieves a polynomial speedup in condition numbers. When $A$ is $s$-sparse, our algorithm has complexity $\\widetilde{O}(s \\kappa\\log(1/\\epsilon))$, matching the quantum lower bound for solving linear systems in $\\kappa$ and $1/\\epsilon$ up to poly-logarithmic factors [Harrow and Kothari]. When $A$ is $s$-sparse and symmetric positive-definite, our algorithm has complexity $\\widetilde{O}(s\\sqrt{\\kappa}\\log(1/\\epsilon))$.   Technically, our main contribution is the application of the heavy ball momentum method to quantum-inspired classical algorithms for solving linear systems, where we propose two new methods with speedups: quantum-inspired Kaczmarz method with momentum and quantum-inspired coordinate descent method with momentum. Their analysis exploits careful decomposition of the momentum transition matrix and the application of novel spectral norm concentration bounds for independent random matrices. Finally, we also conduct numerical experiments for our algorithms on both synthetic and real-world datasets, and the experimental results support our theoretical claims.","sentences":["We propose fast and practical quantum-inspired classical algorithms for solving linear systems.","Specifically, given sampling and query access to a matrix $A\\in\\mathbb{R}^{m\\times n}$ and a vector $b\\in\\mathbb{R}^m$, we propose classical algorithms that produce a data structure for the solution $x\\in\\mathbb{R}^{n}$ of the linear system $Ax=b$ with the ability to sample and query its entries.","The resulting $x$ satisfies $\\|x-A^{+}b\\|\\leq\\epsilon\\|A^{+}b\\|$, where $\\|\\cdot\\|$ is the spectral norm and $A^+$ is the Moore-Penrose inverse of $A$.","Our algorithm has time complexity $\\widetilde{O}(\\kappa_F^4/\\kappa\\epsilon^2)$ in the general case, where $\\kappa_{F} =\\|A\\|_F\\|A^+\\|$ and $\\kappa=\\|A\\|\\|A^+\\|$ are condition numbers.","Compared to the prior state-of-the-art result [Shao and Montanaro, arXiv:2103.10309v2], our algorithm achieves a polynomial speedup in condition numbers.","When $A$ is $s$-sparse, our algorithm has complexity $\\widetilde{O}(s \\kappa\\log(1/\\epsilon))$, matching the quantum lower bound for solving linear systems in $\\kappa$ and $1/\\epsilon$ up to poly-logarithmic factors [Harrow and Kothari].","When $A$ is $s$-sparse and symmetric positive-definite, our algorithm has complexity $\\widetilde{O}(s\\sqrt{\\kappa}\\log(1/\\epsilon))$.   Technically, our main contribution is the application of the heavy ball momentum method to quantum-inspired classical algorithms for solving linear systems, where we propose two new methods with speedups: quantum-inspired Kaczmarz method with momentum and quantum-inspired coordinate descent method with momentum.","Their analysis exploits careful decomposition of the momentum transition matrix and the application of novel spectral norm concentration bounds for independent random matrices.","Finally, we also conduct numerical experiments for our algorithms on both synthetic and real-world datasets, and the experimental results support our theoretical claims."],"url":"http://arxiv.org/abs/2307.06627v1"}
{"created":"2023-07-13 08:45:15","title":"Automated Deception Detection from Videos: Using End-to-End Learning Based High-Level Features and Classification Approaches","abstract":"Deception detection is an interdisciplinary field attracting researchers from psychology, criminology, computer science, and economics. We propose a multimodal approach combining deep learning and discriminative models for automated deception detection. Using video modalities, we employ convolutional end-to-end learning to analyze gaze, head pose, and facial expressions, achieving promising results compared to state-of-the-art methods. Due to limited training data, we also utilize discriminative models for deception detection. Although sequence-to-class approaches are explored, discriminative models outperform them due to data scarcity. Our approach is evaluated on five datasets, including a new Rolling-Dice Experiment motivated by economic factors. Results indicate that facial expressions outperform gaze and head pose, and combining modalities with feature selection enhances detection performance. Differences in expressed features across datasets emphasize the importance of scenario-specific training data and the influence of context on deceptive behavior. Cross-dataset experiments reinforce these findings. Despite the challenges posed by low-stake datasets, including the Rolling-Dice Experiment, deception detection performance exceeds chance levels. Our proposed multimodal approach and comprehensive evaluation shed light on the potential of automating deception detection from video modalities, opening avenues for future research.","sentences":["Deception detection is an interdisciplinary field attracting researchers from psychology, criminology, computer science, and economics.","We propose a multimodal approach combining deep learning and discriminative models for automated deception detection.","Using video modalities, we employ convolutional end-to-end learning to analyze gaze, head pose, and facial expressions, achieving promising results compared to state-of-the-art methods.","Due to limited training data, we also utilize discriminative models for deception detection.","Although sequence-to-class approaches are explored, discriminative models outperform them due to data scarcity.","Our approach is evaluated on five datasets, including a new Rolling-Dice Experiment motivated by economic factors.","Results indicate that facial expressions outperform gaze and head pose, and combining modalities with feature selection enhances detection performance.","Differences in expressed features across datasets emphasize the importance of scenario-specific training data and the influence of context on deceptive behavior.","Cross-dataset experiments reinforce these findings.","Despite the challenges posed by low-stake datasets, including the Rolling-Dice Experiment, deception detection performance exceeds chance levels.","Our proposed multimodal approach and comprehensive evaluation shed light on the potential of automating deception detection from video modalities, opening avenues for future research."],"url":"http://arxiv.org/abs/2307.06625v1"}
{"created":"2023-07-13 08:36:36","title":"cjdb: a simple, fast, and lean database solution for the CityGML data model","abstract":"When it comes to storing 3D city models in a database, the implementation of the CityGML data model can be quite demanding and often results in complicated schemas. As an example, 3DCityDB, a widely used solution, depends on a schema having 66 tables, mapping closely the CityGML architecture. In this paper, we propose an alternative (called cjdb) for storing CityGML models efficiently in PostgreSQL with a much simpler table structure and data model design (only 3 tables are necessary). This is achieved by storing the attributes and geometries of the objects directly in JSON. In the case of the geometries we thus adopt the Simple Feature paradigm and we use the structure of CityJSON. We compare our solution against 3DCityDB with large real-world 3D city models, and we find that cjdb has significantly lower demands in storage space (around a factor of 10), allows for faster import/export of data, and has a comparable data retrieval speed with some queries being faster and some slower. The accompanying software (importer and exporter) is available at https://github.com/cityjson/cjdb/ under a permissive open-source license.","sentences":["When it comes to storing 3D city models in a database, the implementation of the CityGML data model can be quite demanding and often results in complicated schemas.","As an example, 3DCityDB, a widely used solution, depends on a schema having 66 tables, mapping closely the CityGML architecture.","In this paper, we propose an alternative (called cjdb) for storing CityGML models efficiently in PostgreSQL with a much simpler table structure and data model design (only 3 tables are necessary).","This is achieved by storing the attributes and geometries of the objects directly in JSON.","In the case of the geometries we thus adopt the Simple Feature paradigm and we use the structure of CityJSON.","We compare our solution against 3DCityDB with large real-world 3D city models, and we find that cjdb has significantly lower demands in storage space (around a factor of 10), allows for faster import/export of data, and has a comparable data retrieval speed with some queries being faster and some slower.","The accompanying software (importer and exporter) is available at https://github.com/cityjson/cjdb/ under a permissive open-source license."],"url":"http://arxiv.org/abs/2307.06621v1"}
{"created":"2023-07-13 08:36:15","title":"Online Distributed Learning with Quantized Finite-Time Coordination","abstract":"In this paper we consider online distributed learning problems. Online distributed learning refers to the process of training learning models on distributed data sources. In our setting a set of agents need to cooperatively train a learning model from streaming data. Differently from federated learning, the proposed approach does not rely on a central server but only on peer-to-peer communications among the agents. This approach is often used in scenarios where data cannot be moved to a centralized location due to privacy, security, or cost reasons. In order to overcome the absence of a central server, we propose a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate the locally trained models. Furthermore, our algorithm allows for the use of stochastic gradients during local training. Stochastic gradients are computed using a randomly sampled subset of the local training data, which makes the proposed algorithm more efficient and scalable than traditional gradient descent. In our paper, we analyze the performance of the proposed algorithm in terms of the mean distance from the online solution. Finally, we present numerical results for a logistic regression task.","sentences":["In this paper we consider online distributed learning problems.","Online distributed learning refers to the process of training learning models on distributed data sources.","In our setting a set of agents need to cooperatively train a learning model from streaming data.","Differently from federated learning, the proposed approach does not rely on a central server but only on peer-to-peer communications among the agents.","This approach is often used in scenarios where data cannot be moved to a centralized location due to privacy, security, or cost reasons.","In order to overcome the absence of a central server, we propose a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate the locally trained models.","Furthermore, our algorithm allows for the use of stochastic gradients during local training.","Stochastic gradients are computed using a randomly sampled subset of the local training data, which makes the proposed algorithm more efficient and scalable than traditional gradient descent.","In our paper, we analyze the performance of the proposed algorithm in terms of the mean distance from the online solution.","Finally, we present numerical results for a logistic regression task."],"url":"http://arxiv.org/abs/2307.06620v1"}
{"created":"2023-07-13 08:35:40","title":"Learning IMM Filter Parameters from Measurements using Gradient Descent","abstract":"The performance of data fusion and tracking algorithms often depends on parameters that not only describe the sensor system, but can also be task-specific. While for the sensor system tuning these variables is time-consuming and mostly requires expert knowledge, intrinsic parameters of targets under track can even be completely unobservable until the system is deployed. With state-of-the-art sensor systems growing more and more complex, the number of parameters naturally increases, necessitating the automatic optimization of the model variables. In this paper, the parameters of an interacting multiple model (IMM) filter are optimized solely using measurements, thus without necessity for any ground-truth data. The resulting method is evaluated through an ablation study on simulated data, where the trained model manages to match the performance of a filter parametrized with ground-truth values.","sentences":["The performance of data fusion and tracking algorithms often depends on parameters that not only describe the sensor system, but can also be task-specific.","While for the sensor system tuning these variables is time-consuming and mostly requires expert knowledge, intrinsic parameters of targets under track can even be completely unobservable until the system is deployed.","With state-of-the-art sensor systems growing more and more complex, the number of parameters naturally increases, necessitating the automatic optimization of the model variables.","In this paper, the parameters of an interacting multiple model (IMM) filter are optimized solely using measurements, thus without necessity for any ground-truth data.","The resulting method is evaluated through an ablation study on simulated data, where the trained model manages to match the performance of a filter parametrized with ground-truth values."],"url":"http://arxiv.org/abs/2307.06618v1"}
{"created":"2023-07-13 08:34:09","title":"SecureFalcon: The Next Cyber Reasoning System for Cyber Security","abstract":"Software vulnerabilities leading to various detriments such as crashes, data loss, and security breaches, significantly hinder the quality, affecting the market adoption of software applications and systems. Although traditional methods such as automated software testing, fault localization, and repair have been intensively studied, static analysis tools are most commonly used and have an inherent false positives rate, posing a solid challenge to developer productivity. Large Language Models (LLMs) offer a promising solution to these persistent issues. Among these, FalconLLM has shown substantial potential in identifying intricate patterns and complex vulnerabilities, hence crucial in software vulnerability detection. In this paper, for the first time, FalconLLM is being fine-tuned for cybersecurity applications, thus introducing SecureFalcon, an innovative model architecture built upon FalconLLM. SecureFalcon is trained to differentiate between vulnerable and non-vulnerable C code samples. We build a new training dataset, FormAI, constructed thanks to Generative Artificial Intelligence (AI) and formal verification to evaluate its performance. SecureFalcon achieved an impressive 94% accuracy rate in detecting software vulnerabilities, emphasizing its significant potential to redefine software vulnerability detection methods in cybersecurity.","sentences":["Software vulnerabilities leading to various detriments such as crashes, data loss, and security breaches, significantly hinder the quality, affecting the market adoption of software applications and systems.","Although traditional methods such as automated software testing, fault localization, and repair have been intensively studied, static analysis tools are most commonly used and have an inherent false positives rate, posing a solid challenge to developer productivity.","Large Language Models (LLMs) offer a promising solution to these persistent issues.","Among these, FalconLLM has shown substantial potential in identifying intricate patterns and complex vulnerabilities, hence crucial in software vulnerability detection.","In this paper, for the first time, FalconLLM is being fine-tuned for cybersecurity applications, thus introducing SecureFalcon, an innovative model architecture built upon FalconLLM.","SecureFalcon is trained to differentiate between vulnerable and non-vulnerable C code samples.","We build a new training dataset, FormAI, constructed thanks to Generative Artificial Intelligence (AI) and formal verification to evaluate its performance.","SecureFalcon achieved an impressive 94% accuracy rate in detecting software vulnerabilities, emphasizing its significant potential to redefine software vulnerability detection methods in cybersecurity."],"url":"http://arxiv.org/abs/2307.06616v1"}
{"created":"2023-07-13 08:17:58","title":"Entropic Risk for Turn-Based Stochastic Games","abstract":"Entropic risk (ERisk) is an established risk measure in finance, quantifying risk by an exponential re-weighting of rewards. We study ERisk for the first time in the context of turn-based stochastic games with the total reward objective. This gives rise to an objective function that demands the control of systems in a risk-averse manner. We show that the resulting games are determined and, in particular, admit optimal memoryless deterministic strategies. This contrasts risk measures that previously have been considered in the special case of Markov decision processes and that require randomization and/or memory. We provide several results on the decidability and the computational complexity of the threshold problem, i.e. whether the optimal value of ERisk exceeds a given threshold. In the most general case, the problem is decidable subject to Shanuel's conjecture. If all inputs are rational, the resulting threshold problem can be solved using algebraic numbers, leading to decidability via a polynomial-time reduction to the existential theory of the reals. Further restrictions on the encoding of the input allow the solution of the threshold problem in NP$\\cap$coNP. Finally, an approximation algorithm for the optimal value of ERisk is provided.","sentences":["Entropic risk (ERisk) is an established risk measure in finance, quantifying risk by an exponential re-weighting of rewards.","We study ERisk for the first time in the context of turn-based stochastic games with the total reward objective.","This gives rise to an objective function that demands the control of systems in a risk-averse manner.","We show that the resulting games are determined and, in particular, admit optimal memoryless deterministic strategies.","This contrasts risk measures that previously have been considered in the special case of Markov decision processes and that require randomization and/or memory.","We provide several results on the decidability and the computational complexity of the threshold problem, i.e. whether the optimal value of ERisk exceeds a given threshold.","In the most general case, the problem is decidable subject to Shanuel's conjecture.","If all inputs are rational, the resulting threshold problem can be solved using algebraic numbers, leading to decidability via a polynomial-time reduction to the existential theory of the reals.","Further restrictions on the encoding of the input allow the solution of the threshold problem in NP$\\cap$coNP.","Finally, an approximation algorithm for the optimal value of ERisk is provided."],"url":"http://arxiv.org/abs/2307.06611v1"}
{"created":"2023-07-13 08:10:48","title":"Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks","abstract":"Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster performance of foundational models (e.g., CLIP) to their significant representational capacity and, conversely, their lack of discriminative prowess. To mitigate this issue, we propose the use of a margin-based loss strategy for the fine-tuning of foundational models on target images. The experimental results verify that our approach, which employs the basic Fast Gradient Sign Method (FGSM) attack algorithm, outstrips the performance of other, more convoluted algorithms. We conclude by advocating for the research community to consider surrogate models as crucial determinants in the effectiveness of adversarial attacks in no-box settings. The implications of our work bear relevance for improving the efficacy of such adversarial attacks and the overall robustness of AI systems.","sentences":["Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup.","However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting.","Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task.","Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models.","Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role.","However, paradoxically, we observe that these foundational models underperform.","Analyzing this unexpected behavior within the feature space, we attribute the lackluster performance of foundational models (e.g., CLIP) to their significant representational capacity and, conversely, their lack of discriminative prowess.","To mitigate this issue, we propose the use of a margin-based loss strategy for the fine-tuning of foundational models on target images.","The experimental results verify that our approach, which employs the basic Fast Gradient Sign Method (FGSM) attack algorithm, outstrips the performance of other, more convoluted algorithms.","We conclude by advocating for the research community to consider surrogate models as crucial determinants in the effectiveness of adversarial attacks in no-box settings.","The implications of our work bear relevance for improving the efficacy of such adversarial attacks and the overall robustness of AI systems."],"url":"http://arxiv.org/abs/2307.06608v1"}
{"created":"2023-07-13 07:48:06","title":"On the Weight Spectrum Improvement of Pre-transformed Reed-Muller Codes and Polar Codes","abstract":"Pre-transformation with an upper-triangular matrix (including cyclic redundancy check (CRC), parity-check (PC) and polarization-adjusted convolutional (PAC) codes) improves the weight spectrum of Reed-Muller (RM) codes and polar codes significantly. However, a theoretical analysis to quantify the improvement is missing. In this paper, we provide asymptotic analysis on the number of low-weight codewords of the original and pre-transformed RM codes respectively, and prove that pre-transformation significantly reduces low-weight codewords, even in the order sense. For polar codes, we prove that the average number of minimum-weight codewords does not increase after pre-transformation. Both results confirm the advantages of pre-transformation.","sentences":["Pre-transformation with an upper-triangular matrix (including cyclic redundancy check (CRC), parity-check (PC) and polarization-adjusted convolutional (PAC) codes) improves the weight spectrum of Reed-Muller (RM) codes and polar codes significantly.","However, a theoretical analysis to quantify the improvement is missing.","In this paper, we provide asymptotic analysis on the number of low-weight codewords of the original and pre-transformed RM codes respectively, and prove that pre-transformation significantly reduces low-weight codewords, even in the order sense.","For polar codes, we prove that the average number of minimum-weight codewords does not increase after pre-transformation.","Both results confirm the advantages of pre-transformation."],"url":"http://arxiv.org/abs/2307.06599v1"}
{"created":"2023-07-13 07:33:47","title":"Integer sequences that are generalized weights of a linear code","abstract":"Which integer sequences are sequences of generalized weights of a linear code? In this paper, we answer this question for linear block codes, rank-metric codes, and more generally for sum-rank metric codes. We do so under an existence assumption for MDS and MSRD codes. We also prove that the same integer sequences appear as sequences of greedy weights of linear block codes, rank-metric codes, and sum-rank metric codes. Finally, we characterize the integer sequences which appear as sequences of relative generalized weights (respectively, relative greedy weights) of linear block codes.","sentences":["Which integer sequences are sequences of generalized weights of a linear code?","In this paper, we answer this question for linear block codes, rank-metric codes, and more generally for sum-rank metric codes.","We do so under an existence assumption for MDS and MSRD codes.","We also prove that the same integer sequences appear as sequences of greedy weights of linear block codes, rank-metric codes, and sum-rank metric codes.","Finally, we characterize the integer sequences which appear as sequences of relative generalized weights (respectively, relative greedy weights) of linear block codes."],"url":"http://arxiv.org/abs/2307.06595v1"}
{"created":"2023-07-13 06:44:24","title":"Edge-Coloring Algorithms for Bounded Degree Multigraphs","abstract":"In this paper, we consider algorithms for edge-coloring multigraphs $G$ of bounded maximum degree, i.e., $\\Delta(G) = O(1)$. Shannon's theorem states that any multigraph of maximum degree $\\Delta$ can be properly edge-colored with $\\lfloor 3\\Delta/2\\rfloor$ colors. Our main results include algorithms for computing such colorings. We design deterministic and randomized sequential algorithms with running time $O(n\\log n)$ and $O(n)$, respectively. This is the first improvement since the $O(n^2)$ algorithm in Shannon's original paper, and our randomized algorithm is optimal up to constant factors. We also develop distributed algorithms in the $\\mathsf{LOCAL}$ model of computation. Namely, we design deterministic and randomized $\\mathsf{LOCAL}$ algorithms with running time $\\tilde O(\\log^5 n)$ and $O(\\log^2n)$, respectively. The deterministic sequential algorithm is a simplified extension of earlier work of Gabow et al. in edge-coloring simple graphs. The other algorithms apply the entropy compression method in a similar way to recent work by the author and Bernshteyn, where the authors design algorithms for Vizing's theorem for simple graphs. We also extend their results to Vizing's theorem for multigraphs.","sentences":["In this paper, we consider algorithms for edge-coloring multigraphs $G$ of bounded maximum degree, i.e., $\\Delta(G) = O(1)$. Shannon's theorem states that any multigraph of maximum degree $\\Delta$ can be properly edge-colored with $\\lfloor 3\\Delta/2\\rfloor$ colors.","Our main results include algorithms for computing such colorings.","We design deterministic and randomized sequential algorithms with running time $O(n\\log n)$ and $O(n)$, respectively.","This is the first improvement since the $O(n^2)$ algorithm in Shannon's original paper, and our randomized algorithm is optimal up to constant factors.","We also develop distributed algorithms in the $\\mathsf{LOCAL}$ model of computation.","Namely, we design deterministic and randomized $\\mathsf{LOCAL}$ algorithms with running time $\\tilde O(\\log^5 n)$ and $O(\\log^2n)$, respectively.","The deterministic sequential algorithm is a simplified extension of earlier work of Gabow et al. in edge-coloring simple graphs.","The other algorithms apply the entropy compression method in a similar way to recent work by the author and Bernshteyn, where the authors design algorithms for Vizing's theorem for simple graphs.","We also extend their results to Vizing's theorem for multigraphs."],"url":"http://arxiv.org/abs/2307.06579v1"}
{"created":"2023-07-13 06:30:09","title":"RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation","abstract":"Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices. The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility. Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition. The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old. It delivers comprehensive and precise annotations of retinal structures in both spatial and temporal dimensions, aiming to advance the landscape of vasculature segmentation. Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fine-grained vein-artery masks for further characterizing the granularities of each artery and vein. In addition, the dataset offers temporal annotations that capture the vessel pulsation characteristics, assisting in detecting ocular diseases that require fine-grained recognition of hemodynamic fluctuation. In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus posing great challenges to existing methods. In the experiments, we provide evaluation metrics and benchmark results on our dataset, reflecting both the potential and challenges it offers for vessel segmentation tasks. We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early prevention.","sentences":["Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices.","The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility.","Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition.","The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old.","It delivers comprehensive and precise annotations of retinal structures in both spatial and temporal dimensions, aiming to advance the landscape of vasculature segmentation.","Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fine-grained vein-artery masks for further characterizing the granularities of each artery and vein.","In addition, the dataset offers temporal annotations that capture the vessel pulsation characteristics, assisting in detecting ocular diseases that require fine-grained recognition of hemodynamic fluctuation.","In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus posing great challenges to existing methods.","In the experiments, we provide evaluation metrics and benchmark results on our dataset, reflecting both the potential and challenges it offers for vessel segmentation tasks.","We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early prevention."],"url":"http://arxiv.org/abs/2307.06577v1"}
{"created":"2023-07-13 06:25:22","title":"Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations","abstract":"Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems. Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news. However, this approach lacks a global perspective, failing to account for users' hidden motivations and behaviors beyond semantic information. To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems. We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator. Similarly, we extend this approach to a Global Candidate News Encoder, utilizing a global entity graph and a candidate news aggregator to enhance candidate news representation. Evaluation results on two public news datasets demonstrate that our method outperforms existing approaches. Furthermore, our model offers more diverse recommendations.","sentences":["Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems.","Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news.","However, this approach lacks a global perspective, failing to account for users' hidden motivations and behaviors beyond semantic information.","To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems.","We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator.","Similarly, we extend this approach to a Global Candidate News Encoder, utilizing a global entity graph and a candidate news aggregator to enhance candidate news representation.","Evaluation results on two public news datasets demonstrate that our method outperforms existing approaches.","Furthermore, our model offers more diverse recommendations."],"url":"http://arxiv.org/abs/2307.06576v1"}
{"created":"2023-07-13 06:23:00","title":"Deep learning based enhancement of ordered statistics decoding of LDPC codes","abstract":"Aiming at designing plausible decoders with channel information free, low complexity, high throughput, and approaching maximum likelihood performance, we put forward a streamlined architecture which concatenates sequentially three components. Specifically, to tackle the decoding failures of normalized min-sum, the whole decoding trajectory, not limited to the last iteration information conventionally, is fed into a trained convolutional neural network to yield new reliability metric for each sequence bit, termed decoding information aggregation. Then an adapted order statistics decoding, following the suggested decoding path, is adopted to process the sequence ordered with new metric more efficiently in that many invalid searches contained in conventional methods otherwise are evaded. The role of decoding information aggregation is elaborated via statistics data to reveal that it can arrange more error-prone bits into the fore part of most reliable basis of order statistics decoding, which is vital for the effective decoding enhancement. We argue the superposition of improved bitwise reliability of the most reliable basis and the imposed rigorous code structure by OSD enables the proposed architecture being a competitive rival of the state of the art decoders, which was verified in extensive simulation in terms of performance, complexity and latency for short and moderate LDPC codes.","sentences":["Aiming at designing plausible decoders with channel information free, low complexity, high throughput, and approaching maximum likelihood performance, we put forward a streamlined architecture which concatenates sequentially three components.","Specifically, to tackle the decoding failures of normalized min-sum, the whole decoding trajectory, not limited to the last iteration information conventionally, is fed into a trained convolutional neural network to yield new reliability metric for each sequence bit, termed decoding information aggregation.","Then an adapted order statistics decoding, following the suggested decoding path, is adopted to process the sequence ordered with new metric more efficiently in that many invalid searches contained in conventional methods otherwise are evaded.","The role of decoding information aggregation is elaborated via statistics data to reveal that it can arrange more error-prone bits into the fore part of most reliable basis of order statistics decoding, which is vital for the effective decoding enhancement.","We argue the superposition of improved bitwise reliability of the most reliable basis and the imposed rigorous code structure by OSD enables the proposed architecture being a competitive rival of the state of the art decoders, which was verified in extensive simulation in terms of performance, complexity and latency for short and moderate LDPC codes."],"url":"http://arxiv.org/abs/2307.06575v1"}
{"created":"2023-07-13 05:57:48","title":"Unpacking polarization: Antagonism and Alignment in Signed Networks of Online Interaction","abstract":"Online polarization research currently focuses on studying single-issue opinion distributions or computing distance metrics of interaction network structures. Limited data availability often restricts studies to positive interaction data, which can misrepresent the reality of a discussion. We introduce a novel framework that aims at combining these three aspects, content and interactions, as well as their nature (positive or negative), while challenging the prevailing notion of polarization as an umbrella term for all forms of online conflict or opposing opinions. In our approach, built on the concepts of cleavage structures and structural balance of signed social networks, we factorize polarization into two distinct metrics: Antagonism and Alignment. Antagonism quantifies hostility in online discussions, based on the reactions of users to content. Alignment uses signed structural information encoded in long-term user-user relations on the platform to describe how well user interactions fit the global and/or traditional sides of discussion. We can analyse the change of these metrics through time, localizing both relevant trends but also sudden changes that can be mapped to specific contexts or events. We apply our methods to two distinct platforms: Birdwatch, a US crowd-based fact-checking extension of Twitter, and DerStandard, an Austrian online newspaper with discussion forums. In these two use cases, we find that our framework is capable of describing the global status of the groups of users (identification of cleavages) while also providing relevant findings on specific issues or in specific time frames. Furthermore, we show that our four metrics describe distinct phenomena, emphasizing their independent consideration for unpacking polarization complexities.","sentences":["Online polarization research currently focuses on studying single-issue opinion distributions or computing distance metrics of interaction network structures.","Limited data availability often restricts studies to positive interaction data, which can misrepresent the reality of a discussion.","We introduce a novel framework that aims at combining these three aspects, content and interactions, as well as their nature (positive or negative), while challenging the prevailing notion of polarization as an umbrella term for all forms of online conflict or opposing opinions.","In our approach, built on the concepts of cleavage structures and structural balance of signed social networks, we factorize polarization into two distinct metrics: Antagonism and Alignment.","Antagonism quantifies hostility in online discussions, based on the reactions of users to content.","Alignment uses signed structural information encoded in long-term user-user relations on the platform to describe how well user interactions fit the global and/or traditional sides of discussion.","We can analyse the change of these metrics through time, localizing both relevant trends but also sudden changes that can be mapped to specific contexts or events.","We apply our methods to two distinct platforms: Birdwatch, a US crowd-based fact-checking extension of Twitter, and DerStandard, an Austrian online newspaper with discussion forums.","In these two use cases, we find that our framework is capable of describing the global status of the groups of users (identification of cleavages) while also providing relevant findings on specific issues or in specific time frames.","Furthermore, we show that our four metrics describe distinct phenomena, emphasizing their independent consideration for unpacking polarization complexities."],"url":"http://arxiv.org/abs/2307.06571v1"}
{"created":"2023-07-13 05:54:05","title":"A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023","abstract":"In this technical report, we present our findings from a study conducted on the EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for Action Recognition. Our research focuses on the innovative application of a differentiable logic loss in the training to leverage the co-occurrence relations between verb and noun, as well as the pre-trained Large Language Models (LLMs) to generate the logic rules for the adaptation to unseen action labels. Specifically, the model's predictions are treated as the truth assignment of a co-occurrence logic formula to compute the logic loss, which measures the consistency between the predictions and the logic constraints. By using the verb-noun co-occurrence matrix generated from the dataset, we observe a moderate improvement in model performance compared to our baseline framework. To further enhance the model's adaptability to novel action labels, we experiment with rules generated using GPT-3.5, which leads to a slight decrease in performance. These findings shed light on the potential and challenges of incorporating differentiable logic and LLMs for knowledge extraction in unsupervised domain adaptation for action recognition. Our final submission (entitled `NS-LLM') achieved the first place in terms of top-1 action recognition accuracy.","sentences":["In this technical report, we present our findings from a study conducted on the EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for Action Recognition.","Our research focuses on the innovative application of a differentiable logic loss in the training to leverage the co-occurrence relations between verb and noun, as well as the pre-trained Large Language Models (LLMs) to generate the logic rules for the adaptation to unseen action labels.","Specifically, the model's predictions are treated as the truth assignment of a co-occurrence logic formula to compute the logic loss, which measures the consistency between the predictions and the logic constraints.","By using the verb-noun co-occurrence matrix generated from the dataset, we observe a moderate improvement in model performance compared to our baseline framework.","To further enhance the model's adaptability to novel action labels, we experiment with rules generated using GPT-3.5, which leads to a slight decrease in performance.","These findings shed light on the potential and challenges of incorporating differentiable logic and LLMs for knowledge extraction in unsupervised domain adaptation for action recognition.","Our final submission (entitled `NS-LLM') achieved the first place in terms of top-1 action recognition accuracy."],"url":"http://arxiv.org/abs/2307.06569v1"}
{"created":"2023-07-13 05:36:19","title":"Regression-Oriented Knowledge Distillation for Lightweight Ship Orientation Angle Prediction with Optical Remote Sensing Images","abstract":"Ship orientation angle prediction (SOAP) with optical remote sensing images is an important image processing task, which often relies on deep convolutional neural networks (CNNs) to make accurate predictions. This paper proposes a novel framework to reduce the model sizes and computational costs of SOAP models without harming prediction accuracy. First, a new SOAP model called Mobile-SOAP is designed based on MobileNetV2, achieving state-of-the-art prediction accuracy. Four tiny SOAP models are also created by replacing the convolutional blocks in Mobile-SOAP with four small-scale networks, respectively. Then, to transfer knowledge from Mobile-SOAP to four lightweight models, we propose a novel knowledge distillation (KD) framework termed SOAP-KD consisting of a novel feature-based guidance loss and an optimized synthetic samples-based knowledge transfer mechanism. Lastly, extensive experiments on the FGSC-23 dataset confirm the superiority of Mobile-SOAP over existing models and also demonstrate the effectiveness of SOAP-KD in improving the prediction performance of four specially designed tiny models. Notably, by using SOAP-KD, the test mean absolute error of the ShuffleNetV2x1.0-based model is only 8% higher than that of Mobile-SOAP, but its number of parameters and multiply-accumulate operations (MACs) are respectively 61.6% and 60.8% less.","sentences":["Ship orientation angle prediction (SOAP) with optical remote sensing images is an important image processing task, which often relies on deep convolutional neural networks (CNNs) to make accurate predictions.","This paper proposes a novel framework to reduce the model sizes and computational costs of SOAP models without harming prediction accuracy.","First, a new SOAP model called Mobile-SOAP is designed based on MobileNetV2, achieving state-of-the-art prediction accuracy.","Four tiny SOAP models are also created by replacing the convolutional blocks in Mobile-SOAP with four small-scale networks, respectively.","Then, to transfer knowledge from Mobile-SOAP to four lightweight models, we propose a novel knowledge distillation (KD) framework termed SOAP-KD consisting of a novel feature-based guidance loss and an optimized synthetic samples-based knowledge transfer mechanism.","Lastly, extensive experiments on the FGSC-23 dataset confirm the superiority of Mobile-SOAP over existing models and also demonstrate the effectiveness of SOAP-KD in improving the prediction performance of four specially designed tiny models.","Notably, by using SOAP-KD, the test mean absolute error of the ShuffleNetV2x1.0-based model is only 8% higher than that of Mobile-SOAP, but its number of parameters and multiply-accumulate operations (MACs) are respectively 61.6% and 60.8% less."],"url":"http://arxiv.org/abs/2307.06566v1"}
{"created":"2023-07-13 05:33:44","title":"Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification","abstract":"Deep learning has been widely used in many fields, but the model training process usually consumes massive computational resources and time. Therefore, designing an efficient neural network training method with a provable convergence guarantee is a fundamental and important research question. In this paper, we present a static half-space report data structure that consists of a fully connected two-layer neural network for shifted ReLU activation to enable activated neuron identification in sublinear time via geometric search. We also prove that our algorithm can converge in $O(M^2/\\epsilon^2)$ time with network size quadratic in the coefficient norm upper bound $M$ and error term $\\epsilon$.","sentences":["Deep learning has been widely used in many fields, but the model training process usually consumes massive computational resources and time.","Therefore, designing an efficient neural network training method with a provable convergence guarantee is a fundamental and important research question.","In this paper, we present a static half-space report data structure that consists of a fully connected two-layer neural network for shifted ReLU activation to enable activated neuron identification in sublinear time via geometric search.","We also prove that our algorithm can converge in $O(M^2/\\epsilon^2)$ time with network size quadratic in the coefficient norm upper bound $M$ and error term $\\epsilon$."],"url":"http://arxiv.org/abs/2307.06565v1"}
{"created":"2023-07-13 05:31:40","title":"Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach","abstract":"Prescriptive process monitoring methods seek to optimize the performance of business processes by triggering interventions at runtime, thereby increasing the probability of positive case outcomes. These interventions are triggered according to an intervention policy. Reinforcement learning has been put forward as an approach to learning intervention policies through trial and error. Existing approaches in this space assume that the number of resources available to perform interventions in a process is unlimited, an unrealistic assumption in practice. This paper argues that, in the presence of resource constraints, a key dilemma in the field of prescriptive process monitoring is to trigger interventions based not only on predictions of their necessity, timeliness, or effect but also on the uncertainty of these predictions and the level of resource utilization. Indeed, committing scarce resources to an intervention when the necessity or effects of this intervention are highly uncertain may intuitively lead to suboptimal intervention effects. Accordingly, the paper proposes a reinforcement learning approach for prescriptive process monitoring that leverages conformal prediction techniques to consider the uncertainty of the predictions upon which an intervention decision is based. An evaluation using real-life datasets demonstrates that explicitly modeling uncertainty using conformal predictions helps reinforcement learning agents converge towards policies with higher net intervention gain","sentences":["Prescriptive process monitoring methods seek to optimize the performance of business processes by triggering interventions at runtime, thereby increasing the probability of positive case outcomes.","These interventions are triggered according to an intervention policy.","Reinforcement learning has been put forward as an approach to learning intervention policies through trial and error.","Existing approaches in this space assume that the number of resources available to perform interventions in a process is unlimited, an unrealistic assumption in practice.","This paper argues that, in the presence of resource constraints, a key dilemma in the field of prescriptive process monitoring is to trigger interventions based not only on predictions of their necessity, timeliness, or effect but also on the uncertainty of these predictions and the level of resource utilization.","Indeed, committing scarce resources to an intervention when the necessity or effects of this intervention are highly uncertain may intuitively lead to suboptimal intervention effects.","Accordingly, the paper proposes a reinforcement learning approach for prescriptive process monitoring that leverages conformal prediction techniques to consider the uncertainty of the predictions upon which an intervention decision is based.","An evaluation using real-life datasets demonstrates that explicitly modeling uncertainty using conformal predictions helps reinforcement learning agents converge towards policies with higher net intervention gain"],"url":"http://arxiv.org/abs/2307.06564v1"}
{"created":"2023-07-13 05:30:41","title":"Money: Who Has a Stake in the Most Value-Centric Common Design Material?","abstract":"Money is more than just a numeric value. It embodies trust and moral gravity, and it offers flexible ways to transact. However, the emergence of Central Bank Digital Currency (CBDC) is set to bring about a drastic change in the future of money. This paper invites designers to reflect on their role in shaping material and immaterial monetary change. In this rapidly changing landscape, design could be instrumental in uncovering and showcasing the diverse values that money holds for different stakeholders. Understanding these diversities could promote a more equitable and inclusive financial, social, and global landscape within emergent forms of cash-like digital currency. Without such consideration, certain forms of money we have come to know could disappear, along with the values people hold upon them. We report on semi-structured interviews with stakeholders who have current knowledge or involvement in the emerging field of Central Bank Digital Currency (CBDC). Our research indicates that this new form of money presents both challenges and opportunities for designers. Specifically, we emphasise the potential for Central Bank Digital Currency (CBDC) to either positively or negatively reform values through its design. By considering time, reflecting present values, and promoting inclusion in its deployment, we can strive to ensure that Central Bank Digital Currency (CBDC) represents the diverse needs and perspectives of its users.","sentences":["Money is more than just a numeric value.","It embodies trust and moral gravity, and it offers flexible ways to transact.","However, the emergence of Central Bank Digital Currency (CBDC) is set to bring about a drastic change in the future of money.","This paper invites designers to reflect on their role in shaping material and immaterial monetary change.","In this rapidly changing landscape, design could be instrumental in uncovering and showcasing the diverse values that money holds for different stakeholders.","Understanding these diversities could promote a more equitable and inclusive financial, social, and global landscape within emergent forms of cash-like digital currency.","Without such consideration, certain forms of money we have come to know could disappear, along with the values people hold upon them.","We report on semi-structured interviews with stakeholders who have current knowledge or involvement in the emerging field of Central Bank Digital Currency (CBDC).","Our research indicates that this new form of money presents both challenges and opportunities for designers.","Specifically, we emphasise the potential for Central Bank Digital Currency (CBDC) to either positively or negatively reform values through its design.","By considering time, reflecting present values, and promoting inclusion in its deployment, we can strive to ensure that Central Bank Digital Currency (CBDC) represents the diverse needs and perspectives of its users."],"url":"http://arxiv.org/abs/2307.06563v1"}
{"created":"2023-07-13 05:30:36","title":"Investigating Normalization in Preference-based Evolutionary Multi-objective Optimization Using a Reference Point","abstract":"Normalization of objectives plays a crucial role in evolutionary multi-objective optimization (EMO) to handle objective functions with different scales, which can be found in real-world problems. Although the effect of normalization methods on the performance of EMO algorithms has been investigated in the literature, that of preference-based EMO (PBEMO) algorithms is poorly understood. Since PBEMO aims to approximate a region of interest, its population generally does not cover the Pareto front in the objective space. This property may make normalization of objectives in PBEMO difficult. This paper investigates the effectiveness of three normalization methods in three representative PBEMO algorithms. We present a bounded archive-based method for approximating the nadir point. First, we demonstrate that the normalization methods in PBEMO perform significantly worse than that in conventional EMO in terms of approximating the ideal point, nadir point, and range of the PF. Then, we show that PBEMO requires normalization of objectives on problems with differently scaled objectives. Our results show that there is no clear \"best normalization method\" in PBEMO, but an external archive-based method performs relatively well.","sentences":["Normalization of objectives plays a crucial role in evolutionary multi-objective optimization (EMO) to handle objective functions with different scales, which can be found in real-world problems.","Although the effect of normalization methods on the performance of EMO algorithms has been investigated in the literature, that of preference-based EMO (PBEMO) algorithms is poorly understood.","Since PBEMO aims to approximate a region of interest, its population generally does not cover the Pareto front in the objective space.","This property may make normalization of objectives in PBEMO difficult.","This paper investigates the effectiveness of three normalization methods in three representative PBEMO algorithms.","We present a bounded archive-based method for approximating the nadir point.","First, we demonstrate that the normalization methods in PBEMO perform significantly worse than that in conventional EMO in terms of approximating the ideal point, nadir point, and range of the PF.","Then, we show that PBEMO requires normalization of objectives on problems with differently scaled objectives.","Our results show that there is no clear \"best normalization method\" in PBEMO, but an external archive-based method performs relatively well."],"url":"http://arxiv.org/abs/2307.06562v1"}
{"created":"2023-07-13 05:26:31","title":"A Case for Offloading Federated Learning Server on Smart NIC","abstract":"Federated learning is a distributed machine learning approach where local weight parameters trained by clients locally are aggregated as global parameters by a server. The global parameters can be trained without uploading privacy-sensitive raw data owned by clients to the server. The aggregation on the server is simply done by averaging the local weight parameters, so it is an I/O intensive task where a network processing accounts for a large portion compared to the computation. The network processing workload further increases as the number of clients increases. To mitigate the network processing workload, in this paper, the federated learning server is offloaded to NVIDIA BlueField-2 DPU which is a smart NIC (Network Interface Card) that has eight processing cores. Dedicated processing cores are assigned by DPDK (Data Plane Development Kit) for receiving the local weight parameters and sending the global parameters. The aggregation task is parallelized by exploiting multiple cores available on the DPU. To further improve the performance, an approximated design that eliminates an exclusive access control between the computation threads is also implemented. Evaluation results show that the federated learning server on the DPU accelerates the execution time by 1.32 times compared with that on the host CPU with a negligible accuracy loss.","sentences":["Federated learning is a distributed machine learning approach where local weight parameters trained by clients locally are aggregated as global parameters by a server.","The global parameters can be trained without uploading privacy-sensitive raw data owned by clients to the server.","The aggregation on the server is simply done by averaging the local weight parameters, so it is an I/O intensive task where a network processing accounts for a large portion compared to the computation.","The network processing workload further increases as the number of clients increases.","To mitigate the network processing workload, in this paper, the federated learning server is offloaded to NVIDIA BlueField-2 DPU which is a smart NIC (Network Interface Card) that has eight processing cores.","Dedicated processing cores are assigned by DPDK (Data Plane Development Kit) for receiving the local weight parameters and sending the global parameters.","The aggregation task is parallelized by exploiting multiple cores available on the DPU.","To further improve the performance, an approximated design that eliminates an exclusive access control between the computation threads is also implemented.","Evaluation results show that the federated learning server on the DPU accelerates the execution time by 1.32 times compared with that on the host CPU with a negligible accuracy loss."],"url":"http://arxiv.org/abs/2307.06561v1"}
{"created":"2023-07-13 04:46:05","title":"Deep Network Approximation: Beyond ReLU to Diverse Activation Functions","abstract":"This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\\mathtt{ReLU}$, $\\mathtt{LeakyReLU}$, $\\mathtt{ReLU}^2$, $\\mathtt{ELU}$, $\\mathtt{SELU}$, $\\mathtt{Softplus}$, $\\mathtt{GELU}$, $\\mathtt{SiLU}$, $\\mathtt{Swish}$, $\\mathtt{Mish}$, $\\mathtt{Sigmoid}$, $\\mathtt{Tanh}$, $\\mathtt{Arctan}$, $\\mathtt{Softsign}$, $\\mathtt{dSiLU}$, and $\\mathtt{SRS}$. We demonstrate that for any activation function $\\varrho\\in \\mathscr{A}$, a $\\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.","sentences":["This paper explores the expressive power of deep neural networks for a diverse range of activation functions.","An activation function set $\\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\\mathtt{ReLU}$, $\\mathtt{LeakyReLU}$, $\\mathtt{ReLU}^2$, $\\mathtt{ELU}$, $\\mathtt{SELU}$, $\\mathtt{Softplus}$, $\\mathtt{GELU}$, $\\mathtt{SiLU}$, $\\mathtt{Swish}$, $\\mathtt{Mish}$, $\\mathtt{Sigmoid}$, $\\mathtt{Tanh}$, $\\mathtt{Arctan}$, $\\mathtt{Softsign}$, $\\mathtt{dSiLU}$, and $\\mathtt{SRS}$. We demonstrate that for any activation function $\\varrho\\in \\mathscr{A}$, a $\\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set.","This finding enables the extension of most approximation results achieved with $\\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants."],"url":"http://arxiv.org/abs/2307.06555v1"}
{"created":"2023-07-13 04:38:32","title":"TPU as Cryptographic Accelerator","abstract":"Polynomials defined on specific rings are heavily involved in various cryptographic schemes, and the corresponding operations are usually the computation bottleneck of the whole scheme.   We propose to utilize TPU, an emerging hardware designed for AI applications, to speed up polynomial operations and convert TPU to a cryptographic accelerator.   We also conduct preliminary evaluation and discuss the limitations of current work and future plan.","sentences":["Polynomials defined on specific rings are heavily involved in various cryptographic schemes, and the corresponding operations are usually the computation bottleneck of the whole scheme.   ","We propose to utilize TPU, an emerging hardware designed for AI applications, to speed up polynomial operations and convert TPU to a cryptographic accelerator.   ","We also conduct preliminary evaluation and discuss the limitations of current work and future plan."],"url":"http://arxiv.org/abs/2307.06554v1"}
{"created":"2023-07-13 04:18:20","title":"What Exactly is an Insight? A Literature Review","abstract":"Insights are often considered the ideal outcome of visual analysis sessions. However, there is no single definition of what an insight is. Some scholars define insights as correlations, while others define them as hypotheses or aha moments. This lack of a clear definition can make it difficult to build visualization tools that effectively support insight discovery. In this paper, we contribute a comprehensive literature review that maps the landscape of existing insight definitions. We summarize key themes regarding how insight is defined, with the goal of helping readers identify which definitions of insight align closely with their research and tool development goals. Based on our review, we also suggest interesting research directions, such as synthesizing a unified formalism for insight and connecting theories of insight to other critical concepts in visualization research.","sentences":["Insights are often considered the ideal outcome of visual analysis sessions.","However, there is no single definition of what an insight is.","Some scholars define insights as correlations, while others define them as hypotheses or aha moments.","This lack of a clear definition can make it difficult to build visualization tools that effectively support insight discovery.","In this paper, we contribute a comprehensive literature review that maps the landscape of existing insight definitions.","We summarize key themes regarding how insight is defined, with the goal of helping readers identify which definitions of insight align closely with their research and tool development goals.","Based on our review, we also suggest interesting research directions, such as synthesizing a unified formalism for insight and connecting theories of insight to other critical concepts in visualization research."],"url":"http://arxiv.org/abs/2307.06551v1"}
{"created":"2023-07-13 04:08:16","title":"Multi-objective Evolutionary Search of Variable-length Composite Semantic Perturbations","abstract":"Deep neural networks have proven to be vulnerable to adversarial attacks in the form of adding specific perturbations on images to make wrong outputs. Designing stronger adversarial attack methods can help more reliably evaluate the robustness of DNN models. To release the harbor burden and improve the attack performance, auto machine learning (AutoML) has recently emerged as one successful technique to help automatically find the near-optimal adversarial attack strategy. However, existing works about AutoML for adversarial attacks only focus on $L_{\\infty}$-norm-based perturbations. In fact, semantic perturbations attract increasing attention due to their naturalnesses and physical realizability. To bridge the gap between AutoML and semantic adversarial attacks, we propose a novel method called multi-objective evolutionary search of variable-length composite semantic perturbations (MES-VCSP). Specifically, we construct the mathematical model of variable-length composite semantic perturbations, which provides five gradient-based semantic attack methods. The same type of perturbation in an attack sequence is allowed to be performed multiple times. Besides, we introduce the multi-objective evolutionary search consisting of NSGA-II and neighborhood search to find near-optimal variable-length attack sequences. Experimental results on CIFAR10 and ImageNet datasets show that compared with existing methods, MES-VCSP can obtain adversarial examples with a higher attack success rate, more naturalness, and less time cost.","sentences":["Deep neural networks have proven to be vulnerable to adversarial attacks in the form of adding specific perturbations on images to make wrong outputs.","Designing stronger adversarial attack methods can help more reliably evaluate the robustness of DNN models.","To release the harbor burden and improve the attack performance, auto machine learning (AutoML) has recently emerged as one successful technique to help automatically find the near-optimal adversarial attack strategy.","However, existing works about AutoML for adversarial attacks only focus on $L_{\\infty}$-norm-based perturbations.","In fact, semantic perturbations attract increasing attention due to their naturalnesses and physical realizability.","To bridge the gap between AutoML and semantic adversarial attacks, we propose a novel method called multi-objective evolutionary search of variable-length composite semantic perturbations (MES-VCSP).","Specifically, we construct the mathematical model of variable-length composite semantic perturbations, which provides five gradient-based semantic attack methods.","The same type of perturbation in an attack sequence is allowed to be performed multiple times.","Besides, we introduce the multi-objective evolutionary search consisting of NSGA-II and neighborhood search to find near-optimal variable-length attack sequences.","Experimental results on CIFAR10","and ImageNet datasets show that compared with existing methods, MES-VCSP can obtain adversarial examples with a higher attack success rate, more naturalness, and less time cost."],"url":"http://arxiv.org/abs/2307.06548v1"}
{"created":"2023-07-13 03:06:36","title":"On the Effective Horizon of Inverse Reinforcement Learning","abstract":"Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning over a given time horizon to compute an approximately optimal policy for a hypothesized reward function and then match this policy with expert demonstrations. The time horizon plays a critical role in determining both the accuracy of reward estimate and the computational efficiency of IRL algorithms. Interestingly, an effective time horizon shorter than the ground-truth value often produces better results faster. This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data. This analysis leads to a principled choice of the effective horizon for IRL. It also prompts us to reexamine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon together rather than the reward alone with a given horizon. Our experimental results confirm the theoretical analysis.","sentences":["Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning over a given time horizon to compute an approximately optimal policy for a hypothesized reward function and then match this policy with expert demonstrations.","The time horizon plays a critical role in determining both the accuracy of reward estimate and the computational efficiency of IRL algorithms.","Interestingly, an effective time horizon shorter than the ground-truth value often produces better results faster.","This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data.","This analysis leads to a principled choice of the effective horizon for IRL.","It also prompts us to reexamine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon together rather than the reward alone with a given horizon.","Our experimental results confirm the theoretical analysis."],"url":"http://arxiv.org/abs/2307.06541v1"}
{"created":"2023-07-13 03:02:56","title":"Convolutional Neural Networks for Sentiment Analysis on Weibo Data: A Natural Language Processing Approach","abstract":"This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a Convolutional Neural Network (CNN), offering a new approach to Natural Language Processing (NLP). The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized based on sentiment labels. A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment classification. The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across positive, neutral, and negative sentiments. The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applications in social media analysis, market research, and policy studies. The complete experimental content and code have been made publicly available on the Kaggle data platform for further research and development. Future work may involve exploring different architectures, such as Recurrent Neural Networks (RNN) or transformers, or using more complex pre-trained models like BERT, to further improve the model's ability to understand linguistic nuances and context.","sentences":["This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a Convolutional Neural Network (CNN), offering a new approach to Natural Language Processing (NLP).","The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized based on sentiment labels.","A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment classification.","The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across positive, neutral, and negative sentiments.","The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applications in social media analysis, market research, and policy studies.","The complete experimental content and code have been made publicly available on the Kaggle data platform for further research and development.","Future work may involve exploring different architectures, such as Recurrent Neural Networks (RNN) or transformers, or using more complex pre-trained models like BERT, to further improve the model's ability to understand linguistic nuances and context."],"url":"http://arxiv.org/abs/2307.06540v1"}
{"created":"2023-07-13 03:00:01","title":"Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems","abstract":"Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems. While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data. In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions. As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories. Moreover our algorithm works in the challenging partially-observed setting. Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a close relative of modern tensor decomposition methods for learning latent variable models. This gives us a playbook for how to extend it to work with more complicated generative models.","sentences":["Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems.","While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data.","In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions.","As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories.","Moreover our algorithm works in the challenging partially-observed setting.","Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a close relative of modern tensor decomposition methods for learning latent variable models.","This gives us a playbook for how to extend it to work with more complicated generative models."],"url":"http://arxiv.org/abs/2307.06538v1"}
{"created":"2023-07-13 02:47:27","title":"Faster Rectangular Matrix Multiplication by Combination Loss Analysis","abstract":"Duan, Wu and Zhou (FOCS 2023) recently obtained the improved upper bound on the exponent of square matrix multiplication $\\omega<2.3719$ by introducing a new approach to quantify and compensate the ``combination loss\" in prior analyses of powers of the Coppersmith-Winograd tensor. In this paper we show how to use this new approach to improve the exponent of rectangular matrix multiplication as well. Our main technical contribution is showing how to combine this analysis of the combination loss and the analysis of the fourth power of the Coppersmith-Winograd tensor in the context of rectangular matrix multiplication developed by Le Gall and Urrutia (SODA 2018).","sentences":["Duan, Wu and Zhou (FOCS 2023) recently obtained the improved upper bound on the exponent of square matrix multiplication $\\omega<2.3719$ by introducing a new approach to quantify and compensate the ``combination loss\" in prior analyses of powers of the Coppersmith-Winograd tensor.","In this paper we show how to use this new approach to improve the exponent of rectangular matrix multiplication as well.","Our main technical contribution is showing how to combine this analysis of the combination loss and the analysis of the fourth power of the Coppersmith-Winograd tensor in the context of rectangular matrix multiplication developed by Le Gall and Urrutia (SODA 2018)."],"url":"http://arxiv.org/abs/2307.06535v1"}
{"created":"2023-07-13 02:45:29","title":"DSV: An Alignment Validation Loss for Self-supervised Outlier Model Selection","abstract":"Self-supervised learning (SSL) has proven effective in solving various problems by generating internal supervisory signals. Unsupervised anomaly detection, which faces the high cost of obtaining true labels, is an area that can greatly benefit from SSL. However, recent literature suggests that tuning the hyperparameters (HP) of data augmentation functions is crucial to the success of SSL-based anomaly detection (SSAD), yet a systematic method for doing so remains unknown. In this work, we propose DSV (Discordance and Separability Validation), an unsupervised validation loss to select high-performing detection models with effective augmentation HPs. DSV captures the alignment between an augmentation function and the anomaly-generating mechanism with surrogate losses, which approximate the discordance and separability of test data, respectively. As a result, the evaluation via DSV leads to selecting an effective SSAD model exhibiting better alignment, which results in high detection accuracy. We theoretically derive the degree of approximation conducted by the surrogate losses and empirically show that DSV outperforms a wide range of baselines on 21 real-world tasks.","sentences":["Self-supervised learning (SSL) has proven effective in solving various problems by generating internal supervisory signals.","Unsupervised anomaly detection, which faces the high cost of obtaining true labels, is an area that can greatly benefit from SSL.","However, recent literature suggests that tuning the hyperparameters (HP) of data augmentation functions is crucial to the success of SSL-based anomaly detection (SSAD), yet a systematic method for doing so remains unknown.","In this work, we propose DSV (Discordance and Separability Validation), an unsupervised validation loss to select high-performing detection models with effective augmentation HPs.","DSV captures the alignment between an augmentation function and the anomaly-generating mechanism with surrogate losses, which approximate the discordance and separability of test data, respectively.","As a result, the evaluation via DSV leads to selecting an effective SSAD model exhibiting better alignment, which results in high detection accuracy.","We theoretically derive the degree of approximation conducted by the surrogate losses and empirically show that DSV outperforms a wide range of baselines on 21 real-world tasks."],"url":"http://arxiv.org/abs/2307.06534v1"}
{"created":"2023-07-13 02:42:28","title":"Domain-adaptive Person Re-identification without Cross-camera Paired Samples","abstract":"Existing person re-identification (re-ID) research mainly focuses on pedestrian identity matching across cameras in adjacent areas. However, in reality, it is inevitable to face the problem of pedestrian identity matching across long-distance scenes. The cross-camera pedestrian samples collected from long-distance scenes often have no positive samples. It is extremely challenging to use cross-camera negative samples to achieve cross-region pedestrian identity matching. Therefore, a novel domain-adaptive person re-ID method that focuses on cross-camera consistent discriminative feature learning under the supervision of unpaired samples is proposed. This method mainly includes category synergy co-promotion module (CSCM) and cross-camera consistent feature learning module (CCFLM). In CSCM, a task-specific feature recombination (FRT) mechanism is proposed. This mechanism first groups features according to their contributions to specific tasks. Then an interactive promotion learning (IPL) scheme between feature groups is developed and embedded in this mechanism to enhance feature discriminability. Since the control parameters of the specific task model are reduced after division by task, the generalization ability of the model is improved. In CCFLM, instance-level feature distribution alignment and cross-camera identity consistent learning methods are constructed. Therefore, the supervised model training is achieved under the style supervision of the target domain by exchanging styles between source-domain samples and target-domain samples, and the challenges caused by the lack of cross-camera paired samples are solved by utilizing cross-camera similar samples. In experiments, three challenging datasets are used as target domains, and the effectiveness of the proposed method is demonstrated through four experimental settings.","sentences":["Existing person re-identification (re-ID) research mainly focuses on pedestrian identity matching across cameras in adjacent areas.","However, in reality, it is inevitable to face the problem of pedestrian identity matching across long-distance scenes.","The cross-camera pedestrian samples collected from long-distance scenes often have no positive samples.","It is extremely challenging to use cross-camera negative samples to achieve cross-region pedestrian identity matching.","Therefore, a novel domain-adaptive person re-ID method that focuses on cross-camera consistent discriminative feature learning under the supervision of unpaired samples is proposed.","This method mainly includes category synergy co-promotion module (CSCM) and cross-camera consistent feature learning module (CCFLM).","In CSCM, a task-specific feature recombination (FRT) mechanism is proposed.","This mechanism first groups features according to their contributions to specific tasks.","Then an interactive promotion learning (IPL) scheme between feature groups is developed and embedded in this mechanism to enhance feature discriminability.","Since the control parameters of the specific task model are reduced after division by task, the generalization ability of the model is improved.","In CCFLM, instance-level feature distribution alignment and cross-camera identity consistent learning methods are constructed.","Therefore, the supervised model training is achieved under the style supervision of the target domain by exchanging styles between source-domain samples and target-domain samples, and the challenges caused by the lack of cross-camera paired samples are solved by utilizing cross-camera similar samples.","In experiments, three challenging datasets are used as target domains, and the effectiveness of the proposed method is demonstrated through four experimental settings."],"url":"http://arxiv.org/abs/2307.06533v1"}
{"created":"2023-07-13 02:31:55","title":"Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study","abstract":"This paper explores the integration of Large Language Models (LLMs) into Automatic Speech Recognition (ASR) systems to improve transcription accuracy. The increasing sophistication of LLMs, with their in-context learning capabilities and instruction-following behavior, has drawn significant attention in the field of Natural Language Processing (NLP). Our primary focus is to investigate the potential of using an LLM's in-context learning capabilities to enhance the performance of ASR systems, which currently face challenges such as ambient noise, speaker accents, and complex linguistic contexts. We designed a study using the Aishell-1 and LibriSpeech datasets, with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities. Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications. Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the limitations of LLMs in speech applications. This paper provides a detailed overview of these experiments, their results, and implications, establishing that using LLMs' in-context learning capabilities to correct potential errors in speech recognition transcriptions is still a challenging task at the current stage.","sentences":["This paper explores the integration of Large Language Models (LLMs) into Automatic Speech Recognition (ASR) systems to improve transcription accuracy.","The increasing sophistication of LLMs, with their in-context learning capabilities and instruction-following behavior, has drawn significant attention in the field of Natural Language Processing (NLP).","Our primary focus is to investigate the potential of using an LLM's in-context learning capabilities to enhance the performance of ASR systems, which currently face challenges such as ambient noise, speaker accents, and complex linguistic contexts.","We designed a study using the Aishell-1 and LibriSpeech datasets, with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.","Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications.","Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the limitations of LLMs in speech applications.","This paper provides a detailed overview of these experiments, their results, and implications, establishing that using LLMs' in-context learning capabilities to correct potential errors in speech recognition transcriptions is still a challenging task at the current stage."],"url":"http://arxiv.org/abs/2307.06530v1"}
{"created":"2023-07-13 02:31:06","title":"Optimised Least Squares Approach for Accurate Rectangle Fitting","abstract":"This study introduces a novel and efficient least squares based method for rectangle fitting, using a continuous fitness function that approximates a unit square accurately. The proposed method is compared with the existing method in the literature using both simulated data and real data. The real data is derived from aerial photogrammetry point clouds of a rectangular building. The simulated tests show that the proposed method performs better than the reference method, reducing the root-mean-square error by about 93% and 14% for clean datasets and noisy point clouds, respectively. The proposed method also improves the fitting of the real dataset by about 81%, achieving centimetre level accuracy. Furthermore, the test results show that the proposed method converges in fewer than 10 iterations.","sentences":["This study introduces a novel and efficient least squares based method for rectangle fitting, using a continuous fitness function that approximates a unit square accurately.","The proposed method is compared with the existing method in the literature using both simulated data and real data.","The real data is derived from aerial photogrammetry point clouds of a rectangular building.","The simulated tests show that the proposed method performs better than the reference method, reducing the root-mean-square error by about 93% and 14% for clean datasets and noisy point clouds, respectively.","The proposed method also improves the fitting of the real dataset by about 81%, achieving centimetre level accuracy.","Furthermore, the test results show that the proposed method converges in fewer than 10 iterations."],"url":"http://arxiv.org/abs/2307.06528v1"}
{"created":"2023-07-13 02:22:09","title":"Free-Form Composition Networks for Egocentric Action Recognition","abstract":"Egocentric action recognition is gaining significant attention in the field of human action recognition. In this paper, we address data scarcity issue in egocentric action recognition from a compositional generalization perspective. To tackle this problem, we propose a free-form composition network (FFCN) that can simultaneously learn disentangled verb, preposition, and noun representations, and then use them to compose new samples in the feature space for rare classes of action videos. First, we use a graph to capture the spatial-temporal relations among different hand/object instances in each action video. We thus decompose each action into a set of verb and preposition spatial-temporal representations using the edge features in the graph. The temporal decomposition extracts verb and preposition representations from different video frames, while the spatial decomposition adaptively learns verb and preposition representations from action-related instances in each frame. With these spatial-temporal representations of verbs and prepositions, we can compose new samples for those rare classes in a free-form manner, which is not restricted to a rigid form of a verb and a noun. The proposed FFCN can directly generate new training data samples for rare classes, hence significantly improve action recognition performance. We evaluated our method on three popular egocentric action recognition datasets, Something-Something V2, H2O, and EPIC-KITCHENS-100, and the experimental results demonstrate the effectiveness of the proposed method for handling data scarcity problems, including long-tailed and few-shot egocentric action recognition.","sentences":["Egocentric action recognition is gaining significant attention in the field of human action recognition.","In this paper, we address data scarcity issue in egocentric action recognition from a compositional generalization perspective.","To tackle this problem, we propose a free-form composition network (FFCN) that can simultaneously learn disentangled verb, preposition, and noun representations, and then use them to compose new samples in the feature space for rare classes of action videos.","First, we use a graph to capture the spatial-temporal relations among different hand/object instances in each action video.","We thus decompose each action into a set of verb and preposition spatial-temporal representations using the edge features in the graph.","The temporal decomposition extracts verb and preposition representations from different video frames, while the spatial decomposition adaptively learns verb and preposition representations from action-related instances in each frame.","With these spatial-temporal representations of verbs and prepositions, we can compose new samples for those rare classes in a free-form manner, which is not restricted to a rigid form of a verb and a noun.","The proposed FFCN can directly generate new training data samples for rare classes, hence significantly improve action recognition performance.","We evaluated our method on three popular egocentric action recognition datasets, Something-Something V2, H2O, and EPIC-KITCHENS-100, and the experimental results demonstrate the effectiveness of the proposed method for handling data scarcity problems, including long-tailed and few-shot egocentric action recognition."],"url":"http://arxiv.org/abs/2307.06527v1"}
{"created":"2023-07-13 02:19:56","title":"AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion","abstract":"Large-scale pre-trained vision-language models allow for the zero-shot text-based generation of 3D avatars. The previous state-of-the-art method utilized CLIP to supervise neural implicit models that reconstructed a human body mesh. However, this approach has two limitations. Firstly, the lack of avatar-specific models can cause facial distortion and unrealistic clothing in the generated avatars. Secondly, CLIP only provides optimization direction for the overall appearance, resulting in less impressive results. To address these limitations, we propose AvatarFusion, the first framework to use a latent diffusion model to provide pixel-level guidance for generating human-realistic avatars while simultaneously segmenting clothing from the avatar's body. AvatarFusion includes the first clothing-decoupled neural implicit avatar model that employs a novel Dual Volume Rendering strategy to render the decoupled skin and clothing sub-models in one space. We also introduce a novel optimization method, called Pixel-Semantics Difference-Sampling (PS-DS), which semantically separates the generation of body and clothes, and generates a variety of clothing styles. Moreover, we establish the first benchmark for zero-shot text-to-avatar generation. Our experimental results demonstrate that our framework outperforms previous approaches, with significant improvements observed in all metrics. Additionally, since our model is clothing-decoupled, we can exchange the clothes of avatars. Code will be available on Github.","sentences":["Large-scale pre-trained vision-language models allow for the zero-shot text-based generation of 3D avatars.","The previous state-of-the-art method utilized CLIP to supervise neural implicit models that reconstructed a human body mesh.","However, this approach has two limitations.","Firstly, the lack of avatar-specific models can cause facial distortion and unrealistic clothing in the generated avatars.","Secondly, CLIP only provides optimization direction for the overall appearance, resulting in less impressive results.","To address these limitations, we propose AvatarFusion, the first framework to use a latent diffusion model to provide pixel-level guidance for generating human-realistic avatars while simultaneously segmenting clothing from the avatar's body.","AvatarFusion includes the first clothing-decoupled neural implicit avatar model that employs a novel Dual Volume Rendering strategy to render the decoupled skin and clothing sub-models in one space.","We also introduce a novel optimization method, called Pixel-Semantics Difference-Sampling (PS-DS), which semantically separates the generation of body and clothes, and generates a variety of clothing styles.","Moreover, we establish the first benchmark for zero-shot text-to-avatar generation.","Our experimental results demonstrate that our framework outperforms previous approaches, with significant improvements observed in all metrics.","Additionally, since our model is clothing-decoupled, we can exchange the clothes of avatars.","Code will be available on Github."],"url":"http://arxiv.org/abs/2307.06526v1"}
{"created":"2023-07-13 02:00:27","title":"Agreement Tracking for Multi-Issue Negotiation Dialogues","abstract":"Automated negotiation support systems aim to help human negotiators reach more favorable outcomes in multi-issue negotiations (e.g., an employer and a candidate negotiating over issues such as salary, hours, and promotions before a job offer). To be successful, these systems must accurately track agreements reached by participants in real-time. Existing approaches either focus on task-oriented dialogues or produce unstructured outputs, rendering them unsuitable for this objective. Our work introduces the novel task of agreement tracking for two-party multi-issue negotiations, which requires continuous monitoring of agreements within a structured state space. To address the scarcity of annotated corpora with realistic multi-issue negotiation dialogues, we use GPT-3 to build GPT-Negochat, a synthesized dataset that we make publicly available. We present a strong initial baseline for our task by transfer-learning a T5 model trained on the MultiWOZ 2.4 corpus. Pre-training T5-small and T5-base on MultiWOZ 2.4's DST task enhances results by 21% and 9% respectively over training solely on GPT-Negochat. We validate our method's sample-efficiency via smaller training subset experiments. By releasing GPT-Negochat and our baseline models, we aim to encourage further research in multi-issue negotiation dialogue agreement tracking.","sentences":["Automated negotiation support systems aim to help human negotiators reach more favorable outcomes in multi-issue negotiations (e.g., an employer and a candidate negotiating over issues such as salary, hours, and promotions before a job offer).","To be successful, these systems must accurately track agreements reached by participants in real-time.","Existing approaches either focus on task-oriented dialogues or produce unstructured outputs, rendering them unsuitable for this objective.","Our work introduces the novel task of agreement tracking for two-party multi-issue negotiations, which requires continuous monitoring of agreements within a structured state space.","To address the scarcity of annotated corpora with realistic multi-issue negotiation dialogues, we use GPT-3 to build GPT-Negochat, a synthesized dataset that we make publicly available.","We present a strong initial baseline for our task by transfer-learning a T5 model trained on the MultiWOZ 2.4 corpus.","Pre-training T5-small and T5-base on MultiWOZ 2.4's DST task enhances results by 21% and 9% respectively over training solely on GPT-Negochat.","We validate our method's sample-efficiency via smaller training subset experiments.","By releasing GPT-Negochat and our baseline models, we aim to encourage further research in multi-issue negotiation dialogue agreement tracking."],"url":"http://arxiv.org/abs/2307.06524v1"}
{"created":"2023-07-13 01:51:26","title":"Artificial Intelligence for Drug Discovery: Are We There Yet?","abstract":"Drug discovery is adapting to novel technologies such as data science, informatics, and artificial intelligence (AI) to accelerate effective treatment development while reducing costs and animal experiments. AI is transforming drug discovery, as indicated by increasing interest from investors, industrial and academic scientists, and legislators. Successful drug discovery requires optimizing properties related to pharmacodynamics, pharmacokinetics, and clinical outcomes. This review discusses the use of AI in the three pillars of drug discovery: diseases, targets, and therapeutic modalities, with a focus on small molecule drugs. AI technologies, such as generative chemistry, machine learning, and multi-property optimization, have enabled several compounds to enter clinical trials. The scientific community must carefully vet known information to address the reproducibility crisis. The full potential of AI in drug discovery can only be realized with sufficient ground truth and appropriate human intervention at later pipeline stages.","sentences":["Drug discovery is adapting to novel technologies such as data science, informatics, and artificial intelligence (AI) to accelerate effective treatment development while reducing costs and animal experiments.","AI is transforming drug discovery, as indicated by increasing interest from investors, industrial and academic scientists, and legislators.","Successful drug discovery requires optimizing properties related to pharmacodynamics, pharmacokinetics, and clinical outcomes.","This review discusses the use of AI in the three pillars of drug discovery: diseases, targets, and therapeutic modalities, with a focus on small molecule drugs.","AI technologies, such as generative chemistry, machine learning, and multi-property optimization, have enabled several compounds to enter clinical trials.","The scientific community must carefully vet known information to address the reproducibility crisis.","The full potential of AI in drug discovery can only be realized with sufficient ground truth and appropriate human intervention at later pipeline stages."],"url":"http://arxiv.org/abs/2307.06521v1"}
{"created":"2023-07-13 01:51:15","title":"Migrating to Post-Quantum Cryptography: a Framework Using Security Dependency Analysis","abstract":"Quantum computing is emerging as an unprecedented threat to the current state of widely used cryptographic systems. Cryptographic methods that have been considered secure for decades will likely be broken, with enormous impact on the security of sensitive data and communications in enterprises worldwide. A plan to migrate to quantum-resistant cryptographic systems is required. However, migrating an enterprise system to ensure a quantum-safe state is a complex process. Enterprises will require systematic guidance to perform this migration to remain resilient in a post-quantum era, as many organisations do not have staff with the expertise to manage this process unaided. This paper presents a comprehensive framework designed to aid enterprises in their migration. The framework articulates key steps and technical considerations in the cryptographic migration process. It makes use of existing organisational inventories and provides a roadmap for prioritising the replacement of cryptosystems in a post-quantum context. The framework enables the efficient identification of cryptographic objects, and can be integrated with other frameworks in enterprise settings to minimise operational disruption during migration. Practical case studies are included to demonstrate the utility and efficacy of the proposed framework using graph theoretic techniques to determine and evaluate cryptographic dependencies.","sentences":["Quantum computing is emerging as an unprecedented threat to the current state of widely used cryptographic systems.","Cryptographic methods that have been considered secure for decades will likely be broken, with enormous impact on the security of sensitive data and communications in enterprises worldwide.","A plan to migrate to quantum-resistant cryptographic systems is required.","However, migrating an enterprise system to ensure a quantum-safe state is a complex process.","Enterprises will require systematic guidance to perform this migration to remain resilient in a post-quantum era, as many organisations do not have staff with the expertise to manage this process unaided.","This paper presents a comprehensive framework designed to aid enterprises in their migration.","The framework articulates key steps and technical considerations in the cryptographic migration process.","It makes use of existing organisational inventories and provides a roadmap for prioritising the replacement of cryptosystems in a post-quantum context.","The framework enables the efficient identification of cryptographic objects, and can be integrated with other frameworks in enterprise settings to minimise operational disruption during migration.","Practical case studies are included to demonstrate the utility and efficacy of the proposed framework using graph theoretic techniques to determine and evaluate cryptographic dependencies."],"url":"http://arxiv.org/abs/2307.06520v1"}
{"created":"2023-07-13 01:43:28","title":"Machine Learning practices and infrastructures","abstract":"Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers' have demonstrated to be particularly salient for the societal impact of deployed ML systems.","sentences":["Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential.","They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs.","Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers.","This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems.","In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored.","I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners.","I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers' have demonstrated to be particularly salient for the societal impact of deployed ML systems."],"url":"http://arxiv.org/abs/2307.06518v1"}
{"created":"2023-07-13 01:22:18","title":"Leveraging Contextual Counterfactuals Toward Belief Calibration","abstract":"Beliefs and values are increasingly being incorporated into our AI systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training. However, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts. Specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held. Therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment. To do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts). By leveraging our notion of epistemic uncertainty, we introduce `the belief calibration cycle' framework to more holistically calibrate this diversity of beliefs with context-driven counterfactual reasoning by using a multi-objective optimization. We empirically apply our framework for finding a Pareto frontier of clustered optimal belief strengths that generalize across different contexts, demonstrating its efficacy on a toy dataset for credit decisions.","sentences":["Beliefs and values are increasingly being incorporated into our AI systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training.","However, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts.","Specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held.","Therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment.","To do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts).","By leveraging our notion of epistemic uncertainty, we introduce `the belief calibration cycle' framework to more holistically calibrate this diversity of beliefs with context-driven counterfactual reasoning by using a multi-objective optimization.","We empirically apply our framework for finding a Pareto frontier of clustered optimal belief strengths that generalize across different contexts, demonstrating its efficacy on a toy dataset for credit decisions."],"url":"http://arxiv.org/abs/2307.06513v1"}
{"created":"2023-07-13 01:14:08","title":"Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models","abstract":"Integrating deep learning with clinical expertise holds great potential for addressing healthcare challenges and empowering medical professionals with improved diagnostic tools. However, the need for annotated medical images is often an obstacle to leveraging the full power of machine learning models. Our research demonstrates that by combining synthetic images, generated using diffusion models, with real images, we can enhance nonalcoholic fatty liver disease (NAFLD) classification performance. We evaluate the quality of the synthetic images by comparing two metrics: Inception Score (IS) and Fr\\'{e}chet Inception Distance (FID), computed on diffusion-generated images and generative adversarial networks (GANs)-generated images. Our results show superior performance for the diffusion-generated images, with a maximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ compared to $99.53$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNet v1), our synthetic augmentation method achieves a maximum image-level ROC AUC of $0.904$ on a NAFLD prediction task.","sentences":["Integrating deep learning with clinical expertise holds great potential for addressing healthcare challenges and empowering medical professionals with improved diagnostic tools.","However, the need for annotated medical images is often an obstacle to leveraging the full power of machine learning models.","Our research demonstrates that by combining synthetic images, generated using diffusion models, with real images, we can enhance nonalcoholic fatty liver disease (NAFLD) classification performance.","We evaluate the quality of the synthetic images by comparing two metrics: Inception Score (IS) and Fr\\'{e}chet Inception Distance (FID), computed on diffusion-generated images and generative adversarial networks (GANs)-generated images.","Our results show superior performance for the diffusion-generated images, with a maximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ compared to $99.53$ for GANs.","Utilizing a partially frozen CNN backbone (EfficientNet v1), our synthetic augmentation method achieves a maximum image-level ROC AUC of $0.904$ on a NAFLD prediction task."],"url":"http://arxiv.org/abs/2307.06507v1"}
{"created":"2023-07-13 01:09:26","title":"Research Explosion: More Effort to Climb onto Shoulders of the Giant","abstract":"Fast-growing scientific publications present challenges to the scientific community. In this paper, we describe their implications to researchers. As references form explicit foundations for researchers to conduct a study, we investigate the evolution in reference patterns based on 60.8 million papers published from 1960 to 2015. The results demonstrate that recent papers contain more references than older ones, especially the well-cited papers compared with other papers. Well-cited papers receive 10 or more citations within 5 years of publication. Their references cover a longer period from classic research to very recent studies. Authors of well-cited papers are also farsighted to discover the reference papers with good potential to receive high citation numbers in near future. We also discover that the number of accumulative publications has a negative impact on next-5-year citation count for most fields except Chemistry, Materials science, Environmental science, Biology, and Engineering. Our findings suggest that researchers are expected to devote more effort to producing impactful research. Based on all these findings, we strongly advise against judging researchers simply based on the number of their publications. On the other hand, authors and reviewers should ensure that published papers contain adequate contributions. The code for our analysis is on GitHub: https://github.com/ECNU-Text-Computing/Research-Explosion.","sentences":["Fast-growing scientific publications present challenges to the scientific community.","In this paper, we describe their implications to researchers.","As references form explicit foundations for researchers to conduct a study, we investigate the evolution in reference patterns based on 60.8 million papers published from 1960 to 2015.","The results demonstrate that recent papers contain more references than older ones, especially the well-cited papers compared with other papers.","Well-cited papers receive 10 or more citations within 5 years of publication.","Their references cover a longer period from classic research to very recent studies.","Authors of well-cited papers are also farsighted to discover the reference papers with good potential to receive high citation numbers in near future.","We also discover that the number of accumulative publications has a negative impact on next-5-year citation count for most fields except Chemistry, Materials science, Environmental science, Biology, and Engineering.","Our findings suggest that researchers are expected to devote more effort to producing impactful research.","Based on all these findings, we strongly advise against judging researchers simply based on the number of their publications.","On the other hand, authors and reviewers should ensure that published papers contain adequate contributions.","The code for our analysis is on GitHub: https://github.com/ECNU-Text-Computing/Research-Explosion."],"url":"http://arxiv.org/abs/2307.06506v1"}
{"created":"2023-07-13 01:05:12","title":"WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmark for Autonomous Driving on Water Surfaces","abstract":"Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivors rescue, environmental monitoring, hydrography mapping and waste cleaning. This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces. Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for discerning object-related information, including color, shape, texture, range, velocity, azimuth, and elevation. Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixel-level and point-level, respectively. In addition to basic perception tasks, such as object detection, instance segmentation and semantic segmentation, we also provide annotations for free-space segmentation and waterline segmentation. Leveraging the multi-task and multi-modal data, we conduct numerous experiments on the single modality of radar and camera, as well as the fused modalities. Results demonstrate that 4D radar-camera fusion can considerably enhance the robustness of perception on water surfaces, especially in adverse lighting and weather conditions. WaterScenes dataset is public on https://waterscenes.github.io.","sentences":["Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivors rescue, environmental monitoring, hydrography mapping and waste cleaning.","This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces.","Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for discerning object-related information, including color, shape, texture, range, velocity, azimuth, and elevation.","Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixel-level and point-level, respectively.","In addition to basic perception tasks, such as object detection, instance segmentation and semantic segmentation, we also provide annotations for free-space segmentation and waterline segmentation.","Leveraging the multi-task and multi-modal data, we conduct numerous experiments on the single modality of radar and camera, as well as the fused modalities.","Results demonstrate that 4D radar-camera fusion can considerably enhance the robustness of perception on water surfaces, especially in adverse lighting and weather conditions.","WaterScenes dataset is public on https://waterscenes.github.io."],"url":"http://arxiv.org/abs/2307.06505v1"}
{"created":"2023-07-13 00:53:09","title":"Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning","abstract":"Objective: The artificial pancreas (AP) has shown promising potential in achieving closed-loop glucose control for individuals with type 1 diabetes mellitus (T1DM). However, designing an effective control policy for the AP remains challenging due to the complex physiological processes, delayed insulin response, and inaccurate glucose measurements. While model predictive control (MPC) offers safety and stability through the dynamic model and safety constraints, it lacks individualization and is adversely affected by unannounced meals. Conversely, deep reinforcement learning (DRL) provides personalized and adaptive strategies but faces challenges with distribution shifts and substantial data requirements. Methods: We propose a hybrid control policy for the artificial pancreas (HyCPAP) to address the above challenges. HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the strengths of both policies while compensating for their respective limitations. To facilitate faster deployment of AP systems in real-world settings, we further incorporate meta-learning techniques into HyCPAP, leveraging previous experience and patient-shared knowledge to enable fast adaptation to new patients with limited available data. Results: We conduct extensive experiments using the FDA-accepted UVA/Padova T1DM simulator across three scenarios. Our approaches achieve the highest percentage of time spent in the desired euglycemic range and the lowest occurrences of hypoglycemia. Conclusion: The results clearly demonstrate the superiority of our methods for closed-loop glucose management in individuals with T1DM. Significance: The study presents novel control policies for AP systems, affirming the great potential of proposed methods for efficient closed-loop glucose control.","sentences":["Objective: The artificial pancreas (AP) has shown promising potential in achieving closed-loop glucose control for individuals with type 1 diabetes mellitus (T1DM).","However, designing an effective control policy for the AP remains challenging due to the complex physiological processes, delayed insulin response, and inaccurate glucose measurements.","While model predictive control (MPC) offers safety and stability through the dynamic model and safety constraints, it lacks individualization and is adversely affected by unannounced meals.","Conversely, deep reinforcement learning (DRL) provides personalized and adaptive strategies but faces challenges with distribution shifts and substantial data requirements.","Methods: We propose a hybrid control policy for the artificial pancreas (HyCPAP) to address the above challenges.","HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the strengths of both policies while compensating for their respective limitations.","To facilitate faster deployment of AP systems in real-world settings, we further incorporate meta-learning techniques into HyCPAP, leveraging previous experience and patient-shared knowledge to enable fast adaptation to new patients with limited available data.","Results: We conduct extensive experiments using the FDA-accepted UVA/Padova T1DM simulator across three scenarios.","Our approaches achieve the highest percentage of time spent in the desired euglycemic range and the lowest occurrences of hypoglycemia.","Conclusion: The results clearly demonstrate the superiority of our methods for closed-loop glucose management in individuals with T1DM.","Significance: The study presents novel control policies for AP systems, affirming the great potential of proposed methods for efficient closed-loop glucose control."],"url":"http://arxiv.org/abs/2307.06501v1"}
{"created":"2023-07-13 00:36:55","title":"On the ability of CNNs to extract color invariant intensity based features for image classification","abstract":"Convolutional neural networks (CNNs) have demonstrated remarkable success in vision-related tasks. However, their susceptibility to failing when inputs deviate from the training distribution is well-documented. Recent studies suggest that CNNs exhibit a bias toward texture instead of object shape in image classification tasks, and that background information may affect predictions. This paper investigates the ability of CNNs to adapt to different color distributions in an image while maintaining context and background. The results of our experiments on modified MNIST and FashionMNIST data demonstrate that changes in color can substantially affect classification accuracy. The paper explores the effects of various regularization techniques on generalization error across datasets and proposes a minor architectural modification utilizing the dropout regularization in a novel way that enhances model reliance on color-invariant intensity-based features for improved classification accuracy. Overall, this work contributes to ongoing efforts to understand the limitations and challenges of CNNs in image classification tasks and offers potential solutions to enhance their performance.","sentences":["Convolutional neural networks (CNNs) have demonstrated remarkable success in vision-related tasks.","However, their susceptibility to failing when inputs deviate from the training distribution is well-documented.","Recent studies suggest that CNNs exhibit a bias toward texture instead of object shape in image classification tasks, and that background information may affect predictions.","This paper investigates the ability of CNNs to adapt to different color distributions in an image while maintaining context and background.","The results of our experiments on modified MNIST and FashionMNIST data demonstrate that changes in color can substantially affect classification accuracy.","The paper explores the effects of various regularization techniques on generalization error across datasets and proposes a minor architectural modification utilizing the dropout regularization in a novel way that enhances model reliance on color-invariant intensity-based features for improved classification accuracy.","Overall, this work contributes to ongoing efforts to understand the limitations and challenges of CNNs in image classification tasks and offers potential solutions to enhance their performance."],"url":"http://arxiv.org/abs/2307.06500v1"}
{"created":"2023-07-13 00:08:52","title":"Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems","abstract":"Deep learning models are susceptible to adversarial samples in white and black-box environments. Although previous studies have shown high attack success rates, coupling DNN models with interpretation models could offer a sense of security when a human expert is involved, who can identify whether a given sample is benign or malicious. However, in white-box environments, interpretable deep learning systems (IDLSes) have been shown to be vulnerable to malicious manipulations. In black-box settings, as access to the components of IDLSes is limited, it becomes more challenging for the adversary to fool the system. In this work, we propose a Query-efficient Score-based black-box attack against IDLSes, QuScore, which requires no knowledge of the target model and its coupled interpretation model. QuScore is based on transfer-based and score-based methods by employing an effective microbial genetic algorithm. Our method is designed to reduce the number of queries necessary to carry out successful attacks, resulting in a more efficient process. By continuously refining the adversarial samples created based on feedback scores from the IDLS, our approach effectively navigates the search space to identify perturbations that can fool the system. We evaluate the attack's effectiveness on four CNN models (Inception, ResNet, VGG, DenseNet) and two interpretation models (CAM, Grad), using both ImageNet and CIFAR datasets. Our results show that the proposed approach is query-efficient with a high attack success rate that can reach between 95% and 100% and transferability with an average success rate of 69% in the ImageNet and CIFAR datasets. Our attack method generates adversarial examples with attribution maps that resemble benign samples. We have also demonstrated that our attack is resilient against various preprocessing defense techniques and can easily be transferred to different DNN models.","sentences":["Deep learning models are susceptible to adversarial samples in white and black-box environments.","Although previous studies have shown high attack success rates, coupling DNN models with interpretation models could offer a sense of security when a human expert is involved, who can identify whether a given sample is benign or malicious.","However, in white-box environments, interpretable deep learning systems (IDLSes) have been shown to be vulnerable to malicious manipulations.","In black-box settings, as access to the components of IDLSes is limited, it becomes more challenging for the adversary to fool the system.","In this work, we propose a Query-efficient Score-based black-box attack against IDLSes, QuScore, which requires no knowledge of the target model and its coupled interpretation model.","QuScore is based on transfer-based and score-based methods by employing an effective microbial genetic algorithm.","Our method is designed to reduce the number of queries necessary to carry out successful attacks, resulting in a more efficient process.","By continuously refining the adversarial samples created based on feedback scores from the IDLS, our approach effectively navigates the search space to identify perturbations that can fool the system.","We evaluate the attack's effectiveness on four CNN models (Inception, ResNet, VGG, DenseNet) and two interpretation models (CAM, Grad), using both ImageNet and CIFAR datasets.","Our results show that the proposed approach is query-efficient with a high attack success rate that can reach between 95% and 100% and transferability with an average success rate of 69% in the ImageNet and CIFAR datasets.","Our attack method generates adversarial examples with attribution maps that resemble benign samples.","We have also demonstrated that our attack is resilient against various preprocessing defense techniques and can easily be transferred to different DNN models."],"url":"http://arxiv.org/abs/2307.06496v1"}
