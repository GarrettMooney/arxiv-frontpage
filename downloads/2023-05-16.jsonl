{"created":"2023-05-15","title":"Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text","abstract":"Knowledge graphs can represent information about the real-world using entities and their relations in a structured and semantically rich manner and they enable a variety of downstream applications such as question-answering, recommendation systems, semantic search, and advanced analytics. However, at the moment, building a knowledge graph involves a lot of manual effort and thus hinders their application in some situations and the automation of this process might benefit especially for small organizations. Automatically generating structured knowledge graphs from a large volume of natural language is still a challenging task and the research on sub-tasks such as named entity extraction, relation extraction, entity and relation linking, and knowledge graph construction aims to improve the state of the art of automatic construction and completion of knowledge graphs from text. The recent advancement of foundation models with billions of parameters trained in a self-supervised manner with large volumes of training data that can be adapted to a variety of downstream tasks has helped to demonstrate high performance on a large range of Natural Language Processing (NLP) tasks. In this context, one emerging paradigm is in-context learning where a language model is used as it is with a prompt that provides instructions and some examples to perform a task without changing the parameters of the model using traditional approaches such as fine-tuning. This way, no computing resources are needed for re-training/fine-tuning the models and the engineering effort is minimal. Thus, it would be beneficial to utilize such capabilities for generating knowledge graphs from text.","sentences":["Knowledge graphs can represent information about the real-world using entities and their relations in a structured and semantically rich manner and they enable a variety of downstream applications such as question-answering, recommendation systems, semantic search, and advanced analytics.","However, at the moment, building a knowledge graph involves a lot of manual effort and thus hinders their application in some situations and the automation of this process might benefit especially for small organizations.","Automatically generating structured knowledge graphs from a large volume of natural language is still a challenging task and the research on sub-tasks such as named entity extraction, relation extraction, entity and relation linking, and knowledge graph construction aims to improve the state of the art of automatic construction and completion of knowledge graphs from text.","The recent advancement of foundation models with billions of parameters trained in a self-supervised manner with large volumes of training data that can be adapted to a variety of downstream tasks has helped to demonstrate high performance on a large range of Natural Language Processing (NLP) tasks.","In this context, one emerging paradigm is in-context learning where a language model is used as it is with a prompt that provides instructions and some examples to perform a task without changing the parameters of the model using traditional approaches such as fine-tuning.","This way, no computing resources are needed for re-training/fine-tuning the models and the engineering effort is minimal.","Thus, it would be beneficial to utilize such capabilities for generating knowledge graphs from text."],"url":"http://arxiv.org/abs/2305.08804v1"}
{"created":"2023-05-15","title":"Sensitivity and Robustness of Large Language Models to Prompt in Japanese","abstract":"Prompt Engineering has gained significant relevance in recent years, fueled by advancements in pre-trained and large language models. However, a critical issue has been identified within this domain: the lack of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese. This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM), T5. These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context. Our experimental results reveal startling discrepancies. A simple modification in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44. This observation underscores the fact that even the highly performance GPT-4 model encounters significant stability issues when dealing with diverse Japanese prompt templates, rendering the consistency of the model's output results questionable. In light of these findings, we conclude by proposing potential research trajectories to further enhance the development and performance of Large Language Models in their current stage.","sentences":["Prompt Engineering has gained significant relevance in recent years, fueled by advancements in pre-trained and large language models.","However, a critical issue has been identified within this domain: the lack of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese.","This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM), T5.","These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context.","Our experimental results reveal startling discrepancies.","A simple modification in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.","This observation underscores the fact that even the highly performance GPT-4 model encounters significant stability issues when dealing with diverse Japanese prompt templates, rendering the consistency of the model's output results questionable.","In light of these findings, we conclude by proposing potential research trajectories to further enhance the development and performance of Large Language Models in their current stage."],"url":"http://arxiv.org/abs/2305.08714v1"}
{"created":"2023-05-15","title":"Improving ChatGPT Prompt for Code Generation","abstract":"Automated code generation can be a powerful technique for software development, significantly reducing developers' efforts and time required to create new code by generating it automatically based on requirements. Recently, OpenAI's language model ChatGPT has emerged as a powerful tool for generating human-like responses to a wide range of textual inputs (i.e., prompts), including those related to code generation. However, the effectiveness of ChatGPT for code generation is not well understood, and the generation performance could be heavily influenced by the choice of prompt. To answer these questions, we conducted experiments using the CodeXGlue dataset to evaluate ChatGPT's capabilities for two code generation tasks, including text-to-code and code-to-code generation. We designed prompts by leveraging the chain-of-thought strategy with multi-step optimizations. Our results showed that by carefully designing prompts to guide ChatGPT, the generation performance can be improved substantially. We also analyzed the factors that influenced the prompt design and provided insights that could guide future research.","sentences":["Automated code generation can be a powerful technique for software development, significantly reducing developers' efforts and time required to create new code by generating it automatically based on requirements.","Recently, OpenAI's language model ChatGPT has emerged as a powerful tool for generating human-like responses to a wide range of textual inputs (i.e., prompts), including those related to code generation.","However, the effectiveness of ChatGPT for code generation is not well understood, and the generation performance could be heavily influenced by the choice of prompt.","To answer these questions, we conducted experiments using the CodeXGlue dataset to evaluate ChatGPT's capabilities for two code generation tasks, including text-to-code and code-to-code generation.","We designed prompts by leveraging the chain-of-thought strategy with multi-step optimizations.","Our results showed that by carefully designing prompts to guide ChatGPT, the generation performance can be improved substantially.","We also analyzed the factors that influenced the prompt design and provided insights that could guide future research."],"url":"http://arxiv.org/abs/2305.08360v1"}
{"created":"2023-05-15","title":"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs","abstract":"Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show improvements (~5% on average) in multiple text similarity metrics over strong baselines across all three tasks.","sentences":["Despite their unprecedented success, even the largest language models make mistakes.","Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs.","Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback.","However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned.","Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network.","In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size.","RL4F produces critiques that help GPT-3 revise its outputs.","We study three datasets for action planning, summarization and alphabetization and show improvements (~5% on average) in multiple text similarity metrics over strong baselines across all three tasks."],"url":"http://arxiv.org/abs/2305.08844v1"}
{"created":"2023-05-15","title":"Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study","abstract":"Large Language Models (LLMs) like ChatGPT have proven a great shallow understanding of many traditional NLP tasks, such as translation, summarization, etc. However, its performance on high-level understanding, such as dialogue discourse analysis task that requires a higher level of understanding and reasoning, remains less explored. This study investigates ChatGPT's capabilities in three dialogue discourse tasks: topic segmentation, discourse relation recognition, and discourse parsing, of varying difficulty levels. To adapt ChatGPT to these tasks, we propose discriminative and generative paradigms and introduce the Chain of Thought (COT) approach to improve ChatGPT's performance in more difficult tasks. The results show that our generative paradigm allows ChatGPT to achieve comparative performance in the topic segmentation task comparable to state-of-the-art methods but reveals room for improvement in the more complex tasks of discourse relation recognition and discourse parsing. Notably, the COT can significantly enhance ChatGPT's performance with the help of understanding complex structures in more challenging tasks. Through a series of case studies, our in-depth analysis suggests that ChatGPT can be a good annotator in topic segmentation but has difficulties understanding complex rhetorical structures. We hope these findings provide a foundation for future research to refine dialogue discourse analysis approaches in the era of LLMs.","sentences":["Large Language Models (LLMs) like ChatGPT have proven a great shallow understanding of many traditional NLP tasks, such as translation, summarization, etc.","However, its performance on high-level understanding, such as dialogue discourse analysis task that requires a higher level of understanding and reasoning, remains less explored.","This study investigates ChatGPT's capabilities in three dialogue discourse tasks: topic segmentation, discourse relation recognition, and discourse parsing, of varying difficulty levels.","To adapt ChatGPT to these tasks, we propose discriminative and generative paradigms and introduce the Chain of Thought (COT) approach to improve ChatGPT's performance in more difficult tasks.","The results show that our generative paradigm allows ChatGPT to achieve comparative performance in the topic segmentation task comparable to state-of-the-art methods but reveals room for improvement in the more complex tasks of discourse relation recognition and discourse parsing.","Notably, the COT can significantly enhance ChatGPT's performance with the help of understanding complex structures in more challenging tasks.","Through a series of case studies, our in-depth analysis suggests that ChatGPT can be a good annotator in topic segmentation but has difficulties understanding complex rhetorical structures.","We hope these findings provide a foundation for future research to refine dialogue discourse analysis approaches in the era of LLMs."],"url":"http://arxiv.org/abs/2305.08391v1"}
{"created":"2023-05-15","title":"Improving ChatGPT Prompt for Code Generation","abstract":"Automated code generation can be a powerful technique for software development, significantly reducing developers' efforts and time required to create new code by generating it automatically based on requirements. Recently, OpenAI's language model ChatGPT has emerged as a powerful tool for generating human-like responses to a wide range of textual inputs (i.e., prompts), including those related to code generation. However, the effectiveness of ChatGPT for code generation is not well understood, and the generation performance could be heavily influenced by the choice of prompt. To answer these questions, we conducted experiments using the CodeXGlue dataset to evaluate ChatGPT's capabilities for two code generation tasks, including text-to-code and code-to-code generation. We designed prompts by leveraging the chain-of-thought strategy with multi-step optimizations. Our results showed that by carefully designing prompts to guide ChatGPT, the generation performance can be improved substantially. We also analyzed the factors that influenced the prompt design and provided insights that could guide future research.","sentences":["Automated code generation can be a powerful technique for software development, significantly reducing developers' efforts and time required to create new code by generating it automatically based on requirements.","Recently, OpenAI's language model ChatGPT has emerged as a powerful tool for generating human-like responses to a wide range of textual inputs (i.e., prompts), including those related to code generation.","However, the effectiveness of ChatGPT for code generation is not well understood, and the generation performance could be heavily influenced by the choice of prompt.","To answer these questions, we conducted experiments using the CodeXGlue dataset to evaluate ChatGPT's capabilities for two code generation tasks, including text-to-code and code-to-code generation.","We designed prompts by leveraging the chain-of-thought strategy with multi-step optimizations.","Our results showed that by carefully designing prompts to guide ChatGPT, the generation performance can be improved substantially.","We also analyzed the factors that influenced the prompt design and provided insights that could guide future research."],"url":"http://arxiv.org/abs/2305.08360v1"}
{"created":"2023-05-15","title":"Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis","abstract":"Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. Therefore, we propose that LLM-assisted annotation is a promising automated approach for corpus studies.","sentences":["Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding.","In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information.","Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), Bing chatbot (powered by GPT-4), and a human coder in the annotation task.","The results demonstrate that Bing chatbot significantly outperformed ChatGPT in the task.","Compared to human annotator, the overall performance of Bing chatbot was slightly less satisfactory.","However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER.","Therefore, we propose that LLM-assisted annotation is a promising automated approach for corpus studies."],"url":"http://arxiv.org/abs/2305.08339v1"}
{"created":"2023-05-15","title":"Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models","abstract":"Speech-driven animation has gained significant traction in recent years, with current methods achieving near-photorealistic results. However, the field remains underexplored regarding non-verbal communication despite evidence demonstrating its importance in human interaction. In particular, generating laughter sequences presents a unique challenge due to the intricacy and nuances of this behaviour. This paper aims to bridge this gap by proposing a novel model capable of generating realistic laughter sequences, given a still portrait and an audio clip containing laughter. We highlight the failure cases of traditional facial animation methods and leverage recent advances in diffusion models to produce convincing laughter videos. We train our model on a diverse set of laughter datasets and introduce an evaluation metric specifically designed for laughter. When compared with previous speech-driven approaches, our model achieves state-of-the-art performance across all metrics, even when these are re-trained for laughter generation.","sentences":["Speech-driven animation has gained significant traction in recent years, with current methods achieving near-photorealistic results.","However, the field remains underexplored regarding non-verbal communication despite evidence demonstrating its importance in human interaction.","In particular, generating laughter sequences presents a unique challenge due to the intricacy and nuances of this behaviour.","This paper aims to bridge this gap by proposing a novel model capable of generating realistic laughter sequences, given a still portrait and an audio clip containing laughter.","We highlight the failure cases of traditional facial animation methods and leverage recent advances in diffusion models to produce convincing laughter videos.","We train our model on a diverse set of laughter datasets and introduce an evaluation metric specifically designed for laughter.","When compared with previous speech-driven approaches, our model achieves state-of-the-art performance across all metrics, even when these are re-trained for laughter generation."],"url":"http://arxiv.org/abs/2305.08854v1"}
{"created":"2023-05-15","title":"CQE: A Comprehensive Quantity Extractor","abstract":"Quantities are essential in documents to describe factual information. They are ubiquitous in application domains such as finance, business, medicine, and science in general. Compared to other information extraction approaches, interestingly only a few works exist that describe methods for a proper extraction and representation of quantities in text. In this paper, we present such a comprehensive quantity extraction framework from text data. It efficiently detects combinations of values and units, the behavior of a quantity (e.g., rising or falling), and the concept a quantity is associated with. Our framework makes use of dependency parsing and a dictionary of units, and it provides for a proper normalization and standardization of detected quantities. Using a novel dataset for evaluation, we show that our open source framework outperforms other systems and -- to the best of our knowledge -- is the first to detect concepts associated with identified quantities. The code and data underlying our framework are available at https://github.com/vivkaz/CQE.","sentences":["Quantities are essential in documents to describe factual information.","They are ubiquitous in application domains such as finance, business, medicine, and science in general.","Compared to other information extraction approaches, interestingly only a few works exist that describe methods for a proper extraction and representation of quantities in text.","In this paper, we present such a comprehensive quantity extraction framework from text data.","It efficiently detects combinations of values and units, the behavior of a quantity (e.g., rising or falling), and the concept a quantity is associated with.","Our framework makes use of dependency parsing and a dictionary of units, and it provides for a proper normalization and standardization of detected quantities.","Using a novel dataset for evaluation, we show that our open source framework outperforms other systems and -- to the best of our knowledge -- is the first to detect concepts associated with identified quantities.","The code and data underlying our framework are available at https://github.com/vivkaz/CQE."],"url":"http://arxiv.org/abs/2305.08853v1"}
{"created":"2023-05-15","title":"Large Language Models are Zero-Shot Rankers for Recommender Systems","abstract":"Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates. We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction. We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems. We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators. We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies. The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank.","sentences":["Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks.","Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems.","To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates.","We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction.","We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems.","We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators.","We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies.","The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank."],"url":"http://arxiv.org/abs/2305.08845v1"}
{"created":"2023-05-15","title":"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs","abstract":"Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show improvements (~5% on average) in multiple text similarity metrics over strong baselines across all three tasks.","sentences":["Despite their unprecedented success, even the largest language models make mistakes.","Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs.","Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback.","However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned.","Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network.","In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size.","RL4F produces critiques that help GPT-3 revise its outputs.","We study three datasets for action planning, summarization and alphabetization and show improvements (~5% on average) in multiple text similarity metrics over strong baselines across all three tasks."],"url":"http://arxiv.org/abs/2305.08844v1"}
{"created":"2023-05-15","title":"Deep Learning combined with singular value decomposition to reconstruct databases in fluid dynamics","abstract":"Fluid Dynamics problems are characterized by being multidimensional and nonlinear. Therefore, experiments and numerical simulations are complex and time-consuming. Motivated by this, the need arises to find new techniques to obtain data in a simpler way and in less time. In this article, we present a novel methodology based on physical principles to reconstruct three-, four- and five-dimensional databases from a strongly sparse sensors as input. The methodology consists of combining Single Value Decomposition (SVD) with neural networks. The neural network used is characterized by a simple architecture based on combining two autoencoders that work in parallel and are joined in the last layer. This new algorithm has been proved with three databases with different dimensions and complexities: in an Atmospheric Boundary Layer (ABL) with a turbulence model and in the flow past a two- and a three-dimensional cylinder. Summarizing, this work proposes a new hybrid physics-based machine learning model with a simple, robust and generalizable architecture, which allows reconstructing databases from very few sensors and with a very low computational cost.","sentences":["Fluid Dynamics problems are characterized by being multidimensional and nonlinear.","Therefore, experiments and numerical simulations are complex and time-consuming.","Motivated by this, the need arises to find new techniques to obtain data in a simpler way and in less time.","In this article, we present a novel methodology based on physical principles to reconstruct three-, four- and five-dimensional databases from a strongly sparse sensors as input.","The methodology consists of combining Single Value Decomposition (SVD) with neural networks.","The neural network used is characterized by a simple architecture based on combining two autoencoders that work in parallel and are joined in the last layer.","This new algorithm has been proved with three databases with different dimensions and complexities: in an Atmospheric Boundary Layer (ABL) with a turbulence model and in the flow past a two- and a three-dimensional cylinder.","Summarizing, this work proposes a new hybrid physics-based machine learning model with a simple, robust and generalizable architecture, which allows reconstructing databases from very few sensors and with a very low computational cost."],"url":"http://arxiv.org/abs/2305.08832v1"}
{"created":"2023-05-15","title":"PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India","abstract":"This paper introduces PMIndiaSum, a new multilingual and massively parallel headline summarization corpus focused on languages in India. Our corpus covers four language families, 14 languages, and the largest to date, 196 language pairs. It provides a testing ground for all cross-lingual pairs. We detail our workflow to construct the corpus, including data acquisition, processing, and quality assurance. Furthermore, we publish benchmarks for monolingual, cross-lingual, and multilingual summarization by fine-tuning, prompting, as well as translate-and-summarize. Experimental results confirm the crucial role of our data in aiding the summarization of Indian texts. Our dataset is publicly available and can be freely modified and re-distributed.","sentences":["This paper introduces PMIndiaSum, a new multilingual and massively parallel headline summarization corpus focused on languages in India.","Our corpus covers four language families, 14 languages, and the largest to date, 196 language pairs.","It provides a testing ground for all cross-lingual pairs.","We detail our workflow to construct the corpus, including data acquisition, processing, and quality assurance.","Furthermore, we publish benchmarks for monolingual, cross-lingual, and multilingual summarization by fine-tuning, prompting, as well as translate-and-summarize.","Experimental results confirm the crucial role of our data in aiding the summarization of Indian texts.","Our dataset is publicly available and can be freely modified and re-distributed."],"url":"http://arxiv.org/abs/2305.08828v1"}
{"created":"2023-05-15","title":"Learning Better Contrastive View from Radiologist's Gaze","abstract":"Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims to minimizing distances between positive pairs. These methods usually apply random data augmentation to input images, expecting the augmented views of the same images to be similar and positively paired. However, random augmentation may overlook image semantic information and degrade the quality of augmented views in contrastive learning. This issue becomes more challenging in medical images since the abnormalities related to diseases can be tiny, and are easy to be corrupted (e.g., being cropped out) in the current scheme of random augmentation. In this work, we first demonstrate that, for widely-used X-ray images, the conventional augmentation prevalent in contrastive pre-training can affect the performance of the downstream diagnosis or classification tasks. Then, we propose a novel augmentation method, i.e., FocusContrast, to learn from radiologists' gaze in diagnosis and generate contrastive views for medical images with guidance from radiologists' visual attention. Specifically, we track the gaze movement of radiologists and model their visual attention when reading to diagnose X-ray images. The learned model can predict visual attention of the radiologists given a new input image, and further guide the attention-aware augmentation that hardly neglects the disease-related abnormalities. As a plug-and-play and framework-agnostic module, FocusContrast consistently improves state-of-the-art contrastive learning methods of SimCLR, MoCo, and BYOL by 4.0~7.0% in classification accuracy on a knee X-ray dataset.","sentences":["Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims to minimizing distances between positive pairs.","These methods usually apply random data augmentation to input images, expecting the augmented views of the same images to be similar and positively paired.","However, random augmentation may overlook image semantic information and degrade the quality of augmented views in contrastive learning.","This issue becomes more challenging in medical images since the abnormalities related to diseases can be tiny, and are easy to be corrupted (e.g., being cropped out) in the current scheme of random augmentation.","In this work, we first demonstrate that, for widely-used X-ray images, the conventional augmentation prevalent in contrastive pre-training can affect the performance of the downstream diagnosis or classification tasks.","Then, we propose a novel augmentation method, i.e., FocusContrast, to learn from radiologists' gaze in diagnosis and generate contrastive views for medical images with guidance from radiologists' visual attention.","Specifically, we track the gaze movement of radiologists and model their visual attention when reading to diagnose X-ray images.","The learned model can predict visual attention of the radiologists given a new input image, and further guide the attention-aware augmentation that hardly neglects the disease-related abnormalities.","As a plug-and-play and framework-agnostic module, FocusContrast consistently improves state-of-the-art contrastive learning methods of SimCLR, MoCo, and BYOL by 4.0~7.0% in classification accuracy on a knee X-ray dataset."],"url":"http://arxiv.org/abs/2305.08826v1"}
{"created":"2023-05-15","title":"Five A$^{+}$ Network: You Only Need 9K Parameters for Underwater Image Enhancement","abstract":"A lightweight underwater image enhancement network is of great significance for resource-constrained platforms, but balancing model size, computational efficiency, and enhancement performance has proven difficult for previous approaches. In this work, we propose the Five A$^{+}$ Network (FA$^{+}$Net), a highly efficient and lightweight real-time underwater image enhancement network with only $\\sim$ 9k parameters and $\\sim$ 0.01s processing time. The FA$^{+}$Net employs a two-stage enhancement structure. The strong prior stage aims to decompose challenging underwater degradations into sub-problems, while the fine-grained stage incorporates multi-branch color enhancement module and pixel attention module to amplify the network's perception of details. To the best of our knowledge, FA$^{+}$Net is the only network with the capability of real-time enhancement of 1080P images. Thorough extensive experiments and comprehensive visual comparison, we show that FA$^{+}$Net outperforms previous approaches by obtaining state-of-the-art performance on multiple datasets while significantly reducing both parameter count and computational complexity. The code is open source at https://github.com/Owen718/FiveAPlus-Network.","sentences":["A lightweight underwater image enhancement network is of great significance for resource-constrained platforms, but balancing model size, computational efficiency, and enhancement performance has proven difficult for previous approaches.","In this work, we propose the Five A$^{+}$ Network (FA$^{+}$Net), a highly efficient and lightweight real-time underwater image enhancement network with only $\\sim$ 9k parameters and $\\sim$ 0.01s processing time.","The FA$^{+}$Net employs a two-stage enhancement structure.","The strong prior stage aims to decompose challenging underwater degradations into sub-problems, while the fine-grained stage incorporates multi-branch color enhancement module and pixel attention module to amplify the network's perception of details.","To the best of our knowledge, FA$^{+}$Net is the only network with the capability of real-time enhancement of 1080P images.","Thorough extensive experiments and comprehensive visual comparison, we show that FA$^{+}$Net outperforms previous approaches by obtaining state-of-the-art performance on multiple datasets while significantly reducing both parameter count and computational complexity.","The code is open source at https://github.com/Owen718/FiveAPlus-Network."],"url":"http://arxiv.org/abs/2305.08824v1"}
{"created":"2023-05-15","title":"Sentence Level Curriculum Learning for Improved Neural Conversational Models","abstract":"Designing machine intelligence to converse with a human user necessarily requires an understanding of how humans participate in conversation, and thus conversation modeling is an important task in natural language processing. New breakthroughs in architecture and data gathering continue to push the performance of such conversational AI models. However, designs neglect the gradual buildup in sentence structure and complexity experienced by humans as we learn to communicate. During training, our model accepts one or more sentences as input and attempts to predict the next sentence in the conversation one word at a time, so our goal is to separate training into segments, with each segment's corpus comprised of longer sentence pairs than the previous one. This will mimic the desired \"buildup\" component of human learning. We begin with only \"short\" length sentence pairs, then only \"medium\" length pairs, and so on. A majority of our experiments were toward optimizing this technique, ensuring a proper representation of the technique's potential, since many of the details were new questions. Our segment-trained models were then able to achieve lower validation loss at the end of training than models trained with standard text preparation. This segmented training is straightforward to implement and our results provide a general direction for future research to implement and improve it.","sentences":["Designing machine intelligence to converse with a human user necessarily requires an understanding of how humans participate in conversation, and thus conversation modeling is an important task in natural language processing.","New breakthroughs in architecture and data gathering continue to push the performance of such conversational AI models.","However, designs neglect the gradual buildup in sentence structure and complexity experienced by humans as we learn to communicate.","During training, our model accepts one or more sentences as input and attempts to predict the next sentence in the conversation one word at a time, so our goal is to separate training into segments, with each segment's corpus comprised of longer sentence pairs than the previous one.","This will mimic the desired \"buildup\" component of human learning.","We begin with only \"short\" length sentence pairs, then only \"medium\" length pairs, and so on.","A majority of our experiments were toward optimizing this technique, ensuring a proper representation of the technique's potential, since many of the details were new questions.","Our segment-trained models were then able to achieve lower validation loss at the end of training than models trained with standard text preparation.","This segmented training is straightforward to implement and our results provide a general direction for future research to implement and improve it."],"url":"http://arxiv.org/abs/2305.08818v1"}
{"created":"2023-05-15","title":"AutoRecon: Automated 3D Object Discovery and Reconstruction","abstract":"A fully automated object reconstruction pipeline is crucial for digital content creation. While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images. We demonstrate that foreground objects can be robustly located and segmented from SfM point clouds by leveraging self-supervised 2D vision transformer features. Then, we reconstruct decomposed neural scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate object reconstruction and segmentation. Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon.","sentences":["A fully automated object reconstruction pipeline is crucial for digital content creation.","While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations.","In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images.","We demonstrate that foreground objects can be robustly located and segmented from SfM point clouds by leveraging self-supervised 2D vision transformer features.","Then, we reconstruct decomposed neural scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate object reconstruction and segmentation.","Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon."],"url":"http://arxiv.org/abs/2305.08810v1"}
{"created":"2023-05-15","title":"\"Nothing Abnormal\": Disambiguating Medical Reports via Contrastive Knowledge Infusion","abstract":"Sharing medical reports is essential for patient-centered care. A recent line of work has focused on automatically generating reports with NLP methods. However, different audiences have different purposes when writing/reading medical reports -- for example, healthcare professionals care more about pathology, whereas patients are more concerned with the diagnosis (\"Is there any abnormality?\"). The expectation gap results in a common situation where patients find their medical reports to be ambiguous and therefore unsure about the next steps. In this work, we explore the audience expectation gap in healthcare and summarize common ambiguities that lead patients to be confused about their diagnosis into three categories: medical jargon, contradictory findings, and misleading grammatical errors. Based on our analysis, we define a disambiguation rewriting task to regenerate an input to be unambiguous while preserving information about the original content. We further propose a rewriting algorithm based on contrastive pretraining and perturbation-based rewriting. In addition, we create two datasets, OpenI-Annotated based on chest reports and VA-Annotated based on general medical reports, with available binary labels for ambiguity and abnormality presence annotated by radiology specialists. Experimental results on these datasets show that our proposed algorithm effectively rewrites input sentences in a less ambiguous way with high content fidelity. Our code and annotated data are released to facilitate future research.","sentences":["Sharing medical reports is essential for patient-centered care.","A recent line of work has focused on automatically generating reports with NLP methods.","However, different audiences have different purposes when writing/reading medical reports -- for example, healthcare professionals care more about pathology, whereas patients are more concerned with the diagnosis (\"Is there any abnormality?\").","The expectation gap results in a common situation where patients find their medical reports to be ambiguous and therefore unsure about the next steps.","In this work, we explore the audience expectation gap in healthcare and summarize common ambiguities that lead patients to be confused about their diagnosis into three categories: medical jargon, contradictory findings, and misleading grammatical errors.","Based on our analysis, we define a disambiguation rewriting task to regenerate an input to be unambiguous while preserving information about the original content.","We further propose a rewriting algorithm based on contrastive pretraining and perturbation-based rewriting.","In addition, we create two datasets, OpenI-Annotated based on chest reports and VA-Annotated based on general medical reports, with available binary labels for ambiguity and abnormality presence annotated by radiology specialists.","Experimental results on these datasets show that our proposed algorithm effectively rewrites input sentences in a less ambiguous way with high content fidelity.","Our code and annotated data are released to facilitate future research."],"url":"http://arxiv.org/abs/2305.08300v1"}
{"created":"2023-05-15","title":"MV-Map: Offboard HD-Map Generation with Multi-view Consistency","abstract":"While bird's-eye-view (BEV) perception models can be useful for building high-definition maps (HD-Maps) with less human labor, their results are often unreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps from different viewpoints. This is because BEV perception is typically set up in an 'onboard' manner, which restricts the computation and consequently prevents algorithms from reasoning multiple views simultaneously. This paper overcomes these limitations and advocates a more practical 'offboard' HD-Map generation setup that removes the computation constraints, based on the fact that HD-Maps are commonly reusable infrastructures built offline in data centers. To this end, we propose a novel offboard pipeline called MV-Map that capitalizes multi-view consistency and can handle an arbitrary number of frames with the key design of a 'region-centric' framework. In MV-Map, the target HD-Maps are created by aggregating all the frames of onboard predictions, weighted by the confidence scores assigned by an 'uncertainty network'. To further enhance multi-view consistency, we augment the uncertainty network with the global 3D structure optimized by a voxelized neural radiance field (Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map significantly improves the quality of HD-Maps, further highlighting the importance of offboard methods for HD-Map generation.","sentences":["While bird's-eye-view (BEV) perception models can be useful for building high-definition maps (HD-Maps) with less human labor, their results are often unreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps from different viewpoints.","This is because BEV perception is typically set up in an 'onboard' manner, which restricts the computation and consequently prevents algorithms from reasoning multiple views simultaneously.","This paper overcomes these limitations and advocates a more practical 'offboard' HD-Map generation setup that removes the computation constraints, based on the fact that HD-Maps are commonly reusable infrastructures built offline in data centers.","To this end, we propose a novel offboard pipeline called MV-Map that capitalizes multi-view consistency and can handle an arbitrary number of frames with the key design of a 'region-centric' framework.","In MV-Map, the target HD-Maps are created by aggregating all the frames of onboard predictions, weighted by the confidence scores assigned by an 'uncertainty network'.","To further enhance multi-view consistency, we augment the uncertainty network with the global 3D structure optimized by a voxelized neural radiance field (Voxel-NeRF).","Extensive experiments on nuScenes show that our MV-Map significantly improves the quality of HD-Maps, further highlighting the importance of offboard methods for HD-Map generation."],"url":"http://arxiv.org/abs/2305.08851v1"}
{"created":"2023-05-15","title":"PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India","abstract":"This paper introduces PMIndiaSum, a new multilingual and massively parallel headline summarization corpus focused on languages in India. Our corpus covers four language families, 14 languages, and the largest to date, 196 language pairs. It provides a testing ground for all cross-lingual pairs. We detail our workflow to construct the corpus, including data acquisition, processing, and quality assurance. Furthermore, we publish benchmarks for monolingual, cross-lingual, and multilingual summarization by fine-tuning, prompting, as well as translate-and-summarize. Experimental results confirm the crucial role of our data in aiding the summarization of Indian texts. Our dataset is publicly available and can be freely modified and re-distributed.","sentences":["This paper introduces PMIndiaSum, a new multilingual and massively parallel headline summarization corpus focused on languages in India.","Our corpus covers four language families, 14 languages, and the largest to date, 196 language pairs.","It provides a testing ground for all cross-lingual pairs.","We detail our workflow to construct the corpus, including data acquisition, processing, and quality assurance.","Furthermore, we publish benchmarks for monolingual, cross-lingual, and multilingual summarization by fine-tuning, prompting, as well as translate-and-summarize.","Experimental results confirm the crucial role of our data in aiding the summarization of Indian texts.","Our dataset is publicly available and can be freely modified and re-distributed."],"url":"http://arxiv.org/abs/2305.08828v1"}
{"created":"2023-05-15","title":"Learning Better Contrastive View from Radiologist's Gaze","abstract":"Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims to minimizing distances between positive pairs. These methods usually apply random data augmentation to input images, expecting the augmented views of the same images to be similar and positively paired. However, random augmentation may overlook image semantic information and degrade the quality of augmented views in contrastive learning. This issue becomes more challenging in medical images since the abnormalities related to diseases can be tiny, and are easy to be corrupted (e.g., being cropped out) in the current scheme of random augmentation. In this work, we first demonstrate that, for widely-used X-ray images, the conventional augmentation prevalent in contrastive pre-training can affect the performance of the downstream diagnosis or classification tasks. Then, we propose a novel augmentation method, i.e., FocusContrast, to learn from radiologists' gaze in diagnosis and generate contrastive views for medical images with guidance from radiologists' visual attention. Specifically, we track the gaze movement of radiologists and model their visual attention when reading to diagnose X-ray images. The learned model can predict visual attention of the radiologists given a new input image, and further guide the attention-aware augmentation that hardly neglects the disease-related abnormalities. As a plug-and-play and framework-agnostic module, FocusContrast consistently improves state-of-the-art contrastive learning methods of SimCLR, MoCo, and BYOL by 4.0~7.0% in classification accuracy on a knee X-ray dataset.","sentences":["Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims to minimizing distances between positive pairs.","These methods usually apply random data augmentation to input images, expecting the augmented views of the same images to be similar and positively paired.","However, random augmentation may overlook image semantic information and degrade the quality of augmented views in contrastive learning.","This issue becomes more challenging in medical images since the abnormalities related to diseases can be tiny, and are easy to be corrupted (e.g., being cropped out) in the current scheme of random augmentation.","In this work, we first demonstrate that, for widely-used X-ray images, the conventional augmentation prevalent in contrastive pre-training can affect the performance of the downstream diagnosis or classification tasks.","Then, we propose a novel augmentation method, i.e., FocusContrast, to learn from radiologists' gaze in diagnosis and generate contrastive views for medical images with guidance from radiologists' visual attention.","Specifically, we track the gaze movement of radiologists and model their visual attention when reading to diagnose X-ray images.","The learned model can predict visual attention of the radiologists given a new input image, and further guide the attention-aware augmentation that hardly neglects the disease-related abnormalities.","As a plug-and-play and framework-agnostic module, FocusContrast consistently improves state-of-the-art contrastive learning methods of SimCLR, MoCo, and BYOL by 4.0~7.0% in classification accuracy on a knee X-ray dataset."],"url":"http://arxiv.org/abs/2305.08826v1"}
{"created":"2023-05-15","title":"A Matter of Annotation: An Empirical Study on In Situ and Self-Recall Activity Annotations from Wearable Sensors","abstract":"Research into the detection of human activities from wearable sensors is a highly active field, benefiting numerous applications, from ambulatory monitoring of healthcare patients via fitness coaching to streamlining manual work processes. We present an empirical study that compares 4 different commonly used annotation methods utilized in user studies that focus on in-the-wild data. These methods can be grouped in user-driven, in situ annotations - which are performed before or during the activity is recorded - and recall methods - where participants annotate their data in hindsight at the end of the day. Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the capabilities of a deep learning classifier trained with the data respectively. We noticed that in situ methods produce less but more precise labels than recall methods. Furthermore, we combined an activity diary with a visualization tool that enables the participant to inspect and label their activity data. Due to the introduction of such a tool were able to decrease missing annotations and increase the annotation consistency, and therefore the F1-score of the deep learning model by up to 8% (ranging between 82.1 and 90.4% F1-score). Furthermore, we discuss the advantages and disadvantages of the methods compared in our study, the biases they may could introduce and the consequences of their usage on human activity recognition studies and as well as possible solutions.","sentences":["Research into the detection of human activities from wearable sensors is a highly active field, benefiting numerous applications, from ambulatory monitoring of healthcare patients via fitness coaching to streamlining manual work processes.","We present an empirical study that compares 4 different commonly used annotation methods utilized in user studies that focus on in-the-wild data.","These methods can be grouped in user-driven, in situ annotations - which are performed before or during the activity is recorded - and recall methods - where participants annotate their data in hindsight at the end of the day.","Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the capabilities of a deep learning classifier trained with the data respectively.","We noticed that in situ methods produce less but more precise labels than recall methods.","Furthermore, we combined an activity diary with a visualization tool that enables the participant to inspect and label their activity data.","Due to the introduction of such a tool were able to decrease missing annotations and increase the annotation consistency, and therefore the F1-score of the deep learning model by up to 8% (ranging between 82.1 and 90.4% F1-score).","Furthermore, we discuss the advantages and disadvantages of the methods compared in our study, the biases they may could introduce and the consequences of their usage on human activity recognition studies and as well as possible solutions."],"url":"http://arxiv.org/abs/2305.08752v1"}
{"created":"2023-05-15","title":"Refining Amortized Posterior Approximations using Gradient-Based Summary Statistics","abstract":"We present an iterative framework to improve the amortized approximations of posterior distributions in the context of Bayesian inverse problems, which is inspired by loop-unrolled gradient descent methods and is theoretically grounded in maximally informative summary statistics. Amortized variational inference is restricted by the expressive power of the chosen variational distribution and the availability of training data in the form of joint data and parameter samples, which often lead to approximation errors such as the amortization gap. To address this issue, we propose an iterative framework that refines the current amortized posterior approximation at each step. Our approach involves alternating between two steps: (1) constructing a training dataset consisting of pairs of summarized data residuals and parameters, where the summarized data residual is generated using a gradient-based summary statistic, and (2) training a conditional generative model -- a normalizing flow in our examples -- on this dataset to obtain a probabilistic update of the unknown parameter. This procedure leads to iterative refinement of the amortized posterior approximations without the need for extra training data. We validate our method in a controlled setting by applying it to a stylized problem, and observe improved posterior approximations with each iteration. Additionally, we showcase the capability of our method in tackling realistically sized problems by applying it to transcranial ultrasound, a high-dimensional, nonlinear inverse problem governed by wave physics, and observe enhanced posterior quality through better image reconstruction with the posterior mean.","sentences":["We present an iterative framework to improve the amortized approximations of posterior distributions in the context of Bayesian inverse problems, which is inspired by loop-unrolled gradient descent methods and is theoretically grounded in maximally informative summary statistics.","Amortized variational inference is restricted by the expressive power of the chosen variational distribution and the availability of training data in the form of joint data and parameter samples, which often lead to approximation errors such as the amortization gap.","To address this issue, we propose an iterative framework that refines the current amortized posterior approximation at each step.","Our approach involves alternating between two steps: (1) constructing a training dataset consisting of pairs of summarized data residuals and parameters, where the summarized data residual is generated using a gradient-based summary statistic, and (2) training a conditional generative model -- a normalizing flow in our examples -- on this dataset to obtain a probabilistic update of the unknown parameter.","This procedure leads to iterative refinement of the amortized posterior approximations without the need for extra training data.","We validate our method in a controlled setting by applying it to a stylized problem, and observe improved posterior approximations with each iteration.","Additionally, we showcase the capability of our method in tackling realistically sized problems by applying it to transcranial ultrasound, a high-dimensional, nonlinear inverse problem governed by wave physics, and observe enhanced posterior quality through better image reconstruction with the posterior mean."],"url":"http://arxiv.org/abs/2305.08733v1"}
{"created":"2023-05-15","title":"Linear-Sized Sparsifiers via Near-Linear Time Discrepancy Theory","abstract":"Discrepancy theory provides powerful tools for producing higher-quality objects which \"beat the union bound\" in fundamental settings throughout combinatorics and computer science. However, this quality has often come at the price of more expensive algorithms. We introduce a new framework for bridging this gap, by allowing for the efficient implementation of discrepancy-theoretic primitives. Our framework repeatedly solves regularized optimization problems to low accuracy to approximate the partial coloring method of [Rot17], and simplifies and generalizes recent work of [JSS23] on fast algorithms for Spencer's theorem. In particular, our framework only requires that the discrepancy body of interest has exponentially large Gaussian measure and is expressible as a sublevel set of a symmetric, convex function. We combine this framework with new tools for proving Gaussian measure lower bounds to give improved algorithms for a variety of sparsification and coloring problems.   As a first application, we use our framework to obtain an $\\widetilde{O}(m \\cdot \\epsilon^{-3.5})$ time algorithm for constructing an $\\epsilon$-approximate spectral sparsifier of an $m$-edge graph, matching the sparsity of [BSS14] up to constant factors and improving upon the $\\widetilde{O}(m \\cdot \\epsilon^{-6.5})$ runtime of [LeeS17]. We further give a state-of-the-art algorithm for constructing graph ultrasparsifiers and an almost-linear time algorithm for constructing linear-sized degree-preserving sparsifiers via discrepancy theory; in the latter case, such sparsifiers were not known to exist previously. We generalize these results to their analogs in sparsifying isotropic sums of positive semidefinite matrices. Finally, to demonstrate the versatility of our technique, we obtain a nearly-input-sparsity time constructive algorithm for Spencer's theorem (where we recover a recent result of [JSS23]).","sentences":["Discrepancy theory provides powerful tools for producing higher-quality objects which \"beat the union bound\" in fundamental settings throughout combinatorics and computer science.","However, this quality has often come at the price of more expensive algorithms.","We introduce a new framework for bridging this gap, by allowing for the efficient implementation of discrepancy-theoretic primitives.","Our framework repeatedly solves regularized optimization problems to low accuracy to approximate the partial coloring method of [Rot17], and simplifies and generalizes recent work of [JSS23] on fast algorithms for Spencer's theorem.","In particular, our framework only requires that the discrepancy body of interest has exponentially large Gaussian measure and is expressible as a sublevel set of a symmetric, convex function.","We combine this framework with new tools for proving Gaussian measure lower bounds to give improved algorithms for a variety of sparsification and coloring problems.   ","As a first application, we use our framework to obtain an $\\widetilde{O}(m \\cdot \\epsilon^{-3.5})$ time algorithm for constructing an $\\epsilon$-approximate spectral sparsifier of an $m$-edge graph, matching the sparsity of [BSS14] up to constant factors and improving upon the $\\widetilde{O}(m \\cdot \\epsilon^{-6.5})$ runtime of [LeeS17].","We further give a state-of-the-art algorithm for constructing graph ultrasparsifiers and an almost-linear time algorithm for constructing linear-sized degree-preserving sparsifiers via discrepancy theory; in the latter case, such sparsifiers were not known to exist previously.","We generalize these results to their analogs in sparsifying isotropic sums of positive semidefinite matrices.","Finally, to demonstrate the versatility of our technique, we obtain a nearly-input-sparsity time constructive algorithm for Spencer's theorem (where we recover a recent result of [JSS23])."],"url":"http://arxiv.org/abs/2305.08434v1"}
{"created":"2023-05-15","title":"Low-intensity illumination for lensless digital holographic microscopy with minimized sample interaction","abstract":"Exposure to laser light alters cell culture examination via optical microscopic imaging techniques, also based on label-free coherent digital holography. To mitigate this detrimental feature, researchers tend to use a broader spectrum and lower intensity of illumination, which can decrease the quality of holographic imaging due to lower resolution and higher noise. We study the lensless digital holographic microscopy (LDHM) ability to operate in the low photon budget (LPB) regime to enable imaging of unimpaired live cells with minimized sample interaction. Low-cost off-the-shelf components are used, promoting the usability of such a straightforward approach. We show that recording data in the LPB regime (down to 7 uW of illumination power) does not limit the contrast nor resolution of the hologram phase and amplitude reconstruction compared to the regular illumination. The LPB generates hardware camera shot noise, however, to be effectively minimized via numerical denoising. The ability to obtain high-quality, high-resolution optical complex field reconstruction was confirmed using the USAF 1951 amplitude sample, phase resolution test target, and finally, live glial restricted progenitor cells (as a challenging strongly absorbing and scattering biomedical sample). The proposed approach based on severely limiting the photon budget in lensless holographic microscopy method can open new avenues in high-throughout (optimal resolution, large field-of-view and high signal-to-noise-ratio single-hologram reconstruction) cell culture imaging with minimized sample interaction.","sentences":["Exposure to laser light alters cell culture examination via optical microscopic imaging techniques, also based on label-free coherent digital holography.","To mitigate this detrimental feature, researchers tend to use a broader spectrum and lower intensity of illumination, which can decrease the quality of holographic imaging due to lower resolution and higher noise.","We study the lensless digital holographic microscopy (LDHM) ability to operate in the low photon budget (LPB) regime to enable imaging of unimpaired live cells with minimized sample interaction.","Low-cost off-the-shelf components are used, promoting the usability of such a straightforward approach.","We show that recording data in the LPB regime (down to 7 uW of illumination power) does not limit the contrast nor resolution of the hologram phase and amplitude reconstruction compared to the regular illumination.","The LPB generates hardware camera shot noise, however, to be effectively minimized via numerical denoising.","The ability to obtain high-quality, high-resolution optical complex field reconstruction was confirmed using the USAF 1951 amplitude sample, phase resolution test target, and finally, live glial restricted progenitor cells (as a challenging strongly absorbing and scattering biomedical sample).","The proposed approach based on severely limiting the photon budget in lensless holographic microscopy method can open new avenues in high-throughout (optimal resolution, large field-of-view and high signal-to-noise-ratio single-hologram reconstruction) cell culture imaging with minimized sample interaction."],"url":"http://arxiv.org/abs/2305.08392v1"}
{"created":"2023-05-15","title":"A Language Model of Java Methods with Train/Test Deduplication","abstract":"This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.","sentences":["This tool demonstration presents a research toolkit for a language model of Java source code.","The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java.","In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget.","Our model is a GPT2-like architecture with 350m parameters.","Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens).","To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory.","We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods.","We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set.","We make all our tools and data open source and available via Huggingface and Github."],"url":"http://arxiv.org/abs/2305.08286v1"}
{"created":"2023-05-15","title":"Quadratic Functional Encryption for Secure Training in Vertical Federated Learning","abstract":"Vertical federated learning (VFL) enables the collaborative training of machine learning (ML) models in settings where the data is distributed amongst multiple parties who wish to protect the privacy of their individual data. Notably, in VFL, the labels are available to a single party and the complete feature set is formed only when data from all parties is combined. Recently, Xu et al. proposed a new framework called FedV for secure gradient computation for VFL using multi-input functional encryption. In this work, we explain how some of the information leakage in Xu et al. can be avoided by using Quadratic functional encryption when training generalized linear models for vertical federated learning.","sentences":["Vertical federated learning (VFL) enables the collaborative training of machine learning (ML) models in settings where the data is distributed amongst multiple parties who wish to protect the privacy of their individual data.","Notably, in VFL, the labels are available to a single party and the complete feature set is formed only when data from all parties is combined.","Recently, Xu et al. proposed a new framework called FedV for secure gradient computation for VFL using multi-input functional encryption.","In this work, we explain how some of the information leakage in Xu et al. can be avoided by using Quadratic functional encryption when training generalized linear models for vertical federated learning."],"url":"http://arxiv.org/abs/2305.08358v1"}
