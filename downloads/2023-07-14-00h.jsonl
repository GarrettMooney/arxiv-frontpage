{"created":"2023-07-12 17:56:09","title":"Neural Free-Viewpoint Relighting for Glossy Indirect Illumination","abstract":"Precomputed Radiance Transfer (PRT) remains an attractive solution for real-time rendering of complex light transport effects such as glossy global illumination. After precomputation, we can relight the scene with new environment maps while changing viewpoint in real-time. However, practical PRT methods are usually limited to low-frequency spherical harmonic lighting. All-frequency techniques using wavelets are promising but have so far had little practical impact. The curse of dimensionality and much higher data requirements have typically limited them to relighting with fixed view or only direct lighting with triple product integrals. In this paper, we demonstrate a hybrid neural-wavelet PRT solution to high-frequency indirect illumination, including glossy reflection, for relighting with changing view. Specifically, we seek to represent the light transport function in the Haar wavelet basis. For global illumination, we learn the wavelet transport using a small multi-layer perceptron (MLP) applied to a feature field as a function of spatial location and wavelet index, with reflected direction and material parameters being other MLP inputs. We optimize/learn the feature field (compactly represented by a tensor decomposition) and MLP parameters from multiple images of the scene under different lighting and viewing conditions. We demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputed rendering of challenging scenes involving view-dependent reflections and even caustics.","sentences":["Precomputed Radiance Transfer (PRT) remains an attractive solution for real-time rendering of complex light transport effects such as glossy global illumination.","After precomputation, we can relight the scene with new environment maps while changing viewpoint in real-time.","However, practical PRT methods are usually limited to low-frequency spherical harmonic lighting.","All-frequency techniques using wavelets are promising but have so far had little practical impact.","The curse of dimensionality and much higher data requirements have typically limited them to relighting with fixed view or only direct lighting with triple product integrals.","In this paper, we demonstrate a hybrid neural-wavelet PRT solution to high-frequency indirect illumination, including glossy reflection, for relighting with changing view.","Specifically, we seek to represent the light transport function in the Haar wavelet basis.","For global illumination, we learn the wavelet transport using a small multi-layer perceptron (MLP) applied to a feature field as a function of spatial location and wavelet index, with reflected direction and material parameters being other MLP inputs.","We optimize/learn the feature field (compactly represented by a tensor decomposition) and MLP parameters from multiple images of the scene under different lighting and viewing conditions.","We demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputed rendering of challenging scenes involving view-dependent reflections and even caustics."],"url":"http://arxiv.org/abs/2307.06335v1"}
{"created":"2023-07-12 17:55:08","title":"Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation","abstract":"Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments. Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation. However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed. We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts. Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts. The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives. We present experiments validating our framework on discrete and continuous control tasks with real human users. Our method (1) enables users to better understand agent failure, (2) reduces the number of demonstrations required for fine-tuning, and (3) aligns the agent to individual user task preferences.","sentences":["Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments.","Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation.","However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed.","We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts.","Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts.","The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives.","We present experiments validating our framework on discrete and continuous control tasks with real human users.","Our method (1) enables users to better understand agent failure, (2) reduces the number of demonstrations required for fine-tuning, and (3) aligns the agent to individual user task preferences."],"url":"http://arxiv.org/abs/2307.06333v1"}
{"created":"2023-07-12 17:47:35","title":"Budgeting Counterfactual for Offline RL","abstract":"The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvement from taking out-of-distribution actions and the risk of making errors due to extrapolation. Theoretically, we justify our method by the constrained optimality of the fixed point solution to our $Q$ updating rules. Empirically, we show that the overall performance of our method is better than the state-of-the-art offline RL methods on tasks in the widely-used D4RL benchmarks.","sentences":["The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action?","These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon.","Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation.","Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training.","Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy.","It balances between the potential for improvement from taking out-of-distribution actions and the risk of making errors due to extrapolation.","Theoretically, we justify our method by the constrained optimality of the fixed point solution to our $Q$ updating rules.","Empirically, we show that the overall performance of our method is better than the state-of-the-art offline RL methods on tasks in the widely-used D4RL benchmarks."],"url":"http://arxiv.org/abs/2307.06328v1"}
{"created":"2023-07-12 17:40:21","title":"Information-Theoretically Private Federated Submodel Learning with Storage Constrained Databases","abstract":"In federated submodel learning (FSL), a machine learning model is divided into multiple submodels based on different types of data used for training. Each user involved in the training process only downloads and updates the submodel relevant to the user's local data, which significantly reduces the communication cost compared to classical federated learning (FL). However, the index of the submodel updated by the user and the values of the updates reveal information about the user's private data. In order to guarantee information-theoretic privacy in FSL, the model is stored at multiple non-colluding databases, and the user sends queries and updates to each database in such a way that no information is revealed on the updating submodel index or the values of the updates. In this work, we consider the practical scenario where the multiple non-colluding databases are allowed to have arbitrary storage constraints. The goal of this work is to develop read-write schemes and storage mechanisms for FSL that efficiently utilize the available storage in each database to store the submodel parameters in such a way that the total communication cost is minimized while guaranteeing information-theoretic privacy of the updating submodel index and the values of the updates. As the main result, we consider both heterogeneous and homogeneous storage constrained databases, and propose private read-write and storage schemes for the two cases.","sentences":["In federated submodel learning (FSL), a machine learning model is divided into multiple submodels based on different types of data used for training.","Each user involved in the training process only downloads and updates the submodel relevant to the user's local data, which significantly reduces the communication cost compared to classical federated learning (FL).","However, the index of the submodel updated by the user and the values of the updates reveal information about the user's private data.","In order to guarantee information-theoretic privacy in FSL, the model is stored at multiple non-colluding databases, and the user sends queries and updates to each database in such a way that no information is revealed on the updating submodel index or the values of the updates.","In this work, we consider the practical scenario where the multiple non-colluding databases are allowed to have arbitrary storage constraints.","The goal of this work is to develop read-write schemes and storage mechanisms for FSL that efficiently utilize the available storage in each database to store the submodel parameters in such a way that the total communication cost is minimized while guaranteeing information-theoretic privacy of the updating submodel index and the values of the updates.","As the main result, we consider both heterogeneous and homogeneous storage constrained databases, and propose private read-write and storage schemes for the two cases."],"url":"http://arxiv.org/abs/2307.06323v1"}
{"created":"2023-07-12 17:37:46","title":"Deep Learning of Crystalline Defects from TEM images: A Solution for the Problem of \"Never Enough Training Data\"","abstract":"Crystalline defects, such as line-like dislocations, play an important role for the performance and reliability of many metallic devices. Their interaction and evolution still poses a multitude of open questions to materials science and materials physics. In-situ TEM experiments can provide important insights into how dislocations behave and move. During such experiments, the dislocation microstructure is captured in form of videos. The analysis of individual video frames can provide useful insights but is limited by the capabilities of automated identification, digitization, and quantitative extraction of the dislocations as curved objects. The vast amount of data also makes manual annotation very time consuming, thereby limiting the use of Deep Learning-based, automated image analysis and segmentation of the dislocation microstructure. In this work, a parametric model for generating synthetic training data for segmentation of dislocations is developed. Even though domain scientists might dismiss synthetic training images sometimes as too artificial, our findings show that they can result in superior performance, particularly regarding the generalizing of the Deep Learning models with respect to different microstructures and imaging conditions. Additionally, we propose an enhanced deep learning method optimized for segmenting overlapping or intersecting dislocation lines. Upon testing this framework on four distinct real datasets, we find that our synthetic training data are able to yield high-quality results also on real images-even more so if fine-tune on a few real images was done.","sentences":["Crystalline defects, such as line-like dislocations, play an important role for the performance and reliability of many metallic devices.","Their interaction and evolution still poses a multitude of open questions to materials science and materials physics.","In-situ TEM experiments can provide important insights into how dislocations behave and move.","During such experiments, the dislocation microstructure is captured in form of videos.","The analysis of individual video frames can provide useful insights but is limited by the capabilities of automated identification, digitization, and quantitative extraction of the dislocations as curved objects.","The vast amount of data also makes manual annotation very time consuming, thereby limiting the use of Deep Learning-based, automated image analysis and segmentation of the dislocation microstructure.","In this work, a parametric model for generating synthetic training data for segmentation of dislocations is developed.","Even though domain scientists might dismiss synthetic training images sometimes as too artificial, our findings show that they can result in superior performance, particularly regarding the generalizing of the Deep Learning models with respect to different microstructures and imaging conditions.","Additionally, we propose an enhanced deep learning method optimized for segmenting overlapping or intersecting dislocation lines.","Upon testing this framework on four distinct real datasets, we find that our synthetic training data are able to yield high-quality results also on real images-even more so if fine-tune on a few real images was done."],"url":"http://arxiv.org/abs/2307.06322v1"}
{"created":"2023-07-12 17:29:28","title":"SAGE -- A Tool for Optimal Deployments in Kubernetes Clusters","abstract":"Cloud computing has brought a fundamental transformation in how organizations operate their applications, enabling them to achieve affordable high availability of services. Kubernetes has emerged as the preferred choice for container orchestration and service management across many Cloud computing platforms. The scheduler in Kubernetes plays a crucial role in determining the placement of newly deployed service containers. However, the default scheduler, while fast, often lacks optimization, leading to inefficient service placement or even deployment failures.   This paper introduces SAGE, a tool for optimal solutions in Kubernetes clusters that can also assist the Kubernetes default scheduler and any other custom scheduler in application deployment. SAGE computes an optimal deployment plan based on the constraints of the application to be deployed and the available Cloud resources. We show the potential benefits of using SAGE by considering test cases with various characteristics. It turns out that SAGE surpasses other schedulers by comprehensively analyzing the application demand and cluster image. This ability allows it to better understand the needs of the pods, resulting in consistently optimal solutions across all scenarios. The accompanying material of this paper is publicly available at https://github.com/SAGE-Project/SAGE-Predeployer.","sentences":["Cloud computing has brought a fundamental transformation in how organizations operate their applications, enabling them to achieve affordable high availability of services.","Kubernetes has emerged as the preferred choice for container orchestration and service management across many Cloud computing platforms.","The scheduler in Kubernetes plays a crucial role in determining the placement of newly deployed service containers.","However, the default scheduler, while fast, often lacks optimization, leading to inefficient service placement or even deployment failures.   ","This paper introduces SAGE, a tool for optimal solutions in Kubernetes clusters that can also assist the Kubernetes default scheduler and any other custom scheduler in application deployment.","SAGE computes an optimal deployment plan based on the constraints of the application to be deployed and the available Cloud resources.","We show the potential benefits of using SAGE by considering test cases with various characteristics.","It turns out that SAGE surpasses other schedulers by comprehensively analyzing the application demand and cluster image.","This ability allows it to better understand the needs of the pods, resulting in consistently optimal solutions across all scenarios.","The accompanying material of this paper is publicly available at https://github.com/SAGE-Project/SAGE-Predeployer."],"url":"http://arxiv.org/abs/2307.06318v1"}
{"created":"2023-07-12 17:20:05","title":"Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation","abstract":"Semi-supervised learning has become increasingly popular in medical image segmentation due to its ability to leverage large amounts of unlabeled data to extract additional information. However, most existing semi-supervised segmentation methods only focus on extracting information from unlabeled data, disregarding the potential of labeled data to further improve the performance of the model. In this paper, we propose a novel Correlation Aware Mutual Learning (CAML) framework that leverages labeled data to guide the extraction of information from unlabeled data. Our approach is based on a mutual learning strategy that incorporates two modules: the Cross-sample Mutual Attention Module (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module establishes dense cross-sample correlations among a group of samples, enabling the transfer of label prior knowledge to unlabeled data. The OCC module constructs omni-correlations between the unlabeled and labeled datasets and regularizes dual models by constraining the omni-correlation matrix of each sub-model to be consistent. Experiments on the Atrial Segmentation Challenge dataset demonstrate that our proposed approach outperforms state-of-the-art methods, highlighting the effectiveness of our framework in medical image segmentation tasks. The codes, pre-trained weights, and data are publicly available.","sentences":["Semi-supervised learning has become increasingly popular in medical image segmentation due to its ability to leverage large amounts of unlabeled data to extract additional information.","However, most existing semi-supervised segmentation methods only focus on extracting information from unlabeled data, disregarding the potential of labeled data to further improve the performance of the model.","In this paper, we propose a novel Correlation Aware Mutual Learning (CAML) framework that leverages labeled data to guide the extraction of information from unlabeled data.","Our approach is based on a mutual learning strategy that incorporates two modules: the Cross-sample Mutual Attention Module (CMA) and the Omni-Correlation Consistency Module (OCC).","The CMA module establishes dense cross-sample correlations among a group of samples, enabling the transfer of label prior knowledge to unlabeled data.","The OCC module constructs omni-correlations between the unlabeled and labeled datasets and regularizes dual models by constraining the omni-correlation matrix of each sub-model to be consistent.","Experiments on the Atrial Segmentation Challenge dataset demonstrate that our proposed approach outperforms state-of-the-art methods, highlighting the effectiveness of our framework in medical image segmentation tasks.","The codes, pre-trained weights, and data are publicly available."],"url":"http://arxiv.org/abs/2307.06312v1"}
{"created":"2023-07-12 17:09:18","title":"Facial Reenactment Through a Personalized Generator","abstract":"In recent years, the role of image generative models in facial reenactment has been steadily increasing. Such models are usually subject-agnostic and trained on domain-wide datasets. The appearance of the reenacted individual is learned from a single image, and hence, the entire breadth of the individual's appearance is not entirely captured, leading these methods to resort to unfaithful hallucination. Thanks to recent advancements, it is now possible to train a personalized generative model tailored specifically to a given individual. In this paper, we propose a novel method for facial reenactment using a personalized generator. We train the generator using frames from a short, yet varied, self-scan video captured using a simple commodity camera. Images synthesized by the personalized generator are guaranteed to preserve identity. The premise of our work is that the task of reenactment is thus reduced to accurately mimicking head poses and expressions. To this end, we locate the desired frames in the latent space of the personalized generator using carefully designed latent optimization. Through extensive evaluation, we demonstrate state-of-the-art performance for facial reenactment. Furthermore, we show that since our reenactment takes place in a semantic latent space, it can be semantically edited and stylized in post-processing.","sentences":["In recent years, the role of image generative models in facial reenactment has been steadily increasing.","Such models are usually subject-agnostic and trained on domain-wide datasets.","The appearance of the reenacted individual is learned from a single image, and hence, the entire breadth of the individual's appearance is not entirely captured, leading these methods to resort to unfaithful hallucination.","Thanks to recent advancements, it is now possible to train a personalized generative model tailored specifically to a given individual.","In this paper, we propose a novel method for facial reenactment using a personalized generator.","We train the generator using frames from a short, yet varied, self-scan video captured using a simple commodity camera.","Images synthesized by the personalized generator are guaranteed to preserve identity.","The premise of our work is that the task of reenactment is thus reduced to accurately mimicking head poses and expressions.","To this end, we locate the desired frames in the latent space of the personalized generator using carefully designed latent optimization.","Through extensive evaluation, we demonstrate state-of-the-art performance for facial reenactment.","Furthermore, we show that since our reenactment takes place in a semantic latent space, it can be semantically edited and stylized in post-processing."],"url":"http://arxiv.org/abs/2307.06307v1"}
{"created":"2023-07-12 17:02:32","title":"Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes","abstract":"State-of-the-art federated learning algorithms such as FedAvg require carefully tuned stepsizes to achieve their best performance. The improvements proposed by existing adaptive federated methods involve tuning of additional hyperparameters such as momentum parameters, and consider adaptivity only in the server aggregation round, but not locally. These methods can be inefficient in many practical scenarios because they require excessive tuning of hyperparameters and do not capture local geometric information. In this work, we extend the recently proposed stochastic Polyak stepsize (SPS) to the federated learning setting, and propose new locally adaptive and nearly parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that FedSPS converges linearly in strongly convex and sublinearly in convex settings when the interpolation condition (overparametrization) is satisfied, and converges to a neighborhood of the solution in the general case. We extend our proposed method to a decreasing stepsize version FedDecSPS, that converges also when the interpolation condition does not hold. We validate our theoretical claims by performing illustrative convex experiments. Our proposed algorithms match the optimization performance of FedAvg with the best tuned hyperparameters in the i.i.d. case, and outperform FedAvg in the non-i.i.d. case.","sentences":["State-of-the-art federated learning algorithms such as FedAvg require carefully tuned stepsizes to achieve their best performance.","The improvements proposed by existing adaptive federated methods involve tuning of additional hyperparameters such as momentum parameters, and consider adaptivity only in the server aggregation round, but not locally.","These methods can be inefficient in many practical scenarios because they require excessive tuning of hyperparameters and do not capture local geometric information.","In this work, we extend the recently proposed stochastic Polyak stepsize (SPS) to the federated learning setting, and propose new locally adaptive and nearly parameter-free distributed SPS variants (FedSPS and FedDecSPS).","We prove that FedSPS converges linearly in strongly convex and sublinearly in convex settings when the interpolation condition (overparametrization) is satisfied, and converges to a neighborhood of the solution in the general case.","We extend our proposed method to a decreasing stepsize version FedDecSPS, that converges also when the interpolation condition does not hold.","We validate our theoretical claims by performing illustrative convex experiments.","Our proposed algorithms match the optimization performance of FedAvg with the best tuned hyperparameters in the i.i.d. case, and outperform FedAvg in the non-i.i.d. case."],"url":"http://arxiv.org/abs/2307.06306v1"}
{"created":"2023-07-12 17:01:03","title":"Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution","abstract":"The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs.","sentences":["The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged.","However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths.","We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios.","Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining.","NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks.","At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off.","We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs."],"url":"http://arxiv.org/abs/2307.06304v1"}
{"created":"2023-07-12 16:53:52","title":"A Comparative Analysis Between the Additive and the Multiplicative Extended Kalman Filter for Satellite Attitude Determination","abstract":"The general consensus is that the Multiplicative Extended Kalman Filter (MEKF) is superior to the Additive Extended Kalman Filter (AEKF) based on a wealth of theoretical evidence. This paper deals with a practical comparison between the two filters in simulation with the goal of verifying if the previous theoretical foundations are true. The AEKF and MEKF are two variants of the Extended Kalman Filter that differ in their approach to linearizing the system dynamics. The AEKF uses an additive correction term to update the state estimate, while the MEKF uses a multiplicative correction term. The two also differ in the state of which they use. The AEKF uses the quaternion as its state while the MEKF uses the Gibbs vector as its state. The results show that the MEKF consistently outperforms the AEKF in terms of estimation accuracy with lower uncertainty. The AEKF is more computationally efficient, but the difference is so low that it is almost negligible and it has no effect on a real-time application. Overall, the results suggest that the MEKF is a better choise for satellite attitude estimation due to its superior estimation accuracy and lower uncertainty, which agrees with the statements from previous work","sentences":["The general consensus is that the Multiplicative Extended Kalman Filter (MEKF) is superior to the Additive Extended Kalman Filter (AEKF) based on a wealth of theoretical evidence.","This paper deals with a practical comparison between the two filters in simulation with the goal of verifying if the previous theoretical foundations are true.","The AEKF and MEKF are two variants of the Extended Kalman Filter that differ in their approach to linearizing the system dynamics.","The AEKF uses an additive correction term to update the state estimate, while the MEKF uses a multiplicative correction term.","The two also differ in the state of which they use.","The AEKF uses the quaternion as its state while the MEKF uses the Gibbs vector as its state.","The results show that the MEKF consistently outperforms the AEKF in terms of estimation accuracy with lower uncertainty.","The AEKF is more computationally efficient, but the difference is so low that it is almost negligible and it has no effect on a real-time application.","Overall, the results suggest that the MEKF is a better choise for satellite attitude estimation due to its superior estimation accuracy and lower uncertainty, which agrees with the statements from previous work"],"url":"http://arxiv.org/abs/2307.06300v1"}
{"created":"2023-07-12 16:53:32","title":"Towards a Certified Proof Checker for Deep Neural Network Verification","abstract":"Recent developments in deep neural networks (DNNs) have led to their adoption in safety-critical systems, which in turn has heightened the need for guaranteeing their safety. These safety properties of DNNs can be proven using tools developed by the verification community. However, these tools are themselves prone to implementation bugs and numerical stability problems, which make their reliability questionable. To overcome this, some verifiers produce proofs of their results which can be checked by a trusted checker. In this work, we present a novel implementation of a proof checker for DNN verification. It improves on existing implementations by offering numerical stability and greater verifiability. To achieve this, we leverage two key capabilities of Imandra, an industrial theorem prover: its support of infinite precision real arithmetic and its formal verification infrastructure. So far, we have implemented a proof checker in Imandra, specified its correctness properties and started to verify the checker's compliance with them. Our ongoing work focuses on completing the formal verification of the checker and further optimizing its performance.","sentences":["Recent developments in deep neural networks (DNNs) have led to their adoption in safety-critical systems, which in turn has heightened the need for guaranteeing their safety.","These safety properties of DNNs can be proven using tools developed by the verification community.","However, these tools are themselves prone to implementation bugs and numerical stability problems, which make their reliability questionable.","To overcome this, some verifiers produce proofs of their results which can be checked by a trusted checker.","In this work, we present a novel implementation of a proof checker for DNN verification.","It improves on existing implementations by offering numerical stability and greater verifiability.","To achieve this, we leverage two key capabilities of Imandra, an industrial theorem prover: its support of infinite precision real arithmetic and its formal verification infrastructure.","So far, we have implemented a proof checker in Imandra, specified its correctness properties and started to verify the checker's compliance with them.","Our ongoing work focuses on completing the formal verification of the checker and further optimizing its performance."],"url":"http://arxiv.org/abs/2307.06299v1"}
{"created":"2023-07-12 16:52:40","title":"Improved Real-time Image Smoothing with Weak Structures Preserved and High-contrast Details Removed","abstract":"Image smoothing is by reducing pixel-wise gradients to smooth out details. As existing methods always rely on gradients to determine smoothing manners, it is difficult to distinguish structures and details to handle distinctively due to the overlapped ranges of gradients for structures and details. Thus, it is still challenging to achieve high-quality results, especially on preserving weak structures and removing high-contrast details. In this paper, we address this challenge by improving the real-time optimization-based method via iterative least squares (called ILS). We observe that 1) ILS uses gradients as the independent variable in its penalty function for determining smoothing manners, and 2) the framework of ILS can still work for image smoothing when we use some values instead of gradients in the penalty function. Thus, corresponding to the properties of pixels on structures or not, we compute some values to use in the penalty function to determine smoothing manners, and so we can handle structures and details distinctively, no matter whether their gradients are high or low. As a result, we can conveniently remove high-contrast details while preserving weak structures. Moreover, such values can be adjusted to accelerate optimization computation, so that we can use fewer iterations than the original ILS method for efficiency. This also reduces the changes onto structures to help structure preservation. Experimental results show our advantages over existing methods on efficiency and quality.","sentences":["Image smoothing is by reducing pixel-wise gradients to smooth out details.","As existing methods always rely on gradients to determine smoothing manners, it is difficult to distinguish structures and details to handle distinctively due to the overlapped ranges of gradients for structures and details.","Thus, it is still challenging to achieve high-quality results, especially on preserving weak structures and removing high-contrast details.","In this paper, we address this challenge by improving the real-time optimization-based method via iterative least squares (called ILS).","We observe that 1) ILS uses gradients as the independent variable in its penalty function for determining smoothing manners, and 2) the framework of ILS can still work for image smoothing when we use some values instead of gradients in the penalty function.","Thus, corresponding to the properties of pixels on structures or not, we compute some values to use in the penalty function to determine smoothing manners, and so we can handle structures and details distinctively, no matter whether their gradients are high or low.","As a result, we can conveniently remove high-contrast details while preserving weak structures.","Moreover, such values can be adjusted to accelerate optimization computation, so that we can use fewer iterations than the original ILS method for efficiency.","This also reduces the changes onto structures to help structure preservation.","Experimental results show our advantages over existing methods on efficiency and quality."],"url":"http://arxiv.org/abs/2307.06298v1"}
{"created":"2023-07-12 16:42:44","title":"Corona: System Implications of Emerging Nanophotonic Technology","abstract":"We expect that many-core microprocessors will push performance per chip from the 10 gigaflop to the 10 teraflop range in the coming decade. To support this increased performance, memory and inter-core bandwidths will also have to scale by orders of magnitude. Pin limitations, the energy cost of electrical signaling, and the non-scalability of chip-length global wires are significant bandwidth impediments. Recent developments in silicon nanophotonic technology have the potential to meet these off- and on- stack bandwidth requirements at acceptable power levels.   Corona is a 3D many-core architecture that uses nanophotonic communication for both inter-core communication and off-stack communication to memory or I/O devices. Its peak floating-point performance is 10 teraflops. Dense wavelength division multiplexed optically connected memory modules provide 10 terabyte per second memory bandwidth. A photonic crossbar fully interconnects its 256 low-power multithreaded cores at 20 terabyte per second bandwidth. We have simulated a 1024 thread Corona system running synthetic benchmarks and scaled versions of the SPLASH-2 benchmark suite. We believe that in comparison with an electrically-connected many-core alternative that uses the same on-stack interconnect power, Corona can provide 2 to 6 times more performance on many memory-intensive workloads, while simultaneously reducing power.","sentences":["We expect that many-core microprocessors will push performance per chip from the 10 gigaflop to the 10 teraflop range in the coming decade.","To support this increased performance, memory and inter-core bandwidths will also have to scale by orders of magnitude.","Pin limitations, the energy cost of electrical signaling, and the non-scalability of chip-length global wires are significant bandwidth impediments.","Recent developments in silicon nanophotonic technology have the potential to meet these off- and on- stack bandwidth requirements at acceptable power levels.   ","Corona is a 3D many-core architecture that uses nanophotonic communication for both inter-core communication and off-stack communication to memory or I/O devices.","Its peak floating-point performance is 10 teraflops.","Dense wavelength division multiplexed optically connected memory modules provide 10 terabyte per second memory bandwidth.","A photonic crossbar fully interconnects its 256 low-power multithreaded cores at 20 terabyte per second bandwidth.","We have simulated a 1024 thread Corona system running synthetic benchmarks and scaled versions of the SPLASH-2 benchmark suite.","We believe that in comparison with an electrically-connected many-core alternative that uses the same on-stack interconnect power, Corona can provide 2 to 6 times more performance on many memory-intensive workloads, while simultaneously reducing power."],"url":"http://arxiv.org/abs/2307.06294v1"}
{"created":"2023-07-12 16:37:31","title":"Instruction Mining: High-Quality Instruction Data Selection for Large Language Models","abstract":"Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive finetuning experiments. The experiment results are then applied to estimating parameters in InstructMining. To further investigate its performance, we use InstructMining to select high-quality data from unseen datasets. Results demonstrate that InstructMining can help select relatively high-quality samples from various instruction-following datasets. Compared to models finetuned on unfiltered datasets, models finetuned on InstructMining selected datasets perform better on 42.5% cases.","sentences":["Large language models typically undergo two training stages, pretraining and finetuning.","Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times.","To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area.","Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data.","However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow.","In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality.","We formulate InstructMining using specific natural language indicators.","To investigate the relationship between data quality and these indicators, we further conduct extensive finetuning experiments.","The experiment results are then applied to estimating parameters in InstructMining.","To further investigate its performance, we use InstructMining to select high-quality data from unseen datasets.","Results demonstrate that InstructMining can help select relatively high-quality samples from various instruction-following datasets.","Compared to models finetuned on unfiltered datasets, models finetuned on InstructMining selected datasets perform better on 42.5% cases."],"url":"http://arxiv.org/abs/2307.06290v1"}
{"created":"2023-07-12 16:28:21","title":"Tackling Computational Heterogeneity in FL: A Few Theoretical Insights","abstract":"The future of machine learning lies in moving data collection along with training to the edge. Federated Learning, for short FL, has been recently proposed to achieve this goal. The principle of this approach is to aggregate models learned over a large number of distributed clients, i.e., resource-constrained mobile devices that collect data from their environment, to obtain a new more general model. The latter is subsequently redistributed to clients for further training. A key feature that distinguishes federated learning from data-center-based distributed training is the inherent heterogeneity. In this work, we introduce and analyse a novel aggregation framework that allows for formalizing and tackling computational heterogeneity in federated optimization, in terms of both heterogeneous data and local updates. Proposed aggregation algorithms are extensively analyzed from a theoretical, and an experimental prospective.","sentences":["The future of machine learning lies in moving data collection along with training to the edge.","Federated Learning, for short FL, has been recently proposed to achieve this goal.","The principle of this approach is to aggregate models learned over a large number of distributed clients, i.e., resource-constrained mobile devices that collect data from their environment, to obtain a new more general model.","The latter is subsequently redistributed to clients for further training.","A key feature that distinguishes federated learning from data-center-based distributed training is the inherent heterogeneity.","In this work, we introduce and analyse a novel aggregation framework that allows for formalizing and tackling computational heterogeneity in federated optimization, in terms of both heterogeneous data and local updates.","Proposed aggregation algorithms are extensively analyzed from a theoretical, and an experimental prospective."],"url":"http://arxiv.org/abs/2307.06283v1"}
{"created":"2023-07-12 16:23:09","title":"MMBench: Is Your Multi-modal Model an All-around Player?","abstract":"Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluation questions and abilities. The second element introduces a novel CircularEval strategy and incorporates the use of ChatGPT. This implementation is designed to convert free-form predictions into pre-defined choices, thereby facilitating a more robust evaluation of the model's predictions. MMBench is a systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models. We hope MMBench will assist the research community in better evaluating their models and encourage future advancements in this domain. Project page: https://opencompass.org.cn/mmbench.","sentences":["Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information.","However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development.","Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics.","Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias.","In response to these challenges, we propose MMBench, a novel multi-modality benchmark.","MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements.","The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluation questions and abilities.","The second element introduces a novel CircularEval strategy and incorporates the use of ChatGPT.","This implementation is designed to convert free-form predictions into pre-defined choices, thereby facilitating a more robust evaluation of the model's predictions.","MMBench is a systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models.","We hope MMBench will assist the research community in better evaluating their models and encourage future advancements in this domain.","Project page: https://opencompass.org.cn/mmbench."],"url":"http://arxiv.org/abs/2307.06281v1"}
{"created":"2023-07-12 16:20:08","title":"Stochastic Light Field Holography","abstract":"The Visual Turing Test is the ultimate goal to evaluate the realism of holographic displays. Previous studies have focused on addressing challenges such as limited \\'etendue and image quality over a large focal volume, but they have not investigated the effect of pupil sampling on the viewing experience in full 3D holograms. In this work, we tackle this problem with a novel hologram generation algorithm motivated by matching the projection operators of incoherent Light Field and coherent Wigner Function light transport. To this end, we supervise hologram computation using synthesized photographs, which are rendered on-the-fly using Light Field refocusing from stochastically sampled pupil states during optimization. The proposed method produces holograms with correct parallax and focus cues, which are important for passing the Visual Turing Test. We validate that our approach compares favorably to state-of-the-art CGH algorithms that use Light Field and Focal Stack supervision. Our experiments demonstrate that our algorithm significantly improves the realism of the viewing experience for a variety of different pupil states.","sentences":["The Visual Turing Test is the ultimate goal to evaluate the realism of holographic displays.","Previous studies have focused on addressing challenges such as limited \\'etendue and image quality over a large focal volume, but they have not investigated the effect of pupil sampling on the viewing experience in full 3D holograms.","In this work, we tackle this problem with a novel hologram generation algorithm motivated by matching the projection operators of incoherent Light Field and coherent Wigner Function light transport.","To this end, we supervise hologram computation using synthesized photographs, which are rendered on-the-fly using Light Field refocusing from stochastically sampled pupil states during optimization.","The proposed method produces holograms with correct parallax and focus cues, which are important for passing the Visual Turing Test.","We validate that our approach compares favorably to state-of-the-art CGH algorithms that use Light Field and Focal Stack supervision.","Our experiments demonstrate that our algorithm significantly improves the realism of the viewing experience for a variety of different pupil states."],"url":"http://arxiv.org/abs/2307.06277v1"}
{"created":"2023-07-12 16:19:31","title":"Connectivity Labeling for Multiple Vertex Failures","abstract":"We present an efficient labeling scheme for answering connectivity queries in graphs subject to a specified number of vertex failures. Our first result is a randomized construction of a labeling function that assigns vertices $O(f^3\\log^5 n)$-bit labels, such that given the labels of $F\\cup \\{s,t\\}$ where $|F|\\leq f$, we can correctly report, with probability $1-1/\\mathrm{poly}(n)$, whether $s$ and $t$ are connected in $G-F$. However, it is possible that over all $n^{O(f)}$ distinct queries, some are answered incorrectly. Our second result is a deterministic labeling function that produces $O(f^7 \\log^{13} n)$-bit labels such that all connectivity queries are answered correctly. Both upper bounds are polynomially off from an $\\Omega(f)$-bit lower bound.   Our labeling schemes are based on a new low degree decomposition that improves the Duan-Pettie decomposition, and facilitates its distributed representation. We make heavy use of randomization to construct hitting sets, fault-tolerant graph sparsifiers, and in constructing linear sketches. Our derandomized labeling scheme combines a variety of techniques: the method of conditional expectations, hit-miss hash families, and $\\epsilon$-nets for axis-aligned rectangles.   The prior labeling scheme of Parter and Petruschka shows that $f=1$ and $f=2$ vertex faults can be handled with $O(\\log n)$- and $O(\\log^3 n)$-bit labels, respectively, and for $f>2$ vertex faults, $\\tilde{O}(n^{1-1/2^{f-2}})$-bit labels suffice.","sentences":["We present an efficient labeling scheme for answering connectivity queries in graphs subject to a specified number of vertex failures.","Our first result is a randomized construction of a labeling function that assigns vertices $O(f^3\\log^5 n)$-bit labels, such that given the labels of $F\\cup \\{s,t\\}$ where $|F|\\leq f$, we can correctly report, with probability $1-1/\\mathrm{poly}(n)$, whether $s$ and $t$ are connected in $G-F$. However, it is possible that over all $n^{O(f)}$ distinct queries, some are answered incorrectly.","Our second result is a deterministic labeling function that produces $O(f^7 \\log^{13} n)$-bit labels such that all connectivity queries are answered correctly.","Both upper bounds are polynomially off from an $\\Omega(f)$-bit lower bound.   ","Our labeling schemes are based on a new low degree decomposition that improves the Duan-Pettie decomposition, and facilitates its distributed representation.","We make heavy use of randomization to construct hitting sets, fault-tolerant graph sparsifiers, and in constructing linear sketches.","Our derandomized labeling scheme combines a variety of techniques: the method of conditional expectations, hit-miss hash families, and $\\epsilon$-nets for axis-aligned rectangles.   ","The prior labeling scheme of Parter and Petruschka shows that $f=1$ and $f=2$ vertex faults can be handled with $O(\\log n)$- and $O(\\log^3 n)$-bit labels, respectively, and for $f>2$ vertex faults, $\\tilde{O}(n^{1-1/2^{f-2}})$-bit labels suffice."],"url":"http://arxiv.org/abs/2307.06276v1"}
{"created":"2023-07-12 16:16:37","title":"Exposing the Fake: Effective Diffusion-Generated Images Detection","abstract":"Image synthesis has seen significant advancements with the advent of diffusion-based generative models like Denoising Diffusion Probabilistic Models (DDPM) and text-to-image diffusion models. Despite their efficacy, there is a dearth of research dedicated to detecting diffusion-generated images, which could pose potential security and privacy risks. This paper addresses this gap by proposing a novel detection method called Stepwise Error for Diffusion-generated Image Detection (SeDID). Comprising statistical-based $\\text{SeDID}_{\\text{Stat}}$ and neural network-based $\\text{SeDID}_{\\text{NNs}}$, SeDID exploits the unique attributes of diffusion models, namely deterministic reverse and deterministic denoising computation errors. Our evaluations demonstrate SeDID's superior performance over existing methods when applied to diffusion models. Thus, our work makes a pivotal contribution to distinguishing diffusion model-generated images, marking a significant step in the domain of artificial intelligence security.","sentences":["Image synthesis has seen significant advancements with the advent of diffusion-based generative models like Denoising Diffusion Probabilistic Models (DDPM) and text-to-image diffusion models.","Despite their efficacy, there is a dearth of research dedicated to detecting diffusion-generated images, which could pose potential security and privacy risks.","This paper addresses this gap by proposing a novel detection method called Stepwise Error for Diffusion-generated Image Detection (SeDID).","Comprising statistical-based $\\text{SeDID}_{\\text{Stat}}$ and neural network-based $\\text{SeDID}_{\\text{NNs}}$, SeDID exploits the unique attributes of diffusion models, namely deterministic reverse and deterministic denoising computation errors.","Our evaluations demonstrate SeDID's superior performance over existing methods when applied to diffusion models.","Thus, our work makes a pivotal contribution to distinguishing diffusion model-generated images, marking a significant step in the domain of artificial intelligence security."],"url":"http://arxiv.org/abs/2307.06272v1"}
{"created":"2023-07-12 16:15:35","title":"Study on Virtual Gear Hobbing Simulation and Gear Tooth Surface Accuracy","abstract":"This paper presents a digital simulation method for the hobbing process of cylindrical gears. Based on the gear generation principle, taking the professional software as the tool, the problem of virtual hobbing simulation on involute helical gears was studied, and the virtual hobbing simulation of hobbing on the whole gear was completed by using macros of CATIA V5. The validity of this method was validated by analyzing the tooth surface accuracy error of the model which was below 0.001 mm between the virtual tooth surface and the theoretical tooth surface and the possible factors that affected the tooth surface accuracy during manufacturing were also carried on the discussion. It offers a fictitious three-D platform for studying the principle of manufacture errors of a gear-cutting machine as well as the finite element analysis between the ideal tooth surface and the erroneous tooth surface.","sentences":["This paper presents a digital simulation method for the hobbing process of cylindrical gears.","Based on the gear generation principle, taking the professional software as the tool, the problem of virtual hobbing simulation on involute helical gears was studied, and the virtual hobbing simulation of hobbing on the whole gear was completed by using macros of CATIA V5.","The validity of this method was validated by analyzing the tooth surface accuracy error of the model which was below 0.001 mm between the virtual tooth surface and the theoretical tooth surface and the possible factors that affected the tooth surface accuracy during manufacturing were also carried on the discussion.","It offers a fictitious three-D platform for studying the principle of manufacture errors of a gear-cutting machine as well as the finite element analysis between the ideal tooth surface and the erroneous tooth surface."],"url":"http://arxiv.org/abs/2307.06270v1"}
{"created":"2023-07-12 16:11:57","title":"Physics-informed Machine Learning for Calibrating Macroscopic Traffic Flow Models","abstract":"Well-calibrated traffic flow models are fundamental to understanding traffic phenomena and designing control strategies. Traditional calibration has been developed base on optimization methods. In this paper, we propose a novel physics-informed, learning-based calibration approach that achieves performances comparable to and even better than those of optimization-based methods. To this end, we combine the classical deep autoencoder, an unsupervised machine learning model consisting of one encoder and one decoder, with traffic flow models. Our approach informs the decoder of the physical traffic flow models and thus induces the encoder to yield reasonable traffic parameters given flow and speed measurements. We also introduce the denoising autoencoder into our method so that it can handles not only with normal data but also with corrupted data with missing values. We verified our approach with a case study of I-210 E in California.","sentences":["Well-calibrated traffic flow models are fundamental to understanding traffic phenomena and designing control strategies.","Traditional calibration has been developed base on optimization methods.","In this paper, we propose a novel physics-informed, learning-based calibration approach that achieves performances comparable to and even better than those of optimization-based methods.","To this end, we combine the classical deep autoencoder, an unsupervised machine learning model consisting of one encoder and one decoder, with traffic flow models.","Our approach informs the decoder of the physical traffic flow models and thus induces the encoder to yield reasonable traffic parameters given flow and speed measurements.","We also introduce the denoising autoencoder into our method so that it can handles not only with normal data but also with corrupted data with missing values.","We verified our approach with a case study of I-210 E in California."],"url":"http://arxiv.org/abs/2307.06267v1"}
{"created":"2023-07-12 16:08:15","title":"Towards a privacy-preserving distributed cloud service for preprocessing very large medical images","abstract":"Digitized histopathology glass slides, known as Whole Slide Images (WSIs), are often several gigapixels large and contain sensitive metadata information, which makes distributed processing unfeasible. Moreover, artifacts in WSIs may result in unreliable predictions when directly applied by Deep Learning (DL) algorithms. Therefore, preprocessing WSIs is beneficial, e.g., eliminating privacy-sensitive information, splitting a gigapixel medical image into tiles, and removing the diagnostically irrelevant areas. This work proposes a cloud service to parallelize the preprocessing pipeline for large medical images. The data and model parallelization will not only boost the end-to-end processing efficiency for histological tasks but also secure the reconstruction of WSI by randomly distributing tiles across processing nodes. Furthermore, the initial steps of the pipeline will be integrated into the Jupyter-based Virtual Research Environment (VRE) to enable image owners to configure and automate the execution process based on resource allocation.","sentences":["Digitized histopathology glass slides, known as Whole Slide Images (WSIs), are often several gigapixels large and contain sensitive metadata information, which makes distributed processing unfeasible.","Moreover, artifacts in WSIs may result in unreliable predictions when directly applied by Deep Learning (DL) algorithms.","Therefore, preprocessing WSIs is beneficial, e.g., eliminating privacy-sensitive information, splitting a gigapixel medical image into tiles, and removing the diagnostically irrelevant areas.","This work proposes a cloud service to parallelize the preprocessing pipeline for large medical images.","The data and model parallelization will not only boost the end-to-end processing efficiency for histological tasks but also secure the reconstruction of WSI by randomly distributing tiles across processing nodes.","Furthermore, the initial steps of the pipeline will be integrated into the Jupyter-based Virtual Research Environment (VRE) to enable image owners to configure and automate the execution process based on resource allocation."],"url":"http://arxiv.org/abs/2307.06266v1"}
{"created":"2023-07-12 16:03:34","title":"On the hierarchical Bayesian modelling of frequency response functions","abstract":"Population-based structural health monitoring (PBSHM) aims to share valuable information among members of a population, such as normal- and damage-condition data, to improve inferences regarding the health states of the members. Even when the population is comprised of nominally-identical structures, benign variations among the members will exist as a result of slight differences in material properties, geometry, boundary conditions, or environmental effects (e.g., temperature changes). These discrepancies can affect modal properties and present as changes in the characteristics of the resonance peaks of the frequency response function (FRF). Many SHM strategies depend on monitoring the dynamic properties of structures, so benign variations can be challenging for the practical implementation of these systems. Another common challenge with vibration-based SHM is data loss, which may result from transmission issues, sensor failure, a sample-rate mismatch between sensors, and other causes. Missing data in the time domain will result in decreased resolution in the frequency domain, which can impair dynamic characterisation. The hierarchical Bayesian approach provides a useful modelling structure for PBSHM, because statistical distributions at the population and individual (or domain) level are learnt simultaneously to bolster statistical strength among the parameters. As a result, variance is reduced among the parameter estimates, particularly when data are limited. In this paper, combined probabilistic FRF models are developed for a small population of nominally-identical helicopter blades under varying temperature conditions, using a hierarchical Bayesian structure. These models address critical challenges in SHM, by accommodating benign variations that present as differences in the underlying dynamics, while also considering (and utilising), the similarities among the blades.","sentences":["Population-based structural health monitoring (PBSHM) aims to share valuable information among members of a population, such as normal- and damage-condition data, to improve inferences regarding the health states of the members.","Even when the population is comprised of nominally-identical structures, benign variations among the members will exist as a result of slight differences in material properties, geometry, boundary conditions, or environmental effects (e.g., temperature changes).","These discrepancies can affect modal properties and present as changes in the characteristics of the resonance peaks of the frequency response function (FRF).","Many SHM strategies depend on monitoring the dynamic properties of structures, so benign variations can be challenging for the practical implementation of these systems.","Another common challenge with vibration-based SHM is data loss, which may result from transmission issues, sensor failure, a sample-rate mismatch between sensors, and other causes.","Missing data in the time domain will result in decreased resolution in the frequency domain, which can impair dynamic characterisation.","The hierarchical Bayesian approach provides a useful modelling structure for PBSHM, because statistical distributions at the population and individual (or domain) level are learnt simultaneously to bolster statistical strength among the parameters.","As a result, variance is reduced among the parameter estimates, particularly when data are limited.","In this paper, combined probabilistic FRF models are developed for a small population of nominally-identical helicopter blades under varying temperature conditions, using a hierarchical Bayesian structure.","These models address critical challenges in SHM, by accommodating benign variations that present as differences in the underlying dynamics, while also considering (and utilising), the similarities among the blades."],"url":"http://arxiv.org/abs/2307.06263v1"}
{"created":"2023-07-12 16:02:56","title":"An Architecture for Control Plane Slicing in Beyond 5G Networks","abstract":"To accommodate various use cases with differing characteristics, the Fifth Generation (5G) mobile communications system intends to utilize network slicing. Network slicing enables the creation of multiple logical networks over a shared physical network infrastructure. While the problems such as resource allocation for multiple slices in mobile networks have been explored in considerable detail in the existing literature, the suitability of the existing mobile network architecture to support network slicing has not been analysed adequately. We think the existing 5G System (5GS) architecture suffers from certain limitations, such as a lack of slice isolation in its control plane. This work focuses on the future evolution of the existing 5GS architecture from a slicing perspective, especially that of its control plane, addressing some of the limitations of the existing 5GS architecture. We propose a new network architecture which enables efficient slicing in beyond 5G networks. The proposed architecture results in enhanced modularity and scalability of the control plane in sliced mobile networks. In addition, it also brings slice isolation to the control plane, which is not feasible in the existing 5G system. We also present a performance evaluation that confirms the improved performance and scalability of the proposed system viz a viz the existing 5G system.","sentences":["To accommodate various use cases with differing characteristics, the Fifth Generation (5G) mobile communications system intends to utilize network slicing.","Network slicing enables the creation of multiple logical networks over a shared physical network infrastructure.","While the problems such as resource allocation for multiple slices in mobile networks have been explored in considerable detail in the existing literature, the suitability of the existing mobile network architecture to support network slicing has not been analysed adequately.","We think the existing 5G System (5GS) architecture suffers from certain limitations, such as a lack of slice isolation in its control plane.","This work focuses on the future evolution of the existing 5GS architecture from a slicing perspective, especially that of its control plane, addressing some of the limitations of the existing 5GS architecture.","We propose a new network architecture which enables efficient slicing in beyond 5G networks.","The proposed architecture results in enhanced modularity and scalability of the control plane in sliced mobile networks.","In addition, it also brings slice isolation to the control plane, which is not feasible in the existing 5G system.","We also present a performance evaluation that confirms the improved performance and scalability of the proposed system viz a viz the existing 5G system."],"url":"http://arxiv.org/abs/2307.06262v1"}
{"created":"2023-07-12 16:02:31","title":"Cosserat-Rod Based Dynamic Modeling of Soft Slender Robot Interacting with Environment","abstract":"Soft slender robots have attracted more and more research attentions in these years due to their continuity and compliance natures. However, mechanics modeling for soft robots interacting with environment is still an academic challenge because of the non-linearity of deformation and the non-smooth property of the contacts. In this work, starting from a piece-wise local strain field assumption, we propose a nonlinear dynamic model for soft robot via Cosserat rod theory using Newtonian mechanics which handles the frictional contact with environment and transfer them into the nonlinear complementary constraint (NCP) formulation. Moreover, we smooth both the contact and friction constraints in order to convert the inequality equations of NCP to the smooth equality equations. The proposed model allows us to compute the dynamic deformation and frictional contact force under common optimization framework in real time when the soft slender robot interacts with other rigid or soft bodies. In the end, the corresponding experiments are carried out which valid our proposed dynamic model.","sentences":["Soft slender robots have attracted more and more research attentions in these years due to their continuity and compliance natures.","However, mechanics modeling for soft robots interacting with environment is still an academic challenge because of the non-linearity of deformation and the non-smooth property of the contacts.","In this work, starting from a piece-wise local strain field assumption, we propose a nonlinear dynamic model for soft robot via Cosserat rod theory using Newtonian mechanics which handles the frictional contact with environment and transfer them into the nonlinear complementary constraint (NCP) formulation.","Moreover, we smooth both the contact and friction constraints in order to convert the inequality equations of NCP to the smooth equality equations.","The proposed model allows us to compute the dynamic deformation and frictional contact force under common optimization framework in real time when the soft slender robot interacts with other rigid or soft bodies.","In the end, the corresponding experiments are carried out which valid our proposed dynamic model."],"url":"http://arxiv.org/abs/2307.06261v1"}
{"created":"2023-07-12 16:01:56","title":"UGCANet: A Unified Global Context-Aware Transformer-based Network with Feature Alignment for Endoscopic Image Analysis","abstract":"Gastrointestinal endoscopy is a medical procedure that utilizes a flexible tube equipped with a camera and other instruments to examine the digestive tract. This minimally invasive technique allows for diagnosing and managing various gastrointestinal conditions, including inflammatory bowel disease, gastrointestinal bleeding, and colon cancer. The early detection and identification of lesions in the upper gastrointestinal tract and the identification of malignant polyps that may pose a risk of cancer development are critical components of gastrointestinal endoscopy's diagnostic and therapeutic applications. Therefore, enhancing the detection rates of gastrointestinal disorders can significantly improve a patient's prognosis by increasing the likelihood of timely medical intervention, which may prolong the patient's lifespan and improve overall health outcomes. This paper presents a novel Transformer-based deep neural network designed to perform multiple tasks simultaneously, thereby enabling accurate identification of both upper gastrointestinal tract lesions and colon polyps. Our approach proposes a unique global context-aware module and leverages the powerful MiT backbone, along with a feature alignment block, to enhance the network's representation capability. This novel design leads to a significant improvement in performance across various endoscopic diagnosis tasks. Extensive experiments demonstrate the superior performance of our method compared to other state-of-the-art approaches.","sentences":["Gastrointestinal endoscopy is a medical procedure that utilizes a flexible tube equipped with a camera and other instruments to examine the digestive tract.","This minimally invasive technique allows for diagnosing and managing various gastrointestinal conditions, including inflammatory bowel disease, gastrointestinal bleeding, and colon cancer.","The early detection and identification of lesions in the upper gastrointestinal tract and the identification of malignant polyps that may pose a risk of cancer development are critical components of gastrointestinal endoscopy's diagnostic and therapeutic applications.","Therefore, enhancing the detection rates of gastrointestinal disorders can significantly improve a patient's prognosis by increasing the likelihood of timely medical intervention, which may prolong the patient's lifespan and improve overall health outcomes.","This paper presents a novel Transformer-based deep neural network designed to perform multiple tasks simultaneously, thereby enabling accurate identification of both upper gastrointestinal tract lesions and colon polyps.","Our approach proposes a unique global context-aware module and leverages the powerful MiT backbone, along with a feature alignment block, to enhance the network's representation capability.","This novel design leads to a significant improvement in performance across various endoscopic diagnosis tasks.","Extensive experiments demonstrate the superior performance of our method compared to other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2307.06260v1"}
{"created":"2023-07-12 15:55:48","title":"Connected Dependability Cage Approach for Safe Automated Driving","abstract":"Automated driving systems can be helpful in a wide range of societal challenges, e.g., mobility-on-demand and transportation logistics for last-mile delivery, by aiding the vehicle driver or taking over the responsibility for the dynamic driving task partially or completely. Ensuring the safety of automated driving systems is no trivial task, even more so for those systems of SAE Level 3 or above. To achieve this, mechanisms are needed that can continuously monitor the system's operating conditions, also denoted as the system's operational design domain. This paper presents a safety concept for automated driving systems which uses a combination of onboard runtime monitoring via connected dependability cage and off-board runtime monitoring via a remote command control center, to continuously monitor the system's ODD. On one side, the connected dependability cage fulfills a double functionality: (1) to monitor continuously the operational design domain of the automated driving system, and (2) to transfer the responsibility in a smooth and safe manner between the automated driving system and the off-board remote safety driver, who is present in the remote command control center. On the other side, the remote command control center enables the remote safety driver the monitoring and takeover of the vehicle's control. We evaluate our safety concept for automated driving systems in a lab environment and on a test field track and report on results and lessons learned.","sentences":["Automated driving systems can be helpful in a wide range of societal challenges, e.g., mobility-on-demand and transportation logistics for last-mile delivery, by aiding the vehicle driver or taking over the responsibility for the dynamic driving task partially or completely.","Ensuring the safety of automated driving systems is no trivial task, even more so for those systems of SAE Level 3 or above.","To achieve this, mechanisms are needed that can continuously monitor the system's operating conditions, also denoted as the system's operational design domain.","This paper presents a safety concept for automated driving systems which uses a combination of onboard runtime monitoring via connected dependability cage and off-board runtime monitoring via a remote command control center, to continuously monitor the system's ODD.","On one side, the connected dependability cage fulfills a double functionality: (1) to monitor continuously the operational design domain of the automated driving system, and (2) to transfer the responsibility in a smooth and safe manner between the automated driving system and the off-board remote safety driver, who is present in the remote command control center.","On the other side, the remote command control center enables the remote safety driver the monitoring and takeover of the vehicle's control.","We evaluate our safety concept for automated driving systems in a lab environment and on a test field track and report on results and lessons learned."],"url":"http://arxiv.org/abs/2307.06258v1"}
{"created":"2023-07-12 15:50:38","title":"Machine learning and Topological data analysis identify unique features of human papillae in 3D scans","abstract":"The tongue surface houses a range of papillae that are integral to the mechanics and chemistry of taste and textural sensation. Although gustatory function of papillae is well investigated, the uniqueness of papillae within and across individuals remains elusive. Here, we present the first machine learning framework on 3D microscopic scans of human papillae (n = 2092), uncovering the uniqueness of geometric and topological features of papillae. The finer differences in shapes of papillae are investigated computationally based on a number of features derived from discrete differential geometry and computational topology. Interpretable machine learning techniques show that persistent homology features of the papillae shape are the most effective in predicting the biological variables. Models trained on these features with small volumes of data samples predict the type of papillae with an accuracy of 85%. The papillae type classification models can map the spatial arrangement of filiform and fungiform papillae on a surface. Remarkably, the papillae are found to be distinctive across individuals and an individual can be identified with an accuracy of 48% among the 15 participants from a single papillae. Collectively, this is the first unprecedented evidence demonstrating that tongue papillae can serve as a unique identifier inspiring new research direction for food preferences and oral diagnostics.","sentences":["The tongue surface houses a range of papillae that are integral to the mechanics and chemistry of taste and textural sensation.","Although gustatory function of papillae is well investigated, the uniqueness of papillae within and across individuals remains elusive.","Here, we present the first machine learning framework on 3D microscopic scans of human papillae (n = 2092), uncovering the uniqueness of geometric and topological features of papillae.","The finer differences in shapes of papillae are investigated computationally based on a number of features derived from discrete differential geometry and computational topology.","Interpretable machine learning techniques show that persistent homology features of the papillae shape are the most effective in predicting the biological variables.","Models trained on these features with small volumes of data samples predict the type of papillae with an accuracy of 85%.","The papillae type classification models can map the spatial arrangement of filiform and fungiform papillae on a surface.","Remarkably, the papillae are found to be distinctive across individuals and an individual can be identified with an accuracy of 48% among the 15 participants from a single papillae.","Collectively, this is the first unprecedented evidence demonstrating that tongue papillae can serve as a unique identifier inspiring new research direction for food preferences and oral diagnostics."],"url":"http://arxiv.org/abs/2307.06255v1"}
{"created":"2023-07-12 15:34:39","title":"Diffusion Based Multi-Agent Adversarial Tracking","abstract":"Target tracking plays a crucial role in real-world scenarios, particularly in drug-trafficking interdiction, where the knowledge of an adversarial target's location is often limited. Improving autonomous tracking systems will enable unmanned aerial, surface, and underwater vehicles to better assist in interdicting smugglers that use manned surface, semi-submersible, and aerial vessels. As unmanned drones proliferate, accurate autonomous target estimation is even more crucial for security and safety. This paper presents Constrained Agent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach aimed at generating comprehensive predictions of adversary locations by leveraging past sparse state information. To assess the effectiveness of this approach, we evaluate predictions on single-target and multi-target pursuit environments, employing Monte-Carlo sampling of the diffusion model to estimate the probability associated with each generated trajectory. We propose a novel cross-attention based diffusion model that utilizes constraint-based sampling to generate multimodal track hypotheses. Our single-target model surpasses the performance of all baseline methods on Average Displacement Error (ADE) for predictions across all time horizons.","sentences":["Target tracking plays a crucial role in real-world scenarios, particularly in drug-trafficking interdiction, where the knowledge of an adversarial target's location is often limited.","Improving autonomous tracking systems will enable unmanned aerial, surface, and underwater vehicles to better assist in interdicting smugglers that use manned surface, semi-submersible, and aerial vessels.","As unmanned drones proliferate, accurate autonomous target estimation is even more crucial for security and safety.","This paper presents Constrained Agent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach aimed at generating comprehensive predictions of adversary locations by leveraging past sparse state information.","To assess the effectiveness of this approach, we evaluate predictions on single-target and multi-target pursuit environments, employing Monte-Carlo sampling of the diffusion model to estimate the probability associated with each generated trajectory.","We propose a novel cross-attention based diffusion model that utilizes constraint-based sampling to generate multimodal track hypotheses.","Our single-target model surpasses the performance of all baseline methods on Average Displacement Error (ADE) for predictions across all time horizons."],"url":"http://arxiv.org/abs/2307.06244v1"}
{"created":"2023-07-12 15:34:10","title":"Reconstructing Spatiotemporal Data with C-VAEs","abstract":"The continuous representation of spatiotemporal data commonly relies on using abstract data types, such as \\textit{moving regions}, to represent entities whose shape and position continuously change over time. Creating this representation from discrete snapshots of real-world entities requires using interpolation methods to compute in-between data representations and estimate the position and shape of the object of interest at arbitrary temporal points. Existing region interpolation methods often fail to generate smooth and realistic representations of a region's evolution. However, recent advancements in deep learning techniques have revealed the potential of deep models trained on discrete observations to capture spatiotemporal dependencies through implicit feature learning.   In this work, we explore the capabilities of Conditional Variational Autoencoder (C-VAE) models to generate smooth and realistic representations of the spatiotemporal evolution of moving regions. We evaluate our proposed approach on a sparsely annotated dataset on the burnt area of a forest fire. We apply compression operations to sample from the dataset and use the C-VAE model and other commonly used interpolation algorithms to generate in-between region representations. To evaluate the performance of the methods, we compare their interpolation results with manually annotated data and regions generated by a U-Net model. We also assess the quality of generated data considering temporal consistency metrics.   The proposed C-VAE-based approach demonstrates competitive results in geometric similarity metrics. It also exhibits superior temporal consistency, suggesting that C-VAE models may be a viable alternative to modelling the spatiotemporal evolution of 2D moving regions.","sentences":["The continuous representation of spatiotemporal data commonly relies on using abstract data types, such as \\textit{moving regions}, to represent entities whose shape and position continuously change over time.","Creating this representation from discrete snapshots of real-world entities requires using interpolation methods to compute in-between data representations and estimate the position and shape of the object of interest at arbitrary temporal points.","Existing region interpolation methods often fail to generate smooth and realistic representations of a region's evolution.","However, recent advancements in deep learning techniques have revealed the potential of deep models trained on discrete observations to capture spatiotemporal dependencies through implicit feature learning.   ","In this work, we explore the capabilities of Conditional Variational Autoencoder (C-VAE) models to generate smooth and realistic representations of the spatiotemporal evolution of moving regions.","We evaluate our proposed approach on a sparsely annotated dataset on the burnt area of a forest fire.","We apply compression operations to sample from the dataset and use the C-VAE model and other commonly used interpolation algorithms to generate in-between region representations.","To evaluate the performance of the methods, we compare their interpolation results with manually annotated data and regions generated by a U-Net model.","We also assess the quality of generated data considering temporal consistency metrics.   ","The proposed C-VAE-based approach demonstrates competitive results in geometric similarity metrics.","It also exhibits superior temporal consistency, suggesting that C-VAE models may be a viable alternative to modelling the spatiotemporal evolution of 2D moving regions."],"url":"http://arxiv.org/abs/2307.06243v1"}
{"created":"2023-07-12 15:31:20","title":"New Three and Four-Dimensional Toric and Burst-Error-Correcting Quantum Codes","abstract":"Ongoing research and experiments have enabled quantum memory to realize the storage of qubits. On the other hand, interleaving techniques are used to deal with burst of errors. Effective interleaving techniques for combating burst of errors by using classical error-correcting codes have been proposed in several articles found in the literature, however, to the best of our knowledge, little is known regarding interleaving techniques for combating clusters of errors in topological quantum error-correcting codes. Motivated by that, in this work, we present new three and four-dimensional toric quantum codes which are featured by lattice codes and apply a quantum interleaving method to such new three and four-dimensional toric quantum codes. By applying such a method to these new codes we provide new three and four-dimensional quantum burst-error-correcting codes. As a consequence, new three and four-dimensional toric and burst-error-correcting quantum codes are obtained which have better information rates than those three and four-dimensional toric quantum codes from the literature. In addition to these proposed three and four-dimensional quantum burst-error-correcting codes improve such information rates, they can be used for burst-error-correction in errors which are located, quantum data stored and quantum channels with memory.","sentences":["Ongoing research and experiments have enabled quantum memory to realize the storage of qubits.","On the other hand, interleaving techniques are used to deal with burst of errors.","Effective interleaving techniques for combating burst of errors by using classical error-correcting codes have been proposed in several articles found in the literature, however, to the best of our knowledge, little is known regarding interleaving techniques for combating clusters of errors in topological quantum error-correcting codes.","Motivated by that, in this work, we present new three and four-dimensional toric quantum codes which are featured by lattice codes and apply a quantum interleaving method to such new three and four-dimensional toric quantum codes.","By applying such a method to these new codes we provide new three and four-dimensional quantum burst-error-correcting codes.","As a consequence, new three and four-dimensional toric and burst-error-correcting quantum codes are obtained which have better information rates than those three and four-dimensional toric quantum codes from the literature.","In addition to these proposed three and four-dimensional quantum burst-error-correcting codes improve such information rates, they can be used for burst-error-correction in errors which are located, quantum data stored and quantum channels with memory."],"url":"http://arxiv.org/abs/2307.06241v1"}
{"created":"2023-07-12 15:28:26","title":"DSSE: a drone swarm search environment","abstract":"The Drone Swarm Search project is an environment, based on PettingZoo, that is to be used in conjunction with multi-agent (or single-agent) reinforcement learning algorithms. It is an environment in which the agents (drones), have to find the targets (shipwrecked people). The agents do not know the position of the target and do not receive rewards related to their own distance to the target(s). However, the agents receive the probabilities of the target(s) being in a certain cell of the map. The aim of this project is to aid in the study of reinforcement learning algorithms that require dynamic probabilities as inputs.","sentences":["The Drone Swarm Search project is an environment, based on PettingZoo, that is to be used in conjunction with multi-agent (or single-agent) reinforcement learning algorithms.","It is an environment in which the agents (drones), have to find the targets (shipwrecked people).","The agents do not know the position of the target and do not receive rewards related to their own distance to the target(s).","However, the agents receive the probabilities of the target(s) being in a certain cell of the map.","The aim of this project is to aid in the study of reinforcement learning algorithms that require dynamic probabilities as inputs."],"url":"http://arxiv.org/abs/2307.06240v1"}
{"created":"2023-07-12 15:27:06","title":"Unified Molecular Modeling via Modality Blending","abstract":"Self-supervised molecular representation learning is critical for molecule-based tasks such as AI-assisted drug discovery. Recent studies consider leveraging both 2D and 3D information for representation learning, with straightforward alignment strategies that treat each modality separately. In this work, we introduce a novel \"blend-then-predict\" self-supervised learning method (MoleBLEND), which blends atom relations from different modalities into one unified relation matrix for encoding, then recovers modality-specific information for both 2D and 3D structures. By treating atom relationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned and integrated at fine-grained relation-level organically. Extensive experiments show that MoleBLEND achieves state-of-the-art performance across major 2D/3D benchmarks. We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (inter-modal prediction) and mask-then-predict (intra-modal prediction) objectives into a single cohesive blend-then-predict framework.","sentences":["Self-supervised molecular representation learning is critical for molecule-based tasks such as AI-assisted drug discovery.","Recent studies consider leveraging both 2D and 3D information for representation learning, with straightforward alignment strategies that treat each modality separately.","In this work, we introduce a novel \"blend-then-predict\" self-supervised learning method (MoleBLEND), which blends atom relations from different modalities into one unified relation matrix for encoding, then recovers modality-specific information for both 2D and 3D structures.","By treating atom relationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned and integrated at fine-grained relation-level organically.","Extensive experiments show that MoleBLEND achieves state-of-the-art performance across major 2D/3D benchmarks.","We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (inter-modal prediction) and mask-then-predict (intra-modal prediction) objectives into a single cohesive blend-then-predict framework."],"url":"http://arxiv.org/abs/2307.06235v1"}
{"created":"2023-07-12 15:07:16","title":"Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches","abstract":"Poetry holds immense significance within the cultural and traditional fabric of any nation. It serves as a vehicle for poets to articulate their emotions, preserve customs, and convey the essence of their culture. Arabic poetry is no exception, having played a cherished role in the heritage of the Arabic community throughout history and maintaining its relevance in the present era. Typically, comprehending Arabic poetry necessitates the expertise of a linguist who can analyze its content and assess its quality. This paper presents the introduction of a framework called \\textit{Ashaar} https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and pre-trained models designed specifically for the analysis and generation of Arabic poetry. The pipeline established within our proposed approach encompasses various aspects of poetry, such as meter, theme, and era classification. It also incorporates automatic poetry diacritization, enabling more intricate analyses like automated extraction of the \\textit{Arudi} style. Additionally, we explore the feasibility of generating conditional poetry through the pre-training of a character-based GPT model. Furthermore, as part of this endeavor, we provide four datasets: one for poetry generation, another for diacritization, and two for Arudi-style prediction. These datasets aim to facilitate research and development in the field of Arabic poetry by enabling researchers and enthusiasts to delve into the nuances of this rich literary tradition.","sentences":["Poetry holds immense significance within the cultural and traditional fabric of any nation.","It serves as a vehicle for poets to articulate their emotions, preserve customs, and convey the essence of their culture.","Arabic poetry is no exception, having played a cherished role in the heritage of the Arabic community throughout history and maintaining its relevance in the present era.","Typically, comprehending Arabic poetry necessitates the expertise of a linguist who can analyze its content and assess its quality.","This paper presents the introduction of a framework called \\textit{Ashaar} https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and pre-trained models designed specifically for the analysis and generation of Arabic poetry.","The pipeline established within our proposed approach encompasses various aspects of poetry, such as meter, theme, and era classification.","It also incorporates automatic poetry diacritization, enabling more intricate analyses like automated extraction of the \\textit{Arudi} style.","Additionally, we explore the feasibility of generating conditional poetry through the pre-training of a character-based GPT model.","Furthermore, as part of this endeavor, we provide four datasets: one for poetry generation, another for diacritization, and two for Arudi-style prediction.","These datasets aim to facilitate research and development in the field of Arabic poetry by enabling researchers and enthusiasts to delve into the nuances of this rich literary tradition."],"url":"http://arxiv.org/abs/2307.06218v1"}
{"created":"2023-07-12 15:02:53","title":"52 Weeks Later: Attitudes Towards COVID-19 Apps for Different Purposes Over Time","abstract":"The COVID-19 pandemic has prompted countries around the world to introduce smartphone apps to support disease control efforts. Their purposes range from digital contact tracing to quarantine enforcement to vaccination passports, and their effectiveness often depends on widespread adoption. While previous work has identified factors that promote or hinder adoption, it has typically examined data collected at a single point in time or focused exclusively on digital contact tracing apps. In this work, we conduct the first representative study that examines changes in people's attitudes towards COVID-19-related smartphone apps for five different purposes over the first 1.5 years of the pandemic. In three survey rounds conducted between Summer 2020 and Summer 2021 in the United States and Germany, with approximately 1,000 participants per round and country, we investigate people's willingness to use such apps, their perceived utility, and people's attitudes towards them in different stages of the pandemic. Our results indicate that privacy is a consistent concern for participants, even in a public health crisis, and the collection of identity-related data significantly decreases acceptance of COVID-19 apps. Trust in authorities is essential to increase confidence in government-backed apps and foster citizens' willingness to contribute to crisis management. There is a need for continuous communication with app users to emphasize the benefits of health crisis apps both for individuals and society, thus counteracting decreasing willingness to use them and perceived usefulness as the pandemic evolves.","sentences":["The COVID-19 pandemic has prompted countries around the world to introduce smartphone apps to support disease control efforts.","Their purposes range from digital contact tracing to quarantine enforcement to vaccination passports, and their effectiveness often depends on widespread adoption.","While previous work has identified factors that promote or hinder adoption, it has typically examined data collected at a single point in time or focused exclusively on digital contact tracing apps.","In this work, we conduct the first representative study that examines changes in people's attitudes towards COVID-19-related smartphone apps for five different purposes over the first 1.5 years of the pandemic.","In three survey rounds conducted between Summer 2020 and Summer 2021 in the United States and Germany, with approximately 1,000 participants per round and country, we investigate people's willingness to use such apps, their perceived utility, and people's attitudes towards them in different stages of the pandemic.","Our results indicate that privacy is a consistent concern for participants, even in a public health crisis, and the collection of identity-related data significantly decreases acceptance of COVID-19 apps.","Trust in authorities is essential to increase confidence in government-backed apps and foster citizens' willingness to contribute to crisis management.","There is a need for continuous communication with app users to emphasize the benefits of health crisis apps both for individuals and society, thus counteracting decreasing willingness to use them and perceived usefulness as the pandemic evolves."],"url":"http://arxiv.org/abs/2307.06214v1"}
{"created":"2023-07-12 15:00:52","title":"Testing different Log Bases For Vector Model Weighting Technique","abstract":"Information retrieval systems retrieves relevant documents based on a query submitted by the user. The documents are initially indexed and the words in the documents are assigned weights using a weighting technique called TFIDF which is the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF represents the number of occurrences of a term in a document. IDF measures whether the term is common or rare across all documents. It is computed by dividing the total number of documents in the system by the number of documents containing the term and then computing the logarithm of the quotient. By default, we use base 10 to calculate the logarithm. In this paper, we are going to test this weighting technique by using a range of log bases from 0.1 to 100.0 to calculate the IDF. Testing different log bases for vector model weighting technique is to highlight the importance of understanding the performance of the system at different weighting values. We use the documents of MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled explicitly for experiments in data information retrieval systems.","sentences":["Information retrieval systems retrieves relevant documents based on a query submitted by the user.","The documents are initially indexed and the words in the documents are assigned weights using a weighting technique called TFIDF which is the product of Term Frequency (TF) and Inverse Document Frequency (IDF).","TF represents the number of occurrences of a term in a document.","IDF measures whether the term is common or rare across all documents.","It is computed by dividing the total number of documents in the system by the number of documents containing the term and then computing the logarithm of the quotient.","By default, we use base 10 to calculate the logarithm.","In this paper, we are going to test this weighting technique by using a range of log bases from 0.1 to 100.0 to calculate the IDF.","Testing different log bases for vector model weighting technique is to highlight the importance of understanding the performance of the system at different weighting values.","We use the documents of MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled explicitly for experiments in data information retrieval systems."],"url":"http://arxiv.org/abs/2307.06213v1"}
{"created":"2023-07-12 15:00:16","title":"Contract-Based Distributed Synthesis in Two-Objective Parity Games","abstract":"We present a novel method to compute $\\textit{assume-guarantee contracts}$ in non-zerosum two-player games over finite graphs where each player has a different $ \\omega $-regular winning condition. Given a game graph $G$ and two parity winning conditions $\\Phi_0$ and $\\Phi_1$ over $G$, we compute $\\textit{contracted strategy-masks}$ ($\\texttt{csm}$) $(\\Psi_{i},\\Phi_{i})$ for each Player $i$. Within a $\\texttt{csm}$, $\\Phi_{i}$ is a $\\textit{permissive strategy template}$ which collects an infinite number of winning strategies for Player $i$ under the assumption that Player $1-i$ chooses any strategy from the $\\textit{permissive assumption template}$ $\\Psi_{i}$. The main feature of $\\texttt{csm}$'s is their power to $\\textit{fully decentralize all remaining strategy choices}$ -- if the two player's $\\texttt{csm}$'s are compatible, they provide a pair of new local specifications $\\Phi_0^\\bullet$ and $\\Phi_1^\\bullet$ such that Player $i$ can locally and fully independently choose any strategy satisfying $\\Phi_i^\\bullet$ and the resulting strategy profile is ensured to be winning in the original two-objective game $(G,\\Phi_0,\\Phi_1)$.   In addition, the new specifications $\\Phi_i^\\bullet$ are $\\textit{maximally cooperative}$, i.e., allow for the distributed synthesis of any cooperative solution. Further, our algorithmic computation of $\\texttt{csm}$'s is complete and ensured to terminate.   We illustrate how the unique features of our synthesis framework effectively address multiple challenges in the context of \\enquote{correct-by-design} logical control software synthesis for cyber-physical systems and provide empirical evidence that our approach possess desirable structural and computational properties compared to state-of-the-art techniques.","sentences":["We present a novel method to compute $\\textit{assume-guarantee contracts}$ in non-zerosum two-player games over finite graphs where each player has a different $ \\omega $-regular winning condition.","Given a game graph $G$ and two parity winning conditions $\\Phi_0$ and $\\Phi_1$ over $G$, we compute $\\textit{contracted strategy-masks}$ ($\\texttt{csm}$) $(\\Psi_{i},\\Phi_{i})$ for each Player $i$. Within a $\\texttt{csm}$, $\\Phi_{i}$ is a $\\textit{permissive strategy template}$ which collects an infinite number of winning strategies for Player $i$ under the assumption that Player $1-i$ chooses any strategy from the $\\textit{permissive assumption template}$ $\\Psi_{i}$. The main feature of $\\texttt{csm}$'s is their power to $\\textit{fully decentralize all remaining strategy choices}$ -- if the two player's $\\texttt{csm}$'s are compatible, they provide a pair of new local specifications $\\Phi_0^\\bullet$ and $\\Phi_1^\\bullet$ such that Player $i$ can locally and fully independently choose any strategy satisfying $\\Phi_i^\\bullet$ and the resulting strategy profile is ensured to be winning in the original two-objective game $(G,\\Phi_0,\\Phi_1)$.   In addition, the new specifications $\\Phi_i^\\bullet$ are $\\textit{maximally cooperative}$, i.e., allow for the distributed synthesis of any cooperative solution.","Further, our algorithmic computation of $\\texttt{csm}$'s is complete and ensured to terminate.   ","We illustrate how the unique features of our synthesis framework effectively address multiple challenges in the context of \\enquote{correct-by-design} logical control software synthesis for cyber-physical systems and provide empirical evidence that our approach possess desirable structural and computational properties compared to state-of-the-art techniques."],"url":"http://arxiv.org/abs/2307.06212v1"}
{"created":"2023-07-12 14:56:44","title":"Online Information Acquisition: Hiring Multiple Agents","abstract":"We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information. Then, the principal exploits the information to make an informed decision. We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. This poses the challenge of coordinating the agents' efforts and aggregating correlated information. Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems. First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism. Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents. We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings.","sentences":["We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information.","Then, the principal exploits the information to make an informed decision.","We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule.","Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort.","Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule.","While previous work focuses on single-agent problems, we consider multi-agents settings.","This poses the challenge of coordinating the agents' efforts and aggregating correlated information.","Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems.","First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism.","Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents.","We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings."],"url":"http://arxiv.org/abs/2307.06210v1"}
{"created":"2023-07-12 14:52:21","title":"SepVAE: a contrastive VAE to separate pathological patterns from healthy ones","abstract":"Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders (VAEs) that aims at separating the common factors of variation between a background dataset (BG) (i.e., healthy subjects) and a target dataset (TG) (i.e., patients) from the ones that only exist in the target dataset. To do so, these methods separate the latent space into a set of salient features (i.e., proper to the target dataset) and a set of common features (i.e., exist in both datasets). Currently, all models fail to prevent the sharing of information between latent spaces effectively and to capture all salient factors of variation. To this end, we introduce two crucial regularization losses: a disentangling term between common and salient representations and a classification term between background and target samples in the salient space. We show a better performance than previous CA-VAEs methods on three medical applications and a natural images dataset (CelebA). Code and datasets are available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.","sentences":["Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders (VAEs) that aims at separating the common factors of variation between a background dataset (BG) (i.e., healthy subjects) and a target dataset (TG) (i.e., patients) from the ones that only exist in the target dataset.","To do so, these methods separate the latent space into a set of salient features (i.e., proper to the target dataset) and a set of common features (i.e., exist in both datasets).","Currently, all models fail to prevent the sharing of information between latent spaces effectively and to capture all salient factors of variation.","To this end, we introduce two crucial regularization losses: a disentangling term between common and salient representations and a classification term between background and target samples in the salient space.","We show a better performance than previous CA-VAEs methods on three medical applications and a natural images dataset (CelebA).","Code and datasets are available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae."],"url":"http://arxiv.org/abs/2307.06206v1"}
{"created":"2023-07-12 14:26:46","title":"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems","abstract":"In autonomic computing, self-adaptation has been proposed as a fundamental paradigm to manage the complexity of multiagent systems (MASs). This achieved by extending a system with support to monitor and adapt itself to achieve specific concerns of interest. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, improving the expressiveness of the interaction communication with MASs is not without challenges. In this sense, the interplay between self-adaptive systems and effective communication is crucial for future MAS advancements. In this paper, we propose the integration of large language models (LLMs) such as GPT-based technologies into multiagent systems. We anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynamic environments. We also present a practical illustration of the proposed approach, in which we implement and assess a basic MAS-based application. The approach significantly advances the state-of-the-art of self-adaptive systems by proposing a new paradigm for MAS self-adaptation of autonomous systems based on LLM capabilities.","sentences":["In autonomic computing, self-adaptation has been proposed as a fundamental paradigm to manage the complexity of multiagent systems (MASs).","This achieved by extending a system with support to monitor and adapt itself to achieve specific concerns of interest.","Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange.","However, improving the expressiveness of the interaction communication with MASs is not without challenges.","In this sense, the interplay between self-adaptive systems and effective communication is crucial for future MAS advancements.","In this paper, we propose the integration of large language models (LLMs) such as GPT-based technologies into multiagent systems.","We anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynamic environments.","We also present a practical illustration of the proposed approach, in which we implement and assess a basic MAS-based application.","The approach significantly advances the state-of-the-art of self-adaptive systems by proposing a new paradigm for MAS self-adaptation of autonomous systems based on LLM capabilities."],"url":"http://arxiv.org/abs/2307.06187v1"}
{"created":"2023-07-12 14:13:54","title":"CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification","abstract":"Automatic examination of thin-prep cytologic test (TCT) slides can assist pathologists in finding cervical abnormality for accurate and efficient cancer screening. Current solutions mostly need to localize suspicious cells and classify abnormality based on local patches, concerning the fact that whole slide images of TCT are extremely large. It thus requires many annotations of normal and abnormal cervical cells, to supervise the training of the patch-level classifier for promising performance. In this paper, we propose CellGAN to synthesize cytopathological images of various cervical cell types for augmenting patch-level cell classification. Built upon a lightweight backbone, CellGAN is equipped with a non-linear class mapping network to effectively incorporate cell type information into image generation. We also propose the Skip-layer Global Context module to model the complex spatial relationship of the cells, and attain high fidelity of the synthesized images through adversarial learning. Our experiments demonstrate that CellGAN can produce visually plausible TCT cytopathological images for different cell types. We also validate the effectiveness of using CellGAN to greatly augment patch-level cell classification performance.","sentences":["Automatic examination of thin-prep cytologic test (TCT) slides can assist pathologists in finding cervical abnormality for accurate and efficient cancer screening.","Current solutions mostly need to localize suspicious cells and classify abnormality based on local patches, concerning the fact that whole slide images of TCT are extremely large.","It thus requires many annotations of normal and abnormal cervical cells, to supervise the training of the patch-level classifier for promising performance.","In this paper, we propose CellGAN to synthesize cytopathological images of various cervical cell types for augmenting patch-level cell classification.","Built upon a lightweight backbone, CellGAN is equipped with a non-linear class mapping network to effectively incorporate cell type information into image generation.","We also propose the Skip-layer Global Context module to model the complex spatial relationship of the cells, and attain high fidelity of the synthesized images through adversarial learning.","Our experiments demonstrate that CellGAN can produce visually plausible TCT cytopathological images for different cell types.","We also validate the effectiveness of using CellGAN to greatly augment patch-level cell classification performance."],"url":"http://arxiv.org/abs/2307.06182v1"}
{"created":"2023-07-12 14:12:19","title":"B-CLEAN-SC: CLEAN-SC for broadband sources","abstract":"This paper presents B-CLEAN-SC, a variation of CLEAN-SC for broadband sources. Opposed to CLEAN-SC, which ``deconvolves'' the beamforming map for each frequency individually, B-CLEAN-SC processes frequency intervals. Instead of performing a deconvolution iteration at the location of the maximum level, B-CLEAN-SC performs it at the location of the over-frequency-averaged maximum to improve the location estimation. The method is validated and compared to standard CLEAN-SC on synthetic cases, and real-world experiments, for broad- and narrowband sources. It improves the source reconstruction at low and high frequencies and suppresses noise, while it only increases the need for memory but not computational effort.","sentences":["This paper presents B-CLEAN-SC, a variation of CLEAN-SC for broadband sources.","Opposed to CLEAN-SC, which ``deconvolves'' the beamforming map for each frequency individually, B-CLEAN-SC processes frequency intervals.","Instead of performing a deconvolution iteration at the location of the maximum level, B-CLEAN-SC performs it at the location of the over-frequency-averaged maximum to improve the location estimation.","The method is validated and compared to standard CLEAN-SC on synthetic cases, and real-world experiments, for broad- and narrowband sources.","It improves the source reconstruction at low and high frequencies and suppresses noise, while it only increases the need for memory but not computational effort."],"url":"http://arxiv.org/abs/2307.06181v1"}
{"created":"2023-07-12 14:10:15","title":"Large Class Separation is not what you need for Relational Reasoning-based OOD Detection","abstract":"Standard recognition approaches are unable to deal with novel categories at test time. Their overconfidence on the known classes makes the predictions unreliable for safety-critical applications such as healthcare or autonomous driving. Out-Of-Distribution (OOD) detection methods provide a solution by identifying semantic novelty. Most of these methods leverage a learning stage on the known data, which means training (or fine-tuning) a model to capture the concept of normality. This process is clearly sensitive to the amount of available samples and might be computationally expensive for on-board systems. A viable alternative is that of evaluating similarities in the embedding space produced by large pre-trained models without any further learning effort. We focus exactly on such a fine-tuning-free OOD detection setting. This works presents an in-depth analysis of the recently introduced relational reasoning pre-training and investigates the properties of the learned embedding, highlighting the existence of a correlation between the inter-class feature distance and the OOD detection accuracy. As the class separation depends on the chosen pre-training objective, we propose an alternative loss function to control the inter-class margin, and we show its advantage with thorough experiments.","sentences":["Standard recognition approaches are unable to deal with novel categories at test time.","Their overconfidence on the known classes makes the predictions unreliable for safety-critical applications such as healthcare or autonomous driving.","Out-Of-Distribution (OOD) detection methods provide a solution by identifying semantic novelty.","Most of these methods leverage a learning stage on the known data, which means training (or fine-tuning) a model to capture the concept of normality.","This process is clearly sensitive to the amount of available samples and might be computationally expensive for on-board systems.","A viable alternative is that of evaluating similarities in the embedding space produced by large pre-trained models without any further learning effort.","We focus exactly on such a fine-tuning-free OOD detection setting.","This works presents an in-depth analysis of the recently introduced relational reasoning pre-training and investigates the properties of the learned embedding, highlighting the existence of a correlation between the inter-class feature distance and the OOD detection accuracy.","As the class separation depends on the chosen pre-training objective, we propose an alternative loss function to control the inter-class margin, and we show its advantage with thorough experiments."],"url":"http://arxiv.org/abs/2307.06179v1"}
{"created":"2023-07-12 14:04:12","title":"Smart Infrastructure: A Research Junction","abstract":"Complex inner-city junctions are among the most critical traffic areas for injury and fatal accidents. The development of highly automated driving (HAD) systems struggles with the complex and hectic everyday life within those areas. Sensor-equipped smart infrastructures, which can communicate and cooperate with vehicles, are essential to enable a holistic scene understanding to resolve occlusions drivers and vehicle perception systems for themselves can not cover. We introduce an intelligent research infrastructure equipped with visual sensor technology, located at a public inner-city junction in Aschaffenburg, Germany. A multiple-view camera system monitors the traffic situation to perceive road users' behavior. Both motorized and non-motorized traffic is considered. The system is used for research in data generation, evaluating new HAD sensors systems, algorithms, and Artificial Intelligence (AI) training strategies using real-, synthetic- and augmented data. In addition, the junction features a highly accurate digital twin. Real-world data can be taken into the digital twin for simulation purposes and synthetic data generation.","sentences":["Complex inner-city junctions are among the most critical traffic areas for injury and fatal accidents.","The development of highly automated driving (HAD) systems struggles with the complex and hectic everyday life within those areas.","Sensor-equipped smart infrastructures, which can communicate and cooperate with vehicles, are essential to enable a holistic scene understanding to resolve occlusions drivers and vehicle perception systems for themselves can not cover.","We introduce an intelligent research infrastructure equipped with visual sensor technology, located at a public inner-city junction in Aschaffenburg, Germany.","A multiple-view camera system monitors the traffic situation to perceive road users' behavior.","Both motorized and non-motorized traffic is considered.","The system is used for research in data generation, evaluating new HAD sensors systems, algorithms, and Artificial Intelligence (AI) training strategies using real-, synthetic- and augmented data.","In addition, the junction features a highly accurate digital twin.","Real-world data can be taken into the digital twin for simulation purposes and synthetic data generation."],"url":"http://arxiv.org/abs/2307.06177v1"}
{"created":"2023-07-12 14:02:03","title":"Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior","abstract":"Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical results, including a dynamic programming principle, together with optimality guarantees for Dec-POMFC solutions applied to finite swarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradient methods for MARL via centralized training and decentralized execution, together with policy gradient approximation guarantees. In addition, we improve upon state-of-the-art histogram-based MFC by kernel methods, which is of separate interest also for fully observable MFC. We evaluate numerically on representative collective behavior tasks such as adapted Kuramoto and Vicsek swarming models, being on par with state-of-the-art MARL. Overall, our framework takes a step towards RL-based engineering of artificial collective behavior via MFC.","sentences":["Recent reinforcement learning (RL) methods have achieved success in various domains.","However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents.","Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms.","Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems.","In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution.","We provide rigorous theoretical results, including a dynamic programming principle, together with optimality guarantees for Dec-POMFC solutions applied to finite swarms of interest.","Algorithmically, we propose Dec-POMFC-based policy gradient methods for MARL via centralized training and decentralized execution, together with policy gradient approximation guarantees.","In addition, we improve upon state-of-the-art histogram-based MFC by kernel methods, which is of separate interest also for fully observable MFC.","We evaluate numerically on representative collective behavior tasks such as adapted Kuramoto and Vicsek swarming models, being on par with state-of-the-art MARL.","Overall, our framework takes a step towards RL-based engineering of artificial collective behavior via MFC."],"url":"http://arxiv.org/abs/2307.06175v1"}
{"created":"2023-07-12 13:58:11","title":"Assessing Augmented Reality Selection Techniques for Passengers in Moving Vehicles: A Real-World User Study","abstract":"Nowadays, cars offer many possibilities to explore the world around you by providing location-based information displayed on a 2D-Map. However, this information is often only available to front-seat passengers while being restricted to in-car displays. To propose a more natural way of interacting with the environment, we implemented an augmented reality head-mounted display to overlay points of interest onto the real world. We aim to compare multiple selection techniques for digital objects located outside a moving car by investigating head gaze with dwell time, head gaze with hardware button, eye gaze with hardware button, and hand pointing with gesture confirmation. Our study was conducted in a moving car under real-world conditions (N=22), with significant results indicating that hand pointing usage led to slower and less precise content selection while eye gaze was preferred by participants and performed on par with the other techniques.","sentences":["Nowadays, cars offer many possibilities to explore the world around you by providing location-based information displayed on a 2D-Map.","However, this information is often only available to front-seat passengers while being restricted to in-car displays.","To propose a more natural way of interacting with the environment, we implemented an augmented reality head-mounted display to overlay points of interest onto the real world.","We aim to compare multiple selection techniques for digital objects located outside a moving car by investigating head gaze with dwell time, head gaze with hardware button, eye gaze with hardware button, and hand pointing with gesture confirmation.","Our study was conducted in a moving car under real-world conditions (N=22), with significant results indicating that hand pointing usage led to slower and less precise content selection while eye gaze was preferred by participants and performed on par with the other techniques."],"url":"http://arxiv.org/abs/2307.06173v1"}
{"created":"2023-07-12 13:46:40","title":"Auxiliary-Tasks Learning for Physics-Informed Neural Network-Based Partial Differential Equations Solving","abstract":"Physics-informed neural networks (PINNs) have emerged as promising surrogate modes for solving partial differential equations (PDEs). Their effectiveness lies in the ability to capture solution-related features through neural networks. However, original PINNs often suffer from bottlenecks, such as low accuracy and non-convergence, limiting their applicability in complex physical contexts. To alleviate these issues, we proposed auxiliary-task learning-based physics-informed neural networks (ATL-PINNs), which provide four different auxiliary-task learning modes and investigate their performance compared with original PINNs. We also employ the gradient cosine similarity algorithm to integrate auxiliary problem loss with the primary problem loss in ATL-PINNs, which aims to enhance the effectiveness of the auxiliary-task learning modes. To the best of our knowledge, this is the first study to introduce auxiliary-task learning modes in the context of physics-informed learning. We conduct experiments on three PDE problems across different fields and scenarios. Our findings demonstrate that the proposed auxiliary-task learning modes can significantly improve solution accuracy, achieving a maximum performance boost of 96.62% (averaging 28.23%) compared to the original single-task PINNs. The code and dataset are open source at https://github.com/junjun-yan/ATL-PINN.","sentences":["Physics-informed neural networks (PINNs) have emerged as promising surrogate modes for solving partial differential equations (PDEs).","Their effectiveness lies in the ability to capture solution-related features through neural networks.","However, original PINNs often suffer from bottlenecks, such as low accuracy and non-convergence, limiting their applicability in complex physical contexts.","To alleviate these issues, we proposed auxiliary-task learning-based physics-informed neural networks (ATL-PINNs), which provide four different auxiliary-task learning modes and investigate their performance compared with original PINNs.","We also employ the gradient cosine similarity algorithm to integrate auxiliary problem loss with the primary problem loss in ATL-PINNs, which aims to enhance the effectiveness of the auxiliary-task learning modes.","To the best of our knowledge, this is the first study to introduce auxiliary-task learning modes in the context of physics-informed learning.","We conduct experiments on three PDE problems across different fields and scenarios.","Our findings demonstrate that the proposed auxiliary-task learning modes can significantly improve solution accuracy, achieving a maximum performance boost of 96.62% (averaging 28.23%) compared to the original single-task PINNs.","The code and dataset are open source at https://github.com/junjun-yan/ATL-PINN."],"url":"http://arxiv.org/abs/2307.06167v1"}
{"created":"2023-07-12 13:46:28","title":"Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning","abstract":"Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location. To address this question, we propose a two-stage \\recognition\\space and \\reasoning\\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning. We will release our dataset and codes to facilitate future studies.","sentences":["Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings.","One example is that humans can reason where and when an image is taken based on their knowledge.","This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location.","To address this question, we propose a two-stage \\recognition\\space and \\reasoning\\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it.","To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues.","In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning.","We will release our dataset and codes to facilitate future studies."],"url":"http://arxiv.org/abs/2307.06166v1"}
{"created":"2023-07-12 13:46:20","title":"The IMPTC Dataset: An Infrastructural Multi-Person Trajectory and Context Dataset","abstract":"Inner-city intersections are among the most critical traffic areas for injury and fatal accidents. Automated vehicles struggle with the complex and hectic everyday life within those areas. Sensor-equipped smart infrastructures, which can cooperate with vehicles, can benefit automated traffic by extending the perception capabilities of drivers and vehicle perception systems. Additionally, they offer the opportunity to gather reproducible and precise data of a holistic scene understanding, including context information as a basis for training algorithms for various applications in automated traffic. Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context Dataset (IMPTC). We use an intelligent public inner-city intersection in Germany with visual sensor technology. A multi-view camera and LiDAR system perceives traffic situations and road users' behavior. Additional sensors monitor contextual information like weather, lighting, and traffic light signal status. The data acquisition system focuses on Vulnerable Road Users (VRUs) and multi-agent interaction. The resulting dataset consists of eight hours of measurement data. It contains over 2,500 VRU trajectories, including pedestrians, cyclists, e-scooter riders, strollers, and wheelchair users, and over 20,000 vehicle trajectories at different day times, weather conditions, and seasons. In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the sensor-, calibration- and detection data until trajectory and context data. The dataset is continuously expanded and is available online for non-commercial research at https://github.com/kav-institute/imptc-dataset.","sentences":["Inner-city intersections are among the most critical traffic areas for injury and fatal accidents.","Automated vehicles struggle with the complex and hectic everyday life within those areas.","Sensor-equipped smart infrastructures, which can cooperate with vehicles, can benefit automated traffic by extending the perception capabilities of drivers and vehicle perception systems.","Additionally, they offer the opportunity to gather reproducible and precise data of a holistic scene understanding, including context information as a basis for training algorithms for various applications in automated traffic.","Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context Dataset (IMPTC).","We use an intelligent public inner-city intersection in Germany with visual sensor technology.","A multi-view camera and LiDAR system perceives traffic situations and road users' behavior.","Additional sensors monitor contextual information like weather, lighting, and traffic light signal status.","The data acquisition system focuses on Vulnerable Road Users (VRUs) and multi-agent interaction.","The resulting dataset consists of eight hours of measurement data.","It contains over 2,500 VRU trajectories, including pedestrians, cyclists, e-scooter riders, strollers, and wheelchair users, and over 20,000 vehicle trajectories at different day times, weather conditions, and seasons.","In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the sensor-, calibration- and detection data until trajectory and context data.","The dataset is continuously expanded and is available online for non-commercial research at https://github.com/kav-institute/imptc-dataset."],"url":"http://arxiv.org/abs/2307.06165v1"}
{"created":"2023-07-12 13:42:09","title":"Deep Generative Models for Physiological Signals: A Systematic Literature Review","abstract":"In this paper, we present a systematic literature review on deep generative models for physiological signals, particularly electrocardiogram, electroencephalogram, photoplethysmogram and electromyogram. Compared to the existing review papers, we present the first review that summarizes the recent state-of-the-art deep generative models. By analysing the state-of-the-art research related to deep generative models along with their main applications and challenges, this review contributes to the overall understanding of these models applied to physiological signals. Additionally, by highlighting the employed evaluation protocol and the most used physiological databases, this review facilitates the assessment and benchmarking of deep generative models.","sentences":["In this paper, we present a systematic literature review on deep generative models for physiological signals, particularly electrocardiogram, electroencephalogram, photoplethysmogram and electromyogram.","Compared to the existing review papers, we present the first review that summarizes the recent state-of-the-art deep generative models.","By analysing the state-of-the-art research related to deep generative models along with their main applications and challenges, this review contributes to the overall understanding of these models applied to physiological signals.","Additionally, by highlighting the employed evaluation protocol and the most used physiological databases, this review facilitates the assessment and benchmarking of deep generative models."],"url":"http://arxiv.org/abs/2307.06162v1"}
{"created":"2023-07-12 13:32:24","title":"Reflective Hybrid Intelligence for Meaningful Human Control in Decision-Support Systems","abstract":"With the growing capabilities and pervasiveness of AI systems, societies must collectively choose between reduced human autonomy, endangered democracies and limited human rights, and AI that is aligned to human and social values, nurturing collaboration, resilience, knowledge and ethical behaviour. In this chapter, we introduce the notion of self-reflective AI systems for meaningful human control over AI systems. Focusing on decision support systems, we propose a framework that integrates knowledge from psychology and philosophy with formal reasoning methods and machine learning approaches to create AI systems responsive to human values and social norms. We also propose a possible research approach to design and develop self-reflective capability in AI systems. Finally, we argue that self-reflective AI systems can lead to self-reflective hybrid systems (human + AI), thus increasing meaningful human control and empowering human moral reasoning by providing comprehensible information and insights on possible human moral blind spots.","sentences":["With the growing capabilities and pervasiveness of AI systems, societies must collectively choose between reduced human autonomy, endangered democracies and limited human rights, and AI that is aligned to human and social values, nurturing collaboration, resilience, knowledge and ethical behaviour.","In this chapter, we introduce the notion of self-reflective AI systems for meaningful human control over AI systems.","Focusing on decision support systems, we propose a framework that integrates knowledge from psychology and philosophy with formal reasoning methods and machine learning approaches to create AI systems responsive to human values and social norms.","We also propose a possible research approach to design and develop self-reflective capability in AI systems.","Finally, we argue that self-reflective AI systems can lead to self-reflective hybrid systems (human + AI), thus increasing meaningful human control and empowering human moral reasoning by providing comprehensible information and insights on possible human moral blind spots."],"url":"http://arxiv.org/abs/2307.06159v1"}
{"created":"2023-07-12 13:20:18","title":"Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions","abstract":"Maneuver decision-making is the core of unmanned combat aerial vehicle for autonomous air combat. To solve this problem, we propose an automatic curriculum reinforcement learning method, which enables agents to learn effective decisions in air combat from scratch. The range of initial states are used for distinguishing curricula of different difficulty levels, thereby maneuver decision is divided into a series of sub-tasks from easy to difficult, and test results are used to change sub-tasks. As sub-tasks change, agents gradually learn to complete a series of sub-tasks from easy to difficult, enabling them to make effective maneuvering decisions to cope with various states without the need to spend effort designing reward functions. The ablation studied show that the automatic curriculum learning proposed in this article is an essential component for training through reinforcement learning, namely, agents cannot complete effective decisions without curriculum learning. Simulation experiments show that, after training, agents are able to make effective decisions given different states, including tracking, attacking and escaping, which are both rational and interpretable.","sentences":["Maneuver decision-making is the core of unmanned combat aerial vehicle for autonomous air combat.","To solve this problem, we propose an automatic curriculum reinforcement learning method, which enables agents to learn effective decisions in air combat from scratch.","The range of initial states are used for distinguishing curricula of different difficulty levels, thereby maneuver decision is divided into a series of sub-tasks from easy to difficult, and test results are used to change sub-tasks.","As sub-tasks change, agents gradually learn to complete a series of sub-tasks from easy to difficult, enabling them to make effective maneuvering decisions to cope with various states without the need to spend effort designing reward functions.","The ablation studied show that the automatic curriculum learning proposed in this article is an essential component for training through reinforcement learning, namely, agents cannot complete effective decisions without curriculum learning.","Simulation experiments show that, after training, agents are able to make effective decisions given different states, including tracking, attacking and escaping, which are both rational and interpretable."],"url":"http://arxiv.org/abs/2307.06152v1"}
{"created":"2023-07-12 13:10:08","title":"NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services","abstract":"Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on the basis of low-rank adaptation-based light-weight fine-tuning. Subsequently, we highlight substantial essential changes required for a native artificial intelligence (AI) network architecture towards NetGPT, with special emphasis on deeper integration of communications and computing resources and careful calibration of logical AI workflow. Furthermore, we demonstrate several by-product benefits of NetGPT, given edge LLM's astonishing capability to predict trends and infer intents, which possibly leads to a unified solution for intelligent network management \\& orchestration. In a nutshell, we argue that NetGPT is a promising native-AI network architecture beyond provisioning personalized generative services.","sentences":["Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents.","Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources.","In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity.","In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs.","After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on the basis of low-rank adaptation-based light-weight fine-tuning.","Subsequently, we highlight substantial essential changes required for a native artificial intelligence (AI) network architecture towards NetGPT, with special emphasis on deeper integration of communications and computing resources and careful calibration of logical AI workflow.","Furthermore, we demonstrate several by-product benefits of NetGPT, given edge LLM's astonishing capability to predict trends and infer intents, which possibly leads to a unified solution for intelligent network management \\& orchestration.","In a nutshell, we argue that NetGPT is a promising native-AI network architecture beyond provisioning personalized generative services."],"url":"http://arxiv.org/abs/2307.06148v1"}
{"created":"2023-07-12 12:58:03","title":"Learning Kernel-Modulated Neural Representation for Efficient Light Field Compression","abstract":"Light field is a type of image data that captures the 3D scene information by recording light rays emitted from a scene at various orientations. It offers a more immersive perception than classic 2D images but at the cost of huge data volume. In this paper, we draw inspiration from the visual characteristics of Sub-Aperture Images (SAIs) of light field and design a compact neural network representation for the light field compression task. The network backbone takes randomly initialized noise as input and is supervised on the SAIs of the target light field. It is composed of two types of complementary kernels: descriptive kernels (descriptors) that store scene description information learned during training, and modulatory kernels (modulators) that control the rendering of different SAIs from the queried perspectives. To further enhance compactness of the network meanwhile retain high quality of the decoded light field, we accordingly introduce modulator allocation and kernel tensor decomposition mechanisms, followed by non-uniform quantization and lossless entropy coding techniques, to finally form an efficient compression pipeline. Extensive experiments demonstrate that our method outperforms other state-of-the-art (SOTA) methods by a significant margin in the light field compression task. Moreover, after aligning descriptors, the modulators learned from one light field can be transferred to new light fields for rendering dense views, indicating a potential solution for view synthesis task.","sentences":["Light field is a type of image data that captures the 3D scene information by recording light rays emitted from a scene at various orientations.","It offers a more immersive perception than classic 2D images but at the cost of huge data volume.","In this paper, we draw inspiration from the visual characteristics of Sub-Aperture Images (SAIs) of light field and design a compact neural network representation for the light field compression task.","The network backbone takes randomly initialized noise as input and is supervised on the SAIs of the target light field.","It is composed of two types of complementary kernels: descriptive kernels (descriptors) that store scene description information learned during training, and modulatory kernels (modulators) that control the rendering of different SAIs from the queried perspectives.","To further enhance compactness of the network meanwhile retain high quality of the decoded light field, we accordingly introduce modulator allocation and kernel tensor decomposition mechanisms, followed by non-uniform quantization and lossless entropy coding techniques, to finally form an efficient compression pipeline.","Extensive experiments demonstrate that our method outperforms other state-of-the-art (SOTA) methods by a significant margin in the light field compression task.","Moreover, after aligning descriptors, the modulators learned from one light field can be transferred to new light fields for rendering dense views, indicating a potential solution for view synthesis task."],"url":"http://arxiv.org/abs/2307.06143v1"}
{"created":"2023-07-12 12:37:55","title":"SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning","abstract":"Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects, and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute.","sentences":["Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks.","However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics.","We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations.","To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures.","We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects, and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute."],"url":"http://arxiv.org/abs/2307.06135v1"}
{"created":"2023-07-12 12:34:59","title":"Evaluating DNS Resiliency and Responsiveness with Truncation, Fragmentation & DoTCP Fallback","abstract":"Since its introduction in 1987, the DNS has become one of the core components of the Internet. While it was designed to work with both TCP and UDP, DNS-over-UDP (DoUDP) has become the default option due to its low overhead. As new Resource Records were introduced, the sizes of DNS responses increased considerably. This expansion of message body has led to truncation and IP fragmentation more often in recent years where large UDP responses make DNS an easy vector for amplifying denial-of-service attacks which can reduce the resiliency of DNS services. This paper investigates the resiliency, responsiveness, and usage of DoTCP and DoUDP over IPv4 and IPv6 for 10 widely used public DNS resolvers. In these experiments, these aspects are investigated from the edge and from the core of the Internet to represent the communication of the resolvers with DNS clients and authoritative name servers. Overall, more than 14M individual measurements from 2527 RIPE Atlas Probes have been analyzed, highlighting that most resolvers show similar resiliency for both DoTCP and DoUDP. While DNS Flag Day 2020 recommended 1232 bytes of buffer sizes yet we find out that 3 out of 10 resolvers mainly announce very large EDNS(0) buffer sizes both from the edge as well as from the core, which potentially causes fragmentation. In reaction to large response sizes from authoritative name servers, we find that resolvers do not fall back to the usage of DoTCP in many cases, bearing the risk of fragmented responses. As the message sizes in the DNS are expected to grow further, this problem will become more urgent in the future.","sentences":["Since its introduction in 1987, the DNS has become one of the core components of the Internet.","While it was designed to work with both TCP and UDP, DNS-over-UDP (DoUDP) has become the default option due to its low overhead.","As new Resource Records were introduced, the sizes of DNS responses increased considerably.","This expansion of message body has led to truncation and IP fragmentation more often in recent years where large UDP responses make DNS an easy vector for amplifying denial-of-service attacks which can reduce the resiliency of DNS services.","This paper investigates the resiliency, responsiveness, and usage of DoTCP and DoUDP over IPv4 and IPv6 for 10 widely used public DNS resolvers.","In these experiments, these aspects are investigated from the edge and from the core of the Internet to represent the communication of the resolvers with DNS clients and authoritative name servers.","Overall, more than 14M individual measurements from 2527 RIPE Atlas Probes have been analyzed, highlighting that most resolvers show similar resiliency for both DoTCP and DoUDP.","While DNS Flag Day 2020 recommended 1232 bytes of buffer sizes yet we find out that 3 out of 10 resolvers mainly announce very large EDNS(0) buffer sizes both from the edge as well as from the core, which potentially causes fragmentation.","In reaction to large response sizes from authoritative name servers, we find that resolvers do not fall back to the usage of DoTCP in many cases, bearing the risk of fragmented responses.","As the message sizes in the DNS are expected to grow further, this problem will become more urgent in the future."],"url":"http://arxiv.org/abs/2307.06131v1"}
{"created":"2023-07-12 12:25:37","title":"Guided Bottom-Up Interactive Constraint Acquisition","abstract":"Constraint Acquisition (CA) systems can be used to assist in the modeling of constraint satisfaction problems. In (inter)active CA, the system is given a set of candidate constraints and posts queries to the user with the goal of finding the right constraints among the candidates. Current interactive CA algorithms suffer from at least two major bottlenecks. First, in order to converge, they require a large number of queries to be asked to the user. Second, they cannot handle large sets of candidate constraints, since these lead to large waiting times for the user. For this reason, the user must have fairly precise knowledge about what constraints the system should consider. In this paper, we alleviate these bottlenecks by presenting two novel methods that improve the efficiency of CA. First, we introduce a bottom-up approach named GrowAcq that reduces the maximum waiting time for the user and allows the system to handle much larger sets of candidate constraints. It also reduces the total number of queries for problems in which the target constraint network is not sparse. Second, we propose a probability-based method to guide query generation and show that it can significantly reduce the number of queries required to converge. We also propose a new technique that allows the use of openly accessible CP solvers in query generation, removing the dependency of existing methods on less well-maintained custom solvers that are not publicly available. Experimental results show that our proposed methods outperform state-of-the-art CA methods, reducing the number of queries by up to 60%. Our methods work well even in cases where the set of candidate constraints is 50 times larger than the ones commonly used in the literature.","sentences":["Constraint Acquisition (CA) systems can be used to assist in the modeling of constraint satisfaction problems.","In (inter)active CA, the system is given a set of candidate constraints and posts queries to the user with the goal of finding the right constraints among the candidates.","Current interactive CA algorithms suffer from at least two major bottlenecks.","First, in order to converge, they require a large number of queries to be asked to the user.","Second, they cannot handle large sets of candidate constraints, since these lead to large waiting times for the user.","For this reason, the user must have fairly precise knowledge about what constraints the system should consider.","In this paper, we alleviate these bottlenecks by presenting two novel methods that improve the efficiency of CA.","First, we introduce a bottom-up approach named GrowAcq that reduces the maximum waiting time for the user and allows the system to handle much larger sets of candidate constraints.","It also reduces the total number of queries for problems in which the target constraint network is not sparse.","Second, we propose a probability-based method to guide query generation and show that it can significantly reduce the number of queries required to converge.","We also propose a new technique that allows the use of openly accessible CP solvers in query generation, removing the dependency of existing methods on less well-maintained custom solvers that are not publicly available.","Experimental results show that our proposed methods outperform state-of-the-art CA methods, reducing the number of queries by up to 60%.","Our methods work well even in cases where the set of candidate constraints is 50 times larger than the ones commonly used in the literature."],"url":"http://arxiv.org/abs/2307.06126v1"}
{"created":"2023-07-12 12:25:33","title":"Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation","abstract":"Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subpolicies, failures in their execution, and different robot kinematics. These capabilities open the door to a wide range of downstream tasks across embodied AI and real-world use cases.","sentences":["Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs.","In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects.","These new challenges require combining manipulation and navigation skills in unexplored environments.","We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills.","To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points.","We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner.","It shows robustness to unseen subpolicies, failures in their execution, and different robot kinematics.","These capabilities open the door to a wide range of downstream tasks across embodied AI and real-world use cases."],"url":"http://arxiv.org/abs/2307.06125v1"}
{"created":"2023-07-12 12:25:03","title":"Enhancing Portuguese Sign Language Animation with Dynamic Timing and Mouthing","abstract":"Current signing avatars are often described as unnatural as they cannot accurately reproduce all the subtleties of synchronized body behaviors of a human signer. In this paper, we propose a new dynamic approach for transitions between signs, focusing on mouthing animations for Portuguese Sign Language. Although native signers preferred animations with dynamic transitions, we did not find significant differences in comprehension and perceived naturalness scores. On the other hand, we show that including mouthing behaviors improved comprehension and perceived naturalness for novice sign language learners. Results have implications in computational linguistics, human-computer interaction, and synthetic animation of signing avatars.","sentences":["Current signing avatars are often described as unnatural as they cannot accurately reproduce all the subtleties of synchronized body behaviors of a human signer.","In this paper, we propose a new dynamic approach for transitions between signs, focusing on mouthing animations for Portuguese Sign Language.","Although native signers preferred animations with dynamic transitions, we did not find significant differences in comprehension and perceived naturalness scores.","On the other hand, we show that including mouthing behaviors improved comprehension and perceived naturalness for novice sign language learners.","Results have implications in computational linguistics, human-computer interaction, and synthetic animation of signing avatars."],"url":"http://arxiv.org/abs/2307.06124v1"}
{"created":"2023-07-12 12:23:47","title":"SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark","abstract":"Membership inference (MI) attacks threaten user privacy through determining if a given data example has been used to train a target model. However, it has been increasingly recognized that the \"comparing different MI attacks\" methodology used in the existing works has serious limitations. Due to these limitations, we found (through the experiments in this work) that some comparison results reported in the literature are quite misleading. In this paper, we seek to develop a comprehensive benchmark for comparing different MI attacks, called MIBench, which consists not only the evaluation metrics, but also the evaluation scenarios. And we design the evaluation scenarios from four perspectives: the distance distribution of data samples in the target dataset, the distance between data samples of the target dataset, the differential distance between two datasets (i.e., the target dataset and a generated dataset with only nonmembers), and the ratio of the samples that are made no inferences by an MI attack. The evaluation metrics consist of ten typical evaluation metrics. We have identified three principles for the proposed \"comparing different MI attacks\" methodology, and we have designed and implemented the MIBench benchmark with 84 evaluation scenarios for each dataset. In total, we have used our benchmark to fairly and systematically compare 15 state-of-the-art MI attack algorithms across 588 evaluation scenarios, and these evaluation scenarios cover 7 widely used datasets and 7 representative types of models. All codes and evaluations of MIBench are publicly available at https://github.com/MIBench/MIBench.github.io/blob/main/README.md.","sentences":["Membership inference (MI) attacks threaten user privacy through determining if a given data example has been used to train a target model.","However, it has been increasingly recognized that the \"comparing different MI attacks\" methodology used in the existing works has serious limitations.","Due to these limitations, we found (through the experiments in this work) that some comparison results reported in the literature are quite misleading.","In this paper, we seek to develop a comprehensive benchmark for comparing different MI attacks, called MIBench, which consists not only the evaluation metrics, but also the evaluation scenarios.","And we design the evaluation scenarios from four perspectives: the distance distribution of data samples in the target dataset, the distance between data samples of the target dataset, the differential distance between two datasets (i.e., the target dataset and a generated dataset with only nonmembers), and the ratio of the samples that are made no inferences by an MI attack.","The evaluation metrics consist of ten typical evaluation metrics.","We have identified three principles for the proposed \"comparing different MI attacks\" methodology, and we have designed and implemented the MIBench benchmark with 84 evaluation scenarios for each dataset.","In total, we have used our benchmark to fairly and systematically compare 15 state-of-the-art MI attack algorithms across 588 evaluation scenarios, and these evaluation scenarios cover 7 widely used datasets and 7 representative types of models.","All codes and evaluations of MIBench are publicly available at https://github.com/MIBench/MIBench.github.io/blob/main/README.md."],"url":"http://arxiv.org/abs/2307.06123v1"}
{"created":"2023-07-12 12:20:04","title":"Recognizing student identification numbers from the matrix templates using a modified U-net architecture","abstract":"This paper presents an innovative approach to student identification during exams and knowledge tests, which overcomes the limitations of the traditional personal information entry method. The proposed method employs a matrix template on the designated section of the exam, where squares containing numbers are selectively blackened. The methodology involves the development of a neural network specifically designed for recognizing students' personal identification numbers. The neural network utilizes a specially adapted U-Net architecture, trained on an extensive dataset comprising images of blackened tables. The network demonstrates proficiency in recognizing the patterns and arrangement of blackened squares, accurately interpreting the information inscribed within them. Additionally, the model exhibits high accuracy in correctly identifying entered student personal numbers and effectively detecting erroneous entries within the table. This approach offers multiple advantages. Firstly, it significantly accelerates the exam marking process by automatically extracting identifying information from the blackened tables, eliminating the need for manual entry and minimizing the potential for errors. Secondly, the method automates the identification process, thereby reducing administrative effort and expediting data processing. The introduction of this innovative identification system represents a notable advancement in the field of exams and knowledge tests, replacing the conventional manual entry of personal data with a streamlined, efficient, and accurate identification process.","sentences":["This paper presents an innovative approach to student identification during exams and knowledge tests, which overcomes the limitations of the traditional personal information entry method.","The proposed method employs a matrix template on the designated section of the exam, where squares containing numbers are selectively blackened.","The methodology involves the development of a neural network specifically designed for recognizing students' personal identification numbers.","The neural network utilizes a specially adapted U-Net architecture, trained on an extensive dataset comprising images of blackened tables.","The network demonstrates proficiency in recognizing the patterns and arrangement of blackened squares, accurately interpreting the information inscribed within them.","Additionally, the model exhibits high accuracy in correctly identifying entered student personal numbers and effectively detecting erroneous entries within the table.","This approach offers multiple advantages.","Firstly, it significantly accelerates the exam marking process by automatically extracting identifying information from the blackened tables, eliminating the need for manual entry and minimizing the potential for errors.","Secondly, the method automates the identification process, thereby reducing administrative effort and expediting data processing.","The introduction of this innovative identification system represents a notable advancement in the field of exams and knowledge tests, replacing the conventional manual entry of personal data with a streamlined, efficient, and accurate identification process."],"url":"http://arxiv.org/abs/2307.06120v1"}
{"created":"2023-07-12 12:19:36","title":"TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image","abstract":"Automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogrammetry and remote sensing, yet has an important role in forest management. In this paper, we propose the first semisupervised transformer-based framework for tree counting which reduces the expensive tree annotations for remote sensing images. Our method, termed as TreeFormer, first develops a pyramid tree representation module based on transformer blocks to extract multi-scale features during the encoding stage. Contextual attention-based feature fusion and tree density regressor modules are further designed to utilize the robust features from the encoder to estimate tree density maps in the decoder. Moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranking losses to utilize unlabeled images into the training process. Finally, the tree counter token is introduced to regulate the network by computing the global tree counts for both labeled and unlabeled images. Our model was evaluated on two benchmark tree counting datasets, Jiangsu, and Yosemite, as well as a new dataset, KCL-London, created by ourselves. Our TreeFormer outperforms the state of the art semi-supervised methods under the same setting and exceeds the fully-supervised methods using the same number of labeled images. The codes and datasets are available at https://github.com/HAAClassic/TreeFormer.","sentences":["Automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogrammetry and remote sensing, yet has an important role in forest management.","In this paper, we propose the first semisupervised transformer-based framework for tree counting which reduces the expensive tree annotations for remote sensing images.","Our method, termed as TreeFormer, first develops a pyramid tree representation module based on transformer blocks to extract multi-scale features during the encoding stage.","Contextual attention-based feature fusion and tree density regressor modules are further designed to utilize the robust features from the encoder to estimate tree density maps in the decoder.","Moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranking losses to utilize unlabeled images into the training process.","Finally, the tree counter token is introduced to regulate the network by computing the global tree counts for both labeled and unlabeled images.","Our model was evaluated on two benchmark tree counting datasets, Jiangsu, and Yosemite, as well as a new dataset, KCL-London, created by ourselves.","Our TreeFormer outperforms the state of the art semi-supervised methods under the same setting and exceeds the fully-supervised methods using the same number of labeled images.","The codes and datasets are available at https://github.com/HAAClassic/TreeFormer."],"url":"http://arxiv.org/abs/2307.06118v1"}
{"created":"2023-07-12 12:13:33","title":"Sublinear Time Shortest Path in Expander Graphs","abstract":"Computing a shortest path between two nodes in an undirected unweighted graph is among the most basic algorithmic tasks. Breadth first search solves this problem in linear time, which is clearly also a lower bound in the worst case. However, several works have shown how to solve this problem in sublinear time in expectation when the input graph is drawn from one of several classes of random graphs. In this work, we extend these results by giving sublinear time shortest path (and short path) algorithms for expander graphs. We thus identify a natural deterministic property of a graph (that is satisfied by typical random regular graphs) which suffices for sublinear time shortest paths. The algorithms are very simple, involving only bidirectional breadth first search and short random walks. We also complement our new algorithms by near-matching lower bounds.","sentences":["Computing a shortest path between two nodes in an undirected unweighted graph is among the most basic algorithmic tasks.","Breadth first search solves this problem in linear time, which is clearly also a lower bound in the worst case.","However, several works have shown how to solve this problem in sublinear time in expectation when the input graph is drawn from one of several classes of random graphs.","In this work, we extend these results by giving sublinear time shortest path (and short path) algorithms for expander graphs.","We thus identify a natural deterministic property of a graph (that is satisfied by typical random regular graphs) which suffices for sublinear time shortest paths.","The algorithms are very simple, involving only bidirectional breadth first search and short random walks.","We also complement our new algorithms by near-matching lower bounds."],"url":"http://arxiv.org/abs/2307.06113v1"}
{"created":"2023-07-12 12:07:35","title":"Fast Decoding of Lifted Interleaved Linearized Reed-Solomon Codes for Multishot Network Coding","abstract":"Mart{\\'\\i}nez-Pe{\\~n}as and Kschischang (IEEE Trans.\\ Inf.\\ Theory, 2019) proposed lifted linearized Reed--Solomon codes as suitable codes for error control in multishot network coding. We show how to construct and decode \\ac{LILRS} codes. Compared to the construction by Mart{\\'\\i}nez-Pe{\\~n}as--Kschischang, interleaving allows to increase the decoding region significantly and decreases the overhead due to the lifting (i.e., increases the code rate), at the cost of an increased packet size. We propose two decoding schemes for \\ac{LILRS} that are both capable of correcting insertions and deletions beyond half the minimum distance of the code by either allowing a list or a small decoding failure probability. We propose a probabilistic unique {\\LOlike} decoder for \\ac{LILRS} codes and an efficient interpolation-based decoding scheme that can be either used as a list decoder (with exponential worst-case list size) or as a probabilistic unique decoder. We derive upper bounds on the decoding failure probability of the probabilistic-unique decoders which show that the decoding failure probability is very small for most channel realizations up to the maximal decoding radius. The tightness of the bounds is verified by Monte Carlo simulations.","sentences":["Mart{\\'\\i}nez-Pe{\\~n}as and Kschischang (IEEE Trans.\\ Inf.\\ Theory, 2019)","proposed lifted linearized Reed--Solomon codes as suitable codes for error control in multishot network coding.","We show how to construct and decode \\ac{LILRS} codes.","Compared to the construction by Mart{\\'\\i}nez-Pe{\\~n}as--Kschischang, interleaving allows to increase the decoding region significantly and decreases the overhead due to the lifting (i.e., increases the code rate), at the cost of an increased packet size.","We propose two decoding schemes for \\ac{LILRS} that are both capable of correcting insertions and deletions beyond half the minimum distance of the code by either allowing a list or a small decoding failure probability.","We propose a probabilistic unique {\\LOlike} decoder for \\ac{LILRS} codes and an efficient interpolation-based decoding scheme that can be either used as a list decoder (with exponential worst-case list size) or as a probabilistic unique decoder.","We derive upper bounds on the decoding failure probability of the probabilistic-unique decoders which show that the decoding failure probability is very small for most channel realizations up to the maximal decoding radius.","The tightness of the bounds is verified by Monte Carlo simulations."],"url":"http://arxiv.org/abs/2307.06108v1"}
{"created":"2023-07-12 12:02:36","title":"Deep learning for dynamic graphs: models and benchmarks","abstract":"Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches","sentences":["Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs.","Despite the growth of this research field, there are still important challenges that are yet unsolved.","Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time.","With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs.","Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches"],"url":"http://arxiv.org/abs/2307.06104v1"}
{"created":"2023-07-12 11:49:15","title":"Air Bumper: A Collision Detection and Reaction Framework for Autonomous MAV Navigation","abstract":"Autonomous navigation in unknown environments with obstacles remains challenging for micro aerial vehicles (MAVs) due to their limited onboard computing and sensing resources. Although various collision avoidance methods have been developed, it is still possible for drones to collide with unobserved obstacles due to unpredictable disturbances, sensor limitations, and control uncertainty. Instead of completely avoiding collisions, this article proposes Air Bumper, a collision detection and reaction framework, for fully autonomous flight in 3D environments to improve the safety of drones. Our framework only utilizes the onboard inertial measurement unit (IMU) to detect and estimate collisions. We further design a collision recovery control for rapid recovery and collision-aware mapping to integrate collision information into general LiDAR-based sensing and planning frameworks. Our simulation and experimental results show that the quadrotor can rapidly detect, estimate, and recover from collisions with obstacles in 3D space and continue the flight smoothly with the help of the collision-aware map.","sentences":["Autonomous navigation in unknown environments with obstacles remains challenging for micro aerial vehicles (MAVs) due to their limited onboard computing and sensing resources.","Although various collision avoidance methods have been developed, it is still possible for drones to collide with unobserved obstacles due to unpredictable disturbances, sensor limitations, and control uncertainty.","Instead of completely avoiding collisions, this article proposes Air Bumper, a collision detection and reaction framework, for fully autonomous flight in 3D environments to improve the safety of drones.","Our framework only utilizes the onboard inertial measurement unit (IMU) to detect and estimate collisions.","We further design a collision recovery control for rapid recovery and collision-aware mapping to integrate collision information into general LiDAR-based sensing and planning frameworks.","Our simulation and experimental results show that the quadrotor can rapidly detect, estimate, and recover from collisions with obstacles in 3D space and continue the flight smoothly with the help of the collision-aware map."],"url":"http://arxiv.org/abs/2307.06101v1"}
{"created":"2023-07-12 11:48:16","title":"Agilicious: Open-Source and Open-Hardware Agile Quadrotor for Vision-Based Flight","abstract":"Autonomous, agile quadrotor flight raises fundamental challenges for robotics research in terms of perception, planning, learning, and control. A versatile and standardized platform is needed to accelerate research and let practitioners focus on the core problems. To this end, we present Agilicious, a co-designed hardware and software framework tailored to autonomous, agile quadrotor flight. It is completely open-source and open-hardware and supports both model-based and neural-network--based controllers. Also, it provides high thrust-to-weight and torque-to-inertia ratios for agility, onboard vision sensors, GPU-accelerated compute hardware for real-time perception and neural-network inference, a real-time flight controller, and a versatile software stack. In contrast to existing frameworks, Agilicious offers a unique combination of flexible software stack and high-performance hardware. We compare Agilicious with prior works and demonstrate it on different agile tasks, using both model-based and neural-network--based controllers. Our demonstrators include trajectory tracking at up to 5g and 70 km/h in a motion-capture system, and vision-based acrobatic flight and obstacle avoidance in both structured and unstructured environments using solely onboard perception. Finally, we demonstrate its use for hardware-in-the-loop simulation in virtual-reality environments. Thanks to its versatility, we believe that Agilicious supports the next generation of scientific and industrial quadrotor research.","sentences":["Autonomous, agile quadrotor flight raises fundamental challenges for robotics research in terms of perception, planning, learning, and control.","A versatile and standardized platform is needed to accelerate research and let practitioners focus on the core problems.","To this end, we present Agilicious, a co-designed hardware and software framework tailored to autonomous, agile quadrotor flight.","It is completely open-source and open-hardware and supports both model-based and neural-network--based controllers.","Also, it provides high thrust-to-weight and torque-to-inertia ratios for agility, onboard vision sensors, GPU-accelerated compute hardware for real-time perception and neural-network inference, a real-time flight controller, and a versatile software stack.","In contrast to existing frameworks, Agilicious offers a unique combination of flexible software stack and high-performance hardware.","We compare Agilicious with prior works and demonstrate it on different agile tasks, using both model-based and neural-network--based controllers.","Our demonstrators include trajectory tracking at up to 5g and 70 km/h in a motion-capture system, and vision-based acrobatic flight and obstacle avoidance in both structured and unstructured environments using solely onboard perception.","Finally, we demonstrate its use for hardware-in-the-loop simulation in virtual-reality environments.","Thanks to its versatility, we believe that Agilicious supports the next generation of scientific and industrial quadrotor research."],"url":"http://arxiv.org/abs/2307.06100v1"}
{"created":"2023-07-12 11:45:22","title":"RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation","abstract":"Glass-like objects are widespread in daily life but remain intractable to be segmented for most existing methods. The transparent property makes it difficult to be distinguished from background, while the tiny separation boundary further impedes the acquisition of their exact contour. In this paper, by revealing the key co-evolution demand of semantic and boundary learning, we propose a Selective Mutual Evolution (SME) module to enable the reciprocal feature learning between them. Then to exploit the global shape context, we propose a Structurally Attentive Refinement (SAR) module to conduct a fine-grained feature refinement for those ambiguous points around the boundary. Finally, to further utilize the multi-scale representation, we integrate the above two modules into a cascaded structure and then introduce a Reciprocal Feature Evolution Network (RFENet) for effective glass-like object segmentation. Extensive experiments demonstrate that our RFENet achieves state-of-the-art performance on three popular public datasets.","sentences":["Glass-like objects are widespread in daily life but remain intractable to be segmented for most existing methods.","The transparent property makes it difficult to be distinguished from background, while the tiny separation boundary further impedes the acquisition of their exact contour.","In this paper, by revealing the key co-evolution demand of semantic and boundary learning, we propose a Selective Mutual Evolution (SME) module to enable the reciprocal feature learning between them.","Then to exploit the global shape context, we propose a Structurally Attentive Refinement (SAR) module to conduct a fine-grained feature refinement for those ambiguous points around the boundary.","Finally, to further utilize the multi-scale representation, we integrate the above two modules into a cascaded structure and then introduce a Reciprocal Feature Evolution Network (RFENet) for effective glass-like object segmentation.","Extensive experiments demonstrate that our RFENet achieves state-of-the-art performance on three popular public datasets."],"url":"http://arxiv.org/abs/2307.06099v1"}
{"created":"2023-07-12 11:38:34","title":"Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks","abstract":"Stochastic Gumbel graph networks are proposed to learn high-dimensional time series, where the observed dimensions are often spatially correlated. To that end, the observed randomness and spatial-correlations are captured by learning the drift and diffusion terms of the stochastic differential equation with a Gumble matrix embedding, respectively. In particular, this novel framework enables us to investigate the implicit regularization effect of the noise terms in S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by deriving the difference between the two corresponding loss functions in a small neighborhood of weight. Then, we employ Kuramoto's model to generate data for comparing the spectral density from the Hessian Matrix of the two loss functions. Experimental results on real-world data, demonstrate that S-GGNs exhibit superior convergence, robustness, and generalization, compared with state-of-the-arts.","sentences":["Stochastic Gumbel graph networks are proposed to learn high-dimensional time series, where the observed dimensions are often spatially correlated.","To that end, the observed randomness and spatial-correlations are captured by learning the drift and diffusion terms of the stochastic differential equation with a Gumble matrix embedding, respectively.","In particular, this novel framework enables us to investigate the implicit regularization effect of the noise terms in S-GGNs.","We provide a theoretical guarantee for the proposed S-GGNs by deriving the difference between the two corresponding loss functions in a small neighborhood of weight.","Then, we employ Kuramoto's model to generate data for comparing the spectral density from the Hessian Matrix of the two loss functions.","Experimental results on real-world data, demonstrate that S-GGNs exhibit superior convergence, robustness, and generalization, compared with state-of-the-arts."],"url":"http://arxiv.org/abs/2307.06097v1"}
{"created":"2023-07-12 11:38:17","title":"Exact Resource Allocation over Fair Wireless Relay Networks","abstract":"In relay-enabled cellular networks, the intertwined nature of network agents calls for complex schemes to allocate wireless resources. Resources need to be distributed among mobile users while considering how relay resources are allocated, and constrained by the traffic rate achievable by base stations and over backhaul links. In this work, we derive a resource allocation scheme that achieves max-min fairness across mobile users. Furthermore, the optimal allocation is found with linear complexity with respect to the number of mobile users and relays.","sentences":["In relay-enabled cellular networks, the intertwined nature of network agents calls for complex schemes to allocate wireless resources.","Resources need to be distributed among mobile users while considering how relay resources are allocated, and constrained by the traffic rate achievable by base stations and over backhaul links.","In this work, we derive a resource allocation scheme that achieves max-min fairness across mobile users.","Furthermore, the optimal allocation is found with linear complexity with respect to the number of mobile users and relays."],"url":"http://arxiv.org/abs/2307.06095v1"}
{"created":"2023-07-12 11:36:27","title":"Online Laplace Model Selection Revisited","abstract":"The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradient descent on UCI regression datasets. The optimised hyperparameters prevent overfitting and outperform validation-based early stopping.","sentences":["The Laplace approximation provides a closed-form model selection objective for neural networks (NN).","Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community.","However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness.","This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions.","Online Laplace and its mode-corrected counterpart share stationary points where 1.","the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2.","the hyperparameters maximise the Laplace evidence, motivating online methods.","We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradient descent on UCI regression datasets.","The optimised hyperparameters prevent overfitting and outperform validation-based early stopping."],"url":"http://arxiv.org/abs/2307.06093v1"}
{"created":"2023-07-12 11:35:37","title":"Quantitative CLTs in Deep Neural Networks","abstract":"We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\\gamma}$ for $\\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.","sentences":["We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth.","Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\\gamma}$ for $\\gamma>0,$ with the exponent depending on the metric used to measure discrepancy.","Our bounds are stronger in terms of their dependence on network width than any previously available in the literature."],"url":"http://arxiv.org/abs/2307.06092v1"}
{"created":"2023-07-12 11:32:02","title":"AICT: An Adaptive Image Compression Transformer","abstract":"Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.","sentences":["Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT).","Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors.","On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents.","Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images.","Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM."],"url":"http://arxiv.org/abs/2307.06091v1"}
{"created":"2023-07-12 11:27:40","title":"Can Large Language Models Aid in Annotating Speech Emotional Data? Uncovering New Frontiers","abstract":"Despite recent advancements in speech emotion recognition (SER) models, state-of-the-art deep learning (DL) approaches face the challenge of the limited availability of annotated data. Large language models (LLMs) have revolutionised our understanding of natural language, introducing emergent properties that broaden comprehension in language, speech, and vision. This paper examines the potential of LLMs to annotate abundant speech data, aiming to enhance the state-of-the-art in SER. We evaluate this capability across various settings using publicly available speech emotion classification datasets. Leveraging ChatGPT, we experimentally demonstrate the promising role of LLMs in speech emotion data annotation. Our evaluation encompasses single-shot and few-shots scenarios, revealing performance variability in SER. Notably, we achieve improved results through data augmentation, incorporating ChatGPT-annotated samples into existing datasets. Our work uncovers new frontiers in speech emotion classification, highlighting the increasing significance of LLMs in this field moving forward.","sentences":["Despite recent advancements in speech emotion recognition (SER) models, state-of-the-art deep learning (DL) approaches face the challenge of the limited availability of annotated data.","Large language models (LLMs) have revolutionised our understanding of natural language, introducing emergent properties that broaden comprehension in language, speech, and vision.","This paper examines the potential of LLMs to annotate abundant speech data, aiming to enhance the state-of-the-art in SER.","We evaluate this capability across various settings using publicly available speech emotion classification datasets.","Leveraging ChatGPT, we experimentally demonstrate the promising role of LLMs in speech emotion data annotation.","Our evaluation encompasses single-shot and few-shots scenarios, revealing performance variability in SER.","Notably, we achieve improved results through data augmentation, incorporating ChatGPT-annotated samples into existing datasets.","Our work uncovers new frontiers in speech emotion classification, highlighting the increasing significance of LLMs in this field moving forward."],"url":"http://arxiv.org/abs/2307.06090v1"}
{"created":"2023-07-12 11:27:09","title":"Exploring Millions of User Interactions with ICEBOAT: Big Data Analytics for Automotive User Interfaces","abstract":"User Experience (UX) professionals need to be able to analyze large amounts of usage data on their own to make evidence-based design decisions. However, the design process for In-Vehicle Information Systems (IVIS) lacks data-driven support and effective tools for visualizing and analyzing user interaction data. Therefore, we propose ICEBOAT, an interactive visualization tool tailored to the needs of automotive UX experts to effectively and efficiently evaluate driver interactions with IVISs. ICEBOAT visualizes telematics data collected from production line vehicles, allowing UX experts to perform task-specific analyses. Following a mixed methods User-centered design (UCD) approach, we conducted an interview study (N=4) to extract the domain specific information and interaction needs of automotive UX experts and used a co-design approach (N=4) to develop an interactive analysis tool. Our evaluation (N=12) shows that ICEBOAT enables UX experts to efficiently generate knowledge that facilitates data-driven design decisions.","sentences":["User Experience (UX) professionals need to be able to analyze large amounts of usage data on their own to make evidence-based design decisions.","However, the design process for In-Vehicle Information Systems (IVIS) lacks data-driven support and effective tools for visualizing and analyzing user interaction data.","Therefore, we propose ICEBOAT, an interactive visualization tool tailored to the needs of automotive UX experts to effectively and efficiently evaluate driver interactions with IVISs.","ICEBOAT visualizes telematics data collected from production line vehicles, allowing UX experts to perform task-specific analyses.","Following a mixed methods User-centered design (UCD) approach, we conducted an interview study (N=4) to extract the domain specific information and interaction needs of automotive UX experts and used a co-design approach (N=4) to develop an interactive analysis tool.","Our evaluation (N=12) shows that ICEBOAT enables UX experts to efficiently generate knowledge that facilitates data-driven design decisions."],"url":"http://arxiv.org/abs/2307.06089v1"}
{"created":"2023-07-12 11:19:55","title":"Non-Ideal Program-Time Conservation in Charge Trap Flash for Deep Learning","abstract":"Training deep neural networks (DNNs) is computationally intensive but arrays of non-volatile memories like Charge Trap Flash (CTF) can accelerate DNN operations using in-memory computing. Specifically, the Resistive Processing Unit (RPU) architecture uses the voltage-threshold program by stochastic encoded pulse trains and analog memory features to accelerate vector-vector outer product and weight update for the gradient descent algorithms. Although CTF, offering high precision, has been regarded as an excellent choice for implementing RPU, the accumulation of charge due to the applied stochastic pulse trains is ultimately of critical significance in determining the final weight update. In this paper, we report the non-ideal program-time conservation in CTF through pulsing input measurements. We experimentally measure the effect of pulse width and pulse gap, keeping the total ON-time of the input pulse train constant, and report three non-idealities: (1) Cumulative V_T shift reduces when total ON-time is fragmented into a larger number of shorter pulses, (2) Cumulative V_T shift drops abruptly for pulse widths < 2 {\\mu}s, (3) Cumulative V_T shift depends on the gap between consecutive pulses and the V_T shift reduction gets recovered for smaller gaps. We present an explanation based on a transient tunneling field enhancement due to blocking oxide trap-charge dynamics to explain these non-idealities. Identifying and modeling the responsible mechanisms and predicting their system-level effects during learning is critical. This non-ideal accumulation is expected to affect algorithms and architectures relying on devices for implementing mathematically equivalent functions for in-memory computing-based acceleration.","sentences":["Training deep neural networks (DNNs) is computationally intensive but arrays of non-volatile memories like Charge Trap Flash (CTF) can accelerate DNN operations using in-memory computing.","Specifically, the Resistive Processing Unit (RPU) architecture uses the voltage-threshold program by stochastic encoded pulse trains and analog memory features to accelerate vector-vector outer product and weight update for the gradient descent algorithms.","Although CTF, offering high precision, has been regarded as an excellent choice for implementing RPU, the accumulation of charge due to the applied stochastic pulse trains is ultimately of critical significance in determining the final weight update.","In this paper, we report the non-ideal program-time conservation in CTF through pulsing input measurements.","We experimentally measure the effect of pulse width and pulse gap, keeping the total ON-time of the input pulse train constant, and report three non-idealities: (1) Cumulative V_T shift reduces when total ON-time is fragmented into a larger number of shorter pulses, (2) Cumulative V_T shift drops abruptly for pulse widths < 2 {\\mu}s, (3) Cumulative V_T shift depends on the gap between consecutive pulses and the V_T shift reduction gets recovered for smaller gaps.","We present an explanation based on a transient tunneling field enhancement due to blocking oxide trap-charge dynamics to explain these non-idealities.","Identifying and modeling the responsible mechanisms and predicting their system-level effects during learning is critical.","This non-ideal accumulation is expected to affect algorithms and architectures relying on devices for implementing mathematically equivalent functions for in-memory computing-based acceleration."],"url":"http://arxiv.org/abs/2307.06088v1"}
{"created":"2023-07-12 11:14:25","title":"Neuromorphic analog circuits for robust on-chip always-on learning in spiking neural networks","abstract":"Mixed-signal neuromorphic systems represent a promising solution for solving extreme-edge computing tasks without relying on external computing resources. Their spiking neural network circuits are optimized for processing sensory data on-line in continuous-time. However, their low precision and high variability can severely limit their performance. To address this issue and improve their robustness to inhomogeneities and noise in both their internal state variables and external input signals, we designed on-chip learning circuits with short-term analog dynamics and long-term tristate discretization mechanisms. An additional hysteretic stop-learning mechanism is included to improve stability and automatically disable weight updates when necessary, to enable continuous always-on learning. We designed a spiking neural network with these learning circuits in a prototype chip using a 180 nm CMOS technology. Simulation and silicon measurement results from the prototype chip are presented. These circuits enable the construction of large-scale spiking neural networks with online learning capabilities for real-world edge computing tasks.","sentences":["Mixed-signal neuromorphic systems represent a promising solution for solving extreme-edge computing tasks without relying on external computing resources.","Their spiking neural network circuits are optimized for processing sensory data on-line in continuous-time.","However, their low precision and high variability can severely limit their performance.","To address this issue and improve their robustness to inhomogeneities and noise in both their internal state variables and external input signals, we designed on-chip learning circuits with short-term analog dynamics and long-term tristate discretization mechanisms.","An additional hysteretic stop-learning mechanism is included to improve stability and automatically disable weight updates when necessary, to enable continuous always-on learning.","We designed a spiking neural network with these learning circuits in a prototype chip using a 180 nm CMOS technology.","Simulation and silicon measurement results from the prototype chip are presented.","These circuits enable the construction of large-scale spiking neural networks with online learning capabilities for real-world edge computing tasks."],"url":"http://arxiv.org/abs/2307.06084v1"}
{"created":"2023-07-12 11:08:24","title":"VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View","abstract":"Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve 25%-30% relative improvement in task completion over the previous state-of-the-art for two datasets.","sentences":["Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence.","One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities.","The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View.","Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment.","In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action.","Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view.","We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples.","We further finetune the LLM agent on a few thousand examples and achieve 25%-30% relative improvement in task completion over the previous state-of-the-art for two datasets."],"url":"http://arxiv.org/abs/2307.06082v1"}
{"created":"2023-07-12 11:05:19","title":"Navigating the Complexity of Generative AI Adoption in Software Engineering","abstract":"In this paper, the adoption patterns of Generative Artificial Intelligence (AI) tools within software engineering are investigated. Influencing factors at the individual, technological, and societal levels are analyzed using a mixed-methods approach for an extensive comprehension of AI adoption. An initial structured interview was conducted with 100 software engineers, employing the Technology Acceptance Model (TAM), the Diffusion of Innovations theory (DOI), and the Social Cognitive Theory (SCT) as guiding theories.   A theoretical model named the Human-AI Collaboration and Adaptation Framework (HACAF) was deduced using the Gioia Methodology, characterizing AI adoption in software engineering. This model's validity was subsequently tested through Partial Least Squares - Structural Equation Modeling (PLS-SEM), using data collected from 183 software professionals.   The results indicate that the adoption of AI tools in these early integration stages is primarily driven by their compatibility with existing development workflows. This finding counters the traditional theories of technology acceptance. Contrary to expectations, the influence of perceived usefulness, social aspects, and personal innovativeness on adoption appeared to be less significant. This paper yields significant insights for the design of future AI tools and supplies a structure for devising effective strategies for organizational implementation.","sentences":["In this paper, the adoption patterns of Generative Artificial Intelligence (AI) tools within software engineering are investigated.","Influencing factors at the individual, technological, and societal levels are analyzed using a mixed-methods approach for an extensive comprehension of AI adoption.","An initial structured interview was conducted with 100 software engineers, employing the Technology Acceptance Model (TAM), the Diffusion of Innovations theory (DOI), and the Social Cognitive Theory (SCT) as guiding theories.   ","A theoretical model named the Human-AI Collaboration and Adaptation Framework (HACAF) was deduced using the Gioia Methodology, characterizing AI adoption in software engineering.","This model's validity was subsequently tested through Partial Least Squares - Structural Equation Modeling (PLS-SEM), using data collected from 183 software professionals.   ","The results indicate that the adoption of AI tools in these early integration stages is primarily driven by their compatibility with existing development workflows.","This finding counters the traditional theories of technology acceptance.","Contrary to expectations, the influence of perceived usefulness, social aspects, and personal innovativeness on adoption appeared to be less significant.","This paper yields significant insights for the design of future AI tools and supplies a structure for devising effective strategies for organizational implementation."],"url":"http://arxiv.org/abs/2307.06081v1"}
{"created":"2023-07-12 11:01:08","title":"Better bounds on the minimal Lee distance","abstract":"This paper provides new and improved Singleton-like bounds for Lee metric codes over integer residue rings. We derive the bounds using various novel definitions of generalized Lee weights based on different notions of a support of a linear code. In this regard, we introduce three main different support types for codes in the Lee metric and analyze their utility to derive bounds on the minimum Lee distance. Eventually, we propose a new point of view to generalized weights and give an improved bound on the minimum distance of codes in the Lee metric for which we discuss the density of maximum Lee distance codes with respect to this novel Singleton-like bound.","sentences":["This paper provides new and improved Singleton-like bounds for Lee metric codes over integer residue rings.","We derive the bounds using various novel definitions of generalized Lee weights based on different notions of a support of a linear code.","In this regard, we introduce three main different support types for codes in the Lee metric and analyze their utility to derive bounds on the minimum Lee distance.","Eventually, we propose a new point of view to generalized weights and give an improved bound on the minimum distance of codes in the Lee metric for which we discuss the density of maximum Lee distance codes with respect to this novel Singleton-like bound."],"url":"http://arxiv.org/abs/2307.06079v1"}
{"created":"2023-07-12 10:58:27","title":"Group Fairness in Social Choice","abstract":"We consider a voting model, where a number of candidates need to be selected subject to certain feasibility constraints. The model generalises committee elections (where there is a single constraint on the number of candidates that need to be selected), various elections with diversity constraints, the model of public decisions (where decisions needs to be taken on a number of independent issues), and the model of collective scheduling. A critical property of voting is that it should be fair -- not only to individuals but also to groups of voters with similar opinions on the subject of the vote; in other words, the outcome of an election should proportionally reflect the voters' preferences. We formulate axioms of proportionality in this general model. Our axioms do not require predefining groups of voters; to the contrary, we ensure that the opinion of every subset of voters whose preferences are cohesive-enough are taken into account to the extent that is proportional to the size of the subset. Our axioms are always satisfiable, and generalize the strongest known satisfiable axioms for the more specific models. We explain how to adapt two prominent committee election rules, Proportional Approval Voting (PAV) and Phragmen Sequential Rule, as well as the concept of stable-priceability to our general model. The two rules satisfy our proportionality axioms if and only if the feasibility constraints are matroids.","sentences":["We consider a voting model, where a number of candidates need to be selected subject to certain feasibility constraints.","The model generalises committee elections (where there is a single constraint on the number of candidates that need to be selected), various elections with diversity constraints, the model of public decisions (where decisions needs to be taken on a number of independent issues), and the model of collective scheduling.","A critical property of voting is that it should be fair -- not only to individuals but also to groups of voters with similar opinions on the subject of the vote; in other words, the outcome of an election should proportionally reflect the voters' preferences.","We formulate axioms of proportionality in this general model.","Our axioms do not require predefining groups of voters; to the contrary, we ensure that the opinion of every subset of voters whose preferences are cohesive-enough are taken into account to the extent that is proportional to the size of the subset.","Our axioms are always satisfiable, and generalize the strongest known satisfiable axioms for the more specific models.","We explain how to adapt two prominent committee election rules, Proportional Approval Voting (PAV) and Phragmen Sequential Rule, as well as the concept of stable-priceability to our general model.","The two rules satisfy our proportionality axioms if and only if the feasibility constraints are matroids."],"url":"http://arxiv.org/abs/2307.06077v1"}
{"created":"2023-07-12 10:53:02","title":"Integrating Enzyme-generated functions into CoDiPack","abstract":"In operator overloading algorithmic differentiation, it can be beneficial to create custom derivative functions for some parts of the code base. For manual implementations of the derivative functions, it can be quite cumbersome to derive, implement, test, and maintain these. The process can be automated with source transformation algorithmic differentiation tools like Tapenade or compiler-based algorithmic differentiation tools like Enzyme. This eliminates most of the work required from a manual implementation but usually has the same efficiency with respect to timing and memory. We present a new helper in CoDiPack that allows Enzyme-generated derivative functions to be automatically added during the recording process of CoDiPack. The validity of the approach is demonstrated on a synthetic benchmark, which shows promising results.","sentences":["In operator overloading algorithmic differentiation, it can be beneficial to create custom derivative functions for some parts of the code base.","For manual implementations of the derivative functions, it can be quite cumbersome to derive, implement, test, and maintain these.","The process can be automated with source transformation algorithmic differentiation tools like Tapenade or compiler-based algorithmic differentiation tools like Enzyme.","This eliminates most of the work required from a manual implementation but usually has the same efficiency with respect to timing and memory.","We present a new helper in CoDiPack that allows Enzyme-generated derivative functions to be automatically added during the recording process of CoDiPack.","The validity of the approach is demonstrated on a synthetic benchmark, which shows promising results."],"url":"http://arxiv.org/abs/2307.06075v1"}
{"created":"2023-07-12 10:50:01","title":"On the Binary Symmetric Channel with a Transition Probability Determined by a Poisson Distribution","abstract":"The classical Binary Symmetric Channel has a fixed transition probability. We discuss the Binary Symmetric Channel with a variable transition probability that depends on a Poisson distribution. The error rate for this channel is determined and we also give bounds for the channel capacity. We give a motivation for the model based on the Class-A impulse noise model, as given by Middleton. The channel model can be extended to the Additive White Gaussian Channel model, where the noise variance also depends on a Poisson distribution.","sentences":["The classical Binary Symmetric Channel has a fixed transition probability.","We discuss the Binary Symmetric Channel with a variable transition probability that depends on a Poisson distribution.","The error rate for this channel is determined and we also give bounds for the channel capacity.","We give a motivation for the model based on the Class-A impulse noise model, as given by Middleton.","The channel model can be extended to the Additive White Gaussian Channel model, where the noise variance also depends on a Poisson distribution."],"url":"http://arxiv.org/abs/2307.06073v1"}
{"created":"2023-07-12 10:35:27","title":"Security in Online Freelance Software Development: A case for Distributed Security Responsibility","abstract":"Secure software is a cornerstone to safe and resilient digital ecosystems. It offers strong foundation to protect users' sensitive data and guard against cyber-threats. The rapidly increasing landscape of digital economy has encouraged developers from different socio-technical and socio-economic backgrounds to join online freelance marketplaces. While, secure software practices facilitate software developers in developing secure software, there is paucity of research on how freelance developers adhere to security practices and how they can be facilitated to improve their security behavior in under-resourced environments. Moreover, freelance developers are often held responsible for producing insecure code. In this position paper, we review existing literature and argue for the case of distributed security responsibilities in online freelance environment. We propose a research agenda aimed at offering an organized and systematic effort by researchers to address security needs and challenges of online freelance marketplaces. These include: characterising software security and defining separation of responsibilities, building trust in online freelance development communities, leveraging the potential of online freelancing platforms in the promotion of secure software development and building adaptive security interventions for online freelance software development. The research has the potential to bring forth existing security solutions to wider developer community and deliver substantial benefits to the broader security ecosystem.","sentences":["Secure software is a cornerstone to safe and resilient digital ecosystems.","It offers strong foundation to protect users' sensitive data and guard against cyber-threats.","The rapidly increasing landscape of digital economy has encouraged developers from different socio-technical and socio-economic backgrounds to join online freelance marketplaces.","While, secure software practices facilitate software developers in developing secure software, there is paucity of research on how freelance developers adhere to security practices and how they can be facilitated to improve their security behavior in under-resourced environments.","Moreover, freelance developers are often held responsible for producing insecure code.","In this position paper, we review existing literature and argue for the case of distributed security responsibilities in online freelance environment.","We propose a research agenda aimed at offering an organized and systematic effort by researchers to address security needs and challenges of online freelance marketplaces.","These include: characterising software security and defining separation of responsibilities, building trust in online freelance development communities, leveraging the potential of online freelancing platforms in the promotion of secure software development and building adaptive security interventions for online freelance software development.","The research has the potential to bring forth existing security solutions to wider developer community and deliver substantial benefits to the broader security ecosystem."],"url":"http://arxiv.org/abs/2307.06066v1"}
{"created":"2023-07-12 10:29:40","title":"Operational Support Estimator Networks","abstract":"In this work, we propose a novel approach called Operational Support Estimator Networks (OSENs) for the support estimation task. Support Estimation (SE) is defined as finding the locations of non-zero elements in a sparse signal. By its very nature, the mapping between the measurement and sparse signal is a non-linear operation. Traditional support estimators rely on computationally expensive iterative signal recovery techniques to achieve such non-linearity. Contrary to the convolution layers, the proposed OSEN approach consists of operational layers that can learn such complex non-linearities without the need for deep networks. In this way, the performance of the non-iterative support estimation is greatly improved. Moreover, the operational layers comprise so-called generative \\textit{super neurons} with non-local kernels. The kernel location for each neuron/feature map is optimized jointly for the SE task during the training. We evaluate the OSENs in three different applications: i. support estimation from Compressive Sensing (CS) measurements, ii. representation-based classification, and iii. learning-aided CS reconstruction where the output of OSENs is used as prior knowledge to the CS algorithm for an enhanced reconstruction. Experimental results show that the proposed approach achieves computational efficiency and outperforms competing methods, especially at low measurement rates by a significant margin. The software implementation is publicly shared at https://github.com/meteahishali/OSEN.","sentences":["In this work, we propose a novel approach called Operational Support Estimator Networks (OSENs) for the support estimation task.","Support Estimation (SE) is defined as finding the locations of non-zero elements in a sparse signal.","By its very nature, the mapping between the measurement and sparse signal is a non-linear operation.","Traditional support estimators rely on computationally expensive iterative signal recovery techniques to achieve such non-linearity.","Contrary to the convolution layers, the proposed OSEN approach consists of operational layers that can learn such complex non-linearities without the need for deep networks.","In this way, the performance of the non-iterative support estimation is greatly improved.","Moreover, the operational layers comprise so-called generative \\textit{super neurons} with non-local kernels.","The kernel location for each neuron/feature map is optimized jointly for the SE task during the training.","We evaluate the OSENs in three different applications: i. support estimation from Compressive Sensing (CS) measurements, ii. representation-based classification, and iii.","learning-aided CS reconstruction where the output of OSENs is used as prior knowledge to the CS algorithm for an enhanced reconstruction.","Experimental results show that the proposed approach achieves computational efficiency and outperforms competing methods, especially at low measurement rates by a significant margin.","The software implementation is publicly shared at https://github.com/meteahishali/OSEN."],"url":"http://arxiv.org/abs/2307.06065v2"}
{"created":"2023-07-12 10:18:58","title":"How Many Papers Should You Review? A Research Synthesis of Systematic Literature Reviews in Software Engineering","abstract":"[Context] Systematic Literature Review (SLR) has been a major type of study published in Software Engineering (SE) venues for about two decades. However, there is a lack of understanding of whether an SLR is really needed in comparison to a more conventional literature review. Very often, SE researchers embark on an SLR with such doubts. We aspire to provide more understanding of when an SLR in SE should be conducted. [Objective] The first step of our investigation was focused on the dataset, i.e., the reviewed papers, in an SLR, which indicates the development of a research topic or area. The objective of this step is to provide a better understanding of the characteristics of the datasets of SLRs in SE. [Method] A research synthesis was conducted on a sample of 170 SLRs published in top-tier SE journals. We extracted and analysed the quantitative attributes of the datasets of these SLRs. [Results] The findings show that the median size of the datasets in our sample is 57 reviewed papers, and the median review period covered is 14 years. The number of reviewed papers and review period have a very weak and non-significant positive correlation. [Conclusions] The results of our study can be used by SE researchers as an indicator or benchmark to understand whether an SLR is conducted at a good time.","sentences":["[Context] Systematic Literature Review (SLR) has been a major type of study published in Software Engineering (SE) venues for about two decades.","However, there is a lack of understanding of whether an SLR is really needed in comparison to a more conventional literature review.","Very often, SE researchers embark on an SLR with such doubts.","We aspire to provide more understanding of when an SLR in SE should be conducted.","[Objective] The first step of our investigation was focused on the dataset, i.e., the reviewed papers, in an SLR, which indicates the development of a research topic or area.","The objective of this step is to provide a better understanding of the characteristics of the datasets of SLRs in SE.","[Method] A research synthesis was conducted on a sample of 170 SLRs published in top-tier SE journals.","We extracted and analysed the quantitative attributes of the datasets of these SLRs.","[Results]","The findings show that the median size of the datasets in our sample is 57 reviewed papers, and the median review period covered is 14 years.","The number of reviewed papers and review period have a very weak and non-significant positive correlation.","[Conclusions] The results of our study can be used by SE researchers as an indicator or benchmark to understand whether an SLR is conducted at a good time."],"url":"http://arxiv.org/abs/2307.06056v1"}
{"created":"2023-07-12 10:17:54","title":"Function-Space Regularization for Deep Bayesian Classification","abstract":"Bayesian deep learning approaches assume model parameters to be latent random variables and infer posterior distributions to quantify uncertainty, increase safety and trust, and prevent overconfident and unpredictable behavior. However, weight-space priors are model-specific, can be difficult to interpret and are hard to specify. Instead, we apply a Dirichlet prior in predictive space and perform approximate function-space variational inference. To this end, we interpret conventional categorical predictions from stochastic neural network classifiers as samples from an implicit Dirichlet distribution. By adapting the inference, the same function-space prior can be combined with different models without affecting model architecture or size. We illustrate the flexibility and efficacy of such a prior with toy experiments and demonstrate scalability, improved uncertainty quantification and adversarial robustness with large-scale image classification experiments.","sentences":["Bayesian deep learning approaches assume model parameters to be latent random variables and infer posterior distributions to quantify uncertainty, increase safety and trust, and prevent overconfident and unpredictable behavior.","However, weight-space priors are model-specific, can be difficult to interpret and are hard to specify.","Instead, we apply a Dirichlet prior in predictive space and perform approximate function-space variational inference.","To this end, we interpret conventional categorical predictions from stochastic neural network classifiers as samples from an implicit Dirichlet distribution.","By adapting the inference, the same function-space prior can be combined with different models without affecting model architecture or size.","We illustrate the flexibility and efficacy of such a prior with toy experiments and demonstrate scalability, improved uncertainty quantification and adversarial robustness with large-scale image classification experiments."],"url":"http://arxiv.org/abs/2307.06055v1"}
{"created":"2023-07-12 10:12:57","title":"Visualization for Multivariate Gaussian Anomaly Detection in Images","abstract":"This paper introduces a simplified variation of the PaDiM (Pixel-Wise Anomaly Detection through Instance Modeling) method for anomaly detection in images, fitting a single multivariate Gaussian (MVG) distribution to the feature vectors extracted from a backbone convolutional neural network (CNN) and using their Mahalanobis distance as the anomaly score. We introduce an intermediate step in this framework by applying a whitening transformation to the feature vectors, which enables the generation of heatmaps capable of visually explaining the features learned by the MVG. The proposed technique is evaluated on the MVTec-AD dataset, and the results show the importance of visual model validation, providing insights into issues in this framework that were otherwise invisible. The visualizations generated for this paper are publicly available at https://doi.org/10.5281/zenodo.7937978.","sentences":["This paper introduces a simplified variation of the PaDiM (Pixel-Wise Anomaly Detection through Instance Modeling) method for anomaly detection in images, fitting a single multivariate Gaussian (MVG) distribution to the feature vectors extracted from a backbone convolutional neural network (CNN) and using their Mahalanobis distance as the anomaly score.","We introduce an intermediate step in this framework by applying a whitening transformation to the feature vectors, which enables the generation of heatmaps capable of visually explaining the features learned by the MVG.","The proposed technique is evaluated on the MVTec-AD dataset, and the results show the importance of visual model validation, providing insights into issues in this framework that were otherwise invisible.","The visualizations generated for this paper are publicly available at https://doi.org/10.5281/zenodo.7937978."],"url":"http://arxiv.org/abs/2307.06052v1"}
{"created":"2023-07-12 10:10:24","title":"A Study on the Appropriate size of the Mongolian general corpus","abstract":"This study aims to determine the appropriate size of the Mongolian general corpus. This study used the Heaps function and Type Token Ratio to determine the appropriate size of the Mongolian general corpus. The sample corpus of 906,064 tokens comprised texts from 10 domains of newspaper politics, economy, society, culture, sports, world articles and laws, middle and high school literature textbooks, interview articles, and podcast transcripts. First, we estimated the Heaps function with this sample corpus. Next, we observed changes in the number of types and TTR values while increasing the number of tokens by one million using the estimated Heaps function. As a result of observation, we found that the TTR value hardly changed when the number of tokens exceeded from 39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian general corpus is from 39 to 42 million tokens.","sentences":["This study aims to determine the appropriate size of the Mongolian general corpus.","This study used the Heaps function and Type Token Ratio to determine the appropriate size of the Mongolian general corpus.","The sample corpus of 906,064 tokens comprised texts from 10 domains of newspaper politics, economy, society, culture, sports, world articles and laws, middle and high school literature textbooks, interview articles, and podcast transcripts.","First, we estimated the Heaps function with this sample corpus.","Next, we observed changes in the number of types and TTR values while increasing the number of tokens by one million using the estimated Heaps function.","As a result of observation, we found that the TTR value hardly changed when the number of tokens exceeded from 39 to 42 million.","Thus, we conclude that an appropriate size for a Mongolian general corpus is from 39 to 42 million tokens."],"url":"http://arxiv.org/abs/2307.06050v1"}
{"created":"2023-07-12 09:49:15","title":"An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes","abstract":"The task of inductive link prediction in (discrete) attributed multigraphs infers missing attributed links (relations) between nodes in new test multigraphs. Traditional relational learning methods face the challenge of limited generalization to OOD test multigraphs containing both novel nodes and novel relation types not seen in training. Recently, under the only assumption that all relation types share the same structural predictive patterns (single task), Gao et al. (2023) proposed an OOD link prediction method using the theoretical concept of double exchangeability (for nodes & relation types), in contrast to the (single) exchangeability (only for nodes) used to design Graph Neural Networks (GNNs). In this work we further extend the double exchangeability concept to multi-task double exchangeability, where we define link prediction in attributed multigraphs that can have distinct and potentially conflicting predictive patterns for different sets of relation types (multiple tasks). Our empirical results on real-world datasets demonstrate that our approach can effectively generalize to entirely new relation types in test, without access to additional information, yielding significant performance improvements over existing methods.","sentences":["The task of inductive link prediction in (discrete) attributed multigraphs infers missing attributed links (relations) between nodes in new test multigraphs.","Traditional relational learning methods face the challenge of limited generalization to OOD test multigraphs containing both novel nodes and novel relation types not seen in training.","Recently, under the only assumption that all relation types share the same structural predictive patterns (single task), Gao et al. (2023) proposed an OOD link prediction method using the theoretical concept of double exchangeability (for nodes & relation types), in contrast to the (single) exchangeability (only for nodes) used to design Graph Neural Networks (GNNs).","In this work we further extend the double exchangeability concept to multi-task double exchangeability, where we define link prediction in attributed multigraphs that can have distinct and potentially conflicting predictive patterns for different sets of relation types (multiple tasks).","Our empirical results on real-world datasets demonstrate that our approach can effectively generalize to entirely new relation types in test, without access to additional information, yielding significant performance improvements over existing methods."],"url":"http://arxiv.org/abs/2307.06046v1"}
{"created":"2023-07-12 09:33:21","title":"Pyramid Deep Fusion Network for Two-Hand Reconstruction from RGB-D Images","abstract":"Accurately recovering the dense 3D mesh of both hands from monocular images poses considerable challenges due to occlusions and projection ambiguity. Most of the existing methods extract features from color images to estimate the root-aligned hand meshes, which neglect the crucial depth and scale information in the real world. Given the noisy sensor measurements with limited resolution, depth-based methods predict 3D keypoints rather than a dense mesh. These limitations motivate us to take advantage of these two complementary inputs to acquire dense hand meshes on a real-world scale. In this work, we propose an end-to-end framework for recovering dense meshes for both hands, which employ single-view RGB-D image pairs as input. The primary challenge lies in effectively utilizing two different input modalities to mitigate the blurring effects in RGB images and noises in depth images. Instead of directly treating depth maps as additional channels for RGB images, we encode the depth information into the unordered point cloud to preserve more geometric details. Specifically, our framework employs ResNet50 and PointNet++ to derive features from RGB and point cloud, respectively. Additionally, we introduce a novel pyramid deep fusion network (PDFNet) to aggregate features at different scales, which demonstrates superior efficacy compared to previous fusion strategies. Furthermore, we employ a GCN-based decoder to process the fused features and recover the corresponding 3D pose and dense mesh. Through comprehensive ablation experiments, we have not only demonstrated the effectiveness of our proposed fusion algorithm but also outperformed the state-of-the-art approaches on publicly available datasets. To reproduce the results, we will make our source code and models publicly available at {\\url{https://github.com/zijinxuxu/PDFNet}}.","sentences":["Accurately recovering the dense 3D mesh of both hands from monocular images poses considerable challenges due to occlusions and projection ambiguity.","Most of the existing methods extract features from color images to estimate the root-aligned hand meshes, which neglect the crucial depth and scale information in the real world.","Given the noisy sensor measurements with limited resolution, depth-based methods predict 3D keypoints rather than a dense mesh.","These limitations motivate us to take advantage of these two complementary inputs to acquire dense hand meshes on a real-world scale.","In this work, we propose an end-to-end framework for recovering dense meshes for both hands, which employ single-view RGB-D image pairs as input.","The primary challenge lies in effectively utilizing two different input modalities to mitigate the blurring effects in RGB images and noises in depth images.","Instead of directly treating depth maps as additional channels for RGB images, we encode the depth information into the unordered point cloud to preserve more geometric details.","Specifically, our framework employs ResNet50 and PointNet++ to derive features from RGB and point cloud, respectively.","Additionally, we introduce a novel pyramid deep fusion network (PDFNet) to aggregate features at different scales, which demonstrates superior efficacy compared to previous fusion strategies.","Furthermore, we employ a GCN-based decoder to process the fused features and recover the corresponding 3D pose and dense mesh.","Through comprehensive ablation experiments, we have not only demonstrated the effectiveness of our proposed fusion algorithm but also outperformed the state-of-the-art approaches on publicly available datasets.","To reproduce the results, we will make our source code and models publicly available at {\\url{https://github.com/zijinxuxu/PDFNet}}."],"url":"http://arxiv.org/abs/2307.06038v1"}
{"created":"2023-07-12 09:32:25","title":"Rate-Power Tradeoff in THz SWIPT Systems Employing Resonant Tunnelling Diode-based EH Circuits","abstract":"In this paper, we study THz simultaneous wireless information and power transfer (SWIPT) systems. Since coherent information detection is challenging at THz frequencies and Schottky diodes may not be efficient for THz energy harvesting (EH) and information detection, we employ unipolar amplitude shift keying (ASK) modulation at the transmitter (TX) and a resonant tunnelling diode (RTD)-based EH circuit at the receiver (RX) to extract both information and power from the RX signal. We model the dependence of the instantaneous output power at the RX on the instantaneous received power by a non-linear piecewise function, whose parameters are adjusted to fit circuit simulation results. To determine the rate-power tradeoff in THz SWIPT systems, we derive the distribution of the TX signal that maximizes the mutual information between the TX and RX signals subject to constraints on the required average harvested power at the RX and the peak signal amplitude at the TX. Since the computational complexity of maximizing the mutual information may be too high for real-time THz SWIPT systems, for high and low required average harvested powers, we also obtain the suboptimal input signal distribution that maximizes the achievable information rate numerically and in closed form, respectively. Furthermore, based on the obtained results, we propose a suboptimal closed-form TX distribution which also achieves a desired harvested power at the RX. Our simulation results show that a lower reverse current flow and a higher breakdown voltage of the employed RTD are preferable when the input signal power at the RX is low and high, respectively. Finally, we demonstrate that for low and high received signal powers, the rate-power tradeoff of THz SWIPT systems is determined by the peak amplitude of the TX signal and the maximum instantaneous harvested power, respectively.","sentences":["In this paper, we study THz simultaneous wireless information and power transfer (SWIPT) systems.","Since coherent information detection is challenging at THz frequencies and Schottky diodes may not be efficient for THz energy harvesting (EH) and information detection, we employ unipolar amplitude shift keying (ASK) modulation at the transmitter (TX) and a resonant tunnelling diode (RTD)-based EH circuit at the receiver (RX) to extract both information and power from the RX signal.","We model the dependence of the instantaneous output power at the RX on the instantaneous received power by a non-linear piecewise function, whose parameters are adjusted to fit circuit simulation results.","To determine the rate-power tradeoff in THz SWIPT systems, we derive the distribution of the TX signal that maximizes the mutual information between the TX and RX signals subject to constraints on the required average harvested power at the RX and the peak signal amplitude at the TX.","Since the computational complexity of maximizing the mutual information may be too high for real-time THz SWIPT systems, for high and low required average harvested powers, we also obtain the suboptimal input signal distribution that maximizes the achievable information rate numerically and in closed form, respectively.","Furthermore, based on the obtained results, we propose a suboptimal closed-form TX distribution which also achieves a desired harvested power at the RX.","Our simulation results show that a lower reverse current flow and a higher breakdown voltage of the employed RTD are preferable when the input signal power at the RX is low and high, respectively.","Finally, we demonstrate that for low and high received signal powers, the rate-power tradeoff of THz SWIPT systems is determined by the peak amplitude of the TX signal and the maximum instantaneous harvested power, respectively."],"url":"http://arxiv.org/abs/2307.06036v1"}
{"created":"2023-07-12 09:25:56","title":"AI-Generated Imagery: A New Era for the `Readymade'","abstract":"While the term `art' defies any concrete definition, this paper aims to examine how digital images produced by generative AI systems, such as Midjourney, have come to be so regularly referred to as such. The discourse around the classification of AI-generated imagery as art is currently somewhat homogeneous, lacking the more nuanced aspects that would apply to more traditional modes of artistic media production. This paper aims to bring important philosophical considerations to the surface of the discussion around AI-generated imagery in the context of art. We employ existing philosophical frameworks and theories of language to suggest that some AI-generated imagery, by virtue of its visual properties within these frameworks, can be presented as `readymades' for consideration as art.","sentences":["While the term `art' defies any concrete definition, this paper aims to examine how digital images produced by generative AI systems, such as Midjourney, have come to be so regularly referred to as such.","The discourse around the classification of AI-generated imagery as art is currently somewhat homogeneous, lacking the more nuanced aspects that would apply to more traditional modes of artistic media production.","This paper aims to bring important philosophical considerations to the surface of the discussion around AI-generated imagery in the context of art.","We employ existing philosophical frameworks and theories of language to suggest that some AI-generated imagery, by virtue of its visual properties within these frameworks, can be presented as `readymades' for consideration as art."],"url":"http://arxiv.org/abs/2307.06033v1"}
{"created":"2023-07-12 09:23:41","title":"Pluggable Neural Machine Translation Models via Memory-augmented Adapters","abstract":"Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.","sentences":["Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users.","Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner.","Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results.","We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory.","We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines."],"url":"http://arxiv.org/abs/2307.06029v1"}
{"created":"2023-07-12 09:16:33","title":"Semantic Communications System with Model Division Multiple Access and Controllable Coding Rate for Point Cloud","abstract":"Point cloud, as a 3D representation, is widely used in autonomous driving, virtual reality (VR), and augmented reality (AR). However, traditional communication systems think that the point cloud's semantic information is irrelevant to communication, which hinders the efficient transmission of point clouds in the era of artificial intelligence (AI). This paper proposes a point cloud based semantic communication system (PCSC), which uses AI-based encoding techniques to extract the semantic information of the point cloud and joint source-channel coding (JSCC) technology to overcome the distortion caused by noise channels and solve the \"cliff effect\" in traditional communication. In addition, the system realizes the controllable coding rate without fine-tuning the network. The method analyzes the coded semantic vector's importance and discards semantically-unimportant information, thereby improving the transmission efficiency. Besides, PCSC and the recently proposed non-orthogonal model division multiple access (MDMA) technology are combined to design a point cloud MDMA transmission system (M-PCSC) for multi-user transmission. Relevant experimental results show that the proposed method outperforms the traditional method 10dB in the same channel bandwidth ratio under the PSNR D1 and PSNR D2 metrics. In terms of transmission, the proposed method can effectively solve the \"cliff effect\" in the traditional methods.","sentences":["Point cloud, as a 3D representation, is widely used in autonomous driving, virtual reality (VR), and augmented reality (AR).","However, traditional communication systems think that the point cloud's semantic information is irrelevant to communication, which hinders the efficient transmission of point clouds in the era of artificial intelligence (AI).","This paper proposes a point cloud based semantic communication system (PCSC), which uses AI-based encoding techniques to extract the semantic information of the point cloud and joint source-channel coding (JSCC) technology to overcome the distortion caused by noise channels and solve the \"cliff effect\" in traditional communication.","In addition, the system realizes the controllable coding rate without fine-tuning the network.","The method analyzes the coded semantic vector's importance and discards semantically-unimportant information, thereby improving the transmission efficiency.","Besides, PCSC and the recently proposed non-orthogonal model division multiple access (MDMA) technology are combined to design a point cloud MDMA transmission system (M-PCSC) for multi-user transmission.","Relevant experimental results show that the proposed method outperforms the traditional method 10dB in the same channel bandwidth ratio under the PSNR D1 and PSNR D2 metrics.","In terms of transmission, the proposed method can effectively solve the \"cliff effect\" in the traditional methods."],"url":"http://arxiv.org/abs/2307.06027v1"}
{"created":"2023-07-12 09:14:35","title":"Learning from Exemplary Explanations","abstract":"eXplanation Based Learning (XBL) is a form of Interactive Machine Learning (IML) that provides a model refining approach via user feedback collected on model explanations. Although the interactivity of XBL promotes model transparency, XBL requires a huge amount of user interaction and can become expensive as feedback is in the form of detailed annotation rather than simple category labelling which is more common in IML. This expense is exacerbated in high stakes domains such as medical image classification. To reduce the effort and expense of XBL we introduce a new approach that uses two input instances and their corresponding Gradient Weighted Class Activation Mapping (GradCAM) model explanations as exemplary explanations to implement XBL. Using a medical image classification task, we demonstrate that, using minimal human input, our approach produces improved explanations (+0.02, +3%) and achieves reduced classification performance (-0.04, -4%) when compared against a model trained without interactions.","sentences":["eXplanation Based Learning (XBL) is a form of Interactive Machine Learning (IML) that provides a model refining approach via user feedback collected on model explanations.","Although the interactivity of XBL promotes model transparency, XBL requires a huge amount of user interaction and can become expensive as feedback is in the form of detailed annotation rather than simple category labelling which is more common in IML.","This expense is exacerbated in high stakes domains such as medical image classification.","To reduce the effort and expense of XBL we introduce a new approach that uses two input instances and their corresponding Gradient Weighted Class Activation Mapping (GradCAM) model explanations as exemplary explanations to implement XBL.","Using a medical image classification task, we demonstrate that, using minimal human input, our approach produces improved explanations (+0.02, +3%) and achieves reduced classification performance (-0.04, -4%) when compared against a model trained without interactions."],"url":"http://arxiv.org/abs/2307.06026v1"}
{"created":"2023-07-12 09:05:07","title":"On the Uplink Distributed Detection in UAV-enabled Aerial Cell-Free mMIMO Systems","abstract":"In this paper, we investigate the uplink signal detection approaches in the cell-free massive MIMO systems with unmanned aerial vehicles (UAVs) serving as aerial access points (APs). The ground users are equipped with multiple antennas and the ground-to-air propagation channels are subject to correlated Rician fading. To overcome huge signaling overhead in the fully-centralized detection, we propose a two-layer distributed uplink detection scheme, where the uplink signals are first detected in the AP-UAVs by using the minimum mean-squared error (MMSE) detector depending on local channel state information (CSI), and then collected and weighted combined at the CPU-UAV to obtain the refined detection. By using the operator-valued free probability theory, the asymptotic expressions of the combining weights are obtained, which only depend on the statistical CSI and show excellent accuracy. Based on the proposed distributed scheme, we further investigate the impacts of different distributed deployments on the achieved spectral efficiency (SE). Numerical results show that in urban and dense urban environments, it is more beneficial to deploy more AP-UAVs to achieve higher SE. On the other hand, in suburban environment, an optimal ratio between the number of deployed UAVs and the number of antennas per UAV exists to maximize the SE.","sentences":["In this paper, we investigate the uplink signal detection approaches in the cell-free massive MIMO systems with unmanned aerial vehicles (UAVs) serving as aerial access points (APs).","The ground users are equipped with multiple antennas and the ground-to-air propagation channels are subject to correlated Rician fading.","To overcome huge signaling overhead in the fully-centralized detection, we propose a two-layer distributed uplink detection scheme, where the uplink signals are first detected in the AP-UAVs by using the minimum mean-squared error (MMSE) detector depending on local channel state information (CSI), and then collected and weighted combined at the CPU-UAV to obtain the refined detection.","By using the operator-valued free probability theory, the asymptotic expressions of the combining weights are obtained, which only depend on the statistical CSI and show excellent accuracy.","Based on the proposed distributed scheme, we further investigate the impacts of different distributed deployments on the achieved spectral efficiency (SE).","Numerical results show that in urban and dense urban environments, it is more beneficial to deploy more AP-UAVs to achieve higher SE.","On the other hand, in suburban environment, an optimal ratio between the number of deployed UAVs and the number of antennas per UAV exists to maximize the SE."],"url":"http://arxiv.org/abs/2307.06023v1"}
{"created":"2023-07-12 09:00:37","title":"PolyLM: An Open Source Polyglot Large Language Model","abstract":"Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, generation, and translation. Extensive experiments show that PolyLM surpasses other open-source models such as LLaMA and BLOOM on multilingual tasks while maintaining comparable performance in English. Our models, alone with the instruction data and multilingual benchmark, are available at: \\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.","sentences":["Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions.","However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages.","Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training.","Further, we propose a multilingual self-instruct method which automatically generates 132.7K diverse multilingual instructions for model fine-tuning.","To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, generation, and translation.","Extensive experiments show that PolyLM surpasses other open-source models such as LLaMA and BLOOM on multilingual tasks while maintaining comparable performance in English.","Our models, alone with the instruction data and multilingual benchmark, are available at: \\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}."],"url":"http://arxiv.org/abs/2307.06018v1"}
{"created":"2023-07-12 08:59:21","title":"Safety and Liveness of Quantitative Automata","abstract":"The safety-liveness dichotomy is a fundamental concept in formal languages which plays a key role in verification. Recently, this dichotomy has been lifted to quantitative properties, which are arbitrary functions from infinite words to partially-ordered domains. We look into harnessing the dichotomy for the specific classes of quantitative properties expressed by quantitative automata. These automata contain finitely many states and rational-valued transition weights, and their common value functions Inf, Sup, LimInf, LimSup, LimInfAvg, LimSupAvg, and DSum map infinite words into the totally-ordered domain of real numbers. In this automata-theoretic setting, we establish a connection between quantitative safety and topological continuity and provide an alternative characterization of quantitative safety and liveness in terms of their boolean counterparts. For all common value functions, we show how the safety closure of a quantitative automaton can be constructed in PTime, and we provide PSpace-complete checks of whether a given quantitative automaton is safe or live, with the exception of LimInfAvg and LimSupAvg automata, for which the safety check is in ExpSpace. Moreover, for deterministic Sup, LimInf, and LimSup automata, we give PTime decompositions into safe and live automata. These decompositions enable the separation of techniques for safety and liveness verification for quantitative specifications.","sentences":["The safety-liveness dichotomy is a fundamental concept in formal languages which plays a key role in verification.","Recently, this dichotomy has been lifted to quantitative properties, which are arbitrary functions from infinite words to partially-ordered domains.","We look into harnessing the dichotomy for the specific classes of quantitative properties expressed by quantitative automata.","These automata contain finitely many states and rational-valued transition weights, and their common value functions Inf, Sup, LimInf, LimSup, LimInfAvg, LimSupAvg, and DSum map infinite words into the totally-ordered domain of real numbers.","In this automata-theoretic setting, we establish a connection between quantitative safety and topological continuity and provide an alternative characterization of quantitative safety and liveness in terms of their boolean counterparts.","For all common value functions, we show how the safety closure of a quantitative automaton can be constructed in PTime, and we provide PSpace-complete checks of whether a given quantitative automaton is safe or live, with the exception of LimInfAvg and LimSupAvg automata, for which the safety check is in ExpSpace.","Moreover, for deterministic Sup, LimInf, and LimSup automata, we give PTime decompositions into safe and live automata.","These decompositions enable the separation of techniques for safety and liveness verification for quantitative specifications."],"url":"http://arxiv.org/abs/2307.06016v1"}
{"created":"2023-07-12 08:51:20","title":"An Effective and Efficient Time-aware Entity Alignment Framework via Two-aspect Three-view Label Propagation","abstract":"Entity alignment (EA) aims to find the equivalent entity pairs between different knowledge graphs (KGs), which is crucial to promote knowledge fusion. With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA) methods appear to enhance EA. Existing TEA models are based on Graph Neural Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is difficult to transfer them to large-scale TKGs due to the scalability issue of GNN. In this paper, we propose an effective and efficient non-neural EA framework between TKGs, namely LightTEA, which consists of four essential components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative Learning. All of these modules work together to improve the performance of EA while reducing the time consumption of the model. Extensive experiments on public datasets indicate that our proposed model significantly outperforms the SOTA methods for EA between TKGs, and the time consumed by LightTEA is only dozens of seconds at most, no more than 10% of the most efficient TEA method.","sentences":["Entity alignment (EA) aims to find the equivalent entity pairs between different knowledge graphs (KGs), which is crucial to promote knowledge fusion.","With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA) methods appear to enhance EA.","Existing TEA models are based on Graph Neural Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is difficult to transfer them to large-scale TKGs due to the scalability issue of GNN.","In this paper, we propose an effective and efficient non-neural EA framework between TKGs, namely LightTEA, which consists of four essential components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative Learning.","All of these modules work together to improve the performance of EA while reducing the time consumption of the model.","Extensive experiments on public datasets indicate that our proposed model significantly outperforms the SOTA methods for EA between TKGs, and the time consumed by LightTEA is only dozens of seconds at most, no more than 10% of the most efficient TEA method."],"url":"http://arxiv.org/abs/2307.06013v1"}
{"created":"2023-07-12 08:36:41","title":"Building Persuasive Robots with Social Power Strategies","abstract":"Can social power endow social robots with the capacity to persuade? This paper represents our recent endeavor to design persuasive social robots. We have designed and run three different user studies to investigate the effectiveness of different bases of social power (inspired by French and Raven's theory) on peoples' compliance to the requests of social robots. The results show that robotic persuaders that exert social power (specifically from expert, reward, and coercion bases) demonstrate increased ability to influence humans. The first study provides a positive answer and shows that under the same circumstances, people with different personalities prefer robots using a specific social power base. In addition, social rewards can be useful in persuading individuals. The second study suggests that by employing social power, social robots are capable of persuading people objectively to select a less desirable choice among others. Finally, the third study shows that the effect of power on persuasion does not decay over time and might strengthen under specific circumstances. Moreover, exerting stronger social power does not necessarily lead to higher persuasion. Overall, we argue that the results of these studies are relevant for designing human--robot-interaction scenarios especially the ones aiming at behavioral change.","sentences":["Can social power endow social robots with the capacity to persuade?","This paper represents our recent endeavor to design persuasive social robots.","We have designed and run three different user studies to investigate the effectiveness of different bases of social power (inspired by French and Raven's theory) on peoples' compliance to the requests of social robots.","The results show that robotic persuaders that exert social power (specifically from expert, reward, and coercion bases) demonstrate increased ability to influence humans.","The first study provides a positive answer and shows that under the same circumstances, people with different personalities prefer robots using a specific social power base.","In addition, social rewards can be useful in persuading individuals.","The second study suggests that by employing social power, social robots are capable of persuading people objectively to select a less desirable choice among others.","Finally, the third study shows that the effect of power on persuasion does not decay over time and might strengthen under specific circumstances.","Moreover, exerting stronger social power does not necessarily lead to higher persuasion.","Overall, we argue that the results of these studies are relevant for designing human--robot-interaction scenarios especially the ones aiming at behavioral change."],"url":"http://arxiv.org/abs/2307.06007v1"}
{"created":"2023-07-12 08:35:24","title":"What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation","abstract":"The pretrain-finetune paradigm usually improves downstream performance over training a model from scratch on the same task, becoming commonplace across many areas of machine learning. While pretraining is empirically observed to be beneficial for a range of tasks, there is not a clear understanding yet of the reasons for this effect. In this work, we examine the relationship between pretrained vision transformers and the corresponding finetuned versions on several benchmark datasets and tasks. We present new metrics that specifically investigate the degree to which invariances learned by a pretrained model are retained or forgotten during finetuning. Using these metrics, we present a suite of empirical findings, including that pretraining induces transferable invariances in shallow layers and that invariances from deeper pretrained layers are compressed towards shallower layers during finetuning. Together, these findings contribute to understanding some of the reasons for the successes of pretrained models and the changes that a pretrained model undergoes when finetuned on a downstream task.","sentences":["The pretrain-finetune paradigm usually improves downstream performance over training a model from scratch on the same task, becoming commonplace across many areas of machine learning.","While pretraining is empirically observed to be beneficial for a range of tasks, there is not a clear understanding yet of the reasons for this effect.","In this work, we examine the relationship between pretrained vision transformers and the corresponding finetuned versions on several benchmark datasets and tasks.","We present new metrics that specifically investigate the degree to which invariances learned by a pretrained model are retained or forgotten during finetuning.","Using these metrics, we present a suite of empirical findings, including that pretraining induces transferable invariances in shallow layers and that invariances from deeper pretrained layers are compressed towards shallower layers during finetuning.","Together, these findings contribute to understanding some of the reasons for the successes of pretrained models and the changes that a pretrained model undergoes when finetuned on a downstream task."],"url":"http://arxiv.org/abs/2307.06006v1"}
{"created":"2023-07-12 08:33:16","title":"DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification","abstract":"Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input. This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the candidates of NAS building blocks, its promising performance is noticeable and extensible to obtain further improvement by adding more different operations.","sentences":["Neural Architecture Search (NAS) has shown promising capability in learning text representation.","However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input.","This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification.","With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search.","We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation.","Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods.","While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the candidates of NAS building blocks, its promising performance is noticeable and extensible to obtain further improvement by adding more different operations."],"url":"http://arxiv.org/abs/2307.06005v1"}
{"created":"2023-07-12 08:30:42","title":"Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera","abstract":"Efficiently selecting an appropriate spike stream data length to extract precise information is the key to the spike vision tasks. To address this issue, we propose a dynamic timing representation for spike streams. Based on multi-layers architecture, it applies dilated convolutions on temporal dimension to extract features on multi-temporal scales with few parameters. And we design layer attention to dynamically fuse these features. Moreover, we propose an unsupervised learning method for optical flow estimation in a spike-based manner to break the dependence on labeled data. In addition, to verify the robustness, we also build a spike-based synthetic validation dataset for extreme scenarios in autonomous driving, denoted as SSES dataset. It consists of various corner cases. Experiments show that our method can predict optical flow from spike streams in different high-speed scenes, including real scenes. For instance, our method gets $15\\%$ and $19\\%$ error reduction from the best spike-based work, SCFlow, in $\\Delta t=10$ and $\\Delta t=20$ respectively which are the same settings as the previous works.","sentences":["Efficiently selecting an appropriate spike stream data length to extract precise information is the key to the spike vision tasks.","To address this issue, we propose a dynamic timing representation for spike streams.","Based on multi-layers architecture, it applies dilated convolutions on temporal dimension to extract features on multi-temporal scales with few parameters.","And we design layer attention to dynamically fuse these features.","Moreover, we propose an unsupervised learning method for optical flow estimation in a spike-based manner to break the dependence on labeled data.","In addition, to verify the robustness, we also build a spike-based synthetic validation dataset for extreme scenarios in autonomous driving, denoted as SSES dataset.","It consists of various corner cases.","Experiments show that our method can predict optical flow from spike streams in different high-speed scenes, including real scenes.","For instance, our method gets $15\\%$ and $19\\%$ error reduction from the best spike-based work, SCFlow, in $\\Delta t=10$ and $\\Delta t=20$ respectively which are the same settings as the previous works."],"url":"http://arxiv.org/abs/2307.06003v1"}
{"created":"2023-07-12 08:26:42","title":"Reactive and human-in-the-loop planning and control of multi-robot systems under LTL specifications in dynamic environments","abstract":"This paper investigates the planning and control problems for multi-robot systems under linear temporal logic (LTL) specifications. In contrast to most of existing literature, which presumes a static and known environment, our study focuses on dynamic environments that can have unknown moving obstacles like humans walking through. Depending on whether local communication is allowed between robots, we consider two different online re-planning approaches. When local communication is allowed, we propose a local trajectory generation algorithm for each robot to resolve conflicts that are detected on-line. In the other case, i.e., no communication is allowed, we develop a model predictive controller to reactively avoid potential collisions. In both cases, task satisfaction is guaranteed whenever it is feasible. In addition, we consider the human-in-the-loop scenario where humans may additionally take control of one or multiple robots. We design a mixed initiative controller for each robot to prevent unsafe human behaviors while guarantee the LTL satisfaction. Using our previous developed ROS software package, several experiments are conducted to demonstrate the effectiveness and the applicability of the proposed strategies.","sentences":["This paper investigates the planning and control problems for multi-robot systems under linear temporal logic (LTL) specifications.","In contrast to most of existing literature, which presumes a static and known environment, our study focuses on dynamic environments that can have unknown moving obstacles like humans walking through.","Depending on whether local communication is allowed between robots, we consider two different online re-planning approaches.","When local communication is allowed, we propose a local trajectory generation algorithm for each robot to resolve conflicts that are detected on-line.","In the other case, i.e., no communication is allowed, we develop a model predictive controller to reactively avoid potential collisions.","In both cases, task satisfaction is guaranteed whenever it is feasible.","In addition, we consider the human-in-the-loop scenario where humans may additionally take control of one or multiple robots.","We design a mixed initiative controller for each robot to prevent unsafe human behaviors while guarantee the LTL satisfaction.","Using our previous developed ROS software package, several experiments are conducted to demonstrate the effectiveness and the applicability of the proposed strategies."],"url":"http://arxiv.org/abs/2307.06000v1"}
{"created":"2023-07-12 08:26:27","title":"Flexible and Fully Quantized Ultra-Lightweight TinyissimoYOLO for Ultra-Low-Power Edge Systems","abstract":"This paper deploys and explores variants of TinyissimoYOLO, a highly flexible and fully quantized ultra-lightweight object detection network designed for edge systems with a power envelope of a few milliwatts. With experimental measurements, we present a comprehensive characterization of the network's detection performance, exploring the impact of various parameters, including input resolution, number of object classes, and hidden layer adjustments. We deploy variants of TinyissimoYOLO on state-of-the-art ultra-low-power extreme edge platforms, presenting an in-depth a comparison on latency, energy efficiency, and their ability to efficiently parallelize the workload. In particular, the paper presents a comparison between a novel parallel RISC-V processor (GAP9 from Greenwaves) with and without use of its on-chip hardware accelerator, an ARM Cortex-M7 core (STM32H7 from ST Microelectronics), two ARM Cortex-M4 cores (STM32L4 from STM and Apollo4b from Ambiq), and a multi-core platform with a CNN hardware accelerator (Analog Devices MAX78000). Experimental results show that the GAP9's hardware accelerator achieves the lowest inference latency and energy at 2.12ms and 150uJ respectively, which is around 2x faster and 20% more efficient than the next best platform, the MAX78000. The hardware accelerator of GAP9 can even run an increased resolution version of TinyissimoYOLO with 112x112 pixels and 10 detection classes within 3.2ms, consuming 245uJ. To showcase the competitiveness of a versatile general-purpose system we also deployed and profiled a multi-core implementation on GAP9 at different operating points, achieving 11.3ms with the lowest-latency and 490uJ with the most energy-efficient configuration. With this paper, we demonstrate the suitability and flexibility of TinyissimoYOLO on state-of-the-art detection datasets for real-time ultra-low-power edge inference.","sentences":["This paper deploys and explores variants of TinyissimoYOLO, a highly flexible and fully quantized ultra-lightweight object detection network designed for edge systems with a power envelope of a few milliwatts.","With experimental measurements, we present a comprehensive characterization of the network's detection performance, exploring the impact of various parameters, including input resolution, number of object classes, and hidden layer adjustments.","We deploy variants of TinyissimoYOLO on state-of-the-art ultra-low-power extreme edge platforms, presenting an in-depth a comparison on latency, energy efficiency, and their ability to efficiently parallelize the workload.","In particular, the paper presents a comparison between a novel parallel RISC-V processor (GAP9 from Greenwaves) with and without use of its on-chip hardware accelerator, an ARM Cortex-M7 core (STM32H7 from ST Microelectronics), two ARM Cortex-M4 cores (STM32L4 from STM and Apollo4b from Ambiq), and a multi-core platform with a CNN hardware accelerator (Analog Devices MAX78000).","Experimental results show that the GAP9's hardware accelerator achieves the lowest inference latency and energy at 2.12ms and 150uJ respectively, which is around 2x faster and 20% more efficient than the next best platform, the MAX78000.","The hardware accelerator of GAP9 can even run an increased resolution version of TinyissimoYOLO with 112x112 pixels and 10 detection classes within 3.2ms, consuming 245uJ. To showcase the competitiveness of a versatile general-purpose system we also deployed and profiled a multi-core implementation on GAP9 at different operating points, achieving 11.3ms with the lowest-latency and 490uJ with the most energy-efficient configuration.","With this paper, we demonstrate the suitability and flexibility of TinyissimoYOLO on state-of-the-art detection datasets for real-time ultra-low-power edge inference."],"url":"http://arxiv.org/abs/2307.05999v1"}
{"created":"2023-07-12 08:14:23","title":"Robbed withdrawal","abstract":"In this article we show that Theorem 2 in Lie et al. (2023) is incorrect. Since Wombat Exchange, a decentralized exchange, is built upon Lie et al. (2023) and Theorem 2 is fundamental to Wombat Finance, we show that an undesirable phenomenon, which we call the robbed withdrawal, can happen as a consequence.","sentences":["In this article we show that Theorem 2 in Lie et al.","(2023) is incorrect.","Since Wombat Exchange, a decentralized exchange, is built upon Lie et al. (2023) and Theorem 2 is fundamental to Wombat Finance, we show that an undesirable phenomenon, which we call the robbed withdrawal, can happen as a consequence."],"url":"http://arxiv.org/abs/2307.05992v1"}
{"created":"2023-07-12 08:04:29","title":"A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition","abstract":"Human Activity Recognition (HAR) has become one of the leading research topics of the last decade. As sensing technologies have matured and their economic costs have declined, a host of novel applications, e.g., in healthcare, industry, sports, and daily life activities have become popular. The design of HAR systems requires different time-consuming processing steps, such as data collection, annotation, and model training and optimization. In particular, data annotation represents the most labor-intensive and cumbersome step in HAR, since it requires extensive and detailed manual work from human annotators. Therefore, different methodologies concerning the automation of the annotation procedure in HAR have been proposed. The annotation problem occurs in different notions and scenarios, which all require individual solutions. In this paper, we provide the first systematic review on data annotation techniques for HAR. By grouping existing approaches into classes and providing a taxonomy, our goal is to support the decision on which techniques can be beneficially used in a given scenario.","sentences":["Human Activity Recognition (HAR) has become one of the leading research topics of the last decade.","As sensing technologies have matured and their economic costs have declined, a host of novel applications, e.g., in healthcare, industry, sports, and daily life activities have become popular.","The design of HAR systems requires different time-consuming processing steps, such as data collection, annotation, and model training and optimization.","In particular, data annotation represents the most labor-intensive and cumbersome step in HAR, since it requires extensive and detailed manual work from human annotators.","Therefore, different methodologies concerning the automation of the annotation procedure in HAR have been proposed.","The annotation problem occurs in different notions and scenarios, which all require individual solutions.","In this paper, we provide the first systematic review on data annotation techniques for HAR.","By grouping existing approaches into classes and providing a taxonomy, our goal is to support the decision on which techniques can be beneficially used in a given scenario."],"url":"http://arxiv.org/abs/2307.05988v1"}
{"created":"2023-07-12 08:01:48","title":"Beyond Hiding and Revealing: Exploring Effects of Visibility and Form of Interaction on the Witness Experience","abstract":"Our interactions with technology do not just shape our individual experiences. They also affect people around us. Although previous research has addressed such \"witness\" experiences, the actual effect of interaction design on the witness experience remains largely unknown. In an online study (n = 407), we explored how witnesses perceive mid-air gesture-based interactions with a hearing aid, using four video vignettes. We studied witnesses' subjective visibility of manipulations and effects (following Reeves and colleagues' taxonomy), perceived form of interaction, subjective experience, and relationships between these measures. Although visibility patterns matched the intended form, they did not lead to the supposed experience (i.e., \"suspenseful\" gestures did not lead to suspenseful experiences). The paper illustrates gaps in current research about witness experiences, demonstrates the need to overcome basic hiding/revealing profiles, and indicates a path forward by focusing on aesthetic forms and experiences.","sentences":["Our interactions with technology do not just shape our individual experiences.","They also affect people around us.","Although previous research has addressed such \"witness\" experiences, the actual effect of interaction design on the witness experience remains largely unknown.","In an online study (n = 407), we explored how witnesses perceive mid-air gesture-based interactions with a hearing aid, using four video vignettes.","We studied witnesses' subjective visibility of manipulations and effects (following Reeves and colleagues' taxonomy), perceived form of interaction, subjective experience, and relationships between these measures.","Although visibility patterns matched the intended form, they did not lead to the supposed experience (i.e., \"suspenseful\" gestures did not lead to suspenseful experiences).","The paper illustrates gaps in current research about witness experiences, demonstrates the need to overcome basic hiding/revealing profiles, and indicates a path forward by focusing on aesthetic forms and experiences."],"url":"http://arxiv.org/abs/2307.05986v1"}
{"created":"2023-07-12 07:51:12","title":"Transformers in Reinforcement Learning: A Survey","abstract":"Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers in RL, using visualization techniques and efficient training strategies. Often, the transformer architecture must be tailored to the specific needs of a given application. We present a broad overview of how transformers have been adapted for several applications, including robotics, medicine, language modeling, cloud computing, and combinatorial optimization. We conclude by discussing the limitations of using transformers in RL and assess their potential for catalyzing future breakthroughs in this field.","sentences":["Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks.","This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability.","We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms.","Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL.","We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization.","We also discuss recent research that aims to enhance the interpretability and efficiency of transformers in RL, using visualization techniques and efficient training strategies.","Often, the transformer architecture must be tailored to the specific needs of a given application.","We present a broad overview of how transformers have been adapted for several applications, including robotics, medicine, language modeling, cloud computing, and combinatorial optimization.","We conclude by discussing the limitations of using transformers in RL and assess their potential for catalyzing future breakthroughs in this field."],"url":"http://arxiv.org/abs/2307.05979v1"}
{"created":"2023-07-12 07:48:29","title":"Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models","abstract":"Large-scale image generation models, with impressive quality made possible by the vast amount of data available on the Internet, raise social concerns that these models may generate harmful or copyrighted content. The biases and harmfulness arise throughout the entire training process and are hard to completely remove, which have become significant hurdles to the safe deployment of these models. In this paper, we propose a method called SDD to prevent problematic content generation in text-to-image diffusion models. We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one. Compared to the previous methods, our method eliminates a much greater proportion of harmful content from the generated images without degrading the overall image quality. Furthermore, our method allows the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time.","sentences":["Large-scale image generation models, with impressive quality made possible by the vast amount of data available on the Internet, raise social concerns that these models may generate harmful or copyrighted content.","The biases and harmfulness arise throughout the entire training process and are hard to completely remove, which have become significant hurdles to the safe deployment of these models.","In this paper, we propose a method called SDD to prevent problematic content generation in text-to-image diffusion models.","We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one.","Compared to the previous methods, our method eliminates a much greater proportion of harmful content from the generated images without degrading the overall image quality.","Furthermore, our method allows the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time."],"url":"http://arxiv.org/abs/2307.05977v1"}
{"created":"2023-07-12 07:42:52","title":"Contrastive Learning for Conversion Rate Prediction","abstract":"Conversion rate (CVR) prediction plays an important role in advertising systems. Recently, supervised deep neural network-based models have shown promising performance in CVR prediction. However, they are data hungry and require an enormous amount of training data. In online advertising systems, although there are millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set. This data sparsity issue restricts the power of these deep models. In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the supervised CVR prediction task with a contrastive learning task, which can learn better data representations exploiting abundant unlabeled data and improve the CVR prediction performance. To tailor the contrastive learning task to the CVR prediction problem, we propose embedding masking (EM), rather than feature masking, to create two views of augmented samples. We also propose a false negative elimination (FNE) component to eliminate samples with the same feature as the anchor sample, to account for the natural property in user behavior data. We further propose a supervised positive inclusion (SPI) component to include additional positive samples for each anchor sample, in order to make full use of sparse but precious user conversion events. Experimental results on two real-world conversion datasets demonstrate the superior performance of CL4CVR. The source code is available at https://github.com/DongRuiHust/CL4CVR.","sentences":["Conversion rate (CVR) prediction plays an important role in advertising systems.","Recently, supervised deep neural network-based models have shown promising performance in CVR prediction.","However, they are data hungry and require an enormous amount of training data.","In online advertising systems, although there are millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set.","This data sparsity issue restricts the power of these deep models.","In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework.","It associates the supervised CVR prediction task with a contrastive learning task, which can learn better data representations exploiting abundant unlabeled data and improve the CVR prediction performance.","To tailor the contrastive learning task to the CVR prediction problem, we propose embedding masking (EM), rather than feature masking, to create two views of augmented samples.","We also propose a false negative elimination (FNE) component to eliminate samples with the same feature as the anchor sample, to account for the natural property in user behavior data.","We further propose a supervised positive inclusion (SPI) component to include additional positive samples for each anchor sample, in order to make full use of sparse but precious user conversion events.","Experimental results on two real-world conversion datasets demonstrate the superior performance of CL4CVR.","The source code is available at https://github.com/DongRuiHust/CL4CVR."],"url":"http://arxiv.org/abs/2307.05974v1"}
{"created":"2023-07-12 07:40:48","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models","abstract":"Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Project website: https://voxposer.github.io","sentences":["Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning.","Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck.","In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects.","We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction.","More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent.","The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations.","We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions.","We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language.","Project website: https://voxposer.github.io"],"url":"http://arxiv.org/abs/2307.05973v1"}
{"created":"2023-07-12 07:38:24","title":"Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models","abstract":"We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark. Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on.","sentences":["We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models.","We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines.","We apply SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark.","Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on."],"url":"http://arxiv.org/abs/2307.05972v1"}
{"created":"2023-07-12 07:12:20","title":"GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation","abstract":"Language-Guided Robotic Manipulation (LGRM) is a challenging task as it requires a robot to understand human instructions to manipulate everyday objects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG) models to detect objects without adapting to manipulation environments. This results in a performance drop due to a substantial domain gap between the pre-training and real-world data. A straightforward solution is to collect additional training data, but the cost of human-annotation is extortionate. In this paper, we propose Grounding Vision to Ceaselessly Created Instructions (GVCCI), a lifelong learning framework for LGRM, which continuously learns VG without human supervision. GVCCI iteratively generates synthetic instruction via object detection and trains the VG model with the generated data. We validate our framework in offline and online settings across diverse environments on different VG models. Experimental results show that accumulating synthetic data from GVCCI leads to a steady improvement in VG by up to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the qualitative analysis shows that the unadapted VG model often fails to find correct objects due to a strong bias learned from the pre-training data. Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k triplets of image-object-instruction from diverse manipulation environments.","sentences":["Language-Guided Robotic Manipulation (LGRM) is a challenging task as it requires a robot to understand human instructions to manipulate everyday objects.","Recent approaches in LGRM rely on pre-trained Visual Grounding (VG) models to detect objects without adapting to manipulation environments.","This results in a performance drop due to a substantial domain gap between the pre-training and real-world data.","A straightforward solution is to collect additional training data, but the cost of human-annotation is extortionate.","In this paper, we propose Grounding Vision to Ceaselessly Created Instructions (GVCCI), a lifelong learning framework for LGRM, which continuously learns VG without human supervision.","GVCCI iteratively generates synthetic instruction via object detection and trains the VG model with the generated data.","We validate our framework in offline and online settings across diverse environments on different VG models.","Experimental results show that accumulating synthetic data from GVCCI leads to a steady improvement in VG by up to 56.7% and improves resultant LGRM by up to 29.4%.","Furthermore, the qualitative analysis shows that the unadapted VG model often fails to find correct objects due to a strong bias learned from the pre-training data.","Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k triplets of image-object-instruction from diverse manipulation environments."],"url":"http://arxiv.org/abs/2307.05963v1"}
{"created":"2023-07-12 07:06:30","title":"FGo: A Directed Grey-box Fuzzer with Probabilistic Exponential cut-the-loss Strategies","abstract":"Traditional coverage grey-box fuzzers perform a breadth-first search of the state space of Program Under Test (PUT). This aimlessness wastes a lot of computing resources. Directed grey-box fuzzing focuses on the target of PUT and becomes one of the most popular topics of software testing. The early termination of unreachable test cases is a method to improve directed grey-box fuzzing. However, existing solutions have two problems: firstly, reachability analysis needs to introduce extra technologies (e.g., static analysis); secondly, the performance of reachability analysis and auxiliary technologies lack versatility.   We propose FGo, a probabilistic exponential cut-the-loss directed grey-box fuzzer. FGo terminates unreachable test cases early with exponentially increasing probability. Compared to other technologies, FGo makes full use of the unreachable information contained in iCFG and doesn't generate any additional overhead caused by reachability analysis. Moreover, it is easy to generalize to all PUT. This strategy based on probability is perfectly adapted to the randomness of fuzzing.   The experiment results show that FGo is 106% faster than AFLGo in reproducing crashes. We compare multiple parameters of probabilistic exponential cut-the-loss algorithm and analyze them in detail. In addition, for enhancing the inerpretability of FGo, this paper discusses the difference between the theoretical performance and the practical performance of probabilistic exponential cut-the-loss algorithm.","sentences":["Traditional coverage grey-box fuzzers perform a breadth-first search of the state space of Program Under Test (PUT).","This aimlessness wastes a lot of computing resources.","Directed grey-box fuzzing focuses on the target of PUT and becomes one of the most popular topics of software testing.","The early termination of unreachable test cases is a method to improve directed grey-box fuzzing.","However, existing solutions have two problems: firstly, reachability analysis needs to introduce extra technologies (e.g., static analysis); secondly, the performance of reachability analysis and auxiliary technologies lack versatility.   ","We propose FGo, a probabilistic exponential cut-the-loss directed grey-box fuzzer.","FGo terminates unreachable test cases early with exponentially increasing probability.","Compared to other technologies, FGo makes full use of the unreachable information contained in iCFG and doesn't generate any additional overhead caused by reachability analysis.","Moreover, it is easy to generalize to all PUT.","This strategy based on probability is perfectly adapted to the randomness of fuzzing.   ","The experiment results show that FGo is 106% faster than AFLGo in reproducing crashes.","We compare multiple parameters of probabilistic exponential cut-the-loss algorithm and analyze them in detail.","In addition, for enhancing the inerpretability of FGo, this paper discusses the difference between the theoretical performance and the practical performance of probabilistic exponential cut-the-loss algorithm."],"url":"http://arxiv.org/abs/2307.05961v1"}
{"created":"2023-07-12 07:05:58","title":"An adaptive approach to remove tensile instability in SPH for weakly compressible fluids","abstract":"Smoothed Particle Hydrodynamics (SPH) is plagued by the phenomenon of tensile instability, which is the occurrence of short wavelength zero energy modes resulting in unphysical clustering of particles. The root cause of the instability is the shape of derivative of the compactly supported kernel function which may yield negative stiffness in the particle interaction under certain circumstances. In this work, an adaptive algorithm is developed to remove tensile instability in SPH for weakly compressible fluids. Herein, a B-spline function is used as the SPH kernel and the knots of the B-spline are adapted to change the shape of the kernel, thereby satisfying the condition associated with stability. The knot-shifting criterion is based on the particle movement within the influence domain. This enables the prevention of instability in fluid problems where excessive rearrangement of particle positions occurs. A 1D dispersion analysis of an Oldroyd B fluid material model is performed to show how the algorithm prevents instabilities for short wavelengths but ensures accuracy at large wavelengths. The efficacy of the approach is demonstrated through a few benchmark fluid dynamics simulations where a visco-elastic Oldroyd B material model and a non-viscous Eulerian fluid material model are considered.","sentences":["Smoothed Particle Hydrodynamics (SPH) is plagued by the phenomenon of tensile instability, which is the occurrence of short wavelength zero energy modes resulting in unphysical clustering of particles.","The root cause of the instability is the shape of derivative of the compactly supported kernel function which may yield negative stiffness in the particle interaction under certain circumstances.","In this work, an adaptive algorithm is developed to remove tensile instability in SPH for weakly compressible fluids.","Herein, a B-spline function is used as the SPH kernel and the knots of the B-spline are adapted to change the shape of the kernel, thereby satisfying the condition associated with stability.","The knot-shifting criterion is based on the particle movement within the influence domain.","This enables the prevention of instability in fluid problems where excessive rearrangement of particle positions occurs.","A 1D dispersion analysis of an Oldroyd B fluid material model is performed to show how the algorithm prevents instabilities for short wavelengths but ensures accuracy at large wavelengths.","The efficacy of the approach is demonstrated through a few benchmark fluid dynamics simulations where a visco-elastic Oldroyd B material model and a non-viscous Eulerian fluid material model are considered."],"url":"http://arxiv.org/abs/2307.05960v1"}
{"created":"2023-07-12 07:04:53","title":"Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations","abstract":"Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of eye-in-hand cameras as well as a simple fixed image masking scheme. On a suite of eight real-world tasks involving both 3-DoF and 6-DoF robot arm control, our method improves the success rates of eye-in-hand manipulation policies by 58% (absolute) on average, enabling robots to generalize to both new environment configurations and new tasks that are unseen in the robot demonstration data. See video results at https://giving-robots-a-hand.github.io/ .","sentences":["Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation.","However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot.","Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios.","Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale.","In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies.","Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of eye-in-hand cameras as well as a simple fixed image masking scheme.","On a suite of eight real-world tasks involving both 3-DoF and 6-DoF robot arm control, our method improves the success rates of eye-in-hand manipulation policies by 58% (absolute) on average, enabling robots to generalize to both new environment configurations and new tasks that are unseen in the robot demonstration data.","See video results at https://giving-robots-a-hand.github.io/ ."],"url":"http://arxiv.org/abs/2307.05959v1"}
{"created":"2023-07-12 07:00:12","title":"Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition","abstract":"Multilingual speech recognition for both monolingual and code-switching speech is a challenging task. Recently, based on the Mixture of Experts (MoE), many works have made good progress in multilingual and code-switching ASR, but present huge computational complexity with the increase of supported languages. In this work, we propose a computation-efficient network named Language-Routing Mixture of Experts (LR-MoE) for multilingual and code-switching ASR. LR-MoE extracts language-specific representations through the Mixture of Language Experts (MLE), which is guided to learn by a frame-wise language routing mechanism. The weight-shared frame-level language identification (LID) network is jointly trained as the shared pre-router of each MoE layer. Experiments show that the proposed method significantly improves multilingual and code-switching speech recognition performances over baseline with comparable computational efficiency.","sentences":["Multilingual speech recognition for both monolingual and code-switching speech is a challenging task.","Recently, based on the Mixture of Experts (MoE), many works have made good progress in multilingual and code-switching ASR, but present huge computational complexity with the increase of supported languages.","In this work, we propose a computation-efficient network named Language-Routing Mixture of Experts (LR-MoE) for multilingual and code-switching ASR.","LR-MoE extracts language-specific representations through the Mixture of Language Experts (MLE), which is guided to learn by a frame-wise language routing mechanism.","The weight-shared frame-level language identification (LID) network is jointly trained as the shared pre-router of each MoE layer.","Experiments show that the proposed method significantly improves multilingual and code-switching speech recognition performances over baseline with comparable computational efficiency."],"url":"http://arxiv.org/abs/2307.05956v1"}
{"created":"2023-07-12 06:50:07","title":"Reward Selection with Noisy Observations","abstract":"We study a fundamental problem in optimization under uncertainty. There are $n$ boxes; each box $i$ contains a hidden reward $x_i$. Rewards are drawn i.i.d. from an unknown distribution $\\mathcal{D}$. For each box $i$, we see $y_i$, an unbiased estimate of its reward, which is drawn from a Normal distribution with known standard deviation $\\sigma_i$ (and an unknown mean $x_i$). Our task is to select a single box, with the goal of maximizing our reward. This problem captures a wide range of applications, e.g. ad auctions, where the hidden reward is the click-through rate of an ad. Previous work in this model [BKMR12] proves that the naive policy, which selects the box with the largest estimate $y_i$, is suboptimal, and suggests a linear policy, which selects the box $i$ with the largest $y_i - c \\cdot \\sigma_i$, for some $c > 0$. However, no formal guarantees are given about the performance of either policy (e.g., whether their expected reward is within some factor of the optimal policy's reward).   In this work, we prove that both the naive policy and the linear policy are arbitrarily bad compared to the optimal policy, even when $\\mathcal{D}$ is well-behaved, e.g. has monotone hazard rate (MHR), and even under a \"small tail\" condition, which requires that not too many boxes have arbitrarily large noise. On the flip side, we propose a simple threshold policy that gives a constant approximation to the reward of a prophet (who knows the realized values $x_1, \\dots, x_n$) under the same \"small tail\" condition. We prove that when this condition is not satisfied, even an optimal clairvoyant policy (that knows $\\mathcal{D}$) cannot get a constant approximation to the prophet, even for MHR distributions, implying that our threshold policy is optimal against the prophet benchmark, up to constants.","sentences":["We study a fundamental problem in optimization under uncertainty.","There are $n$ boxes; each box $i$ contains a hidden reward $x_i$.","Rewards are drawn i.i.d.","from an unknown distribution $\\mathcal{D}$. For each box $i$, we see $y_i$, an unbiased estimate of its reward, which is drawn from a Normal distribution with known standard deviation $\\sigma_i$ (and an unknown mean $x_i$).","Our task is to select a single box, with the goal of maximizing our reward.","This problem captures a wide range of applications, e.g. ad auctions, where the hidden reward is the click-through rate of an ad.","Previous work in this model [BKMR12] proves that the naive policy, which selects the box with the largest estimate $y_i$, is suboptimal, and suggests a linear policy, which selects the box $i$ with the largest $y_i - c \\cdot \\sigma_i$, for some $c > 0$.","However, no formal guarantees are given about the performance of either policy (e.g., whether their expected reward is within some factor of the optimal policy's reward).   ","In this work, we prove that both the naive policy and the linear policy are arbitrarily bad compared to the optimal policy, even when $\\mathcal{D}$ is well-behaved, e.g. has monotone hazard rate (MHR), and even under a \"small tail\" condition, which requires that not too many boxes have arbitrarily large noise.","On the flip side, we propose a simple threshold policy that gives a constant approximation to the reward of a prophet (who knows the realized values $x_1, \\dots, x_n$) under the same \"small tail\" condition.","We prove that when this condition is not satisfied, even an optimal clairvoyant policy (that knows $\\mathcal{D}$) cannot get a constant approximation to the prophet, even for MHR distributions, implying that our threshold policy is optimal against the prophet benchmark, up to constants."],"url":"http://arxiv.org/abs/2307.05953v1"}
{"created":"2023-07-12 06:32:51","title":"Exploring the Effectiveness of LLMs in Automated Logging Generation: An Empirical Study","abstract":"Automated logging statement generation techniques facilitate developers in writing appropriate logging statements that document software behaviors. Current retrieval-based and learning-based logging methods fail to provide accurate logging statements in complex software. Although existing large language models (LLMs) might be a good fit for the task due to their great success in natural language generation and programming language comprehension, their effectiveness and generalization capabilities have not been explored. To this end, this paper performs the first extensive study on applying LLMs for logging statement generation. We build LogBench, the first logging statement generation dataset. On LogBench, we evaluate the effectiveness and generalization capabilities of eight state-of-the-art LLMs, which include general-purpose and code-specific models ranging from 60M to 175B in size. Specifically, we evaluate LLM's logging effectiveness by studying 1) their ability to decide logging ingredients, 2) the impact of the internal characteristics of LLMs, and 3) the influence of external factors. We further evaluate LLM's logging generalization capabilities using unseen data derived from code transformation techniques. Our study demonstrates that existing LLMs fall short of practical requirements for generating proper logging statement texts. We also disclose the impact of internal characteristics and external factors for LLMs in automated logging. In addition, we observe that existing LLMs cannot generalize to logging unseen code, revealing their unsatisfactory generalization capabilities. Based on our findings, we further discuss three implications that can enhance logging statement generation in the future, such as developing a unified metric for logging quality, incorporating shareable code knowledge into LLMs, and devising suitable prompts.","sentences":["Automated logging statement generation techniques facilitate developers in writing appropriate logging statements that document software behaviors.","Current retrieval-based and learning-based logging methods fail to provide accurate logging statements in complex software.","Although existing large language models (LLMs) might be a good fit for the task due to their great success in natural language generation and programming language comprehension, their effectiveness and generalization capabilities have not been explored.","To this end, this paper performs the first extensive study on applying LLMs for logging statement generation.","We build LogBench, the first logging statement generation dataset.","On LogBench, we evaluate the effectiveness and generalization capabilities of eight state-of-the-art LLMs, which include general-purpose and code-specific models ranging from 60M to 175B in size.","Specifically, we evaluate LLM's logging effectiveness by studying 1) their ability to decide logging ingredients, 2) the impact of the internal characteristics of LLMs, and 3) the influence of external factors.","We further evaluate LLM's logging generalization capabilities using unseen data derived from code transformation techniques.","Our study demonstrates that existing LLMs fall short of practical requirements for generating proper logging statement texts.","We also disclose the impact of internal characteristics and external factors for LLMs in automated logging.","In addition, we observe that existing LLMs cannot generalize to logging unseen code, revealing their unsatisfactory generalization capabilities.","Based on our findings, we further discuss three implications that can enhance logging statement generation in the future, such as developing a unified metric for logging quality, incorporating shareable code knowledge into LLMs, and devising suitable prompts."],"url":"http://arxiv.org/abs/2307.05950v1"}
{"created":"2023-07-12 06:31:43","title":"Newell's theory based feature transformations for spatio-temporal traffic prediction","abstract":"Deep learning (DL) models for spatio-temporal traffic flow forecasting employ convolutional or graph-convolutional filters along with recurrent neural networks to capture spatial and temporal dependencies in traffic data. These models, such as CNN-LSTM, utilize traffic flows from neighboring detector stations to predict flows at a specific location of interest. However, these models are limited in their ability to capture the broader dynamics of the traffic system, as they primarily learn features specific to the detector configuration and traffic characteristics at the target location. Hence, the transferability of these models to different locations becomes challenging, particularly when data is unavailable at the new location for model training. To address this limitation, we propose a traffic flow physics-based feature transformation for spatio-temporal DL models. This transformation incorporates Newell's uncongested and congested-state estimators of traffic flows at the target locations, enabling the models to learn broader dynamics of the system. Our methodology is empirically validated using traffic data from two different locations. The results demonstrate that the proposed feature transformation improves the models' performance in predicting traffic flows over different prediction horizons, as indicated by better goodness-of-fit statistics. An important advantage of our framework is its ability to be transferred to new locations where data is unavailable. This is achieved by appropriately accounting for spatial dependencies based on station distances and various traffic parameters. In contrast, regular DL models are not easily transferable as their inputs remain fixed. It should be noted that due to data limitations, we were unable to perform spatial sensitivity analysis, which calls for further research using simulated data.","sentences":["Deep learning (DL) models for spatio-temporal traffic flow forecasting employ convolutional or graph-convolutional filters along with recurrent neural networks to capture spatial and temporal dependencies in traffic data.","These models, such as CNN-LSTM, utilize traffic flows from neighboring detector stations to predict flows at a specific location of interest.","However, these models are limited in their ability to capture the broader dynamics of the traffic system, as they primarily learn features specific to the detector configuration and traffic characteristics at the target location.","Hence, the transferability of these models to different locations becomes challenging, particularly when data is unavailable at the new location for model training.","To address this limitation, we propose a traffic flow physics-based feature transformation for spatio-temporal DL models.","This transformation incorporates Newell's uncongested and congested-state estimators of traffic flows at the target locations, enabling the models to learn broader dynamics of the system.","Our methodology is empirically validated using traffic data from two different locations.","The results demonstrate that the proposed feature transformation improves the models' performance in predicting traffic flows over different prediction horizons, as indicated by better goodness-of-fit statistics.","An important advantage of our framework is its ability to be transferred to new locations where data is unavailable.","This is achieved by appropriately accounting for spatial dependencies based on station distances and various traffic parameters.","In contrast, regular DL models are not easily transferable as their inputs remain fixed.","It should be noted that due to data limitations, we were unable to perform spatial sensitivity analysis, which calls for further research using simulated data."],"url":"http://arxiv.org/abs/2307.05949v1"}
{"created":"2023-07-12 06:29:02","title":"Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation","abstract":"Generating unlabeled data has been recently shown to help address the few-shot hypothesis adaptation (FHA) problem, where we aim to train a classifier for the target domain with a few labeled target-domain data and a well-trained source-domain classifier (i.e., a source hypothesis), for the additional information of the highly-compatible unlabeled data. However, the generated data of the existing methods are extremely similar or even the same. The strong dependency among the generated data will lead the learning to fail. In this paper, we propose a diversity-enhancing generative network (DEG-Net) for the FHA problem, which can generate diverse unlabeled data with the help of a kernel independence measure: the Hilbert-Schmidt independence criterion (HSIC). Specifically, DEG-Net will generate data via minimizing the HSIC value (i.e., maximizing the independence) among the semantic features of the generated data. By DEG-Net, the generated unlabeled data are more diverse and more effective for addressing the FHA problem. Experimental results show that the DEG-Net outperforms existing FHA baselines and further verifies that generating diverse data plays a vital role in addressing the FHA problem","sentences":["Generating unlabeled data has been recently shown to help address the few-shot hypothesis adaptation (FHA) problem, where we aim to train a classifier for the target domain with a few labeled target-domain data and a well-trained source-domain classifier (i.e., a source hypothesis), for the additional information of the highly-compatible unlabeled data.","However, the generated data of the existing methods are extremely similar or even the same.","The strong dependency among the generated data will lead the learning to fail.","In this paper, we propose a diversity-enhancing generative network (DEG-Net) for the FHA problem, which can generate diverse unlabeled data with the help of a kernel independence measure: the Hilbert-Schmidt independence criterion (HSIC).","Specifically, DEG-Net will generate data via minimizing the HSIC value (i.e., maximizing the independence) among the semantic features of the generated data.","By DEG-Net, the generated unlabeled data are more diverse and more effective for addressing the FHA problem.","Experimental results show that the DEG-Net outperforms existing FHA baselines and further verifies that generating diverse data plays a vital role in addressing the FHA problem"],"url":"http://arxiv.org/abs/2307.05948v1"}
{"created":"2023-07-12 06:23:31","title":"A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models","abstract":"Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization performance of the model on out-of-distribution datasets. Results demonstrate that spectral normalization improves uncertainty estimates and significantly outperforms both the layer normalization and model without normalization in single-step prediction horizons. This improved performance can be attributed to the ability of spectral normalization to better localize the feature space of the data under perturbations. Our findings are especially relevant to traffic management applications, where predicting traffic conditions across multiple locations is the goal, but the availability of training data from multiple locations is limited. Spectral normalization, therefore, provides a more generalizable approach that can effectively capture the underlying patterns in traffic data without requiring location-specific models.","sentences":["Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture.","However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control.","Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions.","In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers.","In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data.","This, in turn, helps improve the generalization performance of the model on out-of-distribution datasets.","Results demonstrate that spectral normalization improves uncertainty estimates and significantly outperforms both the layer normalization and model without normalization in single-step prediction horizons.","This improved performance can be attributed to the ability of spectral normalization to better localize the feature space of the data under perturbations.","Our findings are especially relevant to traffic management applications, where predicting traffic conditions across multiple locations is the goal, but the availability of training data from multiple locations is limited.","Spectral normalization, therefore, provides a more generalizable approach that can effectively capture the underlying patterns in traffic data without requiring location-specific models."],"url":"http://arxiv.org/abs/2307.05946v1"}
{"created":"2023-07-12 06:22:51","title":"YOGA: Deep Object Detection in the Wild with Lightweight Feature Learning and Multiscale Attention","abstract":"We introduce YOGA, a deep learning based yet lightweight object detection model that can operate on low-end edge devices while still achieving competitive accuracy. The YOGA architecture consists of a two-phase feature learning pipeline with a cheap linear transformation, which learns feature maps using only half of the convolution filters required by conventional convolutional neural networks. In addition, it performs multi-scale feature fusion in its neck using an attention mechanism instead of the naive concatenation used by conventional detectors. YOGA is a flexible model that can be easily scaled up or down by several orders of magnitude to fit a broad range of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets with other over 10 state-of-the-art object detectors. The results show that YOGA strikes the best trade-off between model size and accuracy (up to 22% increase of AP and 23-34% reduction of parameters and FLOPs), making it an ideal choice for deployment in the wild on low-end edge devices. This is further affirmed by our hardware implementation and evaluation on NVIDIA Jetson Nano.","sentences":["We introduce YOGA, a deep learning based yet lightweight object detection model that can operate on low-end edge devices while still achieving competitive accuracy.","The YOGA architecture consists of a two-phase feature learning pipeline with a cheap linear transformation, which learns feature maps using only half of the convolution filters required by conventional convolutional neural networks.","In addition, it performs multi-scale feature fusion in its neck using an attention mechanism instead of the naive concatenation used by conventional detectors.","YOGA is a flexible model that can be easily scaled up or down by several orders of magnitude to fit a broad range of hardware constraints.","We evaluate YOGA on COCO-val and COCO-testdev datasets with other over 10 state-of-the-art object detectors.","The results show that YOGA strikes the best trade-off between model size and accuracy (up to 22% increase of AP and 23-34% reduction of parameters and FLOPs), making it an ideal choice for deployment in the wild on low-end edge devices.","This is further affirmed by our hardware implementation and evaluation on NVIDIA Jetson Nano."],"url":"http://arxiv.org/abs/2307.05945v1"}
{"created":"2023-07-12 06:20:19","title":"A 137.5 TOPS/W SRAM Compute-in-Memory Macro with 9-b Memory Cell-Embedded ADCs and Signal Margin Enhancement Techniques for AI Edge Applications","abstract":"In this paper, we propose a high-precision SRAM-based CIM macro that can perform 4x4-bit MAC operations and yield 9-bit signed output. The inherent discharge branches of SRAM cells are utilized to apply time-modulated MAC and 9-bit ADC readout operations on two bit-line capacitors. The same principle is used for both MAC and A-to-D conversion ensuring high linearity and thus supporting large number of analog MAC accumulations. The memory cell-embedded ADC eliminates the use of separate ADCs and enhances energy and area efficiency. Additionally, two signal margin enhancement techniques, namely the MAC-folding and boosted-clipping schemes, are proposed to further improve the CIM computation accuracy.","sentences":["In this paper, we propose a high-precision SRAM-based CIM macro that can perform 4x4-bit MAC operations and yield 9-bit signed output.","The inherent discharge branches of SRAM cells are utilized to apply time-modulated MAC and 9-bit ADC readout operations on two bit-line capacitors.","The same principle is used for both MAC and A-to-D conversion ensuring high linearity and thus supporting large number of analog MAC accumulations.","The memory cell-embedded ADC eliminates the use of separate ADCs and enhances energy and area efficiency.","Additionally, two signal margin enhancement techniques, namely the MAC-folding and boosted-clipping schemes, are proposed to further improve the CIM computation accuracy."],"url":"http://arxiv.org/abs/2307.05944v1"}
{"created":"2023-07-12 06:14:36","title":"Prototypical Contrastive Transfer Learning for Multimodal Language Understanding","abstract":"Although domestic service robots are expected to assist individuals who require support, they cannot currently interact smoothly with people through natural language. For example, given the instruction \"Bring me a bottle from the kitchen,\" it is difficult for such robots to specify the bottle in an indoor environment. Most conventional models have been trained on real-world datasets that are labor-intensive to collect, and they have not fully leveraged simulation data through a transfer learning framework. In this study, we propose a novel transfer learning approach for multimodal language understanding called Prototypical Contrastive Transfer Learning (PCTL), which uses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task of identifying target objects in domestic environments according to free-form natural language instructions. To validate PCTL, we built new real-world and simulation datasets. Our experiment demonstrated that PCTL outperformed existing methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas simple fine-tuning achieved an accuracy of 73.4%.","sentences":["Although domestic service robots are expected to assist individuals who require support, they cannot currently interact smoothly with people through natural language.","For example, given the instruction \"Bring me a bottle from the kitchen,\" it is difficult for such robots to specify the bottle in an indoor environment.","Most conventional models have been trained on real-world datasets that are labor-intensive to collect, and they have not fully leveraged simulation data through a transfer learning framework.","In this study, we propose a novel transfer learning approach for multimodal language understanding called Prototypical Contrastive Transfer Learning (PCTL), which uses a new contrastive loss called Dual ProtoNCE.","We introduce PCTL to the task of identifying target objects in domestic environments according to free-form natural language instructions.","To validate PCTL, we built new real-world and simulation datasets.","Our experiment demonstrated that PCTL outperformed existing methods.","Specifically, PCTL achieved an accuracy of 78.1%, whereas simple fine-tuning achieved an accuracy of 73.4%."],"url":"http://arxiv.org/abs/2307.05942v1"}
{"created":"2023-07-12 06:07:53","title":"Automatically Reconciling the Trade-off between Prediction Accuracy and Earliness in Prescriptive Business Process Monitoring","abstract":"Prescriptive business process monitoring provides decision support to process managers on when and how to adapt an ongoing business process to prevent or mitigate an undesired process outcome. We focus on the problem of automatically reconciling the trade-off between prediction accuracy and prediction earliness in determining when to adapt. Adaptations should happen sufficiently early to provide enough lead time for the adaptation to become effective. However, earlier predictions are typically less accurate than later predictions. This means that acting on less accurate predictions may lead to unnecessary adaptations or missed adaptations.   Different approaches were presented in the literature to reconcile the trade-off between prediction accuracy and earliness. So far, these approaches were compared with different baselines, and evaluated using different data sets or even confidential data sets. This limits the comparability and replicability of the approaches and makes it difficult to choose a concrete approach in practice.   We perform a comparative evaluation of the main alternative approaches for reconciling the trade-off between prediction accuracy and earliness. Using four public real-world event log data sets and two types of prediction models, we assess and compare the cost savings of these approaches. The experimental results indicate which criteria affect the effectiveness of an approach and help us state initial recommendations for the selection of a concrete approach in practice.","sentences":["Prescriptive business process monitoring provides decision support to process managers on when and how to adapt an ongoing business process to prevent or mitigate an undesired process outcome.","We focus on the problem of automatically reconciling the trade-off between prediction accuracy and prediction earliness in determining when to adapt.","Adaptations should happen sufficiently early to provide enough lead time for the adaptation to become effective.","However, earlier predictions are typically less accurate than later predictions.","This means that acting on less accurate predictions may lead to unnecessary adaptations or missed adaptations.   ","Different approaches were presented in the literature to reconcile the trade-off between prediction accuracy and earliness.","So far, these approaches were compared with different baselines, and evaluated using different data sets or even confidential data sets.","This limits the comparability and replicability of the approaches and makes it difficult to choose a concrete approach in practice.   ","We perform a comparative evaluation of the main alternative approaches for reconciling the trade-off between prediction accuracy and earliness.","Using four public real-world event log data sets and two types of prediction models, we assess and compare the cost savings of these approaches.","The experimental results indicate which criteria affect the effectiveness of an approach and help us state initial recommendations for the selection of a concrete approach in practice."],"url":"http://arxiv.org/abs/2307.05939v1"}
{"created":"2023-07-12 06:05:53","title":"Decalf: A Directed, Effectful Cost-Aware Logical Framework","abstract":"We present ${\\bf decalf}$, a ${\\bf d}$irected, ${\\bf e}$ffectful ${\\bf c}$ost-${\\bf a}$ware ${\\bf l}$ogical ${\\bf f}$ramework for studying quantitative aspects of functional programs with effects. Like ${\\bf calf}$, the language is based on a formal phase distinction between the extension and the intension of a program, its pure behavior as distinct from its cost measured by an effectful step-counting primitive. The type theory ensures that the behavior is unaffected by the cost accounting. Unlike ${\\bf calf}$, the present language takes account of effects, such as probabilistic choice and mutable state; this extension requires a reformulation of ${\\bf calf}$'s approach to cost accounting: rather than rely on a \"separable\" notion of cost, here a cost bound is simply another program. To make this formal, we equip every type with an intrinsic preorder, relaxing the precise cost accounting intrinsic to a program to a looser but nevertheless informative estimate. For example, the cost bound of a probabilistic program is itself a probabilistic program that specifies the distribution of costs. This approach serves as a streamlined alternative to the standard method of isolating a recurrence that bounds the cost in a manner that readily extends to higher-order, effectful programs.   The development proceeds by first introducing the ${\\bf decalf}$ type system, which is based on an intrinsic ordering among terms that restricts in the extensional phase to extensional equality, but in the intensional phase reflects an approximation of the cost of a program of interest. This formulation is then applied to a number of illustrative examples, including pure and effectful sorting algorithms, simple probabilistic programs, and higher-order functions. Finally, we justify ${\\bf decalf}$ via a model in the topos of augmented simplicial sets.","sentences":["We present ${\\bf decalf}$, a ${\\bf d}$irected, ${\\bf e}$ffectful ${\\bf c}$ost-${\\bf a}$ware ${\\bf l}$ogical ${\\bf f}$ramework for studying quantitative aspects of functional programs with effects.","Like ${\\bf calf}$, the language is based on a formal phase distinction between the extension and the intension of a program, its pure behavior as distinct from its cost measured by an effectful step-counting primitive.","The type theory ensures that the behavior is unaffected by the cost accounting.","Unlike ${\\bf calf}$, the present language takes account of effects, such as probabilistic choice and mutable state; this extension requires a reformulation of ${\\bf calf}$'s approach to cost accounting: rather than rely on a \"separable\" notion of cost, here a cost bound is simply another program.","To make this formal, we equip every type with an intrinsic preorder, relaxing the precise cost accounting intrinsic to a program to a looser","but nevertheless informative estimate.","For example, the cost bound of a probabilistic program is itself a probabilistic program that specifies the distribution of costs.","This approach serves as a streamlined alternative to the standard method of isolating a recurrence that bounds the cost in a manner that readily extends to higher-order, effectful programs.   ","The development proceeds by first introducing the ${\\bf decalf}$ type system, which is based on an intrinsic ordering among terms that restricts in the extensional phase to extensional equality, but in the intensional phase reflects an approximation of the cost of a program of interest.","This formulation is then applied to a number of illustrative examples, including pure and effectful sorting algorithms, simple probabilistic programs, and higher-order functions.","Finally, we justify ${\\bf decalf}$ via a model in the topos of augmented simplicial sets."],"url":"http://arxiv.org/abs/2307.05938v1"}
{"created":"2023-07-12 06:04:01","title":"Introducing Packet-Level Analysis in Programmable Data Planes to Advance Network Intrusion Detection","abstract":"Programmable data planes offer precise control over the low-level processing steps applied to network packets, serving as a valuable tool for analysing malicious flows in the field of intrusion detection. Albeit with limitations on physical resources and capabilities, they allow for the efficient extraction of detailed traffic information, which can then be utilised by Machine Learning (ML) algorithms responsible for identifying security threats. In addressing resource constraints, existing solutions in the literature rely on compressing network data through the collection of statistical traffic features in the data plane. While this compression saves memory resources in switches and minimises the burden on the control channel between the data and the control plane, it also results in a loss of information available to the Network Intrusion Detection System (NIDS), limiting access to packet payload, categorical features, and the semantic understanding of network communications, such as the behaviour of packets within traffic flows. This paper proposes P4DDLe, a framework that exploits the flexibility of P4-based programmable data planes for packet-level feature extraction and pre-processing. P4DDLe leverages the programmable data plane to extract raw packet features from the network traffic, categorical features included, and to organise them in a way that the semantics of traffic flows is preserved. To minimise memory and control channel overheads, P4DDLe selectively processes and filters packet-level data, so that all and only the relevant features required by the NIDS are collected. The experimental evaluation with recent Distributed Denial of Service (DDoS) attack data demonstrates that the proposed approach is very efficient in collecting compact and high-quality representations of network flows, ensuring precise detection of DDoS attacks.","sentences":["Programmable data planes offer precise control over the low-level processing steps applied to network packets, serving as a valuable tool for analysing malicious flows in the field of intrusion detection.","Albeit with limitations on physical resources and capabilities, they allow for the efficient extraction of detailed traffic information, which can then be utilised by Machine Learning (ML) algorithms responsible for identifying security threats.","In addressing resource constraints, existing solutions in the literature rely on compressing network data through the collection of statistical traffic features in the data plane.","While this compression saves memory resources in switches and minimises the burden on the control channel between the data and the control plane, it also results in a loss of information available to the Network Intrusion Detection System (NIDS), limiting access to packet payload, categorical features, and the semantic understanding of network communications, such as the behaviour of packets within traffic flows.","This paper proposes P4DDLe, a framework that exploits the flexibility of P4-based programmable data planes for packet-level feature extraction and pre-processing.","P4DDLe leverages the programmable data plane to extract raw packet features from the network traffic, categorical features included, and to organise them in a way that the semantics of traffic flows is preserved.","To minimise memory and control channel overheads, P4DDLe selectively processes and filters packet-level data, so that all and only the relevant features required by the NIDS are collected.","The experimental evaluation with recent Distributed Denial of Service (DDoS) attack data demonstrates that the proposed approach is very efficient in collecting compact and high-quality representations of network flows, ensuring precise detection of DDoS attacks."],"url":"http://arxiv.org/abs/2307.05936v1"}
{"created":"2023-07-12 06:00:14","title":"GRAINS: Proximity Sensing of Objects in Granular Materials","abstract":"Proximity sensing detects an object's presence without contact. However, research has rarely explored proximity sensing in granular materials (GM) due to GM's lack of visual and complex properties. In this paper, we propose a granular-material-embedded autonomous proximity sensing system (GRAINS) based on three granular phenomena (fluidization, jamming, and failure wedge zone). GRAINS can automatically sense buried objects beneath GM in real-time manner (at least ~20 hertz) and perceive them 0.5 ~ 7 centimeters ahead in different granules without the use of vision or touch. We introduce a new spiral trajectory for the probe raking in GM, combining linear and circular motions, inspired by a common granular fluidization technique. Based on the observation of force-raising when granular jamming occurs in the failure wedge zone in front of the probe during its raking, we employ Gaussian process regression to constantly learn and predict the force patterns and detect the force anomaly resulting from granular jamming to identify the proximity sensing of buried objects. Finally, we apply GRAINS to a Bayesian-optimization-algorithm-guided exploration strategy to successfully localize underground objects and outline their distribution using proximity sensing without contact or digging. This work offers a simple yet reliable method with potential for safe operation in building habitation infrastructure on an alien planet without human intervention.","sentences":["Proximity sensing detects an object's presence without contact.","However, research has rarely explored proximity sensing in granular materials (GM) due to GM's lack of visual and complex properties.","In this paper, we propose a granular-material-embedded autonomous proximity sensing system (GRAINS) based on three granular phenomena (fluidization, jamming, and failure wedge zone).","GRAINS can automatically sense buried objects beneath GM in real-time manner (at least ~20 hertz) and perceive them 0.5 ~ 7 centimeters ahead in different granules without the use of vision or touch.","We introduce a new spiral trajectory for the probe raking in GM, combining linear and circular motions, inspired by a common granular fluidization technique.","Based on the observation of force-raising when granular jamming occurs in the failure wedge zone in front of the probe during its raking, we employ Gaussian process regression to constantly learn and predict the force patterns and detect the force anomaly resulting from granular jamming to identify the proximity sensing of buried objects.","Finally, we apply GRAINS to a Bayesian-optimization-algorithm-guided exploration strategy to successfully localize underground objects and outline their distribution using proximity sensing without contact or digging.","This work offers a simple yet reliable method with potential for safe operation in building habitation infrastructure on an alien planet without human intervention."],"url":"http://arxiv.org/abs/2307.05935v1"}
{"created":"2023-07-12 05:59:42","title":"Sem-CS: Semantic CLIPStyler for Text-Based Image Style Transfer","abstract":"CLIPStyler demonstrated image style transfer with realistic textures using only a style text description (instead of requiring a reference style image). However, the ground semantics of objects in the style transfer output is lost due to style spill-over on salient and background objects (content mismatch) or over-stylization. To solve this, we propose Semantic CLIPStyler (Sem-CS), that performs semantic style transfer. Sem-CS first segments the content image into salient and non-salient objects and then transfers artistic style based on a given style text description. The semantic style transfer is achieved using global foreground loss (for salient objects) and global background loss (for non-salient objects). Our empirical results, including DISTS, NIMA and user study scores, show that our proposed framework yields superior qualitative and quantitative performance. Our code is available at github.com/chandagrover/sem-cs.","sentences":["CLIPStyler demonstrated image style transfer with realistic textures using only a style text description (instead of requiring a reference style image).","However, the ground semantics of objects in the style transfer output is lost due to style spill-over on salient and background objects (content mismatch) or over-stylization.","To solve this, we propose Semantic CLIPStyler (Sem-CS), that performs semantic style transfer.","Sem-CS first segments the content image into salient and non-salient objects and then transfers artistic style based on a given style text description.","The semantic style transfer is achieved using global foreground loss (for salient objects) and global background loss (for non-salient objects).","Our empirical results, including DISTS, NIMA and user study scores, show that our proposed framework yields superior qualitative and quantitative performance.","Our code is available at github.com/chandagrover/sem-cs."],"url":"http://arxiv.org/abs/2307.05934v1"}
{"created":"2023-07-12 05:58:59","title":"BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration","abstract":"Human bimanual manipulation can perform more complex tasks than a simple combination of two single arms, which is credited to the spatio-temporal coordination between the arms. However, the description of bimanual coordination is still an open topic in robotics. This makes it difficult to give an explainable coordination paradigm, let alone applied to robotics. In this work, we divide the main bimanual tasks in human daily activities into two types: leader-follower and synergistic coordination. Then we propose a relative parameterization method to learn these types of coordination from human demonstration. It represents coordination as Gaussian mixture models from bimanual demonstration to describe the change in the importance of coordination throughout the motions by probability. The learned coordinated representation can be generalized to new task parameters while ensuring spatio-temporal coordination. We demonstrate the method using synthetic motions and human demonstration data and deploy it to a humanoid robot to perform a generalized bimanual coordination motion. We believe that this easy-to-use bimanual learning from demonstration (LfD) method has the potential to be used as a data augmentation plugin for robot large manipulation model training. The corresponding codes are open-sourced in https://github.com/Skylark0924/Rofunc.","sentences":["Human bimanual manipulation can perform more complex tasks than a simple combination of two single arms, which is credited to the spatio-temporal coordination between the arms.","However, the description of bimanual coordination is still an open topic in robotics.","This makes it difficult to give an explainable coordination paradigm, let alone applied to robotics.","In this work, we divide the main bimanual tasks in human daily activities into two types: leader-follower and synergistic coordination.","Then we propose a relative parameterization method to learn these types of coordination from human demonstration.","It represents coordination as Gaussian mixture models from bimanual demonstration to describe the change in the importance of coordination throughout the motions by probability.","The learned coordinated representation can be generalized to new task parameters while ensuring spatio-temporal coordination.","We demonstrate the method using synthetic motions and human demonstration data and deploy it to a humanoid robot to perform a generalized bimanual coordination motion.","We believe that this easy-to-use bimanual learning from demonstration (LfD) method has the potential to be used as a data augmentation plugin for robot large manipulation model training.","The corresponding codes are open-sourced in https://github.com/Skylark0924/Rofunc."],"url":"http://arxiv.org/abs/2307.05933v1"}
{"created":"2023-07-12 05:49:21","title":"A New Dataset and Comparative Study for Aphid Cluster Detection","abstract":"Aphids are one of the main threats to crops, rural families, and global food security. Chemical pest control is a necessary component of crop production for maximizing yields, however, it is unnecessary to apply the chemical approaches to the entire fields in consideration of the environmental pollution and the cost. Thus, accurately localizing the aphid and estimating the infestation level is crucial to the precise local application of pesticides. Aphid detection is very challenging as each individual aphid is really small and all aphids are crowded together as clusters. In this paper, we propose to estimate the infection level by detecting aphid clusters. We have taken millions of images in the sorghum fields, manually selected 5,447 images that contain aphids, and annotated each aphid cluster in the image. To use these images for machine learning models, we crop the images into patches and created a labeled dataset with over 151,000 image patches. Then, we implement and compare the performance of four state-of-the-art object detection models.","sentences":["Aphids are one of the main threats to crops, rural families, and global food security.","Chemical pest control is a necessary component of crop production for maximizing yields, however, it is unnecessary to apply the chemical approaches to the entire fields in consideration of the environmental pollution and the cost.","Thus, accurately localizing the aphid and estimating the infestation level is crucial to the precise local application of pesticides.","Aphid detection is very challenging as each individual aphid is really small and all aphids are crowded together as clusters.","In this paper, we propose to estimate the infection level by detecting aphid clusters.","We have taken millions of images in the sorghum fields, manually selected 5,447 images that contain aphids, and annotated each aphid cluster in the image.","To use these images for machine learning models, we crop the images into patches and created a labeled dataset with over 151,000 image patches.","Then, we implement and compare the performance of four state-of-the-art object detection models."],"url":"http://arxiv.org/abs/2307.05929v1"}
{"created":"2023-07-12 05:46:37","title":"Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation","abstract":"Building energy prediction and management has become increasingly important in recent decades, driven by the growth of Internet of Things (IoT) devices and the availability of more energy data. However, energy data is often collected from multiple sources and can be incomplete or inconsistent, which can hinder accurate predictions and management of energy systems and limit the usefulness of the data for decision-making and research. To address this issue, past studies have focused on imputing missing gaps in energy data, including random and continuous gaps. One of the main challenges in this area is the lack of validation on a benchmark dataset with various building and meter types, making it difficult to accurately evaluate the performance of different imputation methods. Another challenge is the lack of application of state-of-the-art imputation methods for missing gaps in energy data. Contemporary image-inpainting methods, such as Partial Convolution (PConv), have been widely used in the computer vision domain and have demonstrated their effectiveness in dealing with complex missing patterns. To study whether energy data imputation can benefit from the image-based deep learning method, this study compared PConv, Convolutional neural networks (CNNs), and weekly persistence method using one of the biggest publicly available whole building energy datasets, consisting of 1479 power meters worldwide, as the benchmark. The results show that, compared to the CNN with the raw time series (1D-CNN) and the weekly persistence method, neural network models with reshaped energy data with two dimensions reduced the Mean Squared Error (MSE) by 10% to 30%. The advanced deep learning method, Partial convolution (PConv), has further reduced the MSE by 20-30% than 2D-CNN and stands out among all models.","sentences":["Building energy prediction and management has become increasingly important in recent decades, driven by the growth of Internet of Things (IoT) devices and the availability of more energy data.","However, energy data is often collected from multiple sources and can be incomplete or inconsistent, which can hinder accurate predictions and management of energy systems and limit the usefulness of the data for decision-making and research.","To address this issue, past studies have focused on imputing missing gaps in energy data, including random and continuous gaps.","One of the main challenges in this area is the lack of validation on a benchmark dataset with various building and meter types, making it difficult to accurately evaluate the performance of different imputation methods.","Another challenge is the lack of application of state-of-the-art imputation methods for missing gaps in energy data.","Contemporary image-inpainting methods, such as Partial Convolution (PConv), have been widely used in the computer vision domain and have demonstrated their effectiveness in dealing with complex missing patterns.","To study whether energy data imputation can benefit from the image-based deep learning method, this study compared PConv, Convolutional neural networks (CNNs), and weekly persistence method using one of the biggest publicly available whole building energy datasets, consisting of 1479 power meters worldwide, as the benchmark.","The results show that, compared to the CNN with the raw time series (1D-CNN) and the weekly persistence method, neural network models with reshaped energy data with two dimensions reduced the Mean Squared Error (MSE) by 10% to 30%.","The advanced deep learning method, Partial convolution (PConv), has further reduced the MSE by 20-30% than 2D-CNN and stands out among all models."],"url":"http://arxiv.org/abs/2307.05926v2"}
{"created":"2023-07-12 05:45:27","title":"A Tractable Statistical Representation of IFTR Fading with Applications","abstract":"The recently introduced independent fluctuating two-ray (IFTR) fading model, consisting of two specular components fluctuating independently plus a diffuse component, has proven to provide an excellent fit to different wireless environments, including the millimeter-wave band. However, the original formulations of the probability density function (PDF) and cumulative distribution function (CDF) of this model are not applicable to all possible values of its defining parameters, and are given in terms of multifold generalized hypergeometric functions, which prevents their widespread use for the derivation of performance metric expressions. In this paper we present a new formulation of the IFTR model as a countable mixture of Gamma distributions which greatly facilitates the performance evaluation for this model in terms of the metrics already known for the much simpler and widely used Nakagami-m fading. Additionally, a closed-form expression is presented for the generalized moment generating function (GMGF), which permits to readily obtain all the moments of the distribution of the model, as well as several relevant performance metrics. Based on these new derivations, the IFTR model is evaluated for the average channel capacity, the outage probability with and without co-channel interference, and the bit error rate (BER), which are verified by Monte Carlo simulations.","sentences":["The recently introduced independent fluctuating two-ray (IFTR) fading model, consisting of two specular components fluctuating independently plus a diffuse component, has proven to provide an excellent fit to different wireless environments, including the millimeter-wave band.","However, the original formulations of the probability density function (PDF) and cumulative distribution function (CDF) of this model are not applicable to all possible values of its defining parameters, and are given in terms of multifold generalized hypergeometric functions, which prevents their widespread use for the derivation of performance metric expressions.","In this paper we present a new formulation of the IFTR model as a countable mixture of Gamma distributions which greatly facilitates the performance evaluation for this model in terms of the metrics already known for the much simpler and widely used Nakagami-m fading.","Additionally, a closed-form expression is presented for the generalized moment generating function (GMGF), which permits to readily obtain all the moments of the distribution of the model, as well as several relevant performance metrics.","Based on these new derivations, the IFTR model is evaluated for the average channel capacity, the outage probability with and without co-channel interference, and the bit error rate (BER), which are verified by Monte Carlo simulations."],"url":"http://arxiv.org/abs/2307.05925v1"}
{"created":"2023-07-12 05:44:33","title":"Applying SDN to Mobile Networks: A New Perspective for 6G Architecture","abstract":"The upcoming Sixth Generation (6G) mobile communications system envisions supporting a variety of use cases with differing characteristics, e.g., very low to extremely high data rates, diverse latency needs, ultra massive connectivity, sustainable communications, ultra-wide coverage etc. To accommodate these diverse use cases, the 6G system architecture needs to be scalable, modular, and flexible; both in its user plane and the control plane. In this paper, we identify some limitations of the existing Fifth Generation System (5GS) architecture, especially that of its control plane. Further, we propose a novel architecture for the 6G System (6GS) employing Software Defined Networking (SDN) technology to address these limitations of the control plane. The control plane in existing 5GS supports two different categories of functionalities handling end user signalling (e.g., user registration, authentication) and control of user plane functions. We propose to move the end-user signalling functionality out of the mobile network control plane and treat it as user service, i.e., as payload or data. This proposal results in an evolved service-driven architecture for mobile networks bringing increased simplicity, modularity, scalability, flexibility and security to its control plane. The proposed architecture can also support service specific signalling support, if needed, making it better suited for diverse 6GS use cases. To demonstrate the advantages of the proposed architecture, we also compare its performance with the 5GS using a process algebra-based simulation tool.","sentences":["The upcoming Sixth Generation (6G) mobile communications system envisions supporting a variety of use cases with differing characteristics, e.g., very low to extremely high data rates, diverse latency needs, ultra massive connectivity, sustainable communications, ultra-wide coverage etc.","To accommodate these diverse use cases, the 6G system architecture needs to be scalable, modular, and flexible; both in its user plane and the control plane.","In this paper, we identify some limitations of the existing Fifth Generation System (5GS) architecture, especially that of its control plane.","Further, we propose a novel architecture for the 6G System (6GS) employing Software Defined Networking (SDN) technology to address these limitations of the control plane.","The control plane in existing 5GS supports two different categories of functionalities handling end user signalling (e.g., user registration, authentication) and control of user plane functions.","We propose to move the end-user signalling functionality out of the mobile network control plane and treat it as user service, i.e., as payload or data.","This proposal results in an evolved service-driven architecture for mobile networks bringing increased simplicity, modularity, scalability, flexibility and security to its control plane.","The proposed architecture can also support service specific signalling support, if needed, making it better suited for diverse 6GS use cases.","To demonstrate the advantages of the proposed architecture, we also compare its performance with the 5GS using a process algebra-based simulation tool."],"url":"http://arxiv.org/abs/2307.05924v1"}
{"created":"2023-07-12 05:41:39","title":"Pairs-trading System using Quantum-inspired Combinatorial Optimization Accelerator for Optimal Path Search in Market Graphs","abstract":"Pairs-trading is a trading strategy that involves matching a long position with a short position in two stocks aiming at market-neutral profits. While a typical pairs-trading system monitors the prices of two statistically correlated stocks for detecting a temporary divergence, monitoring and analyzing the prices of more stocks would potentially lead to finding more trading opportunities. Here we report a stock pairs-trading system that finds trading opportunities for any two stocks in an $N$-stock universe using a combinatorial optimization accelerator based on a quantum-inspired algorithm called simulated bifurcation. The trading opportunities are detected through solving an optimal path search problem in an $N$-node directed graph with edge weights corresponding to the products of instantaneous price differences and statistical correlation factors between two stocks. The accelerator is one of Ising machines and operates consecutively to find multiple opportunities in a market situation with avoiding duplicate detections by a tabu search technique. It has been demonstrated in the Tokyo Stock Exchange that the FPGA (field-programmable gate array)-based trading system has a sufficiently low latency (33 $\\mu$s for $N$=15 or 210 pairs) to execute the pairs-trading strategy based on optimal path search in market graphs.","sentences":["Pairs-trading is a trading strategy that involves matching a long position with a short position in two stocks aiming at market-neutral profits.","While a typical pairs-trading system monitors the prices of two statistically correlated stocks for detecting a temporary divergence, monitoring and analyzing the prices of more stocks would potentially lead to finding more trading opportunities.","Here we report a stock pairs-trading system that finds trading opportunities for any two stocks in an $N$-stock universe using a combinatorial optimization accelerator based on a quantum-inspired algorithm called simulated bifurcation.","The trading opportunities are detected through solving an optimal path search problem in an $N$-node directed graph with edge weights corresponding to the products of instantaneous price differences and statistical correlation factors between two stocks.","The accelerator is one of Ising machines and operates consecutively to find multiple opportunities in a market situation with avoiding duplicate detections by a tabu search technique.","It has been demonstrated in the Tokyo Stock Exchange that the FPGA (field-programmable gate array)-based trading system has a sufficiently low latency (33 $\\mu$s for $N$=15 or 210 pairs) to execute the pairs-trading strategy based on optimal path search in market graphs."],"url":"http://arxiv.org/abs/2307.05923v1"}
{"created":"2023-07-12 05:40:07","title":"Sublinear Message Bounds of Authenticated Implicit Byzantine Agreement","abstract":"This paper studies the message complexity of authenticated Byzantine agreement (BA) in synchronous, fully-connected distributed networks under an honest majority. We focus on the so-called {\\em implicit} Byzantine agreement problem where each node starts with an input value and at the end a non-empty subset of the honest nodes should agree on a common input value by satisfying the BA properties (i.e., there can be undecided nodes). We show that a sublinear (in $n$, number of nodes) message complexity BA protocol under honest majority is possible in the standard PKI model when the nodes have access to an unbiased global coin and hash function. In particular, we present a randomized Byzantine agreement algorithm which, with high probability achieves implicit agreement, uses $\\tilde{O}(\\sqrt{n})$ messages, and runs in $\\tilde{O}(1)$ rounds while tolerating $(1/2 - \\epsilon)n$ Byzantine nodes for any fixed $\\epsilon > 0$, the notation $\\Tilde{O}$ hides a $O(\\polylog{n})$ factor. The algorithm requires standard cryptographic setup PKI and hash function with a static Byzantine adversary. The algorithm works in the CONGEST model and each node does not need to know the identity of its neighbors, i.e., works in the $KT_0$ model. The message complexity (and also the time complexity) of our algorithm is optimal up to a $\\polylog n$ factor, as we show a $\\Omega(\\sqrt{n})$ lower bound on the message complexity.","sentences":["This paper studies the message complexity of authenticated Byzantine agreement (BA) in synchronous, fully-connected distributed networks under an honest majority.","We focus on the so-called {\\em implicit} Byzantine agreement problem where each node starts with an input value and at the end a non-empty subset of the honest nodes should agree on a common input value by satisfying the BA properties (i.e., there can be undecided nodes).","We show that a sublinear (in $n$, number of nodes) message complexity BA protocol under honest majority is possible in the standard PKI model when the nodes have access to an unbiased global coin and hash function.","In particular, we present a randomized Byzantine agreement algorithm which, with high probability achieves implicit agreement, uses $\\tilde{O}(\\sqrt{n})$ messages, and runs in $\\tilde{O}(1)$ rounds while tolerating $(1/2 - \\epsilon)n$ Byzantine nodes for any fixed $\\epsilon > 0$, the notation $\\Tilde{O}$ hides a $O(\\polylog{n})$ factor.","The algorithm requires standard cryptographic setup PKI and hash function with a static Byzantine adversary.","The algorithm works in the CONGEST model and each node does not need to know the identity of its neighbors, i.e., works in the $KT_0$ model.","The message complexity (and also the time complexity) of our algorithm is optimal up to a $\\polylog n$ factor, as we show a $\\Omega(\\sqrt{n})$ lower bound on the message complexity."],"url":"http://arxiv.org/abs/2307.05922v1"}
{"created":"2023-07-12 05:36:47","title":"Reading Radiology Imaging Like The Radiologist","abstract":"Automated radiology report generation aims to generate radiology reports that contain rich, fine-grained descriptions of radiology imaging. Compared with image captioning in the natural image domain, medical images are very similar to each other, with only minor differences in the occurrence of diseases. Given the importance of these minor differences in the radiology report, it is crucial to encourage the model to focus more on the subtle regions of disease occurrence. Secondly, the problem of visual and textual data biases is serious. Not only do normal cases make up the majority of the dataset, but sentences describing areas with pathological changes also constitute only a small part of the paragraph. Lastly, generating medical image reports involves the challenge of long text generation, which requires more expertise and empirical training in medical knowledge. As a result, the difficulty of generating such reports is increased. To address these challenges, we propose a disease-oriented retrieval framework that utilizes similar reports as prior knowledge references. We design a factual consistency captioning generator to generate more accurate and factually consistent disease descriptions. Our framework can find most similar reports for a given disease from the CXR database by retrieving a disease-oriented mask consisting of the position and morphological characteristics. By referencing the disease-oriented similar report and the visual features, the factual consistency model can generate a more accurate radiology report.","sentences":["Automated radiology report generation aims to generate radiology reports that contain rich, fine-grained descriptions of radiology imaging.","Compared with image captioning in the natural image domain, medical images are very similar to each other, with only minor differences in the occurrence of diseases.","Given the importance of these minor differences in the radiology report, it is crucial to encourage the model to focus more on the subtle regions of disease occurrence.","Secondly, the problem of visual and textual data biases is serious.","Not only do normal cases make up the majority of the dataset, but sentences describing areas with pathological changes also constitute only a small part of the paragraph.","Lastly, generating medical image reports involves the challenge of long text generation, which requires more expertise and empirical training in medical knowledge.","As a result, the difficulty of generating such reports is increased.","To address these challenges, we propose a disease-oriented retrieval framework that utilizes similar reports as prior knowledge references.","We design a factual consistency captioning generator to generate more accurate and factually consistent disease descriptions.","Our framework can find most similar reports for a given disease from the CXR database by retrieving a disease-oriented mask consisting of the position and morphological characteristics.","By referencing the disease-oriented similar report and the visual features, the factual consistency model can generate a more accurate radiology report."],"url":"http://arxiv.org/abs/2307.05921v1"}
{"created":"2023-07-12 05:12:14","title":"DSPC: Efficiently Answering Shortest Path Counting on Dynamic Graphs","abstract":"The widespread use of graph data in various applications and the highly dynamic nature of today's networks have made it imperative to analyze structural trends in dynamic graphs on a continual basis. The shortest path is a fundamental concept in graph analysis and recent research shows that counting the number of shortest paths between two vertices is crucial in applications like potential friend recommendation and betweenness analysis. However, current studies that use hub labeling techniques for real-time shortest path counting are limited by their reliance on a pre-computed index, which cannot tackle frequent updates over dynamic graphs. To address this, we propose a novel approach for maintaining the index in response to changes in the graph structure and develop incremental (IncSPC) and decremental (DecSPC) update algorithms for inserting and deleting vertices/edges, respectively. The main idea of these two algorithms is that we only locate the affected vertices to update the index. Our experiments demonstrate that our dynamic algorithms are up to four orders of magnitude faster processing for incremental updates and up to three orders of magnitude faster processing for hybrid updates than reconstruction.","sentences":["The widespread use of graph data in various applications and the highly dynamic nature of today's networks have made it imperative to analyze structural trends in dynamic graphs on a continual basis.","The shortest path is a fundamental concept in graph analysis and recent research shows that counting the number of shortest paths between two vertices is crucial in applications like potential friend recommendation and betweenness analysis.","However, current studies that use hub labeling techniques for real-time shortest path counting are limited by their reliance on a pre-computed index, which cannot tackle frequent updates over dynamic graphs.","To address this, we propose a novel approach for maintaining the index in response to changes in the graph structure and develop incremental (IncSPC) and decremental (DecSPC) update algorithms for inserting and deleting vertices/edges, respectively.","The main idea of these two algorithms is that we only locate the affected vertices to update the index.","Our experiments demonstrate that our dynamic algorithms are up to four orders of magnitude faster processing for incremental updates and up to three orders of magnitude faster processing for hybrid updates than reconstruction."],"url":"http://arxiv.org/abs/2307.05918v1"}
{"created":"2023-07-12 04:53:36","title":"SwiFT: Swin 4D fMRI Transformer","abstract":"The modeling of spatiotemporal brain dynamics from high-dimensional data, such as 4D functional MRI, is a formidable task in neuroscience. To address this challenge, we present SwiFT (Swin 4D fMRI Transformer), a Swin Transformer architecture that can learn brain dynamics directly from 4D functional brain MRI data in a memory and computation-efficient manner. SwiFT achieves this by implementing a 4D window multi-head self-attention mechanism and absolute positional embeddings. We evaluate SwiFT using multiple largest-scale human functional brain imaging datasets in tasks such as predicting sex, age, and cognitive intelligence. Our experimental outcomes reveal that SwiFT consistently outperforms recent state-of-the-art models. To the best of our knowledge, SwiFT is the first Swin Transformer architecture that can process dimensional spatiotemporal brain functional data in an end-to-end fashion. Furthermore, due to the end-to-end learning capability, we also show that contrastive loss-based self-supervised pre-training of SwiFT is also feasible for achieving improved performance on a downstream task. We believe that our work holds substantial potential in facilitating scalable learning of functional brain imaging in neuroscience research by reducing the hurdles associated with applying Transformer models to high-dimensional fMRI.","sentences":["The modeling of spatiotemporal brain dynamics from high-dimensional data, such as 4D functional MRI, is a formidable task in neuroscience.","To address this challenge, we present SwiFT (Swin 4D fMRI Transformer), a Swin Transformer architecture that can learn brain dynamics directly from 4D functional brain MRI data in a memory and computation-efficient manner.","SwiFT achieves this by implementing a 4D window multi-head self-attention mechanism and absolute positional embeddings.","We evaluate SwiFT using multiple largest-scale human functional brain imaging datasets in tasks such as predicting sex, age, and cognitive intelligence.","Our experimental outcomes reveal that SwiFT consistently outperforms recent state-of-the-art models.","To the best of our knowledge, SwiFT is the first Swin Transformer architecture that can process dimensional spatiotemporal brain functional data in an end-to-end fashion.","Furthermore, due to the end-to-end learning capability, we also show that contrastive loss-based self-supervised pre-training of SwiFT is also feasible for achieving improved performance on a downstream task.","We believe that our work holds substantial potential in facilitating scalable learning of functional brain imaging in neuroscience research by reducing the hurdles associated with applying Transformer models to high-dimensional fMRI."],"url":"http://arxiv.org/abs/2307.05916v1"}
{"created":"2023-07-12 04:44:31","title":"Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering","abstract":"We present a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents. The framework adapts a retriever augmented generation model to the target domain using supervised finetuning and reinforcement learning with synthetic feedback in a few-shot setting. This yields an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs. The synthetic generation pipeline generates high quality synthetic training data musing a medium sized LLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is designed to generate both abstractive and extractive questions that span the entire corpus. Using samples from this dataset, the framework fine-tunes a smaller RAG model comprising a dense retriever and a smaller sized LLM on samples from the dataset. In parallel, the framework trains a Reward model to score domain grounded answers higher than hallucinated answers. In the next phase, the framework aligns to the RAG model with the target domain using reinforcement learning. This step improves the RAG model's ability to generate grounded answers and ignore out of domain questions. In the final phase, the framework calibrates the model uncertainty for extractive question-answers. This is a desirable feature since the model can be integrated into a cascading system where the RAG model's answer is surfaced only when the model is confident of its answer.","sentences":["We present a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents.","The framework adapts a retriever augmented generation model to the target domain using supervised finetuning and reinforcement learning with synthetic feedback in a few-shot setting.","This yields an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs.","The synthetic generation pipeline generates high quality synthetic training data musing a medium sized LLM, Flan-T5 XXL, and a novel consistency filtering scheme.","The pipeline is designed to generate both abstractive and extractive questions that span the entire corpus.","Using samples from this dataset, the framework fine-tunes a smaller RAG model comprising a dense retriever and a smaller sized LLM on samples from the dataset.","In parallel, the framework trains a Reward model to score domain grounded answers higher than hallucinated answers.","In the next phase, the framework aligns to the RAG model with the target domain using reinforcement learning.","This step improves the RAG model's ability to generate grounded answers and ignore out of domain questions.","In the final phase, the framework calibrates the model uncertainty for extractive question-answers.","This is a desirable feature since the model can be integrated into a cascading system where the RAG model's answer is surfaced only when the model is confident of its answer."],"url":"http://arxiv.org/abs/2307.05915v1"}
{"created":"2023-07-12 04:43:59","title":"FIS-ONE: Floor Identification System with One Label for Crowdsourced RF Signals","abstract":"Floor labels of crowdsourced RF signals are crucial for many smart-city applications, such as multi-floor indoor localization, geofencing, and robot surveillance. To build a prediction model to identify the floor number of a new RF signal upon its measurement, conventional approaches using the crowdsourced RF signals assume that at least few labeled signal samples are available on each floor. In this work, we push the envelope further and demonstrate that it is technically feasible to enable such floor identification with only one floor-labeled signal sample on the bottom floor while having the rest of signal samples unlabeled.   We propose FIS-ONE, a novel floor identification system with only one labeled sample. FIS-ONE consists of two steps, namely signal clustering and cluster indexing. We first build a bipartite graph to model the RF signal samples and obtain a latent representation of each node (each signal sample) using our attention-based graph neural network model so that the RF signal samples can be clustered more accurately. Then, we tackle the problem of indexing the clusters with proper floor labels, by leveraging the observation that signals from an access point can be detected on different floors, i.e., signal spillover. Specifically, we formulate a cluster indexing problem as a combinatorial optimization problem and show that it is equivalent to solving a traveling salesman problem, whose (near-)optimal solution can be found efficiently. We have implemented FIS-ONE and validated its effectiveness on the Microsoft dataset and in three large shopping malls. Our results show that FIS-ONE outperforms other baseline algorithms significantly, with up to 23% improvement in adjusted rand index and 25% improvement in normalized mutual information using only one floor-labeled signal sample.","sentences":["Floor labels of crowdsourced RF signals are crucial for many smart-city applications, such as multi-floor indoor localization, geofencing, and robot surveillance.","To build a prediction model to identify the floor number of a new RF signal upon its measurement, conventional approaches using the crowdsourced RF signals assume that at least few labeled signal samples are available on each floor.","In this work, we push the envelope further and demonstrate that it is technically feasible to enable such floor identification with only one floor-labeled signal sample on the bottom floor while having the rest of signal samples unlabeled.   ","We propose FIS-ONE, a novel floor identification system with only one labeled sample.","FIS-ONE consists of two steps, namely signal clustering and cluster indexing.","We first build a bipartite graph to model the RF signal samples and obtain a latent representation of each node (each signal sample) using our attention-based graph neural network model so that the RF signal samples can be clustered more accurately.","Then, we tackle the problem of indexing the clusters with proper floor labels, by leveraging the observation that signals from an access point can be detected on different floors, i.e., signal spillover.","Specifically, we formulate a cluster indexing problem as a combinatorial optimization problem and show that it is equivalent to solving a traveling salesman problem, whose (near-)optimal solution can be found efficiently.","We have implemented FIS-ONE and validated its effectiveness on the Microsoft dataset and in three large shopping malls.","Our results show that FIS-ONE outperforms other baseline algorithms significantly, with up to 23% improvement in adjusted rand index and 25% improvement in normalized mutual information using only one floor-labeled signal sample."],"url":"http://arxiv.org/abs/2307.05914v1"}
{"created":"2023-07-12 04:40:00","title":"Close-up View synthesis by Interpolating Optical Flow","abstract":"The virtual viewpoint is perceived as a new technique in virtual navigation, as yet not supported due to the lack of depth information and obscure camera parameters. In this paper, a method for achieving close-up virtual view is proposed and it only uses optical flow to build parallax effects to realize pseudo 3D projection without using depth sensor. We develop a bidirectional optical flow method to obtain any virtual viewpoint by proportional interpolation of optical flow. Moreover, with the ingenious application of the optical-flow-value, we achieve clear and visual-fidelity magnified results through lens stretching in any corner, which overcomes the visual distortion and image blur through viewpoint magnification and transition in Google Street View system.","sentences":["The virtual viewpoint is perceived as a new technique in virtual navigation, as yet not supported due to the lack of depth information and obscure camera parameters.","In this paper, a method for achieving close-up virtual view is proposed and it only uses optical flow to build parallax effects to realize pseudo 3D projection without using depth sensor.","We develop a bidirectional optical flow method to obtain any virtual viewpoint by proportional interpolation of optical flow.","Moreover, with the ingenious application of the optical-flow-value, we achieve clear and visual-fidelity magnified results through lens stretching in any corner, which overcomes the visual distortion and image blur through viewpoint magnification and transition in Google Street View system."],"url":"http://arxiv.org/abs/2307.05913v1"}
{"created":"2023-07-12 04:31:34","title":"Exploring the Sector-Specific Influence and Response of AI Tools: A Critical Review","abstract":"AI Tool is designed to generate human-like responses in natural language conversations. Using deep learning techniques, AI Tool has been trained on a diverse range of internet text to understand and generate coherent responses to a wide array of prompts and questions. It can provide information, engage in conversations, assist with tasks, and even offer creative suggestions. The underlying technology behind AI Tool is a transformer neural network. Transformers excel at capturing long-range dependencies in text, making them well-suited for language-related tasks. AI Tool, has 175 billion parameters, making it one of the largest and most powerful language models to date. AI Tool has been trained on a massive corpus of text from the internet, which allows it to leverage a broad understanding of language, general knowledge, and various domains. While AI Tool aims to provide accurate and helpful responses, it may occasionally produce incorrect or nonsensical answers. It's essential to critically evaluate the information it provides and verify it from reliable sources when necessary. This work presents an overview on AI Tool. It will helps to research community and others users to understand the uses of AI Tool and its interaction pattern.","sentences":["AI Tool is designed to generate human-like responses in natural language conversations.","Using deep learning techniques, AI Tool has been trained on a diverse range of internet text to understand and generate coherent responses to a wide array of prompts and questions.","It can provide information, engage in conversations, assist with tasks, and even offer creative suggestions.","The underlying technology behind AI Tool is a transformer neural network.","Transformers excel at capturing long-range dependencies in text, making them well-suited for language-related tasks.","AI Tool, has 175 billion parameters, making it one of the largest and most powerful language models to date.","AI Tool has been trained on a massive corpus of text from the internet, which allows it to leverage a broad understanding of language, general knowledge, and various domains.","While AI Tool aims to provide accurate and helpful responses, it may occasionally produce incorrect or nonsensical answers.","It's essential to critically evaluate the information it provides and verify it from reliable sources when necessary.","This work presents an overview on AI Tool.","It will helps to research community and others users to understand the uses of AI Tool and its interaction pattern."],"url":"http://arxiv.org/abs/2307.05909v1"}
{"created":"2023-07-12 04:28:41","title":"Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding","abstract":"This paper presents \"Predictive Pipelined Decoding (PPD),\" an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding.","sentences":["This paper presents \"Predictive Pipelined Decoding (PPD),\" an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding.","Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding.","This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies.","We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency.","Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct.","The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding."],"url":"http://arxiv.org/abs/2307.05908v1"}
{"created":"2023-07-12 04:23:26","title":"Mini-Batch Optimization of Contrastive Loss","abstract":"Contrastive learning has gained significant attention as a method for self-supervised learning. The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar. Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization. In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning. We show that mini-batch optimization is equivalent to full-batch optimization if and only if all $\\binom{N}{B}$ mini-batches are selected, while sub-optimality may arise when examining only a subset. We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches. Our experimental results validate our theoretical findings and demonstrate that our proposed algorithm outperforms vanilla SGD in practically relevant settings, providing a better understanding of mini-batch optimization in contrastive learning.","sentences":["Contrastive learning has gained significant attention as a method for self-supervised learning.","The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar.","Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization.","In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning.","We show that mini-batch optimization is equivalent to full-batch optimization if and only if all $\\binom{N}{B}$ mini-batches are selected, while sub-optimality may arise when examining only a subset.","We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches.","Our experimental results validate our theoretical findings and demonstrate that our proposed algorithm outperforms vanilla SGD in practically relevant settings, providing a better understanding of mini-batch optimization in contrastive learning."],"url":"http://arxiv.org/abs/2307.05906v1"}
{"created":"2023-07-12 04:19:47","title":"Stability Guarantees for Feature Attributions with Multiplicative Smoothing","abstract":"Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.","sentences":["Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process.","In this work, we analyze stability as a property for reliable feature attribution methods.","We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features.","To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS).","We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method.","We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees."],"url":"http://arxiv.org/abs/2307.05902v1"}
{"created":"2023-07-12 04:15:36","title":"Single Domain Generalization via Normalised Cross-correlation Based Convolutions","abstract":"Deep learning techniques often perform poorly in the presence of domain shift, where the test data follows a different distribution than the training data. The most practically desirable approach to address this issue is Single Domain Generalization (S-DG), which aims to train robust models using data from a single source. Prior work on S-DG has primarily focused on using data augmentation techniques to generate diverse training data. In this paper, we explore an alternative approach by investigating the robustness of linear operators, such as convolution and dense layers commonly used in deep learning. We propose a novel operator called XCNorm that computes the normalized cross-correlation between weights and an input feature patch. This approach is invariant to both affine shifts and changes in energy within a local feature patch and eliminates the need for commonly used non-linear activation functions. We show that deep neural networks composed of this operator are robust to common semantic distribution shifts. Furthermore, our empirical results on single-domain generalization benchmarks demonstrate that our proposed technique performs comparably to the state-of-the-art methods.","sentences":["Deep learning techniques often perform poorly in the presence of domain shift, where the test data follows a different distribution than the training data.","The most practically desirable approach to address this issue is Single Domain Generalization (S-DG), which aims to train robust models using data from a single source.","Prior work on S-DG has primarily focused on using data augmentation techniques to generate diverse training data.","In this paper, we explore an alternative approach by investigating the robustness of linear operators, such as convolution and dense layers commonly used in deep learning.","We propose a novel operator called XCNorm that computes the normalized cross-correlation between weights and an input feature patch.","This approach is invariant to both affine shifts and changes in energy within a local feature patch and eliminates the need for commonly used non-linear activation functions.","We show that deep neural networks composed of this operator are robust to common semantic distribution shifts.","Furthermore, our empirical results on single-domain generalization benchmarks demonstrate that our proposed technique performs comparably to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2307.05901v1"}
{"created":"2023-07-12 04:11:08","title":"DiffuseGAE: Controllable and High-fidelity Image Manipulation from Disentangled Representation","abstract":"Diffusion probabilistic models (DPMs) have shown remarkable results on various image synthesis tasks such as text-to-image generation and image inpainting. However, compared to other generative methods like VAEs and GANs, DPMs lack a low-dimensional, interpretable, and well-decoupled latent code. Recently, diffusion autoencoders (Diff-AE) were proposed to explore the potential of DPMs for representation learning via autoencoding. Diff-AE provides an accessible latent space that exhibits remarkable interpretability, allowing us to manipulate image attributes based on latent codes from the space. However, previous works are not generic as they only operated on a few limited attributes. To further explore the latent space of Diff-AE and achieve a generic editing pipeline, we proposed a module called Group-supervised AutoEncoder(dubbed GAE) for Diff-AE to achieve better disentanglement on the latent code. Our proposed GAE has trained via an attribute-swap strategy to acquire the latent codes for multi-attribute image manipulation based on examples. We empirically demonstrate that our method enables multiple-attributes manipulation and achieves convincing sample quality and attribute alignments, while significantly reducing computational requirements compared to pixel-based approaches for representational decoupling. Code will be released soon.","sentences":["Diffusion probabilistic models (DPMs) have shown remarkable results on various image synthesis tasks such as text-to-image generation and image inpainting.","However, compared to other generative methods like VAEs and GANs, DPMs lack a low-dimensional, interpretable, and well-decoupled latent code.","Recently, diffusion autoencoders (Diff-AE) were proposed to explore the potential of DPMs for representation learning via autoencoding.","Diff-AE provides an accessible latent space that exhibits remarkable interpretability, allowing us to manipulate image attributes based on latent codes from the space.","However, previous works are not generic as they only operated on a few limited attributes.","To further explore the latent space of Diff-AE and achieve a generic editing pipeline, we proposed a module called Group-supervised AutoEncoder(dubbed GAE) for Diff-AE to achieve better disentanglement on the latent code.","Our proposed GAE has trained via an attribute-swap strategy to acquire the latent codes for multi-attribute image manipulation based on examples.","We empirically demonstrate that our method enables multiple-attributes manipulation and achieves convincing sample quality and attribute alignments, while significantly reducing computational requirements compared to pixel-based approaches for representational decoupling.","Code will be released soon."],"url":"http://arxiv.org/abs/2307.05899v1"}
{"created":"2023-07-12 04:10:16","title":"Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation","abstract":"Noisy label problems are inevitably in existence within medical image segmentation causing severe performance degradation. Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the correlation between images has been overlooked. Especially for video segmentation, adjacent frames contain rich contextual information beneficial in cognizing noisy labels. Based on two insights, we propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to resolve noisy-labeled medical video segmentation issues. First, we argue the sequential prior of videos is an effective reference, i.e., pixel-level features from adjacent frames are close in distance for the same class and far in distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) is devised to indicate possible noisy labels by evaluating the affinity between pixels in two adjacent frames. We also notice that the noise distribution exhibits considerable variations across video, image, and pixel levels. In this way, we introduce Multi-Scale Supervision (MSS) to supervise the network from three different perspectives by re-weighting and refining the samples. This design enables the network to concentrate on clean samples in a coarse-to-fine manner. Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-art robust segmentation approaches. Code is available at https://github.com/BeileiCui/MS-TFAL.","sentences":["Noisy label problems are inevitably in existence within medical image segmentation causing severe performance degradation.","Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the correlation between images has been overlooked.","Especially for video segmentation, adjacent frames contain rich contextual information beneficial in cognizing noisy labels.","Based on two insights, we propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to resolve noisy-labeled medical video segmentation issues.","First, we argue the sequential prior of videos is an effective reference, i.e., pixel-level features from adjacent frames are close in distance for the same class and far in distance otherwise.","Therefore, Temporal Feature Affinity Learning (TFAL) is devised to indicate possible noisy labels by evaluating the affinity between pixels in two adjacent frames.","We also notice that the noise distribution exhibits considerable variations across video, image, and pixel levels.","In this way, we introduce Multi-Scale Supervision (MSS) to supervise the network from three different perspectives by re-weighting and refining the samples.","This design enables the network to concentrate on clean samples in a coarse-to-fine manner.","Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-art robust segmentation approaches.","Code is available at https://github.com/BeileiCui/MS-TFAL."],"url":"http://arxiv.org/abs/2307.05898v1"}
{"created":"2023-07-12 03:51:57","title":"Deep learning-based estimation of whole-body kinematics from multi-view images","abstract":"It is necessary to analyze the whole-body kinematics (including joint locations and joint angles) to assess risks of fatal and musculoskeletal injuries in occupational tasks. Human pose estimation has gotten more attention in recent years as a method to minimize the errors in determining joint locations. However, the joint angles are not often estimated, nor is the quality of joint angle estimation assessed. In this paper, we presented an end-to-end approach on direct joint angle estimation from multi-view images. Our method leveraged the volumetric pose representation and mapped the rotation representation to a continuous space where each rotation was uniquely represented. We also presented a new kinematic dataset in the domain of residential roofing with a data processing pipeline to generate necessary annotations for the supervised training procedure on direct joint angle estimation. We achieved a mean angle error of $7.19^\\circ$ on the new Roofing dataset and $8.41^\\circ$ on the Human3.6M dataset, paving the way for employment of on-site kinematic analysis using multi-view images.","sentences":["It is necessary to analyze the whole-body kinematics (including joint locations and joint angles) to assess risks of fatal and musculoskeletal injuries in occupational tasks.","Human pose estimation has gotten more attention in recent years as a method to minimize the errors in determining joint locations.","However, the joint angles are not often estimated, nor is the quality of joint angle estimation assessed.","In this paper, we presented an end-to-end approach on direct joint angle estimation from multi-view images.","Our method leveraged the volumetric pose representation and mapped the rotation representation to a continuous space where each rotation was uniquely represented.","We also presented a new kinematic dataset in the domain of residential roofing with a data processing pipeline to generate necessary annotations for the supervised training procedure on direct joint angle estimation.","We achieved a mean angle error of $7.19^\\circ$ on the new Roofing dataset and $8.41^\\circ$ on the Human3.6M dataset, paving the way for employment of on-site kinematic analysis using multi-view images."],"url":"http://arxiv.org/abs/2307.05896v1"}
{"created":"2023-07-12 03:45:45","title":"SC-NeuS: Consistent Neural Surface Reconstruction from Sparse and Noisy Views","abstract":"The recent neural surface reconstruction by volume rendering approaches have made much progress by achieving impressive surface reconstruction quality, but are still limited to dense and highly accurate posed views. To overcome such drawbacks, this paper pays special attention on the consistent surface reconstruction from sparse views with noisy camera poses. Unlike previous approaches, the key difference of this paper is to exploit the multi-view constraints directly from the explicit geometry of the neural surface, which can be used as effective regularization to jointly learn the neural surface and refine the camera poses. To build effective multi-view constraints, we introduce a fast differentiable on-surface intersection to generate on-surface points, and propose view-consistent losses based on such differentiable points to regularize the neural surface learning. Based on this point, we propose a jointly learning strategy for neural surface and camera poses, named SC-NeuS, to perform geometry-consistent surface reconstruction in an end-to-end manner. With extensive evaluation on public datasets, our SC-NeuS can achieve consistently better surface reconstruction results with fine-grained details than previous state-of-the-art neural surface reconstruction approaches, especially from sparse and noisy camera views.","sentences":["The recent neural surface reconstruction by volume rendering approaches have made much progress by achieving impressive surface reconstruction quality, but are still limited to dense and highly accurate posed views.","To overcome such drawbacks, this paper pays special attention on the consistent surface reconstruction from sparse views with noisy camera poses.","Unlike previous approaches, the key difference of this paper is to exploit the multi-view constraints directly from the explicit geometry of the neural surface, which can be used as effective regularization to jointly learn the neural surface and refine the camera poses.","To build effective multi-view constraints, we introduce a fast differentiable on-surface intersection to generate on-surface points, and propose view-consistent losses based on such differentiable points to regularize the neural surface learning.","Based on this point, we propose a jointly learning strategy for neural surface and camera poses, named SC-NeuS, to perform geometry-consistent surface reconstruction in an end-to-end manner.","With extensive evaluation on public datasets, our SC-NeuS can achieve consistently better surface reconstruction results with fine-grained details than previous state-of-the-art neural surface reconstruction approaches, especially from sparse and noisy camera views."],"url":"http://arxiv.org/abs/2307.05892v1"}
{"created":"2023-07-12 03:42:24","title":"PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks","abstract":"Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses PID features and another that extends these core ideas and can be used in arbitrary control tasks. When compared with prior approaches, our encoders produce policies that are often more robust and achieve better performance on a variety of tracking tasks. Going beyond tracking tasks, our policies achieve 1.7x better performance on average over previous state-of-the-art methods on a suite of high dimensional control tasks.","sentences":["Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone.","However, one challenge deep RL faces is that the full state of the system is often not observable.","When this is the case, the policy needs to leverage the history of observations to infer the current state.","At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time.","As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment.","To strike this balance, we look to the PID controller for inspiration.","We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks.","Following this principle, we propose two architectures for encoding history: one that directly uses PID features and another that extends these core ideas and can be used in arbitrary control tasks.","When compared with prior approaches, our encoders produce policies that are often more robust and achieve better performance on a variety of tracking tasks.","Going beyond tracking tasks, our policies achieve 1.7x better performance on average over previous state-of-the-art methods on a suite of high dimensional control tasks."],"url":"http://arxiv.org/abs/2307.05891v1"}
{"created":"2023-07-12 03:33:11","title":"Rethinking Mitosis Detection: Towards Diverse Data and Feature Representation","abstract":"Mitosis detection is one of the fundamental tasks in computational pathology, which is extremely challenging due to the heterogeneity of mitotic cell. Most of the current studies solve the heterogeneity in the technical aspect by increasing the model complexity. However, lacking consideration of the biological knowledge and the complex model design may lead to the overfitting problem while limited the generalizability of the detection model. In this paper, we systematically study the morphological appearances in different mitotic phases as well as the ambiguous non-mitotic cells and identify that balancing the data and feature diversity can achieve better generalizability. Based on this observation, we propose a novel generalizable framework (MitDet) for mitosis detection. The data diversity is considered by the proposed diversity-guided sample balancing (DGSB). And the feature diversity is preserved by inter- and intra- class feature diversity-preserved module (InCDP). Stain enhancement (SE) module is introduced to enhance the domain-relevant diversity of both data and features simultaneously. Extensive experiments have demonstrated that our proposed model outperforms all the SOTA approaches in several popular mitosis detection datasets in both internal and external test sets using minimal annotation efforts with point annotations only. Comprehensive ablation studies have also proven the effectiveness of the rethinking of data and feature diversity balancing. By analyzing the results quantitatively and qualitatively, we believe that our proposed model not only achieves SOTA performance but also might inspire the future studies in new perspectives. Source code is at https://github.com/Onehour0108/MitDet.","sentences":["Mitosis detection is one of the fundamental tasks in computational pathology, which is extremely challenging due to the heterogeneity of mitotic cell.","Most of the current studies solve the heterogeneity in the technical aspect by increasing the model complexity.","However, lacking consideration of the biological knowledge and the complex model design may lead to the overfitting problem while limited the generalizability of the detection model.","In this paper, we systematically study the morphological appearances in different mitotic phases as well as the ambiguous non-mitotic cells and identify that balancing the data and feature diversity can achieve better generalizability.","Based on this observation, we propose a novel generalizable framework (MitDet) for mitosis detection.","The data diversity is considered by the proposed diversity-guided sample balancing (DGSB).","And the feature diversity is preserved by inter- and intra- class feature diversity-preserved module (InCDP).","Stain enhancement (SE) module is introduced to enhance the domain-relevant diversity of both data and features simultaneously.","Extensive experiments have demonstrated that our proposed model outperforms all the SOTA approaches in several popular mitosis detection datasets in both internal and external test sets using minimal annotation efforts with point annotations only.","Comprehensive ablation studies have also proven the effectiveness of the rethinking of data and feature diversity balancing.","By analyzing the results quantitatively and qualitatively, we believe that our proposed model not only achieves SOTA performance but also might inspire the future studies in new perspectives.","Source code is at https://github.com/Onehour0108/MitDet."],"url":"http://arxiv.org/abs/2307.05889v1"}
{"created":"2023-07-12 03:31:34","title":"Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment","abstract":"In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to empower various areas as a bridge between physical objects and the digital world. Through virtualization and simulation techniques, multiple functions can be achieved by leveraging computing resources. In this process, Mobile Cloud Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key factors to achieve real-time feedback. However, current works only considered edge servers or cloud servers in the DT system models. Besides, The models ignore the DT with not only one data resource. In this paper, we propose a new DT system model considering a heterogeneous MEC/MCC environment. Each DT in the model is maintained in one of the servers via multiple data collection devices. The offloading decision-making problem is also considered and a new offloading scheme is proposed based on Distributed Deep Learning (DDL). Simulation results demonstrate that our proposed algorithm can effectively and efficiently decrease the system's average latency and energy consumption. Significant improvement is achieved compared with the baselines under the dynamic environment of DTs.","sentences":["In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to empower various areas as a bridge between physical objects and the digital world.","Through virtualization and simulation techniques, multiple functions can be achieved by leveraging computing resources.","In this process, Mobile Cloud Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key factors to achieve real-time feedback.","However, current works only considered edge servers or cloud servers in the DT system models.","Besides, The models ignore the DT with not only one data resource.","In this paper, we propose a new DT system model considering a heterogeneous MEC/MCC environment.","Each DT in the model is maintained in one of the servers via multiple data collection devices.","The offloading decision-making problem is also considered and a new offloading scheme is proposed based on Distributed Deep Learning (DDL).","Simulation results demonstrate that our proposed algorithm can effectively and efficiently decrease the system's average latency and energy consumption.","Significant improvement is achieved compared with the baselines under the dynamic environment of DTs."],"url":"http://arxiv.org/abs/2307.05888v2"}
{"created":"2023-07-12 02:02:18","title":"Multi-Object Tracking as Attention Mechanism","abstract":"We propose a conceptually simple and thus fast multi-object tracking (MOT) model that does not require any attached modules, such as the Kalman filter, Hungarian algorithm, transformer blocks, or graph networks. Conventional MOT models are built upon the multi-step modules listed above, and thus the computational cost is high. Our proposed end-to-end MOT model, \\textit{TicrossNet}, is composed of a base detector and a cross-attention module only. As a result, the overhead of tracking does not increase significantly even when the number of instances ($N_t$) increases. We show that TicrossNet runs \\textit{in real-time}; specifically, it achieves 32.6 FPS on MOT17 and 31.0 FPS on MOT20 (Tesla V100), which includes as many as $>$100 instances per frame. We also demonstrate that TicrossNet is robust to $N_t$; thus, it does not have to change the size of the base detector, depending on $N_t$, as is often done by other models for real-time processing.","sentences":["We propose a conceptually simple and thus fast multi-object tracking (MOT) model that does not require any attached modules, such as the Kalman filter, Hungarian algorithm, transformer blocks, or graph networks.","Conventional MOT models are built upon the multi-step modules listed above, and thus the computational cost is high.","Our proposed end-to-end MOT model, \\textit{TicrossNet}, is composed of a base detector and a cross-attention module only.","As a result, the overhead of tracking does not increase significantly even when the number of instances ($N_t$) increases.","We show that TicrossNet runs \\textit{in real-time}; specifically, it achieves 32.6 FPS on MOT17 and 31.0 FPS on MOT20 (Tesla V100), which includes as many as $>$100 instances per frame.","We also demonstrate that TicrossNet is robust to $N_t$; thus, it does not have to change the size of the base detector, depending on $N_t$, as is often done by other models for real-time processing."],"url":"http://arxiv.org/abs/2307.05874v1"}
{"created":"2023-07-12 01:59:26","title":"OG: Equip vision occupancy with instance segmentation and visual grounding","abstract":"Occupancy prediction tasks focus on the inference of both geometry and semantic labels for each voxel, which is an important perception mission. However, it is still a semantic segmentation task without distinguishing various instances. Further, although some existing works, such as Open-Vocabulary Occupancy (OVO), have already solved the problem of open vocabulary detection, visual grounding in occupancy has not been solved to the best of our knowledge. To tackle the above two limitations, this paper proposes Occupancy Grounding (OG), a novel method that equips vanilla occupancy instance segmentation ability and could operate visual grounding in a voxel manner with the help of grounded-SAM. Keys to our approach are (1) affinity field prediction for instance clustering and (2) association strategy for aligning 2D instance masks and 3D occupancy instances. Extensive experiments have been conducted whose visualization results and analysis are shown below. Our code will be publicly released soon.","sentences":["Occupancy prediction tasks focus on the inference of both geometry and semantic labels for each voxel, which is an important perception mission.","However, it is still a semantic segmentation task without distinguishing various instances.","Further, although some existing works, such as Open-Vocabulary Occupancy (OVO), have already solved the problem of open vocabulary detection, visual grounding in occupancy has not been solved to the best of our knowledge.","To tackle the above two limitations, this paper proposes Occupancy Grounding (OG), a novel method that equips vanilla occupancy instance segmentation ability and could operate visual grounding in a voxel manner with the help of grounded-SAM.","Keys to our approach are (1) affinity field prediction for instance clustering and (2) association strategy for aligning 2D instance masks and 3D occupancy instances.","Extensive experiments have been conducted whose visualization results and analysis are shown below.","Our code will be publicly released soon."],"url":"http://arxiv.org/abs/2307.05873v1"}
{"created":"2023-07-12 01:55:55","title":"Useful but Distracting: Keyword Highlights and Time-Synchronization in Captions for Language Learning","abstract":"Captions provide language learners with a scaffold for comprehension and vocabulary acquisition. Past work has proposed several enhancements such as keyword highlights for increased learning gains. However, little is known about learners' experience with enhanced captions, although this is critical for adoption in everyday life. We conducted a survey and focus group to elicit learner preferences and requirements and implemented a processing pipeline for enhanced captions with keyword highlights, time-synchronized keyword highlights, and keyword captions. A subsequent online study (n = 49) showed that time-synchronized keyword highlights were the preferred design for learning but were perceived as too distracting to replace standard captions in everyday viewing scenarios. We conclude that keyword highlights and time-synchronization are suitable for integrating learning into an entertaining everyday-life activity, but the design should be optimized to provide a more seamless experience.","sentences":["Captions provide language learners with a scaffold for comprehension and vocabulary acquisition.","Past work has proposed several enhancements such as keyword highlights for increased learning gains.","However, little is known about learners' experience with enhanced captions, although this is critical for adoption in everyday life.","We conducted a survey and focus group to elicit learner preferences and requirements and implemented a processing pipeline for enhanced captions with keyword highlights, time-synchronized keyword highlights, and keyword captions.","A subsequent online study (n = 49) showed that time-synchronized keyword highlights were the preferred design for learning but were perceived as too distracting to replace standard captions in everyday viewing scenarios.","We conclude that keyword highlights and time-synchronization are suitable for integrating learning into an entertaining everyday-life activity, but the design should be optimized to provide a more seamless experience."],"url":"http://arxiv.org/abs/2307.05870v1"}
{"created":"2023-07-12 01:50:12","title":"Autonomous and Ubiquitous In-node Learning Algorithms of Active Directed Graphs and Its Storage Behavior","abstract":"Memory is an important cognitive function for humans. How a brain with such a small power can complete such a complex memory function, the working mechanism behind this is undoubtedly fascinating. Engram theory views memory as the co-activation of specific neuronal clusters. From the perspective of graph theory, nodes represent neurons, and directed edges represent synapses. Then the memory engram is the connected subgraph formed between the activated nodes. In this paper, we use subgraphs as physical carriers of information and propose a parallel distributed information storage algorithm based on node scale in active-directed graphs. An active-directed graph is defined as a graph in which each node has autonomous and independent behavior and relies only on information obtained within the local field of view to make decisions. Unlike static directed graphs used for recording facts, active-directed graphs are decentralized like biological neuron networks and do not have a super manager who has a global view and can control the behavior of each node. Distinct from traditional algorithms with a global field of view, this algorithm is characterized by nodes collaborating globally on resource usage through their limited local field of view. While this strategy may not achieve global optimality as well as algorithms with a global field of view, it offers better robustness, concurrency, decentralization, and bioviability. Finally, it was tested in network capacity, fault tolerance, and robustness. It was found that the algorithm exhibits a larger network capacity in a more sparse network structure because the subgraph generated by a single sample is not a whole but consists of multiple weakly connected components. In this case, the network capacity can be understood as the number of permutations of several weakly connected components in the network.","sentences":["Memory is an important cognitive function for humans.","How a brain with such a small power can complete such a complex memory function, the working mechanism behind this is undoubtedly fascinating.","Engram theory views memory as the co-activation of specific neuronal clusters.","From the perspective of graph theory, nodes represent neurons, and directed edges represent synapses.","Then the memory engram is the connected subgraph formed between the activated nodes.","In this paper, we use subgraphs as physical carriers of information and propose a parallel distributed information storage algorithm based on node scale in active-directed graphs.","An active-directed graph is defined as a graph in which each node has autonomous and independent behavior and relies only on information obtained within the local field of view to make decisions.","Unlike static directed graphs used for recording facts, active-directed graphs are decentralized like biological neuron networks and do not have a super manager who has a global view and can control the behavior of each node.","Distinct from traditional algorithms with a global field of view, this algorithm is characterized by nodes collaborating globally on resource usage through their limited local field of view.","While this strategy may not achieve global optimality as well as algorithms with a global field of view, it offers better robustness, concurrency, decentralization, and bioviability.","Finally, it was tested in network capacity, fault tolerance, and robustness.","It was found that the algorithm exhibits a larger network capacity in a more sparse network structure because the subgraph generated by a single sample is not a whole but consists of multiple weakly connected components.","In this case, the network capacity can be understood as the number of permutations of several weakly connected components in the network."],"url":"http://arxiv.org/abs/2307.05869v1"}
{"created":"2023-07-12 01:11:52","title":"Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes","abstract":"Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we find these improvements rarely reduce the prevalence of systemic failure. Instead, the benefits of these improvements predominantly accrue to individuals who are already correctly classified by other models. In light of these trends, we consider medical imaging for dermatology where the costs of systemic failure are especially high. While traditional analyses reveal racial performance disparities for both models and humans, ecosystem-level analysis reveals new forms of racial disparity in model predictions that do not present in human predictions. These examples demonstrate ecosystem-level analysis has unique strengths for characterizing the societal impact of machine learning.","sentences":["Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models.","In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments.","To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context.","For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to.","Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available.","Even when individual models improve at the population level over time, we find these improvements rarely reduce the prevalence of systemic failure.","Instead, the benefits of these improvements predominantly accrue to individuals who are already correctly classified by other models.","In light of these trends, we consider medical imaging for dermatology where the costs of systemic failure are especially high.","While traditional analyses reveal racial performance disparities for both models and humans, ecosystem-level analysis reveals new forms of racial disparity in model predictions that do not present in human predictions.","These examples demonstrate ecosystem-level analysis has unique strengths for characterizing the societal impact of machine learning."],"url":"http://arxiv.org/abs/2307.05862v1"}
{"created":"2023-07-12 01:08:11","title":"DeepMapping: The Case for Learned Data Mapping for Compression and Efficient Query Processing","abstract":"Storing tabular data in a way that balances storage and query efficiencies is a long standing research question in the database community. While there are several lossless compression techniques in the literature, in this work we argue and show that a novel Deep Learned Data Mapping (or DeepMapping) abstraction, which relies on the impressive memorization capabilities of deep neural networks, can provide better storage cost, better latency, and better run-time memory footprint, all at the same time. Our proposed DeepMapping abstraction transforms a data set into multiple key-value mappings and constructs a multi-tasking neural network model that outputs the corresponding values for a given input key. In order to deal with the memorization errors, DeepMapping couples the learned neural network with a light-weight auxiliary data structure capable of correcting errors. The auxiliary structure further enables DeepMapping to efficiently deal with insertions, deletions, and updates, without having to re-train the mapping. Since the shape of the network has a significant impact on the overall size of the DeepMapping structure, we further propose a multi-task hybrid architecture search strategy to identify DeepMapping architectures that strike a desirable balance among memorization capacity, size, and efficiency. Extensive experiments with synthetic and benchmark datasets, including TPC-H and TPC-DS, demonstrated that the proposed DeepMapping approach can significantly reduce the latency of the key-based queries, while simultaneously improving both offline and run-time storage requirements against several cutting-edge competitors.","sentences":["Storing tabular data in a way that balances storage and query efficiencies is a long standing research question in the database community.","While there are several lossless compression techniques in the literature, in this work we argue and show that a novel Deep Learned Data Mapping (or DeepMapping) abstraction, which relies on the impressive memorization capabilities of deep neural networks, can provide better storage cost, better latency, and better run-time memory footprint, all at the same time.","Our proposed DeepMapping abstraction transforms a data set into multiple key-value mappings and constructs a multi-tasking neural network model that outputs the corresponding values for a given input key.","In order to deal with the memorization errors, DeepMapping couples the learned neural network with a light-weight auxiliary data structure capable of correcting errors.","The auxiliary structure further enables DeepMapping to efficiently deal with insertions, deletions, and updates, without having to re-train the mapping.","Since the shape of the network has a significant impact on the overall size of the DeepMapping structure, we further propose a multi-task hybrid architecture search strategy to identify DeepMapping architectures that strike a desirable balance among memorization capacity, size, and efficiency.","Extensive experiments with synthetic and benchmark datasets, including TPC-H and TPC-DS, demonstrated that the proposed DeepMapping approach can significantly reduce the latency of the key-based queries, while simultaneously improving both offline and run-time storage requirements against several cutting-edge competitors."],"url":"http://arxiv.org/abs/2307.05861v1"}
{"created":"2023-07-12 00:35:19","title":"FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems","abstract":"Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design FAIRO to generalize to three types of HITL application setups that have the shared adaptation decision problem. Furthermore, we recognize that fairness-aware policies can sometimes conflict with the application's utility. To address this challenge, we provide a fairness-utility tradeoff in FAIRO, allowing system designers to balance the objectives of fairness and utility based on specific application requirements. Extensive evaluations of FAIRO on the three HITL applications demonstrate its generalizability and effectiveness in promoting fairness while accounting for human variability. On average, FAIRO can improve fairness compared with other methods across all three applications by 35.36%.","sentences":["Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system.","This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability.","This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time.","We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process.","In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework.","We design FAIRO to generalize to three types of HITL application setups that have the shared adaptation decision problem.","Furthermore, we recognize that fairness-aware policies can sometimes conflict with the application's utility.","To address this challenge, we provide a fairness-utility tradeoff in FAIRO, allowing system designers to balance the objectives of fairness and utility based on specific application requirements.","Extensive evaluations of FAIRO on the three HITL applications demonstrate its generalizability and effectiveness in promoting fairness while accounting for human variability.","On average, FAIRO can improve fairness compared with other methods across all three applications by 35.36%."],"url":"http://arxiv.org/abs/2307.05857v1"}
{"created":"2023-07-12 00:13:04","title":"GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human","abstract":"3D human pose estimation has been researched for decades with promising fruits. 3D human pose lifting is one of the promising research directions toward the task where both estimated pose and ground truth pose data are used for training. Existing pose lifting works mainly focus on improving the performance of estimated pose, but they usually underperform when testing on the ground truth pose data. We observe that the performance of the estimated pose can be easily improved by preparing good quality 2D pose, such as fine-tuning the 2D pose or using advanced 2D pose detectors. As such, we concentrate on improving the 3D human pose lifting via ground truth data for the future improvement of more quality estimated pose data. Towards this goal, a simple yet effective model called Global-local Adaptive Graph Convolutional Network (GLA-GCN) is proposed in this work. Our GLA-GCN globally models the spatiotemporal structure via a graph representation and backtraces local joint features for 3D human pose estimation via individually connected layers. To validate our model design, we conduct extensive experiments on three benchmark datasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results show that our GLA-GCN implemented with ground truth 2D poses significantly outperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% error reductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).","sentences":["3D human pose estimation has been researched for decades with promising fruits.","3D human pose lifting is one of the promising research directions toward the task where both estimated pose and ground truth pose data are used for training.","Existing pose lifting works mainly focus on improving the performance of estimated pose, but they usually underperform when testing on the ground truth pose data.","We observe that the performance of the estimated pose can be easily improved by preparing good quality 2D pose, such as fine-tuning the 2D pose or using advanced 2D pose detectors.","As such, we concentrate on improving the 3D human pose lifting via ground truth data for the future improvement of more quality estimated pose data.","Towards this goal, a simple yet effective model called Global-local Adaptive Graph Convolutional Network (GLA-GCN) is proposed in this work.","Our GLA-GCN globally models the spatiotemporal structure via a graph representation and backtraces local joint features for 3D human pose estimation via individually connected layers.","To validate our model design, we conduct extensive experiments on three benchmark datasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP.","Experimental results show that our GLA-GCN implemented with ground truth 2D poses significantly outperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% error reductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively)."],"url":"http://arxiv.org/abs/2307.05853v1"}
{"created":"2023-07-11 23:36:49","title":"PIGEON: Predicting Image Geolocations","abstract":"We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.","sentences":["We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation.","Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells.","The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset.","Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task.","Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding."],"url":"http://arxiv.org/abs/2307.05845v1"}
{"created":"2023-07-11 23:32:26","title":"The Butterfly Effect in AI Fairness and Bias","abstract":"The Butterfly Effect, a concept originating from chaos theory, underscores how small changes can have significant and unpredictable impacts on complex systems. In the context of AI fairness and bias, the Butterfly Effect can stem from a variety of sources, such as small biases or skewed data inputs during algorithm development, saddle points in training, or distribution shifts in data between training and testing phases. These seemingly minor alterations can lead to unexpected and substantial unfair outcomes, disproportionately affecting underrepresented individuals or groups and perpetuating pre-existing inequalities. Moreover, the Butterfly Effect can amplify inherent biases within data or algorithms, exacerbate feedback loops, and create vulnerabilities for adversarial attacks. Given the intricate nature of AI systems and their societal implications, it is crucial to thoroughly examine any changes to algorithms or input data for potential unintended consequences. In this paper, we envision both algorithmic and empirical strategies to detect, quantify, and mitigate the Butterfly Effect in AI systems, emphasizing the importance of addressing these challenges to promote fairness and ensure responsible AI development.","sentences":["The Butterfly Effect, a concept originating from chaos theory, underscores how small changes can have significant and unpredictable impacts on complex systems.","In the context of AI fairness and bias, the Butterfly Effect can stem from a variety of sources, such as small biases or skewed data inputs during algorithm development, saddle points in training, or distribution shifts in data between training and testing phases.","These seemingly minor alterations can lead to unexpected and substantial unfair outcomes, disproportionately affecting underrepresented individuals or groups and perpetuating pre-existing inequalities.","Moreover, the Butterfly Effect can amplify inherent biases within data or algorithms, exacerbate feedback loops, and create vulnerabilities for adversarial attacks.","Given the intricate nature of AI systems and their societal implications, it is crucial to thoroughly examine any changes to algorithms or input data for potential unintended consequences.","In this paper, we envision both algorithmic and empirical strategies to detect, quantify, and mitigate the Butterfly Effect in AI systems, emphasizing the importance of addressing these challenges to promote fairness and ensure responsible AI development."],"url":"http://arxiv.org/abs/2307.05842v2"}
{"created":"2023-07-11 23:27:26","title":"Influential Simplices Mining via Simplicial Convolutional Network","abstract":"Simplicial complexes have recently been in the limelight of higher-order network analysis, where a minority of simplices play crucial roles in structures and functions due to network heterogeneity. We find a significant inconsistency between identifying influential nodes and simplices. Therefore, it remains elusive how to characterize simplices' influence and identify influential simplices, despite the relative maturity of research on influential nodes (0-simplices) identification. Meanwhile, graph neural networks (GNNs) are potent tools that can exploit network topology and node features simultaneously, but they struggle to tackle higher-order tasks. In this paper, we propose a higher-order graph learning model, named influential simplices mining neural network (ISMnet), to identify vital h-simplices in simplicial complexes. It can tackle higher-order tasks by leveraging novel higher-order presentations: hierarchical bipartite graphs and higher-order hierarchical (HoH) Laplacians, where targeted simplices are grouped into a hub set and can interact with other simplices. Furthermore, ISMnet employs learnable graph convolutional operators in each HoH Laplacian domain to capture interactions among simplices, and it can identify influential simplices of arbitrary order by changing the hub set. Empirical results demonstrate that ISMnet significantly outperforms existing methods in ranking 0-simplices (nodes) and 2-simplices. In general, this novel framework excels in identifying influential simplices and promises to serve as a potent tool in higher-order network analysis.","sentences":["Simplicial complexes have recently been in the limelight of higher-order network analysis, where a minority of simplices play crucial roles in structures and functions due to network heterogeneity.","We find a significant inconsistency between identifying influential nodes and simplices.","Therefore, it remains elusive how to characterize simplices' influence and identify influential simplices, despite the relative maturity of research on influential nodes (0-simplices) identification.","Meanwhile, graph neural networks (GNNs) are potent tools that can exploit network topology and node features simultaneously, but they struggle to tackle higher-order tasks.","In this paper, we propose a higher-order graph learning model, named influential simplices mining neural network (ISMnet), to identify vital h-simplices in simplicial complexes.","It can tackle higher-order tasks by leveraging novel higher-order presentations: hierarchical bipartite graphs and higher-order hierarchical (HoH) Laplacians, where targeted simplices are grouped into a hub set and can interact with other simplices.","Furthermore, ISMnet employs learnable graph convolutional operators in each HoH Laplacian domain to capture interactions among simplices, and it can identify influential simplices of arbitrary order by changing the hub set.","Empirical results demonstrate that ISMnet significantly outperforms existing methods in ranking 0-simplices (nodes) and 2-simplices.","In general, this novel framework excels in identifying influential simplices and promises to serve as a potent tool in higher-order network analysis."],"url":"http://arxiv.org/abs/2307.05841v1"}
{"created":"2023-07-11 22:58:53","title":"Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing","abstract":"Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $\\epsilon$-optimal policies for the tasks. Our research demonstrates that to achieve $\\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number of episodes that is at most $\\tilde{\\mathcal{O}}({d^3H^6(\\epsilon^{-2}+c_{\\rm sep}^{-2})}\\cdot M/N)$, where $c_{\\rm sep}>0$ is a constant representing task separability, $H$ is the horizon of each episode, and $d$ is the feature dimension of the dynamics and rewards. Notably, DistMT-LSVI improves the sample complexity of non-distributed settings by a factor of $1/N$, as each agent independently learns $\\epsilon$-optimal policies for all $M$ tasks using $\\tilde{\\mathcal{O}}(d^3H^6M\\epsilon^{-2})$ episodes. Additionally, we provide numerical experiments conducted on OpenAI Gym Atari environments that validate our theoretical findings.","sentences":["Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges.","In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities.","We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards.","To tackle this problem, we propose an algorithm called DistMT-LSVI.","First, the agents identify the tasks, and then they exchange information through a central server to derive $\\epsilon$-optimal policies for the tasks.","Our research demonstrates that to achieve $\\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number of episodes that is at most $\\tilde{\\mathcal{O}}({d^3H^6(\\epsilon^{-2}+c_{\\rm sep}^{-2})}\\cdot M/N)$, where $c_{\\rm sep}>0$ is a constant representing task separability, $H$ is the horizon of each episode, and $d$ is the feature dimension of the dynamics and rewards.","Notably, DistMT-LSVI improves the sample complexity of non-distributed settings by a factor of $1/N$, as each agent independently learns $\\epsilon$-optimal policies for all $M$ tasks using $\\tilde{\\mathcal{O}}(d^3H^6M\\epsilon^{-2})$ episodes.","Additionally, we provide numerical experiments conducted on OpenAI Gym Atari environments that validate our theoretical findings."],"url":"http://arxiv.org/abs/2307.05834v1"}
{"created":"2023-07-11 22:56:55","title":"Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction","abstract":"UAV-based intelligent data acquisition for 3D reconstruction and monitoring of infrastructure has been experiencing an increasing surge of interest due to the recent advancements in image processing and deep learning-based techniques. View planning is an essential part of this task that dictates the information capture strategy and heavily impacts the quality of the 3D model generated from the captured data. Recent methods have used prior knowledge or partial reconstruction of the target to accomplish view planning for active reconstruction; the former approach poses a challenge for complex or newly identified targets while the latter is computationally expensive. In this work, we present Bag-of-Views (BoV), a fully appearance-based model used to assign utility to the captured views for both offline dataset refinement and online next-best-view (NBV) planning applications targeting the task of 3D reconstruction. With this contribution, we also developed the View Planning Toolbox (VPT), a lightweight package for training and testing machine learning-based view planning frameworks, custom view dataset generation of arbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of our model in reducing the number of required views for high-quality reconstructions in dataset refinement and NBV planning.","sentences":["UAV-based intelligent data acquisition for 3D reconstruction and monitoring of infrastructure has been experiencing an increasing surge of interest due to the recent advancements in image processing and deep learning-based techniques.","View planning is an essential part of this task that dictates the information capture strategy and heavily impacts the quality of the 3D model generated from the captured data.","Recent methods have used prior knowledge or partial reconstruction of the target to accomplish view planning for active reconstruction; the former approach poses a challenge for complex or newly identified targets while the latter is computationally expensive.","In this work, we present Bag-of-Views (BoV), a fully appearance-based model used to assign utility to the captured views for both offline dataset refinement and online next-best-view (NBV) planning applications targeting the task of 3D reconstruction.","With this contribution, we also developed the View Planning Toolbox (VPT), a lightweight package for training and testing machine learning-based view planning frameworks, custom view dataset generation of arbitrary 3D scenes, and 3D reconstruction.","Through experiments which pair a BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of our model in reducing the number of required views for high-quality reconstructions in dataset refinement and NBV planning."],"url":"http://arxiv.org/abs/2307.05832v1"}
{"created":"2023-07-11 22:53:09","title":"Memorization Through the Lens of Curvature of Loss Function Around Samples","abstract":"Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.","sentences":["Neural networks are overparametrized and easily overfit the datasets they train on.","In the extreme case, it is shown that they can memorize a training set with fully randomized labels.","We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs.","We use this to study the generalization versus memorization properties of different samples in popular image datasets.","We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples.","This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels.","We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples."],"url":"http://arxiv.org/abs/2307.05831v1"}
{"created":"2023-07-11 22:51:54","title":"SnakeSynth: New Interactions for Generative Audio Synthesis","abstract":"I present \"SnakeSynth,\" a web-based lightweight audio synthesizer that combines audio generated by a deep generative model and real-time continuous two-dimensional (2D) input to create and control variable-length generative sounds through 2D interaction gestures. Interaction gestures are touch and mobile-compatible with analogies to strummed, bowed, and plucked musical instrument controls. Point-and-click and drag-and-drop gestures directly control audio playback length and I show that sound length and intensity are modulated by interactions with a programmable 2D coordinate grid. Leveraging the speed and ubiquity of browser-based audio and hardware acceleration in Google's TensorFlow.js we generate time-varying high-fidelity sounds with real-time interactivity. SnakeSynth adaptively reproduces and interpolates between sounds encountered during model training, notably without long training times, and I briefly discuss possible futures for deep generative models as an interactive paradigm for musical expression.","sentences":["I present \"SnakeSynth,\" a web-based lightweight audio synthesizer that combines audio generated by a deep generative model and real-time continuous two-dimensional (2D) input to create and control variable-length generative sounds through 2D interaction gestures.","Interaction gestures are touch and mobile-compatible with analogies to strummed, bowed, and plucked musical instrument controls.","Point-and-click and drag-and-drop gestures directly control audio playback length and I show that sound length and intensity are modulated by interactions with a programmable 2D coordinate grid.","Leveraging the speed and ubiquity of browser-based audio and hardware acceleration in Google's TensorFlow.js we generate time-varying high-fidelity sounds with real-time interactivity.","SnakeSynth adaptively reproduces and interpolates between sounds encountered during model training, notably without long training times, and I briefly discuss possible futures for deep generative models as an interactive paradigm for musical expression."],"url":"http://arxiv.org/abs/2307.05830v1"}
{"created":"2023-07-11 22:42:10","title":"Distance-Preserving Graph Compression Techniques","abstract":"We study the problem of distance-preserving graph compression for weighted paths and trees. The problem entails a weighted graph $G = (V, E)$ with non-negative weights, and a subset of edges $E^{\\prime} \\subset E$ which needs to be removed from G (with their endpoints merged as a supernode). The goal is to redistribute the weights of the deleted edges in a way that minimizes the error. The error is defined as the sum of the absolute differences of the shortest path lengths between different pairs of nodes before and after contracting $E^{\\prime}$. Based on this error function, we propose optimal approaches for merging any subset of edges in a path and a single edge in a tree. Previous works on graph compression techniques aimed at preserving different graph properties (such as the chromatic number) or solely focused on identifying the optimal set of edges to contract. However, our focus in this paper is on achieving optimal edge contraction (when the contracted edges are provided as input) specifically for weighted trees and paths.","sentences":["We study the problem of distance-preserving graph compression for weighted paths and trees.","The problem entails a weighted graph $G = (V, E)$ with non-negative weights, and a subset of edges $E^{\\prime} \\subset E$ which needs to be removed from G (with their endpoints merged as a supernode).","The goal is to redistribute the weights of the deleted edges in a way that minimizes the error.","The error is defined as the sum of the absolute differences of the shortest path lengths between different pairs of nodes before and after contracting $E^{\\prime}$. Based on this error function, we propose optimal approaches for merging any subset of edges in a path and a single edge in a tree.","Previous works on graph compression techniques aimed at preserving different graph properties (such as the chromatic number) or solely focused on identifying the optimal set of edges to contract.","However, our focus in this paper is on achieving optimal edge contraction (when the contracted edges are provided as input) specifically for weighted trees and paths."],"url":"http://arxiv.org/abs/2307.05829v1"}
{"created":"2023-07-11 22:40:58","title":"List Privacy Under Function Recoverability","abstract":"For a given function of user data, a querier must recover with at least a prescribed probability, the value of the function based on a user-provided query response. Subject to this requirement, the user forms the query response so as to minimize the likelihood of the querier guessing a list of prescribed size to which the data value belongs based on the query response. We obtain a general converse upper bound for the maximum list privacy which is shown to be tight for the special case of a binary-valued function through an explicit achievability scheme for the query response.","sentences":["For a given function of user data, a querier must recover with at least a prescribed probability, the value of the function based on a user-provided query response.","Subject to this requirement, the user forms the query response so as to minimize the likelihood of the querier guessing a list of prescribed size to which the data value belongs based on the query response.","We obtain a general converse upper bound for the maximum list privacy which is shown to be tight for the special case of a binary-valued function through an explicit achievability scheme for the query response."],"url":"http://arxiv.org/abs/2307.05828v1"}
{"created":"2023-07-11 22:36:47","title":"Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks","abstract":"Relation extraction (RE) is the task of extracting relations between entities in text. Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables. We explore RE from the perspective of applying neural methods on tabularly organized data. We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively. We evaluate our model on a large and recent dataset and compare results with previous neural methods. Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data. We perform comprehensive error analyses and ablation study to show the contribution of various components of our model. Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research.","sentences":["Relation extraction (RE) is the task of extracting relations between entities in text.","Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables.","We explore RE from the perspective of applying neural methods on tabularly organized data.","We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively.","We evaluate our model on a large and recent dataset and compare results with previous neural methods.","Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data.","We perform comprehensive error analyses and ablation study to show the contribution of various components of our model.","Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research."],"url":"http://arxiv.org/abs/2307.05827v1"}
