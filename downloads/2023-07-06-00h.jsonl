{"created":"2023-07-03 17:59:45","title":"Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning","abstract":"Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner. However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space. In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space. Such proxy data enables us to build a learning-based network with accurate full-body supervision while also mitigating the generalization issues. For more accurate and physically plausible predictions, a contact-aware neural motion descent module is proposed in our network so that it can be aware of foot-ground contact and motion misalignment with the proxy observations. Additionally, we share the body-hand context information in our network for more compatible wrist poses recovery with the full-body model. With the proposed learning-based solution, we demonstrate the first real-time monocular full-body capture system with plausible foot-ground contact in world space. More video results can be found at our project page: https://liuyebin.com/proxycap.","sentences":["Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner.","However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space.","In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space.","Such proxy data enables us to build a learning-based network with accurate full-body supervision while also mitigating the generalization issues.","For more accurate and physically plausible predictions, a contact-aware neural motion descent module is proposed in our network so that it can be aware of foot-ground contact and motion misalignment with the proxy observations.","Additionally, we share the body-hand context information in our network for more compatible wrist poses recovery with the full-body model.","With the proposed learning-based solution, we demonstrate the first real-time monocular full-body capture system with plausible foot-ground contact in world space.","More video results can be found at our project page: https://liuyebin.com/proxycap."],"url":"http://arxiv.org/abs/2307.01200v1"}
{"created":"2023-07-03 17:59:20","title":"NeuBTF: Neural fields for BTF encoding and transfer","abstract":"Neural material representations are becoming a popular way to represent materials for rendering. They are more expressive than analytic models and occupy less memory than tabulated BTFs. However, existing neural materials are immutable, meaning that their output for a certain query of UVs, camera, and light vector is fixed once they are trained. While this is practical when there is no need to edit the material, it can become very limiting when the fragment of the material used for training is too small or not tileable, which frequently happens when the material has been captured with a gonioreflectometer. In this paper, we propose a novel neural material representation which jointly tackles the problems of BTF compression, tiling, and extrapolation. At test time, our method uses a guidance image as input to condition the neural BTF to the structural features of this input image. Then, the neural BTF can be queried as a regular BTF using UVs, camera, and light vectors. Every component in our framework is purposefully designed to maximize BTF encoding quality at minimal parameter count and computational complexity, achieving competitive compression rates compared with previous work. We demonstrate the results of our method on a variety of synthetic and captured materials, showing its generality and capacity to learn to represent many optical properties.","sentences":["Neural material representations are becoming a popular way to represent materials for rendering.","They are more expressive than analytic models and occupy less memory than tabulated BTFs.","However, existing neural materials are immutable, meaning that their output for a certain query of UVs, camera, and light vector is fixed once they are trained.","While this is practical when there is no need to edit the material, it can become very limiting when the fragment of the material used for training is too small or not tileable, which frequently happens when the material has been captured with a gonioreflectometer.","In this paper, we propose a novel neural material representation which jointly tackles the problems of BTF compression, tiling, and extrapolation.","At test time, our method uses a guidance image as input to condition the neural BTF to the structural features of this input image.","Then, the neural BTF can be queried as a regular BTF using UVs, camera, and light vectors.","Every component in our framework is purposefully designed to maximize BTF encoding quality at minimal parameter count and computational complexity, achieving competitive compression rates compared with previous work.","We demonstrate the results of our method on a variety of synthetic and captured materials, showing its generality and capacity to learn to represent many optical properties."],"url":"http://arxiv.org/abs/2307.01199v1"}
{"created":"2023-07-03 17:58:26","title":"Improved sampling via learned diffusions","abstract":"Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr\\\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered approaches.","sentences":["Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes.","In this work, we identify these approaches as special cases of the Schr\\\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target.","We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes.","This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases.","At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse.","In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered approaches."],"url":"http://arxiv.org/abs/2307.01198v1"}
{"created":"2023-07-03 17:58:01","title":"Segment Anything Meets Point Tracking","abstract":"The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, employing interactive prompts such as points to generate masks. This paper presents SAM-PT, a method extending SAM's capability to tracking and segmenting anything in dynamic videos. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation, demonstrating that a SAM-based segmentation tracker can yield strong zero-shot performance across popular video object segmentation benchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information that is agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video Objects (UVO) benchmark. To further enhance our approach, we utilize K-Medoids clustering for point initialization and track both positive and negative points to clearly distinguish the target object. We also employ multiple mask decoding passes for mask refinement and devise a point re-initialization strategy to improve tracking accuracy. Our code integrates different point trackers and video segmentation benchmarks and will be released at https://github.com/SysCV/sam-pt.","sentences":["The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, employing interactive prompts such as points to generate masks.","This paper presents SAM-PT, a method extending SAM's capability to tracking and segmenting anything in dynamic videos.","SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation, demonstrating that a SAM-based segmentation tracker can yield strong zero-shot performance across popular video object segmentation benchmarks, including DAVIS, YouTube-VOS, and MOSE.","Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information that is agnostic to object semantics.","We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video Objects (UVO) benchmark.","To further enhance our approach, we utilize K-Medoids clustering for point initialization and track both positive and negative points to clearly distinguish the target object.","We also employ multiple mask decoding passes for mask refinement and devise a point re-initialization strategy to improve tracking accuracy.","Our code integrates different point trackers and video segmentation benchmarks and will be released at https://github.com/SysCV/sam-pt."],"url":"http://arxiv.org/abs/2307.01197v1"}
{"created":"2023-07-03 17:54:40","title":"Squeezing Large-Scale Diffusion Models for Mobile","abstract":"The emergence of diffusion models has greatly broadened the scope of high-fidelity image synthesis, resulting in notable advancements in both practical implementation and academic research. With the active adoption of the model in various real-world applications, the need for on-device deployment has grown considerably. However, deploying large diffusion models such as Stable Diffusion with more than one billion parameters to mobile devices poses distinctive challenges due to the limited computational and memory resources, which may vary according to the device. In this paper, we present the challenges and solutions for deploying Stable Diffusion on mobile devices with TensorFlow Lite framework, which supports both iOS and Android devices. The resulting Mobile Stable Diffusion achieves the inference latency of smaller than 7 seconds for a 512x512 image generation on Android devices with mobile GPUs.","sentences":["The emergence of diffusion models has greatly broadened the scope of high-fidelity image synthesis, resulting in notable advancements in both practical implementation and academic research.","With the active adoption of the model in various real-world applications, the need for on-device deployment has grown considerably.","However, deploying large diffusion models such as Stable Diffusion with more than one billion parameters to mobile devices poses distinctive challenges due to the limited computational and memory resources, which may vary according to the device.","In this paper, we present the challenges and solutions for deploying Stable Diffusion on mobile devices with TensorFlow Lite framework, which supports both iOS and Android devices.","The resulting Mobile Stable Diffusion achieves the inference latency of smaller than 7 seconds for a 512x512 image generation on Android devices with mobile GPUs."],"url":"http://arxiv.org/abs/2307.01193v1"}
{"created":"2023-07-03 17:53:39","title":"Trainable Transformer in Transformer","abstract":"Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.","sentences":["Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference.","However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable.","In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models).","In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass.","TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers.","We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks.","For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines.","To facilitate further work, a modular and extensible codebase for TinT is included."],"url":"http://arxiv.org/abs/2307.01189v1"}
{"created":"2023-07-03 17:52:44","title":"SAMAug: Point Prompt Augmentation for Segment Anything Model","abstract":"This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information to SAM. From the initial point prompt, SAM produces the initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We evaluate four point augmentation techniques: random selection, maximum difference entropy, maximum distance, and a saliency model. Experiments on the COCO, Fundus, and Chest X-ray datasets demonstrate that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency model methods. SAMAug underscores the potential of visual prompt engineering to advance interactive computer vision models.","sentences":["This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance.","SAMAug generates augmented point prompts to provide more information to SAM.","From the initial point prompt, SAM produces the initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts.","By incorporating these extra points, SAM can generate augmented segmentation masks based on the augmented point prompts and the initial prompt, resulting in improved segmentation performance.","We evaluate four point augmentation techniques: random selection, maximum difference entropy, maximum distance, and a saliency model.","Experiments on the COCO, Fundus, and Chest X-ray datasets demonstrate that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency model methods.","SAMAug underscores the potential of visual prompt engineering to advance interactive computer vision models."],"url":"http://arxiv.org/abs/2307.01187v1"}
{"created":"2023-07-03 17:45:01","title":"PlanE: Representation Learning over Planar Graphs","abstract":"Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose PlanE as a framework for planar representation learning. PlanE includes architectures which can learn complete invariants over planar graphs while remaining practically scalable. We empirically validate the strong performance of the resulting model architectures on well-known planar graph benchmarks, achieving multiple state-of-the-art results.","sentences":["Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants.","On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks.","This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs.","The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs.","Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose PlanE as a framework for planar representation learning.","PlanE includes architectures which can learn complete invariants over planar graphs while remaining practically scalable.","We empirically validate the strong performance of the resulting model architectures on well-known planar graph benchmarks, achieving multiple state-of-the-art results."],"url":"http://arxiv.org/abs/2307.01180v1"}
{"created":"2023-07-03 17:44:22","title":"Learning Mixtures of Gaussians Using the DDPM Objective","abstract":"Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.   In this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings: 1) We show gradient descent with random initialization learns mixtures of two spherical Gaussians in $d$ dimensions with $1/\\text{poly}(d)$-separated centers. 2) We show gradient descent with a warm start learns mixtures of $K$ spherical Gaussians with $\\Omega(\\sqrt{\\log(\\min(K,d))})$-separated centers. A key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, the EM algorithm and spectral methods.","sentences":["Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation.","Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.   ","In this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models.","We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings: 1) We show gradient descent with random initialization learns mixtures of two spherical Gaussians in $d$ dimensions with $1/\\text{poly}(d)$-separated centers.","2) We show gradient descent with a warm start learns mixtures of $K$ spherical Gaussians with $\\Omega(\\sqrt{\\log(\\min(K,d))})$-separated centers.","A key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, the EM algorithm and spectral methods."],"url":"http://arxiv.org/abs/2307.01178v1"}
{"created":"2023-07-03 17:40:58","title":"Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space","abstract":"The characterization of the functions spaces explored by neural networks (NNs) is an important aspect of deep learning theory. In this work, we view a multi-layer NN with arbitrary width as defining a particular hierarchy of reproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL). This allows us to define a function space and a complexity measure that generalize prior results for shallow NNs, and we then examine their theoretical properties and implications in several aspects. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with the complexity measure controlled. Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized as the dynamics of multiple random fields. Fourth, we show examples of depth separation in NHLs under ReLU and quadratic activation functions. Finally, we complement the theory with numerical results to illustrate the learning of RKHS in NN training.","sentences":["The characterization of the functions spaces explored by neural networks (NNs) is an important aspect of deep learning theory.","In this work, we view a multi-layer NN with arbitrary width as defining a particular hierarchy of reproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL).","This allows us to define a function space and a complexity measure that generalize prior results for shallow NNs, and we then examine their theoretical properties and implications in several aspects.","First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs.","Second, we prove generalization guarantees for learning an NHL with the complexity measure controlled.","Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized as the dynamics of multiple random fields.","Fourth, we show examples of depth separation in NHLs under ReLU and quadratic activation functions.","Finally, we complement the theory with numerical results to illustrate the learning of RKHS in NN training."],"url":"http://arxiv.org/abs/2307.01177v1"}
{"created":"2023-07-03 17:39:02","title":"Patient-centric health data sovereignty: an approach using Proxy re-encryption","abstract":"The exponential growth in the digitisation of services implies the handling and storage of large volumes of data. Businesses and services see data sharing and crossing as an opportunity to improve and produce new business opportunities. The health sector is one area where this proves to be true, enabling better and more innovative treatments. Notwithstanding, this raises concerns regarding personal data being treated and processed. In this paper, we present a patient-centric platform for the secure sharing of health records by shifting the control over the data to the patient, therefore, providing a step further towards data sovereignty. Data sharing is performed only with the consent of the patient, allowing it to revoke access at any given time. Furthermore, we also provide a break-glass approach, resorting to Proxy Re-encryption (PRE) and the concept of a centralised trusted entity that possesses instant access to patients' medical records. Lastly, an analysis is made to assess the performance of the platform's key operations, and the impact that a PRE scheme has on those operations.","sentences":["The exponential growth in the digitisation of services implies the handling and storage of large volumes of data.","Businesses and services see data sharing and crossing as an opportunity to improve and produce new business opportunities.","The health sector is one area where this proves to be true, enabling better and more innovative treatments.","Notwithstanding, this raises concerns regarding personal data being treated and processed.","In this paper, we present a patient-centric platform for the secure sharing of health records by shifting the control over the data to the patient, therefore, providing a step further towards data sovereignty.","Data sharing is performed only with the consent of the patient, allowing it to revoke access at any given time.","Furthermore, we also provide a break-glass approach, resorting to Proxy Re-encryption (PRE) and the concept of a centralised trusted entity that possesses instant access to patients' medical records.","Lastly, an analysis is made to assess the performance of the platform's key operations, and the impact that a PRE scheme has on those operations."],"url":"http://arxiv.org/abs/2307.01175v1"}
{"created":"2023-07-03 17:38:38","title":"Anonymous and Copy-Robust Delegations for Liquid Democracy","abstract":"Liquid democracy with ranked delegations is a novel voting scheme that unites the practicability of representative democracy with the idealistic appeal of direct democracy: Every voter decides between casting their vote on a question at hand or delegating their voting weight to some other, trusted agent. Delegations are transitive, and since voters may end up in a delegation cycle, they are encouraged to indicate not only a single delegate, but a set of potential delegates and a ranking among them. Based on the delegation preferences of all voters, a delegation rule selects one representative per voter. Previous work has revealed a trade-off between two properties of delegation rules called anonymity and copy-robustness.   To overcome this issue we study two fractional delegation rules: Mixed Borda branching, which generalizes a rule satisfying copy-robustness, and the random walk rule, which satisfies anonymity. Using the Markov chain tree theorem, we show that the two rules are in fact equivalent, and simultaneously satisfy generalized versions of the two properties. Combining the same theorem with Fulkerson's algorithm, we develop a polynomial-time algorithm for computing the outcome of the studied delegation rule. This algorithm is of independent interest, having applications in semi-supervised learning and graph theory.","sentences":["Liquid democracy with ranked delegations is a novel voting scheme that unites the practicability of representative democracy with the idealistic appeal of direct democracy: Every voter decides between casting their vote on a question at hand or delegating their voting weight to some other, trusted agent.","Delegations are transitive, and since voters may end up in a delegation cycle, they are encouraged to indicate not only a single delegate, but a set of potential delegates and a ranking among them.","Based on the delegation preferences of all voters, a delegation rule selects one representative per voter.","Previous work has revealed a trade-off between two properties of delegation rules called anonymity and copy-robustness.   ","To overcome this issue we study two fractional delegation rules: Mixed Borda branching, which generalizes a rule satisfying copy-robustness, and the random walk rule, which satisfies anonymity.","Using the Markov chain tree theorem, we show that the two rules are in fact equivalent, and simultaneously satisfy generalized versions of the two properties.","Combining the same theorem with Fulkerson's algorithm, we develop a polynomial-time algorithm for computing the outcome of the studied delegation rule.","This algorithm is of independent interest, having applications in semi-supervised learning and graph theory."],"url":"http://arxiv.org/abs/2307.01174v1"}
{"created":"2023-07-03 17:29:58","title":"Online nearest neighbor classification","abstract":"We study an instance of online non-parametric classification in the realizable setting. In particular, we consider the classical 1-nearest neighbor algorithm, and show that it achieves sublinear regret - that is, a vanishing mistake rate - against dominated or smoothed adversaries in the realizable setting.","sentences":["We study an instance of online non-parametric classification in the realizable setting.","In particular, we consider the classical 1-nearest neighbor algorithm, and show that it achieves sublinear regret - that is, a vanishing mistake rate - against dominated or smoothed adversaries in the realizable setting."],"url":"http://arxiv.org/abs/2307.01170v1"}
{"created":"2023-07-03 17:23:34","title":"Don't freeze: Finetune encoders for better Self-Supervised HAR","abstract":"Recently self-supervised learning has been proposed in the field of human activity recognition as a solution to the labelled data availability problem. The idea being that by using pretext tasks such as reconstruction or contrastive predictive coding, useful representations can be learned that then can be used for classification. Those approaches follow the pretrain, freeze and fine-tune procedure. In this paper we will show how a simple change - not freezing the representation - leads to substantial performance gains across pretext tasks. The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data. Moreover the effect is present whether the pretext task is carried on the Capture24 dataset or directly in unlabelled data of the target dataset.","sentences":["Recently self-supervised learning has been proposed in the field of human activity recognition as a solution to the labelled data availability problem.","The idea being that by using pretext tasks such as reconstruction or contrastive predictive coding, useful representations can be learned that then can be used for classification.","Those approaches follow the pretrain, freeze and fine-tune procedure.","In this paper we will show how a simple change - not freezing the representation - leads to substantial performance gains across pretext tasks.","The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data.","Moreover the effect is present whether the pretext task is carried on the Capture24 dataset or directly in unlabelled data of the target dataset."],"url":"http://arxiv.org/abs/2307.01168v1"}
{"created":"2023-07-03 17:18:50","title":"Coupled Gradient Flows for Strategic Non-Local Distribution Shift","abstract":"We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-state, both in finite and in infinite dimensions, obtaining explicit rates in terms of the model parameters. To do so we derive new results on the convergence of coupled PDEs that extends what is known on multi-species systems. Empirically, we show that our approach captures well-documented forms of distribution shifts like polarization and disparate impacts that simpler models cannot capture.","sentences":["We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed.","Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure.","In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift.","We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users.","For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-state, both in finite and in infinite dimensions, obtaining explicit rates in terms of the model parameters.","To do so we derive new results on the convergence of coupled PDEs that extends what is known on multi-species systems.","Empirically, we show that our approach captures well-documented forms of distribution shifts like polarization and disparate impacts that simpler models cannot capture."],"url":"http://arxiv.org/abs/2307.01166v1"}
{"created":"2023-07-03 17:12:44","title":"Improving Language Plasticity via Pretraining with Active Forgetting","abstract":"Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, particularly for languages that are distant from English.","sentences":["Pretrained language models (PLMs) are today the primary model for natural language processing.","Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible.","While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient.","We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages.","Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect.","Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, particularly for languages that are distant from English."],"url":"http://arxiv.org/abs/2307.01163v1"}
{"created":"2023-07-03 17:10:19","title":"Soft Gripping: Specifying for Trustworthiness","abstract":"Soft robotics is an emerging technology in which engineers create flexible devices for use in a variety of applications. In order to advance the wide adoption of soft robots, ensuring their trustworthiness is essential; if soft robots are not trusted, they will not be used to their full potential. In order to demonstrate trustworthiness, a specification needs to be formulated to define what is trustworthy. However, even for soft robotic grippers, which is one of the most mature areas in soft robotics, the soft robotics community has so far given very little attention to formulating specifications. In this work, we discuss the importance of developing specifications during development of soft robotic systems, and present an extensive example specification for a soft gripper for pick-and-place tasks for grocery items. The proposed specification covers both functional and non-functional requirements, such as reliability, safety, adaptability, predictability, ethics, and regulations. We also highlight the need to promote verifiability as a first-class objective in the design of a soft gripper.","sentences":["Soft robotics is an emerging technology in which engineers create flexible devices for use in a variety of applications.","In order to advance the wide adoption of soft robots, ensuring their trustworthiness is essential; if soft robots are not trusted, they will not be used to their full potential.","In order to demonstrate trustworthiness, a specification needs to be formulated to define what is trustworthy.","However, even for soft robotic grippers, which is one of the most mature areas in soft robotics, the soft robotics community has so far given very little attention to formulating specifications.","In this work, we discuss the importance of developing specifications during development of soft robotic systems, and present an extensive example specification for a soft gripper for pick-and-place tasks for grocery items.","The proposed specification covers both functional and non-functional requirements, such as reliability, safety, adaptability, predictability, ethics, and regulations.","We also highlight the need to promote verifiability as a first-class objective in the design of a soft gripper."],"url":"http://arxiv.org/abs/2307.01159v1"}
{"created":"2023-07-03 17:07:18","title":"Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning","abstract":"The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.","sentences":["The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings.","We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks.","We then consider the task of 2nd-order belief prediction.","We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning.","Finally, we present preliminary empirical results in a mixed cooperative-competitive environment."],"url":"http://arxiv.org/abs/2307.01158v1"}
{"created":"2023-07-03 17:05:29","title":"A novel approach for predicting epidemiological forecasting parameters based on real-time signals and Data Assimilation","abstract":"This paper proposes a novel approach to predict epidemiological parameters by integrating new real-time signals from various sources of information, such as novel social media-based population density maps and Air Quality data. We implement an ensemble of Convolutional Neural Networks (CNN) models using various data sources and fusion methodology to build robust predictions and simulate several dynamic parameters that could improve the decision-making process for policymakers. Additionally, we used data assimilation to estimate the state of our system from fused CNN predictions. The combination of meteorological signals and social media-based population density maps improved the performance and flexibility of our prediction of the COVID-19 outbreak in London. While the proposed approach outperforms standard models, such as compartmental models traditionally used in disease forecasting (SEIR), generating robust and consistent predictions allows us to increase the stability of our model while increasing its accuracy.","sentences":["This paper proposes a novel approach to predict epidemiological parameters by integrating new real-time signals from various sources of information, such as novel social media-based population density maps and Air Quality data.","We implement an ensemble of Convolutional Neural Networks (CNN) models using various data sources and fusion methodology to build robust predictions and simulate several dynamic parameters that could improve the decision-making process for policymakers.","Additionally, we used data assimilation to estimate the state of our system from fused CNN predictions.","The combination of meteorological signals and social media-based population density maps improved the performance and flexibility of our prediction of the COVID-19 outbreak in London.","While the proposed approach outperforms standard models, such as compartmental models traditionally used in disease forecasting (SEIR), generating robust and consistent predictions allows us to increase the stability of our model while increasing its accuracy."],"url":"http://arxiv.org/abs/2307.01157v1"}
{"created":"2023-07-03 16:39:28","title":"Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis","abstract":"Generative latent diffusion models have been established as state-of-the-art in data generation. One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy. Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored. Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets. To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning. Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization.","sentences":["Generative latent diffusion models have been established as state-of-the-art in data generation.","One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy.","Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored.","Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets.","To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning.","Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization."],"url":"http://arxiv.org/abs/2307.01148v1"}
{"created":"2023-07-03 16:37:10","title":"AVSegFormer: Audio-Visual Segmentation with Transformer","abstract":"The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate that AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is available at https://github.com/vvvb-github/AVSegFormer.","sentences":["The combination of audio and vision has long been a topic of interest in the multi-modal community.","Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video.","This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges.","In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture.","Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features.","Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels.","Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions.","Extensive experiments demonstrate that AVSegFormer achieves state-of-the-art results on the AVS benchmark.","The code is available at https://github.com/vvvb-github/AVSegFormer."],"url":"http://arxiv.org/abs/2307.01146v1"}
{"created":"2023-07-03 16:32:46","title":"Prompt Middleware: Mapping Prompts for Large Language Models to UI Affordances","abstract":"To help users do complex work, researchers have developed techniques to integrate AI and human intelligence into user interfaces (UIs). With the recent introduction of large language models (LLMs), which can generate text in response to a natural language prompt, there are new opportunities to consider how to integrate LLMs into UIs. We present Prompt Middleware, a framework for generating prompts for LLMs based on UI affordances. These include prompts that are predefined by experts (static prompts), generated from templates with fill-in options in the UI (template-based prompts), or created from scratch (free-form prompts). We demonstrate this framework with FeedbackBuffet, a writing assistant that automatically generates feedback based on a user's text input. Inspired by prior research showing how templates can help non-experts perform more like experts, FeedbackBuffet leverages template-based prompt middleware to enable feedback seekers to specify the types of feedback they want to receive as options in a UI. These options are composed using a template to form a feedback request prompt to GPT-3. We conclude with a discussion about how Prompt Middleware can help developers integrate LLMs into UIs.","sentences":["To help users do complex work, researchers have developed techniques to integrate AI and human intelligence into user interfaces (UIs).","With the recent introduction of large language models (LLMs), which can generate text in response to a natural language prompt, there are new opportunities to consider how to integrate LLMs into UIs.","We present Prompt Middleware, a framework for generating prompts for LLMs based on UI affordances.","These include prompts that are predefined by experts (static prompts), generated from templates with fill-in options in the UI (template-based prompts), or created from scratch (free-form prompts).","We demonstrate this framework with FeedbackBuffet, a writing assistant that automatically generates feedback based on a user's text input.","Inspired by prior research showing how templates can help non-experts perform more like experts, FeedbackBuffet leverages template-based prompt middleware to enable feedback seekers to specify the types of feedback they want to receive as options in a UI.","These options are composed using a template to form a feedback request prompt to GPT-3.","We conclude with a discussion about how Prompt Middleware can help developers integrate LLMs into UIs."],"url":"http://arxiv.org/abs/2307.01142v1"}
{"created":"2023-07-03 16:25:49","title":"SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions","abstract":"Instruction finetuning is a popular paradigm to align large language models (LLM) with human intent. Despite its popularity, this idea is less explored in improving the LLMs to align existing foundation models with scientific disciplines, concepts and goals. In this work, we present SciTune as a tuning framework to improve the ability of LLMs to follow scientific multimodal instructions. To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding. In comparison to the models that are finetuned with machine generated data only, LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark.","sentences":["Instruction finetuning is a popular paradigm to align large language models (LLM) with human intent.","Despite its popularity, this idea is less explored in improving the LLMs to align existing foundation models with scientific disciplines, concepts and goals.","In this work, we present SciTune as a tuning framework to improve the ability of LLMs to follow scientific multimodal instructions.","To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding.","In comparison to the models that are finetuned with machine generated data only, LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark."],"url":"http://arxiv.org/abs/2307.01139v1"}
{"created":"2023-07-03 16:19:50","title":"Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking","abstract":"The biomedical field relies heavily on concept linking in various areas such as literature mining, graph alignment, information retrieval, question-answering, data, and knowledge integration. Although large language models (LLMs) have made significant strides in many natural language processing tasks, their effectiveness in biomedical concept mapping is yet to be fully explored. This research investigates a method that exploits the in-context learning (ICL) capabilities of large models for biomedical concept linking. The proposed approach adopts a two-stage retrieve-and-rank framework. Initially, biomedical concepts are embedded using language models, and then embedding similarity is utilized to retrieve the top candidates. These candidates' contextual information is subsequently incorporated into the prompt and processed by a large language model to re-rank the concepts. This approach achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7% in chemical entity normalization, exhibiting a competitive performance relative to supervised learning methods. Further, it showed a significant improvement, with an over 20-point absolute increase in F1 score on an oncology matching dataset. Extensive qualitative assessments were conducted, and the benefits and potential shortcomings of using large language models within the biomedical domain were discussed. were discussed.","sentences":["The biomedical field relies heavily on concept linking in various areas such as literature mining, graph alignment, information retrieval, question-answering, data, and knowledge integration.","Although large language models (LLMs) have made significant strides in many natural language processing tasks, their effectiveness in biomedical concept mapping is yet to be fully explored.","This research investigates a method that exploits the in-context learning (ICL) capabilities of large models for biomedical concept linking.","The proposed approach adopts a two-stage retrieve-and-rank framework.","Initially, biomedical concepts are embedded using language models, and then embedding similarity is utilized to retrieve the top candidates.","These candidates' contextual information is subsequently incorporated into the prompt and processed by a large language model to re-rank the concepts.","This approach achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7% in chemical entity normalization, exhibiting a competitive performance relative to supervised learning methods.","Further, it showed a significant improvement, with an over 20-point absolute increase in F1 score on an oncology matching dataset.","Extensive qualitative assessments were conducted, and the benefits and potential shortcomings of using large language models within the biomedical domain were discussed.","were discussed."],"url":"http://arxiv.org/abs/2307.01137v1"}
{"created":"2023-07-03 16:15:34","title":"ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience","abstract":"The advent of ChatGPT, a large language model-powered chatbot, has prompted questions about its potential implications for traditional search engines. In this study, we investigate the differences in user behavior when employing search engines and chatbot tools for information-seeking tasks. We carry out a randomized online experiment, dividing participants into two groups: one using a ChatGPT-like tool and the other using a Google Search-like tool. Our findings reveal that the ChatGPT group consistently spends less time on all tasks, with no significant difference in overall task performance between the groups. Notably, ChatGPT levels user search performance across different education levels and excels in answering straightforward questions and providing general solutions but falls short in fact-checking tasks. Users perceive ChatGPT's responses as having higher information quality compared to Google Search, despite displaying a similar level of trust in both tools. Furthermore, participants using ChatGPT report significantly better user experiences in terms of usefulness, enjoyment, and satisfaction, while perceived ease of use remains comparable between the two tools. However, ChatGPT may also lead to overreliance and generate or replicate misinformation, yielding inconsistent results. Our study offers valuable insights for search engine management and highlights opportunities for integrating chatbot technologies into search engine designs.","sentences":["The advent of ChatGPT, a large language model-powered chatbot, has prompted questions about its potential implications for traditional search engines.","In this study, we investigate the differences in user behavior when employing search engines and chatbot tools for information-seeking tasks.","We carry out a randomized online experiment, dividing participants into two groups: one using a ChatGPT-like tool and the other using a Google Search-like tool.","Our findings reveal that the ChatGPT group consistently spends less time on all tasks, with no significant difference in overall task performance between the groups.","Notably, ChatGPT levels user search performance across different education levels and excels in answering straightforward questions and providing general solutions but falls short in fact-checking tasks.","Users perceive ChatGPT's responses as having higher information quality compared to Google Search, despite displaying a similar level of trust in both tools.","Furthermore, participants using ChatGPT report significantly better user experiences in terms of usefulness, enjoyment, and satisfaction, while perceived ease of use remains comparable between the two tools.","However, ChatGPT may also lead to overreliance and generate or replicate misinformation, yielding inconsistent results.","Our study offers valuable insights for search engine management and highlights opportunities for integrating chatbot technologies into search engine designs."],"url":"http://arxiv.org/abs/2307.01135v1"}
{"created":"2023-07-03 16:10:42","title":"Passive Query-Recovery Attack Against Secure Conjunctive Keyword Search Schemes","abstract":"While storing documents on the cloud can be attractive, the question remains whether cloud providers can be trusted with storing private documents. Even if trusted, data breaches are ubiquitous. To prevent information leakage one can store documents encrypted. If encrypted under traditional schemes, one loses the ability to perform simple operations over the documents, such as searching through them. Searchable encryption schemes were proposed allowing some search functionality while documents remain encrypted. Orthogonally, research is done to find attacks that exploit search and access pattern leakage that most efficient schemes have. One type of such an attack is the ability to recover plaintext queries. Passive query-recovery attacks on single-keyword search schemes have been proposed in literature, however, conjunctive keyword search has not been considered, although keyword searches with two or three keywords appear more frequently in online searches.   We introduce a generic extension strategy for existing passive query-recovery attacks against single-keyword search schemes and explore its applicability for the attack presented by Damie et al. (USENIX Security '21). While the original attack achieves up to a recovery rate of 85% against single-keyword search schemes for an attacker without exact background knowledge, our experiments show that the generic extension to conjunctive queries comes with a significant performance decrease achieving recovery rates of at most 32%. Assuming a stronger attacker with partial knowledge of the indexed document set boosts the recovery rate to 85% for conjunctive keyword queries with two keywords and achieves similar recovery rates as previous attacks by Cash et al. (CCS '15) and Islam et al. (NDSS '12) in the same setting for single-keyword search schemes.","sentences":["While storing documents on the cloud can be attractive, the question remains whether cloud providers can be trusted with storing private documents.","Even if trusted, data breaches are ubiquitous.","To prevent information leakage one can store documents encrypted.","If encrypted under traditional schemes, one loses the ability to perform simple operations over the documents, such as searching through them.","Searchable encryption schemes were proposed allowing some search functionality while documents remain encrypted.","Orthogonally, research is done to find attacks that exploit search and access pattern leakage that most efficient schemes have.","One type of such an attack is the ability to recover plaintext queries.","Passive query-recovery attacks on single-keyword search schemes have been proposed in literature, however, conjunctive keyword search has not been considered, although keyword searches with two or three keywords appear more frequently in online searches.   ","We introduce a generic extension strategy for existing passive query-recovery attacks against single-keyword search schemes and explore its applicability for the attack presented by Damie et al.","(USENIX Security '21).","While the original attack achieves up to a recovery rate of 85% against single-keyword search schemes for an attacker without exact background knowledge, our experiments show that the generic extension to conjunctive queries comes with a significant performance decrease achieving recovery rates of at most 32%.","Assuming a stronger attacker with partial knowledge of the indexed document set boosts the recovery rate to 85% for conjunctive keyword queries with two keywords and achieves similar recovery rates as previous attacks by Cash et al.","(CCS '15) and Islam et al.","(NDSS '12) in the same setting for single-keyword search schemes."],"url":"http://arxiv.org/abs/2307.01131v1"}
{"created":"2023-07-03 16:01:45","title":"Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction","abstract":"In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for \"guiding\" the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.","sentences":["In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios.","In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure.","However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields.","This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building.","The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process.","Our unique manifold approach may encompass significant benefits to the scientific community.","In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for \"guiding\" the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise.","To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain.","We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts."],"url":"http://arxiv.org/abs/2307.01128v1"}
{"created":"2023-07-03 15:51:39","title":"Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization","abstract":"Geometric navigation is nowadays a well-established field of robotics and the research focus is shifting towards higher-level scene understanding, such as Semantic Mapping. When a robot needs to interact with its environment, it must be able to comprehend the contextual information of its surroundings. This work focuses on classifying and localising objects within a map, which is under construction (SLAM) or already built. To further explore this direction, we propose a framework that can autonomously detect and localize predefined objects in a known environment using a multi-modal sensor fusion approach (combining RGB and depth data from an RGB-D camera and a lidar). The framework consists of three key elements: understanding the environment through RGB data, estimating depth through multi-modal sensor fusion, and managing artifacts (i.e., filtering and stabilizing measurements). The experiments show that the proposed framework can accurately detect 98% of the objects in the real sample environment, without post-processing, while 85% and 80% of the objects were mapped using the single RGBD camera or RGB + lidar setup respectively. The comparison with single-sensor (camera or lidar) experiments is performed to show that sensor fusion allows the robot to accurately detect near and far obstacles, which would have been noisy or imprecise in a purely visual or laser-based approach.","sentences":["Geometric navigation is nowadays a well-established field of robotics and the research focus is shifting towards higher-level scene understanding, such as Semantic Mapping.","When a robot needs to interact with its environment, it must be able to comprehend the contextual information of its surroundings.","This work focuses on classifying and localising objects within a map, which is under construction (SLAM) or already built.","To further explore this direction, we propose a framework that can autonomously detect and localize predefined objects in a known environment using a multi-modal sensor fusion approach (combining RGB and depth data from an RGB-D camera and a lidar).","The framework consists of three key elements: understanding the environment through RGB data, estimating depth through multi-modal sensor fusion, and managing artifacts (i.e., filtering and stabilizing measurements).","The experiments show that the proposed framework can accurately detect 98% of the objects in the real sample environment, without post-processing, while 85% and 80% of the objects were mapped using the single RGBD camera or RGB + lidar setup respectively.","The comparison with single-sensor (camera or lidar) experiments is performed to show that sensor fusion allows the robot to accurately detect near and far obstacles, which would have been noisy or imprecise in a purely visual or laser-based approach."],"url":"http://arxiv.org/abs/2307.01121v1"}
{"created":"2023-07-03 15:49:15","title":"musif: a Python package for symbolic music feature extraction","abstract":"In this work, we introduce musif, a Python package that facilitates the automatic extraction of features from symbolic music scores. The package includes the implementation of a large number of features, which have been developed by a team of experts in musicology, music theory, statistics, and computer science. Additionally, the package allows for the easy creation of custom features using commonly available Python libraries. musif is primarily geared towards processing high-quality musicological data encoded in MusicXML format, but also supports other formats commonly used in music information retrieval tasks, including MIDI, MEI, Kern, and others. We provide comprehensive documentation and tutorials to aid in the extension of the framework and to facilitate the introduction of new and inexperienced users to its usage.","sentences":["In this work, we introduce musif, a Python package that facilitates the automatic extraction of features from symbolic music scores.","The package includes the implementation of a large number of features, which have been developed by a team of experts in musicology, music theory, statistics, and computer science.","Additionally, the package allows for the easy creation of custom features using commonly available Python libraries.","musif is primarily geared towards processing high-quality musicological data encoded in MusicXML format, but also supports other formats commonly used in music information retrieval tasks, including MIDI, MEI, Kern, and others.","We provide comprehensive documentation and tutorials to aid in the extension of the framework and to facilitate the introduction of new and inexperienced users to its usage."],"url":"http://arxiv.org/abs/2307.01120v1"}
{"created":"2023-07-03 15:45:14","title":"MeT: A Graph Transformer for Semantic Segmentation of 3D Meshes","abstract":"Polygonal meshes have become the standard for discretely approximating 3D shapes, thanks to their efficiency and high flexibility in capturing non-uniform shapes. This non-uniformity, however, leads to irregularity in the mesh structure, making tasks like segmentation of 3D meshes particularly challenging. Semantic segmentation of 3D mesh has been typically addressed through CNN-based approaches, leading to good accuracy. Recently, transformers have gained enough momentum both in NLP and computer vision fields, achieving performance at least on par with CNN models, supporting the long-sought architecture universalism. Following this trend, we propose a transformer-based method for semantic segmentation of 3D mesh motivated by a better modeling of the graph structure of meshes, by means of global attention mechanisms. In order to address the limitations of standard transformer architectures in modeling relative positions of non-sequential data, as in the case of 3D meshes, as well as in capturing the local context, we perform positional encoding by means the Laplacian eigenvectors of the adjacency matrix, replacing the traditional sinusoidal positional encodings, and by introducing clustering-based features into the self-attention and cross-attention operators. Experimental results, carried out on three sets of the Shape COSEG Dataset, on the human segmentation dataset proposed in Maron et al., 2017 and on the ShapeNet benchmark, show how the proposed approach yields state-of-the-art performance on semantic segmentation of 3D meshes.","sentences":["Polygonal meshes have become the standard for discretely approximating 3D shapes, thanks to their efficiency and high flexibility in capturing non-uniform shapes.","This non-uniformity, however, leads to irregularity in the mesh structure, making tasks like segmentation of 3D meshes particularly challenging.","Semantic segmentation of 3D mesh has been typically addressed through CNN-based approaches, leading to good accuracy.","Recently, transformers have gained enough momentum both in NLP and computer vision fields, achieving performance at least on par with CNN models, supporting the long-sought architecture universalism.","Following this trend, we propose a transformer-based method for semantic segmentation of 3D mesh motivated by a better modeling of the graph structure of meshes, by means of global attention mechanisms.","In order to address the limitations of standard transformer architectures in modeling relative positions of non-sequential data, as in the case of 3D meshes, as well as in capturing the local context, we perform positional encoding by means the Laplacian eigenvectors of the adjacency matrix, replacing the traditional sinusoidal positional encodings, and by introducing clustering-based features into the self-attention and cross-attention operators.","Experimental results, carried out on three sets of the Shape COSEG Dataset, on the human segmentation dataset proposed in Maron et al., 2017 and on the ShapeNet benchmark, show how the proposed approach yields state-of-the-art performance on semantic segmentation of 3D meshes."],"url":"http://arxiv.org/abs/2307.01115v1"}
{"created":"2023-07-03 15:35:42","title":"Complexity Dichotomies for the Maximum Weighted Digraph Partition Problem","abstract":"We introduce and study a new optimization problem on digraphs, termed Maximum Weighted Digraph Partition (MWDP) problem. We prove three complexity dichotomies for MWDP: on arbitrary digraphs, on oriented digraphs, and on symmetric digraphs. We demonstrate applications of the dichotomies for binary-action polymatrix games and several graph theory problems.","sentences":["We introduce and study a new optimization problem on digraphs, termed Maximum Weighted Digraph Partition (MWDP) problem.","We prove three complexity dichotomies for MWDP: on arbitrary digraphs, on oriented digraphs, and on symmetric digraphs.","We demonstrate applications of the dichotomies for binary-action polymatrix games and several graph theory problems."],"url":"http://arxiv.org/abs/2307.01109v1"}
{"created":"2023-07-03 15:32:55","title":"A phase field-based framework for electro-chemo-mechanical fracture: crack-contained electrolytes, chemical reactions and stabilisation","abstract":"We present a new theoretical and computational framework for modelling electro-chemo-mechanical fracture. The model combines a phase field description of fracture with a fully coupled characterisation of electrolyte behaviour, surface chemical reactions and stress-assisted diffusion. Importantly, a new physics-based formulation is presented to describe electrolyte-containing phase field cracks, appropriately capturing the sensitivity of electrochemical transport and reaction kinetics to the crack opening height. Unlike other existing methods, this approach is shown to accurately capture the results obtained with discrete fracture simulations. The potential of the electro-chemo-mechanical model presented is demonstrated by particularising it to the analysis of hydrogen embrittlement in metallic samples exposed to aqueous electrolytes. The finite element implementation takes as nodal degrees-of-freedom the electrolyte potential, the concentrations of relevant ionic species, the surface coverage, the concentration of diluted species, the displacement field and the phase field order parameter. Particular attention is devoted to improve stability and efficiency, resulting in the development of strategies for avoiding ill-constrained degrees of freedom and lumped integration schemes that eliminate numerical oscillations. The numerical experiments conducted showcase the ability of the model to deliver assumptions-free predictions for systems involving both free-flowing and crack-contained electrolytes. The results obtained highlight the role of electrolyte behaviour in driving the cracking process, evidencing the limitations of existing models.","sentences":["We present a new theoretical and computational framework for modelling electro-chemo-mechanical fracture.","The model combines a phase field description of fracture with a fully coupled characterisation of electrolyte behaviour, surface chemical reactions and stress-assisted diffusion.","Importantly, a new physics-based formulation is presented to describe electrolyte-containing phase field cracks, appropriately capturing the sensitivity of electrochemical transport and reaction kinetics to the crack opening height.","Unlike other existing methods, this approach is shown to accurately capture the results obtained with discrete fracture simulations.","The potential of the electro-chemo-mechanical model presented is demonstrated by particularising it to the analysis of hydrogen embrittlement in metallic samples exposed to aqueous electrolytes.","The finite element implementation takes as nodal degrees-of-freedom the electrolyte potential, the concentrations of relevant ionic species, the surface coverage, the concentration of diluted species, the displacement field and the phase field order parameter.","Particular attention is devoted to improve stability and efficiency, resulting in the development of strategies for avoiding ill-constrained degrees of freedom and lumped integration schemes that eliminate numerical oscillations.","The numerical experiments conducted showcase the ability of the model to deliver assumptions-free predictions for systems involving both free-flowing and crack-contained electrolytes.","The results obtained highlight the role of electrolyte behaviour in driving the cracking process, evidencing the limitations of existing models."],"url":"http://arxiv.org/abs/2307.01105v1"}
{"created":"2023-07-03 15:26:08","title":"Assessment of the Utilization of Quadruped Robots in Pharmaceutical Research and Development Laboratories","abstract":"Drug development is becoming more and more complex and resource-intensive. To reduce the costs and the time-to-market, the pharmaceutical industry employs cutting-edge automation solutions. Supportive robotics technologies, such as stationary and mobile manipulators, exist in various laboratory settings. However, they still lack the mobility and dexterity to navigate and operate in human-centered environments. We evaluate the feasibility of quadruped robots for the specific use case of remote inspection, utilizing the out-of-the-box capabilities of Boston Dynamics' Spot platform. We also provide an outlook on the newest technological advancements and the future applications these are anticipated to enable.","sentences":["Drug development is becoming more and more complex and resource-intensive.","To reduce the costs and the time-to-market, the pharmaceutical industry employs cutting-edge automation solutions.","Supportive robotics technologies, such as stationary and mobile manipulators, exist in various laboratory settings.","However, they still lack the mobility and dexterity to navigate and operate in human-centered environments.","We evaluate the feasibility of quadruped robots for the specific use case of remote inspection, utilizing the out-of-the-box capabilities of Boston Dynamics' Spot platform.","We also provide an outlook on the newest technological advancements and the future applications these are anticipated to enable."],"url":"http://arxiv.org/abs/2307.01101v1"}
{"created":"2023-07-03 15:19:17","title":"MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion","abstract":"This paper introduces MVDiffusion, a simple yet effective multi-view image generation method for scenarios where pixel-to-pixel correspondences are available, such as perspective crops from panorama or multi-view images given geometry (depth maps and poses). Unlike prior models that rely on iterative image warping and inpainting, MVDiffusion concurrently generates all images with a global awareness, encompassing high resolution and rich content, effectively addressing the error accumulation prevalent in preceding models. MVDiffusion specifically incorporates a correspondence-aware attention mechanism, enabling effective cross-view interaction. This mechanism underpins three pivotal modules: 1) a generation module that produces low-resolution images while maintaining global correspondence, 2) an interpolation module that densifies spatial coverage between images, and 3) a super-resolution module that upscales into high-resolution outputs. In terms of panoramic imagery, MVDiffusion can generate high-resolution photorealistic images up to 1024$\\times$1024 pixels. For geometry-conditioned multi-view image generation, MVDiffusion demonstrates the first method capable of generating a textured map of a scene mesh. The project page is at https://mvdiffusion.github.io.","sentences":["This paper introduces MVDiffusion, a simple yet effective multi-view image generation method for scenarios where pixel-to-pixel correspondences are available, such as perspective crops from panorama or multi-view images given geometry (depth maps and poses).","Unlike prior models that rely on iterative image warping and inpainting, MVDiffusion concurrently generates all images with a global awareness, encompassing high resolution and rich content, effectively addressing the error accumulation prevalent in preceding models.","MVDiffusion specifically incorporates a correspondence-aware attention mechanism, enabling effective cross-view interaction.","This mechanism underpins three pivotal modules: 1) a generation module that produces low-resolution images while maintaining global correspondence, 2) an interpolation module that densifies spatial coverage between images, and 3) a super-resolution module that upscales into high-resolution outputs.","In terms of panoramic imagery, MVDiffusion can generate high-resolution photorealistic images up to 1024$\\times$1024 pixels.","For geometry-conditioned multi-view image generation, MVDiffusion demonstrates the first method capable of generating a textured map of a scene mesh.","The project page is at https://mvdiffusion.github.io."],"url":"http://arxiv.org/abs/2307.01097v1"}
{"created":"2023-07-03 15:16:42","title":"Coded Orthogonal Modulation for the Multi-Antenna Multiple-Access Channel","abstract":"This study focuses on (traditional and unsourced) multiple-access communication over a single transmit and multiple ($M$) receive antennas. We assume full or partial channel state information (CSI) at the receiver. It is known that to fully achieve the fundamental limits (even asymptotically) the decoder needs to jointly estimate all user codewords, doing which directly is computationally infeasible. We propose a low-complexity solution, termed coded orthogonal modulation multiple-access (COMMA), in which users first encode their messages via a long (multi-user interference aware) outer code operating over a $q$-ary alphabet. These symbols are modulated onto $q$ orthogonal waveforms. At the decoder a multiple-measurement vector approximate message passing (MMV-AMP) algorithm estimates several candidates (out of $q$) for each user, with the remaining uncertainty resolved by the single-user outer decoders. Numerically, we show that COMMA outperforms a standard solution based on linear multiuser detection (MUD) with Gaussian signaling. Theoretically, we derive bounds and scaling laws for $M$, the number of users $K_a$, SNR, and $q$, allowing to quantify the trade-off between receive antennas and spectral efficiency. The orthogonal signaling scheme is applicable to unsourced random access and, with chirp sequences as basis, allows for low-complexity fast Fourier transform (FFT) based receivers that are resilient to frequency and timing offsets.","sentences":["This study focuses on (traditional and unsourced) multiple-access communication over a single transmit and multiple ($M$) receive antennas.","We assume full or partial channel state information (CSI) at the receiver.","It is known that to fully achieve the fundamental limits (even asymptotically) the decoder needs to jointly estimate all user codewords, doing which directly is computationally infeasible.","We propose a low-complexity solution, termed coded orthogonal modulation multiple-access (COMMA), in which users first encode their messages via a long (multi-user interference aware) outer code operating over a $q$-ary alphabet.","These symbols are modulated onto $q$ orthogonal waveforms.","At the decoder a multiple-measurement vector approximate message passing (MMV-AMP) algorithm estimates several candidates (out of $q$) for each user, with the remaining uncertainty resolved by the single-user outer decoders.","Numerically, we show that COMMA outperforms a standard solution based on linear multiuser detection (MUD) with Gaussian signaling.","Theoretically, we derive bounds and scaling laws for $M$, the number of users $K_a$, SNR, and $q$, allowing to quantify the trade-off between receive antennas and spectral efficiency.","The orthogonal signaling scheme is applicable to unsourced random access and, with chirp sequences as basis, allows for low-complexity fast Fourier transform (FFT) based receivers that are resilient to frequency and timing offsets."],"url":"http://arxiv.org/abs/2307.01095v1"}
{"created":"2023-07-03 15:09:37","title":"The Lawn Mowing Problem: From Algebra to Algorithms","abstract":"For a given polygonal region $P$, the Lawn Mowing Problem (LMP) asks for a shortest tour $T$ that gets within Euclidean distance 1/2 of every point in $P$; this is equivalent to computing a shortest tour for a unit-diameter cutter $C$ that covers all of $P$. As a generalization of the Traveling Salesman Problem, the LMP is NP-hard; unlike the discrete TSP, however, the LMP has defied efforts to achieve exact solutions, due to its combination of combinatorial complexity with continuous geometry.   We provide a number of new contributions that provide insights into the involved difficulties, as well as positive results that enable both theoretical and practical progress. (1) We show that the LMP is algebraically hard: it is not solvable by radicals over the field of rationals, even for the simple case in which $P$ is a $2\\times 2$ square. This implies that it is impossible to compute exact optimal solutions under models of computation that rely on elementary arithmetic operations and the extraction of $k$th roots, and explains the perceived practical difficulty. (2) We exploit this algebraic analysis for the natural class of polygons with axis-parallel edges and integer vertices (i.e., polyominoes), highlighting the relevance of turn-cost minimization for Lawn Mowing tours, and leading to a general construction method for feasible tours. (3) We show that this construction method achieves theoretical worst-case guarantees that improve previous approximation factors for polyominoes. (4) We demonstrate the practical usefulness \\emph{beyond polyominoes} by performing an extensive practical study on a spectrum of more general benchmark polygons: We obtain solutions that are better than the previous best values by Fekete et al., for instance sizes up to $20$ times larger.","sentences":["For a given polygonal region $P$, the Lawn Mowing Problem (LMP) asks for a shortest tour $T$ that gets within Euclidean distance 1/2 of every point in $P$; this is equivalent to computing a shortest tour for a unit-diameter cutter $C$ that covers all of $P$. As a generalization of the Traveling Salesman Problem, the LMP is NP-hard; unlike the discrete TSP, however, the LMP has defied efforts to achieve exact solutions, due to its combination of combinatorial complexity with continuous geometry.   ","We provide a number of new contributions that provide insights into the involved difficulties, as well as positive results that enable both theoretical and practical progress.","(1) We show that the LMP is algebraically hard: it is not solvable by radicals over the field of rationals, even for the simple case in which $P$ is a $2\\times 2$ square.","This implies that it is impossible to compute exact optimal solutions under models of computation that rely on elementary arithmetic operations and the extraction of $k$th roots, and explains the perceived practical difficulty.","(2) We exploit this algebraic analysis for the natural class of polygons with axis-parallel edges and integer vertices (i.e., polyominoes), highlighting the relevance of turn-cost minimization for Lawn Mowing tours, and leading to a general construction method for feasible tours.","(3) We show that this construction method achieves theoretical worst-case guarantees that improve previous approximation factors for polyominoes.","(4) We demonstrate the practical usefulness \\emph{beyond polyominoes} by performing an extensive practical study on a spectrum of more general benchmark polygons: We obtain solutions that are better than the previous best values by Fekete et al., for instance sizes up to $20$ times larger."],"url":"http://arxiv.org/abs/2307.01092v1"}
{"created":"2023-07-03 15:09:32","title":"UW-ProCCaps: UnderWater Progressive Colourisation with Capsules","abstract":"Underwater images are fundamental for studying and understanding the status of marine life. We focus on reducing the memory space required for image storage while the memory space consumption in the collecting phase limits the time lasting of this phase leading to the need for more image collection campaigns. We present a novel machine-learning model that reconstructs the colours of underwater images from their luminescence channel, thus saving 2/3 of the available storage space. Our model specialises in underwater colour reconstruction and consists of an encoder-decoder architecture. The encoder is composed of a convolutional encoder and a parallel specialised classifier trained with webly-supervised data. The encoder and the decoder use layers of capsules to capture the features of the entities in the image. The colour reconstruction process recalls the progressive and the generative adversarial training procedures. The progressive training gives the ground for a generative adversarial routine focused on the refining of colours giving the image bright and saturated colours which bring the image back to life. We validate the model both qualitatively and quantitatively on four benchmark datasets. This is the first attempt at colour reconstruction in greyscale underwater images. Extensive results on four benchmark datasets demonstrate that our solution outperforms state-of-the-art (SOTA) solutions. We also demonstrate that the generated colourisation enhances the quality of images compared to enhancement models at the SOTA.","sentences":["Underwater images are fundamental for studying and understanding the status of marine life.","We focus on reducing the memory space required for image storage while the memory space consumption in the collecting phase limits the time lasting of this phase leading to the need for more image collection campaigns.","We present a novel machine-learning model that reconstructs the colours of underwater images from their luminescence channel, thus saving 2/3 of the available storage space.","Our model specialises in underwater colour reconstruction and consists of an encoder-decoder architecture.","The encoder is composed of a convolutional encoder and a parallel specialised classifier trained with webly-supervised data.","The encoder and the decoder use layers of capsules to capture the features of the entities in the image.","The colour reconstruction process recalls the progressive and the generative adversarial training procedures.","The progressive training gives the ground for a generative adversarial routine focused on the refining of colours giving the image bright and saturated colours which bring the image back to life.","We validate the model both qualitatively and quantitatively on four benchmark datasets.","This is the first attempt at colour reconstruction in greyscale underwater images.","Extensive results on four benchmark datasets demonstrate that our solution outperforms state-of-the-art (SOTA) solutions.","We also demonstrate that the generated colourisation enhances the quality of images compared to enhancement models at the SOTA."],"url":"http://arxiv.org/abs/2307.01091v1"}
{"created":"2023-07-03 15:08:28","title":"Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data","abstract":"Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. Yet, its performance is known to degrade under distribution shift and long-tailed class distributions, which are often present in real world applications. Here, we characterize the performance of several post-hoc and training-based conformal prediction methods under these settings, providing the first empirical evaluation on large-scale datasets and models. We show that across numerous conformal methods and neural network families, performance greatly degrades under distribution shifts violating safety guarantees. Similarly, we show that in long-tailed settings the guarantees are frequently violated on many classes. Understanding the limitations of these methods is necessary for deployment in real world and safety-critical applications.","sentences":["Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees.","Yet, its performance is known to degrade under distribution shift and long-tailed class distributions, which are often present in real world applications.","Here, we characterize the performance of several post-hoc and training-based conformal prediction methods under these settings, providing the first empirical evaluation on large-scale datasets and models.","We show that across numerous conformal methods and neural network families, performance greatly degrades under distribution shifts violating safety guarantees.","Similarly, we show that in long-tailed settings the guarantees are frequently violated on many classes.","Understanding the limitations of these methods is necessary for deployment in real world and safety-critical applications."],"url":"http://arxiv.org/abs/2307.01088v1"}
{"created":"2023-07-03 15:07:10","title":"Some challenges of calibrating differentiable agent-based models","abstract":"Agent-based models (ABMs) are a promising approach to modelling and reasoning about complex systems, yet their application in practice is impeded by their complexity, discrete nature, and the difficulty of performing parameter inference and optimisation tasks. This in turn has sparked interest in the construction of differentiable ABMs as a strategy for combatting these difficulties, yet a number of challenges remain. In this paper, we discuss and present experiments that highlight some of these challenges, along with potential solutions.","sentences":["Agent-based models (ABMs) are a promising approach to modelling and reasoning about complex systems, yet their application in practice is impeded by their complexity, discrete nature, and the difficulty of performing parameter inference and optimisation tasks.","This in turn has sparked interest in the construction of differentiable ABMs as a strategy for combatting these difficulties, yet a number of challenges remain.","In this paper, we discuss and present experiments that highlight some of these challenges, along with potential solutions."],"url":"http://arxiv.org/abs/2307.01085v1"}
{"created":"2023-07-03 14:58:28","title":"Meaning and identity of proofs in a bilateralist setting: A two-sorted typed lambda-calculus for proofs and refutations","abstract":"In this paper I will develop a lambda-term calculus, lambda-2Int, for a bi-intuitionistic logic and discuss its implications for the notions of sense and denotation of derivations in a bilateralist setting. Thus, I will use the Curry-Howard correspondence, which has been well-established between the simply typed lambda-calculus and natural deduction systems for intuitionistic logic, and apply it to a bilateralist proof system displaying two derivability relations, one for proving and one for refuting. The basis will be the natural deduction system of Wansing's bi-intuitionistic logic 2Int (2016a; 2017), which I will turn into a term-annotated form. Therefore, we need a type theory that extends to a two-sorted typed lambda-calculus. I will present such a term-annotated proof system for 2Int and prove some properties and results for it, most importantly for this paper a Dualization Theorem relating proofs and refutations in this system. On the basis of these formal results I will argue that this gives us interesting insights into questions about sense and denotation as well as synonymy and identity of proofs from a bilateralist point of view.","sentences":["In this paper I will develop a lambda-term calculus, lambda-2Int, for a bi-intuitionistic logic and discuss its implications for the notions of sense and denotation of derivations in a bilateralist setting.","Thus, I will use the Curry-Howard correspondence, which has been well-established between the simply typed lambda-calculus and natural deduction systems for intuitionistic logic, and apply it to a bilateralist proof system displaying two derivability relations, one for proving and one for refuting.","The basis will be the natural deduction system of Wansing's bi-intuitionistic logic 2Int (2016a; 2017), which I will turn into a term-annotated form.","Therefore, we need a type theory that extends to a two-sorted typed lambda-calculus.","I will present such a term-annotated proof system for 2Int and prove some properties and results for it, most importantly for this paper a Dualization Theorem relating proofs and refutations in this system.","On the basis of these formal results I will argue that this gives us interesting insights into questions about sense and denotation as well as synonymy and identity of proofs from a bilateralist point of view."],"url":"http://arxiv.org/abs/2307.01079v1"}
{"created":"2023-07-03 14:55:02","title":"Analyzing Multiple-Choice Reading and Listening Comprehension Tests","abstract":"Multiple-choice reading and listening comprehension tests are an important part of language assessment. Content creators for standard educational tests need to carefully curate questions that assess the comprehension abilities of candidates taking the tests. However, recent work has shown that a large number of questions in general multiple-choice reading comprehension datasets can be answered without comprehension, by leveraging world knowledge instead. This work investigates how much of a contextual passage needs to be read in multiple-choice reading based on conversation transcriptions and listening comprehension tests to be able to work out the correct answer. We find that automated reading comprehension systems can perform significantly better than random with partial or even no access to the context passage. These findings offer an approach for content creators to automatically capture the trade-off between comprehension and world knowledge required for their proposed questions.","sentences":["Multiple-choice reading and listening comprehension tests are an important part of language assessment.","Content creators for standard educational tests need to carefully curate questions that assess the comprehension abilities of candidates taking the tests.","However, recent work has shown that a large number of questions in general multiple-choice reading comprehension datasets can be answered without comprehension, by leveraging world knowledge instead.","This work investigates how much of a contextual passage needs to be read in multiple-choice reading based on conversation transcriptions and listening comprehension tests to be able to work out the correct answer.","We find that automated reading comprehension systems can perform significantly better than random with partial or even no access to the context passage.","These findings offer an approach for content creators to automatically capture the trade-off between comprehension and world knowledge required for their proposed questions."],"url":"http://arxiv.org/abs/2307.01076v1"}
{"created":"2023-07-03 14:54:13","title":"When Can Linear Learners be Robust to Indiscriminate Poisoning Attacks?","abstract":"We study indiscriminate poisoning for linear learners where an adversary injects a few crafted examples into the training data with the goal of forcing the induced model to incur higher test error. Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners. For theoretical Gaussian distributions, we rigorously characterize the behavior of an optimal poisoning attack, defined as the poisoning strategy that attains the maximum risk of the induced model at a given poisoning budget. Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small. These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks.","sentences":["We study indiscriminate poisoning for linear learners where an adversary injects a few crafted examples into the training data with the goal of forcing the induced model to incur higher test error.","Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners.","For theoretical Gaussian distributions, we rigorously characterize the behavior of an optimal poisoning attack, defined as the poisoning strategy that attains the maximum risk of the induced model at a given poisoning budget.","Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small.","These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks."],"url":"http://arxiv.org/abs/2307.01073v1"}
{"created":"2023-07-03 14:53:41","title":"Scenario-Based Motion Planning with Bounded Probability of Collision","abstract":"Robots will increasingly operate near humans that introduce uncertainties in the motion planning problem due to their complex nature. Typically, chance constraints are introduced in the planner to optimize performance while guaranteeing probabilistic safety. However, existing methods do not consider the actual probability of collision for the planned trajectory, but rather its marginalization, that is, the independent collision probabilities for each planning step and/or dynamic obstacle, resulting in conservative trajectories. To address this issue, we introduce a novel real-time capable method termed Safe Horizon MPC, that explicitly constrains the joint probability of collision with all obstacles over the duration of the motion plan. This is achieved by reformulating the chance-constrained planning problem using scenario optimization and predictive control. Our method is less conservative than state-of-the-art approaches, applicable to arbitrary probability distributions of the obstacles' trajectories, computationally tractable and scalable. We demonstrate our proposed approach using a mobile robot and an autonomous vehicle in an environment shared with humans.","sentences":["Robots will increasingly operate near humans that introduce uncertainties in the motion planning problem due to their complex nature.","Typically, chance constraints are introduced in the planner to optimize performance while guaranteeing probabilistic safety.","However, existing methods do not consider the actual probability of collision for the planned trajectory, but rather its marginalization, that is, the independent collision probabilities for each planning step and/or dynamic obstacle, resulting in conservative trajectories.","To address this issue, we introduce a novel real-time capable method termed Safe Horizon MPC, that explicitly constrains the joint probability of collision with all obstacles over the duration of the motion plan.","This is achieved by reformulating the chance-constrained planning problem using scenario optimization and predictive control.","Our method is less conservative than state-of-the-art approaches, applicable to arbitrary probability distributions of the obstacles' trajectories, computationally tractable and scalable.","We demonstrate our proposed approach using a mobile robot and an autonomous vehicle in an environment shared with humans."],"url":"http://arxiv.org/abs/2307.01070v1"}
{"created":"2023-07-03 14:50:14","title":"Shi-NeSS: Detecting Good and Stable Keypoints with a Neural Stability Score","abstract":"Learning a feature point detector presents a challenge both due to the ambiguity of the definition of a keypoint and correspondingly the need for a specially prepared ground truth labels for such points. In our work, we address both of these issues by utilizing a combination of a hand-crafted Shi detector and a neural network. We build on the principled and localized keypoints provided by the Shi detector and perform their selection using the keypoint stability score regressed by the neural network - Neural Stability Score (NeSS). Therefore, our method is named Shi-NeSS since it combines the Shi detector and the properties of the keypoint stability score, and it only requires for training sets of images without dataset pre-labeling or the need for reconstructed correspondence labels. We evaluate Shi-NeSS on HPatches, ScanNet, MegaDepth and IMC-PT, demonstrating state-of-the-art performance and good generalization on downstream tasks.","sentences":["Learning a feature point detector presents a challenge both due to the ambiguity of the definition of a keypoint and correspondingly the need for a specially prepared ground truth labels for such points.","In our work, we address both of these issues by utilizing a combination of a hand-crafted Shi detector and a neural network.","We build on the principled and localized keypoints provided by the Shi detector and perform their selection using the keypoint stability score regressed by the neural network - Neural Stability Score (NeSS).","Therefore, our method is named Shi-NeSS since it combines the Shi detector and the properties of the keypoint stability score, and it only requires for training sets of images without dataset pre-labeling or the need for reconstructed correspondence labels.","We evaluate Shi-NeSS on HPatches, ScanNet, MegaDepth and IMC-PT, demonstrating state-of-the-art performance and good generalization on downstream tasks."],"url":"http://arxiv.org/abs/2307.01069v1"}
{"created":"2023-07-03 14:47:18","title":"Localized Questions in Medical Visual Question Answering","abstract":"Visual Question Answering (VQA) models aim to answer natural language questions about given images. Due to its ability to ask questions that differ from those used when training the model, medical VQA has received substantial attention in recent years. However, existing medical VQA models typically focus on answering questions that refer to an entire image rather than where the relevant content may be located in the image. Consequently, VQA models are limited in their interpretability power and the possibility to probe the model about specific image regions. This paper proposes a novel approach for medical VQA that addresses this limitation by developing a model that can answer questions about image regions while considering the context necessary to answer the questions. Our experimental results demonstrate the effectiveness of our proposed model, outperforming existing methods on three datasets. Our code and data are available at https://github.com/sergiotasconmorales/locvqa.","sentences":["Visual Question Answering (VQA) models aim to answer natural language questions about given images.","Due to its ability to ask questions that differ from those used when training the model, medical VQA has received substantial attention in recent years.","However, existing medical VQA models typically focus on answering questions that refer to an entire image rather than where the relevant content may be located in the image.","Consequently, VQA models are limited in their interpretability power and the possibility to probe the model about specific image regions.","This paper proposes a novel approach for medical VQA that addresses this limitation by developing a model that can answer questions about image regions while considering the context necessary to answer the questions.","Our experimental results demonstrate the effectiveness of our proposed model, outperforming existing methods on three datasets.","Our code and data are available at https://github.com/sergiotasconmorales/locvqa."],"url":"http://arxiv.org/abs/2307.01067v1"}
{"created":"2023-07-03 14:43:40","title":"TomatoDIFF: On-plant Tomato Segmentation with Denoising Diffusion Models","abstract":"Artificial intelligence applications enable farmers to optimize crop growth and production while reducing costs and environmental impact. Computer vision-based algorithms in particular, are commonly used for fruit segmentation, enabling in-depth analysis of the harvest quality and accurate yield estimation. In this paper, we propose TomatoDIFF, a novel diffusion-based model for semantic segmentation of on-plant tomatoes. When evaluated against other competitive methods, our model demonstrates state-of-the-art (SOTA) performance, even in challenging environments with highly occluded fruits. Additionally, we introduce Tomatopia, a new, large and challenging dataset of greenhouse tomatoes. The dataset comprises high-resolution RGB-D images and pixel-level annotations of the fruits.","sentences":["Artificial intelligence applications enable farmers to optimize crop growth and production while reducing costs and environmental impact.","Computer vision-based algorithms in particular, are commonly used for fruit segmentation, enabling in-depth analysis of the harvest quality and accurate yield estimation.","In this paper, we propose TomatoDIFF, a novel diffusion-based model for semantic segmentation of on-plant tomatoes.","When evaluated against other competitive methods, our model demonstrates state-of-the-art (SOTA) performance, even in challenging environments with highly occluded fruits.","Additionally, we introduce Tomatopia, a new, large and challenging dataset of greenhouse tomatoes.","The dataset comprises high-resolution RGB-D images and pixel-level annotations of the fruits."],"url":"http://arxiv.org/abs/2307.01064v1"}
{"created":"2023-07-03 14:41:04","title":"Synthesising Full-Information Protocols","abstract":"We lay out a model of games with imperfect information that features explicit communication actions, by which the entire observation history of a player is revealed to another player. Such full-information protocols are common in asynchronous distributed systems; here, we consider a synchronous setting with a single active player who may communicate with multiple passive observers in an indeterminate environment. We present a procedure for solving the basic strategy-synthesis problem under regular winning conditions.   We present our solution in an abstract framework of games with imperfect information and we split the proof in two conceptual parts: (i) a generic reduction schema from imperfect-information to perfect-information games, and (ii) a specific construction for full-information protocols that satisfies the requirement of the reduction schema.","sentences":["We lay out a model of games with imperfect information that features explicit communication actions, by which the entire observation history of a player is revealed to another player.","Such full-information protocols are common in asynchronous distributed systems; here, we consider a synchronous setting with a single active player who may communicate with multiple passive observers in an indeterminate environment.","We present a procedure for solving the basic strategy-synthesis problem under regular winning conditions.   ","We present our solution in an abstract framework of games with imperfect information and we split the proof in two conceptual parts: (i) a generic reduction schema from imperfect-information to perfect-information games, and (ii) a specific construction for full-information protocols that satisfies the requirement of the reduction schema."],"url":"http://arxiv.org/abs/2307.01063v1"}
{"created":"2023-07-03 14:41:02","title":"A Data-Driven Approach to Geometric Modeling of Systems with Low-Bandwidth Actuator Dynamics","abstract":"It is challenging to perform identification on soft robots due to their underactuated, high dimensional dynamics. In this work, we present a data-driven modeling framework, based on geometric mechanics (also known as gauge theory), that can be applied to systems with low-bandwidth actuation of the shape space. By exploiting temporal asymmetries in actuator dynamics, our approach enables the design of robots that can be driven by a single control input. We present a method for constructing a series connected model comprising actuator and locomotor dynamics based on data points from stochastically perturbed, repeated behaviors around the observed limit cycle. We demonstrate our methods on a real-world example of a soft crawler made by stimuli-responsive hydrogels that locomotes on merely one cycling control signal by utilizing its geometric and temporal asymmetry. For systems with first-order, low-pass actuator dynamics, such as swelling-driven actuators used in hydrogel crawlers, we show that first order Taylor approximations can well capture the dynamics of the system shape as well as its movements. Finally, we propose an approach of numerically optimizing control signals by iteratively refining models and optimizing the input waveform.","sentences":["It is challenging to perform identification on soft robots due to their underactuated, high dimensional dynamics.","In this work, we present a data-driven modeling framework, based on geometric mechanics (also known as gauge theory), that can be applied to systems with low-bandwidth actuation of the shape space.","By exploiting temporal asymmetries in actuator dynamics, our approach enables the design of robots that can be driven by a single control input.","We present a method for constructing a series connected model comprising actuator and locomotor dynamics based on data points from stochastically perturbed, repeated behaviors around the observed limit cycle.","We demonstrate our methods on a real-world example of a soft crawler made by stimuli-responsive hydrogels that locomotes on merely one cycling control signal by utilizing its geometric and temporal asymmetry.","For systems with first-order, low-pass actuator dynamics, such as swelling-driven actuators used in hydrogel crawlers, we show that first order Taylor approximations can well capture the dynamics of the system shape as well as its movements.","Finally, we propose an approach of numerically optimizing control signals by iteratively refining models and optimizing the input waveform."],"url":"http://arxiv.org/abs/2307.01062v1"}
{"created":"2023-07-03 14:35:24","title":"A 3 TOPS/W RISC-V Parallel Cluster for Inference of Fine-Grain Mixed-Precision Quantized Neural Networks","abstract":"The emerging trend of deploying complex algorithms, such as Deep Neural Networks (DNNs), increasingly poses strict memory and energy efficiency requirements on Internet-of-Things (IoT) end-nodes. Mixed-precision quantization has been proposed as a technique to minimize a DNN's memory footprint and maximize its execution efficiency, with negligible end-to-end precision degradation. In this work, we present a novel hardware and software stack for energy-efficient inference of mixed-precision Quantized Neural Networks (QNNs). We introduce Flex-V, a processor based on the RISC-V Instruction Set Architecture (ISA) that features fused Mac&Load mixed-precision dot product instructions; to avoid the exponential growth of the encoding space due to mixed-precision variants, we encode formats into the Control-Status Registers (CSRs). Flex-V core is integrated into a tightly-coupled cluster of eight processors; in addition, we provide a full framework for the end-to-end deployment of DNNs including a compiler, optimized libraries, and a memory-aware deployment flow. Our results show up to 91.5 MAC/cycle and 3.26 TOPS/W on the cluster, implemented in a commercial 22nm FDX technology, with up to 8.5x speed-up, and an area overhead of only 5.6% with respect to the baseline. To demonstrate the capabilities of the architecture, we benchmark it with end-to-end real-life QNNs, improving performance by 2x - 2.5x with respect to existing solutions using fully flexible programmable processors.","sentences":["The emerging trend of deploying complex algorithms, such as Deep Neural Networks (DNNs), increasingly poses strict memory and energy efficiency requirements on Internet-of-Things (IoT) end-nodes.","Mixed-precision quantization has been proposed as a technique to minimize a DNN's memory footprint and maximize its execution efficiency, with negligible end-to-end precision degradation.","In this work, we present a novel hardware and software stack for energy-efficient inference of mixed-precision Quantized Neural Networks (QNNs).","We introduce Flex-V, a processor based on the RISC-V Instruction Set Architecture (ISA) that features fused Mac&Load mixed-precision dot product instructions; to avoid the exponential growth of the encoding space due to mixed-precision variants, we encode formats into the Control-Status Registers (CSRs).","Flex-V core is integrated into a tightly-coupled cluster of eight processors; in addition, we provide a full framework for the end-to-end deployment of DNNs including a compiler, optimized libraries, and a memory-aware deployment flow.","Our results show up to 91.5 MAC/cycle and 3.26 TOPS/W on the cluster, implemented in a commercial 22nm FDX technology, with up to 8.5x speed-up, and an area overhead of only 5.6% with respect to the baseline.","To demonstrate the capabilities of the architecture, we benchmark it with end-to-end real-life QNNs, improving performance by 2x - 2.5x with respect to existing solutions using fully flexible programmable processors."],"url":"http://arxiv.org/abs/2307.01056v1"}
{"created":"2023-07-03 14:33:14","title":"ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning","abstract":"The recent contrastive learning methods, due to their effectiveness in representation learning, have been widely applied to modeling graph data. Random perturbation is widely used to build contrastive views for graph data, which however, could accidentally break graph structures and lead to suboptimal performance. In addition, graph data is usually highly abstract, so it is hard to extract intuitive meanings and design more informed augmentation schemes. Effective representations should preserve key characteristics in data and abandon superfluous information. In this paper, we propose ENGAGE (ExplaNation Guided data AuGmEntation), where explanation guides the contrastive augmentation process to preserve the key parts in graphs and explore removing superfluous information. Specifically, we design an efficient unsupervised explanation method called smoothed activation map as the indicator of node importance in representation learning. Then, we design two data augmentation schemes on graphs for perturbing structural and feature information, respectively. We also provide justification for the proposed method in the framework of information theories. Experiments of both graph-level and node-level tasks, on various model architectures and on different real-world graphs, are conducted to demonstrate the effectiveness and flexibility of ENGAGE. The code of ENGAGE can be found: https://github.com/sycny/ENGAGE.","sentences":["The recent contrastive learning methods, due to their effectiveness in representation learning, have been widely applied to modeling graph data.","Random perturbation is widely used to build contrastive views for graph data, which however, could accidentally break graph structures and lead to suboptimal performance.","In addition, graph data is usually highly abstract, so it is hard to extract intuitive meanings and design more informed augmentation schemes.","Effective representations should preserve key characteristics in data and abandon superfluous information.","In this paper, we propose ENGAGE (ExplaNation Guided data AuGmEntation), where explanation guides the contrastive augmentation process to preserve the key parts in graphs and explore removing superfluous information.","Specifically, we design an efficient unsupervised explanation method called smoothed activation map as the indicator of node importance in representation learning.","Then, we design two data augmentation schemes on graphs for perturbing structural and feature information, respectively.","We also provide justification for the proposed method in the framework of information theories.","Experiments of both graph-level and node-level tasks, on various model architectures and on different real-world graphs, are conducted to demonstrate the effectiveness and flexibility of ENGAGE.","The code of ENGAGE can be found: https://github.com/sycny/ENGAGE."],"url":"http://arxiv.org/abs/2307.01053v1"}
{"created":"2023-07-03 14:24:04","title":"Cross-modal Place Recognition in Image Databases using Event-based Sensors","abstract":"Visual place recognition is an important problem towards global localization in many robotics tasks. One of the biggest challenges is that it may suffer from illumination or appearance changes in surrounding environments. Event cameras are interesting alternatives to frame-based sensors as their high dynamic range enables robust perception in difficult illumination conditions. However, current event-based place recognition methods only rely on event information, which restricts downstream applications of VPR. In this paper, we present the first cross-modal visual place recognition framework that is capable of retrieving regular images from a database given an event query. Our method demonstrates promising results with respect to the state-of-the-art frame-based and event-based methods on the Brisbane-Event-VPR dataset under different scenarios. We also verify the effectiveness of the combination of retrieval and classification, which can boost performance by a large margin.","sentences":["Visual place recognition is an important problem towards global localization in many robotics tasks.","One of the biggest challenges is that it may suffer from illumination or appearance changes in surrounding environments.","Event cameras are interesting alternatives to frame-based sensors as their high dynamic range enables robust perception in difficult illumination conditions.","However, current event-based place recognition methods only rely on event information, which restricts downstream applications of VPR.","In this paper, we present the first cross-modal visual place recognition framework that is capable of retrieving regular images from a database given an event query.","Our method demonstrates promising results with respect to the state-of-the-art frame-based and event-based methods on the Brisbane-Event-VPR dataset under different scenarios.","We also verify the effectiveness of the combination of retrieval and classification, which can boost performance by a large margin."],"url":"http://arxiv.org/abs/2307.01047v1"}
{"created":"2023-07-03 14:23:47","title":"A Fine-Grained Classification of the Complexity of Evaluating the Tutte Polynomial on Integer Points Parameterized by Treewidth and Cutwidth","abstract":"We give a fine-grained classification of evaluating the Tutte polynomial $T(G;x,y)$ on all integer points on graphs with small treewidth and cutwidth. Specifically, we show for any point $(x,y) \\in \\mathbb{Z}^2$ that either   - can be computed in polynomial time,   - can be computed in $2^{O(tw)}n^{O(1)}$ time, but not in $2^{o(ctw)}n^{O(1)}$ time assuming the Exponential Time Hypothesis (ETH),   - can be computed in $2^{O(tw \\log tw)}n^{O(1)}$ time, but not in $2^{o(ctw \\log ctw)}n^{O(1)}$ time assuming the ETH,   where we assume tree decompositions of treewidth $tw$ and cutwidth decompositions of cutwidth $ctw$ are given as input along with the input graph on $n$ vertices and point $(x,y)$.   To obtain these results, we refine the existing reductions that were instrumental for the seminal dichotomy by Jaeger, Welsh and Vertigan~[Math. Proc. Cambridge Philos. Soc'90].   One of our technical contributions is a new rank bound of a matrix that indicates whether the union of two forests is a forest itself, which we use to show that the number of forests of a graph can be counted in $2^{O(tw)}n^{O(1)}$ time.","sentences":["We give a fine-grained classification of evaluating the Tutte polynomial $T(G;x,y)$ on all integer points on graphs with small treewidth and cutwidth.","Specifically, we show for any point $(x,y) \\in \\mathbb{Z}^2$ that either   - can be computed in polynomial time,   - can be computed in $2^{O(tw)}n^{O(1)}$ time, but not in $2^{o(ctw)}n^{O(1)}$ time assuming the Exponential Time Hypothesis (ETH),   - can be computed in $2^{O(tw \\log tw)}n^{O(1)}$ time, but not in $2^{o(ctw \\log ctw)}n^{O(1)}$ time assuming the ETH,   where we assume tree decompositions of treewidth $tw$ and cutwidth decompositions of cutwidth $ctw$ are given as input along with the input graph on $n$ vertices and point $(x,y)$.   To obtain these results, we refine the existing reductions that were instrumental for the seminal dichotomy by Jaeger, Welsh and Vertigan~[Math.","Proc.","Cambridge Philos.","Soc'90].   ","One of our technical contributions is a new rank bound of a matrix that indicates whether the union of two forests is a forest itself, which we use to show that the number of forests of a graph can be counted in $2^{O(tw)}n^{O(1)}$ time."],"url":"http://arxiv.org/abs/2307.01046v1"}
{"created":"2023-07-03 14:22:05","title":"Cloud Native Software Engineering","abstract":"Cloud compute adoption has been growing since its inception in the early 2000's with estimates that the size of this market in terms of worldwide spend will increase from \\$700 billion in 2021 to \\$1.3 trillion in 2025. While there is a significant research activity in many areas of cloud computing technologies, we see little attention being paid to advancing software engineering practices needed to support the current and next generation of cloud native applications. By cloud native, we mean software that is designed and built specifically for deployment to a modern cloud platform. This paper frames the landscape of Cloud Native Software Engineering from a practitioners standpoint, and identifies several software engineering research opportunities that should be investigated. We cover specific engineering challenges associated with software architectures commonly used in cloud applications along with incremental challenges that are expected with emerging IoT/Edge computing use cases.","sentences":["Cloud compute adoption has been growing since its inception in the early 2000's with estimates that the size of this market in terms of worldwide spend will increase from \\$700 billion in 2021 to \\$1.3 trillion in 2025.","While there is a significant research activity in many areas of cloud computing technologies, we see little attention being paid to advancing software engineering practices needed to support the current and next generation of cloud native applications.","By cloud native, we mean software that is designed and built specifically for deployment to a modern cloud platform.","This paper frames the landscape of Cloud Native Software Engineering from a practitioners standpoint, and identifies several software engineering research opportunities that should be investigated.","We cover specific engineering challenges associated with software architectures commonly used in cloud applications along with incremental challenges that are expected with emerging IoT/Edge computing use cases."],"url":"http://arxiv.org/abs/2307.01045v1"}
{"created":"2023-07-03 14:20:15","title":"Practical Non-Invasive Probing Attacks Against Novel Carbon-Nanotube-Based Physical Unclonable Functions","abstract":"As the number of devices being interconnected increases, so does also the demand for (lightweight) security. To this end, Physical Unclonable Functions (PUFs) have been proposed as hardware primitives that can act as roots of trust and security. Recently, a new type of PUF based on Carbon NanoTubes (CNTs) has been proposed. At the same time, attacks and testing based on direct electrical probing appear to be moving towards non-invasive techniques. In this context, this work attempts to examine the potential for practical non-invasive probing attacks against the CNT-PUF, a novel PUF based on CNTs. Our results indicate that direct probing might potentially compromise the security of this PUF. Nevertheless, we note that this holds true only in the case that the attacker can directly probe the wire corresponding to the secret value of each CNT-PUF cell. Thus, we can conclude that the examined CNT-PUFs are rather resilient to direct probing attacks, that non-invasive probing methods appear to be promising for testing such PUFs, and that, in order for the attacker to gain the full-length value of the secret, all the relevant channels would need to be probed. Nevertheless, as our work proves, practical non-invasive attacks against the CNT-PUF are feasible and adequate countermeasures need to be employed in order to address this issue.","sentences":["As the number of devices being interconnected increases, so does also the demand for (lightweight) security.","To this end, Physical Unclonable Functions (PUFs) have been proposed as hardware primitives that can act as roots of trust and security.","Recently, a new type of PUF based on Carbon NanoTubes (CNTs) has been proposed.","At the same time, attacks and testing based on direct electrical probing appear to be moving towards non-invasive techniques.","In this context, this work attempts to examine the potential for practical non-invasive probing attacks against the CNT-PUF, a novel PUF based on CNTs.","Our results indicate that direct probing might potentially compromise the security of this PUF.","Nevertheless, we note that this holds true only in the case that the attacker can directly probe the wire corresponding to the secret value of each CNT-PUF cell.","Thus, we can conclude that the examined CNT-PUFs are rather resilient to direct probing attacks, that non-invasive probing methods appear to be promising for testing such PUFs, and that, in order for the attacker to gain the full-length value of the secret, all the relevant channels would need to be probed.","Nevertheless, as our work proves, practical non-invasive attacks against the CNT-PUF are feasible and adequate countermeasures need to be employed in order to address this issue."],"url":"http://arxiv.org/abs/2307.01041v1"}
{"created":"2023-07-03 14:01:24","title":"Advancing O-RAN to Facilitate Intelligence in V2X","abstract":"Vehicular communications at high frequencies are envisioned to be a breakthrough application for the 6G cellular systems. Traditional Radio Access Networks (RANs) lack the flexibility to enable sophisticated control mechanisms that are demanded by the strict performance requirements of the dynamic vehicular environment. In contrast, the features of Open RAN (O-RAN) can be exploited to support advanced use cases. Indeed, the emerging paradigm of O-RAN represents an ideal framework for the orchestration of vehicular communication. Although the high potential stemming from their integration can be easily seen and recognized, the effective combination of the two ecosystems is an open issue. This article pioneers the integration of the two strategies for seamlessly incorporating vehicle-to-everything (V2X) control within O-RAN's ecosystem. We propose and discuss an enabling architecture that tightly integrates V2X and O-RAN. In the proposed solution, an O-RAN-based control plane operates at low frequencies to achieve reliable and efficient connectivity among autonomous vehicles at higher frequencies. The technological feasibility of this integrated architecture is investigated. A detailed case study is presented and analyzed to demonstrate the design of an xApp to showcase a practical example of O-RAN solution for a specific V2X scenario.","sentences":["Vehicular communications at high frequencies are envisioned to be a breakthrough application for the 6G cellular systems.","Traditional Radio Access Networks (RANs) lack the flexibility to enable sophisticated control mechanisms that are demanded by the strict performance requirements of the dynamic vehicular environment.","In contrast, the features of Open RAN (O-RAN) can be exploited to support advanced use cases.","Indeed, the emerging paradigm of O-RAN represents an ideal framework for the orchestration of vehicular communication.","Although the high potential stemming from their integration can be easily seen and recognized, the effective combination of the two ecosystems is an open issue.","This article pioneers the integration of the two strategies for seamlessly incorporating vehicle-to-everything (V2X) control within O-RAN's ecosystem.","We propose and discuss an enabling architecture that tightly integrates V2X and O-RAN.","In the proposed solution, an O-RAN-based control plane operates at low frequencies to achieve reliable and efficient connectivity among autonomous vehicles at higher frequencies.","The technological feasibility of this integrated architecture is investigated.","A detailed case study is presented and analyzed to demonstrate the design of an xApp to showcase a practical example of O-RAN solution for a specific V2X scenario."],"url":"http://arxiv.org/abs/2307.01029v1"}
{"created":"2023-07-03 13:58:20","title":"Temporal Graph Benchmark for Machine Learning on Temporal Graphs","abstract":"We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs. TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks. For both tasks, we design evaluation protocols based on realistic use-cases. We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets. In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models. We believe that these findings open up opportunities for future research on temporal graphs. Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, including data loading, experiment setup and performance evaluation. TGB will be maintained and updated on a regular basis and welcomes community feedback. TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/ .","sentences":["We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs.","TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks.","For both tasks, we design evaluation protocols based on realistic use-cases.","We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets.","In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models.","We believe that these findings open up opportunities for future research on temporal graphs.","Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, including data loading, experiment setup and performance evaluation.","TGB will be maintained and updated on a regular basis and welcomes community feedback.","TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/ ."],"url":"http://arxiv.org/abs/2307.01026v1"}
{"created":"2023-07-03 13:55:44","title":"SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation","abstract":"Domain adaptation (DA) has demonstrated significant promise for real-time nighttime unmanned aerial vehicle (UAV) tracking. However, the state-of-the-art (SOTA) DA still lacks the potential object with accurate pixel-level location and boundary to generate the high-quality target domain training sample. This key issue constrains the transfer learning of the real-time daytime SOTA trackers for challenging nighttime UAV tracking. Recently, the notable Segment Anything Model (SAM) has achieved remarkable zero-shot generalization ability to discover abundant potential objects due to its huge data-driven training approach. To solve the aforementioned issue, this work proposes a novel SAM-powered DA framework for real-time nighttime UAV tracking, i.e., SAM-DA. Specifically, an innovative SAM-powered target domain training sample swelling is designed to determine enormous high-quality target domain training samples from every single raw nighttime image. This novel one-to-many method significantly expands the high-quality target domain training sample for DA. Comprehensive experiments on extensive nighttime UAV videos prove the robustness and domain adaptability of SAM-DA for nighttime UAV tracking. Especially, compared to the SOTA DA, SAM-DA can achieve better performance with fewer raw nighttime images, i.e., the fewer-better training. This economized training approach facilitates the quick validation and deployment of algorithms for UAVs. The code is available at https://github.com/vision4robotics/SAM-DA.","sentences":["Domain adaptation (DA) has demonstrated significant promise for real-time nighttime unmanned aerial vehicle (UAV) tracking.","However, the state-of-the-art (SOTA) DA still lacks the potential object with accurate pixel-level location and boundary to generate the high-quality target domain training sample.","This key issue constrains the transfer learning of the real-time daytime SOTA trackers for challenging nighttime UAV tracking.","Recently, the notable Segment Anything Model (SAM) has achieved remarkable zero-shot generalization ability to discover abundant potential objects due to its huge data-driven training approach.","To solve the aforementioned issue, this work proposes a novel SAM-powered DA framework for real-time nighttime UAV tracking, i.e., SAM-DA.","Specifically, an innovative SAM-powered target domain training sample swelling is designed to determine enormous high-quality target domain training samples from every single raw nighttime image.","This novel one-to-many method significantly expands the high-quality target domain training sample for DA.","Comprehensive experiments on extensive nighttime UAV videos prove the robustness and domain adaptability of SAM-DA for nighttime UAV tracking.","Especially, compared to the SOTA DA, SAM-DA can achieve better performance with fewer raw nighttime images, i.e., the fewer-better training.","This economized training approach facilitates the quick validation and deployment of algorithms for UAVs.","The code is available at https://github.com/vision4robotics/SAM-DA."],"url":"http://arxiv.org/abs/2307.01024v1"}
{"created":"2023-07-03 13:54:50","title":"Neural Chronos ODE: Unveiling Temporal Patterns and Forecasting Future and Past Trends in Time Series Data","abstract":"This work introduces Neural Chronos Ordinary Differential Equations (Neural CODE), a deep neural network architecture that fits a continuous-time ODE dynamics for predicting the chronology of a system both forward and backward in time. To train the model, we solve the ODE as an initial value problem and a final value problem, similar to Neural ODEs. We also explore two approaches to combining Neural CODE with Recurrent Neural Networks by replacing Neural ODE with Neural CODE (CODE-RNN), and incorporating a bidirectional RNN for full information flow in both time directions (CODE-BiRNN), and variants with other update cells namely GRU and LSTM: CODE-GRU, CODE-BiGRU, CODE-LSTM, CODE-BiLSTM.   Experimental results demonstrate that Neural CODE outperforms Neural ODE in learning the dynamics of a spiral forward and backward in time, even with sparser data. We also compare the performance of CODE-RNN/-GRU/-LSTM and CODE-BiRNN/-BiGRU/-BiLSTM against ODE-RNN/-GRU/-LSTM on three real-life time series data tasks: imputation of missing data for lower and higher dimensional data, and forward and backward extrapolation with shorter and longer time horizons. Our findings show that the proposed architectures converge faster, with CODE-BiRNN/-BiGRU/-BiLSTM consistently outperforming the other architectures on all tasks.","sentences":["This work introduces Neural Chronos Ordinary Differential Equations (Neural CODE), a deep neural network architecture that fits a continuous-time ODE dynamics for predicting the chronology of a system both forward and backward in time.","To train the model, we solve the ODE as an initial value problem and a final value problem, similar to Neural ODEs.","We also explore two approaches to combining Neural CODE with Recurrent Neural Networks by replacing Neural ODE with Neural CODE (CODE-RNN), and incorporating a bidirectional RNN for full information flow in both time directions (CODE-BiRNN), and variants with other update cells namely GRU and LSTM: CODE-GRU, CODE-BiGRU, CODE-LSTM, CODE-BiLSTM.   ","Experimental results demonstrate that Neural CODE outperforms Neural ODE in learning the dynamics of a spiral forward and backward in time, even with sparser data.","We also compare the performance of CODE-RNN/-GRU/-LSTM and CODE-BiRNN/-BiGRU/-BiLSTM against ODE-RNN/-GRU/-LSTM on three real-life time series data tasks: imputation of missing data for lower and higher dimensional data, and forward and backward extrapolation with shorter and longer time horizons.","Our findings show that the proposed architectures converge faster, with CODE-BiRNN/-BiGRU/-BiLSTM consistently outperforming the other architectures on all tasks."],"url":"http://arxiv.org/abs/2307.01023v1"}
{"created":"2023-07-03 13:49:14","title":"Estimating Post-OCR Denoising Complexity on Numerical Texts","abstract":"Post-OCR processing has significantly improved over the past few years. However, these have been primarily beneficial for texts consisting of natural, alphabetical words, as opposed to documents of numerical nature such as invoices, payslips, medical certificates, etc. To evaluate the OCR post-processing difficulty of these datasets, we propose a method to estimate the denoising complexity of a text and evaluate it on several datasets of varying nature, and show that texts of numerical nature have a significant disadvantage. We evaluate the estimated complexity ranking with respect to the error rates of modern-day denoising approaches to show the validity of our estimator.","sentences":["Post-OCR processing has significantly improved over the past few years.","However, these have been primarily beneficial for texts consisting of natural, alphabetical words, as opposed to documents of numerical nature such as invoices, payslips, medical certificates, etc.","To evaluate the OCR post-processing difficulty of these datasets, we propose a method to estimate the denoising complexity of a text and evaluate it on several datasets of varying nature, and show that texts of numerical nature have a significant disadvantage.","We evaluate the estimated complexity ranking with respect to the error rates of modern-day denoising approaches to show the validity of our estimator."],"url":"http://arxiv.org/abs/2307.01020v1"}
{"created":"2023-07-03 13:45:24","title":"CGAM: Click-Guided Attention Module for Interactive Pathology Image Segmentation via Backpropagating Refinement","abstract":"Tumor region segmentation is an essential task for the quantitative analysis of digital pathology. Recently presented deep neural networks have shown state-of-the-art performance in various image-segmentation tasks. However, because of the unclear boundary between the cancerous and normal regions in pathology images, despite using modern methods, it is difficult to produce satisfactory segmentation results in terms of the reliability and accuracy required for medical data. In this study, we propose an interactive segmentation method that allows users to refine the output of deep neural networks through click-type user interactions. The primary method is to formulate interactive segmentation as an optimization problem that leverages both user-provided click constraints and semantic information in a feature map using a click-guided attention module (CGAM). Unlike other existing methods, CGAM avoids excessive changes in segmentation results, which can lead to the overfitting of user clicks. Another advantage of CGAM is that the model size is independent of input image size. Experimental results on pathology image datasets indicated that our method performs better than existing state-of-the-art methods.","sentences":["Tumor region segmentation is an essential task for the quantitative analysis of digital pathology.","Recently presented deep neural networks have shown state-of-the-art performance in various image-segmentation tasks.","However, because of the unclear boundary between the cancerous and normal regions in pathology images, despite using modern methods, it is difficult to produce satisfactory segmentation results in terms of the reliability and accuracy required for medical data.","In this study, we propose an interactive segmentation method that allows users to refine the output of deep neural networks through click-type user interactions.","The primary method is to formulate interactive segmentation as an optimization problem that leverages both user-provided click constraints and semantic information in a feature map using a click-guided attention module (CGAM).","Unlike other existing methods, CGAM avoids excessive changes in segmentation results, which can lead to the overfitting of user clicks.","Another advantage of CGAM is that the model size is independent of input image size.","Experimental results on pathology image datasets indicated that our method performs better than existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2307.01015v1"}
{"created":"2023-07-03 13:44:36","title":"SynthCal: A Synthetic Benchmarking Pipeline to Compare Camera Calibration Algorithms","abstract":"Accurate camera calibration is crucial for various computer vision applications. However, measuring camera parameters in the real world is challenging and arduous, and there needs to be a dataset with ground truth to evaluate calibration algorithms' accuracy. In this paper, we present SynthCal, a synthetic camera calibration benchmarking pipeline that generates images of calibration patterns to measure and enable accurate quantification of calibration algorithm performance in camera parameter estimation. We present a SynthCal-generated calibration dataset with four common patterns, two camera types, and two environments with varying view, distortion, lighting, and noise levels. The dataset evaluates single-view calibration algorithms by measuring reprojection and root-mean-square errors for identical patterns and camera settings. Additionally, we analyze the significance of different patterns using Zhang's method, which estimates intrinsic and extrinsic camera parameters with known correspondences between 3D points and their 2D projections in different configurations and environments. The experimental results demonstrate the effectiveness of SynthCal in evaluating various calibration algorithms and patterns.","sentences":["Accurate camera calibration is crucial for various computer vision applications.","However, measuring camera parameters in the real world is challenging and arduous, and there needs to be a dataset with ground truth to evaluate calibration algorithms' accuracy.","In this paper, we present SynthCal, a synthetic camera calibration benchmarking pipeline that generates images of calibration patterns to measure and enable accurate quantification of calibration algorithm performance in camera parameter estimation.","We present a SynthCal-generated calibration dataset with four common patterns, two camera types, and two environments with varying view, distortion, lighting, and noise levels.","The dataset evaluates single-view calibration algorithms by measuring reprojection and root-mean-square errors for identical patterns and camera settings.","Additionally, we analyze the significance of different patterns using Zhang's method, which estimates intrinsic and extrinsic camera parameters with known correspondences between 3D points and their 2D projections in different configurations and environments.","The experimental results demonstrate the effectiveness of SynthCal in evaluating various calibration algorithms and patterns."],"url":"http://arxiv.org/abs/2307.01013v1"}
{"created":"2023-07-03 13:41:13","title":"APEIRON: composing smart TDAQ systems for high energy physics experiments","abstract":"APEIRON is a framework encompassing the general architecture of a distributed heterogeneous processing platform and the corresponding software stack, from the low level device drivers up to the high level programming model. The framework is designed to be efficiently used for studying, prototyping and deploying smart trigger and data acquisition (TDAQ) systems for high energy physics experiments.","sentences":["APEIRON is a framework encompassing the general architecture of a distributed heterogeneous processing platform and the corresponding software stack, from the low level device drivers up to the high level programming model.","The framework is designed to be efficiently used for studying, prototyping and deploying smart trigger and data acquisition (TDAQ) systems for high energy physics experiments."],"url":"http://arxiv.org/abs/2307.01009v1"}
{"created":"2023-07-03 13:40:20","title":"Joint Coordinate Regression and Association For Multi-Person Pose Estimation, A Pure Neural Network Approach","abstract":"We introduce a novel one-stage end-to-end multi-person 2D pose estimation algorithm, known as Joint Coordinate Regression and Association (JCRA), that produces human pose joints and associations without requiring any post-processing. The proposed algorithm is fast, accurate, effective, and simple. The one-stage end-to-end network architecture significantly improves the inference speed of JCRA. Meanwhile, we devised a symmetric network structure for both the encoder and decoder, which ensures high accuracy in identifying keypoints. It follows an architecture that directly outputs part positions via a transformer network, resulting in a significant improvement in performance. Extensive experiments on the MS COCO and CrowdPose benchmarks demonstrate that JCRA outperforms state-of-the-art approaches in both accuracy and efficiency. Moreover, JCRA demonstrates 69.2 mAP and is 78\\% faster at inference acceleration than previous state-of-the-art bottom-up algorithms. The code for this algorithm will be publicly available.","sentences":["We introduce a novel one-stage end-to-end multi-person 2D pose estimation algorithm, known as Joint Coordinate Regression and Association (JCRA), that produces human pose joints and associations without requiring any post-processing.","The proposed algorithm is fast, accurate, effective, and simple.","The one-stage end-to-end network architecture significantly improves the inference speed of JCRA.","Meanwhile, we devised a symmetric network structure for both the encoder and decoder, which ensures high accuracy in identifying keypoints.","It follows an architecture that directly outputs part positions via a transformer network, resulting in a significant improvement in performance.","Extensive experiments on the MS COCO and CrowdPose benchmarks demonstrate that JCRA outperforms state-of-the-art approaches in both accuracy and efficiency.","Moreover, JCRA demonstrates 69.2 mAP and is 78\\% faster at inference acceleration than previous state-of-the-art bottom-up algorithms.","The code for this algorithm will be publicly available."],"url":"http://arxiv.org/abs/2307.01004v1"}
{"created":"2023-07-03 13:37:00","title":"Visual Instruction Tuning with Polite Flamingo","abstract":"Recent research has demonstrated that the multi-task fine-tuning of multi-modal Large Language Models (LLMs) using an assortment of annotated downstream vision-language datasets significantly enhances their performance. Yet, during this process, a side effect, which we termed as the \"multi-modal alignment tax\", surfaces. This side effect negatively impacts the model's ability to format responses appropriately -- for instance, its \"politeness\" -- due to the overly succinct and unformatted nature of raw annotations, resulting in reduced human preference. In this paper, we introduce Polite Flamingo, a multi-modal response rewriter that transforms raw annotations into a more appealing, \"polite\" format. Polite Flamingo is trained to reconstruct high-quality responses from their automatically distorted counterparts and is subsequently applied to a vast array of vision-language datasets for response rewriting. After rigorous filtering, we generate the PF-1M dataset and further validate its value by fine-tuning a multi-modal LLM with it. Combined with novel methodologies including U-shaped multi-stage tuning and multi-turn augmentation, the resulting model, Clever Flamingo, demonstrates its advantages in both multi-modal understanding and response politeness according to automated and human evaluations.","sentences":["Recent research has demonstrated that the multi-task fine-tuning of multi-modal Large Language Models (LLMs) using an assortment of annotated downstream vision-language datasets significantly enhances their performance.","Yet, during this process, a side effect, which we termed as the \"multi-modal alignment tax\", surfaces.","This side effect negatively impacts the model's ability to format responses appropriately -- for instance, its \"politeness\" -- due to the overly succinct and unformatted nature of raw annotations, resulting in reduced human preference.","In this paper, we introduce Polite Flamingo, a multi-modal response rewriter that transforms raw annotations into a more appealing, \"polite\" format.","Polite Flamingo is trained to reconstruct high-quality responses from their automatically distorted counterparts and is subsequently applied to a vast array of vision-language datasets for response rewriting.","After rigorous filtering, we generate the PF-1M dataset and further validate its value by fine-tuning a multi-modal LLM with it.","Combined with novel methodologies including U-shaped multi-stage tuning and multi-turn augmentation, the resulting model, Clever Flamingo, demonstrates its advantages in both multi-modal understanding and response politeness according to automated and human evaluations."],"url":"http://arxiv.org/abs/2307.01003v1"}
{"created":"2023-07-03 13:21:58","title":"RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation","abstract":"The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which for the first time explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Subsequently, a parameter-efficient tuning strategy is employed to effectively align and fuse the language and vision features. Through comprehensive ablation studies, we demonstrate the practical and effective design choices of our strategy. Extensive experiments conducted on Ref-Youtu-VOS and Ref-DAVIS17 datasets validate the superiority and effectiveness of our RefSAM model over existing methods. The code and models will be made publicly at \\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.","sentences":["The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation.","However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and limited understanding of different modalities, such as language and vision.","This paper presents the RefSAM model, which for the first time explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps.","Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts.","Subsequently, a parameter-efficient tuning strategy is employed to effectively align and fuse the language and vision features.","Through comprehensive ablation studies, we demonstrate the practical and effective design choices of our strategy.","Extensive experiments conducted on Ref-Youtu-VOS and Ref-DAVIS17 datasets validate the superiority and effectiveness of our RefSAM model over existing methods.","The code and models will be made publicly at \\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}."],"url":"http://arxiv.org/abs/2307.00997v1"}
{"created":"2023-07-03 13:20:28","title":"Kernelizing Problems on Planar Graphs in Sublinear Space and Polynomial Time","abstract":"In this paper, we devise a scheme for kernelizing, in sublinear space and polynomial time, various problems on planar graphs. The scheme exploits planarity to ensure that the resulting algorithms run in polynomial time and use O((sqrt(n) + k) log n) bits of space, where n is the number of vertices in the input instance and k is the intended solution size. As examples, we apply the scheme to Dominating Set and Vertex Cover. For Dominating Set, we also show that a well-known kernelization algorithm due to Alber et al. (JACM 2004) can be carried out in polynomial time and space O(k log n). Along the way, we devise restricted-memory procedures for computing region decompositions and approximating the aforementioned problems, which might be of independent interest.","sentences":["In this paper, we devise a scheme for kernelizing, in sublinear space and polynomial time, various problems on planar graphs.","The scheme exploits planarity to ensure that the resulting algorithms run in polynomial time and use O((sqrt(n)","+","k) log n) bits of space, where n is the number of vertices in the input instance and k is the intended solution size.","As examples, we apply the scheme to Dominating Set and Vertex Cover.","For Dominating Set, we also show that a well-known kernelization algorithm due to Alber et al. (JACM 2004) can be carried out in polynomial time and space O(k log n).","Along the way, we devise restricted-memory procedures for computing region decompositions and approximating the aforementioned problems, which might be of independent interest."],"url":"http://arxiv.org/abs/2307.00996v1"}
{"created":"2023-07-03 13:18:55","title":"Towards Suicide Prevention from Bipolar Disorder with Temporal Symptom-Aware Multitask Learning","abstract":"Bipolar disorder (BD) is closely associated with an increased risk of suicide. However, while the prior work has revealed valuable insight into understanding the behavior of BD patients on social media, little attention has been paid to developing a model that can predict the future suicidality of a BD patient. Therefore, this study proposes a multi-task learning model for predicting the future suicidality of BD patients by jointly learning current symptoms. We build a novel BD dataset clinically validated by psychiatrists, including 14 years of posts on bipolar-related subreddits written by 818 BD patients, along with the annotations of future suicidality and BD symptoms. We also suggest a temporal symptom-aware attention mechanism to determine which symptoms are the most influential for predicting future suicidality over time through a sequence of BD posts. Our experiments demonstrate that the proposed model outperforms the state-of-the-art models in both BD symptom identification and future suicidality prediction tasks. In addition, the proposed temporal symptom-aware attention provides interpretable attention weights, helping clinicians to apprehend BD patients more comprehensively and to provide timely intervention by tracking mental state progression.","sentences":["Bipolar disorder (BD) is closely associated with an increased risk of suicide.","However, while the prior work has revealed valuable insight into understanding the behavior of BD patients on social media, little attention has been paid to developing a model that can predict the future suicidality of a BD patient.","Therefore, this study proposes a multi-task learning model for predicting the future suicidality of BD patients by jointly learning current symptoms.","We build a novel BD dataset clinically validated by psychiatrists, including 14 years of posts on bipolar-related subreddits written by 818 BD patients, along with the annotations of future suicidality and BD symptoms.","We also suggest a temporal symptom-aware attention mechanism to determine which symptoms are the most influential for predicting future suicidality over time through a sequence of BD posts.","Our experiments demonstrate that the proposed model outperforms the state-of-the-art models in both BD symptom identification and future suicidality prediction tasks.","In addition, the proposed temporal symptom-aware attention provides interpretable attention weights, helping clinicians to apprehend BD patients more comprehensively and to provide timely intervention by tracking mental state progression."],"url":"http://arxiv.org/abs/2307.00995v1"}
{"created":"2023-07-03 13:16:08","title":"NOMA-Assisted Grant-Free Transmission: How to Design Pre-Configured SNR Levels?","abstract":"An effective way to realize non-orthogonal multiple access (NOMA) assisted grant-free transmission is to first create multiple receive signal-to-noise ratio (SNR) levels and then serve multiple grant-free users by employing these SNR levels as bandwidth resources. These SNR levels need to be pre-configured prior to the grant-free transmission and have great impact on the performance of grant-free networks. The aim of this letter is to illustrate different designs for configuring the SNR levels and investigate their impact on the performance of grant-free transmission, where age-of-information is used as the performance metric. The presented analytical and simulation results demonstrate the performance gain achieved by NOMA over orthogonal multiple access, and also reveal the relative merits of the considered designs for pre-configured SNR levels.","sentences":["An effective way to realize non-orthogonal multiple access (NOMA) assisted grant-free transmission is to first create multiple receive signal-to-noise ratio (SNR) levels and then serve multiple grant-free users by employing these SNR levels as bandwidth resources.","These SNR levels need to be pre-configured prior to the grant-free transmission and have great impact on the performance of grant-free networks.","The aim of this letter is to illustrate different designs for configuring the SNR levels and investigate their impact on the performance of grant-free transmission, where age-of-information is used as the performance metric.","The presented analytical and simulation results demonstrate the performance gain achieved by NOMA over orthogonal multiple access, and also reveal the relative merits of the considered designs for pre-configured SNR levels."],"url":"http://arxiv.org/abs/2307.00990v1"}
{"created":"2023-07-03 13:05:46","title":"Effect of the cross-section architecture on the impact resistance of bio-inspired low-porosity structures using neural networks","abstract":"Biological structural designs in nature, like hoof walls, horns, and antlers, can be used as inspiration for generating structures with excellent mechanical properties. A common theme in these designs is the small percent porosity in the structure ranging from 1 - 5\\%. In this work, the sheep horn was used as an inspiration due to its higher toughness when loaded in the radial direction compared to the longitudinal direction. Under dynamic transverse compression, we investigated the structure-property relations in low porosity structures characterized by their two-dimensional (2D) cross-sections. A diverse design space was created by combining polygonal tubules with different numbers of sides placed on a grid with different numbers of rows and columns. The volume fraction and the orientation angle of the tubules were also varied. The finite element (FE) method was used with a rate-dependent elastoplastic material model to generate the stress-strain curves in plane strain conditions. A gated recurrent unit (GRU) model was trained to predict the structures' stress-strain response and energy absorption under different strain rates and applied strains. The parameter-based model uses eight discrete parameters to characterize the design space and as inputs to the model. The trained GRU model can efficiently predict the response of a new design in as little as 0.16 ms and allows rapid performance evaluation of 128000 designs in the design space. The GRU predictions identified high-performance structures and four design trends that affect the specific energy absorption were extracted and discussed.","sentences":["Biological structural designs in nature, like hoof walls, horns, and antlers, can be used as inspiration for generating structures with excellent mechanical properties.","A common theme in these designs is the small percent porosity in the structure ranging from 1 - 5\\%.","In this work, the sheep horn was used as an inspiration due to its higher toughness when loaded in the radial direction compared to the longitudinal direction.","Under dynamic transverse compression, we investigated the structure-property relations in low porosity structures characterized by their two-dimensional (2D) cross-sections.","A diverse design space was created by combining polygonal tubules with different numbers of sides placed on a grid with different numbers of rows and columns.","The volume fraction and the orientation angle of the tubules were also varied.","The finite element (FE) method was used with a rate-dependent elastoplastic material model to generate the stress-strain curves in plane strain conditions.","A gated recurrent unit (GRU) model was trained to predict the structures' stress-strain response and energy absorption under different strain rates and applied strains.","The parameter-based model uses eight discrete parameters to characterize the design space and as inputs to the model.","The trained GRU model can efficiently predict the response of a new design in as little as 0.16 ms and allows rapid performance evaluation of 128000 designs in the design space.","The GRU predictions identified high-performance structures and four design trends that affect the specific energy absorption were extracted and discussed."],"url":"http://arxiv.org/abs/2307.00986v1"}
{"created":"2023-07-03 13:05:15","title":"An embarrassingly parallel optimal-space cardinality estimation algorithm","abstract":"In 2020 Blasiok (ACM Trans. Algorithms 16(2) 3:1-3:28) constructed an optimal space streaming algorithm for the cardinality estimation problem with the space complexity of $\\mathcal O(\\varepsilon^{-2} \\ln(\\delta^{-1}) + \\ln n)$ where $\\varepsilon$, $\\delta$ and $n$ denote the relative accuracy, failure probability and universe size, respectively. However, his solution requires the stream to be processed sequentially. On the other hand, there are algorithms that admit a merge operation; they can be used in a distributed setting, allowing parallel processing of sections of the stream, and are highly relevant for large-scale distributed applications. The best-known such algorithm, unfortunately, has a space complexity exceeding $\\Omega(\\ln(\\delta^{-1}) (\\varepsilon^{-2} \\ln \\ln n + \\ln n))$. This work presents a new algorithm that improves on the solution by Blasiok, preserving its space complexity, but with the benefit that it admits such a merge operation, thus providing an optimal solution for the problem for both sequential and parallel applications. Orthogonally, the new algorithm also improves algorithmically on Blasiok's solution (even in the sequential setting) by reducing its implementation complexity and requiring fewer distinct pseudo-random objects.","sentences":["In 2020 Blasiok (ACM Trans.","Algorithms 16(2) 3:1-3:28) constructed an optimal space streaming algorithm for the cardinality estimation problem with the space complexity of $\\mathcal O(\\varepsilon^{-2} \\ln(\\delta^{-1})","+ \\ln n)$ where $\\varepsilon$, $\\delta$ and $n$ denote the relative accuracy, failure probability and universe size, respectively.","However, his solution requires the stream to be processed sequentially.","On the other hand, there are algorithms that admit a merge operation; they can be used in a distributed setting, allowing parallel processing of sections of the stream, and are highly relevant for large-scale distributed applications.","The best-known such algorithm, unfortunately, has a space complexity exceeding $\\Omega(\\ln(\\delta^{-1}) (\\varepsilon^{-2} \\ln \\ln n + \\ln n))$.","This work presents a new algorithm that improves on the solution by Blasiok, preserving its space complexity, but with the benefit that it admits such a merge operation, thus providing an optimal solution for the problem for both sequential and parallel applications.","Orthogonally, the new algorithm also improves algorithmically on Blasiok's solution (even in the sequential setting) by reducing its implementation complexity and requiring fewer distinct pseudo-random objects."],"url":"http://arxiv.org/abs/2307.00985v1"}
{"created":"2023-07-03 13:03:17","title":"Predicting beauty, liking, and aesthetic quality: A comparative analysis of image databases for visual aesthetics research","abstract":"In the fields of Experimental and Computational Aesthetics, numerous image datasets have been created over the last two decades. In the present work, we provide a comparative overview of twelve image datasets that include aesthetic ratings (beauty, liking or aesthetic quality) and investigate the reproducibility of results across different datasets. Specifically, we examine how consistently the ratings can be predicted by using either (A) a set of 20 previously studied statistical image properties, or (B) the layers of a convolutional neural network developed for object recognition. Our findings reveal substantial variation in the predictability of aesthetic ratings across the different datasets. However, consistent similarities were found for datasets containing either photographs or paintings, suggesting different relevant features in the aesthetic evaluation of these two image genres. To our surprise, statistical image properties and the convolutional neural network predict aesthetic ratings with similar accuracy, highlighting a significant overlap in the image information captured by the two methods. Nevertheless, the discrepancies between the datasets call into question the generalizability of previous research findings on single datasets. Our study underscores the importance of considering multiple datasets to improve the validity and generalizability of research results in the fields of experimental and computational aesthetics.","sentences":["In the fields of Experimental and Computational Aesthetics, numerous image datasets have been created over the last two decades.","In the present work, we provide a comparative overview of twelve image datasets that include aesthetic ratings (beauty, liking or aesthetic quality) and investigate the reproducibility of results across different datasets.","Specifically, we examine how consistently the ratings can be predicted by using either (A) a set of 20 previously studied statistical image properties, or (B) the layers of a convolutional neural network developed for object recognition.","Our findings reveal substantial variation in the predictability of aesthetic ratings across the different datasets.","However, consistent similarities were found for datasets containing either photographs or paintings, suggesting different relevant features in the aesthetic evaluation of these two image genres.","To our surprise, statistical image properties and the convolutional neural network predict aesthetic ratings with similar accuracy, highlighting a significant overlap in the image information captured by the two methods.","Nevertheless, the discrepancies between the datasets call into question the generalizability of previous research findings on single datasets.","Our study underscores the importance of considering multiple datasets to improve the validity and generalizability of research results in the fields of experimental and computational aesthetics."],"url":"http://arxiv.org/abs/2307.00984v1"}
{"created":"2023-07-03 12:46:19","title":"Autism Spectrum Disorder Classification in Children based on Structural MRI Features Extracted using Contrastive Variational Autoencoder","abstract":"Autism spectrum disorder (ASD) is a highly disabling mental disease that brings significant impairments of social interaction ability to the patients, making early screening and intervention of ASD critical. With the development of the machine learning and neuroimaging technology, extensive research has been conducted on machine classification of ASD based on structural MRI (s-MRI). However, most studies involve with datasets where participants' age are above 5. Few studies conduct machine classification of ASD for participants below 5-year-old, but, with mediocre predictive accuracy. In this paper, we push the boundary of predictive accuracy (above 0.97) of machine classification of ASD in children (age range: 0.92-4.83 years), based on s-MRI features extracted using contrastive variational autoencoder (CVAE). 78 s-MRI, collected from Shenzhen Children's Hospital, are used for training CVAE, which consists of both ASD-specific feature channel and common shared feature channel. The ASD participants represented by ASD-specific features can be easily discriminated from TC participants represented by the common shared features, leading to high classification accuracy. In case of degraded predictive accuracy when data size is extremely small, a transfer learning strategy is proposed here as a potential solution. Finally, we conduct neuroanatomical interpretation based on the correlation between s-MRI features extracted from CVAE and surface area of different cortical regions, which discloses potential biomarkers that could help target treatments of ASD in the future.","sentences":["Autism spectrum disorder (ASD) is a highly disabling mental disease that brings significant impairments of social interaction ability to the patients, making early screening and intervention of ASD critical.","With the development of the machine learning and neuroimaging technology, extensive research has been conducted on machine classification of ASD based on structural MRI (s-MRI).","However, most studies involve with datasets where participants' age are above 5.","Few studies conduct machine classification of ASD for participants below 5-year-old, but, with mediocre predictive accuracy.","In this paper, we push the boundary of predictive accuracy (above 0.97) of machine classification of ASD in children (age range: 0.92-4.83 years), based on s-MRI features extracted using contrastive variational autoencoder (CVAE).","78 s-MRI, collected from Shenzhen Children's Hospital, are used for training CVAE, which consists of both ASD-specific feature channel and common shared feature channel.","The ASD participants represented by ASD-specific features can be easily discriminated from TC participants represented by the common shared features, leading to high classification accuracy.","In case of degraded predictive accuracy when data size is extremely small, a transfer learning strategy is proposed here as a potential solution.","Finally, we conduct neuroanatomical interpretation based on the correlation between s-MRI features extracted from CVAE and surface area of different cortical regions, which discloses potential biomarkers that could help target treatments of ASD in the future."],"url":"http://arxiv.org/abs/2307.00976v1"}
{"created":"2023-07-03 12:44:19","title":"Digital Twin-Empowered Communications: A New Frontier of Wireless Networks","abstract":"The future of wireless network generations is revolving toward unlocking the opportunities offered by virtualization and digitization services, with the aim to realize improved quality-of-experience (QoE) and bring several advantages to network users. According to the rapid development in the field of network virtualization, we envision that future wireless networks will run over ubiquitous deployment of virtualized components that are controlled by artificial intelligence (AI), i.e., the conceptualization of the Digital Twin (DT) paradigm. The key principle of the DT relies on creating a holistic representation of wireless network elements, in addition to decoupling the information pertaining to physical objects and dynamics, into a cyber twin. The cyber twin will then leverage this information for AI models training, and then reasoning and decision-making operations, which will be then reflected to the physical environment, for improved sustainability. Motivated by this, in this article, we dig deep into the intertwined role of wireless technologies as being enablers and enabled by the DT. Furthermore, we put a forward-looking vision of the integral role that future 6G networks are anticipated to play in order to realize an efficient DT. Finally, we sketch the roadmap toward identifying the limitations of the DT in 6G-enabled wireless networks, and open new horizons for further developments in different design aspects.","sentences":["The future of wireless network generations is revolving toward unlocking the opportunities offered by virtualization and digitization services, with the aim to realize improved quality-of-experience (QoE) and bring several advantages to network users.","According to the rapid development in the field of network virtualization, we envision that future wireless networks will run over ubiquitous deployment of virtualized components that are controlled by artificial intelligence (AI), i.e., the conceptualization of the Digital Twin (DT) paradigm.","The key principle of the DT relies on creating a holistic representation of wireless network elements, in addition to decoupling the information pertaining to physical objects and dynamics, into a cyber twin.","The cyber twin will then leverage this information for AI models training, and then reasoning and decision-making operations, which will be then reflected to the physical environment, for improved sustainability.","Motivated by this, in this article, we dig deep into the intertwined role of wireless technologies as being enablers and enabled by the DT.","Furthermore, we put a forward-looking vision of the integral role that future 6G networks are anticipated to play in order to realize an efficient DT.","Finally, we sketch the roadmap toward identifying the limitations of the DT in 6G-enabled wireless networks, and open new horizons for further developments in different design aspects."],"url":"http://arxiv.org/abs/2307.00973v1"}
{"created":"2023-07-03 12:44:07","title":"MoVie: Visual Model-Based Policy Adaptation for View Generalization","abstract":"Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\\textbf{Mo}$del-based policies for $\\textbf{Vie}$w generalization ($\\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\\mathbf{33}$%, $\\mathbf{86}$%, and $\\mathbf{152}$% respectively. The superior results highlight the immense potential of our approach for real-world robotics applications. Videos are available at https://yangsizhe.github.io/MoVie/ .","sentences":["Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views.","This inherent difficulty is known as the problem of $\\textit{view generalization}$.","In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations.","Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\\textbf{Mo}$del-based policies for $\\textbf{Vie}$w generalization ($\\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time.","Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\\mathbf{33}$%, $\\mathbf{86}$%, and $\\mathbf{152}$% respectively.","The superior results highlight the immense potential of our approach for real-world robotics applications.","Videos are available at https://yangsizhe.github.io/MoVie/ ."],"url":"http://arxiv.org/abs/2307.00972v1"}
{"created":"2023-07-03 12:44:00","title":"Fishing For Better Constants: The Prophet Secretary Via Poissonization","abstract":"Given n random variables $X_1, \\ldots , X_n$ taken from known distributions, a gambler observes their realizations in this order, and needs to select one of them, immediately after it is being observed, so that its value is as high as possible. The classical prophet inequality shows a strategy that guarantees a value at least half (in expectation) of that an omniscience prophet that picks the maximum, and this ratio is tight.   Esfandiari, Hajiaghayi, Liaghat, and Monemizadeh introduced a variant of the prophet inequality, the prophet secretary problem in [1]. The difference being that that the realizations arrive at a random permutation order, and not an adversarial order. Esfandiari et al. gave a simple $1-1/e \\approx 0.632$ competitive algorithm for the problem. This was later improved in a surprising result by Azar, Chiplunkar and Kaplan [2] into a $1-1/e + 1/400 \\approx 0.634$ competitive algorithm. In a subsequent result, Correa, Saona, and Ziliotto [3] took a systematic approach, introducing blind strategies, and gave an improved $0.669$ competitive algorithm. Since then, there has been no improvements on the lower bounds. Meanwhile, current upper bounds show that no algorithm can achieve a competitive ratio better than $0.7235$ [4].   In this paper, we give a $0.6724$-competitive algorithm for the prophet secretary problem. The algorithm follows blind strategies introduced by [3] but has a technical difference. We do this by re-interpretting the blind strategies, framing them as Poissonization strategies. We break the non-iid random variables into iid shards and argue about the competitive ratio in terms of events on shards. This gives significantly simpler and direct proofs, in addition to a tighter analysis on the competitive ratio. The analysis might be of independent interest for similar problems such as the prophet inequality with order-selection","sentences":["Given n random variables $X_1, \\ldots , X_n$ taken from known distributions, a gambler observes their realizations in this order, and needs to select one of them, immediately after it is being observed, so that its value is as high as possible.","The classical prophet inequality shows a strategy that guarantees a value at least half (in expectation) of that an omniscience prophet that picks the maximum, and this ratio is tight.   ","Esfandiari, Hajiaghayi, Liaghat, and Monemizadeh introduced a variant of the prophet inequality, the prophet secretary problem in [1].","The difference being that that the realizations arrive at a random permutation order, and not an adversarial order.","Esfandiari et al. gave a simple $1-1/e \\approx 0.632$ competitive algorithm for the problem.","This was later improved in a surprising result by Azar, Chiplunkar and Kaplan","[2] into a $1-1/e + 1/400 \\approx 0.634$ competitive algorithm.","In a subsequent result, Correa, Saona, and Ziliotto","[3] took a systematic approach, introducing blind strategies, and gave an improved $0.669$ competitive algorithm.","Since then, there has been no improvements on the lower bounds.","Meanwhile, current upper bounds show that no algorithm can achieve a competitive ratio better than $0.7235$ [4].   ","In this paper, we give a $0.6724$-competitive algorithm for the prophet secretary problem.","The algorithm follows blind strategies introduced by [3] but has a technical difference.","We do this by re-interpretting the blind strategies, framing them as Poissonization strategies.","We break the non-iid random variables into iid shards and argue about the competitive ratio in terms of events on shards.","This gives significantly simpler and direct proofs, in addition to a tighter analysis on the competitive ratio.","The analysis might be of independent interest for similar problems such as the prophet inequality with order-selection"],"url":"http://arxiv.org/abs/2307.00971v1"}
{"created":"2023-07-03 12:40:11","title":"High Altitude Platform Stations: the New Network Energy Efficiency Enabler in the 6G Era","abstract":"The rapidly evolving communication landscape, with the advent of 6G technology, brings new challenges to the design and operation of wireless networks. One of the key concerns is the energy efficiency of the Radio Access Network (RAN), as the exponential growth in wireless traffic demands increasingly higher energy consumption. In this paper, we assess the potential of integrating a High Altitude Platform Station (HAPS) to improve the energy efficiency of a RAN, and quantify the potential energy conservation through meticulously designed simulations. We propose a quantitative framework based on real traffic patterns to estimate the energy consumption of the HAPS integrated RAN and compare it with the conventional terrestrial RAN. Our simulation results elucidate that HAPS can significantly reduce energy consumption by up to almost 30\\% by exploiting the unique advantages of HAPS, such as its self-sustainability, high altitude, and wide coverage. We further analyze the impact of different system parameters on performance, and provide insights for the design and optimization of future 6G networks. Our work sheds light on the potential of HAPS integrated RAN to mitigate the energy challenges in the 6G era, and contributes to the sustainable development of wireless communications.","sentences":["The rapidly evolving communication landscape, with the advent of 6G technology, brings new challenges to the design and operation of wireless networks.","One of the key concerns is the energy efficiency of the Radio Access Network (RAN), as the exponential growth in wireless traffic demands increasingly higher energy consumption.","In this paper, we assess the potential of integrating a High Altitude Platform Station (HAPS) to improve the energy efficiency of a RAN, and quantify the potential energy conservation through meticulously designed simulations.","We propose a quantitative framework based on real traffic patterns to estimate the energy consumption of the HAPS integrated RAN and compare it with the conventional terrestrial RAN.","Our simulation results elucidate that HAPS can significantly reduce energy consumption by up to almost 30\\% by exploiting the unique advantages of HAPS, such as its self-sustainability, high altitude, and wide coverage.","We further analyze the impact of different system parameters on performance, and provide insights for the design and optimization of future 6G networks.","Our work sheds light on the potential of HAPS integrated RAN to mitigate the energy challenges in the 6G era, and contributes to the sustainable development of wireless communications."],"url":"http://arxiv.org/abs/2307.00969v1"}
{"created":"2023-07-03 12:39:26","title":"REAL: A Representative Error-Driven Approach for Active Learning","abstract":"Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\\underline{R}$epresentative $\\underline{E}$rrors for $\\underline{A}$ctive $\\underline{L}$earning. It identifies minority predictions as \\emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of hyperparameter settings. Our analysis also shows that $REAL$ selects the most representative pseudo errors that match the distribution of ground-truth errors along the decision boundary. Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.","sentences":["Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training.","To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity.","However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance.","To address this limitation, we propose $REAL$, a novel approach to select data instances with $\\underline{R}$epresentative $\\underline{E}$rrors for $\\underline{A}$ctive $\\underline{L}$earning.","It identifies minority predictions as \\emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density.","Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of hyperparameter settings.","Our analysis also shows that $REAL$ selects the most representative pseudo errors that match the distribution of ground-truth errors along the decision boundary.","Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real."],"url":"http://arxiv.org/abs/2307.00968v1"}
{"created":"2023-07-03 12:35:03","title":"OpenClinicalAI: An Open and Dynamic Model for Alzheimer's Disease Diagnosis","abstract":"Although Alzheimer's disease (AD) cannot be reversed or cured, timely diagnosis can significantly reduce the burden of treatment and care. Current research on AD diagnosis models usually regards the diagnosis task as a typical classification task with two primary assumptions: 1) All target categories are known a priori; 2) The diagnostic strategy for each patient is consistent, that is, the number and type of model input data for each patient are the same. However, real-world clinical settings are open, with complexity and uncertainty in terms of both subjects and the resources of the medical institutions. This means that diagnostic models may encounter unseen disease categories and need to dynamically develop diagnostic strategies based on the subject's specific circumstances and available medical resources. Thus, the AD diagnosis task is tangled and coupled with the diagnosis strategy formulation. To promote the application of diagnostic systems in real-world clinical settings, we propose OpenClinicalAI for direct AD diagnosis in complex and uncertain clinical settings. This is the first powerful end-to-end model to dynamically formulate diagnostic strategies and provide diagnostic results based on the subject's conditions and available medical resources. OpenClinicalAI combines reciprocally coupled deep multiaction reinforcement learning (DMARL) for diagnostic strategy formulation and multicenter meta-learning (MCML) for open-set recognition. The experimental results show that OpenClinicalAI achieves better performance and fewer clinical examinations than the state-of-the-art model. Our method provides an opportunity to embed the AD diagnostic system into the current health care system to cooperate with clinicians to improve current health care.","sentences":["Although Alzheimer's disease (AD) cannot be reversed or cured, timely diagnosis can significantly reduce the burden of treatment and care.","Current research on AD diagnosis models usually regards the diagnosis task as a typical classification task with two primary assumptions: 1) All target categories are known a priori; 2) The diagnostic strategy for each patient is consistent, that is, the number and type of model input data for each patient are the same.","However, real-world clinical settings are open, with complexity and uncertainty in terms of both subjects and the resources of the medical institutions.","This means that diagnostic models may encounter unseen disease categories and need to dynamically develop diagnostic strategies based on the subject's specific circumstances and available medical resources.","Thus, the AD diagnosis task is tangled and coupled with the diagnosis strategy formulation.","To promote the application of diagnostic systems in real-world clinical settings, we propose OpenClinicalAI for direct AD diagnosis in complex and uncertain clinical settings.","This is the first powerful end-to-end model to dynamically formulate diagnostic strategies and provide diagnostic results based on the subject's conditions and available medical resources.","OpenClinicalAI combines reciprocally coupled deep multiaction reinforcement learning (DMARL) for diagnostic strategy formulation and multicenter meta-learning (MCML) for open-set recognition.","The experimental results show that OpenClinicalAI achieves better performance and fewer clinical examinations than the state-of-the-art model.","Our method provides an opportunity to embed the AD diagnostic system into the current health care system to cooperate with clinicians to improve current health care."],"url":"http://arxiv.org/abs/2307.00965v1"}
{"created":"2023-07-03 12:26:44","title":"Challenges in Domain-Specific Abstractive Summarization and How to Overcome them","abstract":"Large Language Models work quite well with general-purpose data and many tasks in Natural Language Processing. However, they show several limitations when used for a task such as domain-specific abstractive text summarization. This paper identifies three of those limitations as research problems in the context of abstractive text summarization: 1) Quadratic complexity of transformer-based models with respect to the input text length; 2) Model Hallucination, which is a model's ability to generate factually incorrect text; and 3) Domain Shift, which happens when the distribution of the model's training and test corpus is not the same. Along with a discussion of the open research questions, this paper also provides an assessment of existing state-of-the-art techniques relevant to domain-specific text summarization to address the research gaps.","sentences":["Large Language Models work quite well with general-purpose data and many tasks in Natural Language Processing.","However, they show several limitations when used for a task such as domain-specific abstractive text summarization.","This paper identifies three of those limitations as research problems in the context of abstractive text summarization: 1) Quadratic complexity of transformer-based models with respect to the input text length; 2) Model Hallucination, which is a model's ability to generate factually incorrect text; and 3) Domain Shift, which happens when the distribution of the model's training and test corpus is not the same.","Along with a discussion of the open research questions, this paper also provides an assessment of existing state-of-the-art techniques relevant to domain-specific text summarization to address the research gaps."],"url":"http://arxiv.org/abs/2307.00963v1"}
{"created":"2023-07-03 12:25:09","title":"Neural Architecture Transfer 2: A Paradigm for Improving Efficiency in Multi-Objective Neural Architecture Search","abstract":"Deep learning is increasingly impacting various aspects of contemporary society. Artificial neural networks have emerged as the dominant models for solving an expanding range of tasks. The introduction of Neural Architecture Search (NAS) techniques, which enable the automatic design of task-optimal networks, has led to remarkable advances. However, the NAS process is typically associated with long execution times and significant computational resource requirements. Once-For-All (OFA) and its successor, Once-For-All-2 (OFAv2), have been developed to mitigate these challenges. While maintaining exceptional performance and eliminating the need for retraining, they aim to build a single super-network model capable of directly extracting sub-networks satisfying different constraints. Neural Architecture Transfer (NAT) was developed to maximise the effectiveness of extracting sub-networks from a super-network. In this paper, we present NATv2, an extension of NAT that improves multi-objective search algorithms applied to dynamic super-network architectures. NATv2 achieves qualitative improvements in the extractable sub-networks by exploiting the improved super-networks generated by OFAv2 and incorporating new policies for initialisation, pre-processing and updating its networks archive. In addition, a post-processing pipeline based on fine-tuning is introduced. Experimental results show that NATv2 successfully improves NAT and is highly recommended for investigating high-performance architectures with a minimal number of parameters.","sentences":["Deep learning is increasingly impacting various aspects of contemporary society.","Artificial neural networks have emerged as the dominant models for solving an expanding range of tasks.","The introduction of Neural Architecture Search (NAS) techniques, which enable the automatic design of task-optimal networks, has led to remarkable advances.","However, the NAS process is typically associated with long execution times and significant computational resource requirements.","Once-For-All (OFA) and its successor, Once-For-All-2 (OFAv2), have been developed to mitigate these challenges.","While maintaining exceptional performance and eliminating the need for retraining, they aim to build a single super-network model capable of directly extracting sub-networks satisfying different constraints.","Neural Architecture Transfer (NAT) was developed to maximise the effectiveness of extracting sub-networks from a super-network.","In this paper, we present NATv2, an extension of NAT that improves multi-objective search algorithms applied to dynamic super-network architectures.","NATv2 achieves qualitative improvements in the extractable sub-networks by exploiting the improved super-networks generated by OFAv2 and incorporating new policies for initialisation, pre-processing and updating its networks archive.","In addition, a post-processing pipeline based on fine-tuning is introduced.","Experimental results show that NATv2 successfully improves NAT and is highly recommended for investigating high-performance architectures with a minimal number of parameters."],"url":"http://arxiv.org/abs/2307.00960v1"}
{"created":"2023-07-03 12:23:56","title":"5G Wings: Investigating 5G-Connected Drones Performance in Non-Urban Areas","abstract":"Unmanned aerial vehicles (UAVs) have become extremely popular for both military and civilian applications due to their ease of deployment, cost-effectiveness, high maneuverability, and availability. Both applications, however, need reliable communication for command and control (C2) and/or data transmission. Utilizing commercial cellular networks for drone communication can enable beyond visual line of sight (BVLOS) operation, high data rate transmission, and secure communication. However, deployment of cellular-connected drones over commercial LTE/5G networks still presents various challenges such as sparse coverage outside urban areas, and interference caused to the network as the UAV is visible to many towers. Commercial 5G networks can offer various features for aerial user equipment (UE) far beyond what LTE could provide by taking advantage of mmWave, flexible numerology, slicing, and the capability of applying AI-based solutions. Limited experimental data is available to investigate the operation of aerial UEs over current, without any modification, commercial 5G networks, particularly in suburban and NON-URBAN areas. In this paper, we perform a comprehensive study of drone communications over the existing low-band and mid-band 5G networks in a suburban area for different velocities and elevations, comparing the performance against that of LTE. It is important to acknowledge that the network examined in this research is primarily designed and optimized to meet the requirements of terrestrial users, and may not adequately address the needs of aerial users. This paper not only reports the Key Performance Indicators (KPIs) compared among all combinations of the test cases but also provides recommendations for aerial users to enhance their communication quality by controlling their trajectory.","sentences":["Unmanned aerial vehicles (UAVs) have become extremely popular for both military and civilian applications due to their ease of deployment, cost-effectiveness, high maneuverability, and availability.","Both applications, however, need reliable communication for command and control (C2) and/or data transmission.","Utilizing commercial cellular networks for drone communication can enable beyond visual line of sight (BVLOS) operation, high data rate transmission, and secure communication.","However, deployment of cellular-connected drones over commercial LTE/5G networks still presents various challenges such as sparse coverage outside urban areas, and interference caused to the network as the UAV is visible to many towers.","Commercial 5G networks can offer various features for aerial user equipment (UE) far beyond what LTE could provide by taking advantage of mmWave, flexible numerology, slicing, and the capability of applying AI-based solutions.","Limited experimental data is available to investigate the operation of aerial UEs over current, without any modification, commercial 5G networks, particularly in suburban and NON-URBAN areas.","In this paper, we perform a comprehensive study of drone communications over the existing low-band and mid-band 5G networks in a suburban area for different velocities and elevations, comparing the performance against that of LTE.","It is important to acknowledge that the network examined in this research is primarily designed and optimized to meet the requirements of terrestrial users, and may not adequately address the needs of aerial users.","This paper not only reports the Key Performance Indicators (KPIs) compared among all combinations of the test cases but also provides recommendations for aerial users to enhance their communication quality by controlling their trajectory."],"url":"http://arxiv.org/abs/2307.00959v1"}
