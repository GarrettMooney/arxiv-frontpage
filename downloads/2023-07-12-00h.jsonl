{"created":"2023-07-10 17:59:40","title":"Semantic-SAM: Segment and Recognize Anything at Any Granularity","abstract":"In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity. Our model offers two key advantages: semantic-awareness and granularity-abundance. To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts. This allows our model to capture rich semantic information. For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks. Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets. Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance. Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements. We will provide code and a demo for further exploration and evaluation.","sentences":["In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity.","Our model offers two key advantages: semantic-awareness and granularity-abundance.","To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts.","This allows our model to capture rich semantic information.","For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks.","Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets.","Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance.","Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements.","We will provide code and a demo for further exploration and evaluation."],"url":"http://arxiv.org/abs/2307.04767v1"}
{"created":"2023-07-10 17:58:17","title":"Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos","abstract":"We propose a self-supervised method for learning representations based on spatial audio-visual correspondences in egocentric videos. In particular, our method leverages a masked auto-encoding framework to synthesize masked binaural audio through the synergy of audio and vision, thereby learning useful spatial relationships between the two modalities. We use our pretrained features to tackle two downstream video tasks requiring spatial understanding in social scenarios: active speaker detection and spatial audio denoising. We show through extensive experiments that our features are generic enough to improve over multiple state-of-the-art baselines on two public challenging egocentric video datasets, EgoCom and EasyCom. Project: http://vision.cs.utexas.edu/projects/ego_av_corr.","sentences":["We propose a self-supervised method for learning representations based on spatial audio-visual correspondences in egocentric videos.","In particular, our method leverages a masked auto-encoding framework to synthesize masked binaural audio through the synergy of audio and vision, thereby learning useful spatial relationships between the two modalities.","We use our pretrained features to tackle two downstream video tasks requiring spatial understanding in social scenarios: active speaker detection and spatial audio denoising.","We show through extensive experiments that our features are generic enough to improve over multiple state-of-the-art baselines on two public challenging egocentric video datasets, EgoCom and EasyCom.","Project: http://vision.cs.utexas.edu/projects/ego_av_corr."],"url":"http://arxiv.org/abs/2307.04760v1"}
{"created":"2023-07-10 17:57:32","title":"Information decomposition to identify relevant variation in complex systems with machine learning","abstract":"One of the fundamental steps toward understanding a complex system is identifying variation at the scale of the system's components that is most relevant to behavior on a macroscopic scale. Mutual information is a natural means of linking variation across scales of a system due to its independence of the particular functional relationship between variables. However, estimating mutual information given high-dimensional, continuous-valued data is notoriously difficult, and the desideratum -- to reveal important variation in a comprehensible manner -- is only readily achieved through exhaustive search. Here we propose a practical, efficient, and broadly applicable methodology to decompose the information contained in a set of measurements by lossily compressing each measurement with machine learning. Guided by the distributed information bottleneck as a learning objective, the information decomposition sorts variation in the measurements of the system state by relevance to specified macroscale behavior, revealing the most important subsets of measurements for different amounts of predictive information. Additional granularity is achieved by inspection of the learned compression schemes: the variation transmitted during compression is composed of distinctions among measurement values that are most relevant to the macroscale behavior. We focus our analysis on two paradigmatic complex systems: a Boolean circuit and an amorphous material undergoing plastic deformation. In both examples, specific bits of entropy are identified out of the high entropy of the system state as most related to macroscale behavior for insight about the connection between micro- and macro- in the complex system. The identification of meaningful variation in data, with the full generality brought by information theory, is made practical for the study of complex systems.","sentences":["One of the fundamental steps toward understanding a complex system is identifying variation at the scale of the system's components that is most relevant to behavior on a macroscopic scale.","Mutual information is a natural means of linking variation across scales of a system due to its independence of the particular functional relationship between variables.","However, estimating mutual information given high-dimensional, continuous-valued data is notoriously difficult, and the desideratum -- to reveal important variation in a comprehensible manner -- is only readily achieved through exhaustive search.","Here we propose a practical, efficient, and broadly applicable methodology to decompose the information contained in a set of measurements by lossily compressing each measurement with machine learning.","Guided by the distributed information bottleneck as a learning objective, the information decomposition sorts variation in the measurements of the system state by relevance to specified macroscale behavior, revealing the most important subsets of measurements for different amounts of predictive information.","Additional granularity is achieved by inspection of the learned compression schemes: the variation transmitted during compression is composed of distinctions among measurement values that are most relevant to the macroscale behavior.","We focus our analysis on two paradigmatic complex systems: a Boolean circuit and an amorphous material undergoing plastic deformation.","In both examples, specific bits of entropy are identified out of the high entropy of the system state as most related to macroscale behavior for insight about the connection between micro- and macro- in the complex system.","The identification of meaningful variation in data, with the full generality brought by information theory, is made practical for the study of complex systems."],"url":"http://arxiv.org/abs/2307.04755v1"}
{"created":"2023-07-10 17:56:06","title":"Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement","abstract":"We propose a system for rearranging objects in a scene to achieve a desired object-scene placing relationship, such as a book inserted in an open slot of a bookshelf. The pipeline generalizes to novel geometries, poses, and layouts of both scenes and objects, and is trained from demonstrations to operate directly on 3D point clouds. Our system overcomes challenges associated with the existence of many geometrically-similar rearrangement solutions for a given scene. By leveraging an iterative pose de-noising training procedure, we can fit multi-modal demonstration data and produce multi-modal outputs while remaining precise and accurate. We also show the advantages of conditioning on relevant local geometric features while ignoring irrelevant global structure that harms both generalization and precision. We demonstrate our approach on three distinct rearrangement tasks that require handling multi-modality and generalization over object shape and pose in both simulation and the real world. Project website, code, and videos: https://anthonysimeonov.github.io/rpdiff-multi-modal/","sentences":["We propose a system for rearranging objects in a scene to achieve a desired object-scene placing relationship, such as a book inserted in an open slot of a bookshelf.","The pipeline generalizes to novel geometries, poses, and layouts of both scenes and objects, and is trained from demonstrations to operate directly on 3D point clouds.","Our system overcomes challenges associated with the existence of many geometrically-similar rearrangement solutions for a given scene.","By leveraging an iterative pose de-noising training procedure, we can fit multi-modal demonstration data and produce multi-modal outputs while remaining precise and accurate.","We also show the advantages of conditioning on relevant local geometric features while ignoring irrelevant global structure that harms both generalization and precision.","We demonstrate our approach on three distinct rearrangement tasks that require handling multi-modality and generalization over object shape and pose in both simulation and the real world.","Project website, code, and videos: https://anthonysimeonov.github.io/rpdiff-multi-modal/"],"url":"http://arxiv.org/abs/2307.04751v1"}
{"created":"2023-07-10 17:54:57","title":"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback","abstract":"The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy. Project page for our paper is available at https://1jsingh.github.io/divide-evaluate-and-refine","sentences":["The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models.","While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt.","Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP.","To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment.","In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions.","The alignment of each assertion with generated images is then measured using a VQA model.","Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score.","Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional CLIP, BLIP scores.","Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs.","Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy.","Project page for our paper is available at https://1jsingh.github.io/divide-evaluate-and-refine"],"url":"http://arxiv.org/abs/2307.04749v1"}
{"created":"2023-07-10 17:52:01","title":"RoCo: Dialectic Multi-Robot Collaboration with Large Language Models","abstract":"We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code.","sentences":["We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning.","Robots are equipped with LLMs to discuss and collectively reason task strategies.","They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning.","We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context.","For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning.","We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics.","Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.","See project website https://project-roco.github.io for videos and code."],"url":"http://arxiv.org/abs/2307.04738v1"}
{"created":"2023-07-10 17:40:51","title":"Open Access: Who are the Ghost Readers?","abstract":"To develop a funding model for Open Access journal publication, it is necessary first to understand who benefits. This is a difficult task, because, in Open Access, no credentials are needed to read a journal article, and, thus, those people who access journal articles through Open Access leave no self-identification. We might call these readers \"ghost readers\". In this paper, I propose a method to learn the reading habits of the ghost readers. I explore this method using a database of downloads from the Open Access volumes of the Annual Reviews journals. I find that the habits of the ghost readers are very similar to those of academic readers from known institutions.","sentences":["To develop a funding model for Open Access journal publication, it is necessary first to understand who benefits.","This is a difficult task, because, in Open Access, no credentials are needed to read a journal article, and, thus, those people who access journal articles through Open Access leave no self-identification.","We might call these readers \"ghost readers\".","In this paper, I propose a method to learn the reading habits of the ghost readers.","I explore this method using a database of downloads from the Open Access volumes of the Annual Reviews journals.","I find that the habits of the ghost readers are very similar to those of academic readers from known institutions."],"url":"http://arxiv.org/abs/2307.04731v1"}
{"created":"2023-07-10 17:37:14","title":"Deceptive Information Retrieval","abstract":"We introduce the problem of deceptive information retrieval (DIR), in which a user wishes to download a required file out of multiple independent files stored in a system of databases while \\emph{deceiving} the databases by making the databases' predictions on the user-required file index incorrect with high probability. Conceptually, DIR is an extension of private information retrieval (PIR). In PIR, a user downloads a required file without revealing its index to any of the databases. The metric of deception is defined as the probability of error of databases' prediction on the user-required file, minus the corresponding probability of error in PIR. The problem is defined on time-sensitive data that keeps updating from time to time. In the proposed scheme, the user deceives the databases by sending \\emph{real} queries to download the required file at the time of the requirement and \\emph{dummy} queries at multiple distinct future time instances to manipulate the probabilities of sending each query for each file requirement, using which the databases' make the predictions on the user-required file index. The proposed DIR scheme is based on a capacity achieving probabilistic PIR scheme, and achieves rates lower than the PIR capacity due to the additional downloads made to deceive the databases. When the required level of deception is zero, the proposed scheme achieves the PIR capacity.","sentences":["We introduce the problem of deceptive information retrieval (DIR), in which a user wishes to download a required file out of multiple independent files stored in a system of databases while \\emph{deceiving} the databases by making the databases' predictions on the user-required file index incorrect with high probability.","Conceptually, DIR is an extension of private information retrieval (PIR).","In PIR, a user downloads a required file without revealing its index to any of the databases.","The metric of deception is defined as the probability of error of databases' prediction on the user-required file, minus the corresponding probability of error in PIR.","The problem is defined on time-sensitive data that keeps updating from time to time.","In the proposed scheme, the user deceives the databases by sending \\emph{real} queries to download the required file at the time of the requirement and \\emph{dummy} queries at multiple distinct future time instances to manipulate the probabilities of sending each query for each file requirement, using which the databases' make the predictions on the user-required file index.","The proposed DIR scheme is based on a capacity achieving probabilistic PIR scheme, and achieves rates lower than the PIR capacity due to the additional downloads made to deceive the databases.","When the required level of deception is zero, the proposed scheme achieves the PIR capacity."],"url":"http://arxiv.org/abs/2307.04727v1"}
{"created":"2023-07-10 17:34:23","title":"Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for experience collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to obtain expressive policies to represent multimodal behavior in the dataset. Nevertheless, they are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descriptive representation learning of states to alleviate the distribution shift incurred by the out-of-distribution states. We design a 2D Multimodal Contextual Bandit environment to demonstrate and evaluate our proposed model. We assess the performance of our model not only in this new environment but also on several D4RL benchmark tasks, achieving state-of-the-art results.","sentences":["Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for experience collection.","In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies.","However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training.","Prior work on offline RL uses conditional diffusion models to obtain expressive policies to represent multimodal behavior in the dataset.","Nevertheless, they are not tailored toward alleviating the out-of-distribution state generalization.","We introduce a novel method incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem.","State reconstruction loss promotes more descriptive representation learning of states to alleviate the distribution shift incurred by the out-of-distribution states.","We design a 2D Multimodal Contextual Bandit environment to demonstrate and evaluate our proposed model.","We assess the performance of our model not only in this new environment but also on several D4RL benchmark tasks, achieving state-of-the-art results."],"url":"http://arxiv.org/abs/2307.04726v1"}
{"created":"2023-07-10 17:34:16","title":"AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning","abstract":"With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics. In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning. At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors. Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images. We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs. Code and pre-trained weights will be publicly available at https://animatediff.github.io/ .","sentences":["With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost.","Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics.","In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning.","At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors.","Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images.","We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs.","Code and pre-trained weights will be publicly available at https://animatediff.github.io/ ."],"url":"http://arxiv.org/abs/2307.04725v1"}
{"created":"2023-07-10 17:32:15","title":"Advances and Challenges in Meta-Learning: A Technical Review","abstract":"Meta-learning empowers learning systems with the ability to acquire knowledge from multiple tasks, enabling faster adaptation and generalization to new tasks. This review provides a comprehensive technical overview of meta-learning, emphasizing its importance in real-world applications where data may be scarce or expensive to obtain. The paper covers the state-of-the-art meta-learning approaches and explores the relationship between meta-learning and multi-task learning, transfer learning, domain adaptation and generalization, self-supervised learning, personalized federated learning, and continual learning. By highlighting the synergies between these topics and the field of meta-learning, the paper demonstrates how advancements in one area can benefit the field as a whole, while avoiding unnecessary duplication of efforts. Additionally, the paper delves into advanced meta-learning topics such as learning from complex multi-modal task distributions, unsupervised meta-learning, learning to efficiently adapt to data distribution shifts, and continual meta-learning. Lastly, the paper highlights open problems and challenges for future research in the field. By synthesizing the latest research developments, this paper provides a thorough understanding of meta-learning and its potential impact on various machine learning applications. We believe that this technical overview will contribute to the advancement of meta-learning and its practical implications in addressing real-world problems.","sentences":["Meta-learning empowers learning systems with the ability to acquire knowledge from multiple tasks, enabling faster adaptation and generalization to new tasks.","This review provides a comprehensive technical overview of meta-learning, emphasizing its importance in real-world applications where data may be scarce or expensive to obtain.","The paper covers the state-of-the-art meta-learning approaches and explores the relationship between meta-learning and multi-task learning, transfer learning, domain adaptation and generalization, self-supervised learning, personalized federated learning, and continual learning.","By highlighting the synergies between these topics and the field of meta-learning, the paper demonstrates how advancements in one area can benefit the field as a whole, while avoiding unnecessary duplication of efforts.","Additionally, the paper delves into advanced meta-learning topics such as learning from complex multi-modal task distributions, unsupervised meta-learning, learning to efficiently adapt to data distribution shifts, and continual meta-learning.","Lastly, the paper highlights open problems and challenges for future research in the field.","By synthesizing the latest research developments, this paper provides a thorough understanding of meta-learning and its potential impact on various machine learning applications.","We believe that this technical overview will contribute to the advancement of meta-learning and its practical implications in addressing real-world problems."],"url":"http://arxiv.org/abs/2307.04722v1"}
{"created":"2023-07-10 17:32:13","title":"Large Language Models as General Pattern Machines","abstract":"We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.","sentences":["We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art.","Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary.","These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning.","In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole).","While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions."],"url":"http://arxiv.org/abs/2307.04721v1"}
{"created":"2023-07-10 17:31:56","title":"Visibility and Separability for a Declarative Linearizability Proof of the Timestamped Stack: Extended Version","abstract":"Linearizability is a standard correctness criterion for concurrent algorithms, typically proved by establishing the algorithms' linearization points (LP). However, LPs often hinder abstraction, and for some algorithms such as the timestamped stack, it is unclear how to even identify their LPs. In this paper, we show how to develop declarative proofs of linearizability by foregoing LPs and instead employing axiomatization of so-called visibility relations. While visibility relations have been considered before for the timestamped stack, our study is the first to show how to derive the axiomatization systematically and intuitively from the sequential specification of the stack. In addition to the visibility relation, a novel separability relation emerges to generalize real-time precedence of procedure invocation. The visibility and separability relations have natural definitions for the timestamped stack, and enable a novel proof that reduces the algorithm to a simplified form where the timestamps are generated atomically.","sentences":["Linearizability is a standard correctness criterion for concurrent algorithms, typically proved by establishing the algorithms' linearization points (LP).","However, LPs often hinder abstraction, and for some algorithms such as the timestamped stack, it is unclear how to even identify their LPs.","In this paper, we show how to develop declarative proofs of linearizability by foregoing LPs and instead employing axiomatization of so-called visibility relations.","While visibility relations have been considered before for the timestamped stack, our study is the first to show how to derive the axiomatization systematically and intuitively from the sequential specification of the stack.","In addition to the visibility relation, a novel separability relation emerges to generalize real-time precedence of procedure invocation.","The visibility and separability relations have natural definitions for the timestamped stack, and enable a novel proof that reduces the algorithm to a simplified form where the timestamps are generated atomically."],"url":"http://arxiv.org/abs/2307.04720v1"}
{"created":"2023-07-10 17:31:39","title":"On the curvature of the loss landscape","abstract":"One of the main challenges in modern deep learning is to understand why such over-parameterized models perform so well when trained on finite data. A way to analyze this generalization concept is through the properties of the associated loss landscape. In this work, we consider the loss landscape as an embedded Riemannian manifold and show that the differential geometric properties of the manifold can be used when analyzing the generalization abilities of a deep net. In particular, we focus on the scalar curvature, which can be computed analytically for our manifold, and show connections to several settings that potentially imply generalization.","sentences":["One of the main challenges in modern deep learning is to understand why such over-parameterized models perform so well when trained on finite data.","A way to analyze this generalization concept is through the properties of the associated loss landscape.","In this work, we consider the loss landscape as an embedded Riemannian manifold and show that the differential geometric properties of the manifold can be used when analyzing the generalization abilities of a deep net.","In particular, we focus on the scalar curvature, which can be computed analytically for our manifold, and show connections to several settings that potentially imply generalization."],"url":"http://arxiv.org/abs/2307.04719v1"}
{"created":"2023-07-10 17:25:04","title":"CVPR MultiEarth 2023 Deforestation Estimation Challenge:SpaceVision4Amazon","abstract":"In this paper, we present a deforestation estimation method based on attention guided UNet architecture using Electro-Optical (EO) and Synthetic Aperture Radar (SAR) satellite imagery. For optical images, Landsat-8 and for SAR imagery, Sentinel-1 data have been used to train and validate the proposed model. Due to the unavailability of temporally and spatially collocated data, individual model has been trained for each sensor. During training time Landsat-8 model achieved training and validation pixel accuracy of 93.45% and Sentinel-2 model achieved 83.87% pixel accuracy. During the test set evaluation, the model achieved pixel accuracy of 84.70% with F1-Score of 0.79 and IoU of 0.69.","sentences":["In this paper, we present a deforestation estimation method based on attention guided UNet architecture using Electro-Optical (EO) and Synthetic Aperture Radar (SAR) satellite imagery.","For optical images, Landsat-8 and for SAR imagery, Sentinel-1 data have been used to train and validate the proposed model.","Due to the unavailability of temporally and spatially collocated data, individual model has been trained for each sensor.","During training time Landsat-8 model achieved training and validation pixel accuracy of 93.45% and Sentinel-2 model achieved 83.87% pixel accuracy.","During the test set evaluation, the model achieved pixel accuracy of 84.70% with F1-Score of 0.79 and IoU of 0.69."],"url":"http://arxiv.org/abs/2307.04715v1"}
{"created":"2023-07-10 17:06:51","title":"Asymptotic Complexity Estimates for Probabilistic Programs and their VASS Abstractions","abstract":"The standard approach to analyzing the asymptotic complexity of probabilistic programs is based on studying the asymptotic growth of certain expected values (such as the expected termination time) for increasing input size. We argue that this approach is not sufficiently robust, especially in situations when the expectations are infinite. We propose new estimates for the asymptotic analysis of probabilistic programs with non-deterministic choice that overcome this deficiency. Furthermore, we show how to efficiently compute/analyze these estimates for selected classes of programs represented as Markov decision processes over vector addition systems with states.","sentences":["The standard approach to analyzing the asymptotic complexity of probabilistic programs is based on studying the asymptotic growth of certain expected values (such as the expected termination time) for increasing input size.","We argue that this approach is not sufficiently robust, especially in situations when the expectations are infinite.","We propose new estimates for the asymptotic analysis of probabilistic programs with non-deterministic choice that overcome this deficiency.","Furthermore, we show how to efficiently compute/analyze these estimates for selected classes of programs represented as Markov decision processes over vector addition systems with states."],"url":"http://arxiv.org/abs/2307.04707v1"}
{"created":"2023-07-10 16:59:49","title":"Vocal Tract Area Estimation by Gradient Descent","abstract":"Articulatory features can provide interpretable and flexible controls for the synthesis of human vocalizations by allowing the user to directly modify parameters like vocal strain or lip position. To make this manipulation through resynthesis possible, we need to estimate the features that result in a desired vocalization directly from audio recordings. In this work, we propose a white-box optimization technique for estimating glottal source parameters and vocal tract shapes from audio recordings of human vowels. The approach is based on inverse filtering and optimizing the frequency response of a wave\\-guide model of the vocal tract with gradient descent, propagating error gradients through the mapping of articulatory features to the vocal tract area function. We apply this method to the task of matching the sound of the Pink Trombone, an interactive articulatory synthesizer, to a given vocalization. We find that our method accurately recovers control functions for audio generated by the Pink Trombone itself. We then compare our technique against evolutionary optimization algorithms and a neural network trained to predict control parameters from audio. A subjective evaluation finds that our approach outperforms these black-box optimization baselines on the task of reproducing human vocalizations.","sentences":["Articulatory features can provide interpretable and flexible controls for the synthesis of human vocalizations by allowing the user to directly modify parameters like vocal strain or lip position.","To make this manipulation through resynthesis possible, we need to estimate the features that result in a desired vocalization directly from audio recordings.","In this work, we propose a white-box optimization technique for estimating glottal source parameters and vocal tract shapes from audio recordings of human vowels.","The approach is based on inverse filtering and optimizing the frequency response of a wave\\-guide model of the vocal tract with gradient descent, propagating error gradients through the mapping of articulatory features to the vocal tract area function.","We apply this method to the task of matching the sound of the Pink Trombone, an interactive articulatory synthesizer, to a given vocalization.","We find that our method accurately recovers control functions for audio generated by the Pink Trombone itself.","We then compare our technique against evolutionary optimization algorithms and a neural network trained to predict control parameters from audio.","A subjective evaluation finds that our approach outperforms these black-box optimization baselines on the task of reproducing human vocalizations."],"url":"http://arxiv.org/abs/2307.04702v1"}
{"created":"2023-07-10 16:58:37","title":"Understanding Real-World AI Planning Domains: A Conceptual Framework","abstract":"Planning is a pivotal ability of any intelligent system being developed for real-world applications. AI planning is concerned with researching and developing planning systems that automatically compute plans that satisfy some user objective. Identifying and understanding the relevant and realistic aspects that characterise real-world application domains are crucial to the development of AI planning systems. This provides guidance to knowledge engineers and software engineers in the process of designing, identifying, and categorising resources required for the development process. To the best of our knowledge, such support does not exist. We address this research gap by developing a conceptual framework that identifies and categorises the aspects of real-world planning domains in varying levels of granularity. Our framework provides not only a common terminology but also a comprehensive overview of a broad range of planning aspects exemplified using the domain of sustainable buildings as a prominent application domain of AI planning. The framework has the potential to impact the design, development, and applicability of AI planning systems in real-world application domains.","sentences":["Planning is a pivotal ability of any intelligent system being developed for real-world applications.","AI planning is concerned with researching and developing planning systems that automatically compute plans that satisfy some user objective.","Identifying and understanding the relevant and realistic aspects that characterise real-world application domains are crucial to the development of AI planning systems.","This provides guidance to knowledge engineers and software engineers in the process of designing, identifying, and categorising resources required for the development process.","To the best of our knowledge, such support does not exist.","We address this research gap by developing a conceptual framework that identifies and categorises the aspects of real-world planning domains in varying levels of granularity.","Our framework provides not only a common terminology but also a comprehensive overview of a broad range of planning aspects exemplified using the domain of sustainable buildings as a prominent application domain of AI planning.","The framework has the potential to impact the design, development, and applicability of AI planning systems in real-world application domains."],"url":"http://arxiv.org/abs/2307.04701v1"}
{"created":"2023-07-10 16:55:55","title":"International Institutions for Advanced AI","abstract":"International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.","sentences":["International institutions may have an important role to play in ensuring advanced AI systems benefit humanity.","International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits.","Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose.","This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards.","It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research.","We explore the utility of these models and identify open questions about their viability."],"url":"http://arxiv.org/abs/2307.04699v1"}
{"created":"2023-07-10 16:50:58","title":"Cobalt: Optimizing Mining Rewards in Proof-of-Work Network Games","abstract":"Mining in proof-of-work blockchains has become an expensive affair requiring specialized hardware capable of executing several megahashes per second at huge electricity costs. Miners earn a reward each time they mine a block within the longest chain, which helps offset their mining costs. It is therefore of interest to miners to maximize the number of mined blocks in the blockchain and increase revenue. A key factor affecting mining rewards earned is the connectivity between miners in the peer-to-peer network. To maximize rewards a miner must choose its network connections carefully, ensuring existence of paths to other miners that are on average of a lower latency compared to paths between other miners. We formulate the problem of deciding whom to connect to for miners as a combinatorial bandit problem. Each node picks its neighbors strategically to minimize the latency to reach 90\\% of the hash power of the network relative to the 90-th percentile latency from other nodes. A key contribution of our work is the use of a network coordinates based model for learning the network structure within the bandit algorithm. Experimentally we show our proposed algorithm outperforming or matching baselines on diverse network settings.","sentences":["Mining in proof-of-work blockchains has become an expensive affair requiring specialized hardware capable of executing several megahashes per second at huge electricity costs.","Miners earn a reward each time they mine a block within the longest chain, which helps offset their mining costs.","It is therefore of interest to miners to maximize the number of mined blocks in the blockchain and increase revenue.","A key factor affecting mining rewards earned is the connectivity between miners in the peer-to-peer network.","To maximize rewards a miner must choose its network connections carefully, ensuring existence of paths to other miners that are on average of a lower latency compared to paths between other miners.","We formulate the problem of deciding whom to connect to for miners as a combinatorial bandit problem.","Each node picks its neighbors strategically to minimize the latency to reach 90\\% of the hash power of the network relative to the 90-th percentile latency from other nodes.","A key contribution of our work is the use of a network coordinates based model for learning the network structure within the bandit algorithm.","Experimentally we show our proposed algorithm outperforming or matching baselines on diverse network settings."],"url":"http://arxiv.org/abs/2307.04695v1"}
{"created":"2023-07-10 16:46:34","title":"COMEX: A Tool for Generating Customized Source Code Representations","abstract":"Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE.   Tool: https://pypi.org/project/comex - GitHub: https://github.com/IBM/tree-sitter-codeviews - Demo: https://youtu.be/GER6U87FVbU","sentences":["Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system.","Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks.","However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language.","Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc.","Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming.","To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks.","Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages.","We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE.   ","Tool: https://pypi.org/project/comex - GitHub: https://github.com/IBM/tree-sitter-codeviews - Demo: https://youtu.be/GER6U87FVbU"],"url":"http://arxiv.org/abs/2307.04693v1"}
{"created":"2023-07-10 16:42:03","title":"VampNet: Music Generation via Masked Acoustic Token Modeling","abstract":"We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.","sentences":["We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation.","We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference.","VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass.","With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms.","We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping).","Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music.","This flexible prompting capability makes VampNet a powerful music co-creation tool.","Code and audio samples are available online."],"url":"http://arxiv.org/abs/2307.04686v1"}
{"created":"2023-07-10 16:37:46","title":"FreeDrag: Point Tracking is Not You Need for Interactive Point-based Image Editing","abstract":"To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable. Recently, DragGAN has achieved impressive editing results through point-based manipulation. However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points. To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN. The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing. Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar structures, fine details, or under multi-point targets.","sentences":["To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable.","Recently, DragGAN has achieved impressive editing results through point-based manipulation.","However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points.","To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN.","The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing.","Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar structures, fine details, or under multi-point targets."],"url":"http://arxiv.org/abs/2307.04684v1"}
{"created":"2023-07-10 16:29:05","title":"Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles","abstract":"In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle. Our analysis relies on the regularity of the gradient w.r.t. the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization. These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle. As a consequence, our analysis provides a precise meaning to the intuition that optimization of the statistical learning objective is as hard as the estimation of its gradient. Finally, we show that, in the case of standard supervised learning, mini-batch gradient descent with increasing batch sizes and a warm start can reach a generalization error that is optimal up to a multiplicative factor, thus motivating the use of this optimization scheme in practical applications.","sentences":["In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle.","Our analysis relies on the regularity of the gradient w.r.t.","the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization.","These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption.","In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle.","As a consequence, our analysis provides a precise meaning to the intuition that optimization of the statistical learning objective is as hard as the estimation of its gradient.","Finally, we show that, in the case of standard supervised learning, mini-batch gradient descent with increasing batch sizes and a warm start can reach a generalization error that is optimal up to a multiplicative factor, thus motivating the use of this optimization scheme in practical applications."],"url":"http://arxiv.org/abs/2307.04679v1"}
{"created":"2023-07-10 16:21:35","title":"Practical Trustworthiness Model for DNN in Dedicated 6G Application","abstract":"Artificial intelligence (AI) is considered an efficient response to several challenges facing 6G technology. However, AI still suffers from a huge trust issue due to its ambiguous way of making predictions. Therefore, there is a need for a method to evaluate the AI's trustworthiness in practice for future 6G applications. This paper presents a practical model to analyze the trustworthiness of AI in a dedicated 6G application. In particular, we present two customized Deep Neural Networks (DNNs) to solve the Automatic Modulation Recognition (AMR) problem in Terahertz communications-based 6G technology. Then, a specific trustworthiness model and its attributes, namely data robustness, parameter sensitivity, and security covering adversarial examples, are introduced. The evaluation results indicate that the proposed trustworthiness attributes are crucial to evaluate the trustworthiness of DNN for this 6G application.","sentences":["Artificial intelligence (AI) is considered an efficient response to several challenges facing 6G technology.","However, AI still suffers from a huge trust issue due to its ambiguous way of making predictions.","Therefore, there is a need for a method to evaluate the AI's trustworthiness in practice for future 6G applications.","This paper presents a practical model to analyze the trustworthiness of AI in a dedicated 6G application.","In particular, we present two customized Deep Neural Networks (DNNs) to solve the Automatic Modulation Recognition (AMR) problem in Terahertz communications-based 6G technology.","Then, a specific trustworthiness model and its attributes, namely data robustness, parameter sensitivity, and security covering adversarial examples, are introduced.","The evaluation results indicate that the proposed trustworthiness attributes are crucial to evaluate the trustworthiness of DNN for this 6G application."],"url":"http://arxiv.org/abs/2307.04677v1"}
{"created":"2023-07-10 16:21:05","title":"LINFA: a Python library for variational inference with normalizing flow and annealing","abstract":"Variational inference is an increasingly popular method in statistics and machine learning for approximating probability distributions. We developed LINFA (Library for Inference with Normalizing Flow and Annealing), a Python library for variational inference to accommodate computationally expensive models and difficult-to-sample distributions with dependent parameters. We discuss the theoretical background, capabilities, and performance of LINFA in various benchmarks. LINFA is publicly available on GitHub at https://github.com/desResLab/LINFA.","sentences":["Variational inference is an increasingly popular method in statistics and machine learning for approximating probability distributions.","We developed LINFA (Library for Inference with Normalizing Flow and Annealing), a Python library for variational inference to accommodate computationally expensive models and difficult-to-sample distributions with dependent parameters.","We discuss the theoretical background, capabilities, and performance of LINFA in various benchmarks.","LINFA is publicly available on GitHub at https://github.com/desResLab/LINFA."],"url":"http://arxiv.org/abs/2307.04675v1"}
{"created":"2023-07-10 16:20:20","title":"Optimal Robot Path Planning In a Collaborative Human-Robot Team with Intermittent Human Availability","abstract":"This paper presents a solution for the problem of optimal planning for a robot in a collaborative human-robot team, where the human supervisor is intermittently available to assist the robot in completing tasks more quickly. Specifically, we address the challenge of computing the fastest path between two configurations in an environment with time constraints on how long the robot can wait for assistance. To solve this problem, we propose a novel approach that utilizes the concepts of budget and critical departure times, which enables us to obtain optimal solutions while scaling to larger problem instances than existing methods. We demonstrate the effectiveness of our approach by comparing it with several baseline algorithms on a city road network and analyzing the quality of the solutions obtained. Our work contributes to the field of robot planning by addressing the critical issue of incorporating human assistance and environmental restrictions, which has significant implications for real-world applications.","sentences":["This paper presents a solution for the problem of optimal planning for a robot in a collaborative human-robot team, where the human supervisor is intermittently available to assist the robot in completing tasks more quickly.","Specifically, we address the challenge of computing the fastest path between two configurations in an environment with time constraints on how long the robot can wait for assistance.","To solve this problem, we propose a novel approach that utilizes the concepts of budget and critical departure times, which enables us to obtain optimal solutions while scaling to larger problem instances than existing methods.","We demonstrate the effectiveness of our approach by comparing it with several baseline algorithms on a city road network and analyzing the quality of the solutions obtained.","Our work contributes to the field of robot planning by addressing the critical issue of incorporating human assistance and environmental restrictions, which has significant implications for real-world applications."],"url":"http://arxiv.org/abs/2307.04674v1"}
{"created":"2023-07-10 16:11:33","title":"Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach","abstract":"The rise of social media platforms has facilitated the formation of echo chambers, which are online spaces where users predominantly encounter viewpoints that reinforce their existing beliefs while excluding dissenting perspectives. This phenomenon significantly hinders information dissemination across communities and fuels societal polarization. Therefore, it is crucial to develop methods for quantifying echo chambers. In this paper, we present the Echo Chamber Score (ECS), a novel metric that assesses the cohesion and separation of user communities by measuring distances between users in the embedding space. In contrast to existing approaches, ECS is able to function without labels for user ideologies and makes no assumptions about the structure of the interaction graph. To facilitate measuring distances between users, we propose EchoGAE, a self-supervised graph autoencoder-based user embedding model that leverages users' posts and the interaction graph to embed them in a manner that reflects their ideological similarity. To assess the effectiveness of ECS, we use a Twitter dataset consisting of four topics - two polarizing and two non-polarizing. Our results showcase ECS's effectiveness as a tool for quantifying echo chambers and shedding light on the dynamics of online discourse.","sentences":["The rise of social media platforms has facilitated the formation of echo chambers, which are online spaces where users predominantly encounter viewpoints that reinforce their existing beliefs while excluding dissenting perspectives.","This phenomenon significantly hinders information dissemination across communities and fuels societal polarization.","Therefore, it is crucial to develop methods for quantifying echo chambers.","In this paper, we present the Echo Chamber Score (ECS), a novel metric that assesses the cohesion and separation of user communities by measuring distances between users in the embedding space.","In contrast to existing approaches, ECS is able to function without labels for user ideologies and makes no assumptions about the structure of the interaction graph.","To facilitate measuring distances between users, we propose EchoGAE, a self-supervised graph autoencoder-based user embedding model that leverages users' posts and the interaction graph to embed them in a manner that reflects their ideological similarity.","To assess the effectiveness of ECS, we use a Twitter dataset consisting of four topics - two polarizing and two non-polarizing.","Our results showcase ECS's effectiveness as a tool for quantifying echo chambers and shedding light on the dynamics of online discourse."],"url":"http://arxiv.org/abs/2307.04668v1"}
{"created":"2023-07-10 15:59:09","title":"On the power of graph neural networks and the role of the activation function","abstract":"In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of depth two (our results hold for activations like the sigmoid, hyperbolic tan and others). This shows how the power of graph neural networks can change drastically if one changes the activation function of the neural networks. The proof of this result utilizes the Lindemann-Weierstrauss theorem from transcendental number theory.","sentences":["In this article we present new results about the expressivity of Graph Neural Networks (GNNs).","We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations.","The proof relies on tools from the algebra of symmetric polynomials.","In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations.","Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021].","We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of depth two (our results hold for activations like the sigmoid, hyperbolic tan and others).","This shows how the power of graph neural networks can change drastically if one changes the activation function of the neural networks.","The proof of this result utilizes the Lindemann-Weierstrauss theorem from transcendental number theory."],"url":"http://arxiv.org/abs/2307.04661v1"}
{"created":"2023-07-10 15:56:17","title":"BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset","abstract":"In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.","sentences":["In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs).","This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes.","In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics.","We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs.","We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs.","Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails."],"url":"http://arxiv.org/abs/2307.04657v1"}
{"created":"2023-07-10 15:51:44","title":"Declarative Linearizability Proofs for Descriptor-Based Concurrent Helping Algorithms","abstract":"Linearizability is a standard correctness criterion for concurrent algorithms, typically proved by establishing the algorithms' linearization points. However, relying on linearization points leads to proofs that are implementation-dependent, and thus hinder abstraction and reuse. In this paper we show that one can develop more declarative proofs by foregoing linearization points and instead relying on a technique of axiomatization of visibility relations. While visibility relations have been considered before, ours is the first study where the challenge is to formalize the helping nature of the algorithms. In particular, we show that by axiomatizing the properties of separation between events that contain bunches of help requests, we can extract what is common for high-level understanding of several descriptor-based helping algorithms of Harris et al. (RDCSS, MCAS, and optimizations), and produce novel proofs of their linearizability that share significant components.","sentences":["Linearizability is a standard correctness criterion for concurrent algorithms, typically proved by establishing the algorithms' linearization points.","However, relying on linearization points leads to proofs that are implementation-dependent, and thus hinder abstraction and reuse.","In this paper we show that one can develop more declarative proofs by foregoing linearization points and instead relying on a technique of axiomatization of visibility relations.","While visibility relations have been considered before, ours is the first study where the challenge is to formalize the helping nature of the algorithms.","In particular, we show that by axiomatizing the properties of separation between events that contain bunches of help requests, we can extract what is common for high-level understanding of several descriptor-based helping algorithms of Harris et al. (RDCSS, MCAS, and optimizations), and produce novel proofs of their linearizability that share significant components."],"url":"http://arxiv.org/abs/2307.04653v1"}
{"created":"2023-07-10 15:49:37","title":"Joint Salient Object Detection and Camouflaged Object Detection via Uncertainty-aware Learning","abstract":"Salient objects attract human attention and usually stand out clearly from their surroundings. In contrast, camouflaged objects share similar colors or textures with the environment. In this case, salient objects are typically non-camouflaged, and camouflaged objects are usually not salient. Due to this inherent contradictory attribute, we introduce an uncertainty-aware learning pipeline to extensively explore the contradictory information of salient object detection (SOD) and camouflaged object detection (COD) via data-level and task-wise contradiction modeling. We first exploit the dataset correlation of these two tasks and claim that the easy samples in the COD dataset can serve as hard samples for SOD to improve the robustness of the SOD model. Based on the assumption that these two models should lead to activation maps highlighting different regions of the same input image, we further introduce a contrastive module with a joint-task contrastive learning framework to explicitly model the contradictory attributes of these two tasks. Different from conventional intra-task contrastive learning for unsupervised representation learning, our contrastive module is designed to model the task-wise correlation, leading to cross-task representation learning. To better understand the two tasks from the perspective of uncertainty, we extensively investigate the uncertainty estimation techniques for modeling the main uncertainties of the two tasks, namely task uncertainty (for SOD) and data uncertainty (for COD), and aiming to effectively estimate the challenging regions for each task to achieve difficulty-aware learning. Experimental results on benchmark datasets demonstrate that our solution leads to both state-of-the-art performance and informative uncertainty estimation.","sentences":["Salient objects attract human attention and usually stand out clearly from their surroundings.","In contrast, camouflaged objects share similar colors or textures with the environment.","In this case, salient objects are typically non-camouflaged, and camouflaged objects are usually not salient.","Due to this inherent contradictory attribute, we introduce an uncertainty-aware learning pipeline to extensively explore the contradictory information of salient object detection (SOD) and camouflaged object detection (COD) via data-level and task-wise contradiction modeling.","We first exploit the dataset correlation of these two tasks and claim that the easy samples in the COD dataset can serve as hard samples for SOD to improve the robustness of the SOD model.","Based on the assumption that these two models should lead to activation maps highlighting different regions of the same input image, we further introduce a contrastive module with a joint-task contrastive learning framework to explicitly model the contradictory attributes of these two tasks.","Different from conventional intra-task contrastive learning for unsupervised representation learning, our contrastive module is designed to model the task-wise correlation, leading to cross-task representation learning.","To better understand the two tasks from the perspective of uncertainty, we extensively investigate the uncertainty estimation techniques for modeling the main uncertainties of the two tasks, namely task uncertainty (for SOD) and data uncertainty (for COD), and aiming to effectively estimate the challenging regions for each task to achieve difficulty-aware learning.","Experimental results on benchmark datasets demonstrate that our solution leads to both state-of-the-art performance and informative uncertainty estimation."],"url":"http://arxiv.org/abs/2307.04651v1"}
{"created":"2023-07-10 15:40:49","title":"Fairness and Diversity in Recommender Systems: A Survey","abstract":"Recommender systems are effective tools for mitigating information overload and have seen extensive applications across various domains. However, the single focus on utility goals proves to be inadequate in addressing real-world concerns, leading to increasing attention to fairness-aware and diversity-aware recommender systems. While most existing studies explore fairness and diversity independently, we identify strong connections between these two domains. In this survey, we first discuss each of them individually and then dive into their connections. Additionally, motivated by the concepts of user-level and item-level fairness, we broaden the understanding of diversity to encompass not only the item level but also the user level. With this expanded perspective on user and item-level diversity, we re-interpret fairness studies from the viewpoint of diversity. This fresh perspective enhances our understanding of fairness-related work and paves the way for potential future research directions. Papers discussed in this survey along with public code links are available at https://github.com/YuyingZhao/Awesome-Fairness-and-Diversity-Papers-in-Recommender-Systems .","sentences":["Recommender systems are effective tools for mitigating information overload and have seen extensive applications across various domains.","However, the single focus on utility goals proves to be inadequate in addressing real-world concerns, leading to increasing attention to fairness-aware and diversity-aware recommender systems.","While most existing studies explore fairness and diversity independently, we identify strong connections between these two domains.","In this survey, we first discuss each of them individually and then dive into their connections.","Additionally, motivated by the concepts of user-level and item-level fairness, we broaden the understanding of diversity to encompass not only the item level but also the user level.","With this expanded perspective on user and item-level diversity, we re-interpret fairness studies from the viewpoint of diversity.","This fresh perspective enhances our understanding of fairness-related work and paves the way for potential future research directions.","Papers discussed in this survey along with public code links are available at https://github.com/YuyingZhao/Awesome-Fairness-and-Diversity-Papers-in-Recommender-Systems ."],"url":"http://arxiv.org/abs/2307.04644v1"}
{"created":"2023-07-10 15:35:31","title":"Multimodal brain age estimation using interpretable adaptive population-graph learning","abstract":"Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeline can be trained end-to-end. Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph. We use the UK Biobank, which provides a large variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification. The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods. We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature.","sentences":["Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's.","Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks.","A population graph is usually static and constructed manually using non-imaging information.","However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure.","In this work, we propose a framework that learns a population graph structure optimized for the downstream task.","An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction.","The resulting graph is used to train the GCN.","The entire pipeline can be trained end-to-end.","Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph.","We use the UK Biobank, which provides a large variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification.","The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods.","We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature."],"url":"http://arxiv.org/abs/2307.04639v1"}
{"created":"2023-07-10 15:21:57","title":"Toward optimal placement of spatial sensors","abstract":"This paper addresses the challenges of optimally placing a finite number of sensors to detect Poisson-distributed targets in a bounded domain. We seek to rigorously account for uncertainty in the target arrival model throughout the problem. Sensor locations are selected to maximize the probability that no targets are missed. While this objective function is well-suited to applications where failure to detect targets is highly undesirable, it does not lead to a computationally efficient optimization problem. We propose an approximation of the objective function that is non-negative, submodular, and monotone and for which greedy selection of sensor locations works well. We also characterize the gap between the desired objective function and our approximation. For numerical illustrations, we consider the case of the detection of ship traffic using sensors mounted on the seafloor.","sentences":["This paper addresses the challenges of optimally placing a finite number of sensors to detect Poisson-distributed targets in a bounded domain.","We seek to rigorously account for uncertainty in the target arrival model throughout the problem.","Sensor locations are selected to maximize the probability that no targets are missed.","While this objective function is well-suited to applications where failure to detect targets is highly undesirable, it does not lead to a computationally efficient optimization problem.","We propose an approximation of the objective function that is non-negative, submodular, and monotone and for which greedy selection of sensor locations works well.","We also characterize the gap between the desired objective function and our approximation.","For numerical illustrations, we consider the case of the detection of ship traffic using sensors mounted on the seafloor."],"url":"http://arxiv.org/abs/2307.04634v1"}
{"created":"2023-07-10 15:19:37","title":"An End-To-End Analysis of Deep Learning-Based Remaining Useful Life Algorithms for Satefy-Critical 5G-Enabled IIoT Networks","abstract":"Remaining Useful Life (RUL) prediction is a critical task that aims to estimate the amount of time until a system fails, where the latter is formed by three main components, that is, the application, communication network, and RUL logic. In this paper, we provide an end-to-end analysis of an entire RUL-based chain. Specifically, we consider a factory floor where Automated Guided Vehicles (AGVs) transport dangerous liquids whose fall may cause injuries to workers. Regarding the communication infrastructure, the AGVs are equipped with 5G User Equipments (UEs) that collect real-time data of their movements and send them to an application server. The RUL logic consists of a Deep Learning (DL)-based pipeline that assesses if there will be liquid falls by analyzing the collected data, and, eventually, sending commands to the AGVs to avoid such a danger. According to this scenario, we performed End-to-End 5G NR-compliant network simulations to study the Round-Trip Time (RTT) as a function of the overall system bandwidth, subcarrier spacing, and modulation order. Then, via real-world experiments, we collect data to train, test and compare 7 DL models and 1 baseline threshold-based algorithm in terms of cost and average advance. Finally, we assess whether or not the RTT provided by four different 5G NR network architectures is compatible with the average advance provided by the best-performing one-Dimensional Convolutional Neural Network (1D-CNN). Numerical results show under which conditions the DL-based approach for RUL estimation matches with the RTT performance provided by different 5G NR network architectures.","sentences":["Remaining Useful Life (RUL) prediction is a critical task that aims to estimate the amount of time until a system fails, where the latter is formed by three main components, that is, the application, communication network, and RUL logic.","In this paper, we provide an end-to-end analysis of an entire RUL-based chain.","Specifically, we consider a factory floor where Automated Guided Vehicles (AGVs) transport dangerous liquids whose fall may cause injuries to workers.","Regarding the communication infrastructure, the AGVs are equipped with 5G User Equipments (UEs) that collect real-time data of their movements and send them to an application server.","The RUL logic consists of a Deep Learning (DL)-based pipeline that assesses if there will be liquid falls by analyzing the collected data, and, eventually, sending commands to the AGVs to avoid such a danger.","According to this scenario, we performed End-to-End 5G NR-compliant network simulations to study the Round-Trip Time (RTT) as a function of the overall system bandwidth, subcarrier spacing, and modulation order.","Then, via real-world experiments, we collect data to train, test and compare 7 DL models and 1 baseline threshold-based algorithm in terms of cost and average advance.","Finally, we assess whether or not the RTT provided by four different 5G NR network architectures is compatible with the average advance provided by the best-performing one-Dimensional Convolutional Neural Network (1D-CNN).","Numerical results show under which conditions the DL-based approach for RUL estimation matches with the RTT performance provided by different 5G NR network architectures."],"url":"http://arxiv.org/abs/2307.04632v1"}
{"created":"2023-07-10 15:15:17","title":"The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech Translation Task","abstract":"This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech translation (S2ST) task which aims to translate from English speech of multi-source to Chinese speech. The system is built in a cascaded manner consisting of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS). We make tremendous efforts to handle the challenging multi-source input. Specifically, to improve the robustness to multi-source speech input, we adopt various data augmentation strategies and a ROVER-based score fusion on multiple ASR model outputs. To better handle the noisy ASR transcripts, we introduce a three-stage fine-tuning strategy to improve translation accuracy. Finally, we build a TTS model with high naturalness and sound quality, which leverages a two-stage framework, using network bottleneck features as a robust intermediate representation for speaker timbre and linguistic content disentanglement. Based on the two-stage framework, pre-trained speaker embedding is leveraged as a condition to transfer the speaker timbre in the source English speech to the translated Chinese speech. Experimental results show that our system has high translation accuracy, speech naturalness, sound quality, and speaker similarity. Moreover, it shows good robustness to multi-source data.","sentences":["This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech translation (S2ST) task which aims to translate from English speech of multi-source to Chinese speech.","The system is built in a cascaded manner consisting of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS).","We make tremendous efforts to handle the challenging multi-source input.","Specifically, to improve the robustness to multi-source speech input, we adopt various data augmentation strategies and a ROVER-based score fusion on multiple ASR model outputs.","To better handle the noisy ASR transcripts, we introduce a three-stage fine-tuning strategy to improve translation accuracy.","Finally, we build a TTS model with high naturalness and sound quality, which leverages a two-stage framework, using network bottleneck features as a robust intermediate representation for speaker timbre and linguistic content disentanglement.","Based on the two-stage framework, pre-trained speaker embedding is leveraged as a condition to transfer the speaker timbre in the source English speech to the translated Chinese speech.","Experimental results show that our system has high translation accuracy, speech naturalness, sound quality, and speaker similarity.","Moreover, it shows good robustness to multi-source data."],"url":"http://arxiv.org/abs/2307.04630v1"}
{"created":"2023-07-10 15:14:13","title":"Tight Algorithmic Applications of Clique-Width Generalizations","abstract":"In this work, we study two natural generalizations of clique-width introduced by Martin F\\\"urer. Multi-clique-width (mcw) allows every vertex to hold multiple labels [ITCS 2017], while for fusion-width (fw) we have a possibility to merge all vertices of a certain label [LATIN 2014]. F\\\"urer has shown that both parameters are upper-bounded by treewidth thus making them more appealing from an algorithmic perspective than clique-width and asked for applications of these parameters for problem solving. First, we determine the relation between these two parameters by showing that $\\operatorname{mcw} \\leq \\operatorname{fw} + 1$. Then we show that when parameterized by multi-clique-width, many problems (e.g., Connected Dominating Set) admit algorithms with the same running time as for clique-width despite the exponential gap between these two parameters. For some problems (e.g., Hamiltonian Cycle) we show an analogous result for fusion-width: For this we present an alternative view on fusion-width by introducing so-called glue-expressions which might be interesting on their own. All algorithms obtained in this work are tight up to (Strong) Exponential Time Hypothesis.","sentences":["In this work, we study two natural generalizations of clique-width introduced by Martin F\\\"urer.","Multi-clique-width (mcw) allows every vertex to hold multiple labels","[ITCS 2017], while for fusion-width (fw) we have a possibility to merge all vertices of a certain label [LATIN 2014].","F\\\"urer has shown that both parameters are upper-bounded by treewidth thus making them more appealing from an algorithmic perspective than clique-width and asked for applications of these parameters for problem solving.","First, we determine the relation between these two parameters by showing that $\\operatorname{mcw} \\leq \\operatorname{fw} +","1$.","Then we show that when parameterized by multi-clique-width, many problems (e.g., Connected Dominating Set) admit algorithms with the same running time as for clique-width despite the exponential gap between these two parameters.","For some problems (e.g., Hamiltonian Cycle) we show an analogous result for fusion-width: For this we present an alternative view on fusion-width by introducing so-called glue-expressions which might be interesting on their own.","All algorithms obtained in this work are tight up to (Strong) Exponential Time Hypothesis."],"url":"http://arxiv.org/abs/2307.04628v1"}
{"created":"2023-07-10 15:10:56","title":"Measuring Lexical Diversity in Texts: The Twofold Length Problem","abstract":"The impact of text length on the estimation of lexical diversity has captured the attention of the scientific community for more than a century. Numerous indices have been proposed, and many studies have been conducted to evaluate them, but the problem remains. This methodological review provides a critical analysis not only of the most commonly used indices in language learning studies, but also of the length problem itself, as well as of the methodology for evaluating the proposed solutions. The analysis of three datasets of English language-learners' texts revealed that indices that reduce all texts to the same length using a probabilistic or an algorithmic approach solve the length dependency problem; however, all these indices failed to address the second problem, which is their sensitivity to the parameter that determines the length to which the texts are reduced. The paper concludes with recommendations for optimizing lexical diversity analysis.","sentences":["The impact of text length on the estimation of lexical diversity has captured the attention of the scientific community for more than a century.","Numerous indices have been proposed, and many studies have been conducted to evaluate them, but the problem remains.","This methodological review provides a critical analysis not only of the most commonly used indices in language learning studies, but also of the length problem itself, as well as of the methodology for evaluating the proposed solutions.","The analysis of three datasets of English language-learners' texts revealed that indices that reduce all texts to the same length using a probabilistic or an algorithmic approach solve the length dependency problem; however, all these indices failed to address the second problem, which is their sensitivity to the parameter that determines the length to which the texts are reduced.","The paper concludes with recommendations for optimizing lexical diversity analysis."],"url":"http://arxiv.org/abs/2307.04626v1"}
{"created":"2023-07-10 15:07:29","title":"Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data","abstract":"This work develops a data-efficient learning from demonstration framework which exploits the use of rich tactile sensing and achieves fine dexterous bimanual manipulation. Specifically, we formulated a convolutional autoencoder network that can effectively extract and encode high-dimensional tactile information. Further, we developed a behaviour cloning network that can learn human-like sensorimotor skills demonstrated directly on the robot hardware in the task space by fusing both proprioceptive and tactile feedback. Our comparison study with the baseline method revealed the effectiveness of the contact information, which enabled successful extraction and replication of the demonstrated motor skills. Extensive experiments on real dual-arm robots demonstrated the robustness and effectiveness of the fine pinch grasp policy directly learned from one-shot demonstration, including grasping of the same object with different initial poses, generalizing to ten unseen new objects, robust and firm grasping against external pushes, as well as contact-aware and reactive re-grasping in case of dropping objects under very large perturbations. Moreover, the saliency map method is employed to describe the weight distribution across various modalities during pinch grasping. The video is available online at: \\href{https://youtu.be/4Pg29bUBKqs}{https://youtu.be/4Pg29bUBKqs}.","sentences":["This work develops a data-efficient learning from demonstration framework which exploits the use of rich tactile sensing and achieves fine dexterous bimanual manipulation.","Specifically, we formulated a convolutional autoencoder network that can effectively extract and encode high-dimensional tactile information.","Further, we developed a behaviour cloning network that can learn human-like sensorimotor skills demonstrated directly on the robot hardware in the task space by fusing both proprioceptive and tactile feedback.","Our comparison study with the baseline method revealed the effectiveness of the contact information, which enabled successful extraction and replication of the demonstrated motor skills.","Extensive experiments on real dual-arm robots demonstrated the robustness and effectiveness of the fine pinch grasp policy directly learned from one-shot demonstration, including grasping of the same object with different initial poses, generalizing to ten unseen new objects, robust and firm grasping against external pushes, as well as contact-aware and reactive re-grasping in case of dropping objects under very large perturbations.","Moreover, the saliency map method is employed to describe the weight distribution across various modalities during pinch grasping.","The video is available online at: \\href{https://youtu.be/4Pg29bUBKqs}{https://youtu.be/4Pg29bUBKqs}."],"url":"http://arxiv.org/abs/2307.04619v1"}
{"created":"2023-07-10 15:02:13","title":"Weakly-supervised positional contrastive learning: application to cirrhosis classification","abstract":"Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations, and small strongly-labeled (i.e., high-confidence) datasets. The proposed model improves the classification AUC by 5% with respect to a baseline model on our internal dataset, and by 26% on the public LIHC dataset from the Cancer Genome Atlas. The code is available at: https://github.com/Guerbet-AI/wsp-contrastive.","sentences":["Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores).","Access to high-confidence labels, such as histology-based diagnoses, is rare and costly.","Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets.","These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory.","Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications.","In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function.","We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations, and small strongly-labeled (i.e., high-confidence) datasets.","The proposed model improves the classification AUC by 5% with respect to a baseline model on our internal dataset, and by 26% on the public LIHC dataset from the Cancer Genome Atlas.","The code is available at: https://github.com/Guerbet-AI/wsp-contrastive."],"url":"http://arxiv.org/abs/2307.04617v1"}
{"created":"2023-07-10 14:58:10","title":"MiVOLO: Multi-input Transformer for Age and Gender Estimation","abstract":"Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded. We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer. Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data. This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image. To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities. Additionally, we introduce a novel benchmark based on images from the Open Images Dataset. The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes. Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges. Finally, we grant public access to our models, along with the code for validation and inference. In addition, we provide extra annotations for used datasets and introduce our new benchmark.","sentences":["Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded.","We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer.","Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data.","This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image.","To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities.","Additionally, we introduce a novel benchmark based on images from the Open Images Dataset.","The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes.","Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges.","Finally, we grant public access to our models, along with the code for validation and inference.","In addition, we provide extra annotations for used datasets and introduce our new benchmark."],"url":"http://arxiv.org/abs/2307.04616v1"}
{"created":"2023-07-10 14:55:55","title":"Encapsulation Structure and Dynamics in Hypergraphs","abstract":"Hypergraphs have emerged as a powerful modeling framework to represent systems with multiway interactions, that is systems where interactions may involve an arbitrary number of agents. Here we explore the properties of real-world hypergraphs, focusing on the encapsulation of their hyperedges, which is the extent that smaller hyperedges are subsets of larger hyperedges. Building on the concept of line graphs, our measures quantify the relations existing between hyperedges of different sizes and, as a byproduct, the compatibility of the data with a simplicial complex representation -- whose encapsulation would be maximum. We then turn to the impact of the observed structural patterns on diffusive dynamics, focusing on a variant of threshold models, called encapsulation dynamics, and demonstrate that non-random patterns can accelerate the spreading in the system.","sentences":["Hypergraphs have emerged as a powerful modeling framework to represent systems with multiway interactions, that is systems where interactions may involve an arbitrary number of agents.","Here we explore the properties of real-world hypergraphs, focusing on the encapsulation of their hyperedges, which is the extent that smaller hyperedges are subsets of larger hyperedges.","Building on the concept of line graphs, our measures quantify the relations existing between hyperedges of different sizes and, as a byproduct, the compatibility of the data with a simplicial complex representation -- whose encapsulation would be maximum.","We then turn to the impact of the observed structural patterns on diffusive dynamics, focusing on a variant of threshold models, called encapsulation dynamics, and demonstrate that non-random patterns can accelerate the spreading in the system."],"url":"http://arxiv.org/abs/2307.04613v1"}
{"created":"2023-07-10 14:53:24","title":"SPLAL: Similarity-based pseudo-labeling with alignment loss for semi-supervised medical image classification","abstract":"Medical image classification is a challenging task due to the scarcity of labeled samples and class imbalance caused by the high variance in disease prevalence. Semi-supervised learning (SSL) methods can mitigate these challenges by leveraging both labeled and unlabeled data. However, SSL methods for medical image classification need to address two key challenges: (1) estimating reliable pseudo-labels for the images in the unlabeled dataset and (2) reducing biases caused by class imbalance. In this paper, we propose a novel SSL approach, SPLAL, that effectively addresses these challenges. SPLAL leverages class prototypes and a weighted combination of classifiers to predict reliable pseudo-labels over a subset of unlabeled images. Additionally, we introduce alignment loss to mitigate model biases toward majority classes. To evaluate the performance of our proposed approach, we conduct experiments on two publicly available medical image classification benchmark datasets: the skin lesion classification (ISIC 2018) and the blood cell classification dataset (BCCD). The experimental results empirically demonstrate that our approach outperforms several state-of-the-art SSL methods over various evaluation metrics. Specifically, our proposed approach achieves a significant improvement over the state-of-the-art approach on the ISIC 2018 dataset in both Accuracy and F1 score, with relative margins of 2.24\\% and 11.40\\%, respectively. Finally, we conduct extensive ablation experiments to examine the contribution of different components of our approach, validating its effectiveness.","sentences":["Medical image classification is a challenging task due to the scarcity of labeled samples and class imbalance caused by the high variance in disease prevalence.","Semi-supervised learning (SSL) methods can mitigate these challenges by leveraging both labeled and unlabeled data.","However, SSL methods for medical image classification need to address two key challenges: (1) estimating reliable pseudo-labels for the images in the unlabeled dataset and (2) reducing biases caused by class imbalance.","In this paper, we propose a novel SSL approach, SPLAL, that effectively addresses these challenges.","SPLAL leverages class prototypes and a weighted combination of classifiers to predict reliable pseudo-labels over a subset of unlabeled images.","Additionally, we introduce alignment loss to mitigate model biases toward majority classes.","To evaluate the performance of our proposed approach, we conduct experiments on two publicly available medical image classification benchmark datasets: the skin lesion classification (ISIC 2018) and the blood cell classification dataset (BCCD).","The experimental results empirically demonstrate that our approach outperforms several state-of-the-art SSL methods over various evaluation metrics.","Specifically, our proposed approach achieves a significant improvement over the state-of-the-art approach on the ISIC 2018 dataset in both Accuracy and F1 score, with relative margins of 2.24\\% and 11.40\\%, respectively.","Finally, we conduct extensive ablation experiments to examine the contribution of different components of our approach, validating its effectiveness."],"url":"http://arxiv.org/abs/2307.04610v1"}
{"created":"2023-07-10 14:52:14","title":"Learning Interpretable Heuristics for WalkSAT","abstract":"Local search algorithms are well-known methods for solving large, hard instances of the satisfiability problem (SAT). The performance of these algorithms crucially depends on heuristics for setting noise parameters and scoring variables. The optimal setting for these heuristics varies for different instance distributions. In this paper, we present an approach for learning effective variable scoring functions and noise parameters by using reinforcement learning. We consider satisfiability problems from different instance distributions and learn specialized heuristics for each of them. Our experimental results show improvements with respect to both a WalkSAT baseline and another local search learned heuristic.","sentences":["Local search algorithms are well-known methods for solving large, hard instances of the satisfiability problem (SAT).","The performance of these algorithms crucially depends on heuristics for setting noise parameters and scoring variables.","The optimal setting for these heuristics varies for different instance distributions.","In this paper, we present an approach for learning effective variable scoring functions and noise parameters by using reinforcement learning.","We consider satisfiability problems from different instance distributions and learn specialized heuristics for each of them.","Our experimental results show improvements with respect to both a WalkSAT baseline and another local search learned heuristic."],"url":"http://arxiv.org/abs/2307.04608v1"}
{"created":"2023-07-10 14:43:32","title":"EchoVest: Real-Time Sound Classification and Depth Perception Expressed through Transcutaneous Electrical Nerve Stimulation","abstract":"Over 1.5 billion people worldwide live with hearing impairment. Despite various technologies that have been created for individuals with such disabilities, most of these technologies are either extremely expensive or inaccessible for everyday use in low-medium income countries. In order to combat this issue, we have developed a new assistive device, EchoVest, for blind/deaf people to intuitively become more aware of their environment. EchoVest transmits vibrations to the user's body by utilizing transcutaneous electric nerve stimulation (TENS) based on the source of the sounds. EchoVest also provides various features, including sound localization, sound classification, noise reduction, and depth perception. We aimed to outperform CNN-based machine-learning models, the most commonly used machine learning model for classification tasks, in accuracy and computational costs. To do so, we developed and employed a novel audio pipeline that adapts the Audio Spectrogram Transformer (AST) model, an attention-based model, for our sound classification purposes, and Fast Fourier Transforms for noise reduction. The application of Otsu's Method helped us find the optimal thresholds for background noise sound filtering and gave us much greater accuracy. In order to calculate direction and depth accurately, we applied Complex Time Difference of Arrival algorithms and SOTA localization. Our last improvement was to use blind source separation to make our algorithms applicable to multiple microphone inputs. The final algorithm achieved state-of-the-art results on numerous checkpoints, including a 95.7\\% accuracy on the ESC-50 dataset for environmental sound classification.","sentences":["Over 1.5 billion people worldwide live with hearing impairment.","Despite various technologies that have been created for individuals with such disabilities, most of these technologies are either extremely expensive or inaccessible for everyday use in low-medium income countries.","In order to combat this issue, we have developed a new assistive device, EchoVest, for blind/deaf people to intuitively become more aware of their environment.","EchoVest transmits vibrations to the user's body by utilizing transcutaneous electric nerve stimulation (TENS) based on the source of the sounds.","EchoVest also provides various features, including sound localization, sound classification, noise reduction, and depth perception.","We aimed to outperform CNN-based machine-learning models, the most commonly used machine learning model for classification tasks, in accuracy and computational costs.","To do so, we developed and employed a novel audio pipeline that adapts the Audio Spectrogram Transformer (AST) model, an attention-based model, for our sound classification purposes, and Fast Fourier Transforms for noise reduction.","The application of Otsu's Method helped us find the optimal thresholds for background noise sound filtering and gave us much greater accuracy.","In order to calculate direction and depth accurately, we applied Complex Time Difference of Arrival algorithms and SOTA localization.","Our last improvement was to use blind source separation to make our algorithms applicable to multiple microphone inputs.","The final algorithm achieved state-of-the-art results on numerous checkpoints, including a 95.7\\% accuracy on the ESC-50 dataset for environmental sound classification."],"url":"http://arxiv.org/abs/2307.04604v1"}
{"created":"2023-07-10 14:39:43","title":"InPars Toolkit: A Unified and Reproducible Synthetic Data Generation Pipeline for Neural Information Retrieval","abstract":"Recent work has explored Large Language Models (LLMs) to overcome the lack of training data for Information Retrieval (IR) tasks. The generalization abilities of these models have enabled the creation of synthetic in-domain data by providing instructions and a few examples on a prompt. InPars and Promptagator have pioneered this approach and both methods have demonstrated the potential of using LLMs as synthetic data generators for IR tasks. This makes them an attractive solution for IR tasks that suffer from a lack of annotated data. However, the reproducibility of these methods was limited, because InPars' training scripts are based on TPUs -- which are not widely accessible -- and because the code for Promptagator was not released and its proprietary LLM is not publicly accessible. To fully realize the potential of these methods and make their impact more widespread in the research community, the resources need to be accessible and easy to reproduce by researchers and practitioners. Our main contribution is a unified toolkit for end-to-end reproducible synthetic data generation research, which includes generation, filtering, training and evaluation. Additionally, we provide an interface to IR libraries widely used by the community and support for GPU. Our toolkit not only reproduces the InPars method and partially reproduces Promptagator, but also provides a plug-and-play functionality allowing the use of different LLMs, exploring filtering methods and finetuning various reranker models on the generated data. We also made available all the synthetic data generated in this work for the 18 different datasets in the BEIR benchmark which took more than 2,000 GPU hours to be generated as well as the reranker models finetuned on the synthetic data. Code and data are available at https://github.com/zetaalphavector/InPars","sentences":["Recent work has explored Large Language Models (LLMs) to overcome the lack of training data for Information Retrieval (IR) tasks.","The generalization abilities of these models have enabled the creation of synthetic in-domain data by providing instructions and a few examples on a prompt.","InPars and Promptagator have pioneered this approach and both methods have demonstrated the potential of using LLMs as synthetic data generators for IR tasks.","This makes them an attractive solution for IR tasks that suffer from a lack of annotated data.","However, the reproducibility of these methods was limited, because InPars' training scripts are based on TPUs -- which are not widely accessible -- and because the code for Promptagator was not released and its proprietary LLM is not publicly accessible.","To fully realize the potential of these methods and make their impact more widespread in the research community, the resources need to be accessible and easy to reproduce by researchers and practitioners.","Our main contribution is a unified toolkit for end-to-end reproducible synthetic data generation research, which includes generation, filtering, training and evaluation.","Additionally, we provide an interface to IR libraries widely used by the community and support for GPU.","Our toolkit not only reproduces the InPars method and partially reproduces Promptagator, but also provides a plug-and-play functionality allowing the use of different LLMs, exploring filtering methods and finetuning various reranker models on the generated data.","We also made available all the synthetic data generated in this work for the 18 different datasets in the BEIR benchmark which took more than 2,000 GPU hours to be generated as well as the reranker models finetuned on the synthetic data.","Code and data are available at https://github.com/zetaalphavector/InPars"],"url":"http://arxiv.org/abs/2307.04601v1"}
{"created":"2023-07-10 14:38:38","title":"Model-Driven Engineering for Artificial Intelligence -- A Systematic Literature Review","abstract":"Objective: This study aims to investigate the existing body of knowledge in the field of Model-Driven Engineering MDE in support of AI (MDE4AI) to sharpen future research further and define the current state of the art.   Method: We conducted a Systemic Literature Review (SLR), collecting papers from five major databases resulting in 703 candidate studies, eventually retaining 15 primary studies. Each primary study will be evaluated and discussed with respect to the adoption of (1) MDE principles and practices and (2) the phases of AI development support aligned with the stages of the CRISP-DM methodology.   Results: The study's findings show that the pillar concepts of MDE (metamodel, concrete syntax and model transformation), are leveraged to define domain-specific languages (DSL) explicitly addressing AI concerns. Different MDE technologies are used, leveraging different language workbenches. The most prominent AI-related concerns are training and modeling of the AI algorithm, while minor emphasis is given to the time-consuming preparation of the data sets. Early project phases that support interdisciplinary communication of requirements, such as the CRISP-DM \\textit{Business Understanding} phase, are rarely reflected.   Conclusion: The study found that the use of MDE for AI is still in its early stages, and there is no single tool or method that is widely used. Additionally, current approaches tend to focus on specific stages of development rather than providing support for the entire development process. As a result, the study suggests several research directions to further improve the use of MDE for AI and to guide future research in this area.","sentences":["Objective: This study aims to investigate the existing body of knowledge in the field of Model-Driven Engineering MDE in support of AI (MDE4AI) to sharpen future research further and define the current state of the art.   ","Method: We conducted a Systemic Literature Review (SLR), collecting papers from five major databases resulting in 703 candidate studies, eventually retaining 15 primary studies.","Each primary study will be evaluated and discussed with respect to the adoption of (1) MDE principles and practices and (2) the phases of AI development support aligned with the stages of the CRISP-DM methodology.   ","Results:","The study's findings show that the pillar concepts of MDE (metamodel, concrete syntax and model transformation), are leveraged to define domain-specific languages (DSL) explicitly addressing AI concerns.","Different MDE technologies are used, leveraging different language workbenches.","The most prominent AI-related concerns are training and modeling of the AI algorithm, while minor emphasis is given to the time-consuming preparation of the data sets.","Early project phases that support interdisciplinary communication of requirements, such as the CRISP-DM \\textit{Business Understanding} phase, are rarely reflected.   ","Conclusion: The study found that the use of MDE for AI is still in its early stages, and there is no single tool or method that is widely used.","Additionally, current approaches tend to focus on specific stages of development rather than providing support for the entire development process.","As a result, the study suggests several research directions to further improve the use of MDE for AI and to guide future research in this area."],"url":"http://arxiv.org/abs/2307.04599v1"}
{"created":"2023-07-10 14:36:51","title":"Source-Free Open-Set Domain Adaptation for Histopathological Images via Distilling Self-Supervised Vision Transformer","abstract":"There is a strong incentive to develop computational pathology models to i) ease the burden of tissue typology annotation from whole slide histological images; ii) transfer knowledge, e.g., tissue class separability from the withheld source domain to the distributionally shifted unlabeled target domain, and simultaneously iii) detect Open Set samples, i.e., unseen novel categories not present in the training source domain. This paper proposes a highly practical setting by addressing the abovementioned challenges in one fell swoop, i.e., source-free Open Set domain adaptation (SF-OSDA), which addresses the situation where a model pre-trained on the inaccessible source dataset can be adapted on the unlabeled target dataset containing Open Set samples. The central tenet of our proposed method is distilling knowledge from a self-supervised vision transformer trained in the target domain. We propose a novel style-based data augmentation used as hard positives for self-training a vision transformer in the target domain, yielding strongly contextualized embedding. Subsequently, semantically similar target images are clustered while the source model provides their corresponding weak pseudo-labels with unreliable confidence. Furthermore, we propose cluster relative maximum logit score (CRMLS) to rectify the confidence of the weak pseudo-labels and compute weighted class prototypes in the contextualized embedding space that are utilized for adapting the source model on the target domain. Our method significantly outperforms the previous methods, including open set detection, test-time adaptation, and SF-OSDA methods, setting the new state-of-the-art on three public histopathological datasets of colorectal cancer (CRC) assessment- Kather-16, Kather-19, and CRCTP. Our code is available at https://github.com/LTS5/Proto-SF-OSDA.","sentences":["There is a strong incentive to develop computational pathology models to i) ease the burden of tissue typology annotation from whole slide histological images; ii) transfer knowledge, e.g., tissue class separability from the withheld source domain to the distributionally shifted unlabeled target domain, and simultaneously iii) detect Open Set samples, i.e., unseen novel categories not present in the training source domain.","This paper proposes a highly practical setting by addressing the abovementioned challenges in one fell swoop, i.e., source-free Open Set domain adaptation (SF-OSDA), which addresses the situation where a model pre-trained on the inaccessible source dataset can be adapted on the unlabeled target dataset containing Open Set samples.","The central tenet of our proposed method is distilling knowledge from a self-supervised vision transformer trained in the target domain.","We propose a novel style-based data augmentation used as hard positives for self-training a vision transformer in the target domain, yielding strongly contextualized embedding.","Subsequently, semantically similar target images are clustered while the source model provides their corresponding weak pseudo-labels with unreliable confidence.","Furthermore, we propose cluster relative maximum logit score (CRMLS) to rectify the confidence of the weak pseudo-labels and compute weighted class prototypes in the contextualized embedding space that are utilized for adapting the source model on the target domain.","Our method significantly outperforms the previous methods, including open set detection, test-time adaptation, and SF-OSDA methods, setting the new state-of-the-art on three public histopathological datasets of colorectal cancer (CRC) assessment-","Kather-16, Kather-19, and CRCTP.","Our code is available at https://github.com/LTS5/Proto-SF-OSDA."],"url":"http://arxiv.org/abs/2307.04596v1"}
{"created":"2023-07-10 14:35:29","title":"Parameterized Analysis of the Cops and Robber Problem","abstract":"\\textit{Pursuit-evasion games} have been intensively studied for several decades due to their numerous applications in artificial intelligence, robot motion planning, database theory, distributed computing, and algorithmic theory. \\textsc{Cops and Robber} (\\CR) is one of the most well-known pursuit-evasion games played on graphs, where multiple \\textit{cops} pursue a single \\textit{robber}. The aim is to compute the \\textit{cop number} of a graph, $k$, which is the minimum number of cops that ensures the \\textit{capture} of the robber.   From the viewpoint of parameterized complexity, \\CR is W[2]-hard parameterized by $k$~[Fomin et al., TCS, 2010]. Thus, we study structural parameters of the input graph. We begin with the \\textit{vertex cover number} ($\\mathsf{vcn}$). First, we establish that $k \\leq \\frac{\\mathsf{vcn}}{3}+1$. Second, we prove that \\CR parameterized by $\\mathsf{vcn}$ is \\FPT by designing an exponential kernel. We complement this result by showing that it is unlikely for \\CR parameterized by $\\mathsf{vcn}$ to admit a polynomial compression. We extend our exponential kernels to the parameters \\textit{cluster vertex deletion number} and \\textit{deletion to stars number}, and design a linear vertex kernel for \\textit{neighborhood diversity}. Additionally, we extend all of our results to several well-studied variations of \\CR.","sentences":["\\textit{Pursuit-evasion games} have been intensively studied for several decades due to their numerous applications in artificial intelligence, robot motion planning, database theory, distributed computing, and algorithmic theory.","\\textsc{Cops and Robber} (\\CR) is one of the most well-known pursuit-evasion games played on graphs, where multiple \\textit{cops} pursue a single \\textit{robber}.","The aim is to compute the \\textit{cop number} of a graph, $k$, which is the minimum number of cops that ensures the \\textit{capture} of the robber.   ","From the viewpoint of parameterized complexity, \\CR is W[2]-hard parameterized by $k$~[Fomin et al., TCS, 2010].","Thus, we study structural parameters of the input graph.","We begin with the \\textit{vertex cover number} ($\\mathsf{vcn}$).","First, we establish that $k \\leq \\frac{\\mathsf{vcn}}{3}+1$.","Second, we prove that \\CR parameterized by $\\mathsf{vcn}$ is \\FPT by designing an exponential kernel.","We complement this result by showing that it is unlikely for \\CR parameterized by $\\mathsf{vcn}$ to admit a polynomial compression.","We extend our exponential kernels to the parameters \\textit{cluster vertex deletion number} and \\textit{deletion to stars number}, and design a linear vertex kernel for \\textit{neighborhood diversity}.","Additionally, we extend all of our results to several well-studied variations of \\CR."],"url":"http://arxiv.org/abs/2307.04594v1"}
{"created":"2023-07-10 14:32:24","title":"A Graph Multi-separator Problem for Image Segmentation","abstract":"We propose a novel abstraction of the image segmentation task in the form of a combinatorial optimization problem that we call the multi-separator problem. Feasible solutions indicate for every pixel whether it belongs to a segment or a segment separator, and indicate for pairs of pixels whether or not the pixels belong to the same segment. This is in contrast to the closely related lifted multicut problem where every pixel is associated to a segment and no pixel explicitly represents a separating structure. While the multi-separator problem is NP-hard, we identify two special cases for which it can be solved efficiently. Moreover, we define two local search algorithms for the general case and demonstrate their effectiveness in segmenting simulated volume images of foam cells and filaments.","sentences":["We propose a novel abstraction of the image segmentation task in the form of a combinatorial optimization problem that we call the multi-separator problem.","Feasible solutions indicate for every pixel whether it belongs to a segment or a segment separator, and indicate for pairs of pixels whether or not the pixels belong to the same segment.","This is in contrast to the closely related lifted multicut problem where every pixel is associated to a segment and no pixel explicitly represents a separating structure.","While the multi-separator problem is NP-hard, we identify two special cases for which it can be solved efficiently.","Moreover, we define two local search algorithms for the general case and demonstrate their effectiveness in segmenting simulated volume images of foam cells and filaments."],"url":"http://arxiv.org/abs/2307.04592v1"}
{"created":"2023-07-10 14:21:43","title":"Parameterised distance to local irregularity","abstract":"A graph $G$ is \\emph{locally irregular} if no two of its adjacent vertices have the same degree. In [Fioravantes et al. Complexity of finding maximum locally irregular induced subgraph. {\\it SWAT}, 2022], the authors introduced and studied the problem of finding a locally irregular induced subgraph of a given a graph $G$ of maximum order, or, equivalently, computing a subset $S$ of $V(G)$ of minimum order, whose deletion from $G$ results in a locally irregular graph; $S$ is denoted as an \\emph{optimal vertex-irregulator of $G$}. In this work we provide an in-depth analysis of the parameterised complexity of computing an optimal vertex-irregulator of a given graph $G$. Moreover, we introduce and study a variation of this problem, where $S$ is a substet of the edges of $G$; in this case, $S$ is denoted as an \\emph{optimal edge-irregulator of $G$}. In particular, we prove that computing an optimal vertex-irregulator of a graph $G$ is in FPT when parameterised by the vertex integrity, neighborhood diversity or cluster deletion number of $G$, while it is $W[1]$-hard when parameterised by the feedback vertex set number or the treedepth of $G$. In the case of computing an optimal edge-irregulator of a graph $G$, we prove that this problem is in FPT when parameterised by the vertex integrity of $G$, while it is NP-hard even if $G$ is a planar bipartite graph of maximum degree $4$, and $W[1]$-hard when parameterised by the size of the solution, the feedback vertex set or the treedepth of $G$. Our results paint a comprehensive picture of the tractability of both problems studied here, considering most of the standard graph-structural parameters.","sentences":["A graph $G$ is \\emph{locally irregular} if no two of its adjacent vertices have the same degree.","In [Fioravantes et al. Complexity of finding maximum locally irregular induced subgraph.","{\\it SWAT}, 2022], the authors introduced and studied the problem of finding a locally irregular induced subgraph of a given a graph $G$ of maximum order, or, equivalently, computing a subset $S$ of $V(G)$ of minimum order, whose deletion from $G$ results in a locally irregular graph; $S$ is denoted as an \\emph{optimal vertex-irregulator of $G$}.","In this work we provide an in-depth analysis of the parameterised complexity of computing an optimal vertex-irregulator of a given graph $G$.","Moreover, we introduce and study a variation of this problem, where $S$ is a substet of the edges of $G$; in this case, $S$ is denoted as an \\emph{optimal edge-irregulator of $G$}.","In particular, we prove that computing an optimal vertex-irregulator of a graph $G$ is in FPT when parameterised by the vertex integrity, neighborhood diversity or cluster deletion number of $G$, while it is $W[1]$-hard when parameterised by the feedback vertex set number or the treedepth of $G$.","In the case of computing an optimal edge-irregulator of a graph $G$, we prove that this problem is in FPT when parameterised by the vertex integrity of $G$, while it is NP-hard even if $G$ is a planar bipartite graph of maximum degree $4$, and $W[1]$-hard when parameterised by the size of the solution, the feedback vertex set or the treedepth of $G$. Our results paint a comprehensive picture of the tractability of both problems studied here, considering most of the standard graph-structural parameters."],"url":"http://arxiv.org/abs/2307.04583v1"}
{"created":"2023-07-10 14:11:07","title":"AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System","abstract":"Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator. Project page: http://anyteleop.com/.","sentences":["Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors.","However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases.","In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system.","Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance.","For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot.","For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator.","Project page: http://anyteleop.com/."],"url":"http://arxiv.org/abs/2307.04577v1"}
{"created":"2023-07-10 14:07:37","title":"TFR: Texture Defect Detection with Fourier Transform using Normal Reconstructed Template of Simple Autoencoder","abstract":"Texture is an essential information in image representation, capturing patterns and structures. As a result, texture plays a crucial role in the manufacturing industry and is extensively studied in the fields of computer vision and pattern recognition. However, real-world textures are susceptible to defects, which can degrade image quality and cause various issues. Therefore, there is a need for accurate and effective methods to detect texture defects. In this study, a simple autoencoder and Fourier transform are employed for texture defect detection. The proposed method combines Fourier transform analysis with the reconstructed template obtained from the simple autoencoder. Fourier transform is a powerful tool for analyzing the frequency domain of images and signals. Moreover, since texture defects often exhibit characteristic changes in specific frequency ranges, analyzing the frequency domain enables effective defect detection. The proposed method demonstrates effectiveness and accuracy in detecting texture defects. Experimental results are presented to evaluate its performance and compare it with existing approaches.","sentences":["Texture is an essential information in image representation, capturing patterns and structures.","As a result, texture plays a crucial role in the manufacturing industry and is extensively studied in the fields of computer vision and pattern recognition.","However, real-world textures are susceptible to defects, which can degrade image quality and cause various issues.","Therefore, there is a need for accurate and effective methods to detect texture defects.","In this study, a simple autoencoder and Fourier transform are employed for texture defect detection.","The proposed method combines Fourier transform analysis with the reconstructed template obtained from the simple autoencoder.","Fourier transform is a powerful tool for analyzing the frequency domain of images and signals.","Moreover, since texture defects often exhibit characteristic changes in specific frequency ranges, analyzing the frequency domain enables effective defect detection.","The proposed method demonstrates effectiveness and accuracy in detecting texture defects.","Experimental results are presented to evaluate its performance and compare it with existing approaches."],"url":"http://arxiv.org/abs/2307.04574v1"}
{"created":"2023-07-10 14:07:28","title":"A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI/ML in Oncology","abstract":"In today's vast literature landscape, a manual review is very time-consuming. To address this challenge, this paper proposes a semi-automated tool for solution method review and selection. It caters to researchers, practitioners, and decision-makers while serving as a benchmark for future work. The tool comprises three modules: (1) paper selection and scoring, using a keyword selection scheme to query Scopus API and compute relevancy; (2) solution method extraction in papers utilizing OpenAI API; (3) sensitivity analysis and post-analyzes. It reveals trends, relevant papers, and methods. AI in the oncology case study and several use cases are presented with promising results, comparing the tool to manual ground truth.","sentences":["In today's vast literature landscape, a manual review is very time-consuming.","To address this challenge, this paper proposes a semi-automated tool for solution method review and selection.","It caters to researchers, practitioners, and decision-makers while serving as a benchmark for future work.","The tool comprises three modules: (1) paper selection and scoring, using a keyword selection scheme to query Scopus API and compute relevancy; (2) solution method extraction in papers utilizing OpenAI API; (3) sensitivity analysis and post-analyzes.","It reveals trends, relevant papers, and methods.","AI in the oncology case study and several use cases are presented with promising results, comparing the tool to manual ground truth."],"url":"http://arxiv.org/abs/2307.04573v1"}
{"created":"2023-07-10 14:03:34","title":"Alleviating Matthew Effect of Offline Reinforcement Learning in Interactive Recommendation","abstract":"Offline reinforcement learning (RL), a technology that offline learns a policy from logged data without the need to interact with online environments, has become a favorable choice in decision-making processes like interactive recommendation. Offline RL faces the value overestimation problem. To address it, existing methods employ conservatism, e.g., by constraining the learned policy to be close to behavior policies or punishing the rarely visited state-action pairs. However, when applying such offline RL to recommendation, it will cause a severe Matthew effect, i.e., the rich get richer and the poor get poorer, by promoting popular items or categories while suppressing the less popular ones. It is a notorious issue that needs to be addressed in practical recommender systems.   In this paper, we aim to alleviate the Matthew effect in offline RL-based recommendation. Through theoretical analyses, we find that the conservatism of existing methods fails in pursuing users' long-term satisfaction. It inspires us to add a penalty term to relax the pessimism on states with high entropy of the logging policy and indirectly penalizes actions leading to less diverse states. This leads to the main technical contribution of the work: Debiased model-based Offline RL (DORL) method. Experiments show that DORL not only captures user interests well but also alleviates the Matthew effect. The implementation is available via https://github.com/chongminggao/DORL-codes.","sentences":["Offline reinforcement learning (RL), a technology that offline learns a policy from logged data without the need to interact with online environments, has become a favorable choice in decision-making processes like interactive recommendation.","Offline RL faces the value overestimation problem.","To address it, existing methods employ conservatism, e.g., by constraining the learned policy to be close to behavior policies or punishing the rarely visited state-action pairs.","However, when applying such offline RL to recommendation, it will cause a severe Matthew effect, i.e., the rich get richer and the poor get poorer, by promoting popular items or categories while suppressing the less popular ones.","It is a notorious issue that needs to be addressed in practical recommender systems.   ","In this paper, we aim to alleviate the Matthew effect in offline RL-based recommendation.","Through theoretical analyses, we find that the conservatism of existing methods fails in pursuing users' long-term satisfaction.","It inspires us to add a penalty term to relax the pessimism on states with high entropy of the logging policy and indirectly penalizes actions leading to less diverse states.","This leads to the main technical contribution of the work: Debiased model-based Offline RL (DORL) method.","Experiments show that DORL not only captures user interests well but also alleviates the Matthew effect.","The implementation is available via https://github.com/chongminggao/DORL-codes."],"url":"http://arxiv.org/abs/2307.04571v1"}
{"created":"2023-07-10 14:02:31","title":"Unraveling the Age Estimation Puzzle: Comparative Analysis of Deep Learning Approaches for Facial Age Estimation","abstract":"Comparing different age estimation methods poses a challenge due to the unreliability of published results, stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. We argue that, for age estimation tasks outside of the low-data regime, designing specialized methods is unnecessary, and the standard approach of utilizing cross-entropy loss is sufficient. This paper aims to address the benchmark shortcomings by evaluating state-of-the-art age estimation methods in a unified and comparable setting. We systematically analyze the impact of various factors, including facial alignment, facial coverage, image resolution, image representation, model architecture, and the amount of data on age estimation results. Surprisingly, these factors often exert a more significant influence than the choice of the age estimation method itself. We assess the generalization capability of each method by evaluating the cross-dataset performance for publicly available age estimation datasets. The results emphasize the importance of using consistent data preprocessing practices and establishing standardized benchmarks to ensure reliable and meaningful comparisons. The source code is available at https://github.com/paplhjak/Facial-Age-Estimation-Benchmark.","sentences":["Comparing different age estimation methods poses a challenge due to the unreliability of published results, stemming from inconsistencies in the benchmarking process.","Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims.","We argue that, for age estimation tasks outside of the low-data regime, designing specialized methods is unnecessary, and the standard approach of utilizing cross-entropy loss is sufficient.","This paper aims to address the benchmark shortcomings by evaluating state-of-the-art age estimation methods in a unified and comparable setting.","We systematically analyze the impact of various factors, including facial alignment, facial coverage, image resolution, image representation, model architecture, and the amount of data on age estimation results.","Surprisingly, these factors often exert a more significant influence than the choice of the age estimation method itself.","We assess the generalization capability of each method by evaluating the cross-dataset performance for publicly available age estimation datasets.","The results emphasize the importance of using consistent data preprocessing practices and establishing standardized benchmarks to ensure reliable and meaningful comparisons.","The source code is available at https://github.com/paplhjak/Facial-Age-Estimation-Benchmark."],"url":"http://arxiv.org/abs/2307.04570v1"}
{"created":"2023-07-10 14:01:29","title":"Interpreting and generalizing deep learning in physics-based problems with functional linear models","abstract":"Although deep learning has achieved remarkable success in various scientific machine learning applications, its black-box nature poses concerns regarding interpretability and generalization capabilities beyond the training data. Interpretability is crucial and often desired in modeling physical systems. Moreover, acquiring extensive datasets that encompass the entire range of input features is challenging in many physics-based learning tasks, leading to increased errors when encountering out-of-distribution (OOD) data. In this work, motivated by the field of functional data analysis (FDA), we propose generalized functional linear models as an interpretable surrogate for a trained deep learning model. We demonstrate that our model could be trained either based on a trained neural network (post-hoc interpretation) or directly from training data (interpretable operator learning). A library of generalized functional linear models with different kernel functions is considered and sparse regression is used to discover an interpretable surrogate model that could be analytically presented. We present test cases in solid mechanics, fluid mechanics, and transport. Our results demonstrate that our model can achieve comparable accuracy to deep learning and can improve OOD generalization while providing more transparency and interpretability. Our study underscores the significance of interpretability in scientific machine learning and showcases the potential of functional linear models as a tool for interpreting and generalizing deep learning.","sentences":["Although deep learning has achieved remarkable success in various scientific machine learning applications, its black-box nature poses concerns regarding interpretability and generalization capabilities beyond the training data.","Interpretability is crucial and often desired in modeling physical systems.","Moreover, acquiring extensive datasets that encompass the entire range of input features is challenging in many physics-based learning tasks, leading to increased errors when encountering out-of-distribution (OOD) data.","In this work, motivated by the field of functional data analysis (FDA), we propose generalized functional linear models as an interpretable surrogate for a trained deep learning model.","We demonstrate that our model could be trained either based on a trained neural network (post-hoc interpretation) or directly from training data (interpretable operator learning).","A library of generalized functional linear models with different kernel functions is considered and sparse regression is used to discover an interpretable surrogate model that could be analytically presented.","We present test cases in solid mechanics, fluid mechanics, and transport.","Our results demonstrate that our model can achieve comparable accuracy to deep learning and can improve OOD generalization while providing more transparency and interpretability.","Our study underscores the significance of interpretability in scientific machine learning and showcases the potential of functional linear models as a tool for interpreting and generalizing deep learning."],"url":"http://arxiv.org/abs/2307.04569v1"}
{"created":"2023-07-10 13:52:09","title":"Automatically detecting activities of daily living from in-home sensors as indicators of routine behaviour in an older population","abstract":"Objective: The NEX project has developed an integrated Internet of Things (IoT) system coupled with data analytics to offer unobtrusive health and wellness monitoring supporting older adults living independently at home. Monitoring {currently} involves visualising a set of automatically detected activities of daily living (ADLs) for each participant. The detection of ADLs is achieved {} to allow the incorporation of additional participants whose ADLs are detected without re-training the system.   Methods: Following an extensive User Needs and Requirements study involving 426 participants, a pilot trial and a friendly trial of the deployment, an Action Research Cycle (ARC) trial was completed. This involved 23 participants over a 10-week period each with c.20 IoT sensors in their homes. During the ARC trial, participants each took part in two data-informed briefings which presented visualisations of their own in-home activities. The briefings also gathered training data on the accuracy of detected activities. Association rule mining was then used on the combination of data from sensors and participant feedback to improve the automatic detection of ADLs.   Results: Association rule mining was used to detect a range of ADLs for each participant independently of others and was then used to detect ADLs across participants using a single set of rules {for each ADL}. This allows additional participants to be added without the necessity of them providing training data.   Conclusions: Additional participants can be added to the NEX system without the necessity to re-train the system for automatic detection of the set of their activities of daily living.","sentences":["Objective: The NEX project has developed an integrated Internet of Things (IoT) system coupled with data analytics to offer unobtrusive health and wellness monitoring supporting older adults living independently at home.","Monitoring {currently} involves visualising a set of automatically detected activities of daily living (ADLs) for each participant.","The detection of ADLs is achieved {} to allow the incorporation of additional participants whose ADLs are detected without re-training the system.   ","Methods: Following an extensive User Needs and Requirements study involving 426 participants, a pilot trial and a friendly trial of the deployment, an Action Research Cycle (ARC) trial was completed.","This involved 23 participants over a 10-week period each with c.20 IoT sensors in their homes.","During the ARC trial, participants each took part in two data-informed briefings which presented visualisations of their own in-home activities.","The briefings also gathered training data on the accuracy of detected activities.","Association rule mining was then used on the combination of data from sensors and participant feedback to improve the automatic detection of ADLs.   ","Results: Association rule mining was used to detect a range of ADLs for each participant independently of others and was then used to detect ADLs across participants using a single set of rules {for each ADL}.","This allows additional participants to be added without the necessity of them providing training data.   ","Conclusions: Additional participants can be added to the NEX system without the necessity to re-train the system for automatic detection of the set of their activities of daily living."],"url":"http://arxiv.org/abs/2307.04563v1"}
{"created":"2023-07-10 13:51:00","title":"Performance comparison of timing-based anomaly detectors for Controller Area Network: a reproducible study","abstract":"This work presents an experimental evaluation of the detection performance of eight different algorithms for anomaly detection on the Controller Area Network (CAN) bus of modern vehicles based on the analysis of the timing or frequency of CAN messages. This work solves the current limitations of related scientific literature, that is based on private dataset, lacks of open implementations, and detailed description of the detection algorithms. These drawback prevent the reproducibility of published results, and makes it impossible to compare a novel proposal against related work, thus hindering the advancement of science. This paper solves these issues by publicly releasing implementations, labeled datasets and by describing an unbiased experimental comparisons.","sentences":["This work presents an experimental evaluation of the detection performance of eight different algorithms for anomaly detection on the Controller Area Network (CAN) bus of modern vehicles based on the analysis of the timing or frequency of CAN messages.","This work solves the current limitations of related scientific literature, that is based on private dataset, lacks of open implementations, and detailed description of the detection algorithms.","These drawback prevent the reproducibility of published results, and makes it impossible to compare a novel proposal against related work, thus hindering the advancement of science.","This paper solves these issues by publicly releasing implementations, labeled datasets and by describing an unbiased experimental comparisons."],"url":"http://arxiv.org/abs/2307.04561v1"}
{"created":"2023-07-10 13:34:13","title":"SparseVSR: Lightweight and Noise Robust Visual Speech Recognition","abstract":"Recent advances in deep neural networks have achieved unprecedented success in visual speech recognition. However, there remains substantial disparity between current methods and their deployment in resource-constrained devices. In this work, we explore different magnitude-based pruning techniques to generate a lightweight model that achieves higher performance than its dense model equivalent, especially under the presence of visual noise. Our sparse models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and outperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparse model on 7 different visual noise types and achieve an overall absolute improvement of more than 2% WER compared to the dense equivalent. Our results confirm that sparse networks are more resistant to noise than dense networks.","sentences":["Recent advances in deep neural networks have achieved unprecedented success in visual speech recognition.","However, there remains substantial disparity between current methods and their deployment in resource-constrained devices.","In this work, we explore different magnitude-based pruning techniques to generate a lightweight model that achieves higher performance than its dense model equivalent, especially under the presence of visual noise.","Our sparse models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and outperform the dense equivalent up to 70% sparsity.","We evaluate our 50% sparse model on 7 different visual noise types and achieve an overall absolute improvement of more than 2% WER compared to the dense equivalent.","Our results confirm that sparse networks are more resistant to noise than dense networks."],"url":"http://arxiv.org/abs/2307.04552v1"}
{"created":"2023-07-10 13:27:13","title":"Needs, Passions and Loot Boxes -- Exploring Reasons for Problem Behaviour in Relation to Loot Box Engagement","abstract":"Research on the convergence of gaming and gambling has been around since the 1990s. The emergence of loot boxes in video games in the mid 2010s, a game mechanic with a chance-based outcome that shares structural and psychological similarities to gambling, caused public controversy and lead to the inception of a new field of study, loot box research. Since then, various studies have found a relationship between loot box engagement and problem gambling as well as problem gaming. Due to the cross-sectional nature of this data, however, inferences about causality are limited. While loot box research has extensively investigated the relationship between loot box engagement and problem behaviour, little research has been done to explain the underlying motivations of players that drive them to interact with loot boxes. The goal of this thesis is to provide possible explanations for the relationship between loot box engagement and problem gamblers or problem gamers. In doing so, it draws upon two prominent psychological theories. Self-Determination Theory and the Dualistic Model of Passion. Self-Determination Theory's concept of psychological needs and their satisfaction or frustration is hereby used to explain the development of harmonious or obsessive passions, which are introduced in the Dualistic Model of Passion. These obsessive passions have been shown to be possible antecedents of behavioural addictions, such as problem gambling or problem gaming. Thus, the interplay between needs, passions and loot box opening could elucidate the aforementioned correlations between loot box engagement and problem behaviour. However, further research, especially utilising longitudinal data, is needed to better understand these processes.","sentences":["Research on the convergence of gaming and gambling has been around since the 1990s.","The emergence of loot boxes in video games in the mid 2010s, a game mechanic with a chance-based outcome that shares structural and psychological similarities to gambling, caused public controversy and lead to the inception of a new field of study, loot box research.","Since then, various studies have found a relationship between loot box engagement and problem gambling as well as problem gaming.","Due to the cross-sectional nature of this data, however, inferences about causality are limited.","While loot box research has extensively investigated the relationship between loot box engagement and problem behaviour, little research has been done to explain the underlying motivations of players that drive them to interact with loot boxes.","The goal of this thesis is to provide possible explanations for the relationship between loot box engagement and problem gamblers or problem gamers.","In doing so, it draws upon two prominent psychological theories.","Self-Determination Theory and the Dualistic Model of Passion.","Self-Determination Theory's concept of psychological needs and their satisfaction or frustration is hereby used to explain the development of harmonious or obsessive passions, which are introduced in the Dualistic Model of Passion.","These obsessive passions have been shown to be possible antecedents of behavioural addictions, such as problem gambling or problem gaming.","Thus, the interplay between needs, passions and loot box opening could elucidate the aforementioned correlations between loot box engagement and problem behaviour.","However, further research, especially utilising longitudinal data, is needed to better understand these processes."],"url":"http://arxiv.org/abs/2307.04549v1"}
{"created":"2023-07-10 13:24:37","title":"Safety Analysis of Parameterised Networks with Non-Blocking Rendez-Vous","abstract":"We consider networks of processes that all execute the same finite-state protocol and communicate via a rendez-vous mechanism. When a process requests a rendez-vous, another process can respond to it and they both change their control states accordingly. We focus here on a specific semantics, called non-blocking, where the process requesting a rendez-vous can change its state even if no process can respond to it. In this context, we study the parameterised coverability problem of a configuration, which consists in determining whether there is an initialnumber of processes and an execution allowing to reach a configuration bigger than a given one. We show that this problem is EXPSPACE-complete and can be solved in polynomial time if the protocol is partitioned into two sets of states, the states from which a process can request a rendez-vous and the ones from which it can answer one. We also prove that the problem of the existence of an execution bringing all the processes in a final state is undecidable in our context. These two problems can be solved in polynomial time with the classical rendez-vous semantics.","sentences":["We consider networks of processes that all execute the same finite-state protocol and communicate via a rendez-vous mechanism.","When a process requests a rendez-vous, another process can respond to it and they both change their control states accordingly.","We focus here on a specific semantics, called non-blocking, where the process requesting a rendez-vous can change its state even if no process can respond to it.","In this context, we study the parameterised coverability problem of a configuration, which consists in determining whether there is an initialnumber of processes and an execution allowing to reach a configuration bigger than a given one.","We show that this problem is EXPSPACE-complete and can be solved in polynomial time if the protocol is partitioned into two sets of states, the states from which a process can request a rendez-vous and the ones from which it can answer one.","We also prove that the problem of the existence of an execution bringing all the processes in a final state is undecidable in our context.","These two problems can be solved in polynomial time with the classical rendez-vous semantics."],"url":"http://arxiv.org/abs/2307.04546v1"}
{"created":"2023-07-10 13:17:29","title":"Customizing Synthetic Data for Data-Free Student Learning","abstract":"Data-free knowledge distillation (DFKD) aims to obtain a lightweight student model without original training data. Existing works generally synthesize data from the pre-trained teacher model to replace the original training data for student learning. To more effectively train the student model, the synthetic data shall be customized to the current student learning ability. However, this is ignored in the existing DFKD methods and thus negatively affects the student training. To address this issue, we propose Customizing Synthetic Data for Data-Free Student Learning (CSD) in this paper, which achieves adaptive data synthesis using a self-supervised augmented auxiliary task to estimate the student learning ability. Specifically, data synthesis is dynamically adjusted to enlarge the cross entropy between the labels and the predictions from the self-supervised augmented task, thus generating hard samples for the student model. The experiments on various datasets and teacher-student models show the effectiveness of our proposed method. Code is available at: $\\href{https://github.com/luoshiya/CSD}{https://github.com/luoshiya/CSD}$","sentences":["Data-free knowledge distillation (DFKD) aims to obtain a lightweight student model without original training data.","Existing works generally synthesize data from the pre-trained teacher model to replace the original training data for student learning.","To more effectively train the student model, the synthetic data shall be customized to the current student learning ability.","However, this is ignored in the existing DFKD methods and thus negatively affects the student training.","To address this issue, we propose Customizing Synthetic Data for Data-Free Student Learning (CSD) in this paper, which achieves adaptive data synthesis using a self-supervised augmented auxiliary task to estimate the student learning ability.","Specifically, data synthesis is dynamically adjusted to enlarge the cross entropy between the labels and the predictions from the self-supervised augmented task, thus generating hard samples for the student model.","The experiments on various datasets and teacher-student models show the effectiveness of our proposed method.","Code is available at: $\\href{https://github.com/luoshiya/CSD}{https://github.com/luoshiya/CSD}$"],"url":"http://arxiv.org/abs/2307.04542v1"}
{"created":"2023-07-10 13:09:42","title":"Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis","abstract":"Fueled by deep learning, computer-aided diagnosis achieves huge advances. However, out of controlled lab environments, algorithms could face multiple challenges. Open set recognition (OSR), as an important one, states that categories unseen in training could appear in testing. In medical fields, it could derive from incompletely collected training datasets and the constantly emerging new or rare diseases. OSR requires an algorithm to not only correctly classify known classes, but also recognize unknown classes and forward them to experts for further diagnosis. To tackle OSR, we assume that known classes could densely occupy small parts of the embedding space and the remaining sparse regions could be recognized as unknowns. Following it, we propose Open Margin Cosine Loss (OMCL) unifying two mechanisms. The former, called Margin Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing intra-class compactness and inter-class separability, together with an adaptive scaling factor to strengthen the generalization capacity. The latter, called Open-Space Suppression (OSS), opens the classifier by recognizing sparse embedding space as unknowns using proposed feature space descriptors. Besides, since medical OSR is still a nascent field, two publicly available benchmark datasets are proposed for comparison. Extensive ablation studies and feature visualization demonstrate the effectiveness of each design. Compared with state-of-the-art methods, MLAS achieves superior performances, measured by ACC, AUROC, and OSCR.","sentences":["Fueled by deep learning, computer-aided diagnosis achieves huge advances.","However, out of controlled lab environments, algorithms could face multiple challenges.","Open set recognition (OSR), as an important one, states that categories unseen in training could appear in testing.","In medical fields, it could derive from incompletely collected training datasets and the constantly emerging new or rare diseases.","OSR requires an algorithm to not only correctly classify known classes, but also recognize unknown classes and forward them to experts for further diagnosis.","To tackle OSR, we assume that known classes could densely occupy small parts of the embedding space and the remaining sparse regions could be recognized as unknowns.","Following it, we propose Open Margin Cosine Loss (OMCL) unifying two mechanisms.","The former, called Margin Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing intra-class compactness and inter-class separability, together with an adaptive scaling factor to strengthen the generalization capacity.","The latter, called Open-Space Suppression (OSS), opens the classifier by recognizing sparse embedding space as unknowns using proposed feature space descriptors.","Besides, since medical OSR is still a nascent field, two publicly available benchmark datasets are proposed for comparison.","Extensive ablation studies and feature visualization demonstrate the effectiveness of each design.","Compared with state-of-the-art methods, MLAS achieves superior performances, measured by ACC, AUROC, and OSCR."],"url":"http://arxiv.org/abs/2307.04541v1"}
{"created":"2023-07-10 13:02:46","title":"Q-YOLOP: Quantization-aware You Only Look Once for Panoptic Driving Perception","abstract":"In this work, we present an efficient and quantization-aware panoptic driving perception model (Q- YOLOP) for object detection, drivable area segmentation, and lane line segmentation, in the context of autonomous driving. Our model employs the Efficient Layer Aggregation Network (ELAN) as its backbone and task-specific heads for each task. We employ a four-stage training process that includes pretraining on the BDD100K dataset, finetuning on both the BDD100K and iVS datasets, and quantization-aware training (QAT) on BDD100K. During the training process, we use powerful data augmentation techniques, such as random perspective and mosaic, and train the model on a combination of the BDD100K and iVS datasets. Both strategies enhance the model's generalization capabilities. The proposed model achieves state-of-the-art performance with an mAP@0.5 of 0.622 for object detection and an mIoU of 0.612 for segmentation, while maintaining low computational and memory requirements.","sentences":["In this work, we present an efficient and quantization-aware panoptic driving perception model (Q- YOLOP) for object detection, drivable area segmentation, and lane line segmentation, in the context of autonomous driving.","Our model employs the Efficient Layer Aggregation Network (ELAN) as its backbone and task-specific heads for each task.","We employ a four-stage training process that includes pretraining on the BDD100K dataset, finetuning on both the BDD100K and iVS datasets, and quantization-aware training (QAT) on BDD100K. During the training process, we use powerful data augmentation techniques, such as random perspective and mosaic, and train the model on a combination of the BDD100K and iVS datasets.","Both strategies enhance the model's generalization capabilities.","The proposed model achieves state-of-the-art performance with an mAP@0.5 of 0.622 for object detection and an mIoU of 0.612 for segmentation, while maintaining low computational and memory requirements."],"url":"http://arxiv.org/abs/2307.04537v1"}
{"created":"2023-07-10 13:01:27","title":"DADO -- Low-Cost Selection Strategies for Deep Active Design Optimization","abstract":"In this experience report, we apply deep active learning to the field of design optimization to reduce the number of computationally expensive numerical simulations. We are interested in optimizing the design of structural components, where the shape is described by a set of parameters. If we can predict the performance based on these parameters and consider only the promising candidates for simulation, there is an enormous potential for saving computing power. We present two selection strategies for self-optimization to reduce the computational cost in multi-objective design optimization problems. Our proposed methodology provides an intuitive approach that is easy to apply, offers significant improvements over random sampling, and circumvents the need for uncertainty estimation. We evaluate our strategies on a large dataset from the domain of fluid dynamics and introduce two new evaluation metrics to determine the model's performance. Findings from our evaluation highlights the effectiveness of our selection strategies in accelerating design optimization. We believe that the introduced method is easily transferable to other self-optimization problems.","sentences":["In this experience report, we apply deep active learning to the field of design optimization to reduce the number of computationally expensive numerical simulations.","We are interested in optimizing the design of structural components, where the shape is described by a set of parameters.","If we can predict the performance based on these parameters and consider only the promising candidates for simulation, there is an enormous potential for saving computing power.","We present two selection strategies for self-optimization to reduce the computational cost in multi-objective design optimization problems.","Our proposed methodology provides an intuitive approach that is easy to apply, offers significant improvements over random sampling, and circumvents the need for uncertainty estimation.","We evaluate our strategies on a large dataset from the domain of fluid dynamics and introduce two new evaluation metrics to determine the model's performance.","Findings from our evaluation highlights the effectiveness of our selection strategies in accelerating design optimization.","We believe that the introduced method is easily transferable to other self-optimization problems."],"url":"http://arxiv.org/abs/2307.04536v1"}
{"created":"2023-07-10 13:01:08","title":"QBitOpt: Fast and Accurate Bitwidth Reallocation during Training","abstract":"Quantizing neural networks is one of the most effective methods for achieving efficient inference on mobile and embedded devices. In particular, mixed precision quantized (MPQ) networks, whose layers can be quantized to different bitwidths, achieve better task performance for the same resource constraint compared to networks with homogeneous bitwidths. However, finding the optimal bitwidth allocation is a challenging problem as the search space grows exponentially with the number of layers in the network. In this paper, we propose QBitOpt, a novel algorithm for updating bitwidths during quantization-aware training (QAT). We formulate the bitwidth allocation problem as a constraint optimization problem. By combining fast-to-compute sensitivities with efficient solvers during QAT, QBitOpt can produce mixed-precision networks with high task performance guaranteed to satisfy strict resource constraints. This contrasts with existing mixed-precision methods that learn bitwidths using gradients and cannot provide such guarantees. We evaluate QBitOpt on ImageNet and confirm that we outperform existing fixed and mixed-precision methods under average bitwidth constraints commonly found in the literature.","sentences":["Quantizing neural networks is one of the most effective methods for achieving efficient inference on mobile and embedded devices.","In particular, mixed precision quantized (MPQ) networks, whose layers can be quantized to different bitwidths, achieve better task performance for the same resource constraint compared to networks with homogeneous bitwidths.","However, finding the optimal bitwidth allocation is a challenging problem as the search space grows exponentially with the number of layers in the network.","In this paper, we propose QBitOpt, a novel algorithm for updating bitwidths during quantization-aware training (QAT).","We formulate the bitwidth allocation problem as a constraint optimization problem.","By combining fast-to-compute sensitivities with efficient solvers during QAT, QBitOpt can produce mixed-precision networks with high task performance guaranteed to satisfy strict resource constraints.","This contrasts with existing mixed-precision methods that learn bitwidths using gradients and cannot provide such guarantees.","We evaluate QBitOpt on ImageNet and confirm that we outperform existing fixed and mixed-precision methods under average bitwidth constraints commonly found in the literature."],"url":"http://arxiv.org/abs/2307.04535v1"}
{"created":"2023-07-10 12:59:30","title":"Preventing Errors in Person Detection: A Part-Based Self-Monitoring Framework","abstract":"The ability to detect learned objects regardless of their appearance is crucial for autonomous systems in real-world applications. Especially for detecting humans, which is often a fundamental task in safety-critical applications, it is vital to prevent errors. To address this challenge, we propose a self-monitoring framework that allows for the perception system to perform plausibility checks at runtime. We show that by incorporating an additional component for detecting human body parts, we are able to significantly reduce the number of missed human detections by factors of up to 9 when compared to a baseline setup, which was trained only on holistic person objects. Additionally, we found that training a model jointly on humans and their body parts leads to a substantial reduction in false positive detections by up to 50% compared to training on humans alone. We performed comprehensive experiments on the publicly available datasets DensePose and Pascal VOC in order to demonstrate the effectiveness of our framework. Code is available at https://github.com/ FraunhoferIKS/smf-object-detection.","sentences":["The ability to detect learned objects regardless of their appearance is crucial for autonomous systems in real-world applications.","Especially for detecting humans, which is often a fundamental task in safety-critical applications, it is vital to prevent errors.","To address this challenge, we propose a self-monitoring framework that allows for the perception system to perform plausibility checks at runtime.","We show that by incorporating an additional component for detecting human body parts, we are able to significantly reduce the number of missed human detections by factors of up to 9 when compared to a baseline setup, which was trained only on holistic person objects.","Additionally, we found that training a model jointly on humans and their body parts leads to a substantial reduction in false positive detections by up to 50% compared to training on humans alone.","We performed comprehensive experiments on the publicly available datasets DensePose and Pascal VOC in order to demonstrate the effectiveness of our framework.","Code is available at https://github.com/ FraunhoferIKS/smf-object-detection."],"url":"http://arxiv.org/abs/2307.04533v1"}
{"created":"2023-07-10 12:57:21","title":"On information content in certain objects","abstract":"The fine approach to measure information dependence is based on the total conditional complexity CT(y|x), which is defined as the minimal length of a total program that outputs y on the input x. It is known that the total conditional complexity can be much larger than than the plain conditional complexity. Such strings x, y are defined by means of a diagonal argument and are not otherwise interesting. In this paper we investigate whether this happens also for some natural objects. More specifically, we consider the following objects: the number of strings of complexity less than n and the lex first string of length n and complexity at least n. It is known that they have negligible mutual conditional complexities. In this paper we prove that their mutual total conditional complexities may be large. This is the first example of natural objects whose plain conditional complexity is much less than the total one.","sentences":["The fine approach to measure information dependence is based on the total conditional complexity CT(y|x), which is defined as the minimal length of a total program that outputs y on the input x.","It is known that the total conditional complexity can be much larger than than the plain conditional complexity.","Such strings x, y are defined by means of a diagonal argument and are not otherwise interesting.","In this paper we investigate whether this happens also for some natural objects.","More specifically, we consider the following objects: the number of strings of complexity less than n and the lex first string of length n and complexity at least n.","It is known that they have negligible mutual conditional complexities.","In this paper we prove that their mutual total conditional complexities may be large.","This is the first example of natural objects whose plain conditional complexity is much less than the total one."],"url":"http://arxiv.org/abs/2307.04530v1"}
{"created":"2023-07-10 12:56:41","title":"Cross-Layer Assisted Early Congestion Control for Cloud VR Services in 5G Edge Network","abstract":"Cloud virtual reality (VR) has emerged as a promising technology, offering users a highly immersive and easily accessible experience. However, the current 5G radio access network faces challenges in accommodating the bursty traffic generated by multiple cloudVR flows simultaneously, leading to congestion at the 5G base station and increased delays. In this research, we present a comprehensive quantitative analysis that highlights the underlying causes for the poor delay performance of cloudVR flows within the existing 5G protocol stack and network. To address these issues, we propose a novel cross-layer informationassisted congestion control mechanism deployed in the 5G edge network. Experiment results show that our mechanism enhances the number of concurrent flows meeting delay standards by 1.5x to 2.5x, while maintaining a smooth network load. These findings underscore the potential of leveraging 5G edge nodes as a valuable resource to effectively meet the anticipated demands of future services.","sentences":["Cloud virtual reality (VR) has emerged as a promising technology, offering users a highly immersive and easily accessible experience.","However, the current 5G radio access network faces challenges in accommodating the bursty traffic generated by multiple cloudVR flows simultaneously, leading to congestion at the 5G base station and increased delays.","In this research, we present a comprehensive quantitative analysis that highlights the underlying causes for the poor delay performance of cloudVR flows within the existing 5G protocol stack and network.","To address these issues, we propose a novel cross-layer informationassisted congestion control mechanism deployed in the 5G edge network.","Experiment results show that our mechanism enhances the number of concurrent flows meeting delay standards by 1.5x to 2.5x, while maintaining a smooth network load.","These findings underscore the potential of leveraging 5G edge nodes as a valuable resource to effectively meet the anticipated demands of future services."],"url":"http://arxiv.org/abs/2307.04529v1"}
{"created":"2023-07-10 12:49:59","title":"Self Expanding Neural Networks","abstract":"The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the \"rate\" at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.","sentences":["The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process.","In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so.","We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss.","We prove an upper bound on the \"rate\" at which neurons are added, and a computationally cheap lower bound on the expansion score.","We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori."],"url":"http://arxiv.org/abs/2307.04526v1"}
{"created":"2023-07-10 12:41:55","title":"Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor","abstract":"SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation. Its efficiency is directly influenced by feature matching. Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction. First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing. Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction. Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.","sentences":["SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation.","Its efficiency is directly influenced by feature matching.","Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook.","Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction.","First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets.","Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing.","Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching.","Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction.","Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets.","The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation."],"url":"http://arxiv.org/abs/2307.04520v1"}
{"created":"2023-07-10 12:31:27","title":"On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion","abstract":"This document chronicles this author's attempt to explore how words come to mean what they do, with a particular focus on child language acquisition and what that means for models of language understanding.\\footnote{I say \\emph{historical} because I synthesize the ideas based on when I discovered them and how those ideas influenced my later thinking.} I explain the setting for child language learning, how embodiment -- being able to perceive and enact in the world, including knowledge of concrete and abstract concepts -- is crucial, and how emotion and cognition relate to each other and the language learning process. I end with what I think are some of the requirements for a language-learning agent that learns language in a setting similar to that of children. This paper can act as a potential guide for ongoing and future work in modeling language.","sentences":["This document chronicles this author's attempt to explore how words come to mean what they do, with a particular focus on child language acquisition and what that means for models of language understanding.\\footnote{I say \\emph{historical} because I synthesize the ideas based on when I discovered them and how those ideas influenced my later thinking.}","I explain the setting for child language learning, how embodiment -- being able to perceive and enact in the world, including knowledge of concrete and abstract concepts -- is crucial, and how emotion and cognition relate to each other and the language learning process.","I end with what I think are some of the requirements for a language-learning agent that learns language in a setting similar to that of children.","This paper can act as a potential guide for ongoing and future work in modeling language."],"url":"http://arxiv.org/abs/2307.04518v1"}
{"created":"2023-07-10 12:24:04","title":"An Examination of Wearable Sensors and Video Data Capture for Human Exercise Classification","abstract":"Wearable sensors such as Inertial Measurement Units (IMUs) are often used to assess the performance of human exercise. Common approaches use handcrafted features based on domain expertise or automatically extracted features using time series analysis. Multiple sensors are required to achieve high classification accuracy, which is not very practical. These sensors require calibration and synchronization and may lead to discomfort over longer time periods. Recent work utilizing computer vision techniques has shown similar performance using video, without the need for manual feature engineering, and avoiding some pitfalls such as sensor calibration and placement on the body. In this paper, we compare the performance of IMUs to a video-based approach for human exercise classification on two real-world datasets consisting of Military Press and Rowing exercises. We compare the performance using a single camera that captures video in the frontal view versus using 5 IMUs placed on different parts of the body. We observe that an approach based on a single camera can outperform a single IMU by 10 percentage points on average. Additionally, a minimum of 3 IMUs are required to outperform a single camera. We observe that working with the raw data using multivariate time series classifiers outperforms traditional approaches based on handcrafted or automatically extracted features. Finally, we show that an ensemble model combining the data from a single camera with a single IMU outperforms either data modality. Our work opens up new and more realistic avenues for this application, where a video captured using a readily available smartphone camera, combined with a single sensor, can be used for effective human exercise classification.","sentences":["Wearable sensors such as Inertial Measurement Units (IMUs) are often used to assess the performance of human exercise.","Common approaches use handcrafted features based on domain expertise or automatically extracted features using time series analysis.","Multiple sensors are required to achieve high classification accuracy, which is not very practical.","These sensors require calibration and synchronization and may lead to discomfort over longer time periods.","Recent work utilizing computer vision techniques has shown similar performance using video, without the need for manual feature engineering, and avoiding some pitfalls such as sensor calibration and placement on the body.","In this paper, we compare the performance of IMUs to a video-based approach for human exercise classification on two real-world datasets consisting of Military Press and Rowing exercises.","We compare the performance using a single camera that captures video in the frontal view versus using 5 IMUs placed on different parts of the body.","We observe that an approach based on a single camera can outperform a single IMU by 10 percentage points on average.","Additionally, a minimum of 3 IMUs are required to outperform a single camera.","We observe that working with the raw data using multivariate time series classifiers outperforms traditional approaches based on handcrafted or automatically extracted features.","Finally, we show that an ensemble model combining the data from a single camera with a single IMU outperforms either data modality.","Our work opens up new and more realistic avenues for this application, where a video captured using a readily available smartphone camera, combined with a single sensor, can be used for effective human exercise classification."],"url":"http://arxiv.org/abs/2307.04516v1"}
{"created":"2023-07-10 12:22:08","title":"SAGC-A68: a space access graph dataset for the classification of spaces and space elements in apartment buildings","abstract":"The analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements. To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable. A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classification is a lack of suitable datasets. To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings. This graph-based dataset is well-suited for developing GDL models for space function and space element classification. To demonstrate the potential of the dataset, we employ it to train and evaluate a graph attention network (GAT) that predicts 22 space function and 6 space element classes. The dataset and code used in the experiment are available online. https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.","sentences":["The analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements.","To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable.","A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classification is a lack of suitable datasets.","To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings.","This graph-based dataset is well-suited for developing GDL models for space function and space element classification.","To demonstrate the potential of the dataset, we employ it to train and evaluate a graph attention network (GAT) that predicts 22 space function and 6 space element classes.","The dataset and code used in the experiment are available online.","https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68."],"url":"http://arxiv.org/abs/2307.04515v1"}
{"created":"2023-07-10 12:20:50","title":"Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature Product Manifold","abstract":"In graph representation learning, it is important that the complex geometric structure of the input graph, e.g. hidden relations among nodes, is well captured in embedding space. However, standard Euclidean embedding spaces have a limited capacity in representing graphs of varying structures. A promising candidate for the faithful embedding of data with varying structure is product manifolds of component spaces of different geometries (spherical, hyperbolic, or euclidean). In this paper, we take a closer look at the structure of product manifold embedding spaces and argue that each component space in a product contributes differently to expressing structures in the input graph, hence should be weighted accordingly. This is different from previous works which consider the roles of different components equally. We then propose WEIGHTED-PM, a data-driven method for learning embedding of heterogeneous graphs in weighted product manifolds. Our method utilizes the topological information of the input graph to automatically determine the weight of each component in product spaces. Extensive experiments on synthetic and real-world graph datasets demonstrate that WEIGHTED-PM is capable of learning better graph representations with lower geometric distortion from input data, and performs better on multiple downstream tasks, such as word similarity learning, top-$k$ recommendation, and knowledge graph embedding.","sentences":["In graph representation learning, it is important that the complex geometric structure of the input graph, e.g. hidden relations among nodes, is well captured in embedding space.","However, standard Euclidean embedding spaces have a limited capacity in representing graphs of varying structures.","A promising candidate for the faithful embedding of data with varying structure is product manifolds of component spaces of different geometries (spherical, hyperbolic, or euclidean).","In this paper, we take a closer look at the structure of product manifold embedding spaces and argue that each component space in a product contributes differently to expressing structures in the input graph, hence should be weighted accordingly.","This is different from previous works which consider the roles of different components equally.","We then propose WEIGHTED-PM, a data-driven method for learning embedding of heterogeneous graphs in weighted product manifolds.","Our method utilizes the topological information of the input graph to automatically determine the weight of each component in product spaces.","Extensive experiments on synthetic and real-world graph datasets demonstrate that WEIGHTED-PM is capable of learning better graph representations with lower geometric distortion from input data, and performs better on multiple downstream tasks, such as word similarity learning, top-$k$ recommendation, and knowledge graph embedding."],"url":"http://arxiv.org/abs/2307.04514v1"}
{"created":"2023-07-10 12:01:18","title":"Improving Factuality of Abstractive Summarization via Contrastive Reward Learning","abstract":"Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries.","sentences":["Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information.","In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics.","Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations.","This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries."],"url":"http://arxiv.org/abs/2307.04507v1"}
{"created":"2023-07-10 11:55:44","title":"Deductive Controller Synthesis for Probabilistic Hyperproperties","abstract":"Probabilistic hyperproperties specify quantitative relations between the probabilities of reaching different target sets of states from different initial sets of states. This class of behavioral properties is suitable for capturing important security, privacy, and system-level requirements. We propose a new approach to solve the controller synthesis problem for Markov decision processes (MDPs) and probabilistic hyperproperties. Our specification language builds on top of the logic HyperPCTL and enhances it with structural constraints over the synthesized controllers. Our approach starts from a family of controllers represented symbolically and defined over the same copy of an MDP. We then introduce an abstraction refinement strategy that can relate multiple computation trees and that we employ to prune the search space deductively. The experimental evaluation demonstrates that the proposed approach considerably outperforms HyperProb, a state-of-the-art SMT-based model checking tool for HyperPCTL. Moreover, our approach is the first one that is able to effectively combine probabilistic hyperproperties with additional intra-controller constraints (e.g. partial observability) as well as inter-controller constraints (e.g. agreements on a common action).","sentences":["Probabilistic hyperproperties specify quantitative relations between the probabilities of reaching different target sets of states from different initial sets of states.","This class of behavioral properties is suitable for capturing important security, privacy, and system-level requirements.","We propose a new approach to solve the controller synthesis problem for Markov decision processes (MDPs) and probabilistic hyperproperties.","Our specification language builds on top of the logic HyperPCTL and enhances it with structural constraints over the synthesized controllers.","Our approach starts from a family of controllers represented symbolically and defined over the same copy of an MDP.","We then introduce an abstraction refinement strategy that can relate multiple computation trees and that we employ to prune the search space deductively.","The experimental evaluation demonstrates that the proposed approach considerably outperforms HyperProb, a state-of-the-art SMT-based model checking tool for HyperPCTL.","Moreover, our approach is the first one that is able to effectively combine probabilistic hyperproperties with additional intra-controller constraints (e.g. partial observability) as well as inter-controller constraints (e.g. agreements on a common action)."],"url":"http://arxiv.org/abs/2307.04503v1"}
{"created":"2023-07-10 11:45:28","title":"A Privacy-Preserving and Accountable Billing Protocol for Peer-to-Peer Energy Trading Markets","abstract":"This paper proposes a privacy-preserving and accountable billing (PA-Bill) protocol for trading in peer-to-peer energy markets, addressing situations where there may be discrepancies between the volume of energy committed and delivered. Such discrepancies can lead to challenges in providing both privacy and accountability while maintaining accurate billing. To overcome these challenges, a universal cost splitting mechanism is proposed that prioritises privacy and accountability. It leverages a homomorphic encryption cryptosystem to provide privacy and employs blockchain technology to establish accountability. A dispute resolution mechanism is also introduced to minimise the occurrence of erroneous bill calculations while ensuring accountability and non-repudiation throughout the billing process. Our evaluation demonstrates that PA-Bill offers an effective billing mechanism that maintains privacy and accountability in peer-to-peer energy markets utilising a semi-decentralised approach.","sentences":["This paper proposes a privacy-preserving and accountable billing (PA-Bill) protocol for trading in peer-to-peer energy markets, addressing situations where there may be discrepancies between the volume of energy committed and delivered.","Such discrepancies can lead to challenges in providing both privacy and accountability while maintaining accurate billing.","To overcome these challenges, a universal cost splitting mechanism is proposed that prioritises privacy and accountability.","It leverages a homomorphic encryption cryptosystem to provide privacy and employs blockchain technology to establish accountability.","A dispute resolution mechanism is also introduced to minimise the occurrence of erroneous bill calculations while ensuring accountability and non-repudiation throughout the billing process.","Our evaluation demonstrates that PA-Bill offers an effective billing mechanism that maintains privacy and accountability in peer-to-peer energy markets utilising a semi-decentralised approach."],"url":"http://arxiv.org/abs/2307.04501v1"}
{"created":"2023-07-10 11:41:20","title":"Algorithm- Versus Human-Generated Academic Plans: Determining Optimality from Community College Articulation Agreements","abstract":"We developed a low-fidelity prototype of a report that contains an algorithmically-generated optimal academic plan. Optimal is defined as the minimal set of community college courses that satisfy the transfer requirements for multiple universities a student is preparing to apply to. We recruited 24 California community college transfer students to participate in a research session, consisting of an experiment, survey, and interview. We experimentally compared the prototype to ASSIST, California's official statewide database of articulation agreement reports. Compared to students who used the prototype, students assigned to use ASSIST reports to manually create an optimal academic plan underperformed in optimality mistakes, time required, and usability scores. Moving to our non-experimental results, a sizable minority of students had a negative assessment of counselors' ability and willingness to manually create optimal academic plans using ASSIST. Our last results revolved around students' recommendations for supplemental software features to improve the optimization prototype.","sentences":["We developed a low-fidelity prototype of a report that contains an algorithmically-generated optimal academic plan.","Optimal is defined as the minimal set of community college courses that satisfy the transfer requirements for multiple universities a student is preparing to apply to.","We recruited 24 California community college transfer students to participate in a research session, consisting of an experiment, survey, and interview.","We experimentally compared the prototype to ASSIST, California's official statewide database of articulation agreement reports.","Compared to students who used the prototype, students assigned to use ASSIST reports to manually create an optimal academic plan underperformed in optimality mistakes, time required, and usability scores.","Moving to our non-experimental results, a sizable minority of students had a negative assessment of counselors' ability and willingness to manually create optimal academic plans using ASSIST.","Our last results revolved around students' recommendations for supplemental software features to improve the optimization prototype."],"url":"http://arxiv.org/abs/2307.04500v1"}
{"created":"2023-07-10 11:40:52","title":"First order synthesis for data words revisited","abstract":"We carry on the study of the synthesis problem on data words for fragments of first order logic, and delineate precisely the border between decidability and undecidability.","sentences":["We carry on the study of the synthesis problem on data words for fragments of first order logic, and delineate precisely the border between decidability and undecidability."],"url":"http://arxiv.org/abs/2307.04499v1"}
{"created":"2023-07-10 11:39:18","title":"RCS-based Quasi-Deterministic Ray Tracing for Statistical Channel Modeling","abstract":"This paper presents a quasi-deterministic ray tracing (QD-RT) method for analyzing the propagation of electromagnetic waves in street canyons. The method uses a statistical bistatic distribution to model the Radar Cross Section (RCS) of various irregular objects such as cars and pedestrians, instead of relying on exact values as in a deterministic propagation model. The performance of the QD-RT method is evaluated by comparing its generated path loss distributions to those of the deterministic ray tracing (D-RT) model using the Two-sample Cramer-von Mises test. The results indicate that the QD-RT method generates the same path loss distributions as the D-RT model while offering lower complexity. This study suggests that the QD-RT method has the potential to be used for analyzing complicated scenarios such as street canyon scenarios in mmWave wireless communication systems.","sentences":["This paper presents a quasi-deterministic ray tracing (QD-RT) method for analyzing the propagation of electromagnetic waves in street canyons.","The method uses a statistical bistatic distribution to model the Radar Cross Section (RCS) of various irregular objects such as cars and pedestrians, instead of relying on exact values as in a deterministic propagation model.","The performance of the QD-RT method is evaluated by comparing its generated path loss distributions to those of the deterministic ray tracing (D-RT) model using the Two-sample Cramer-von Mises test.","The results indicate that the QD-RT method generates the same path loss distributions as the D-RT model while offering lower complexity.","This study suggests that the QD-RT method has the potential to be used for analyzing complicated scenarios such as street canyon scenarios in mmWave wireless communication systems."],"url":"http://arxiv.org/abs/2307.04498v1"}
{"created":"2023-07-10 11:33:46","title":"Enabling Faster Locomotion of Planetary Rovers with a Mechanically-Hybrid Suspension","abstract":"The exploration of the lunar poles and the collection of samples from the martian surface are characterized by shorter time windows demanding increased autonomy and speeds. Autonomous mobile robots must intrinsically cope with a wider range of disturbances. Faster off-road navigation has been explored for terrestrial applications but the combined effects of increased speeds and reduced gravity fields are yet to be fully studied. In this paper, we design and demonstrate a novel fully passive suspension design for wheeled planetary robots, which couples a high-range passive rocker with elastic in-wheel coil-over shock absorbers. The design was initially conceived and verified in a reduced-gravity (1.625 m/s$^2$) simulated environment, where three different passive suspension configurations were evaluated against a set of challenges--climbing steep slopes and surmounting unexpected obstacles like rocks and outcrops--and later prototyped and validated in a series of field tests. The proposed mechanically-hybrid suspension proves to mitigate more effectively the negative effects (high-frequency/high-amplitude vibrations and impact loads) of faster locomotion (>1 m/s) over unstructured terrains under varied gravity fields. This lowers the demand on navigation and control systems, impacting the efficiency of exploration missions in the years to come.","sentences":["The exploration of the lunar poles and the collection of samples from the martian surface are characterized by shorter time windows demanding increased autonomy and speeds.","Autonomous mobile robots must intrinsically cope with a wider range of disturbances.","Faster off-road navigation has been explored for terrestrial applications but the combined effects of increased speeds and reduced gravity fields are yet to be fully studied.","In this paper, we design and demonstrate a novel fully passive suspension design for wheeled planetary robots, which couples a high-range passive rocker with elastic in-wheel coil-over shock absorbers.","The design was initially conceived and verified in a reduced-gravity (1.625 m/s$^2$) simulated environment, where three different passive suspension configurations were evaluated against a set of challenges--climbing steep slopes and surmounting unexpected obstacles like rocks and outcrops--and later prototyped and validated in a series of field tests.","The proposed mechanically-hybrid suspension proves to mitigate more effectively the negative effects (high-frequency/high-amplitude vibrations and impact loads) of faster locomotion (>1 m/s) over unstructured terrains under varied gravity fields.","This lowers the demand on navigation and control systems, impacting the efficiency of exploration missions in the years to come."],"url":"http://arxiv.org/abs/2307.04494v1"}
{"created":"2023-07-10 11:33:46","title":"Model-Driven Engineering Method to Support the Formalization of Machine Learning using SysML","abstract":"Methods: This work introduces a method supporting the collaborative definition of machine learning tasks by leveraging model-based engineering in the formalization of the systems modeling language SysML. The method supports the identification and integration of various data sources, the required definition of semantic connections between data attributes, and the definition of data processing steps within the machine learning support.   Results: By consolidating the knowledge of domain and machine learning experts, a powerful tool to describe machine learning tasks by formalizing knowledge using the systems modeling language SysML is introduced. The method is evaluated based on two use cases, i.e., a smart weather system that allows to predict weather forecasts based on sensor data, and a waste prevention case for 3D printer filament that cancels the printing if the intended result cannot be achieved (image processing). Further, a user study is conducted to gather insights of potential users regarding perceived workload and usability of the elaborated method.   Conclusion: Integrating machine learning-specific properties in systems engineering techniques allows non-data scientists to understand formalized knowledge and define specific aspects of a machine learning problem, document knowledge on the data, and to further support data scientists to use the formalized knowledge as input for an implementation using (semi-) automatic code generation. In this respect, this work contributes by consolidating knowledge from various domains and therefore, fosters the integration of machine learning in industry by involving several stakeholders.","sentences":["Methods: This work introduces a method supporting the collaborative definition of machine learning tasks by leveraging model-based engineering in the formalization of the systems modeling language SysML.","The method supports the identification and integration of various data sources, the required definition of semantic connections between data attributes, and the definition of data processing steps within the machine learning support.   ","Results:","By consolidating the knowledge of domain and machine learning experts, a powerful tool to describe machine learning tasks by formalizing knowledge using the systems modeling language SysML is introduced.","The method is evaluated based on two use cases, i.e., a smart weather system that allows to predict weather forecasts based on sensor data, and a waste prevention case for 3D printer filament that cancels the printing if the intended result cannot be achieved (image processing).","Further, a user study is conducted to gather insights of potential users regarding perceived workload and usability of the elaborated method.   ","Conclusion: Integrating machine learning-specific properties in systems engineering techniques allows non-data scientists to understand formalized knowledge and define specific aspects of a machine learning problem, document knowledge on the data, and to further support data scientists to use the formalized knowledge as input for an implementation using (semi-) automatic code generation.","In this respect, this work contributes by consolidating knowledge from various domains and therefore, fosters the integration of machine learning in industry by involving several stakeholders."],"url":"http://arxiv.org/abs/2307.04495v1"}
{"created":"2023-07-10 11:31:15","title":"Geometric Constraints in Probabilistic Manifolds: A Bridge from Molecular Dynamics to Structured Diffusion Processes","abstract":"Understanding the macroscopic characteristics of biological complexes demands precision and specificity in statistical ensemble modeling. One of the primary challenges in this domain lies in sampling from particular subsets of the state-space, driven either by existing structural knowledge or specific areas of interest within the state-space. We propose a method that enables sampling from distributions that rigorously adhere to arbitrary sets of geometric constraints in Euclidean spaces. This is achieved by integrating a constraint projection operator within the well-regarded architecture of Denoising Diffusion Probabilistic Models, a framework founded in generative modeling and probabilistic inference. The significance of this work becomes apparent, for instance, in the context of deep learning-based drug design, where it is imperative to maintain specific molecular profile interactions to realize the desired therapeutic outcomes and guarantee safety.","sentences":["Understanding the macroscopic characteristics of biological complexes demands precision and specificity in statistical ensemble modeling.","One of the primary challenges in this domain lies in sampling from particular subsets of the state-space, driven either by existing structural knowledge or specific areas of interest within the state-space.","We propose a method that enables sampling from distributions that rigorously adhere to arbitrary sets of geometric constraints in Euclidean spaces.","This is achieved by integrating a constraint projection operator within the well-regarded architecture of Denoising Diffusion Probabilistic Models, a framework founded in generative modeling and probabilistic inference.","The significance of this work becomes apparent, for instance, in the context of deep learning-based drug design, where it is imperative to maintain specific molecular profile interactions to realize the desired therapeutic outcomes and guarantee safety."],"url":"http://arxiv.org/abs/2307.04493v1"}
{"created":"2023-07-10 11:30:46","title":"Calculating Originality of LLM Assisted Source Code","abstract":"The ease of using a Large Language Model (LLM) to answer a wide variety of queries and their high availability has resulted in LLMs getting integrated into various applications. LLM-based recommenders are now routinely used by students as well as professional software programmers for code generation and testing. Though LLM-based technology has proven useful, its unethical and unattributed use by students and professionals is a growing cause of concern. As such, there is a need for tools and technologies which may assist teachers and other evaluators in identifying whether any portion of a source code is LLM generated.   In this paper, we propose a neural network-based tool that instructors can use to determine the original effort (and LLM's contribution) put by students in writing source codes. Our tool is motivated by minimum description length measures like Kolmogorov complexity. Our initial experiments with moderate sized (up to 500 lines of code) have shown promising results that we report in this paper.","sentences":["The ease of using a Large Language Model (LLM) to answer a wide variety of queries and their high availability has resulted in LLMs getting integrated into various applications.","LLM-based recommenders are now routinely used by students as well as professional software programmers for code generation and testing.","Though LLM-based technology has proven useful, its unethical and unattributed use by students and professionals is a growing cause of concern.","As such, there is a need for tools and technologies which may assist teachers and other evaluators in identifying whether any portion of a source code is LLM generated.   ","In this paper, we propose a neural network-based tool that instructors can use to determine the original effort (and LLM's contribution) put by students in writing source codes.","Our tool is motivated by minimum description length measures like Kolmogorov complexity.","Our initial experiments with moderate sized (up to 500 lines of code) have shown promising results that we report in this paper."],"url":"http://arxiv.org/abs/2307.04492v1"}
{"created":"2023-07-10 11:23:40","title":"Predicting Memory Demands of BDD Operations using Maximum Graph Cuts (Extended Paper)","abstract":"The BDD package Adiar manipulates Binary Decision Diagrams (BDDs) in external memory. This enables handling big BDDs, but the performance suffers when dealing with moderate-sized BDDs. This is mostly due to initializing expensive external memory data structures, even if their contents can fit entirely inside internal memory.   The contents of these auxiliary data structures always correspond to a graph cut in an input or output BDD. Specifically, these cuts respect the levels of the BDD. We formalise the shape of these cuts and prove sound upper bounds on their maximum size for each BDD operation.   We have implemented these upper bounds within Adiar. With these bounds, it can predict whether a faster internal memory variant of the auxiliary data structures can be used. In practice, this improves Adiar's running time across the board. Specifically for the moderate-sized BDDs, this results in an average reduction of the computation time by 86.1% (median of 89.7%). In some cases, the difference is even 99.9\\%. When checking equivalence of hardware circuits from the EPFL Benchmark Suite, for one of the instances the time was decreased by 52 hours.","sentences":["The BDD package Adiar manipulates Binary Decision Diagrams (BDDs) in external memory.","This enables handling big BDDs, but the performance suffers when dealing with moderate-sized BDDs.","This is mostly due to initializing expensive external memory data structures, even if their contents can fit entirely inside internal memory.   ","The contents of these auxiliary data structures always correspond to a graph cut in an input or output BDD.","Specifically, these cuts respect the levels of the BDD.","We formalise the shape of these cuts and prove sound upper bounds on their maximum size for each BDD operation.   ","We have implemented these upper bounds within Adiar.","With these bounds, it can predict whether a faster internal memory variant of the auxiliary data structures can be used.","In practice, this improves Adiar's running time across the board.","Specifically for the moderate-sized BDDs, this results in an average reduction of the computation time by 86.1% (median of 89.7%).","In some cases, the difference is even 99.9\\%.","When checking equivalence of hardware circuits from the EPFL Benchmark Suite, for one of the instances the time was decreased by 52 hours."],"url":"http://arxiv.org/abs/2307.04488v1"}
{"created":"2023-07-10 11:14:19","title":"Invertible Low-Dimensional Modelling of X-ray Absorption Spectra for Potential Applications in Spectral X-ray Imaging","abstract":"X-ray interaction with matter is an energy-dependent process that is contingent on the atomic structure of the constituent material elements. The most advanced models to capture this relationship currently rely on Monte Carlo (MC) simulations. Whilst these very accurate models, in many problems in spectral X-ray imaging, such as data compression, noise removal, spectral estimation, and the quantitative measurement of material compositions, these models are of limited use, as these applications typically require the efficient inversion of the model, that is, they require the estimation of the best model parameters for a given spectral measurement. Current models that can be easily inverted however typically only work when modelling spectra in regions away from their K-edges, so they have limited utility when modelling a wider range of materials. In this paper, we thus propose a novel, non-linear model that combines a deep neural network autoencoder with an optimal linear model based on the Singular Value Decomposition (SVD). We compare our new method to other alternative linear and non-linear approaches, a sparse model and an alternative deep learning model. We demonstrate the advantages of our method over traditional models, especially when modelling X-ray absorption spectra that contain K-edges in the energy range of interest.","sentences":["X-ray interaction with matter is an energy-dependent process that is contingent on the atomic structure of the constituent material elements.","The most advanced models to capture this relationship currently rely on Monte Carlo (MC) simulations.","Whilst these very accurate models, in many problems in spectral X-ray imaging, such as data compression, noise removal, spectral estimation, and the quantitative measurement of material compositions, these models are of limited use, as these applications typically require the efficient inversion of the model, that is, they require the estimation of the best model parameters for a given spectral measurement.","Current models that can be easily inverted however typically only work when modelling spectra in regions away from their K-edges, so they have limited utility when modelling a wider range of materials.","In this paper, we thus propose a novel, non-linear model that combines a deep neural network autoencoder with an optimal linear model based on the Singular Value Decomposition (SVD).","We compare our new method to other alternative linear and non-linear approaches, a sparse model and an alternative deep learning model.","We demonstrate the advantages of our method over traditional models, especially when modelling X-ray absorption spectra that contain K-edges in the energy range of interest."],"url":"http://arxiv.org/abs/2307.04484v1"}
{"created":"2023-07-10 11:03:32","title":"Digital Modeling for Everyone: Exploring How Novices Approach Voice-Based 3D Modeling","abstract":"Manufacturing tools like 3D printers have become accessible to the wider society, making the promise of digital fabrication for everyone seemingly reachable. While the actual manufacturing process is largely automated today, users still require knowledge of complex design applications to produce ready-designed objects and adapt them to their needs or design new objects from scratch. To lower the barrier to the design and customization of personalized 3D models, we explored novice mental models in voice-based 3D modeling by conducting a high-fidelity Wizard of Oz study with 22 participants. We performed a thematic analysis of the collected data to understand how the mental model of novices translates into voice-based 3D modeling. We conclude with design implications for voice assistants. For example, they have to: deal with vague, incomplete and wrong commands; provide a set of straightforward commands to shape simple and composite objects; and offer different strategies to select 3D objects.","sentences":["Manufacturing tools like 3D printers have become accessible to the wider society, making the promise of digital fabrication for everyone seemingly reachable.","While the actual manufacturing process is largely automated today, users still require knowledge of complex design applications to produce ready-designed objects and adapt them to their needs or design new objects from scratch.","To lower the barrier to the design and customization of personalized 3D models, we explored novice mental models in voice-based 3D modeling by conducting a high-fidelity Wizard of Oz study with 22 participants.","We performed a thematic analysis of the collected data to understand how the mental model of novices translates into voice-based 3D modeling.","We conclude with design implications for voice assistants.","For example, they have to: deal with vague, incomplete and wrong commands; provide a set of straightforward commands to shape simple and composite objects; and offer different strategies to select 3D objects."],"url":"http://arxiv.org/abs/2307.04481v1"}
{"created":"2023-07-10 11:01:41","title":"A Linear Time Quantum Algorithm for Pairwise Sequence Alignment","abstract":"Sequence Alignment is the process of aligning biological sequences in order to identify similarities between multiple sequences. In this paper, a Quantum Algorithm for finding the optimal alignment between DNA sequences has been demonstrated which works by mapping the sequence alignment problem into a path-searching problem through a 2D graph. The transition, which converges to a fixed path on the graph, is based on a proposed oracle for profit calculation. By implementing Grover's search algorithm, our proposed approach is able to align a pair of sequences and figure out the optimal alignment within linear time, which hasn't been attained by any classical deterministic algorithm. In addition to that, the proposed algorithm is capable of quadratic speeding up to any unstructured search problem by finding out the optimal paths accurately in a deterministic manner, in contrast to existing randomized algorithms that frequently sort out the sub-optimal alignments, therefore, don't always guarantee of finding out the optimal solutions.","sentences":["Sequence Alignment is the process of aligning biological sequences in order to identify similarities between multiple sequences.","In this paper, a Quantum Algorithm for finding the optimal alignment between DNA sequences has been demonstrated which works by mapping the sequence alignment problem into a path-searching problem through a 2D graph.","The transition, which converges to a fixed path on the graph, is based on a proposed oracle for profit calculation.","By implementing Grover's search algorithm, our proposed approach is able to align a pair of sequences and figure out the optimal alignment within linear time, which hasn't been attained by any classical deterministic algorithm.","In addition to that, the proposed algorithm is capable of quadratic speeding up to any unstructured search problem by finding out the optimal paths accurately in a deterministic manner, in contrast to existing randomized algorithms that frequently sort out the sub-optimal alignments, therefore, don't always guarantee of finding out the optimal solutions."],"url":"http://arxiv.org/abs/2307.04479v1"}
{"created":"2023-07-10 11:01:10","title":"A closed form exact formulation of the spectral representation of a second-order symmetric tensor and of its derivatives","abstract":"The spectral decomposition of a symmetric, second-order tensor is widely adopted in many fields of Computational Mechanics. As an example, in elasto-plasticity under large strain and rotations, given the Cauchy deformation tensor, it is a fundamental step to compute the logarithmic strain tensor.   Recently, this approach has been also adopted in small-strain isotropic plasticity to reconstruct the stress tensor as a function of its eigenvalues, allowing the formulation of predictor-corrector return algorithms in the invariants space. These algorithms not only reduce the number of unknowns at the constitutive level, but also allow the correct handling of stress states in which the plastic normals are undefined, thus ensuring a better convergence with respect to the standard approach.   While the eigenvalues of a symmetric, second-order tensor can be simply computed as a function of the tensor invariants, the computation of its eigenbasis can be more difficult, especially when two or more eigenvalues are coincident. Moreover, when a Newton-Rhapson algorithm is adopted to solve nonlinear problems in Computational Mechanics, also the tensorial derivatives of the eigenbasis, whose computation is still more complicate, are required to assemble the tangent matrix.   A simple and comprehensive method is presented, which can be adopted to compute a closed form representation of a second-order tensor, as well as their derivatives with respect to the tensor itself, allowing a simpler implementation of spectral decomposition of a tensor in Computational Mechanics applications.","sentences":["The spectral decomposition of a symmetric, second-order tensor is widely adopted in many fields of Computational Mechanics.","As an example, in elasto-plasticity under large strain and rotations, given the Cauchy deformation tensor, it is a fundamental step to compute the logarithmic strain tensor.   ","Recently, this approach has been also adopted in small-strain isotropic plasticity to reconstruct the stress tensor as a function of its eigenvalues, allowing the formulation of predictor-corrector return algorithms in the invariants space.","These algorithms not only reduce the number of unknowns at the constitutive level, but also allow the correct handling of stress states in which the plastic normals are undefined, thus ensuring a better convergence with respect to the standard approach.   ","While the eigenvalues of a symmetric, second-order tensor can be simply computed as a function of the tensor invariants, the computation of its eigenbasis can be more difficult, especially when two or more eigenvalues are coincident.","Moreover, when a Newton-Rhapson algorithm is adopted to solve nonlinear problems in Computational Mechanics, also the tensorial derivatives of the eigenbasis, whose computation is still more complicate, are required to assemble the tangent matrix.   ","A simple and comprehensive method is presented, which can be adopted to compute a closed form representation of a second-order tensor, as well as their derivatives with respect to the tensor itself, allowing a simpler implementation of spectral decomposition of a tensor in Computational Mechanics applications."],"url":"http://arxiv.org/abs/2307.04478v1"}
{"created":"2023-07-10 10:42:48","title":"Partial Vessels Annotation-based Coronary Artery Segmentation with Self-training and Prototype Learning","abstract":"Coronary artery segmentation on coronary-computed tomography angiography (CCTA) images is crucial for clinical use. Due to the expertise-required and labor-intensive annotation process, there is a growing demand for the relevant label-efficient learning algorithms. To this end, we propose partial vessels annotation (PVA) based on the challenges of coronary artery segmentation and clinical diagnostic characteristics. Further, we propose a progressive weakly supervised learning framework to achieve accurate segmentation under PVA. First, our proposed framework learns the local features of vessels to propagate the knowledge to unlabeled regions. Subsequently, it learns the global structure by utilizing the propagated knowledge, and corrects the errors introduced in the propagation process. Finally, it leverages the similarity between feature embeddings and the feature prototype to enhance testing outputs. Experiments on clinical data reveals that our proposed framework outperforms the competing methods under PVA (24.29% vessels), and achieves comparable performance in trunk continuity with the baseline model using full annotation (100% vessels).","sentences":["Coronary artery segmentation on coronary-computed tomography angiography (CCTA) images is crucial for clinical use.","Due to the expertise-required and labor-intensive annotation process, there is a growing demand for the relevant label-efficient learning algorithms.","To this end, we propose partial vessels annotation (PVA) based on the challenges of coronary artery segmentation and clinical diagnostic characteristics.","Further, we propose a progressive weakly supervised learning framework to achieve accurate segmentation under PVA.","First, our proposed framework learns the local features of vessels to propagate the knowledge to unlabeled regions.","Subsequently, it learns the global structure by utilizing the propagated knowledge, and corrects the errors introduced in the propagation process.","Finally, it leverages the similarity between feature embeddings and the feature prototype to enhance testing outputs.","Experiments on clinical data reveals that our proposed framework outperforms the competing methods under PVA (24.29% vessels), and achieves comparable performance in trunk continuity with the baseline model using full annotation (100% vessels)."],"url":"http://arxiv.org/abs/2307.04472v1"}
{"created":"2023-07-10 10:40:44","title":"Test-Time Adaptation for Nighttime Color-Thermal Semantic Segmentation","abstract":"The ability to scene understanding in adverse visual conditions, e.g., nighttime, has sparked active research for RGB-Thermal (RGB-T) semantic segmentation. However, it is essentially hampered by two critical problems: 1) the day-night gap of RGB images is larger than that of thermal images, and 2) the class-wise performance of RGB images at night is not consistently higher or lower than that of thermal images. we propose the first test-time adaptation (TTA) framework, dubbed Night-TTA, to address the problems for nighttime RGBT semantic segmentation without access to the source (daytime) data during adaptation. Our method enjoys three key technical parts. Firstly, as one modality (e.g., RGB) suffers from a larger domain gap than that of the other (e.g., thermal), Imaging Heterogeneity Refinement (IHR) employs an interaction branch on the basis of RGB and thermal branches to prevent cross-modal discrepancy and performance degradation. Then, Class Aware Refinement (CAR) is introduced to obtain reliable ensemble logits based on pixel-level distribution aggregation of the three branches. In addition, we also design a specific learning scheme for our TTA framework, which enables the ensemble logits and three student logits to collaboratively learn to improve the quality of predictions during the testing phase of our Night TTA. Extensive experiments show that our method achieves state-of-the-art (SoTA) performance with a 13.07% boost in mIoU.","sentences":["The ability to scene understanding in adverse visual conditions, e.g., nighttime, has sparked active research for RGB-Thermal (RGB-T) semantic segmentation.","However, it is essentially hampered by two critical problems: 1) the day-night gap of RGB images is larger than that of thermal images, and 2) the class-wise performance of RGB images at night is not consistently higher or lower than that of thermal images.","we propose the first test-time adaptation (TTA) framework, dubbed Night-TTA, to address the problems for nighttime RGBT semantic segmentation without access to the source (daytime) data during adaptation.","Our method enjoys three key technical parts.","Firstly, as one modality (e.g., RGB) suffers from a larger domain gap than that of the other (e.g., thermal), Imaging Heterogeneity Refinement (IHR) employs an interaction branch on the basis of RGB and thermal branches to prevent cross-modal discrepancy and performance degradation.","Then, Class Aware Refinement (CAR) is introduced to obtain reliable ensemble logits based on pixel-level distribution aggregation of the three branches.","In addition, we also design a specific learning scheme for our TTA framework, which enables the ensemble logits and three student logits to collaboratively learn to improve the quality of predictions during the testing phase of our Night TTA.","Extensive experiments show that our method achieves state-of-the-art (SoTA) performance with a 13.07% boost in mIoU."],"url":"http://arxiv.org/abs/2307.04470v1"}
{"created":"2023-07-10 10:34:12","title":"Badgers: generating data quality deficits with Python","abstract":"Generating context specific data quality deficits is necessary to experimentally assess data quality of data-driven (artificial intelligence (AI) or machine learning (ML)) applications. In this paper we present badgers, an extensible open-source Python library to generate data quality deficits (outliers, imbalanced data, drift, etc.) for different modalities (tabular data, time-series, text, etc.). The documentation is accessible at https://fraunhofer-iese.github.io/badgers/ and the source code at https://github.com/Fraunhofer-IESE/badgers","sentences":["Generating context specific data quality deficits is necessary to experimentally assess data quality of data-driven (artificial intelligence (AI) or machine learning (ML)) applications.","In this paper we present badgers, an extensible open-source Python library to generate data quality deficits (outliers, imbalanced data, drift, etc.) for different modalities (tabular data, time-series, text, etc.).","The documentation is accessible at https://fraunhofer-iese.github.io/badgers/ and the source code at https://github.com/Fraunhofer-IESE/badgers"],"url":"http://arxiv.org/abs/2307.04468v1"}
{"created":"2023-07-10 10:20:09","title":"Utilising Explanations to Mitigate Robot Conversational Failures","abstract":"This paper presents an overview of robot failure detection work from HRI and adjacent fields using failures as an opportunity to examine robot explanation behaviours. As humanoid robots remain experimental tools in the early 2020s, interactions with robots are situated overwhelmingly in controlled environments, typically studying various interactional phenomena. Such interactions suffer from real-world and large-scale experimentation and tend to ignore the 'imperfectness' of the everyday user. Robot explanations can be used to approach and mitigate failures, by expressing robot legibility and incapability, and within the perspective of common-ground. In this paper, I discuss how failures present opportunities for explanations in interactive conversational robots and what the potentials are for the intersection of HRI and explainability research.","sentences":["This paper presents an overview of robot failure detection work from HRI and adjacent fields using failures as an opportunity to examine robot explanation behaviours.","As humanoid robots remain experimental tools in the early 2020s, interactions with robots are situated overwhelmingly in controlled environments, typically studying various interactional phenomena.","Such interactions suffer from real-world and large-scale experimentation and tend to ignore the 'imperfectness' of the everyday user.","Robot explanations can be used to approach and mitigate failures, by expressing robot legibility and incapability, and within the perspective of common-ground.","In this paper, I discuss how failures present opportunities for explanations in interactive conversational robots and what the potentials are for the intersection of HRI and explainability research."],"url":"http://arxiv.org/abs/2307.04462v1"}
{"created":"2023-07-10 10:16:57","title":"Multi-modal Graph Learning over UMLS Knowledge Graphs","abstract":"Clinicians are increasingly looking towards machine learning to gain insights about patient evolutions. We propose a novel approach named Multi-Modal UMLS Graph Learning (MMUGL) for learning meaningful representations of medical concepts using graph neural networks over knowledge graphs based on the unified medical language system. These representations are aggregated to represent entire patient visits and then fed into a sequence model to perform predictions at the granularity of multiple hospital visits of a patient. We improve performance by incorporating prior medical knowledge and considering multiple modalities. We compare our method to existing architectures proposed to learn representations at different granularities on the MIMIC-III dataset and show that our approach outperforms these methods. The results demonstrate the significance of multi-modal medical concept representations based on prior medical knowledge.","sentences":["Clinicians are increasingly looking towards machine learning to gain insights about patient evolutions.","We propose a novel approach named Multi-Modal UMLS Graph Learning (MMUGL) for learning meaningful representations of medical concepts using graph neural networks over knowledge graphs based on the unified medical language system.","These representations are aggregated to represent entire patient visits and then fed into a sequence model to perform predictions at the granularity of multiple hospital visits of a patient.","We improve performance by incorporating prior medical knowledge and considering multiple modalities.","We compare our method to existing architectures proposed to learn representations at different granularities on the MIMIC-III dataset and show that our approach outperforms these methods.","The results demonstrate the significance of multi-modal medical concept representations based on prior medical knowledge."],"url":"http://arxiv.org/abs/2307.04461v1"}
{"created":"2023-07-10 10:12:21","title":"Analyzing the Evolution of Inter-package Dependencies in Operating Systems: A Case Study of Ubuntu","abstract":"An Operating System (OS) combines multiple interdependent software packages, which usually have their own independently developed architectures. When a multitude of independent packages are placed together in an OS, an implicit inter-package architecture is formed. For an evolutionary effort, designers/developers of OS can greatly benefit from fully understanding the system-wide dependency focused on individual files, specifically executable files, and dynamically loadable libraries. We propose a framework, DepEx, aimed at discovering the detailed package relations at the level of individual binary files and their associated evolutionary changes. We demonstrate the utility of DepEx by systematically investigating the evolution of a large-scale Open Source OS, Ubuntu. DepEx enabled us to systematically acquire and analyze the dependencies in different versions of Ubuntu released between 2005 (5.04) to 2023 (23.04). Our analysis revealed various evolutionary trends in package management and their implications based on the analysis of the 84 consecutive versions available for download (these include beta versions). This study has enabled us to assert that DepEx can provide researchers and practitioners with a better understanding of the implicit software dependencies in order to improve the stability, performance, and functionality of their software as well as to reduce the risk of issues arising during maintenance, updating, or migration.","sentences":["An Operating System (OS) combines multiple interdependent software packages, which usually have their own independently developed architectures.","When a multitude of independent packages are placed together in an OS, an implicit inter-package architecture is formed.","For an evolutionary effort, designers/developers of OS can greatly benefit from fully understanding the system-wide dependency focused on individual files, specifically executable files, and dynamically loadable libraries.","We propose a framework, DepEx, aimed at discovering the detailed package relations at the level of individual binary files and their associated evolutionary changes.","We demonstrate the utility of DepEx by systematically investigating the evolution of a large-scale Open Source OS, Ubuntu.","DepEx enabled us to systematically acquire and analyze the dependencies in different versions of Ubuntu released between 2005 (5.04) to 2023 (23.04).","Our analysis revealed various evolutionary trends in package management and their implications based on the analysis of the 84 consecutive versions available for download (these include beta versions).","This study has enabled us to assert that DepEx can provide researchers and practitioners with a better understanding of the implicit software dependencies in order to improve the stability, performance, and functionality of their software as well as to reduce the risk of issues arising during maintenance, updating, or migration."],"url":"http://arxiv.org/abs/2307.04458v1"}
{"created":"2023-07-10 10:07:11","title":"SAM-IQA: Can Segment Anything Boost Image Quality Assessment?","abstract":"Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions. However, due to the lack of IQA data, deep learning-based IQA methods typically rely on pre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet. In this paper, we utilize the encoder of Segment Anything, a recently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction. Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been shown to better represent noise and blur. Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively. Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively. Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks. Code: https://github.com/Hedlen/SAM-IQA","sentences":["Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions.","However, due to the lack of IQA data, deep learning-based IQA methods typically rely on pre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet.","In this paper, we utilize the encoder of Segment Anything, a recently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction.","Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been shown to better represent noise and blur.","Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively.","Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively.","Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks.","Code: https://github.com/Hedlen/SAM-IQA"],"url":"http://arxiv.org/abs/2307.04455v1"}
{"created":"2023-07-10 10:06:55","title":"Runtime Safety Assurance of Autonomous Vehicles used for Last-mile Delivery in Urban Environments","abstract":"Last-mile delivery of goods has gained a lot of attraction during the COVID-19 pandemic. However, current package delivery processes often lead to parking in the second lane, which in turn has negative effects on the urban environment in which the deliveries take place, i.e., traffic congestion and safety issues for other road users. To tackle these challenges, an effective autonomous delivery system is required that guarantees efficient, flexible and safe delivery of goods. The project LogiSmile, co-funded by EIT Urban Mobility, pilots an autonomous delivery vehicle dubbed the Autonomous Hub Vehicle (AHV) that works in cooperation with a small autonomous robot called the Autonomous Delivery Device (ADD). With the two cooperating robots, the project LogiSmile aims to find a possible solution to the challenges of urban goods distribution in congested areas and to demonstrate the future of urban mobility. As a member of Nieders\\\"achsische Forschungszentrum f\\\"ur Fahrzeugtechnik (NFF), the Institute for Software and Systems Engineering (ISSE) developed an integrated software safety architecture for runtime monitoring of the AHV, with (1) a dependability cage (DC) used for the on-board monitoring of the AHV, and (2) a remote command control center (CCC) which enables the remote off-board supervision of a fleet of AHVs. The DC supervises the vehicle continuously and in case of any safety violation, it switches the nominal driving mode to degraded driving mode or fail-safe mode. Additionally, the CCC also manages the communication of the AHV with the ADD and provides fail-operational solutions for the AHV when it cannot handle complex situations autonomously. The runtime monitoring concept developed for the AHV has been demonstrated in 2022 in Hamburg. We report on the obtained results and on the lessons learned.","sentences":["Last-mile delivery of goods has gained a lot of attraction during the COVID-19 pandemic.","However, current package delivery processes often lead to parking in the second lane, which in turn has negative effects on the urban environment in which the deliveries take place, i.e., traffic congestion and safety issues for other road users.","To tackle these challenges, an effective autonomous delivery system is required that guarantees efficient, flexible and safe delivery of goods.","The project LogiSmile, co-funded by EIT Urban Mobility, pilots an autonomous delivery vehicle dubbed the Autonomous Hub Vehicle (AHV) that works in cooperation with a small autonomous robot called the Autonomous Delivery Device (ADD).","With the two cooperating robots, the project LogiSmile aims to find a possible solution to the challenges of urban goods distribution in congested areas and to demonstrate the future of urban mobility.","As a member of Nieders\\\"achsische Forschungszentrum f\\\"ur Fahrzeugtechnik (NFF), the Institute for Software and Systems Engineering (ISSE) developed an integrated software safety architecture for runtime monitoring of the AHV, with (1) a dependability cage (DC) used for the on-board monitoring of the AHV, and (2) a remote command control center (CCC) which enables the remote off-board supervision of a fleet of AHVs.","The DC supervises the vehicle continuously and in case of any safety violation, it switches the nominal driving mode to degraded driving mode or fail-safe mode.","Additionally, the CCC also manages the communication of the AHV with the ADD and provides fail-operational solutions for the AHV when it cannot handle complex situations autonomously.","The runtime monitoring concept developed for the AHV has been demonstrated in 2022 in Hamburg.","We report on the obtained results and on the lessons learned."],"url":"http://arxiv.org/abs/2307.04454v1"}
{"created":"2023-07-10 09:53:14","title":"Learning Behavioral Representations of Routines From Large-scale Unlabeled Wearable Time-series Data Streams using Hawkes Point Process","abstract":"Continuously-worn wearable sensors enable researchers to collect copious amounts of rich bio-behavioral time series recordings of real-life activities of daily living, offering unprecedented opportunities to infer novel human behavior patterns during daily routines. Existing approaches to routine discovery through bio-behavioral data rely either on pre-defined notions of activities or use additional non-behavioral measurements as contexts, such as GPS location or localization within the home, presenting risks to user privacy. In this work, we propose a novel wearable time-series mining framework, Hawkes point process On Time series clusters for ROutine Discovery (HOT-ROD), for uncovering behavioral routines from completely unlabeled wearable recordings. We utilize a covariance-based method to generate time-series clusters and discover routines via the Hawkes point process learning algorithm. We empirically validate our approach for extracting routine behaviors using a completely unlabeled time-series collected continuously from over 100 individuals both in and outside of the workplace during a period of ten weeks. Furthermore, we demonstrate this approach intuitively captures daily transitional relationships between physical activity states without using prior knowledge. We also show that the learned behavioral patterns can assist in illuminating an individual's personality and affect.","sentences":["Continuously-worn wearable sensors enable researchers to collect copious amounts of rich bio-behavioral time series recordings of real-life activities of daily living, offering unprecedented opportunities to infer novel human behavior patterns during daily routines.","Existing approaches to routine discovery through bio-behavioral data rely either on pre-defined notions of activities or use additional non-behavioral measurements as contexts, such as GPS location or localization within the home, presenting risks to user privacy.","In this work, we propose a novel wearable time-series mining framework, Hawkes point process On Time series clusters for ROutine Discovery (HOT-ROD), for uncovering behavioral routines from completely unlabeled wearable recordings.","We utilize a covariance-based method to generate time-series clusters and discover routines via the Hawkes point process learning algorithm.","We empirically validate our approach for extracting routine behaviors using a completely unlabeled time-series collected continuously from over 100 individuals both in and outside of the workplace during a period of ten weeks.","Furthermore, we demonstrate this approach intuitively captures daily transitional relationships between physical activity states without using prior knowledge.","We also show that the learned behavioral patterns can assist in illuminating an individual's personality and affect."],"url":"http://arxiv.org/abs/2307.04445v1"}
{"created":"2023-07-10 09:52:28","title":"Search-time Efficient Device Constraints-Aware Neural Architecture Search","abstract":"Edge computing aims to enable edge devices, such as IoT devices, to process data locally instead of relying on the cloud. However, deep learning techniques like computer vision and natural language processing can be computationally expensive and memory-intensive. Creating manual architectures specialized for each device is infeasible due to their varying memory and computational constraints. To address these concerns, we automate the construction of task-specific deep learning architectures optimized for device constraints through Neural Architecture Search (NAS). We present DCA-NAS, a principled method of fast neural network architecture search that incorporates edge-device constraints such as model size and floating-point operations. It incorporates weight sharing and channel bottleneck techniques to speed up the search time. Based on our experiments, we see that DCA-NAS outperforms manual architectures for similar sized models and is comparable to popular mobile architectures on various image classification datasets like CIFAR-10, CIFAR-100, and Imagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the generalization capabilities of DCA-NAS. On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art performance were discovered.","sentences":["Edge computing aims to enable edge devices, such as IoT devices, to process data locally instead of relying on the cloud.","However, deep learning techniques like computer vision and natural language processing can be computationally expensive and memory-intensive.","Creating manual architectures specialized for each device is infeasible due to their varying memory and computational constraints.","To address these concerns, we automate the construction of task-specific deep learning architectures optimized for device constraints through Neural Architecture Search (NAS).","We present DCA-NAS, a principled method of fast neural network architecture search that incorporates edge-device constraints such as model size and floating-point operations.","It incorporates weight sharing and channel bottleneck techniques to speed up the search time.","Based on our experiments, we see that DCA-NAS outperforms manual architectures for similar sized models and is comparable to popular mobile architectures on various image classification datasets like CIFAR-10, CIFAR-100, and Imagenet-1k.","Experiments with search spaces -- DARTS and NAS-Bench-201 show the generalization capabilities of DCA-NAS.","On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art performance were discovered."],"url":"http://arxiv.org/abs/2307.04443v1"}
{"created":"2023-07-10 09:49:30","title":"Automatic diagnosis of knee osteoarthritis severity using Swin transformer","abstract":"Knee osteoarthritis (KOA) is a widespread condition that can cause chronic pain and stiffness in the knee joint. Early detection and diagnosis are crucial for successful clinical intervention and management to prevent severe complications, such as loss of mobility. In this paper, we propose an automated approach that employs the Swin Transformer to predict the severity of KOA. Our model uses publicly available radiographic datasets with Kellgren and Lawrence scores to enable early detection and severity assessment. To improve the accuracy of our model, we employ a multi-prediction head architecture that utilizes multi-layer perceptron classifiers. Additionally, we introduce a novel training approach that reduces the data drift between multiple datasets to ensure the generalization ability of the model. The results of our experiments demonstrate the effectiveness and feasibility of our approach in predicting KOA severity accurately.","sentences":["Knee osteoarthritis (KOA) is a widespread condition that can cause chronic pain and stiffness in the knee joint.","Early detection and diagnosis are crucial for successful clinical intervention and management to prevent severe complications, such as loss of mobility.","In this paper, we propose an automated approach that employs the Swin Transformer to predict the severity of KOA.","Our model uses publicly available radiographic datasets with Kellgren and Lawrence scores to enable early detection and severity assessment.","To improve the accuracy of our model, we employ a multi-prediction head architecture that utilizes multi-layer perceptron classifiers.","Additionally, we introduce a novel training approach that reduces the data drift between multiple datasets to ensure the generalization ability of the model.","The results of our experiments demonstrate the effectiveness and feasibility of our approach in predicting KOA severity accurately."],"url":"http://arxiv.org/abs/2307.04442v1"}
{"created":"2023-07-10 09:47:18","title":"Randomized Communication and Implicit Representations for Matrices and Graphs of Small Sign-Rank","abstract":"We prove a characterization of the structural conditions on matrices of sign-rank 3 and unit disk graphs (UDGs) which permit constant-cost public-coin randomized communication protocols. Therefore, under these conditions, these graphs also admit implicit representations.   The sign-rank of a matrix $M \\in \\{\\pm 1\\}^{N \\times N}$ is the smallest rank of a matrix $R$ such that $M_{i,j} = \\mathrm{sign}(R_{i,j})$ for all $i,j \\in [N]$; equivalently, it is the smallest dimension $d$ in which $M$ can be represented as a point-halfspace incidence matrix with halfspaces through the origin, and it is essentially equivalent to the unbounded-error communication complexity. Matrices of sign-rank 3 can achieve the maximum possible bounded-error randomized communication complexity $\\Theta(\\log N)$, and meanwhile the existence of implicit representations for graphs of bounded sign-rank (including UDGs, which have sign-rank 4) has been open since at least 2003. We prove that matrices of sign-rank 3, and UDGs, have constant randomized communication complexity if and only if they do not encode arbitrarily large instances of the Greater-Than communication problem, or, equivalently, if they do not contain arbitrarily large half-graphs as semi-induced subgraphs. This also establishes the existence of implicit representations for these graphs under the same conditions.","sentences":["We prove a characterization of the structural conditions on matrices of sign-rank 3 and unit disk graphs (UDGs) which permit constant-cost public-coin randomized communication protocols.","Therefore, under these conditions, these graphs also admit implicit representations.   ","The sign-rank of a matrix $M \\in \\{\\pm 1\\}^{N \\times N}$ is the smallest rank of a matrix $R$ such that $M_{i,j} = \\mathrm{sign}(R_{i,j})$ for all $i,j \\in","[N]$; equivalently, it is the smallest dimension $d$ in which $M$ can be represented as a point-halfspace incidence matrix with halfspaces through the origin, and it is essentially equivalent to the unbounded-error communication complexity.","Matrices of sign-rank 3 can achieve the maximum possible bounded-error randomized communication complexity $\\Theta(\\log N)$, and meanwhile the existence of implicit representations for graphs of bounded sign-rank (including UDGs, which have sign-rank 4) has been open since at least 2003.","We prove that matrices of sign-rank 3, and UDGs, have constant randomized communication complexity if and only if they do not encode arbitrarily large instances of the Greater-Than communication problem, or, equivalently, if they do not contain arbitrarily large half-graphs as semi-induced subgraphs.","This also establishes the existence of implicit representations for these graphs under the same conditions."],"url":"http://arxiv.org/abs/2307.04441v1"}
{"created":"2023-07-10 09:41:16","title":"Time-Frequency-Space Transmit Design and Signal Processing with Dynamic Subarray for Terahertz Integrated Sensing and Communication","abstract":"Terahertz (THz) integrated sensing and communication (ISAC) enables simultaneous data transmission with Terabit-per-second (Tbps) rate and millimeter-level accurate sensing. To realize such a blueprint, ultra-massive antenna arrays with directional beamforming are used to compensate for severe path loss in the THz band. In this paper, the time-frequency-space transmit design is investigated for THz ISAC to generate time-varying scanning sensing beams and stable communication beams. Specifically, with the dynamic array-of-subarray (DAoSA) hybrid beamforming architecture and multi-carrier modulation, two ISAC hybrid precoding algorithms are proposed, namely, a vectorization (VEC) based algorithm that outperforms existing ISAC hybrid precoding methods and a low-complexity sensing codebook assisted (SCA) approach. Meanwhile, coupled with the transmit design, parameter estimation algorithms are proposed to realize high-accuracy sensing, including a wideband DAoSA MUSIC (W-DAoSA-MUSIC) method for angle estimation and a sum-DFT-GSS (S-DFT-GSS) approach for range and velocity estimation. Numerical results indicate that the proposed algorithms can realize centi-degree-level angle estimation accuracy and millimeter-level range estimation accuracy, which are one or two orders of magnitudes better than the methods in the millimeter-wave band. In addition, to overcome the cyclic prefix limitation and Doppler effects in the THz band, an inter-symbol interference- and inter-carrier interference-tackled sensing algorithm is developed to refine sensing capabilities for THz ISAC.","sentences":["Terahertz (THz) integrated sensing and communication (ISAC) enables simultaneous data transmission with Terabit-per-second (Tbps) rate and millimeter-level accurate sensing.","To realize such a blueprint, ultra-massive antenna arrays with directional beamforming are used to compensate for severe path loss in the THz band.","In this paper, the time-frequency-space transmit design is investigated for THz ISAC to generate time-varying scanning sensing beams and stable communication beams.","Specifically, with the dynamic array-of-subarray (DAoSA) hybrid beamforming architecture and multi-carrier modulation, two ISAC hybrid precoding algorithms are proposed, namely, a vectorization (VEC) based algorithm that outperforms existing ISAC hybrid precoding methods and a low-complexity sensing codebook assisted (SCA) approach.","Meanwhile, coupled with the transmit design, parameter estimation algorithms are proposed to realize high-accuracy sensing, including a wideband DAoSA MUSIC (W-DAoSA-MUSIC) method for angle estimation and a sum-DFT-GSS (S-DFT-GSS) approach for range and velocity estimation.","Numerical results indicate that the proposed algorithms can realize centi-degree-level angle estimation accuracy and millimeter-level range estimation accuracy, which are one or two orders of magnitudes better than the methods in the millimeter-wave band.","In addition, to overcome the cyclic prefix limitation and Doppler effects in the THz band, an inter-symbol interference- and inter-carrier interference-tackled sensing algorithm is developed to refine sensing capabilities for THz ISAC."],"url":"http://arxiv.org/abs/2307.04440v1"}
{"created":"2023-07-10 09:21:43","title":"Global and Local Visual Processing: Influence of Perceptual Field Variables","abstract":"The Global Precedence Effect (GPE) suggests that the processing of global properties of a visual stimulus precedes the processing of local properties. The generality of this theory was argued for four decades during different known Perceptual Field Variables. The effect size of various PFVs, regarding the findings during these four decades, were pooled in our recent meta-analysis study. Pursuing the study, in the present paper, we explore the effects of Congruency, Size, and Sparsity and their interaction on global advantage in two different experiments with different task paradigms; Matching judgment and Similarity judgment. Upon results of these experiments, Congruency and Size have significant effects and Sparsity has small effects. Also, the task paradigm and its interaction with other PFVs are shown significant effects in this study, which shows the prominence of the role of task paradigms in evaluating PFVs' effects on GPE. Also, we found that the effects of these parameters were not specific to the special condition that individuals were instructed to retinal stabilize. So, the experiments were more extendible to daily human behavior.","sentences":["The Global Precedence Effect (GPE) suggests that the processing of global properties of a visual stimulus precedes the processing of local properties.","The generality of this theory was argued for four decades during different known Perceptual Field Variables.","The effect size of various PFVs, regarding the findings during these four decades, were pooled in our recent meta-analysis study.","Pursuing the study, in the present paper, we explore the effects of Congruency, Size, and Sparsity and their interaction on global advantage in two different experiments with different task paradigms; Matching judgment and Similarity judgment.","Upon results of these experiments, Congruency and Size have significant effects and Sparsity has small effects.","Also, the task paradigm and its interaction with other PFVs are shown significant effects in this study, which shows the prominence of the role of task paradigms in evaluating PFVs' effects on GPE.","Also, we found that the effects of these parameters were not specific to the special condition that individuals were instructed to retinal stabilize.","So, the experiments were more extendible to daily human behavior."],"url":"http://arxiv.org/abs/2307.04435v1"}
{"created":"2023-07-10 09:11:52","title":"PSO-Based Optimal Coverage Path Planning for Surface Defect Inspection of 3C Components with a Robotic Line Scanner","abstract":"The automatic inspection of surface defects is an important task for quality control in the computers, communications, and consumer electronics (3C) industry. Conventional devices for defect inspection (viz. line-scan sensors) have a limited field of view, thus, a robot-aided defect inspection system needs to scan the object from multiple viewpoints. Optimally selecting the robot's viewpoints and planning a path is regarded as coverage path planning (CPP), a problem that enables inspecting the object's complete surface while reducing the scanning time and avoiding misdetection of defects. However, the development of CPP strategies for robotic line scanners has not been sufficiently studied by researchers. To fill this gap in the literature, in this paper, we present a new approach for robotic line scanners to detect surface defects of 3C free-form objects automatically. Our proposed solution consists of generating a local path by a new hybrid region segmentation method and an adaptive planning algorithm to ensure the coverage of the complete object surface. An optimization method for the global path sequence is developed to maximize the scanning efficiency. To verify our proposed methodology, we conduct detailed simulation-based and experimental studies on various free-form workpieces, and compare its performance with a state-of-the-art solution. The reported results demonstrate the feasibility and effectiveness of our approach.","sentences":["The automatic inspection of surface defects is an important task for quality control in the computers, communications, and consumer electronics (3C) industry.","Conventional devices for defect inspection (viz.","line-scan sensors) have a limited field of view, thus, a robot-aided defect inspection system needs to scan the object from multiple viewpoints.","Optimally selecting the robot's viewpoints and planning a path is regarded as coverage path planning (CPP), a problem that enables inspecting the object's complete surface while reducing the scanning time and avoiding misdetection of defects.","However, the development of CPP strategies for robotic line scanners has not been sufficiently studied by researchers.","To fill this gap in the literature, in this paper, we present a new approach for robotic line scanners to detect surface defects of 3C free-form objects automatically.","Our proposed solution consists of generating a local path by a new hybrid region segmentation method and an adaptive planning algorithm to ensure the coverage of the complete object surface.","An optimization method for the global path sequence is developed to maximize the scanning efficiency.","To verify our proposed methodology, we conduct detailed simulation-based and experimental studies on various free-form workpieces, and compare its performance with a state-of-the-art solution.","The reported results demonstrate the feasibility and effectiveness of our approach."],"url":"http://arxiv.org/abs/2307.04431v1"}
{"created":"2023-07-10 09:09:26","title":"Designing Novel Cognitive Diagnosis Models via Evolutionary Multi-Objective Neural Architecture Search","abstract":"Cognitive diagnosis plays a vital role in modern intelligent education platforms to reveal students' proficiency in knowledge concepts for subsequent adaptive tasks. However, due to the requirement of high model interpretability, existing manually designed cognitive diagnosis models hold too simple architectures to meet the demand of current intelligent education systems, where the bias of human design also limits the emergence of effective cognitive diagnosis models. In this paper, we propose to automatically design novel cognitive diagnosis models by evolutionary multi-objective neural architecture search (NAS). Specifically, we observe existing models can be represented by a general model handling three given types of inputs and thus first design an expressive search space for the NAS task in cognitive diagnosis. Then, we propose multi-objective genetic programming (MOGP) to explore the NAS task's search space by maximizing model performance and interpretability. In the MOGP design, each architecture is transformed into a tree architecture and encoded by a tree for easy optimization, and a tailored genetic operation based on four sub-genetic operations is devised to generate offspring effectively. Besides, an initialization strategy is also suggested to accelerate the convergence by evolving half of the population from existing models' variants. Experiments on two real-world datasets demonstrate that the cognitive diagnosis models searched by the proposed approach exhibit significantly better performance than existing models and also hold as good interpretability as human-designed models.","sentences":["Cognitive diagnosis plays a vital role in modern intelligent education platforms to reveal students' proficiency in knowledge concepts for subsequent adaptive tasks.","However, due to the requirement of high model interpretability, existing manually designed cognitive diagnosis models hold too simple architectures to meet the demand of current intelligent education systems, where the bias of human design also limits the emergence of effective cognitive diagnosis models.","In this paper, we propose to automatically design novel cognitive diagnosis models by evolutionary multi-objective neural architecture search (NAS).","Specifically, we observe existing models can be represented by a general model handling three given types of inputs and thus first design an expressive search space for the NAS task in cognitive diagnosis.","Then, we propose multi-objective genetic programming (MOGP) to explore the NAS task's search space by maximizing model performance and interpretability.","In the MOGP design, each architecture is transformed into a tree architecture and encoded by a tree for easy optimization, and a tailored genetic operation based on four sub-genetic operations is devised to generate offspring effectively.","Besides, an initialization strategy is also suggested to accelerate the convergence by evolving half of the population from existing models' variants.","Experiments on two real-world datasets demonstrate that the cognitive diagnosis models searched by the proposed approach exhibit significantly better performance than existing models and also hold as good interpretability as human-designed models."],"url":"http://arxiv.org/abs/2307.04429v1"}
{"created":"2023-07-10 08:55:28","title":"A Versatile Door Opening System with Mobile Manipulator through Adaptive Position-Force Control and Reinforcement Learning","abstract":"The ability of robots to navigate through doors is crucial for their effective operation in indoor environments. Consequently, extensive research has been conducted to develop robots capable of opening specific doors. However, the diverse combinations of door handles and opening directions necessitate a more versatile door opening system for robots to successfully operate in real-world environments. In this paper, we propose a mobile manipulator system that can autonomously open various doors without prior knowledge. By using convolutional neural networks, point cloud extraction techniques, and external force measurements during exploratory motion, we obtained information regarding handle types, poses, and door characteristics. Through two different approaches, adaptive position-force control and deep reinforcement learning, we successfully opened doors without precise trajectory or excessive external force. The adaptive position-force control method involves moving the end-effector in the direction of the door opening while responding compliantly to external forces, ensuring safety and manipulator workspace. Meanwhile, the deep reinforcement learning policy minimizes applied forces and eliminates unnecessary movements, enabling stable operation across doors with different poses and widths. The RL-based approach outperforms the adaptive position-force control method in terms of compensating for external forces, ensuring smooth motion, and achieving efficient speed. It reduces the maximum force required by 3.27 times and improves motion smoothness by 1.82 times. However, the non-learning-based adaptive position-force control method demonstrates more versatility in opening a wider range of doors, encompassing revolute doors with four distinct opening directions and varying widths.","sentences":["The ability of robots to navigate through doors is crucial for their effective operation in indoor environments.","Consequently, extensive research has been conducted to develop robots capable of opening specific doors.","However, the diverse combinations of door handles and opening directions necessitate a more versatile door opening system for robots to successfully operate in real-world environments.","In this paper, we propose a mobile manipulator system that can autonomously open various doors without prior knowledge.","By using convolutional neural networks, point cloud extraction techniques, and external force measurements during exploratory motion, we obtained information regarding handle types, poses, and door characteristics.","Through two different approaches, adaptive position-force control and deep reinforcement learning, we successfully opened doors without precise trajectory or excessive external force.","The adaptive position-force control method involves moving the end-effector in the direction of the door opening while responding compliantly to external forces, ensuring safety and manipulator workspace.","Meanwhile, the deep reinforcement learning policy minimizes applied forces and eliminates unnecessary movements, enabling stable operation across doors with different poses and widths.","The RL-based approach outperforms the adaptive position-force control method in terms of compensating for external forces, ensuring smooth motion, and achieving efficient speed.","It reduces the maximum force required by 3.27 times and improves motion smoothness by 1.82 times.","However, the non-learning-based adaptive position-force control method demonstrates more versatility in opening a wider range of doors, encompassing revolute doors with four distinct opening directions and varying widths."],"url":"http://arxiv.org/abs/2307.04422v1"}
{"created":"2023-07-10 08:54:07","title":"FedDCT: A Dynamic Cross-Tier Federated Learning Scheme in Wireless Communication Networks","abstract":"With the rapid proliferation of Internet of Things (IoT) devices and the growing concern for data privacy among the public, Federated Learning (FL) has gained significant attention as a privacy-preserving machine learning paradigm. FL enables the training of a global model among clients without exposing local data. However, when a federated learning system runs on wireless communication networks, limited wireless resources, heterogeneity of clients, and network transmission failures affect its performance and accuracy. In this study, we propose a novel dynamic cross-tier FL scheme, named FedDCT to increase training accuracy and performance in wireless communication networks. We utilize a tiering algorithm that dynamically divides clients into different tiers according to specific indicators and assigns specific timeout thresholds to each tier to reduce the training time required. To improve the accuracy of the model without increasing the training time, we introduce a cross-tier client selection algorithm that can effectively select the tiers and participants. Simulation experiments show that our scheme can make the model converge faster and achieve a higher accuracy in wireless communication networks.","sentences":["With the rapid proliferation of Internet of Things (IoT) devices and the growing concern for data privacy among the public, Federated Learning (FL) has gained significant attention as a privacy-preserving machine learning paradigm.","FL enables the training of a global model among clients without exposing local data.","However, when a federated learning system runs on wireless communication networks, limited wireless resources, heterogeneity of clients, and network transmission failures affect its performance and accuracy.","In this study, we propose a novel dynamic cross-tier FL scheme, named FedDCT to increase training accuracy and performance in wireless communication networks.","We utilize a tiering algorithm that dynamically divides clients into different tiers according to specific indicators and assigns specific timeout thresholds to each tier to reduce the training time required.","To improve the accuracy of the model without increasing the training time, we introduce a cross-tier client selection algorithm that can effectively select the tiers and participants.","Simulation experiments show that our scheme can make the model converge faster and achieve a higher accuracy in wireless communication networks."],"url":"http://arxiv.org/abs/2307.04420v1"}
{"created":"2023-07-10 08:45:58","title":"Handling Group Fairness in Federated Learning Using Augmented Lagrangian Approach","abstract":"Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models might be biased towards sensitive factors such as race or gender, even if they are trained using a legally compliant process. To redress this concern, this paper proposes a novel FL algorithm designed explicitly to address group fairness issues. We show empirically on CelebA and ImSitu datasets that the proposed method can improve fairness both quantitatively and qualitatively with minimal loss in accuracy in the presence of statistical heterogeneity and with different numbers of clients. Besides improving fairness, the proposed FL algorithm is compatible with local differential privacy (LDP), has negligible communication costs, and results in minimal overhead when migrating existing FL systems from the common FL protocol such as FederatedAveraging (FedAvg). We also provide the theoretical convergence rate guarantee for the proposed algorithm and the required noise level of the Gaussian mechanism to achieve desired LDP. This innovative approach holds significant potential to enhance the fairness and effectiveness of FL systems, particularly in sensitive applications such as healthcare or criminal justice.","sentences":["Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature.","Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models might be biased towards sensitive factors such as race or gender, even if they are trained using a legally compliant process.","To redress this concern, this paper proposes a novel FL algorithm designed explicitly to address group fairness issues.","We show empirically on CelebA and ImSitu datasets that the proposed method can improve fairness both quantitatively and qualitatively with minimal loss in accuracy in the presence of statistical heterogeneity and with different numbers of clients.","Besides improving fairness, the proposed FL algorithm is compatible with local differential privacy (LDP), has negligible communication costs, and results in minimal overhead when migrating existing FL systems from the common FL protocol such as FederatedAveraging (FedAvg).","We also provide the theoretical convergence rate guarantee for the proposed algorithm and the required noise level of the Gaussian mechanism to achieve desired LDP.","This innovative approach holds significant potential to enhance the fairness and effectiveness of FL systems, particularly in sensitive applications such as healthcare or criminal justice."],"url":"http://arxiv.org/abs/2307.04417v1"}
{"created":"2023-07-10 08:44:01","title":"Towards Automated Cyber Range Design: Characterizing and Matching Demands to Supplies","abstract":"Cyber ranges mimic real-world cyber environments and are in high demand. Before building their own cyber ranges, organizations need to deeply understand what construction supplies are available to them. A fundamental supply is the cyber range architecture, which prompts an important research question: Which cyber range architecture is most appropriate for an organization's requirements? To answer this question, we propose an innovative framework to specify cyber range requirements, characterize cyber range architectures (based on our analysis of 45 cyber range architectures), and match cyber range architectures to cyber range requirements.","sentences":["Cyber ranges mimic real-world cyber environments and are in high demand.","Before building their own cyber ranges, organizations need to deeply understand what construction supplies are available to them.","A fundamental supply is the cyber range architecture, which prompts an important research question: Which cyber range architecture is most appropriate for an organization's requirements?","To answer this question, we propose an innovative framework to specify cyber range requirements, characterize cyber range architectures (based on our analysis of 45 cyber range architectures), and match cyber range architectures to cyber range requirements."],"url":"http://arxiv.org/abs/2307.04416v1"}
{"created":"2023-07-10 08:32:45","title":"Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training","abstract":"Biomedical summarization requires large datasets to train for text generation. We show that while transfer learning offers a viable option for addressing this challenge, an in-domain pre-training does not always offer advantages in a BioASQ summarization task. We identify a suitable model architecture and use it to show a benefit of a general-domain pre-training followed by a task-specific fine-tuning in the context of a BioASQ summarization task, leading to a novel three-step fine-tuning approach that works with only a thousand in-domain examples. Our results indicate that a Large Language Model without domain-specific pre-training can have a significant edge in some domain-specific biomedical text generation tasks.","sentences":["Biomedical summarization requires large datasets to train for text generation.","We show that while transfer learning offers a viable option for addressing this challenge, an in-domain pre-training does not always offer advantages in a BioASQ summarization task.","We identify a suitable model architecture and use it to show a benefit of a general-domain pre-training followed by a task-specific fine-tuning in the context of a BioASQ summarization task, leading to a novel three-step fine-tuning approach that works with only a thousand in-domain examples.","Our results indicate that a Large Language Model without domain-specific pre-training can have a significant edge in some domain-specific biomedical text generation tasks."],"url":"http://arxiv.org/abs/2307.04412v1"}
{"created":"2023-07-10 08:25:12","title":"One Quarter Each (on Average) Ensures Proportionality","abstract":"We consider the problem of fair allocation of $m$ indivisible items to a group of $n$ agents with subsidy (money). Our work mainly focuses on the allocation of chores but most of our results extend to the allocation of goods as well. We consider the case when agents have (general) additive cost functions. Assuming that the maximum cost of an item to an agent can be compensated by one dollar, we show that a total of $n/4$ dollars of subsidy suffices to ensure a proportional allocation. Moreover, we show that $n/4$ is tight in the sense that there exists an instance with $n$ agents for which every proportional allocation requires a total subsidy of at least $n/4$. We also consider the weighted case and show that a total subsidy of $(n-1)/2$ suffices to ensure a weighted proportional allocation.","sentences":["We consider the problem of fair allocation of $m$ indivisible items to a group of $n$ agents with subsidy (money).","Our work mainly focuses on the allocation of chores but most of our results extend to the allocation of goods as well.","We consider the case when agents have (general) additive cost functions.","Assuming that the maximum cost of an item to an agent can be compensated by one dollar, we show that a total of $n/4$ dollars of subsidy suffices to ensure a proportional allocation.","Moreover, we show that $n/4$ is tight in the sense that there exists an instance with $n$ agents for which every proportional allocation requires a total subsidy of at least $n/4$. We also consider the weighted case and show that a total subsidy of $(n-1)/2$ suffices to ensure a weighted proportional allocation."],"url":"http://arxiv.org/abs/2307.04411v1"}
{"created":"2023-07-10 08:15:40","title":"TIM: Teaching Large Language Models to Translate with Comparison","abstract":"Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning. We evaluate our method on WMT2022 test sets and show that it outperforms existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a promising solution for generating high-quality translations. Please refer to Github for more details: https://github.com/lemon0830/TIM.","sentences":["Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning.","However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation.","One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements.","Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data.","To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation.","Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning.","We evaluate our method on WMT2022 test sets and show that it outperforms existing methods.","Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a promising solution for generating high-quality translations.","Please refer to Github for more details: https://github.com/lemon0830/TIM."],"url":"http://arxiv.org/abs/2307.04408v1"}
{"created":"2023-07-10 08:03:41","title":"Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation","abstract":"Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is available at https://github.com/thu-coai/Targeted-Data-Extraction.","sentences":["Large pre-trained language models achieve impressive results across many tasks.","However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage.","In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix.","To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed.","We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix.","In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation.","We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark.","We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length.","Our code is available at https://github.com/thu-coai/Targeted-Data-Extraction."],"url":"http://arxiv.org/abs/2307.04401v1"}
{"created":"2023-07-10 07:55:42","title":"FODVid: Flow-guided Object Discovery in Videos","abstract":"Segmentation of objects in a video is challenging due to the nuances such as motion blurring, parallax, occlusions, changes in illumination, etc. Instead of addressing these nuances separately, we focus on building a generalizable solution that avoids overfitting to the individual intricacies. Such a solution would also help us save enormous resources involved in human annotation of video corpora. To solve Video Object Segmentation (VOS) in an unsupervised setting, we propose a new pipeline (FODVid) based on the idea of guiding segmentation outputs using flow-guided graph-cut and temporal consistency. Basically, we design a segmentation model incorporating intra-frame appearance and flow similarities, and inter-frame temporal continuation of the objects under consideration. We perform an extensive experimental analysis of our straightforward methodology on the standard DAVIS16 video benchmark. Though simple, our approach produces results comparable (within a range of ~2 mIoU) to the existing top approaches in unsupervised VOS. The simplicity and effectiveness of our technique opens up new avenues for research in the video domain.","sentences":["Segmentation of objects in a video is challenging due to the nuances such as motion blurring, parallax, occlusions, changes in illumination, etc.","Instead of addressing these nuances separately, we focus on building a generalizable solution that avoids overfitting to the individual intricacies.","Such a solution would also help us save enormous resources involved in human annotation of video corpora.","To solve Video Object Segmentation (VOS) in an unsupervised setting, we propose a new pipeline (FODVid) based on the idea of guiding segmentation outputs using flow-guided graph-cut and temporal consistency.","Basically, we design a segmentation model incorporating intra-frame appearance and flow similarities, and inter-frame temporal continuation of the objects under consideration.","We perform an extensive experimental analysis of our straightforward methodology on the standard DAVIS16 video benchmark.","Though simple, our approach produces results comparable (within a range of ~2 mIoU) to the existing top approaches in unsupervised VOS.","The simplicity and effectiveness of our technique opens up new avenues for research in the video domain."],"url":"http://arxiv.org/abs/2307.04392v1"}
{"created":"2023-07-10 07:54:59","title":"Vehicle Detection in 6G Systems with OTFS Modulation","abstract":"The recently introduced orthogonal time frequency space modulation (OTFSM) is more robust to large narrow-band Doppler frequency shift than the orthogonal frequency division multiplexing (OFDM), used in the 5G standard. In this paper it is shown how the elecommunication OTFSM-based signal with random padding can be used with success in the 6G standard for detection of high-speed vehicles. Two approaches for detecting targets during the random padded OTFS based transmission are compared in the paper","sentences":["The recently introduced orthogonal time frequency space modulation (OTFSM) is more robust to large narrow-band Doppler frequency shift than the orthogonal frequency division multiplexing (OFDM), used in the 5G standard.","In this paper it is shown how the elecommunication OTFSM-based signal with random padding can be used with success in the 6G standard for detection of high-speed vehicles.","Two approaches for detecting targets during the random padded OTFS based transmission are compared in the paper"],"url":"http://arxiv.org/abs/2307.04391v1"}
{"created":"2023-07-10 07:45:06","title":"Counterfactual Explanation for Fairness in Recommendation","abstract":"Fairness-aware recommendation eliminates discrimination issues to build trustworthy recommendation systems.Explaining the causes of unfair recommendations is critical, as it promotes fairness diagnostics, and thus secures users' trust in recommendation models. Existing fairness explanation methods suffer high computation burdens due to the large-scale search space and the greedy nature of the explanation search process. Besides, they perform score-based optimizations with continuous values, which are not applicable to discrete attributes such as gender and race. In this work, we adopt the novel paradigm of counterfactual explanation from causal inference to explore how minimal alterations in explanations change model fairness, to abandon the greedy search for explanations. We use real-world attributes from Heterogeneous Information Networks (HINs) to empower counterfactual reasoning on discrete attributes. We propose a novel Counterfactual Explanation for Fairness (CFairER) that generates attribute-level counterfactual explanations from HINs for recommendation fairness. Our CFairER conducts off-policy reinforcement learning to seek high-quality counterfactual explanations, with an attentive action pruning reducing the search space of candidate counterfactuals. The counterfactual explanations help to provide rational and proximate explanations for model fairness, while the attentive action pruning narrows the search space of attributes. Extensive experiments demonstrate our proposed model can generate faithful explanations while maintaining favorable recommendation performance.","sentences":["Fairness-aware recommendation eliminates discrimination issues to build trustworthy recommendation systems.","Explaining the causes of unfair recommendations is critical, as it promotes fairness diagnostics, and thus secures users' trust in recommendation models.","Existing fairness explanation methods suffer high computation burdens due to the large-scale search space and the greedy nature of the explanation search process.","Besides, they perform score-based optimizations with continuous values, which are not applicable to discrete attributes such as gender and race.","In this work, we adopt the novel paradigm of counterfactual explanation from causal inference to explore how minimal alterations in explanations change model fairness, to abandon the greedy search for explanations.","We use real-world attributes from Heterogeneous Information Networks (HINs) to empower counterfactual reasoning on discrete attributes.","We propose a novel Counterfactual Explanation for Fairness (CFairER) that generates attribute-level counterfactual explanations from HINs for recommendation fairness.","Our CFairER conducts off-policy reinforcement learning to seek high-quality counterfactual explanations, with an attentive action pruning reducing the search space of candidate counterfactuals.","The counterfactual explanations help to provide rational and proximate explanations for model fairness, while the attentive action pruning narrows the search space of attributes.","Extensive experiments demonstrate our proposed model can generate faithful explanations while maintaining favorable recommendation performance."],"url":"http://arxiv.org/abs/2307.04386v1"}
{"created":"2023-07-10 07:43:14","title":"Growing Fast without Colliding: Polylogarithmic Time Step Construction of Geometric Shapes","abstract":"Building on two recent models of [Almalki and Michail, 2022] and [Gupta et al., 2023], we explore the constructive power of a set of geometric growth processes. The studied processes, by applying a sequence of centralized, parallel, and linear-strength growth operations, can construct shapes from smaller shapes or from a singleton exponentially fast. A technical challenge in growing shapes that fast is the need to avoid collisions caused, for example, when the shape breaks, stretches, or self-intersects. We distinguish two types of growth operations -- one that avoids collisions by preserving cycles and one that achieves the same by breaking them -- and two types of graph models. We study the following types of shape reachability questions in these models. Given a class of initial shapes $\\mathcal{I}$ and a class of final shapes $\\mathcal{F}$, our objective is to determine whether any (some) shape $S \\in \\mathcal{F}$ can be reached from any shape $S_0 \\in \\mathcal{I}$ in a number of time steps which is (poly)logarithmic in the size of $S$. For the reachable classes, we additionally present the respective growth processes. In cycle-preserving growth, we study these problems in basic classes of shapes such as paths, spirals, and trees and reveal the importance of the number of turning points as a parameter. We give both positive and negative results. For cycle-breaking growth, we obtain a strong positive result -- a general growth process that can grow any connected shape from a singleton fast.","sentences":["Building on two recent models of [Almalki and Michail, 2022] and [Gupta et al., 2023], we explore the constructive power of a set of geometric growth processes.","The studied processes, by applying a sequence of centralized, parallel, and linear-strength growth operations, can construct shapes from smaller shapes or from a singleton exponentially fast.","A technical challenge in growing shapes that fast is the need to avoid collisions caused, for example, when the shape breaks, stretches, or self-intersects.","We distinguish two types of growth operations -- one that avoids collisions by preserving cycles and one that achieves the same by breaking them -- and two types of graph models.","We study the following types of shape reachability questions in these models.","Given a class of initial shapes $\\mathcal{I}$ and a class of final shapes $\\mathcal{F}$, our objective is to determine whether any (some) shape $S \\in \\mathcal{F}$ can be reached from any shape $S_0 \\in \\mathcal{I}$ in a number of time steps which is (poly)logarithmic in the size of $S$. For the reachable classes, we additionally present the respective growth processes.","In cycle-preserving growth, we study these problems in basic classes of shapes such as paths, spirals, and trees and reveal the importance of the number of turning points as a parameter.","We give both positive and negative results.","For cycle-breaking growth, we obtain a strong positive result -- a general growth process that can grow any connected shape from a singleton fast."],"url":"http://arxiv.org/abs/2307.04385v1"}
{"created":"2023-07-10 07:43:05","title":"Causal Neural Graph Collaborative Filtering","abstract":"Graph collaborative filtering (GCF) has gained considerable attention in recommendation systems by leveraging graph learning techniques to enhance collaborative filtering (CF) models. One classical approach in GCF is to learn user and item embeddings by modeling complex graph relations and utilizing these embeddings for CF models. However, the quality of the embeddings significantly impacts the recommendation performance of GCF models. In this paper, we argue that existing graph learning methods are insufficient in generating satisfactory embeddings for CF models. This is because they aggregate neighboring node messages directly, which can result in incorrect estimations of user-item correlations. To overcome this limitation, we propose a novel approach that incorporates causal modeling to explicitly encode the causal effects of neighboring nodes on the target node. This approach enables us to identify spurious correlations and uncover the root causes of user preferences. We introduce Causal Neural Graph Collaborative Filtering (CNGCF), the first causality-aware graph learning framework for CF. CNGCF integrates causal modeling into the graph representation learning process, explicitly coupling causal effects between node pairs into the core message-passing process of graph learning. As a result, CNGCF yields causality-aware embeddings that promote robust recommendations. Our extensive experiments demonstrate that CNGCF provides precise recommendations that align with user preferences. Therefore, our proposed framework can address the limitations of existing GCF models and offer a more effective solution for recommendation systems.","sentences":["Graph collaborative filtering (GCF) has gained considerable attention in recommendation systems by leveraging graph learning techniques to enhance collaborative filtering (CF) models.","One classical approach in GCF is to learn user and item embeddings by modeling complex graph relations and utilizing these embeddings for CF models.","However, the quality of the embeddings significantly impacts the recommendation performance of GCF models.","In this paper, we argue that existing graph learning methods are insufficient in generating satisfactory embeddings for CF models.","This is because they aggregate neighboring node messages directly, which can result in incorrect estimations of user-item correlations.","To overcome this limitation, we propose a novel approach that incorporates causal modeling to explicitly encode the causal effects of neighboring nodes on the target node.","This approach enables us to identify spurious correlations and uncover the root causes of user preferences.","We introduce Causal Neural Graph Collaborative Filtering (CNGCF), the first causality-aware graph learning framework for CF.","CNGCF integrates causal modeling into the graph representation learning process, explicitly coupling causal effects between node pairs into the core message-passing process of graph learning.","As a result, CNGCF yields causality-aware embeddings that promote robust recommendations.","Our extensive experiments demonstrate that CNGCF provides precise recommendations that align with user preferences.","Therefore, our proposed framework can address the limitations of existing GCF models and offer a more effective solution for recommendation systems."],"url":"http://arxiv.org/abs/2307.04384v1"}
{"created":"2023-07-10 07:24:44","title":"Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains","abstract":"Diabetic Retinopathy (DR) is a common complication of diabetes and a leading cause of blindness worldwide. Early and accurate grading of its severity is crucial for disease management. Although deep learning has shown great potential for automated DR grading, its real-world deployment is still challenging due to distribution shifts among source and target domains, known as the domain generalization problem. Existing works have mainly attributed the performance degradation to limited domain shifts caused by simple visual discrepancies, which cannot handle complex real-world scenarios. Instead, we present preliminary evidence suggesting the existence of three-fold generalization issues: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance. To tackle these issues, we propose a novel unified framework named Generalizable Diabetic Retinopathy Grading Network (GDRNet). GDRNet consists of three vital components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domain-class-aware re-balancing (DCR). FundusAug generates realistic augmented images via visual transformation and image degradation, while DahLoss jointly leverages pixel-level consistency and image-level semantics to capture the diverse diagnostic patterns and build generalizable feature representations. Moreover, DCR mitigates the data imbalance from a domain-class view and avoids undesired over-emphasis on rare domain-class pairs. Finally, we design a publicly available benchmark for fair evaluations. Extensive comparison experiments against advanced methods and exhaustive ablation studies demonstrate the effectiveness and generalization ability of GDRNet.","sentences":["Diabetic Retinopathy (DR) is a common complication of diabetes and a leading cause of blindness worldwide.","Early and accurate grading of its severity is crucial for disease management.","Although deep learning has shown great potential for automated DR grading, its real-world deployment is still challenging due to distribution shifts among source and target domains, known as the domain generalization problem.","Existing works have mainly attributed the performance degradation to limited domain shifts caused by simple visual discrepancies, which cannot handle complex real-world scenarios.","Instead, we present preliminary evidence suggesting the existence of three-fold generalization issues: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance.","To tackle these issues, we propose a novel unified framework named Generalizable Diabetic Retinopathy Grading Network (GDRNet).","GDRNet consists of three vital components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domain-class-aware re-balancing (DCR).","FundusAug generates realistic augmented images via visual transformation and image degradation, while DahLoss jointly leverages pixel-level consistency and image-level semantics to capture the diverse diagnostic patterns and build generalizable feature representations.","Moreover, DCR mitigates the data imbalance from a domain-class view and avoids undesired over-emphasis on rare domain-class pairs.","Finally, we design a publicly available benchmark for fair evaluations.","Extensive comparison experiments against advanced methods and exhaustive ablation studies demonstrate the effectiveness and generalization ability of GDRNet."],"url":"http://arxiv.org/abs/2307.04378v1"}
{"created":"2023-07-10 07:22:06","title":"HCLAS-X: Hierarchical and Cascaded Lyrics Alignment System Using Multimodal Cross-Correlation","abstract":"In this work, we address the challenge of lyrics alignment, which involves aligning the lyrics and vocal components of songs. This problem requires the alignment of two distinct modalities, namely text and audio. To overcome this challenge, we propose a model that is trained in a supervised manner, utilizing the cross-correlation matrix of latent representations between vocals and lyrics. Our system is designed in a hierarchical and cascaded manner. It predicts synced time first on a sentence-level and subsequently on a word-level. This design enables the system to process long sequences, as the cross-correlation uses quadratic memory with respect to sequence length. In our experiments, we demonstrate that our proposed system achieves a significant improvement in mean average error, showcasing its robustness in comparison to the previous state-of-the-art model. Additionally, we conduct a qualitative analysis of the system after successfully deploying it in several music streaming services.","sentences":["In this work, we address the challenge of lyrics alignment, which involves aligning the lyrics and vocal components of songs.","This problem requires the alignment of two distinct modalities, namely text and audio.","To overcome this challenge, we propose a model that is trained in a supervised manner, utilizing the cross-correlation matrix of latent representations between vocals and lyrics.","Our system is designed in a hierarchical and cascaded manner.","It predicts synced time first on a sentence-level and subsequently on a word-level.","This design enables the system to process long sequences, as the cross-correlation uses quadratic memory with respect to sequence length.","In our experiments, we demonstrate that our proposed system achieves a significant improvement in mean average error, showcasing its robustness in comparison to the previous state-of-the-art model.","Additionally, we conduct a qualitative analysis of the system after successfully deploying it in several music streaming services."],"url":"http://arxiv.org/abs/2307.04377v1"}
{"created":"2023-07-10 07:21:23","title":"Joint Communications and Sensing Hybrid Beamforming Design via Deep Unfolding","abstract":"Joint communications and sensing (JCAS) is envisioned as a key feature in future wireless communications networks. In massive MIMO-JCAS systems, hybrid beamforming (HBF) is typically employed to achieve satisfactory beamforming gains with reasonable hardware cost and power consumption. Due to the coupling of the analog and digital precoders in HBF and the dual objective in JCAS, JCAS-HBF design problems are very challenging and usually require highly complex algorithms. In this paper, we propose a fast HBF design for JCAS based on deep unfolding to optimize a tradeoff between the communications rate and sensing accuracy. We first derive closed-form expressions for the gradients of the communications and sensing objectives with respect to the precoders and demonstrate that the magnitudes of the gradients pertaining to the analog precoder are typically smaller than those associated with the digital precoder. Based on this observation, we propose a modified projected gradient ascent (PGA) method with significantly improved convergence. We then develop a deep unfolded PGA scheme that efficiently optimizes the communications-sensing performance tradeoff with fast convergence thanks to the well-trained hyperparameters. In doing so, we preserve the interpretability and flexibility of the optimizer while leveraging data to improve performance. Finally, our simulations demonstrate the potential of the proposed deep unfolded method, which achieves up to 33.5% higher communications sum rate and 2.5 dB lower beampattern error compared with the conventional design based on successive convex approximation and Riemannian manifold optimization. Furthermore, it attains up to a 65% reduction in run time and computational complexity with respect to the PGA procedure without unfolding.","sentences":["Joint communications and sensing (JCAS) is envisioned as a key feature in future wireless communications networks.","In massive MIMO-JCAS systems, hybrid beamforming (HBF) is typically employed to achieve satisfactory beamforming gains with reasonable hardware cost and power consumption.","Due to the coupling of the analog and digital precoders in HBF and the dual objective in JCAS, JCAS-HBF design problems are very challenging and usually require highly complex algorithms.","In this paper, we propose a fast HBF design for JCAS based on deep unfolding to optimize a tradeoff between the communications rate and sensing accuracy.","We first derive closed-form expressions for the gradients of the communications and sensing objectives with respect to the precoders and demonstrate that the magnitudes of the gradients pertaining to the analog precoder are typically smaller than those associated with the digital precoder.","Based on this observation, we propose a modified projected gradient ascent (PGA) method with significantly improved convergence.","We then develop a deep unfolded PGA scheme that efficiently optimizes the communications-sensing performance tradeoff with fast convergence thanks to the well-trained hyperparameters.","In doing so, we preserve the interpretability and flexibility of the optimizer while leveraging data to improve performance.","Finally, our simulations demonstrate the potential of the proposed deep unfolded method, which achieves up to 33.5% higher communications sum rate and 2.5 dB lower beampattern error compared with the conventional design based on successive convex approximation and Riemannian manifold optimization.","Furthermore, it attains up to a 65% reduction in run time and computational complexity with respect to the PGA procedure without unfolding."],"url":"http://arxiv.org/abs/2307.04376v1"}
{"created":"2023-07-10 07:10:30","title":"Towards Runtime Customizable Trusted Execution Environment on FPGA-SoC","abstract":"Processing sensitive data and deploying well-designed Intellectual Property (IP) cores on remote Field Programmable Gate Array (FPGA) are prone to private data leakage and IP theft. One effective solution is constructing Trusted Execution Environment (TEE) on FPGA-SoCs (FPGA System on Chips). Researchers have integrated this type TEE with Trusted Platform Module (TPM)-based trusted boot, denoted as FPGA-SoC tbTEE. But there is no effort on secure and trusted runtime customization of FPGA-SoC TEE. This paper extends FPGA-SoC tbTEE to build Runtime Customizable TEE (RCTEE) on FPGA-SoC by additive three major components (our work): 1) CrloadIP, which can load an IP core at runtime such that RCTEE can be adjusted dynamically and securely; 2) CexecIP, which can not only execute an IP core without modifying the operating system of FPGA-SoC TEE, but also prevent insider attacks from executing IPs deployed in RCTEE; 3) CremoAT, which can provide the newly measured RCTEE state and establish a secure and trusted communication path between remote verifiers and RCTEE. We conduct a security analysis of RCTEE and its performance evaluation on Xilinx Zynq UltraScale+ XCZU15EG 2FFVB1156 MPSoC.","sentences":["Processing sensitive data and deploying well-designed Intellectual Property (IP) cores on remote Field Programmable Gate Array (FPGA) are prone to private data leakage and IP theft.","One effective solution is constructing Trusted Execution Environment (TEE) on FPGA-SoCs (FPGA System on Chips).","Researchers have integrated this type TEE with Trusted Platform Module (TPM)-based trusted boot, denoted as FPGA-SoC tbTEE.","But there is no effort on secure and trusted runtime customization of FPGA-SoC TEE.","This paper extends FPGA-SoC tbTEE to build Runtime Customizable TEE (RCTEE) on FPGA-SoC by additive three major components (our work): 1) CrloadIP, which can load an IP core at runtime such that RCTEE can be adjusted dynamically and securely; 2) CexecIP, which can not only execute an IP core without modifying the operating system of FPGA-SoC TEE, but also prevent insider attacks from executing IPs deployed in RCTEE; 3) CremoAT, which can provide the newly measured RCTEE state and establish a secure and trusted communication path between remote verifiers and RCTEE.","We conduct a security analysis of RCTEE and its performance evaluation on Xilinx Zynq UltraScale+ XCZU15EG 2FFVB1156 MPSoC."],"url":"http://arxiv.org/abs/2307.04375v1"}
{"created":"2023-07-10 07:00:06","title":"Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey","abstract":"End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This paper presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, main and auxiliary output, learning approaches ranging from imitation to reinforcement learning, and model evaluation techniques. The survey incorporates a detailed discussion of the explainability and safety aspects. Furthermore, it assesses the state-of-the-art, identifies challenges, and explores future possibilities. We maintained the latest advancements and their corresponding open-source implementations at https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning.","sentences":["End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation.","Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings.","This paper presents a comprehensive review of the End-to-End autonomous driving stack.","It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications.","Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality.","These categories encompass sensorial input, main and auxiliary output, learning approaches ranging from imitation to reinforcement learning, and model evaluation techniques.","The survey incorporates a detailed discussion of the explainability and safety aspects.","Furthermore, it assesses the state-of-the-art, identifies challenges, and explores future possibilities.","We maintained the latest advancements and their corresponding open-source implementations at https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning."],"url":"http://arxiv.org/abs/2307.04370v1"}
{"created":"2023-07-10 06:49:18","title":"ECS -- an Interactive Tool for Data Quality Assurance","abstract":"With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.","sentences":["With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important.","In this paper we present a novel approach for the assurance of data quality.","For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples.","This results in the detection of data points with potentially harmful properties for the use in safety-critical systems."],"url":"http://arxiv.org/abs/2307.04368v1"}
{"created":"2023-07-10 06:48:01","title":"Explanation Needs in App Reviews: Taxonomy and Automated Detection","abstract":"Explainability, i.e. the ability of a system to explain its behavior to users, has become an important quality of software-intensive systems. Recent work has focused on methods for generating explanations for various algorithmic paradigms (e.g., machine learning, self-adaptive systems). There is relatively little work on what situations and types of behavior should be explained. There is also a lack of support for eliciting explainability requirements. In this work, we explore the need for explanation expressed by users in app reviews. We manually coded a set of 1,730 app reviews from 8 apps and derived a taxonomy of Explanation Needs. We also explore several approaches to automatically identify Explanation Needs in app reviews. Our best classifier identifies Explanation Needs in 486 unseen reviews of 4 different apps with a weighted F-score of 86%. Our work contributes to a better understanding of users' Explanation Needs. Automated tools can help engineers focus on these needs and ultimately elicit valid Explanation Needs.","sentences":["Explainability, i.e. the ability of a system to explain its behavior to users, has become an important quality of software-intensive systems.","Recent work has focused on methods for generating explanations for various algorithmic paradigms (e.g., machine learning, self-adaptive systems).","There is relatively little work on what situations and types of behavior should be explained.","There is also a lack of support for eliciting explainability requirements.","In this work, we explore the need for explanation expressed by users in app reviews.","We manually coded a set of 1,730 app reviews from 8 apps and derived a taxonomy of Explanation Needs.","We also explore several approaches to automatically identify Explanation Needs in app reviews.","Our best classifier identifies Explanation Needs in 486 unseen reviews of 4 different apps with a weighted F-score of 86%.","Our work contributes to a better understanding of users' Explanation Needs.","Automated tools can help engineers focus on these needs and ultimately elicit valid Explanation Needs."],"url":"http://arxiv.org/abs/2307.04367v1"}
{"created":"2023-07-10 06:44:47","title":"One-Shot Pruning for Fast-adapting Pre-trained Models on Devices","abstract":"Large-scale pre-trained models have been remarkably successful in resolving downstream tasks. Nonetheless, deploying these models on low-capability devices still requires an effective approach, such as model pruning. However, pruning the model from scratch can pose a practical challenge given the limited resources of each downstream task or device. To tackle this issue, we present a scalable one-shot pruning method that leverages pruned knowledge of similar tasks to extract a sub-network from the pre-trained model for a new task. Specifically, we create a score mask using the pruned models of similar tasks to identify task-specific filters/nodes in the pre-trained model for the new task. Based on this mask, we conduct a single round of pruning to extract a suitably-sized sub-network that can quickly adapt to the new task with only a few training iterations. Our experimental analysis demonstrates the effectiveness of the proposed method on the convolutional neural networks (CNNs) and vision transformers (ViT) with various datasets. The proposed method consistently outperforms popular pruning baseline methods in terms of accuracy and efficiency when dealing with diverse downstream tasks with different memory constraints.","sentences":["Large-scale pre-trained models have been remarkably successful in resolving downstream tasks.","Nonetheless, deploying these models on low-capability devices still requires an effective approach, such as model pruning.","However, pruning the model from scratch can pose a practical challenge given the limited resources of each downstream task or device.","To tackle this issue, we present a scalable one-shot pruning method that leverages pruned knowledge of similar tasks to extract a sub-network from the pre-trained model for a new task.","Specifically, we create a score mask using the pruned models of similar tasks to identify task-specific filters/nodes in the pre-trained model for the new task.","Based on this mask, we conduct a single round of pruning to extract a suitably-sized sub-network that can quickly adapt to the new task with only a few training iterations.","Our experimental analysis demonstrates the effectiveness of the proposed method on the convolutional neural networks (CNNs) and vision transformers (ViT) with various datasets.","The proposed method consistently outperforms popular pruning baseline methods in terms of accuracy and efficiency when dealing with diverse downstream tasks with different memory constraints."],"url":"http://arxiv.org/abs/2307.04365v1"}
{"created":"2023-07-10 06:17:33","title":"Enhancing Cross-lingual Transfer via Phonemic Transcription Integration","abstract":"Previous cross-lingual transfer methods are restricted to orthographic representation learning via textual scripts. This limitation hampers cross-lingual transfer and is biased towards languages sharing similar well-known scripts. To alleviate the gap between languages from different writing scripts, we propose PhoneXL, a framework incorporating phonemic transcriptions as an additional linguistic modality beyond the traditional orthographic transcriptions for cross-lingual transfer. Particularly, we propose unsupervised alignment objectives to capture (1) local one-to-one alignment between the two different modalities, (2) alignment via multi-modality contexts to leverage information from additional modalities, and (3) alignment via multilingual contexts where additional bilingual dictionaries are incorporated. We also release the first phonemic-orthographic alignment dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech Tagging) among the understudied but interconnected Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals phonemic transcription provides essential information beyond the orthography to enhance cross-lingual transfer and bridge the gap among CJKV languages, leading to consistent improvements on cross-lingual token-level tasks over orthographic-based multilingual PLMs.","sentences":["Previous cross-lingual transfer methods are restricted to orthographic representation learning via textual scripts.","This limitation hampers cross-lingual transfer and is biased towards languages sharing similar well-known scripts.","To alleviate the gap between languages from different writing scripts, we propose PhoneXL, a framework incorporating phonemic transcriptions as an additional linguistic modality beyond the traditional orthographic transcriptions for cross-lingual transfer.","Particularly, we propose unsupervised alignment objectives to capture (1) local one-to-one alignment between the two different modalities, (2) alignment via multi-modality contexts to leverage information from additional modalities, and (3) alignment via multilingual contexts where additional bilingual dictionaries are incorporated.","We also release the first phonemic-orthographic alignment dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech Tagging) among the understudied but interconnected Chinese-Japanese-Korean-Vietnamese (CJKV) languages.","Our pilot study reveals phonemic transcription provides essential information beyond the orthography to enhance cross-lingual transfer and bridge the gap among CJKV languages, leading to consistent improvements on cross-lingual token-level tasks over orthographic-based multilingual PLMs."],"url":"http://arxiv.org/abs/2307.04361v1"}
{"created":"2023-07-10 06:05:23","title":"False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers","abstract":"The problem of revealing botnet activity through Domain Generation Algorithm (DGA) detection seems to be solved, considering that available deep learning classifiers achieve accuracies of over 99.9%. However, these classifiers provide a false sense of security as they are heavily biased and allow for trivial detection bypass. In this work, we leverage explainable artificial intelligence (XAI) methods to analyze the reasoning of deep learning classifiers and to systematically reveal such biases. We show that eliminating these biases from DGA classifiers considerably deteriorates their performance. Nevertheless we are able to design a context-aware detection system that is free of the identified biases and maintains the detection rate of state-of-the art deep learning classifiers. In this context, we propose a visual analysis system that helps to better understand a classifier's reasoning, thereby increasing trust in and transparency of detection methods and facilitating decision-making.","sentences":["The problem of revealing botnet activity through Domain Generation Algorithm (DGA) detection seems to be solved, considering that available deep learning classifiers achieve accuracies of over 99.9%.","However, these classifiers provide a false sense of security as they are heavily biased and allow for trivial detection bypass.","In this work, we leverage explainable artificial intelligence (XAI) methods to analyze the reasoning of deep learning classifiers and to systematically reveal such biases.","We show that eliminating these biases from DGA classifiers considerably deteriorates their performance.","Nevertheless we are able to design a context-aware detection system that is free of the identified biases and maintains the detection rate of state-of-the art deep learning classifiers.","In this context, we propose a visual analysis system that helps to better understand a classifier's reasoning, thereby increasing trust in and transparency of detection methods and facilitating decision-making."],"url":"http://arxiv.org/abs/2307.04358v1"}
{"created":"2023-07-10 05:49:20","title":"Reducing Information Loss for Spiking Neural Networks","abstract":"The Spiking Neural Network (SNN) has attracted more and more attention recently. It adopts binary spike signals to transmit information. Benefitting from the information passing paradigm of SNNs, the multiplications of activations and weights can be replaced by additions, which are more energy-efficient. However, its ``Hard Reset\" mechanism for the firing activity would ignore the difference among membrane potentials when the membrane potential is above the firing threshold, causing information loss. Meanwhile, quantifying the membrane potential to 0/1 spikes at the firing instants will inevitably introduce the quantization error thus bringing about information loss too. To address these problems, we propose to use the ``Soft Reset\" mechanism for the supervised training-based SNNs, which will drive the membrane potential to a dynamic reset potential according to its magnitude, and Membrane Potential Rectifier (MPR) to reduce the quantization error via redistributing the membrane potential to a range close to the spikes. Results show that the SNNs with the ``Soft Reset\" mechanism and MPR outperform their vanilla counterparts on both static and dynamic datasets.","sentences":["The Spiking Neural Network (SNN) has attracted more and more attention recently.","It adopts binary spike signals to transmit information.","Benefitting from the information passing paradigm of SNNs, the multiplications of activations and weights can be replaced by additions, which are more energy-efficient.","However, its ``Hard Reset\" mechanism for the firing activity would ignore the difference among membrane potentials when the membrane potential is above the firing threshold, causing information loss.","Meanwhile, quantifying the membrane potential to 0/1 spikes at the firing instants will inevitably introduce the quantization error thus bringing about information loss too.","To address these problems, we propose to use the ``Soft Reset\" mechanism for the supervised training-based SNNs, which will drive the membrane potential to a dynamic reset potential according to its magnitude, and Membrane Potential Rectifier (MPR) to reduce the quantization error via redistributing the membrane potential to a range close to the spikes.","Results show that the SNNs with the ``Soft Reset\" mechanism and MPR outperform their vanilla counterparts on both static and dynamic datasets."],"url":"http://arxiv.org/abs/2307.04356v1"}
{"created":"2023-07-10 05:33:41","title":"Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data","abstract":"In some applications of reinforcement learning, a dataset of pre-collected experience is already available but it is also possible to acquire some additional online data to help improve the quality of the policy. However, it may be preferable to gather additional data with a single, non-reactive exploration policy and avoid the engineering costs associated with switching policies.   In this paper we propose an algorithm with provable guarantees that can leverage an offline dataset to design a single non-reactive policy for exploration. We theoretically analyze the algorithm and measure the quality of the final policy as a function of the local coverage of the original dataset and the amount of additional data collected.","sentences":["In some applications of reinforcement learning, a dataset of pre-collected experience is already available but it is also possible to acquire some additional online data to help improve the quality of the policy.","However, it may be preferable to gather additional data with a single, non-reactive exploration policy and avoid the engineering costs associated with switching policies.   ","In this paper we propose an algorithm with provable guarantees that can leverage an offline dataset to design a single non-reactive policy for exploration.","We theoretically analyze the algorithm and measure the quality of the final policy as a function of the local coverage of the original dataset and the amount of additional data collected."],"url":"http://arxiv.org/abs/2307.04354v1"}
{"created":"2023-07-10 05:20:36","title":"The Linked Data Benchmark Council (LDBC): Driving competition and collaboration in the graph data management space","abstract":"Graph data management is instrumental for several use cases such as recommendation, root cause analysis, financial fraud detection, and enterprise knowledge representation. Efficiently supporting these use cases yields a number of unique requirements, including the need for a concise query language and graph-aware query optimization techniques. The goal of the Linked Data Benchmark Council (LDBC) is to design a set of standard benchmarks that capture representative categories of graph data management problems, making the performance of systems comparable and facilitating competition among vendors. LDBC also conducts research on graph schemas and graph query languages. This paper introduces the LDBC organization and its work over the last decade.","sentences":["Graph data management is instrumental for several use cases such as recommendation, root cause analysis, financial fraud detection, and enterprise knowledge representation.","Efficiently supporting these use cases yields a number of unique requirements, including the need for a concise query language and graph-aware query optimization techniques.","The goal of the Linked Data Benchmark Council (LDBC) is to design a set of standard benchmarks that capture representative categories of graph data management problems, making the performance of systems comparable and facilitating competition among vendors.","LDBC also conducts research on graph schemas and graph query languages.","This paper introduces the LDBC organization and its work over the last decade."],"url":"http://arxiv.org/abs/2307.04350v1"}
{"created":"2023-07-10 05:18:18","title":"RLTF: Reinforcement Learning from Unit Test Feedback","abstract":"The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, these RL methods have only used offline frameworks, limiting their exploration of new sample spaces. Additionally, current approaches that utilize unit test signals are rather simple, not accounting for specific error locations within the code. To address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code can be found at: https://github.com/Zyq-scut/RLTF.","sentences":["The goal of program synthesis, or code generation, is to generate executable code based on given descriptions.","Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code.","However, these RL methods have only used offline frameworks, limiting their exploration of new sample spaces.","Additionally, current approaches that utilize unit test signals are rather simple, not accounting for specific error locations within the code.","To address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs.","Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code.","Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.","Our code can be found at: https://github.com/Zyq-scut/RLTF."],"url":"http://arxiv.org/abs/2307.04349v1"}
{"created":"2023-07-10 05:12:05","title":"Injecting Logical Constraints into Neural Networks via Straight-Through Estimators","abstract":"Injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. We find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. More specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints. The experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. Also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.","sentences":["Injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI.","We find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning.","More specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints.","The experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients.","Also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints."],"url":"http://arxiv.org/abs/2307.04347v1"}
{"created":"2023-07-10 05:09:33","title":"Can Large Language Models Write Good Property-Based Tests?","abstract":"Property-based testing (PBT), while an established technique in the software testing research community, is still relatively underused in real-world software. Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test. Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for property-based tests. As large language models (LLMs) have recently shown promise in a variety of coding tasks, we explore the potential of using LLMs to synthesize property-based tests. We call our approach PBT-GPT, and propose three different strategies of prompting the LLM for PBT. We characterize various failure modes of PBT-GPT and detail an evaluation methodology for automatically synthesized property-based tests. PBT-GPT achieves promising results in our preliminary studies on sample Python library APIs in $\\texttt{numpy}$, $\\texttt{networkx}$, and $\\texttt{datetime}$.","sentences":["Property-based testing (PBT), while an established technique in the software testing research community, is still relatively underused in real-world software.","Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test.","Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for property-based tests.","As large language models (LLMs) have recently shown promise in a variety of coding tasks, we explore the potential of using LLMs to synthesize property-based tests.","We call our approach PBT-GPT, and propose three different strategies of prompting the LLM for PBT.","We characterize various failure modes of PBT-GPT and detail an evaluation methodology for automatically synthesized property-based tests.","PBT-GPT achieves promising results in our preliminary studies on sample Python library APIs in $\\texttt{numpy}$, $\\texttt{networkx}$, and $\\texttt{datetime}$."],"url":"http://arxiv.org/abs/2307.04346v1"}
{"created":"2023-07-10 05:06:41","title":"Continual Learning as Computationally Constrained Reinforcement Learning","abstract":"An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.","sentences":["An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities.","The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning.","This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research."],"url":"http://arxiv.org/abs/2307.04345v1"}
{"created":"2023-07-10 05:01:30","title":"ASCH-PUF: A \"Zero\" Bit Error Rate CMOS Physically Unclonable Function with Dual-Mode Low-Cost Stabilization","abstract":"Physically unclonable functions (PUFs) are increasingly adopted for low-cost and secure secret key and chip ID generations for embedded and IoT devices. Achieving 100% reproducible keys across wide temperature and voltage variations over the lifetime of a device is critical and conventionally requires large masking or Error Correction Code (ECC) overhead to guarantee. This paper presents an Automatic Self Checking and Healing (ASCH) stabilization technique for a state-of-the-art PUF cell design based on sub-threshold inverter chains. The ASCH system successfully removes all unstable PUF cells without the need for expensive temperature sweeps during unstable bit detection. By accurately finding all unstable bits without expensive temperature sweeps to find all unstable bits, ASCH achieves ultra-low bit error rate (BER), thus significantly reducing the costs of using ECC and enrollment. Our ASCH can operate in two modes, a static mode (S-ASCH) with a conventional pre-enrolled unstable bit mask and a dynamic mode (D-ASCH) that further eliminates the need for non-volatile memories (NVMs) for storing masks. The proposed ASCH-PUF is fabricated and evaluated in 65nm CMOS. The ASCH system achieves \"0\" Bit Error Rate (BER, < 1.77E-9) across temperature variations of -20{\\deg}C to 125{\\deg}C, and voltage variations of 0.7V to 1.4V, by masking 31% and 35% of all fabricated PUF bits in S-ASCH and D-ASCH mode respectively. The prototype achieves a measured throughput of 11.4 Gbps with 0.057 fJ/b core energy efficiency at 1.2V, 25{\\deg}C.","sentences":["Physically unclonable functions (PUFs) are increasingly adopted for low-cost and secure secret key and chip ID generations for embedded and IoT devices.","Achieving 100% reproducible keys across wide temperature and voltage variations over the lifetime of a device is critical and conventionally requires large masking or Error Correction Code (ECC) overhead to guarantee.","This paper presents an Automatic Self Checking and Healing (ASCH) stabilization technique for a state-of-the-art PUF cell design based on sub-threshold inverter chains.","The ASCH system successfully removes all unstable PUF cells without the need for expensive temperature sweeps during unstable bit detection.","By accurately finding all unstable bits without expensive temperature sweeps to find all unstable bits, ASCH achieves ultra-low bit error rate (BER), thus significantly reducing the costs of using ECC and enrollment.","Our ASCH can operate in two modes, a static mode (S-ASCH) with a conventional pre-enrolled unstable bit mask and a dynamic mode (D-ASCH) that further eliminates the need for non-volatile memories (NVMs) for storing masks.","The proposed ASCH-PUF is fabricated and evaluated in 65nm CMOS.","The ASCH system achieves \"0\" Bit Error Rate (BER, < 1.77E-9) across temperature variations of -20{\\deg}C to 125{\\deg}C, and voltage variations of 0.7V to 1.4V, by masking 31% and 35% of all fabricated PUF bits in S-ASCH and D-ASCH mode respectively.","The prototype achieves a measured throughput of 11.4 Gbps with 0.057 fJ/b core energy efficiency at 1.2V, 25{\\deg}C."],"url":"http://arxiv.org/abs/2307.04344v1"}
{"created":"2023-07-10 04:54:05","title":"Hierarchical Semantic Tree Concept Whitening for Interpretable Image Classification","abstract":"With the popularity of deep neural networks (DNNs), model interpretability is becoming a critical concern. Many approaches have been developed to tackle the problem through post-hoc analysis, such as explaining how predictions are made or understanding the meaning of neurons in middle layers. Nevertheless, these methods can only discover the patterns or rules that naturally exist in models. In this work, rather than relying on post-hoc schemes, we proactively instill knowledge to alter the representation of human-understandable concepts in hidden layers. Specifically, we use a hierarchical tree of semantic concepts to store the knowledge, which is leveraged to regularize the representations of image data instances while training deep models. The axes of the latent space are aligned with the semantic concepts, where the hierarchical relations between concepts are also preserved. Experiments on real-world image datasets show that our method improves model interpretability, showing better disentanglement of semantic concepts, without negatively affecting model classification performance.","sentences":["With the popularity of deep neural networks (DNNs), model interpretability is becoming a critical concern.","Many approaches have been developed to tackle the problem through post-hoc analysis, such as explaining how predictions are made or understanding the meaning of neurons in middle layers.","Nevertheless, these methods can only discover the patterns or rules that naturally exist in models.","In this work, rather than relying on post-hoc schemes, we proactively instill knowledge to alter the representation of human-understandable concepts in hidden layers.","Specifically, we use a hierarchical tree of semantic concepts to store the knowledge, which is leveraged to regularize the representations of image data instances while training deep models.","The axes of the latent space are aligned with the semantic concepts, where the hierarchical relations between concepts are also preserved.","Experiments on real-world image datasets show that our method improves model interpretability, showing better disentanglement of semantic concepts, without negatively affecting model classification performance."],"url":"http://arxiv.org/abs/2307.04343v1"}
{"created":"2023-07-10 04:50:17","title":"Stroke Extraction of Chinese Character Based on Deep Structure Deformable Image Registration","abstract":"Stroke extraction of Chinese characters plays an important role in the field of character recognition and generation. The most existing character stroke extraction methods focus on image morphological features. These methods usually lead to errors of cross strokes extraction and stroke matching due to rarely using stroke semantics and prior information. In this paper, we propose a deep learning-based character stroke extraction method that takes semantic features and prior information of strokes into consideration. This method consists of three parts: image registration-based stroke registration that establishes the rough registration of the reference strokes and the target as prior information; image semantic segmentation-based stroke segmentation that preliminarily separates target strokes into seven categories; and high-precision extraction of single strokes. In the stroke registration, we propose a structure deformable image registration network to achieve structure-deformable transformation while maintaining the stable morphology of single strokes for character images with complex structures. In order to verify the effectiveness of the method, we construct two datasets respectively for calligraphy characters and regular handwriting characters. The experimental results show that our method strongly outperforms the baselines. Code is available at https://github.com/MengLi-l1/StrokeExtraction.","sentences":["Stroke extraction of Chinese characters plays an important role in the field of character recognition and generation.","The most existing character stroke extraction methods focus on image morphological features.","These methods usually lead to errors of cross strokes extraction and stroke matching due to rarely using stroke semantics and prior information.","In this paper, we propose a deep learning-based character stroke extraction method that takes semantic features and prior information of strokes into consideration.","This method consists of three parts: image registration-based stroke registration that establishes the rough registration of the reference strokes and the target as prior information; image semantic segmentation-based stroke segmentation that preliminarily separates target strokes into seven categories; and high-precision extraction of single strokes.","In the stroke registration, we propose a structure deformable image registration network to achieve structure-deformable transformation while maintaining the stable morphology of single strokes for character images with complex structures.","In order to verify the effectiveness of the method, we construct two datasets respectively for calligraphy characters and regular handwriting characters.","The experimental results show that our method strongly outperforms the baselines.","Code is available at https://github.com/MengLi-l1/StrokeExtraction."],"url":"http://arxiv.org/abs/2307.04341v1"}
{"created":"2023-07-10 04:30:44","title":"Miriam: Exploiting Elastic Kernels for Real-time Multi-DNN Inference on Edge GPU","abstract":"Many applications such as autonomous driving and augmented reality, require the concurrent running of multiple deep neural networks (DNN) that poses different levels of real-time performance requirements. However, coordinating multiple DNN tasks with varying levels of criticality on edge GPUs remains an area of limited study. Unlike server-level GPUs, edge GPUs are resource-limited and lack hardware-level resource management mechanisms for avoiding resource contention. Therefore, we propose Miriam, a contention-aware task coordination framework for multi-DNN inference on edge GPU. Miriam consolidates two main components, an elastic-kernel generator, and a runtime dynamic kernel coordinator, to support mixed critical DNN inference. To evaluate Miriam, we build a new DNN inference benchmark based on CUDA with diverse representative DNN workloads. Experiments on two edge GPU platforms show that Miriam can increase system throughput by 92% while only incurring less than 10\\% latency overhead for critical tasks, compared to state of art baselines.","sentences":["Many applications such as autonomous driving and augmented reality, require the concurrent running of multiple deep neural networks (DNN) that poses different levels of real-time performance requirements.","However, coordinating multiple DNN tasks with varying levels of criticality on edge GPUs remains an area of limited study.","Unlike server-level GPUs, edge GPUs are resource-limited and lack hardware-level resource management mechanisms for avoiding resource contention.","Therefore, we propose Miriam, a contention-aware task coordination framework for multi-DNN inference on edge GPU.","Miriam consolidates two main components, an elastic-kernel generator, and a runtime dynamic kernel coordinator, to support mixed critical DNN inference.","To evaluate Miriam, we build a new DNN inference benchmark based on CUDA with diverse representative DNN workloads.","Experiments on two edge GPU platforms show that Miriam can increase system throughput by 92% while only incurring less than 10\\% latency overhead for critical tasks, compared to state of art baselines."],"url":"http://arxiv.org/abs/2307.04339v1"}
{"created":"2023-07-10 04:30:23","title":"Privacy-Preserving Graph Machine Learning from Data to Computation: A Survey","abstract":"In graph machine learning, data collection, sharing, and analysis often involve multiple parties, each of which may require varying levels of data security and privacy. To this end, preserving privacy is of great importance in protecting sensitive information. In the era of big data, the relationships among data entities have become unprecedentedly complex, and more applications utilize advanced data structures (i.e., graphs) that can support network structures and relevant attribute information. To date, many graph-based AI models have been proposed (e.g., graph neural networks) for various domain tasks, like computer vision and natural language processing. In this paper, we focus on reviewing privacy-preserving techniques of graph machine learning. We systematically review related works from the data to the computational aspects. We first review methods for generating privacy-preserving graph data. Then we describe methods for transmitting privacy-preserved information (e.g., graph model parameters) to realize the optimization-based computation when data sharing among multiple parties is risky or impossible. In addition to discussing relevant theoretical methodology and software tools, we also discuss current challenges and highlight several possible future research opportunities for privacy-preserving graph machine learning. Finally, we envision a unified and comprehensive secure graph machine learning system.","sentences":["In graph machine learning, data collection, sharing, and analysis often involve multiple parties, each of which may require varying levels of data security and privacy.","To this end, preserving privacy is of great importance in protecting sensitive information.","In the era of big data, the relationships among data entities have become unprecedentedly complex, and more applications utilize advanced data structures (i.e., graphs) that can support network structures and relevant attribute information.","To date, many graph-based AI models have been proposed (e.g., graph neural networks) for various domain tasks, like computer vision and natural language processing.","In this paper, we focus on reviewing privacy-preserving techniques of graph machine learning.","We systematically review related works from the data to the computational aspects.","We first review methods for generating privacy-preserving graph data.","Then we describe methods for transmitting privacy-preserved information (e.g., graph model parameters) to realize the optimization-based computation when data sharing among multiple parties is risky or impossible.","In addition to discussing relevant theoretical methodology and software tools, we also discuss current challenges and highlight several possible future research opportunities for privacy-preserving graph machine learning.","Finally, we envision a unified and comprehensive secure graph machine learning system."],"url":"http://arxiv.org/abs/2307.04338v1"}
{"created":"2023-07-10 04:22:49","title":"Source-Aware Embedding Training on Heterogeneous Information Networks","abstract":"Heterogeneous information networks (HINs) have been extensively applied to real-world tasks, such as recommendation systems, social networks, and citation networks. While existing HIN representation learning methods can effectively learn the semantic and structural features in the network, little awareness was given to the distribution discrepancy of subgraphs within a single HIN. However, we find that ignoring such distribution discrepancy among subgraphs from multiple sources would hinder the effectiveness of graph embedding learning algorithms. This motivates us to propose SUMSHINE (Scalable Unsupervised Multi-Source Heterogeneous Information Network Embedding) -- a scalable unsupervised framework to align the embedding distributions among multiple sources of an HIN. Experimental results on real-world datasets in a variety of downstream tasks validate the performance of our method over the state-of-the-art heterogeneous information network embedding algorithms.","sentences":["Heterogeneous information networks (HINs) have been extensively applied to real-world tasks, such as recommendation systems, social networks, and citation networks.","While existing HIN representation learning methods can effectively learn the semantic and structural features in the network, little awareness was given to the distribution discrepancy of subgraphs within a single HIN.","However, we find that ignoring such distribution discrepancy among subgraphs from multiple sources would hinder the effectiveness of graph embedding learning algorithms.","This motivates us to propose SUMSHINE (Scalable Unsupervised Multi-Source Heterogeneous Information Network Embedding) -- a scalable unsupervised framework to align the embedding distributions among multiple sources of an HIN.","Experimental results on real-world datasets in a variety of downstream tasks validate the performance of our method over the state-of-the-art heterogeneous information network embedding algorithms."],"url":"http://arxiv.org/abs/2307.04336v1"}
{"created":"2023-07-10 03:59:42","title":"Enhancing Adversarial Robustness via Score-Based Optimization","abstract":"Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.","sentences":["Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations.","Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence.","Recent studies have suggested that score-based diffusion models are effective in adversarial defenses.","However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results.","In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors.","We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet.","Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed."],"url":"http://arxiv.org/abs/2307.04333v1"}
{"created":"2023-07-10 03:52:42","title":"Improved Diversity Maximization Algorithms for Matching and Pseudoforest","abstract":"In this work we consider the diversity maximization problem, where given a data set $X$ of $n$ elements, and a parameter $k$, the goal is to pick a subset of $X$ of size $k$ maximizing a certain diversity measure. [CH01] defined a variety of diversity measures based on pairwise distances between the points. A constant factor approximation algorithm was known for all those diversity measures except ``remote-matching'', where only an $O(\\log k)$ approximation was known. In this work we present an $O(1)$ approximation for this remaining notion. Further, we consider these notions from the perpective of composable coresets. [IMMM14] provided composable coresets with a constant factor approximation for all but ``remote-pseudoforest'' and ``remote-matching'', which again they only obtained a $O(\\log k)$ approximation. Here we also close the gap up to constants and present a constant factor composable coreset algorithm for these two notions. For remote-matching, our coreset has size only $O(k)$, and for remote-pseudoforest, our coreset has size $O(k^{1+\\varepsilon})$ for any $\\varepsilon > 0$, for an $O(1/\\varepsilon)$-approximate coreset.","sentences":["In this work we consider the diversity maximization problem, where given a data set $X$ of $n$ elements, and a parameter $k$, the goal is to pick a subset of $X$ of size $k$ maximizing a certain diversity measure.","[CH01] defined a variety of diversity measures based on pairwise distances between the points.","A constant factor approximation algorithm was known for all those diversity measures except ``remote-matching'', where only an $O(\\log k)$ approximation was known.","In this work we present an $O(1)$ approximation for this remaining notion.","Further, we consider these notions from the perpective of composable coresets.","[IMMM14] provided composable coresets with a constant factor approximation for all but ``remote-pseudoforest'' and ``remote-matching'', which again they only obtained a $O(\\log k)$ approximation.","Here we also close the gap up to constants and present a constant factor composable coreset algorithm for these two notions.","For remote-matching, our coreset has size only $O(k)$, and for remote-pseudoforest, our coreset has size $O(k^{1+\\varepsilon})$ for any $\\varepsilon > 0$, for an $O(1/\\varepsilon)$-approximate coreset."],"url":"http://arxiv.org/abs/2307.04329v1"}
{"created":"2023-07-10 03:47:32","title":"Where to Drop Sensors from Aerial Robots to Monitor a Surface-Level Phenomenon?","abstract":"We consider the problem of routing a team of energy-constrained Unmanned Aerial Vehicles (UAVs) to drop unmovable sensors for monitoring a task area in the presence of stochastic wind disturbances. In prior work on mobile sensor routing problems, sensors and their carrier are one integrated platform, and sensors are assumed to be able to take measurements at exactly desired locations. By contrast, airdropping the sensors onto the ground can introduce stochasticity in the landing locations of the sensors. We focus on addressing this stochasticity in sensor locations from the path-planning perspective. Specifically, we formulate the problem (Multi-UAV Sensor Drop) as a variant of the Submodular Team Orienteering Problem with one additional constraint on the number of sensors on each UAV. The objective is to maximize the Mutual Information between the phenomenon at Points of Interest (PoIs) and the measurements that sensors will take at stochastic locations. We show that such an objective is computationally expensive to evaluate. To tackle this challenge, we propose a surrogate objective with a closed-form expression based on the expected mean and expected covariance of the Gaussian Process. We propose a heuristic algorithm to solve the optimization problem with the surrogate objective. The formulation and the algorithms are validated through extensive simulations.","sentences":["We consider the problem of routing a team of energy-constrained Unmanned Aerial Vehicles (UAVs) to drop unmovable sensors for monitoring a task area in the presence of stochastic wind disturbances.","In prior work on mobile sensor routing problems, sensors and their carrier are one integrated platform, and sensors are assumed to be able to take measurements at exactly desired locations.","By contrast, airdropping the sensors onto the ground can introduce stochasticity in the landing locations of the sensors.","We focus on addressing this stochasticity in sensor locations from the path-planning perspective.","Specifically, we formulate the problem (Multi-UAV Sensor Drop) as a variant of the Submodular Team Orienteering Problem with one additional constraint on the number of sensors on each UAV.","The objective is to maximize the Mutual Information between the phenomenon at Points of Interest (PoIs) and the measurements that sensors will take at stochastic locations.","We show that such an objective is computationally expensive to evaluate.","To tackle this challenge, we propose a surrogate objective with a closed-form expression based on the expected mean and expected covariance of the Gaussian Process.","We propose a heuristic algorithm to solve the optimization problem with the surrogate objective.","The formulation and the algorithms are validated through extensive simulations."],"url":"http://arxiv.org/abs/2307.04328v1"}
{"created":"2023-07-10 03:43:41","title":"Legal Decision-making for Highway Automated Driving","abstract":"Compliance with traffic laws is a fundamental requirement for human drivers on the road, and autonomous vehicles must adhere to traffic laws as well. However, current autonomous vehicles prioritize safety and collision avoidance primarily in their decision-making and planning, which will lead to misunderstandings and distrust from human drivers and may even result in accidents in mixed traffic flow. Therefore, ensuring the compliance of the autonomous driving decision-making system is essential for ensuring the safety of autonomous driving and promoting the widespread adoption of autonomous driving technology. To this end, the paper proposes a trigger-based layered compliance decision-making framework. This framework utilizes the decision intent at the highest level as a signal to activate an online violation monitor that identifies the type of violation committed by the vehicle. Then, a four-layer architecture for compliance decision-making is employed to generate compliantly trajectories. Using this system, autonomous vehicles can detect and correct potential violations in real-time, thereby enhancing safety and building public confidence in autonomous driving technology. Finally, the proposed method is evaluated on the DJI AD4CHE highway dataset under four typical highway scenarios: speed limit, following distance, overtaking, and lane-changing. The results indicate that the proposed method increases the vehicle's overall compliance rate from 13.85% to 84.46%, while reducing the proportion of active violations to 0%, demonstrating its effectiveness.","sentences":["Compliance with traffic laws is a fundamental requirement for human drivers on the road, and autonomous vehicles must adhere to traffic laws as well.","However, current autonomous vehicles prioritize safety and collision avoidance primarily in their decision-making and planning, which will lead to misunderstandings and distrust from human drivers and may even result in accidents in mixed traffic flow.","Therefore, ensuring the compliance of the autonomous driving decision-making system is essential for ensuring the safety of autonomous driving and promoting the widespread adoption of autonomous driving technology.","To this end, the paper proposes a trigger-based layered compliance decision-making framework.","This framework utilizes the decision intent at the highest level as a signal to activate an online violation monitor that identifies the type of violation committed by the vehicle.","Then, a four-layer architecture for compliance decision-making is employed to generate compliantly trajectories.","Using this system, autonomous vehicles can detect and correct potential violations in real-time, thereby enhancing safety and building public confidence in autonomous driving technology.","Finally, the proposed method is evaluated on the DJI AD4CHE highway dataset under four typical highway scenarios: speed limit, following distance, overtaking, and lane-changing.","The results indicate that the proposed method increases the vehicle's overall compliance rate from 13.85% to 84.46%, while reducing the proportion of active violations to 0%, demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2307.04327v1"}
{"created":"2023-07-10 03:32:48","title":"Optimal $(2,\u03b4)$ Locally Repairable Codes via Punctured Simplex Codes","abstract":"Locally repairable codes (LRCs) have attracted a lot of attention due to their applications in distributed storage systems. In this paper, we provide new constructions of optimal $(2, \\delta)$-LRCs. Firstly, by the techniques of finite geometry, we present a sufficient condition to guarantee a punctured simplex code to be a $(2, \\delta)$-LRC. Secondly, by using characteristic sums over finite fields and Krawtchouk polynomials, we construct several families of LRCs with new parameters. All of our new LRCs are optimal with respect to the generalized Cadambe-Mazumdar bound.","sentences":["Locally repairable codes (LRCs) have attracted a lot of attention due to their applications in distributed storage systems.","In this paper, we provide new constructions of optimal $(2, \\delta)$-LRCs.","Firstly, by the techniques of finite geometry, we present a sufficient condition to guarantee a punctured simplex code to be a $(2, \\delta)$-LRC.","Secondly, by using characteristic sums over finite fields and Krawtchouk polynomials, we construct several families of LRCs with new parameters.","All of our new LRCs are optimal with respect to the generalized Cadambe-Mazumdar bound."],"url":"http://arxiv.org/abs/2307.04323v1"}
{"created":"2023-07-10 03:31:33","title":"Graph Contrastive Learning with Multi-Objective for Personalized Product Retrieval in Taobao Search","abstract":"In e-commerce search, personalized retrieval is a crucial technique for improving user shopping experience. Recent works in this domain have achieved significant improvements by the representation learning paradigm, e.g., embedding-based retrieval (EBR) and collaborative filtering (CF). EBR methods do not sufficiently exploit the useful collaborative signal and are difficult to learn the representations of long-tail item well. Graph-based CF methods improve personalization by modeling collaborative signal within the user click graph. However, existing Graph-based methods ignore user's multiple behaviours, such as click/purchase and the relevance constraint between user behaviours and items.In this paper, we propose a Graph Contrastive Learning with Multi-Objective (GCL-MO) collaborative filtering model, which solves the problems of weak relevance and incomplete personalization in e-commerce search. Specifically, GCL-MO builds a homogeneous graph of items and then optimizes a multi-objective function of personalization and relevance. Moreover, we propose a modified contrastive loss for multi-objectives graph learning, which avoids the mutual suppression among positive samples and thus improves the generalization and robustness of long-tail item representations. These learned item embeddings are then used for personalized retrieval by constructing an efficient offline-to-online inverted table. GCL-MO outperforms the online collaborative filtering baseline in both offline/online experimental metrics and shows a significant improvement in the online A/B testing of Taobao search.","sentences":["In e-commerce search, personalized retrieval is a crucial technique for improving user shopping experience.","Recent works in this domain have achieved significant improvements by the representation learning paradigm, e.g., embedding-based retrieval (EBR) and collaborative filtering (CF).","EBR methods do not sufficiently exploit the useful collaborative signal and are difficult to learn the representations of long-tail item well.","Graph-based CF methods improve personalization by modeling collaborative signal within the user click graph.","However, existing Graph-based methods ignore user's multiple behaviours, such as click/purchase and the relevance constraint between user behaviours and items.","In this paper, we propose a Graph Contrastive Learning with Multi-Objective (GCL-MO) collaborative filtering model, which solves the problems of weak relevance and incomplete personalization in e-commerce search.","Specifically, GCL-MO builds a homogeneous graph of items and then optimizes a multi-objective function of personalization and relevance.","Moreover, we propose a modified contrastive loss for multi-objectives graph learning, which avoids the mutual suppression among positive samples and thus improves the generalization and robustness of long-tail item representations.","These learned item embeddings are then used for personalized retrieval by constructing an efficient offline-to-online inverted table.","GCL-MO outperforms the online collaborative filtering baseline in both offline/online experimental metrics and shows a significant improvement in the online A/B testing of Taobao search."],"url":"http://arxiv.org/abs/2307.04322v1"}
{"created":"2023-07-10 03:29:31","title":"RaPlace: Place Recognition for Imaging Radar using Radon Transform and Mutable Threshold","abstract":"Due to the robustness in sensing, radar has been highlighted, overcoming harsh weather conditions such as fog and heavy snow. In this paper, we present a novel radar-only place recognition that measures the similarity score by utilizing Radon-transformed sinogram images and cross-correlation in frequency domain. Doing so achieves rigid transform invariance during place recognition, while ignoring the effects of radar multipath and ring noises. In addition, we compute the radar similarity distance using mutable threshold to mitigate variability of the similarity score, and reduce the time complexity of processing a copious radar data with hierarchical retrieval. We demonstrate the matching performance for both intra-session loop-closure detection and global place recognition using a publicly available imaging radar datasets. We verify reliable performance compared to existing stable radar place recognition method. Furthermore, codes for the proposed imaging radar place recognition is released for community.","sentences":["Due to the robustness in sensing, radar has been highlighted, overcoming harsh weather conditions such as fog and heavy snow.","In this paper, we present a novel radar-only place recognition that measures the similarity score by utilizing Radon-transformed sinogram images and cross-correlation in frequency domain.","Doing so achieves rigid transform invariance during place recognition, while ignoring the effects of radar multipath and ring noises.","In addition, we compute the radar similarity distance using mutable threshold to mitigate variability of the similarity score, and reduce the time complexity of processing a copious radar data with hierarchical retrieval.","We demonstrate the matching performance for both intra-session loop-closure detection and global place recognition using a publicly available imaging radar datasets.","We verify reliable performance compared to existing stable radar place recognition method.","Furthermore, codes for the proposed imaging radar place recognition is released for community."],"url":"http://arxiv.org/abs/2307.04321v1"}
{"created":"2023-07-10 03:20:47","title":"New Variants of Frank-Wolfe Algorithm for Video Co-localization Problem","abstract":"The co-localization problem is a model that simultaneously localizes objects of the same class within a series of images or videos. In \\cite{joulin2014efficient}, authors present new variants of the Frank-Wolfe algorithm (aka conditional gradient) that increase the efficiency in solving the image and video co-localization problems. The authors show the efficiency of their methods with the rate of decrease in a value called the Wolfe gap in each iteration of the algorithm. In this project, inspired by the conditional gradient sliding algorithm (CGS) \\cite{CGS:Lan}, We propose algorithms for solving such problems and demonstrate the efficiency of the proposed algorithms through numerical experiments. The efficiency of these methods with respect to the Wolfe gap is compared with implementing them on the YouTube-Objects dataset for videos.","sentences":["The co-localization problem is a model that simultaneously localizes objects of the same class within a series of images or videos.","In \\cite{joulin2014efficient}, authors present new variants of the Frank-Wolfe algorithm (aka conditional gradient) that increase the efficiency in solving the image and video co-localization problems.","The authors show the efficiency of their methods with the rate of decrease in a value called the Wolfe gap in each iteration of the algorithm.","In this project, inspired by the conditional gradient sliding algorithm (CGS) \\cite{CGS:Lan}, We propose algorithms for solving such problems and demonstrate the efficiency of the proposed algorithms through numerical experiments.","The efficiency of these methods with respect to the Wolfe gap is compared with implementing them on the YouTube-Objects dataset for videos."],"url":"http://arxiv.org/abs/2307.04319v1"}
{"created":"2023-07-10 03:06:45","title":"Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning","abstract":"Modern image classification is based upon directly predicting model classes via large discriminative networks, making it difficult to assess the intuitive visual ``features'' that may constitute a classification decision. At the same time, recent works in joint visual language models such as CLIP provide ways to specify natural language descriptions of image classes but typically focus on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, arguably more akin to our understanding of multiple ``visual features'' per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we automatically enumerate multiple visual descriptions of each class -- via a large language model (LLM) -- then use a vision-image model to translate these descriptions to a set of multiple visual features of each image; we finally use sparse logistic regression to select a relevant subset of these features to classify each image. This both provides an ``intuitive'' set of relevant features for each class, and in the few-shot learning setting, outperforms standard approaches such as linear probing. When combined with finetuning, we also show that the method is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.","sentences":["Modern image classification is based upon directly predicting model classes via large discriminative networks, making it difficult to assess the intuitive visual ``features'' that may constitute a classification decision.","At the same time, recent works in joint visual language models such as CLIP provide ways to specify natural language descriptions of image classes but typically focus on providing single descriptions for each class.","In this work, we demonstrate that an alternative approach, arguably more akin to our understanding of multiple ``visual features'' per class, can also provide compelling performance in the robust few-shot learning setting.","In particular, we automatically enumerate multiple visual descriptions of each class -- via a large language model (LLM) -- then use a vision-image model to translate these descriptions to a set of multiple visual features of each image; we finally use sparse logistic regression to select a relevant subset of these features to classify each image.","This both provides an ``intuitive'' set of relevant features for each class, and in the few-shot learning setting, outperforms standard approaches such as linear probing.","When combined with finetuning, we also show that the method is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance."],"url":"http://arxiv.org/abs/2307.04317v1"}
{"created":"2023-07-10 03:05:24","title":"Accelerating Secure and Verifiable Data Deletion in Cloud Storage via SGX and Blockchain","abstract":"Secure data deletion enables data owners to fully control the erasure of their data stored on local or cloud data centers and is essential for preventing data leakage, especially for cloud storage. However, traditional data deletion based on unlinking, overwriting, and cryptographic key management either ineffectiveness in cloud storage or rely on unpractical assumption. In this paper, we present SevDel, a secure and verifiable data deletion scheme, which leverages the zero-knowledge proof to achieve the verification of the encryption of the outsourced data without retrieving the ciphertexts, while the deletion of the encryption keys are guaranteed based on Intel SGX. SevDel implements secure interfaces to perform data encryption and decryption for secure cloud storage. It also utilizes smart contract to enforce the operations of the cloud service provider to follow service level agreements with data owners and the penalty over the service provider, who discloses the cloud data on its servers. Evaluation on real-world workload demonstrates that SevDel achieves efficient data deletion verification and maintain high bandwidth savings.","sentences":["Secure data deletion enables data owners to fully control the erasure of their data stored on local or cloud data centers and is essential for preventing data leakage, especially for cloud storage.","However, traditional data deletion based on unlinking, overwriting, and cryptographic key management either ineffectiveness in cloud storage or rely on unpractical assumption.","In this paper, we present SevDel, a secure and verifiable data deletion scheme, which leverages the zero-knowledge proof to achieve the verification of the encryption of the outsourced data without retrieving the ciphertexts, while the deletion of the encryption keys are guaranteed based on Intel SGX.","SevDel implements secure interfaces to perform data encryption and decryption for secure cloud storage.","It also utilizes smart contract to enforce the operations of the cloud service provider to follow service level agreements with data owners and the penalty over the service provider, who discloses the cloud data on its servers.","Evaluation on real-world workload demonstrates that SevDel achieves efficient data deletion verification and maintain high bandwidth savings."],"url":"http://arxiv.org/abs/2307.04316v1"}
{"created":"2023-07-10 02:55:35","title":"Robust Feature Learning Against Noisy Labels","abstract":"Supervised learning of deep neural networks heavily relies on large-scale datasets annotated by high-quality labels. In contrast, mislabeled samples can significantly degrade the generalization of models and result in memorizing samples, further learning erroneous associations of data contents to incorrect annotations. To this end, this paper proposes an efficient approach to tackle noisy labels by learning robust feature representation based on unsupervised augmentation restoration and cluster regularization. In addition, progressive self-bootstrapping is introduced to minimize the negative impact of supervision from noisy labels. Our proposed design is generic and flexible in applying to existing classification architectures with minimal overheads. Experimental results show that our proposed method can efficiently and effectively enhance model robustness under severely noisy labels.","sentences":["Supervised learning of deep neural networks heavily relies on large-scale datasets annotated by high-quality labels.","In contrast, mislabeled samples can significantly degrade the generalization of models and result in memorizing samples, further learning erroneous associations of data contents to incorrect annotations.","To this end, this paper proposes an efficient approach to tackle noisy labels by learning robust feature representation based on unsupervised augmentation restoration and cluster regularization.","In addition, progressive self-bootstrapping is introduced to minimize the negative impact of supervision from noisy labels.","Our proposed design is generic and flexible in applying to existing classification architectures with minimal overheads.","Experimental results show that our proposed method can efficiently and effectively enhance model robustness under severely noisy labels."],"url":"http://arxiv.org/abs/2307.04312v1"}
{"created":"2023-07-10 02:27:38","title":"CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training","abstract":"Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades. At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank. Indeed, there have been a few works around this topic. Most (if not all) of them are limited in the scope of a single table or fixed form of a schema. In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario. We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information. (ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT. Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks. We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables. The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches.","sentences":["Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades.","At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank.","Indeed, there have been a few works around this topic.","Most (if not all) of them are limited in the scope of a single table or fixed form of a schema.","In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario.","We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information.","(ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT.","Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks.","We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables.","The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches."],"url":"http://arxiv.org/abs/2307.04308v1"}
{"created":"2023-07-10 02:04:43","title":"Automatic Piano Transcription with Hierarchical Frequency-Time Transformer","abstract":"Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.","sentences":["Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription.","This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content.","In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes.","In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture.","The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis.","The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis.","We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations."],"url":"http://arxiv.org/abs/2307.04305v1"}
{"created":"2023-07-10 01:44:13","title":"Learning to Generate Equitable Text in Dialogue from Biased Training Data","abstract":"The ingrained principles of fairness in a dialogue system's decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn. To exemplify our theory in practice, we look at a group of algorithms for the GuessWhat?! visual dialogue game and, using this example, test our theory empirically. Our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation.","sentences":["The ingrained principles of fairness in a dialogue system's decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement.","Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system.","For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject.","Yet, there is no comprehensive study of equitable text generation in dialogue.","Aptly, in this work, we use theories of computational learning to study this problem.","We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data).","With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn.","To exemplify our theory in practice, we look at a group of algorithms for the GuessWhat?!","visual dialogue game","and, using this example, test our theory empirically.","Our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation."],"url":"http://arxiv.org/abs/2307.04303v1"}
{"created":"2023-07-10 01:41:46","title":"Auction Design for Value Maximizers with Budget and Return-on-spend Constraints","abstract":"The paper designs revenue-maximizing auction mechanisms for agents who aim to maximize their total obtained values rather than the classical quasi-linear utilities. Several models have been proposed to capture the behaviors of such agents in the literature. In the paper, we consider the model where agents are subject to budget and return-on-spend constraints. The budget constraint of an agent limits the maximum payment she can afford, while the return-on-spend constraint means that the ratio of the total obtained value (return) to the total payment (spend) cannot be lower than the targeted bar set by the agent. The problem was first coined by [Balseiro et al., EC 2022]. In their work, only Bayesian mechanisms were considered. We initiate the study of the problem in the worst-case model and compare the revenue of our mechanisms to an offline optimal solution, the most ambitious benchmark. The paper distinguishes two main auction settings based on the accessibility of agents' information: fully private and partially private. In the fully private setting, an agent's valuation, budget, and target bar are all private. We show that if agents are unit-demand, constant approximation mechanisms can be obtained; while for additive agents, there exists a mechanism that achieves a constant approximation ratio under a large market assumption. The partially private setting is the setting considered in the previous work [Balseiro et al., EC 2022] where only the agents' target bars are private. We show that in this setting, the approximation ratio of the single-item auction can be further improved, and a $\\Omega(1/\\sqrt{n})$-approximation mechanism can be derived for additive agents.","sentences":["The paper designs revenue-maximizing auction mechanisms for agents who aim to maximize their total obtained values rather than the classical quasi-linear utilities.","Several models have been proposed to capture the behaviors of such agents in the literature.","In the paper, we consider the model where agents are subject to budget and return-on-spend constraints.","The budget constraint of an agent limits the maximum payment she can afford, while the return-on-spend constraint means that the ratio of the total obtained value (return) to the total payment (spend) cannot be lower than the targeted bar set by the agent.","The problem was first coined by [Balseiro et al., EC 2022].","In their work, only Bayesian mechanisms were considered.","We initiate the study of the problem in the worst-case model and compare the revenue of our mechanisms to an offline optimal solution, the most ambitious benchmark.","The paper distinguishes two main auction settings based on the accessibility of agents' information: fully private and partially private.","In the fully private setting, an agent's valuation, budget, and target bar are all private.","We show that if agents are unit-demand, constant approximation mechanisms can be obtained; while for additive agents, there exists a mechanism that achieves a constant approximation ratio under a large market assumption.","The partially private setting is the setting considered in the previous work","[Balseiro et al., EC 2022] where only the agents' target bars are private.","We show that in this setting, the approximation ratio of the single-item auction can be further improved, and a $\\Omega(1/\\sqrt{n})$-approximation mechanism can be derived for additive agents."],"url":"http://arxiv.org/abs/2307.04302v1"}
{"created":"2023-07-10 01:36:51","title":"NN-EVP: A physics informed neural network-based elasto-viscoplastic framework for predictions of grain size-aware flow response under large deformations","abstract":"We propose a physics informed, neural network-based elasto-viscoplasticity (NN-EVP) constitutive modeling framework for predicting the flow response in metals as a function of underlying grain size. The developed NN-EVP algorithm is based on input convex neural networks as a means to strictly enforce thermodynamic consistency, while allowing high expressivity towards model discovery from limited data. It utilizes state-of-the-art machine learning tools within PyTorch's high-performance library providing a flexible tool for data-driven, automated constitutive modeling. To test the performance of the framework, we generate synthetic stress-strain curves using a power law-based model with phenomenological hardening at small strains and test the trained model for strain amplitudes beyond the training data. Next, experimentally measured flow responses obtained from uniaxial deformations are used to train the framework under large plastic deformations. Ultimately, the Hall-Petch relationship corresponding to grain size strengthening is discovered by training flow response as a function of grain size, also leading to efficient extrapolation. The present work demonstrates a successful integration of neural networks into elasto-viscoplastic constitutive laws, providing a robust automated framework for constitutive model discovery that can efficiently generalize, while also providing insights into predictions of flow response and grain size-property relationships in metals and metallic alloys under large plastic deformations.","sentences":["We propose a physics informed, neural network-based elasto-viscoplasticity (NN-EVP) constitutive modeling framework for predicting the flow response in metals as a function of underlying grain size.","The developed NN-EVP algorithm is based on input convex neural networks as a means to strictly enforce thermodynamic consistency, while allowing high expressivity towards model discovery from limited data.","It utilizes state-of-the-art machine learning tools within PyTorch's high-performance library providing a flexible tool for data-driven, automated constitutive modeling.","To test the performance of the framework, we generate synthetic stress-strain curves using a power law-based model with phenomenological hardening at small strains and test the trained model for strain amplitudes beyond the training data.","Next, experimentally measured flow responses obtained from uniaxial deformations are used to train the framework under large plastic deformations.","Ultimately, the Hall-Petch relationship corresponding to grain size strengthening is discovered by training flow response as a function of grain size, also leading to efficient extrapolation.","The present work demonstrates a successful integration of neural networks into elasto-viscoplastic constitutive laws, providing a robust automated framework for constitutive model discovery that can efficiently generalize, while also providing insights into predictions of flow response and grain size-property relationships in metals and metallic alloys under large plastic deformations."],"url":"http://arxiv.org/abs/2307.04301v1"}
{"created":"2023-07-10 01:30:21","title":"Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection","abstract":"Recent studies show edge computing-based road anomaly detection systems which may also conduct data collection simultaneously. However, the edge computers will have small data storage but we need to store the collected audio samples for a long time in order to update existing models or develop a novel method. Therefore, we should consider an approach for efficient storage management methods while preserving high-fidelity audio. A hardware-perspective approach, such as using a low-resolution microphone, is an intuitive way to reduce file size but is not recommended because it fundamentally cuts off high-frequency components. On the other hand, a computational file compression approach that encodes collected high-resolution audio into a compact code should be recommended because it also provides a corresponding decoding method. Motivated by this, we propose a way of simple yet effective pre-trained autoencoder-based data compression method. The pre-trained autoencoder is trained for the purpose of audio super-resolution so it can be utilized to encode or decode any arbitrary sampling rate. Moreover, it will reduce the communication cost for data transmission from the edge to the central server. Via the comparative experiments, we confirm that the zero-shot audio compression and decompression highly preserve anomaly detection performance while enhancing storage and transmission efficiency.","sentences":["Recent studies show edge computing-based road anomaly detection systems which may also conduct data collection simultaneously.","However, the edge computers will have small data storage but we need to store the collected audio samples for a long time in order to update existing models or develop a novel method.","Therefore, we should consider an approach for efficient storage management methods while preserving high-fidelity audio.","A hardware-perspective approach, such as using a low-resolution microphone, is an intuitive way to reduce file size but is not recommended because it fundamentally cuts off high-frequency components.","On the other hand, a computational file compression approach that encodes collected high-resolution audio into a compact code should be recommended because it also provides a corresponding decoding method.","Motivated by this, we propose a way of simple yet effective pre-trained autoencoder-based data compression method.","The pre-trained autoencoder is trained for the purpose of audio super-resolution so it can be utilized to encode or decode any arbitrary sampling rate.","Moreover, it will reduce the communication cost for data transmission from the edge to the central server.","Via the comparative experiments, we confirm that the zero-shot audio compression and decompression highly preserve anomaly detection performance while enhancing storage and transmission efficiency."],"url":"http://arxiv.org/abs/2307.04298v1"}
{"created":"2023-07-10 00:52:29","title":"Wait, wasn't that code here before? Detecting Outdated Software Documentation","abstract":"Encountering outdated documentation is not a rare occurrence for developers and users in the software engineering community. To ensure that software documentation is up-to-date, developers often have to manually check whether the documentation needs to be updated whenever changes are made to the source code. In our previous work, we proposed an approach to automatically detect outdated code element references in software repositories and found that more than a quarter of the 1000 most popular projects on GitHub contained at least one outdated reference. In this paper, we present a GitHub Actions tool that builds on our previous work's approach that GitHub developers can configure to automatically scan for outdated code element references in their GitHub project's documentation whenever a pull request is submitted.","sentences":["Encountering outdated documentation is not a rare occurrence for developers and users in the software engineering community.","To ensure that software documentation is up-to-date, developers often have to manually check whether the documentation needs to be updated whenever changes are made to the source code.","In our previous work, we proposed an approach to automatically detect outdated code element references in software repositories and found that more than a quarter of the 1000 most popular projects on GitHub contained at least one outdated reference.","In this paper, we present a GitHub Actions tool that builds on our previous work's approach that GitHub developers can configure to automatically scan for outdated code element references in their GitHub project's documentation whenever a pull request is submitted."],"url":"http://arxiv.org/abs/2307.04291v1"}
{"created":"2023-07-10 00:29:25","title":"Generalizing Graph ODE for Learning Complex System Dynamics across Environments","abstract":"Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Networks (GNNs) to capture the continuous interaction among agents. We achieve the model generalization by assuming the dynamics across different environments are governed by common physics laws that can be captured via learning a shared ODE function. The distinct latent exogenous factors learned for each environment are incorporated into the ODE function to account for their differences. To improve model performance, we additionally design two regularization losses to (1) enforce the orthogonality between the learned initial states and exogenous factors via mutual information minimization; and (2) reduce the temporal variance of learned exogenous factors within the same system via contrastive learning. Experiments over various physical simulations show that our model can accurately predict system dynamics, especially in the long range, and can generalize well to new systems with few observations.","sentences":["Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology.","Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory.","In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity.","One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited.","Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments.","Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Networks (GNNs) to capture the continuous interaction among agents.","We achieve the model generalization by assuming the dynamics across different environments are governed by common physics laws that can be captured via learning a shared ODE function.","The distinct latent exogenous factors learned for each environment are incorporated into the ODE function to account for their differences.","To improve model performance, we additionally design two regularization losses to (1) enforce the orthogonality between the learned initial states and exogenous factors via mutual information minimization; and (2) reduce the temporal variance of learned exogenous factors within the same system via contrastive learning.","Experiments over various physical simulations show that our model can accurately predict system dynamics, especially in the long range, and can generalize well to new systems with few observations."],"url":"http://arxiv.org/abs/2307.04287v1"}
{"created":"2023-07-10 00:24:27","title":"HistRED: A Historical Document-Level Relation Extraction Dataset","abstract":"Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years. To promote the historical RE research, we present HistRED constructed from Yeonhaengnok. Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean. HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts. In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models. To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities. Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements the RE predictions. The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.","sentences":["Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years.","To promote the historical RE research, we present HistRED constructed from Yeonhaengnok.","Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean.","HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts.","In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models.","To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities.","Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements the RE predictions.","The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license."],"url":"http://arxiv.org/abs/2307.04285v1"}
{"created":"2023-07-10 00:20:45","title":"Effects of Network Connectivity and Diversity Distribution on Human Collective Ideation","abstract":"Human collectives, e.g., teams and organizations, increasingly require participation of members with diverse backgrounds working in networked social environments. However, little is known about how network structure and the diversity of member backgrounds would affect collective processes. Here we conducted three sets of human-subject experiments which involved 617 participants who collaborated anonymously in a collective ideation task on a custom-made online social network platform. We found that spatially clustered collectives with clustered background distribution tended to explore more diverse ideas than in other conditions, whereas collectives with random background distribution consistently generated ideas with the highest utility. We also found that higher network connectivity may improve individuals' overall experience but may not improve the collective performance regarding idea generation, idea diversity, and final idea quality.","sentences":["Human collectives, e.g., teams and organizations, increasingly require participation of members with diverse backgrounds working in networked social environments.","However, little is known about how network structure and the diversity of member backgrounds would affect collective processes.","Here we conducted three sets of human-subject experiments which involved 617 participants who collaborated anonymously in a collective ideation task on a custom-made online social network platform.","We found that spatially clustered collectives with clustered background distribution tended to explore more diverse ideas than in other conditions, whereas collectives with random background distribution consistently generated ideas with the highest utility.","We also found that higher network connectivity may improve individuals' overall experience but may not improve the collective performance regarding idea generation, idea diversity, and final idea quality."],"url":"http://arxiv.org/abs/2307.04284v1"}
{"created":"2023-07-09 23:12:08","title":"Shaping the Emerging Norms of Using Large Language Models in Social Computing Research","abstract":"The emergence of Large Language Models (LLMs) has brought both excitement and concerns to social computing research. On the one hand, LLMs offer unprecedented capabilities in analyzing vast amounts of textual data and generating human-like responses, enabling researchers to delve into complex social phenomena. On the other hand, concerns are emerging regarding the validity, privacy, and ethics of the research when LLMs are involved. This SIG aims at offering an open space for social computing researchers who are interested in understanding the impacts of LLMs to discuss their current practices, perspectives, challenges when engaging with LLMs in their everyday work and collectively shaping the emerging norms of using LLMs in social computing research.","sentences":["The emergence of Large Language Models (LLMs) has brought both excitement and concerns to social computing research.","On the one hand, LLMs offer unprecedented capabilities in analyzing vast amounts of textual data and generating human-like responses, enabling researchers to delve into complex social phenomena.","On the other hand, concerns are emerging regarding the validity, privacy, and ethics of the research when LLMs are involved.","This SIG aims at offering an open space for social computing researchers who are interested in understanding the impacts of LLMs to discuss their current practices, perspectives, challenges when engaging with LLMs in their everyday work and collectively shaping the emerging norms of using LLMs in social computing research."],"url":"http://arxiv.org/abs/2307.04280v1"}
{"created":"2023-07-09 23:02:19","title":"Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant","abstract":"Automated Essay scoring has been explored as a research and industry problem for over 50 years. It has drawn a lot of attention from the NLP community because of its clear educational value as a research area that can engender the creation of valuable time-saving tools for educators around the world. Yet, these tools are generally focused on detecting good grammar, spelling mistakes, and organization quality but tend to fail at incorporating persuasiveness features in their final assessment. The responsibility to give actionable feedback to the student to improve the strength of their arguments is left solely on the teacher's shoulders. In this work, we present a transformer-based architecture capable of achieving above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality and we expand on planned future work investigating the explainability of our model so that actionable feedback can be offered to the student and thus potentially enable a partnership between the teacher's advice and the machine's advice.","sentences":["Automated Essay scoring has been explored as a research and industry problem for over 50 years.","It has drawn a lot of attention from the NLP community because of its clear educational value as a research area that can engender the creation of valuable time-saving tools for educators around the world.","Yet, these tools are generally focused on detecting good grammar, spelling mistakes, and organization quality but tend to fail at incorporating persuasiveness features in their final assessment.","The responsibility to give actionable feedback to the student to improve the strength of their arguments is left solely on the teacher's shoulders.","In this work, we present a transformer-based architecture capable of achieving above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality and we expand on planned future work investigating the explainability of our model so that actionable feedback can be offered to the student and thus potentially enable a partnership between the teacher's advice and the machine's advice."],"url":"http://arxiv.org/abs/2307.04276v1"}
{"created":"2023-07-09 22:32:46","title":"Assessing the efficacy of large language models in generating accurate teacher responses","abstract":"(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.   We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.","sentences":["(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues.","Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher.","To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT.","Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning.","Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.   ","We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models.","Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills."],"url":"http://arxiv.org/abs/2307.04274v1"}
{"created":"2023-07-09 21:58:39","title":"A Complete Finite Equational Axiomatisation of the Fracterm Calculus for Common Meadows","abstract":"We analyse abstract data types that model numerical structures with a concept of error. Specifically, we focus on arithmetic data types that contain an error flag $\\bot$ whose main purpose is to always return a value for division. To rings and fields we add a division operator $x/y$ and study a class of algebras called \\textit{common meadows} wherein $x/0 = \\bot$. The set of equations true in all common meadows is named the \\textit{fracterm calculus of common meadows}. We give a finite equational axiomatisation of the fracterm calculus of common meadows and prove that it is complete and that the fracterm calculus is decidable.","sentences":["We analyse abstract data types that model numerical structures with a concept of error.","Specifically, we focus on arithmetic data types that contain an error flag $\\bot$ whose main purpose is to always return a value for division.","To rings and fields we add a division operator $x/y$ and study a class of algebras called \\textit{common meadows} wherein $x/0 = \\bot$.","The set of equations true in all common meadows is named the \\textit{fracterm calculus of common meadows}.","We give a finite equational axiomatisation of the fracterm calculus of common meadows and prove that it is complete and that the fracterm calculus is decidable."],"url":"http://arxiv.org/abs/2307.04270v1"}
{"created":"2023-07-09 20:47:42","title":"Thriving Innovation Ecosystems: Synergy Among Stakeholders, Tools, and People","abstract":"An innovation ecosystem is a multi-stakeholder environment, where different stakeholders interact to solve complex socio-technical challenges. We explored how stakeholders use digital tools, human resources, and their combination to gather information and make decisions in innovation ecosystems. To comprehensively understand stakeholders' motivations, information needs and practices, we conducted a three-part interview study across five stakeholder groups (N=13) using an interactive digital dashboard. We found that stakeholders were primarily motivated to participate in innovation ecosystems by the potential social impact of their contributions. We also found that stakeholders used digital tools to seek \"high-level\" information to scaffold initial decision-making efforts but ultimately relied on contextual information provided by human networks to enact final decisions. Therefore, people, not digital tools, appear to be the key source of information in these ecosystems. Guided by our findings, we explored how technology might nevertheless enhance stakeholders' decision-making efforts and enable robust and equitable innovation ecosystems.","sentences":["An innovation ecosystem is a multi-stakeholder environment, where different stakeholders interact to solve complex socio-technical challenges.","We explored how stakeholders use digital tools, human resources, and their combination to gather information and make decisions in innovation ecosystems.","To comprehensively understand stakeholders' motivations, information needs and practices, we conducted a three-part interview study across five stakeholder groups (N=13) using an interactive digital dashboard.","We found that stakeholders were primarily motivated to participate in innovation ecosystems by the potential social impact of their contributions.","We also found that stakeholders used digital tools to seek \"high-level\" information to scaffold initial decision-making efforts but ultimately relied on contextual information provided by human networks to enact final decisions.","Therefore, people, not digital tools, appear to be the key source of information in these ecosystems.","Guided by our findings, we explored how technology might nevertheless enhance stakeholders' decision-making efforts and enable robust and equitable innovation ecosystems."],"url":"http://arxiv.org/abs/2307.04263v1"}
{"created":"2023-07-09 20:29:02","title":"Design Space Exploration and Comparative Evaluation of Memory Technologies for Synaptic Crossbar Arrays: Device-Circuit Non-Idealities and System Accuracy","abstract":"In-memory computing (IMC) utilizing synaptic crossbar arrays is promising for deep neural networks to attain high energy efficiency and integration density. Towards that end, various CMOS and post-CMOS technologies have been explored as promising synaptic device candidates which include SRAM, ReRAM, FeFET, SOT-MRAM, etc. However, each of these technologies has its own pros and cons, which need to be comparatively evaluated in the context of synaptic array designs. For a fair comparison, such an analysis must carefully optimize each technology, specifically for synaptic crossbar design accounting for device and circuit non-idealities in crossbar arrays such as variations, wire resistance, driver/sink resistance, etc. In this work, we perform a comprehensive design space exploration and comparative evaluation of different technologies at 7nm technology node for synaptic crossbar arrays, in the context of IMC robustness and system accuracy. Firstly, we integrate different technologies into a cross-layer simulation flow based on physics-based models of synaptic devices and interconnects. Secondly, we optimize both technology-agnostic design knobs such as input encoding and ON-resistance as well as technology-specific design parameters including ferroelectric thickness in FeFET and MgO thickness in SOT-MRAM. Our optimization methodology accounts for the implications of device- and circuit-level non-idealities on the system-level accuracy for each technology. Finally, based on the optimized designs, we obtain inference results for ResNet-20 on CIFAR-10 dataset and show that FeFET-based crossbar arrays achieve the highest accuracy due to their compactness, low leakage and high ON/OFF current ratio.","sentences":["In-memory computing (IMC) utilizing synaptic crossbar arrays is promising for deep neural networks to attain high energy efficiency and integration density.","Towards that end, various CMOS and post-CMOS technologies have been explored as promising synaptic device candidates which include SRAM, ReRAM, FeFET, SOT-MRAM, etc.","However, each of these technologies has its own pros and cons, which need to be comparatively evaluated in the context of synaptic array designs.","For a fair comparison, such an analysis must carefully optimize each technology, specifically for synaptic crossbar design accounting for device and circuit non-idealities in crossbar arrays such as variations, wire resistance, driver/sink resistance, etc.","In this work, we perform a comprehensive design space exploration and comparative evaluation of different technologies at 7nm technology node for synaptic crossbar arrays, in the context of IMC robustness and system accuracy.","Firstly, we integrate different technologies into a cross-layer simulation flow based on physics-based models of synaptic devices and interconnects.","Secondly, we optimize both technology-agnostic design knobs such as input encoding and ON-resistance as well as technology-specific design parameters including ferroelectric thickness in FeFET and MgO thickness in SOT-MRAM.","Our optimization methodology accounts for the implications of device- and circuit-level non-idealities on the system-level accuracy for each technology.","Finally, based on the optimized designs, we obtain inference results for ResNet-20 on CIFAR-10 dataset and show that FeFET-based crossbar arrays achieve the highest accuracy due to their compactness, low leakage and high ON/OFF current ratio."],"url":"http://arxiv.org/abs/2307.04261v1"}
{"created":"2023-07-09 19:28:46","title":"ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey","abstract":"ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs, outputs, and effects. This paves the way for a comprehensive exploration of the technology and provides a road map for further research and experimentation. We also lay out essential foundational literature on LLMs and GAI in general and their connection with ChatGPT. This overview sheds light on existing and missing research lines in the emerging field of LLMs, benefiting both public users and developers. Furthermore, the paper delves into the broad spectrum of applications and significant concerns in fields such as education, research, healthcare, finance, etc.","sentences":["ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data.","It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities.","ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale.","It has also sparked research interest in developing similar technologies and investigating their applications and implications.","In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution.","We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications.","The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs, outputs, and effects.","This paves the way for a comprehensive exploration of the technology and provides a road map for further research and experimentation.","We also lay out essential foundational literature on LLMs and GAI in general and their connection with ChatGPT.","This overview sheds light on existing and missing research lines in the emerging field of LLMs, benefiting both public users and developers.","Furthermore, the paper delves into the broad spectrum of applications and significant concerns in fields such as education, research, healthcare, finance, etc."],"url":"http://arxiv.org/abs/2307.04251v1"}
{"created":"2023-07-09 19:07:00","title":"Private Data Stream Analysis for Universal Symmetric Norm Estimation","abstract":"We study how to release summary statistics on a data stream subject to the constraint of differential privacy. In particular, we focus on releasing the family of symmetric norms, which are invariant under sign-flips and coordinate-wise permutations on an input data stream and include $L_p$ norms, $k$-support norms, top-$k$ norms, and the box norm as special cases. Although it may be possible to design and analyze a separate mechanism for each symmetric norm, we propose a general parametrizable framework that differentially privately releases a number of sufficient statistics from which the approximation of all symmetric norms can be simultaneously computed. Our framework partitions the coordinates of the underlying frequency vector into different levels based on their magnitude and releases approximate frequencies for the \"heavy\" coordinates in important levels and releases approximate level sizes for the \"light\" coordinates in important levels. Surprisingly, our mechanism allows for the release of an arbitrary number of symmetric norm approximations without any overhead or additional loss in privacy. Moreover, our mechanism permits $(1+\\alpha)$-approximation to each of the symmetric norms and can be implemented using sublinear space in the streaming model for many regimes of the accuracy and privacy parameters.","sentences":["We study how to release summary statistics on a data stream subject to the constraint of differential privacy.","In particular, we focus on releasing the family of symmetric norms, which are invariant under sign-flips and coordinate-wise permutations on an input data stream and include $L_p$ norms, $k$-support norms, top-$k$ norms, and the box norm as special cases.","Although it may be possible to design and analyze a separate mechanism for each symmetric norm, we propose a general parametrizable framework that differentially privately releases a number of sufficient statistics from which the approximation of all symmetric norms can be simultaneously computed.","Our framework partitions the coordinates of the underlying frequency vector into different levels based on their magnitude and releases approximate frequencies for the \"heavy\" coordinates in important levels and releases approximate level sizes for the \"light\" coordinates in important levels.","Surprisingly, our mechanism allows for the release of an arbitrary number of symmetric norm approximations without any overhead or additional loss in privacy.","Moreover, our mechanism permits $(1+\\alpha)$-approximation to each of the symmetric norms and can be implemented using sublinear space in the streaming model for many regimes of the accuracy and privacy parameters."],"url":"http://arxiv.org/abs/2307.04249v1"}
{"created":"2023-07-09 18:53:38","title":"VR Job Interview Using a Gender-Swapped Avatar","abstract":"Virtual Reality (VR) has emerged as a potential solution for mitigating bias in a job interview by hiding the applicants' demographic features. The current study examines the use of a gender-swapped avatar in a virtual job interview that affects the applicants' perceptions and their performance evaluated by recruiters. With a mixed-method approach, we first conducted a lab experiment (N=8) exploring how using a gender-swapped avatar in a virtual job interview impacts perceived anxiety, confidence, competence, and ability to perform. Then, a semi-structured interview investigated the participants' VR interview experiences using an avatar. Our findings suggest that using gender-swapped avatars may reduce the anxiety that job applicants will experience during the interview. Also, the affinity diagram produced seven key themes highlighting the advantages and limitations of VR as an interview platform. These findings contribute to the emerging field of VR-based recruitment and have practical implications for promoting diversity and inclusion in the hiring process.","sentences":["Virtual Reality (VR) has emerged as a potential solution for mitigating bias in a job interview by hiding the applicants' demographic features.","The current study examines the use of a gender-swapped avatar in a virtual job interview that affects the applicants' perceptions and their performance evaluated by recruiters.","With a mixed-method approach, we first conducted a lab experiment (N=8) exploring how using a gender-swapped avatar in a virtual job interview impacts perceived anxiety, confidence, competence, and ability to perform.","Then, a semi-structured interview investigated the participants' VR interview experiences using an avatar.","Our findings suggest that using gender-swapped avatars may reduce the anxiety that job applicants will experience during the interview.","Also, the affinity diagram produced seven key themes highlighting the advantages and limitations of VR as an interview platform.","These findings contribute to the emerging field of VR-based recruitment and have practical implications for promoting diversity and inclusion in the hiring process."],"url":"http://arxiv.org/abs/2307.04247v1"}
{"created":"2023-07-09 18:52:01","title":"Convex Decomposition of Indoor Scenes","abstract":"We describe a method to parse a complex, cluttered indoor scene into primitives which offer a parsimonious abstraction of scene structure. Our primitives are simple convexes. Our method uses a learned regression procedure to parse a scene into a fixed number of convexes from RGBD input, and can optionally accept segmentations to improve the decomposition. The result is then polished with a descent method which adjusts the convexes to produce a very good fit, and greedily removes superfluous primitives. Because the entire scene is parsed, we can evaluate using traditional depth, normal, and segmentation error metrics. Our evaluation procedure demonstrates that the error from our primitive representation is comparable to that of predicting depth from a single image.","sentences":["We describe a method to parse a complex, cluttered indoor scene into primitives which offer a parsimonious abstraction of scene structure.","Our primitives are simple convexes.","Our method uses a learned regression procedure to parse a scene into a fixed number of convexes from RGBD input, and can optionally accept segmentations to improve the decomposition.","The result is then polished with a descent method which adjusts the convexes to produce a very good fit, and greedily removes superfluous primitives.","Because the entire scene is parsed, we can evaluate using traditional depth, normal, and segmentation error metrics.","Our evaluation procedure demonstrates that the error from our primitive representation is comparable to that of predicting depth from a single image."],"url":"http://arxiv.org/abs/2307.04246v1"}
{"created":"2023-07-09 18:51:17","title":"A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing","abstract":"Optical Character Recognition (OCR) technology finds applications in digitizing books and unstructured documents, along with applications in other domains such as mobility statistics, law enforcement, traffic, security systems, etc. The state-of-the-art methods work well with the OCR with printed text on license plates, shop names, etc. However, applications such as printed textbooks and handwritten texts have limited accuracy with existing techniques. The reason may be attributed to similar-looking characters and variations in handwritten characters. Since these issues are challenging to address with OCR technologies exclusively, we propose a post-processing approach using Natural Language Processing (NLP) tools. This work presents an end-to-end pipeline that first performs OCR on the handwritten or printed text and then improves its accuracy using NLP.","sentences":["Optical Character Recognition (OCR) technology finds applications in digitizing books and unstructured documents, along with applications in other domains such as mobility statistics, law enforcement, traffic, security systems, etc.","The state-of-the-art methods work well with the OCR with printed text on license plates, shop names, etc.","However, applications such as printed textbooks and handwritten texts have limited accuracy with existing techniques.","The reason may be attributed to similar-looking characters and variations in handwritten characters.","Since these issues are challenging to address with OCR technologies exclusively, we propose a post-processing approach using Natural Language Processing (NLP) tools.","This work presents an end-to-end pipeline that first performs OCR on the handwritten or printed text and then improves its accuracy using NLP."],"url":"http://arxiv.org/abs/2307.04245v1"}
