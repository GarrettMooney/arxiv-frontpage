{"created":"2023-07-13 17:59:47","title":"HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models","abstract":"Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10000x smaller than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io","sentences":["Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities.","However, the process of personalization presents inherent challenges in terms of time and memory requirements.","Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity.","To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person.","By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications.","Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth.","Also our method yields a model that is 10000x smaller than a normal DreamBooth model.","Project page: https://hyperdreambooth.github.io"],"url":"http://arxiv.org/abs/2307.06949v1"}
{"created":"2023-07-13 17:59:35","title":"Self-regulating Prompts: Foundational Model Adaptation without Forgetting","abstract":"Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks. Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP. This leads to the loss of the model's original generalization capability. To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating {prompted} representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch. To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity. PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization. We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods. Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC.","sentences":["Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks.","Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP.","This leads to the loss of the model's original generalization capability.","To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints).","PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating {prompted} representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch.","To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity.","PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization.","We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods.","Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC."],"url":"http://arxiv.org/abs/2307.06948v1"}
{"created":"2023-07-13 17:59:33","title":"Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition","abstract":"Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modulation-based spatio-temporal context modeling and demonstrate our parallel spatial and temporal encoding design to be the optimal choice. Video-FocalNets perform favorably well against the state-of-the-art transformer-based models for video recognition on three large-scale datasets (Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our code/models are released at https://github.com/TalalWasim/Video-FocalNets.","sentences":["Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling.","Video transformer designs are based on self-attention that can model global context at a high computational cost.","In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling.","Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts.","Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency.","Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations.","We extensively explore the design space of focal modulation-based spatio-temporal context modeling and demonstrate our parallel spatial and temporal encoding design to be the optimal choice.","Video-FocalNets perform favorably well against the state-of-the-art transformer-based models for video recognition on three large-scale datasets (Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost.","Our code/models are released at https://github.com/TalalWasim/Video-FocalNets."],"url":"http://arxiv.org/abs/2307.06947v1"}
{"created":"2023-07-13 17:59:21","title":"In-context Autoencoder for Context Compression in a Large Language Model","abstract":"We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM. Our code and data will be released shortly.","sentences":["We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM).","The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes.","We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context.","Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses.","Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts.","The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM.","Our code and data will be released shortly."],"url":"http://arxiv.org/abs/2307.06945v1"}
{"created":"2023-07-13 17:58:32","title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation","abstract":"This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.","sentences":["This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation.","The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.","Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale.","Specifically, we utilize a multi-scale approach to generate video-related descriptions.","Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance.","Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications.","They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research.","These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation."],"url":"http://arxiv.org/abs/2307.06942v1"}
{"created":"2023-07-13 17:57:21","title":"On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations","abstract":"Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantitatively show the difference in explanations at every stage of the connection between the two approaches and corroborate the theoretical findings.","sentences":["Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations.","These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical.","This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations.","After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent.","We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values.","Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances.","Experiments on three datasets quantitatively show the difference in explanations at every stage of the connection between the two approaches and corroborate the theoretical findings."],"url":"http://arxiv.org/abs/2307.06941v1"}
{"created":"2023-07-13 17:57:13","title":"Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation","abstract":"Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering. To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances. We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts. For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure. For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters. The videos are synthesized by following the structural guidance and appearance instruction. To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts. Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines.","sentences":["Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering.","To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances.","We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts.","For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure.","For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters.","The videos are synthesized by following the structural guidance and appearance instruction.","To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts.","Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines."],"url":"http://arxiv.org/abs/2307.06940v1"}
{"created":"2023-07-13 17:53:25","title":"PHOENI2X -- A European Cyber Resilience Framework With Artificial-Intelligence-Assisted Orchestration, Automation and Response Capabilities for Business Continuity and Recovery, Incident Response, and Information Exchange","abstract":"As digital technologies become more pervasive in society and the economy, cybersecurity incidents become more frequent and impactful. According to the NIS and NIS2 Directives, EU Member States and their Operators of Essential Services must establish a minimum baseline set of cybersecurity capabilities and engage in cross-border coordination and cooperation. However, this is only a small step towards European cyber resilience. In this landscape, preparedness, shared situational awareness, and coordinated incident response are essential for effective cyber crisis management and resilience. Motivated by the above, this paper presents PHOENI2X, an EU-funded project aiming to design, develop, and deliver a Cyber Resilience Framework providing Artificial-Intelligence-assisted orchestration, automation and response capabilities for business continuity and recovery, incident response, and information exchange, tailored to the needs of Operators of Essential Services and the EU Member State authorities entrusted with cybersecurity.","sentences":["As digital technologies become more pervasive in society and the economy, cybersecurity incidents become more frequent and impactful.","According to the NIS and NIS2 Directives, EU Member States and their Operators of Essential Services must establish a minimum baseline set of cybersecurity capabilities and engage in cross-border coordination and cooperation.","However, this is only a small step towards European cyber resilience.","In this landscape, preparedness, shared situational awareness, and coordinated incident response are essential for effective cyber crisis management and resilience.","Motivated by the above, this paper presents PHOENI2X, an EU-funded project aiming to design, develop, and deliver a Cyber Resilience Framework providing Artificial-Intelligence-assisted orchestration, automation and response capabilities for business continuity and recovery, incident response, and information exchange, tailored to the needs of Operators of Essential Services and the EU Member State authorities entrusted with cybersecurity."],"url":"http://arxiv.org/abs/2307.06932v1"}
{"created":"2023-07-13 17:51:58","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs","abstract":"Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder. With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by leveraging a pretrained multilingual LLM. To this end, we \\textit{re-align} an image encoder previously tuned to an English LLM to a new, multilingual LLM -- for this, we leverage multilingual data from a mix of vision-and-language tasks, which we obtain by machine-translating high-quality English data to 95 languages. On the IGLUE benchmark, mBLIP yields results competitive with state-of-the-art models. Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared to these very large multilingual vision-language models trained from scratch, we obtain mBLIP by training orders of magnitude fewer parameters on magnitudes less data. We release our model and code at \\url{https://github.com/gregor-ge/mBLIP}.","sentences":["Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most.","Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder.","With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs.","Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora.","In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by leveraging a pretrained multilingual LLM.","To this end, we \\textit{re-align} an image encoder previously tuned to an English LLM to a new, multilingual LLM -- for this, we leverage multilingual data from a mix of vision-and-language tasks, which we obtain by machine-translating high-quality English data to 95 languages.","On the IGLUE benchmark, mBLIP yields results competitive with state-of-the-art models.","Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms PaLI-X (a model with 55B parameters).","Compared to these very large multilingual vision-language models trained from scratch, we obtain mBLIP by training orders of magnitude fewer parameters on magnitudes less data.","We release our model and code at \\url{https://github.com/gregor-ge/mBLIP}."],"url":"http://arxiv.org/abs/2307.06930v1"}
{"created":"2023-07-13 17:51:04","title":"Ill-Typed Programs Don't Evaluate","abstract":"We introduce two-sided type systems, which are a particular kind of sequent calculi for typing formulas. Two-sided type systems allow for hypothetical reasoning over the typing of compound program expressions, and the refutation of typing formulas. By incorporating a type of all values, these type systems support symmetrical notions of well-typing and ill-typing, guaranteeing both that well-typed programs don't go wrong and that ill-typed programs do not evaluate - that is, reach a value. This makes two-sided type systems suitable for incorrectness reasoning in higher-order program verification, which we illustrate through an application to precise data-flow typing in a language with constructors and pattern matching. Finally, we investigate the internalisation of the meta-level negation in the system as a complement operator on types. This motivates an alternative semantics for the typing judgement, which guarantees that ill-typed programs don't evaluate, but in which well-typed programs may yet go wrong.","sentences":["We introduce two-sided type systems, which are a particular kind of sequent calculi for typing formulas.","Two-sided type systems allow for hypothetical reasoning over the typing of compound program expressions, and the refutation of typing formulas.","By incorporating a type of all values, these type systems support symmetrical notions of well-typing and ill-typing, guaranteeing both that well-typed programs don't go wrong and that ill-typed programs do not evaluate - that is, reach a value.","This makes two-sided type systems suitable for incorrectness reasoning in higher-order program verification, which we illustrate through an application to precise data-flow typing in a language with constructors and pattern matching.","Finally, we investigate the internalisation of the meta-level negation in the system as a complement operator on types.","This motivates an alternative semantics for the typing judgement, which guarantees that ill-typed programs don't evaluate, but in which well-typed programs may yet go wrong."],"url":"http://arxiv.org/abs/2307.06928v1"}
{"created":"2023-07-13 17:46:42","title":"Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models","abstract":"Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts. Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times. However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts. In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models. This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods.","sentences":["Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts.","Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times.","However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts.","In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts.","We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens.","Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models.","This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods."],"url":"http://arxiv.org/abs/2307.06925v1"}
{"created":"2023-07-13 17:46:15","title":"DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding","abstract":"Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, provide a good guiding experience, and connect users with their surrounding environment in an intuitive manner.","sentences":["Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them.","Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment.","Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language.","By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations.","Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language.","We conduct a user study with blindfolded participants in an everyday indoor environment.","Our results demonstrate that DRAGON is able to communicate with the user smoothly, provide a good guiding experience, and connect users with their surrounding environment in an intuitive manner."],"url":"http://arxiv.org/abs/2307.06924v1"}
{"created":"2023-07-13 17:43:12","title":"Crucible: Graphical Test Cases for Alloy Models","abstract":"Alloy is a declarative modeling language that is well suited for verifying system designs. Alloy models are automatically analyzed using the Analyzer, a toolset that helps the user understand their system by displaying the consequences of their properties, helping identify any missing or incorrect properties, and exploring the impact of modifications to those properties. To achieve this, the Analyzer invokes off-the-shelf SAT solvers to search for scenarios, which are assignments to the sets and relations of the model such that all executed formulas hold. To help write more accurate software models, Alloy has a unit testing framework, AUnit, which allows users to outline specific scenarios and check if those scenarios are correctly generated or prevented by their model. Unfortunately, AUnit currently only supports textual specifications of scenarios. This paper introduces Crucible, which allows users to graphically create AUnit test cases. In addition, Crucible provides automated guidance to users to ensure they are creating well structured, valuable test cases. As a result, Crucible eases the burden of adopting AUnit and brings AUnit test case creation more in line with how Alloy scenarios are commonly interacted with, which is graphically.","sentences":["Alloy is a declarative modeling language that is well suited for verifying system designs.","Alloy models are automatically analyzed using the Analyzer, a toolset that helps the user understand their system by displaying the consequences of their properties, helping identify any missing or incorrect properties, and exploring the impact of modifications to those properties.","To achieve this, the Analyzer invokes off-the-shelf SAT solvers to search for scenarios, which are assignments to the sets and relations of the model such that all executed formulas hold.","To help write more accurate software models, Alloy has a unit testing framework, AUnit, which allows users to outline specific scenarios and check if those scenarios are correctly generated or prevented by their model.","Unfortunately, AUnit currently only supports textual specifications of scenarios.","This paper introduces Crucible, which allows users to graphically create AUnit test cases.","In addition, Crucible provides automated guidance to users to ensure they are creating well structured, valuable test cases.","As a result, Crucible eases the burden of adopting AUnit and brings AUnit test case creation more in line with how Alloy scenarios are commonly interacted with, which is graphically."],"url":"http://arxiv.org/abs/2307.06922v1"}
{"created":"2023-07-13 17:42:02","title":"Targeting Completeness: Using Closed Forms for Size Bounds of Integer Programs","abstract":"We present a new procedure to infer size bounds for integer programs automatically. Size bounds are important for the deduction of bounds on the runtime complexity or in general, for the resource analysis of programs. We show that our technique is complete (i.e., it always computes finite size bounds) for a subclass of loops, possibly with non-linear arithmetic. Moreover, we present a novel approach to combine and integrate this complete technique into an incomplete approach to infer size and runtime bounds of general integer programs. We prove completeness of our integration for an important subclass of integer programs. We implemented our new algorithm in the automated complexity analysis tool KoAT to evaluate its power, in particular on programs with non-linear arithmetic.","sentences":["We present a new procedure to infer size bounds for integer programs automatically.","Size bounds are important for the deduction of bounds on the runtime complexity or in general, for the resource analysis of programs.","We show that our technique is complete (i.e., it always computes finite size bounds) for a subclass of loops, possibly with non-linear arithmetic.","Moreover, we present a novel approach to combine and integrate this complete technique into an incomplete approach to infer size and runtime bounds of general integer programs.","We prove completeness of our integration for an important subclass of integer programs.","We implemented our new algorithm in the automated complexity analysis tool KoAT to evaluate its power, in particular on programs with non-linear arithmetic."],"url":"http://arxiv.org/abs/2307.06921v1"}
{"created":"2023-07-13 17:40:30","title":"DAXiot: A Decentralized Authentication and Authorization Scheme for Dynamic IoT Networks","abstract":"Federated and decentralized networks supporting frequently changing system participants are a requirement for future Internet of Things (IoT) use cases. IoT devices and networks often lack adequate authentication and authorization mechanisms, resulting in insufficient privacy for entities in such systems. In this work we address both issues by designing a privacy preserving challenge-response style authentication and authorization scheme based on Decentralized Identifiers and Verifiable Credentials. Our solution allows a decentralized permission management of frequently changing network participants and supports authenticated encryption for data confidentiality. We demonstrate our solution in an MQTT 5.0 scenario and evaluate its security, privacy guarantees, and performance.","sentences":["Federated and decentralized networks supporting frequently changing system participants are a requirement for future Internet of Things (IoT) use cases.","IoT devices and networks often lack adequate authentication and authorization mechanisms, resulting in insufficient privacy for entities in such systems.","In this work we address both issues by designing a privacy preserving challenge-response style authentication and authorization scheme based on Decentralized Identifiers and Verifiable Credentials.","Our solution allows a decentralized permission management of frequently changing network participants and supports authenticated encryption for data confidentiality.","We demonstrate our solution in an MQTT 5.0 scenario and evaluate its security, privacy guarantees, and performance."],"url":"http://arxiv.org/abs/2307.06919v1"}
{"created":"2023-07-13 17:31:41","title":"LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT","abstract":"Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. KGs surpass any other form of representation in terms of effectiveness. However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. It also demands a significant amount of work. Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE. In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs.","sentences":["Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines.","KGs surpass any other form of representation in terms of effectiveness.","However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices.","It also demands a significant amount of work.","Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE.","In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs."],"url":"http://arxiv.org/abs/2307.06917v1"}
{"created":"2023-07-13 17:21:54","title":"Uncovering Unique Concept Vectors through Latent Space Decomposition","abstract":"Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors. This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data.","sentences":["Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety.","Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency.","However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts.","To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training.","By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts.","Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand.","Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors.","This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data."],"url":"http://arxiv.org/abs/2307.06913v1"}
{"created":"2023-07-13 17:20:36","title":"BittyBuzz: A Swarm Robotics Runtime for Tiny Systems","abstract":"Swarm robotics is an emerging field of research which is increasingly attracting attention thanks to the advances in robotics and its potential applications. However, despite the enthusiasm surrounding this area of research, software development for swarm robotics is still a tedious task. That fact is partly due to the lack of dedicated solutions, in particular for low-cost systems to be produced in large numbers and that can have important resource constraints. To address this issue, we introduce BittyBuzz, a novel runtime platform: it allows Buzz, a domain-specific language, to run on microcontrollers while maintaining dynamic memory management. BittyBuzz is designed to fit a flash memory as small as 32 kB (with usable space for scripts) and work with as little as 2 kB of RAM. In this work, we introduce the BittyBuzz implementation, its differences from the original Buzz virtual machine, and its advantages for swarm robotics systems. We show that BittyBuzz is successfully integrated with three robotic platforms with minimal memory footprint and conduct experiments to show computation performance of BittyBuzz. Results show that BittyBuzz can be effectively used to implement common swarm behaviors on microcontroller-based systems.","sentences":["Swarm robotics is an emerging field of research which is increasingly attracting attention thanks to the advances in robotics and its potential applications.","However, despite the enthusiasm surrounding this area of research, software development for swarm robotics is still a tedious task.","That fact is partly due to the lack of dedicated solutions, in particular for low-cost systems to be produced in large numbers and that can have important resource constraints.","To address this issue, we introduce BittyBuzz, a novel runtime platform: it allows Buzz, a domain-specific language, to run on microcontrollers while maintaining dynamic memory management.","BittyBuzz is designed to fit a flash memory as small as 32 kB (with usable space for scripts) and work with as little as 2 kB of RAM.","In this work, we introduce the BittyBuzz implementation, its differences from the original Buzz virtual machine, and its advantages for swarm robotics systems.","We show that BittyBuzz is successfully integrated with three robotic platforms with minimal memory footprint and conduct experiments to show computation performance of BittyBuzz.","Results show that BittyBuzz can be effectively used to implement common swarm behaviors on microcontroller-based systems."],"url":"http://arxiv.org/abs/2307.06912v1"}
{"created":"2023-07-13 17:14:38","title":"Generating Benchmarks for Factuality Evaluation of Language Models","abstract":"Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators. We make our data and code publicly available in https://github.com/AI21Labs/factor.","sentences":["Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain.","Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts.","We propose FACTOR:","Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality.","FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements.","We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR.","We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.","We make our data and code publicly available in https://github.com/AI21Labs/factor."],"url":"http://arxiv.org/abs/2307.06908v1"}
{"created":"2023-07-13 17:05:49","title":"Trajectory-Aware Rate Adaptation for Flying Networks","abstract":"Despite the trend towards ubiquitous wireless connectivity, there are scenarios where the communications infrastructure is damaged and wireless coverage is insufficient or does not exist, such as in natural disasters and temporary crowded events. Flying networks, composed of Unmanned Aerial Vehicles (UAV), have emerged as a flexible and cost-effective solution to provide on-demand wireless connectivity in these scenarios. UAVs have the capability to operate virtually everywhere, and the growing payload capacity makes them suitable platforms to carry wireless communications hardware. The state of the art in the field of flying networks is mainly focused on the optimal positioning of the flying nodes, while the wireless link parameters are configured with default values. On the other hand, current link adaptation algorithms are mainly targeting fixed or low mobility scenarios.   We propose a novel rate adaptation approach for flying networks, named Trajectory Aware Rate Adaptation (TARA), which leverages the knowledge of flying nodes' movement to predict future channel conditions and perform rate adaptation accordingly. Simulation results of 100 different trajectories show that our solution increases throughput by up to 53% and achieves an average improvement of 14%, when compared with conventional rate adaptation algorithms such as Minstrel-HT.","sentences":["Despite the trend towards ubiquitous wireless connectivity, there are scenarios where the communications infrastructure is damaged and wireless coverage is insufficient or does not exist, such as in natural disasters and temporary crowded events.","Flying networks, composed of Unmanned Aerial Vehicles (UAV), have emerged as a flexible and cost-effective solution to provide on-demand wireless connectivity in these scenarios.","UAVs have the capability to operate virtually everywhere, and the growing payload capacity makes them suitable platforms to carry wireless communications hardware.","The state of the art in the field of flying networks is mainly focused on the optimal positioning of the flying nodes, while the wireless link parameters are configured with default values.","On the other hand, current link adaptation algorithms are mainly targeting fixed or low mobility scenarios.   ","We propose a novel rate adaptation approach for flying networks, named Trajectory Aware Rate Adaptation (TARA), which leverages the knowledge of flying nodes' movement to predict future channel conditions and perform rate adaptation accordingly.","Simulation results of 100 different trajectories show that our solution increases throughput by up to 53% and achieves an average improvement of 14%, when compared with conventional rate adaptation algorithms such as Minstrel-HT."],"url":"http://arxiv.org/abs/2307.06905v1"}
{"created":"2023-07-13 16:50:38","title":"Words are not Wind -- How Joint Commitment and Reputation Solve Social Dilemmas, without Repeated Interactions or Enforcement by Third Parties","abstract":"Joint commitment was argued to \"make our social world\" (Gilbert, 2014) and to separate us from other primates. 'Joint' entails that neither of us promises anything, unless the other promises as well. When we need to coordinate for the best mutual outcome, any commitment is beneficial. However, when we are tempted to free-ride (i.e. in social dilemmas), commitment serves no obvious purpose. We show that a reputation system, which judges action in social dilemmas only after joint commitment, can prevent free-riding. Keeping commitments builds trust. We can selectively enter joint commitments with trustworthy individuals to ensure their cooperation (since they will now be judged). We simply do not commit to cooperate with those we do not trust, and hence can freely defect without losing the trust of others. This principle might be the reason for pointedly public joint commitments, such as marriage. It is especially relevant to our evolutionary past, in which no mechanisms existed to enforce commitments reliably and impartially (e.g. via a powerful and accountable government). Much research from anthropology, philosophy and psychology made the assumption that past collaborations were mutually beneficial and had little possibilities to free-ride, for which there is little support. Our evolutionary game theory approach proves that this assumption is not necessary, because free-riding could have been dealt with joint commitments and reputation.","sentences":["Joint commitment was argued to \"make our social world\" (Gilbert, 2014) and to separate us from other primates.","'Joint' entails that neither of us promises anything, unless the other promises as well.","When we need to coordinate for the best mutual outcome, any commitment is beneficial.","However, when we are tempted to free-ride (i.e. in social dilemmas), commitment serves no obvious purpose.","We show that a reputation system, which judges action in social dilemmas only after joint commitment, can prevent free-riding.","Keeping commitments builds trust.","We can selectively enter joint commitments with trustworthy individuals to ensure their cooperation (since they will now be judged).","We simply do not commit to cooperate with those we do not trust, and hence can freely defect without losing the trust of others.","This principle might be the reason for pointedly public joint commitments, such as marriage.","It is especially relevant to our evolutionary past, in which no mechanisms existed to enforce commitments reliably and impartially (e.g. via a powerful and accountable government).","Much research from anthropology, philosophy and psychology made the assumption that past collaborations were mutually beneficial and had little possibilities to free-ride, for which there is little support.","Our evolutionary game theory approach proves that this assumption is not necessary, because free-riding could have been dealt with joint commitments and reputation."],"url":"http://arxiv.org/abs/2307.06898v1"}
{"created":"2023-07-13 16:49:07","title":"Proof Systems for the Modal $\u03bc$-Calculus Obtained by Determinizing Automata","abstract":"Automata operating on infinite objects feature prominently in the theory of the modal $\\mu$-calculus. One such application concerns the tableau games introduced by Niwi\\'{n}ski & Walukiewicz, of which the winning condition for infinite plays can be naturally checked by a nondeterministic parity stream automaton. Inspired by work of Jungteerapanich and Stirling we show how determinization constructions of this automaton may be used to directly obtain proof systems for the $\\mu$-calculus. More concretely, we introduce a binary tree construction for determinizing nondeterministic parity stream automata. Using this construction we define the annotated cyclic proof system $\\mathsf{BT}$, where formulas are annotated by tuples of binary strings. Soundness and Completeness of this system follow almost immediately from the correctness of the determinization method.","sentences":["Automata operating on infinite objects feature prominently in the theory of the modal $\\mu$-calculus.","One such application concerns the tableau games introduced by Niwi\\'{n}ski & Walukiewicz, of which the winning condition for infinite plays can be naturally checked by a nondeterministic parity stream automaton.","Inspired by work of Jungteerapanich and Stirling we show how determinization constructions of this automaton may be used to directly obtain proof systems for the $\\mu$-calculus.","More concretely, we introduce a binary tree construction for determinizing nondeterministic parity stream automata.","Using this construction we define the annotated cyclic proof system $\\mathsf{BT}$, where formulas are annotated by tuples of binary strings.","Soundness and Completeness of this system follow almost immediately from the correctness of the determinization method."],"url":"http://arxiv.org/abs/2307.06897v1"}
{"created":"2023-07-13 16:46:26","title":"Automatic Routing System for Intelligent Warehouses","abstract":"Automation of logistic processes is essential to improve productivity and reduce costs. In this context, intelligent warehouses are becoming a key to logistic systems thanks to their ability of optimizing transportation tasks and, consequently, reducing costs. This paper initially presents briefly routing systems applied on intelligent warehouses. Then, we present the approach used to develop our router system. This router system is able to solve traffic jams and collisions, generate conflict-free and optimized paths before sending the final paths to the robotic forklifts. It also verifies the progress of all tasks. When a problem occurs, the router system can change the task priorities, routes, etc. in order to avoid new conflicts. In the routing simulations, each vehicle executes its tasks starting from a predefined initial pose, moving to the desired position. Our algorithm is based on Dijkstra's shortest path and the time window approaches and it was implemented in C language. Computer simulation tests were used to validate the algorithm efficiency under different working conditions. Several simulations were carried out using the Player/Stage Simulator to test the algorithms. Thanks to the simulations, we could solve many faults and refine the algorithms before embedding them in real robots.","sentences":["Automation of logistic processes is essential to improve productivity and reduce costs.","In this context, intelligent warehouses are becoming a key to logistic systems thanks to their ability of optimizing transportation tasks and, consequently, reducing costs.","This paper initially presents briefly routing systems applied on intelligent warehouses.","Then, we present the approach used to develop our router system.","This router system is able to solve traffic jams and collisions, generate conflict-free and optimized paths before sending the final paths to the robotic forklifts.","It also verifies the progress of all tasks.","When a problem occurs, the router system can change the task priorities, routes, etc. in order to avoid new conflicts.","In the routing simulations, each vehicle executes its tasks starting from a predefined initial pose, moving to the desired position.","Our algorithm is based on Dijkstra's shortest path and the time window approaches and it was implemented in C language.","Computer simulation tests were used to validate the algorithm efficiency under different working conditions.","Several simulations were carried out using the Player/Stage Simulator to test the algorithms.","Thanks to the simulations, we could solve many faults and refine the algorithms before embedding them in real robots."],"url":"http://arxiv.org/abs/2307.06893v1"}
{"created":"2023-07-13 16:40:12","title":"Controlling Epidemic Spread Under Immunization Delay Constraints","abstract":"In this paper, we study the problem of minimizing the spread of a viral epidemic when immunization takes a non-negligible amount of time to take into effect. Specifically, our problem is to determine which set of nodes to be vaccinated when vaccines take a random amount of time in order to maximize the total reward, which is the expected number of saved nodes. We first provide a mathematical analysis for the reward function of vaccinating an arbitrary number of nodes when there is a single source of infection. While it is infeasible to obtain the optimal solution analytically due to the combinatorial nature of the problem, we establish that the problem is a monotone submodular maximization problem and develop a greedy algorithm that achieves a $(1\\!-\\!1/e)$-approximation. We further extend the scenario to the ones with multiple infection sources and discuss how the greedy algorithm can be applied systematically for the multiple-source scenarios. We finally present extensive simulation results to demonstrate the superiority of our greedy algorithm over other baseline vaccination strategies.","sentences":["In this paper, we study the problem of minimizing the spread of a viral epidemic when immunization takes a non-negligible amount of time to take into effect.","Specifically, our problem is to determine which set of nodes to be vaccinated when vaccines take a random amount of time in order to maximize the total reward, which is the expected number of saved nodes.","We first provide a mathematical analysis for the reward function of vaccinating an arbitrary number of nodes when there is a single source of infection.","While it is infeasible to obtain the optimal solution analytically due to the combinatorial nature of the problem, we establish that the problem is a monotone submodular maximization problem and develop a greedy algorithm that achieves a $(1\\!-\\!1/e)$-approximation.","We further extend the scenario to the ones with multiple infection sources and discuss how the greedy algorithm can be applied systematically for the multiple-source scenarios.","We finally present extensive simulation results to demonstrate the superiority of our greedy algorithm over other baseline vaccination strategies."],"url":"http://arxiv.org/abs/2307.06889v1"}
{"created":"2023-07-13 16:39:08","title":"Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks","abstract":"Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first results proving feature learning occurs in a multi-task setting with a nonlinear model. We show that when the tasks are binary classification problems with labels depending on only $r$ directions within the ambient $d\\gg r$-dimensional input space, executing a simple gradient-based multitask learning algorithm on a two-layer ReLU neural network learns the ground-truth $r$ directions. In particular, any downstream task on the $r$ ground-truth coordinates can be solved by learning a linear classifier with sample and neuron complexity independent of the ambient dimension $d$, while a random feature model requires exponential complexity in $d$ for such a guarantee.","sentences":["Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs.","Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs.","But in practice, neural networks are increasingly often trained on {\\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings.","In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models.","However, multi-task learning via {\\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious.","In this work, we present the first results proving feature learning occurs in a multi-task setting with a nonlinear model.","We show that when the tasks are binary classification problems with labels depending on only $r$ directions within the ambient $d\\gg r$-dimensional input space, executing a simple gradient-based multitask learning algorithm on a two-layer ReLU neural network learns the ground-truth $r$ directions.","In particular, any downstream task on the $r$ ground-truth coordinates can be solved by learning a linear classifier with sample and neuron complexity independent of the ambient dimension $d$, while a random feature model requires exponential complexity in $d$ for such a guarantee."],"url":"http://arxiv.org/abs/2307.06887v1"}
{"created":"2023-07-13 16:39:01","title":"Min-Max Optimization under Delays","abstract":"Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role. As such, several works have extensively analyzed stochastic optimization with delayed gradients. However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning. Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates. First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\\texttt{EG}) to diverge on simple instances for which \\texttt{EG} guarantees convergence in the absence of delays. Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms. Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\\texttt{GDA}) and \\texttt{EG} with delayed updates continue to guarantee convergence to saddle points for convex-concave and strongly convex-strongly concave settings. Our complexity bounds reveal, in a transparent manner, the slow-down in convergence caused by delays.","sentences":["Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role.","As such, several works have extensively analyzed stochastic optimization with delayed gradients.","However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning.","Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates.","First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\\texttt{EG}) to diverge on simple instances for which \\texttt{EG} guarantees convergence in the absence of delays.","Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms.","Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\\texttt{GDA}) and \\texttt{EG} with delayed updates continue to guarantee convergence to saddle points for convex-concave and strongly convex-strongly concave settings.","Our complexity bounds reveal, in a transparent manner, the slow-down in convergence caused by delays."],"url":"http://arxiv.org/abs/2307.06886v1"}
{"created":"2023-07-13 16:25:04","title":"The complexity of non-stationary reinforcement learning","abstract":"The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\\neq$ NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that just $\\textit{adding}$ a new state-action pair is considerably easier to implement.","sentences":["The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning.","We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\\neq$ NP conjecture.","Recall that the number of states in current applications of reinforcement learning is typically astronomical.","In contrast, we show that just $\\textit{adding}$ a new state-action pair is considerably easier to implement."],"url":"http://arxiv.org/abs/2307.06877v1"}
{"created":"2023-07-13 16:20:43","title":"Target Acquired? Evaluating Target Generation Algorithms for IPv6","abstract":"Internet measurements are a crucial foundation of IPv6-related research. Due to the infeasibility of full address space scans for IPv6 however, those measurements rely on collections of reliably responsive, unbiased addresses, as provided e.g., by the IPv6 Hitlist service. Although used for various use cases, the hitlist provides an unfiltered list of responsive addresses, the hosts behind which can come from a range of different networks and devices, such as web servers, customer-premises equipment (CPE) devices, and Internet infrastructure. In this paper, we demonstrate the importance of tailoring hitlists in accordance with the research goal in question. By using PeeringDB we classify hitlist addresses into six different network categories, uncovering that 42% of hitlist addresses are in ISP networks. Moreover, we show the different behavior of those addresses depending on their respective category, e.g., ISP addresses exhibiting a relatively low lifetime. Furthermore, we analyze different Target Generation Algorithms (TGAs), which are used to increase the coverage of IPv6 measurements by generating new responsive targets for scans. We evaluate their performance under various conditions and find generated addresses to show vastly differing responsiveness levels for different TGAs.","sentences":["Internet measurements are a crucial foundation of IPv6-related research.","Due to the infeasibility of full address space scans for IPv6 however, those measurements rely on collections of reliably responsive, unbiased addresses, as provided e.g., by the IPv6 Hitlist service.","Although used for various use cases, the hitlist provides an unfiltered list of responsive addresses, the hosts behind which can come from a range of different networks and devices, such as web servers, customer-premises equipment (CPE) devices, and Internet infrastructure.","In this paper, we demonstrate the importance of tailoring hitlists in accordance with the research goal in question.","By using PeeringDB we classify hitlist addresses into six different network categories, uncovering that 42% of hitlist addresses are in ISP networks.","Moreover, we show the different behavior of those addresses depending on their respective category, e.g., ISP addresses exhibiting a relatively low lifetime.","Furthermore, we analyze different Target Generation Algorithms (TGAs), which are used to increase the coverage of IPv6 measurements by generating new responsive targets for scans.","We evaluate their performance under various conditions and find generated addresses to show vastly differing responsiveness levels for different TGAs."],"url":"http://arxiv.org/abs/2307.06872v1"}
{"created":"2023-07-13 16:19:25","title":"Identifying Early Help Referrals For Local Authorities With Machine Learning And Bias Analysis","abstract":"Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools. This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support. LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18. The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models. Bias mitigation techniques were applied to improve the fairness of these models. During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced data, incorrectly identifying individuals who most likely did not need an Early Help referral. This paper empirically explores the suitability of data-driven ML models for identifying young people who may require Early Help services and discusses their appropriateness and limitations for this task.","sentences":["Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools.","This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support.","LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18.","The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models.","Bias mitigation techniques were applied to improve the fairness of these models.","During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced data, incorrectly identifying individuals who most likely did not need an Early Help referral.","This paper empirically explores the suitability of data-driven ML models for identifying young people who may require Early Help services and discusses their appropriateness and limitations for this task."],"url":"http://arxiv.org/abs/2307.06871v1"}
{"created":"2023-07-13 16:18:55","title":"Embodied Lifelong Learning for Task and Motion Planning","abstract":"A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.","sentences":["A robot deployed in a home over long stretches of time faces a true lifelong learning problem.","As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant.","We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP).","Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner.","Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state.","Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark."],"url":"http://arxiv.org/abs/2307.06870v1"}
{"created":"2023-07-13 16:16:51","title":"DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering","abstract":"Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability.","sentences":["Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability.","Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets.","Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained.","To deal with these challenges, we propose a simple yet effective metric called DecompEval.","This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability.","To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence.","The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result.","Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability."],"url":"http://arxiv.org/abs/2307.06869v1"}
{"created":"2023-07-13 16:15:08","title":"Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success","abstract":"The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.","sentences":["The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query.","The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query.","They have even been treated as commodities to be bought and sold.","However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret.","In this paper, we present a framework for systematically measuring the success of prompt extraction attacks.","In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability."],"url":"http://arxiv.org/abs/2307.06865v1"}
{"created":"2023-07-13 16:09:53","title":"LVLane: Deep Learning for Lane Detection and Classification in Challenging Conditions","abstract":"Lane detection plays a pivotal role in the field of autonomous vehicles and advanced driving assistant systems (ADAS). Over the years, numerous algorithms have emerged, spanning from rudimentary image processing techniques to sophisticated deep neural networks. The performance of deep learning-based models is highly dependent on the quality of their training data. Consequently, these models often experience a decline in performance when confronted with challenging scenarios such as extreme lighting conditions, partially visible lane markings, and sparse lane markings like Botts' dots. To address this, we present an end-to-end lane detection and classification system based on deep learning methodologies. In our study, we introduce a unique dataset meticulously curated to encompass scenarios that pose significant challenges for state-of-the-art (SOTA) models. Through fine-tuning selected models, we aim to achieve enhanced localization accuracy. Moreover, we propose a CNN-based classification branch, seamlessly integrated with the detector, facilitating the identification of distinct lane types. This architecture enables informed lane-changing decisions and empowers more resilient ADAS capabilities. We also investigate the effect of using mixed precision training and testing on different models and batch sizes. Experimental evaluations conducted on the widely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset demonstrate the effectiveness of our model in accurately detecting and classifying lanes amidst challenging scenarios. Our method achieves state-of-the-art classification results on the TuSimple dataset. The code of the work will be published upon the acceptance of the paper.","sentences":["Lane detection plays a pivotal role in the field of autonomous vehicles and advanced driving assistant systems (ADAS).","Over the years, numerous algorithms have emerged, spanning from rudimentary image processing techniques to sophisticated deep neural networks.","The performance of deep learning-based models is highly dependent on the quality of their training data.","Consequently, these models often experience a decline in performance when confronted with challenging scenarios such as extreme lighting conditions, partially visible lane markings, and sparse lane markings like Botts' dots.","To address this, we present an end-to-end lane detection and classification system based on deep learning methodologies.","In our study, we introduce a unique dataset meticulously curated to encompass scenarios that pose significant challenges for state-of-the-art (SOTA) models.","Through fine-tuning selected models, we aim to achieve enhanced localization accuracy.","Moreover, we propose a CNN-based classification branch, seamlessly integrated with the detector, facilitating the identification of distinct lane types.","This architecture enables informed lane-changing decisions and empowers more resilient ADAS capabilities.","We also investigate the effect of using mixed precision training and testing on different models and batch sizes.","Experimental evaluations conducted on the widely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset demonstrate the effectiveness of our model in accurately detecting and classifying lanes amidst challenging scenarios.","Our method achieves state-of-the-art classification results on the TuSimple dataset.","The code of the work will be published upon the acceptance of the paper."],"url":"http://arxiv.org/abs/2307.06853v1"}
{"created":"2023-07-13 16:08:03","title":"Self-Supervised Learning for Interactive Perception of Surgical Thread for Autonomous Suture Tail-Shortening","abstract":"Accurate 3D sensing of suturing thread is a challenging problem in automated surgical suturing because of the high state-space complexity, thinness and deformability of the thread, and possibility of occlusion by the grippers and tissue. In this work we present a method for tracking surgical thread in 3D which is robust to occlusions and complex thread configurations, and apply it to autonomously perform the surgical suture \"tail-shortening\" task: pulling thread through tissue until a desired \"tail\" length remains exposed. The method utilizes a learned 2D surgical thread detection network to segment suturing thread in RGB images. It then identifies the thread path in 2D and reconstructs the thread in 3D as a NURBS spline by triangulating the detections from two stereo cameras. Once a 3D thread model is initialized, the method tracks the thread across subsequent frames. Experiments suggest the method achieves a 1.33 pixel average reprojection error on challenging single-frame 3D thread reconstructions, and an 0.84 pixel average reprojection error on two tracking sequences. On the tail-shortening task, it accomplishes a 90% success rate across 20 trials. Supplemental materials are available at https://sites.google.com/berkeley.edu/autolab-surgical-thread/ .","sentences":["Accurate 3D sensing of suturing thread is a challenging problem in automated surgical suturing because of the high state-space complexity, thinness and deformability of the thread, and possibility of occlusion by the grippers and tissue.","In this work we present a method for tracking surgical thread in 3D which is robust to occlusions and complex thread configurations, and apply it to autonomously perform the surgical suture \"tail-shortening\" task: pulling thread through tissue until a desired \"tail\" length remains exposed.","The method utilizes a learned 2D surgical thread detection network to segment suturing thread in RGB images.","It then identifies the thread path in 2D and reconstructs the thread in 3D as a NURBS spline by triangulating the detections from two stereo cameras.","Once a 3D thread model is initialized, the method tracks the thread across subsequent frames.","Experiments suggest the method achieves a 1.33 pixel average reprojection error on challenging single-frame 3D thread reconstructions, and an 0.84 pixel average reprojection error on two tracking sequences.","On the tail-shortening task, it accomplishes a 90% success rate across 20 trials.","Supplemental materials are available at https://sites.google.com/berkeley.edu/autolab-surgical-thread/ ."],"url":"http://arxiv.org/abs/2307.06845v1"}
{"created":"2023-07-13 16:02:09","title":"Dynamic Capacity Enhancement using Air Computing: An Earthquake Case","abstract":"Earthquakes are one of the most destructive natural disasters harming life and the infrastructure of cities. After an earthquake, functioning communication and computational capacity are crucial for rescue teams and healthcare of victims. Therefore, an earthquake can be investigated for dynamic capacity enhancement in which additional resources are deployed since the surviving portion of the infrastructure may not meet the demand of the users. In this study, we propose a new computation paradigm, air computing, which is the air vehicle assisted next generation edge computing through different air platforms, in order to enhance the capacity of the areas affected by an earthquake. To this end, we put forward a novel paradigm that presents a dynamic, responsive, and high-resolution computation environment by explaining its corresponding components, air layers, and essential advantages. Moreover, we focus on the unmanned aerial vehicle (UAV) deployment problem and apply three different methods including the emergency method, the load balancing method, and the location selection index (LSI) method in which we take the delay requirements of applications into account. To test and compare their performance in terms of the task success rate, we developed an earthquake scenario in which three towns are affected with different severity. The experimental results showed that each method can be beneficial considering the circumstances, and goal of the rescue.","sentences":["Earthquakes are one of the most destructive natural disasters harming life and the infrastructure of cities.","After an earthquake, functioning communication and computational capacity are crucial for rescue teams and healthcare of victims.","Therefore, an earthquake can be investigated for dynamic capacity enhancement in which additional resources are deployed since the surviving portion of the infrastructure may not meet the demand of the users.","In this study, we propose a new computation paradigm, air computing, which is the air vehicle assisted next generation edge computing through different air platforms, in order to enhance the capacity of the areas affected by an earthquake.","To this end, we put forward a novel paradigm that presents a dynamic, responsive, and high-resolution computation environment by explaining its corresponding components, air layers, and essential advantages.","Moreover, we focus on the unmanned aerial vehicle (UAV) deployment problem and apply three different methods including the emergency method, the load balancing method, and the location selection index (LSI) method in which we take the delay requirements of applications into account.","To test and compare their performance in terms of the task success rate, we developed an earthquake scenario in which three towns are affected with different severity.","The experimental results showed that each method can be beneficial considering the circumstances, and goal of the rescue."],"url":"http://arxiv.org/abs/2307.06838v1"}
{"created":"2023-07-13 15:40:38","title":"A Causal Framework to Unify Common Domain Generalization Approaches","abstract":"Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s). It is a fundamental problem in machine learning and has attracted much attention in recent years. A large number of approaches have been proposed. Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area. In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework. Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method? (2) Why is it expected to improve generalization to new domains theoretically? (3) How are different DG methods related to each other and what are relative advantages and limitations? By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective approaches for this critical problem in machine learning.","sentences":["Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s).","It is a fundamental problem in machine learning and has attracted much attention in recent years.","A large number of approaches have been proposed.","Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area.","In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework.","Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method?","(2) Why is it expected to improve generalization to new domains theoretically?","(3) How are different DG methods related to each other and what are relative advantages and limitations?","By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective approaches for this critical problem in machine learning."],"url":"http://arxiv.org/abs/2307.06825v1"}
{"created":"2023-07-13 15:39:26","title":"TinyMetaFed: Efficient Federated Meta-Learning for TinyML","abstract":"The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational efficiency via online learning, and robustness to client heterogeneity through few-shot learning. The evaluations on three TinyML use cases demonstrate that TinyMetaFed can significantly reduce energy consumption and communication overhead, accelerate convergence, and stabilize the training process.","sentences":["The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers.","The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications.","Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world.","However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations.","We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML.","TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices.","It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational efficiency via online learning, and robustness to client heterogeneity through few-shot learning.","The evaluations on three TinyML use cases demonstrate that TinyMetaFed can significantly reduce energy consumption and communication overhead, accelerate convergence, and stabilize the training process."],"url":"http://arxiv.org/abs/2307.06822v1"}
{"created":"2023-07-13 15:23:24","title":"Spatio-Temporal Calibration for Omni-Directional Vehicle-Mounted","abstract":"We present a solution to the problem of spatio-temporal calibration for event cameras mounted on an onmi-directional vehicle. Different from traditional methods that typically determine the camera's pose with respect to the vehicle's body frame using alignment of trajectories, our approach leverages the kinematic correlation of two sets of linear velocity estimates from event data and wheel odometers, respectively. The overall calibration task consists of estimating the underlying temporal offset between the two heterogeneous sensors, and furthermore, recovering the extrinsic rotation that defines the linear relationship between the two sets of velocity estimates. The first sub-problem is formulated as an optimization one, which looks for the optimal temporal offset that maximizes a correlation measurement invariant to arbitrary linear transformation. Once the temporal offset is compensated, the extrinsic rotation can be worked out with an iterative closed-form solver that incrementally registers associated linear velocity estimates. The proposed algorithm is proved effective on both synthetic data and real data, outperforming traditional methods based on alignment of trajectories.","sentences":["We present a solution to the problem of spatio-temporal calibration for event cameras mounted on an onmi-directional vehicle.","Different from traditional methods that typically determine the camera's pose with respect to the vehicle's body frame using alignment of trajectories, our approach leverages the kinematic correlation of two sets of linear velocity estimates from event data and wheel odometers, respectively.","The overall calibration task consists of estimating the underlying temporal offset between the two heterogeneous sensors, and furthermore, recovering the extrinsic rotation that defines the linear relationship between the two sets of velocity estimates.","The first sub-problem is formulated as an optimization one, which looks for the optimal temporal offset that maximizes a correlation measurement invariant to arbitrary linear transformation.","Once the temporal offset is compensated, the extrinsic rotation can be worked out with an iterative closed-form solver that incrementally registers associated linear velocity estimates.","The proposed algorithm is proved effective on both synthetic data and real data, outperforming traditional methods based on alignment of trajectories."],"url":"http://arxiv.org/abs/2307.06810v1"}
{"created":"2023-07-13 15:13:58","title":"Decomposing Finite Languages","abstract":"The paper completely characterizes the primality of acyclic DFAs, where a DFA $\\mathcal{A}$ is prime if there do not exist DFAs $\\mathcal{A}_1,\\dots,\\mathcal{A}_t$ with $\\mathcal{L}(\\mathcal{A}) = \\bigcap_{i=1}^{t} \\mathcal{L}({\\mathcal{A}_i})$ such that each $\\mathcal{A}_i$ has strictly less states than the minimal DFA recognizing the same language as $\\mathcal{A}$. A regular language is prime if its minimal DFA is prime. Thus, this result also characterizes the primality of finite languages.   Further, the $\\mathsf{NL}$-completeness of the corresponding decision problem $\\mathsf{PrimeDFA}_{\\text{fin}}$ is proven. The paper also characterizes the primality of acyclic DFAs under two different notions of compositionality, union and union-intersection compositionality.   Additionally, the paper introduces the notion of S-primality, where a DFA $\\mathcal{A}$ is S-prime if there do not exist DFAs $\\mathcal{A}_1,\\dots,\\mathcal{A}_t$ with $\\mathcal{L}(\\mathcal{A}) = \\bigcap_{i=1}^{t} \\mathcal{L}(\\mathcal{A}_i)$ such that each $\\mathcal{A}_i$ has strictly less states than $\\mathcal{A}$ itself. It is proven that the problem of deciding S-primality for a given DFA is $\\mathsf{NL}$-hard. To do so, the $\\mathsf{NL}$-completeness of $\\mathsf{2MinimalDFA}$, the basic problem of deciding minimality for a DFA with at most two letters, is proven.","sentences":["The paper completely characterizes the primality of acyclic DFAs, where a DFA $\\mathcal{A}$ is prime if there do not exist DFAs $\\mathcal{A}_1,\\dots,\\mathcal{A}_t$ with $\\mathcal{L}(\\mathcal{A}) = \\bigcap_{i=1}^{t} \\mathcal{L}({\\mathcal{A}_i})$ such that each $\\mathcal{A}_i$ has strictly less states than the minimal DFA recognizing the same language as $\\mathcal{A}$. A regular language is prime if its minimal DFA is prime.","Thus, this result also characterizes the primality of finite languages.   ","Further, the $\\mathsf{NL}$-completeness of the corresponding decision problem $\\mathsf{PrimeDFA}_{\\text{fin}}$ is proven.","The paper also characterizes the primality of acyclic DFAs under two different notions of compositionality, union and union-intersection compositionality.   ","Additionally, the paper introduces the notion of S-primality, where a DFA $\\mathcal{A}$ is S-prime if there do not exist DFAs $\\mathcal{A}_1,\\dots,\\mathcal{A}_t$ with $\\mathcal{L}(\\mathcal{A}) = \\bigcap_{i=1}^{t} \\mathcal{L}(\\mathcal{A}_i)$ such that each $\\mathcal{A}_i$ has strictly less states than $\\mathcal{A}$ itself.","It is proven that the problem of deciding S-primality for a given DFA is $\\mathsf{NL}$-hard.","To do so, the $\\mathsf{NL}$-completeness of $\\mathsf{2MinimalDFA}$",", the basic problem of deciding minimality for a DFA with at most two letters, is proven."],"url":"http://arxiv.org/abs/2307.06802v1"}
{"created":"2023-07-13 15:08:44","title":"Fast and Functional Structured Data Generators Rooted in Out-of-Equilibrium Physics","abstract":"In this study, we address the challenge of using energy-based models to produce high-quality, label-specific data in complex structured datasets, such as population genetics, RNA or protein sequences data. Traditional training methods encounter difficulties due to inefficient Markov chain Monte Carlo mixing, which affects the diversity of synthetic data and increases generation times. To address these issues, we use a novel training algorithm that exploits non-equilibrium effects. This approach, applied on the Restricted Boltzmann Machine, improves the model's ability to correctly classify samples and generate high-quality synthetic data in only a few sampling steps. The effectiveness of this method is demonstrated by its successful application to four different types of data: handwritten digits, mutations of human genomes classified by continental origin, functionally characterized sequences of an enzyme protein family, and homologous RNA sequences from specific taxonomies.","sentences":["In this study, we address the challenge of using energy-based models to produce high-quality, label-specific data in complex structured datasets, such as population genetics, RNA or protein sequences data.","Traditional training methods encounter difficulties due to inefficient Markov chain Monte Carlo mixing, which affects the diversity of synthetic data and increases generation times.","To address these issues, we use a novel training algorithm that exploits non-equilibrium effects.","This approach, applied on the Restricted Boltzmann Machine, improves the model's ability to correctly classify samples and generate high-quality synthetic data in only a few sampling steps.","The effectiveness of this method is demonstrated by its successful application to four different types of data: handwritten digits, mutations of human genomes classified by continental origin, functionally characterized sequences of an enzyme protein family, and homologous RNA sequences from specific taxonomies."],"url":"http://arxiv.org/abs/2307.06797v1"}
{"created":"2023-07-13 15:05:34","title":"Leveraging Vision-Language Foundation Models for Fine-Grained Downstream Tasks","abstract":"Vision-language foundation models such as CLIP have shown impressive zero-shot performance on many tasks and datasets, especially thanks to their free-text inputs. However, they struggle to handle some downstream tasks, such as fine-grained attribute detection and localization. In this paper, we propose a multitask fine-tuning strategy based on a positive/negative prompt formulation to further leverage the capacities of the vision-language foundation models. Using the CLIP architecture as baseline, we show strong improvements on bird fine-grained attribute detection and localization tasks, while also increasing the classification performance on the CUB200-2011 dataset. We provide source code for reproducibility purposes: it is available at https://github.com/FactoDeepLearning/MultitaskVLFM.","sentences":["Vision-language foundation models such as CLIP have shown impressive zero-shot performance on many tasks and datasets, especially thanks to their free-text inputs.","However, they struggle to handle some downstream tasks, such as fine-grained attribute detection and localization.","In this paper, we propose a multitask fine-tuning strategy based on a positive/negative prompt formulation to further leverage the capacities of the vision-language foundation models.","Using the CLIP architecture as baseline, we show strong improvements on bird fine-grained attribute detection and localization tasks, while also increasing the classification performance on the CUB200-2011 dataset.","We provide source code for reproducibility purposes: it is available at https://github.com/FactoDeepLearning/MultitaskVLFM."],"url":"http://arxiv.org/abs/2307.06795v1"}
{"created":"2023-07-13 15:03:48","title":"Negated Complementary Commonsense using Large Language Models","abstract":"Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense.","sentences":["Larger language models, such as GPT-3, have shown to be excellent in many tasks.","However, we demonstrate that out-of-ordinary questions can throw the model off guard.","This work focuses on finding answers to negated complementary questions in commonsense scenarios.","We illustrate how such questions adversely affect the model responses.","We propose a model-agnostic methodology to improve the performance in negated complementary scenarios.","Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions.","The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense."],"url":"http://arxiv.org/abs/2307.06794v1"}
{"created":"2023-07-13 15:02:49","title":"Planar Disjoint Paths, Treewidth, and Kernels","abstract":"In the Planar Disjoint Paths problem, one is given an undirected planar graph with a set of $k$ vertex pairs $(s_i,t_i)$ and the task is to find $k$ pairwise vertex-disjoint paths such that the $i$-th path connects $s_i$ to $t_i$. We study the problem through the lens of kernelization, aiming at efficiently reducing the input size in terms of a parameter. We show that Planar Disjoint Paths does not admit a polynomial kernel when parameterized by $k$ unless coNP $\\subseteq$ NP/poly, resolving an open problem by [Bodlaender, Thomass{\\'e}, Yeo, ESA'09]. Moreover, we rule out the existence of a polynomial Turing kernel unless the WK-hierarchy collapses. Our reduction carries over to the setting of edge-disjoint paths, where the kernelization status remained open even in general graphs.   On the positive side, we present a polynomial kernel for Planar Disjoint Paths parameterized by $k + tw$, where $tw$ denotes the treewidth of the input graph. As a consequence of both our results, we rule out the possibility of a polynomial-time (Turing) treewidth reduction to $tw= k^{O(1)}$ under the same assumptions. To the best of our knowledge, this is the first hardness result of this kind. Finally, combining our kernel with the known techniques [Adler, Kolliopoulos, Krause, Lokshtanov, Saurabh, Thilikos, JCTB'17; Schrijver, SICOMP'94] yields an alternative (and arguably simpler) proof that Planar Disjoint Paths can be solved in time $2^{O(k^2)}\\cdot n^{O(1)}$, matching the result of [Lokshtanov, Misra, Pilipczuk, Saurabh, Zehavi, STOC'20].","sentences":["In the Planar Disjoint Paths problem, one is given an undirected planar graph with a set of $k$ vertex pairs $(s_i,t_i)$ and the task is to find $k$ pairwise vertex-disjoint paths such that the $i$-th path connects $s_i$ to $t_i$. We study the problem through the lens of kernelization, aiming at efficiently reducing the input size in terms of a parameter.","We show that Planar Disjoint Paths does not admit a polynomial kernel when parameterized by $k$ unless coNP $\\subseteq$ NP/poly, resolving an open problem by [Bodlaender, Thomass{\\'e}, Yeo, ESA'09].","Moreover, we rule out the existence of a polynomial Turing kernel unless the WK-hierarchy collapses.","Our reduction carries over to the setting of edge-disjoint paths, where the kernelization status remained open even in general graphs.   ","On the positive side, we present a polynomial kernel for Planar Disjoint Paths parameterized by $k + tw$, where $tw$ denotes the treewidth of the input graph.","As a consequence of both our results, we rule out the possibility of a polynomial-time (Turing) treewidth reduction to $tw= k^{O(1)}$ under the same assumptions.","To the best of our knowledge, this is the first hardness result of this kind.","Finally, combining our kernel with the known techniques [Adler, Kolliopoulos, Krause, Lokshtanov, Saurabh, Thilikos, JCTB'17; Schrijver, SICOMP'94] yields an alternative (and arguably simpler) proof that Planar Disjoint Paths can be solved in time $2^{O(k^2)}\\cdot n^{O(1)}$, matching the result of [Lokshtanov, Misra, Pilipczuk, Saurabh, Zehavi, STOC'20]."],"url":"http://arxiv.org/abs/2307.06792v1"}
{"created":"2023-07-13 14:59:22","title":"scda: A Minimal, Serial-Equivalent Format for Parallel I/O","abstract":"We specify a file-oriented data format suitable for parallel, partition-independent disk I/O. Here, a partition refers to a disjoint and ordered distribution of the data elements between one or more processes. The format is designed such that the file contents are invariant under linear (i. e., unpermuted), parallel repartition of the data prior to writing. The file contents are indistinguishable from writing in serial. In the same vein, the file can be read on any number of processes that agree on any partition of the number of elements stored.   In addition to the format specification we propose an optional convention to implement transparent per-element data compression. The compressed data and metadata is layered inside ordinary format elements. Overall, we pay special attention to both human and machine readability. If pure ASCII data is written, or compressed data is reencoded to ASCII, the entire file including its header and sectioning metadata remains entirely in ASCII. If binary data is written, the metadata stays easy on the human eye.   We refer to this format as scda. Conceptually, it lies one layer below and is oblivious to the definition of variables, the binary representation of numbers, considerations of endianness, and self-describing headers, which may all be specified on top of scda. The main purpose of the format is to abstract any parallelism and provide sufficient structure as a foundation for a generic and flexible archival and checkpoint/restart. A documented reference implementation is available as part of the general-purpose libsc free software library.","sentences":["We specify a file-oriented data format suitable for parallel, partition-independent disk I/O. Here, a partition refers to a disjoint and ordered distribution of the data elements between one or more processes.","The format is designed such that the file contents are invariant under linear (i. e., unpermuted), parallel repartition of the data prior to writing.","The file contents are indistinguishable from writing in serial.","In the same vein, the file can be read on any number of processes that agree on any partition of the number of elements stored.   ","In addition to the format specification we propose an optional convention to implement transparent per-element data compression.","The compressed data and metadata is layered inside ordinary format elements.","Overall, we pay special attention to both human and machine readability.","If pure ASCII data is written, or compressed data is reencoded to ASCII, the entire file including its header and sectioning metadata remains entirely in ASCII.","If binary data is written, the metadata stays easy on the human eye.   ","We refer to this format as scda.","Conceptually, it lies one layer below and is oblivious to the definition of variables, the binary representation of numbers, considerations of endianness, and self-describing headers, which may all be specified on top of scda.","The main purpose of the format is to abstract any parallelism and provide sufficient structure as a foundation for a generic and flexible archival and checkpoint/restart.","A documented reference implementation is available as part of the general-purpose libsc free software library."],"url":"http://arxiv.org/abs/2307.06789v1"}
{"created":"2023-07-13 14:50:38","title":"Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation","abstract":"This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics. A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments. To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm. Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack. A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator. The motion planner divides the crack into multiple nodes which are then explored individually. Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm. If a crack is detected, also the length, width, orientation and number of branches are calculated. This is repeated until all the nodes of the crack are explored.   In order to validate the complete algorithm, various experiments are performed: comparison of exploration of cracks through full scan and motion planning algorithm, implementation of frequency-based features for crack classification and geometry analysis using a combination of vision and tactile data. From the results of the experiments, it is shown that the proposed algorithm is able to detect cracks and improve the results obtained from vision to correctly classify cracks and their geometry with minimal cost thanks to the motion planning algorithm.","sentences":["This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics.","A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments.","To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm.","Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack.","A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator.","The motion planner divides the crack into multiple nodes which are then explored individually.","Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm.","If a crack is detected, also the length, width, orientation and number of branches are calculated.","This is repeated until all the nodes of the crack are explored.   ","In order to validate the complete algorithm, various experiments are performed: comparison of exploration of cracks through full scan and motion planning algorithm, implementation of frequency-based features for crack classification and geometry analysis using a combination of vision and tactile data.","From the results of the experiments, it is shown that the proposed algorithm is able to detect cracks and improve the results obtained from vision to correctly classify cracks and their geometry with minimal cost thanks to the motion planning algorithm."],"url":"http://arxiv.org/abs/2307.06784v1"}
{"created":"2023-07-13 14:38:15","title":"Data Behind the Walls An Advanced Architecture for Data Privacy Management","abstract":"In today's highly connected society, we are constantly asked to provide personal information to retailers, voter surveys, medical professionals, and other data collection efforts. The collected data is stored in large data warehouses. Organisations and statistical agencies share and use this data to facilitate research in public health, economics, sociology, etc. However, this data contains sensitive information about individuals, which can result in identity theft, financial loss, stress and depression, embarrassment, abuse, etc. Therefore, one must ensure rigorous management of individuals' privacy. We propose, an advanced data privacy management architecture composed of three layers. The data management layer consists of de-identification and anonymisation, the access management layer for re-enforcing data access based on the concepts of Role-Based Access Control and the Chinese Wall Security Policy, and the roles layer for regulating different users. The proposed system architecture is validated on healthcare datasets.","sentences":["In today's highly connected society, we are constantly asked to provide personal information to retailers, voter surveys, medical professionals, and other data collection efforts.","The collected data is stored in large data warehouses.","Organisations and statistical agencies share and use this data to facilitate research in public health, economics, sociology, etc.","However, this data contains sensitive information about individuals, which can result in identity theft, financial loss, stress and depression, embarrassment, abuse, etc.","Therefore, one must ensure rigorous management of individuals' privacy.","We propose, an advanced data privacy management architecture composed of three layers.","The data management layer consists of de-identification and anonymisation, the access management layer for re-enforcing data access based on the concepts of Role-Based Access Control and the Chinese Wall Security Policy, and the roles layer for regulating different users.","The proposed system architecture is validated on healthcare datasets."],"url":"http://arxiv.org/abs/2307.06779v1"}
{"created":"2023-07-13 14:34:18","title":"Deciding Conjugacy of a Rational Relation","abstract":"A rational relation is conjugate if every pair of related words are conjugates. It is shown that checking whether a rational relation is conjugate is decidable. For this, we generalise the Lyndon-Sch\\\"utzenberger's theorem from word combinatorics. A consequence of the generalisation is that a set of pairs generated by a sumfree rational expression is conjugate if and only if there is a word witnessing the conjugacy of all the pairs.","sentences":["A rational relation is conjugate if every pair of related words are conjugates.","It is shown that checking whether a rational relation is conjugate is decidable.","For this, we generalise the Lyndon-Sch\\\"utzenberger's theorem from word combinatorics.","A consequence of the generalisation is that a set of pairs generated by a sumfree rational expression is conjugate if and only if there is a word witnessing the conjugacy of all the pairs."],"url":"http://arxiv.org/abs/2307.06777v1"}
{"created":"2023-07-13 14:33:42","title":"Approximation algorithms for the square min-sum bin packing problem","abstract":"In this work, we study the square min-sum bin packing problem (SMSBPP), where a list of square items has to be packed into indexed square bins of dimensions $1 \\times 1$ with no overlap between the areas of the items. The bins are indexed and the cost of packing each item is equal to the index of the bin in which it is placed in. The objective is to minimize the total cost of packing all items, which is equivalent to minimizing the average cost of items. The problem has applications in minimizing the average time of logistic operations such as cutting stock and delivery of products. We prove that classic algorithms for two-dimensional bin packing that order items in non-increasing order of size, such as Next Fit Decreasing Height or Any Fit Decreasing Height heuristics, can have an arbitrarily bad performance for SMSBPP. We, then, present a $\\frac{53}{22}$-approximation and a PTAS for the problem.","sentences":["In this work, we study the square min-sum bin packing problem (SMSBPP), where a list of square items has to be packed into indexed square bins of dimensions $1 \\times 1$ with no overlap between the areas of the items.","The bins are indexed and the cost of packing each item is equal to the index of the bin in which it is placed in.","The objective is to minimize the total cost of packing all items, which is equivalent to minimizing the average cost of items.","The problem has applications in minimizing the average time of logistic operations such as cutting stock and delivery of products.","We prove that classic algorithms for two-dimensional bin packing that order items in non-increasing order of size, such as Next Fit Decreasing Height or Any Fit Decreasing Height heuristics, can have an arbitrarily bad performance for SMSBPP.","We, then, present a $\\frac{53}{22}$-approximation and a PTAS for the problem."],"url":"http://arxiv.org/abs/2307.06776v1"}
{"created":"2023-07-13 14:25:09","title":"Stackelberg Vertex Cover on a Path","abstract":"A Stackelberg Vertex Cover game is played on an undirected graph $\\mathcal{G}$ where some of the vertices are under the control of a \\emph{leader}. The remaining vertices are assigned a fixed weight. The game is played in two stages. First, the leader chooses prices for the vertices under her control. Afterward, the second player, called \\emph{follower}, selects a min weight vertex cover in the resulting weighted graph. That is, the follower selects a subset of vertices $C^*$ such that every edge has at least one endpoint in $C^*$ of minimum weight w.r.t.\\ to the fixed weights, and the prices set by the leader. Stackelberg Vertex Cover (StackVC) describes the leader's optimization problem to select prices in the first stage of the game so as to maximize her revenue, which is the cumulative price of all her (priceable) vertices that are contained in the follower's solution. Previous research showed that StackVC is \\textsf{NP}-hard on bipartite graphs, but solvable in polynomial time in the special case of bipartite graphs, where all priceable vertices belong to the same side of the bipartition. In this paper, we investigate StackVC on paths and present a dynamic program with linear time and space complexity.","sentences":["A Stackelberg Vertex Cover game is played on an undirected graph $\\mathcal{G}$ where some of the vertices are under the control of a \\emph{leader}.","The remaining vertices are assigned a fixed weight.","The game is played in two stages.","First, the leader chooses prices for the vertices under her control.","Afterward, the second player, called \\emph{follower}, selects a min weight vertex cover in the resulting weighted graph.","That is, the follower selects a subset of vertices $C^*$ such that every edge has at least one endpoint in $C^*$ of minimum weight w.r.t.\\ to the fixed weights, and the prices set by the leader.","Stackelberg Vertex Cover (StackVC) describes the leader's optimization problem to select prices in the first stage of the game so as to maximize her revenue, which is the cumulative price of all her (priceable) vertices that are contained in the follower's solution.","Previous research showed that StackVC is \\textsf{NP}-hard on bipartite graphs, but solvable in polynomial time in the special case of bipartite graphs, where all priceable vertices belong to the same side of the bipartition.","In this paper, we investigate StackVC on paths and present a dynamic program with linear time and space complexity."],"url":"http://arxiv.org/abs/2307.06772v1"}
{"created":"2023-07-13 14:11:40","title":"An update on the coin-moving game on the square grid","abstract":"This paper extends the work started in 2002 by Demaine, Demaine and Verill (DDV) on coin-moving puzzles. These puzzles have a long history in the recreational literature, but were first systematically analyzed by DDV, who gave a full characterization of the solvable puzzles on the triangular grid and a partial characterization of the solvable puzzles on the square grid. This article specifically extends the study of the game on the square grid. Notably, DDV's result on puzzles with two \"extra coins\" is shown to be overly broad: this paper provides counterexamples as well as a revised version of this theorem. A new method for solving puzzles with two extra coins is then presented, which covers some cases where the aforementioned theorem does not apply. Puzzles with just one extra coin seem even more complicated, and are only touched upon by DDV. This paper delves deeper, studying a class of such puzzles that may be considered equivalent to a game of \"poking\" coins. Within this class, some cases are considered that are amenable to analysis.","sentences":["This paper extends the work started in 2002 by Demaine, Demaine and Verill (DDV) on coin-moving puzzles.","These puzzles have a long history in the recreational literature, but were first systematically analyzed by DDV, who gave a full characterization of the solvable puzzles on the triangular grid and a partial characterization of the solvable puzzles on the square grid.","This article specifically extends the study of the game on the square grid.","Notably, DDV's result on puzzles with two \"extra coins\" is shown to be overly broad: this paper provides counterexamples as well as a revised version of this theorem.","A new method for solving puzzles with two extra coins is then presented, which covers some cases where the aforementioned theorem does not apply.","Puzzles with just one extra coin seem even more complicated, and are only touched upon by DDV.","This paper delves deeper, studying a class of such puzzles that may be considered equivalent to a game of \"poking\" coins.","Within this class, some cases are considered that are amenable to analysis."],"url":"http://arxiv.org/abs/2307.06767v1"}
{"created":"2023-07-13 14:02:54","title":"Retroactive Parametrized Monitoring","abstract":"In online monitoring, we first synthesize a monitor from a formal specification, which later runs in tandem with the system under study, incrementally receiving its progress and evolving with the system. In offline monitoring the trace is logged as the system progresses to later do post-mortem analysis after the system has finished executing.   In this paper we propose retroactive dynamic parametrization, a technique that allows a monitor to revisit the past log as it progresses, while still executing in an online manner. This feature allows new monitors to be incorporated into a running system and to revisit the past for particular behaviors based on new information discovered. Retroactive parametrization also allows a monitor to lazily ignore events and revisit and process them later, when it discovers that it should have followed those events. We showcase the use of retroactive dynamic parametrization to monitor denial of service attacks on a network using network logs.","sentences":["In online monitoring, we first synthesize a monitor from a formal specification, which later runs in tandem with the system under study, incrementally receiving its progress and evolving with the system.","In offline monitoring the trace is logged as the system progresses to later do post-mortem analysis after the system has finished executing.   ","In this paper we propose retroactive dynamic parametrization, a technique that allows a monitor to revisit the past log as it progresses, while still executing in an online manner.","This feature allows new monitors to be incorporated into a running system and to revisit the past for particular behaviors based on new information discovered.","Retroactive parametrization also allows a monitor to lazily ignore events and revisit and process them later, when it discovers that it should have followed those events.","We showcase the use of retroactive dynamic parametrization to monitor denial of service attacks on a network using network logs."],"url":"http://arxiv.org/abs/2307.06763v1"}
{"created":"2023-07-13 13:59:54","title":"Privacy-Utility Trade-offs in Neural Networks for Medical Population Graphs: Insights from Differential Privacy and Graph Structure","abstract":"We initiate an empirical investigation into differentially private graph neural networks on population graphs from the medical domain by examining privacy-utility trade-offs at different privacy levels on both real-world and synthetic datasets and performing auditing through membership inference attacks. Our findings highlight the potential and the challenges of this specific DP application area. Moreover, we find evidence that the underlying graph structure constitutes a potential factor for larger performance gaps by showing a correlation between the degree of graph homophily and the accuracy of the trained model.","sentences":["We initiate an empirical investigation into differentially private graph neural networks on population graphs from the medical domain by examining privacy-utility trade-offs at different privacy levels on both real-world and synthetic datasets and performing auditing through membership inference attacks.","Our findings highlight the potential and the challenges of this specific DP application area.","Moreover, we find evidence that the underlying graph structure constitutes a potential factor for larger performance gaps by showing a correlation between the degree of graph homophily and the accuracy of the trained model."],"url":"http://arxiv.org/abs/2307.06760v1"}
{"created":"2023-07-13 13:56:27","title":"Layered controller synthesis for dynamic multi-agent systems","abstract":"In this paper we present a layered approach for multi-agent control problem, decomposed into three stages, each building upon the results of the previous one. First, a high-level plan for a coarse abstraction of the system is computed, relying on parametric timed automata augmented with stopwatches as they allow to efficiently model simplified dynamics of such systems. In the second stage, the high-level plan, based on SMT-formulation, mainly handles the combinatorial aspects of the problem, provides a more dynamically accurate solution. These stages are collectively referred to as the SWA-SMT solver. They are correct by construction but lack a crucial feature: they cannot be executed in real time. To overcome this, we use SWA-SMT solutions as the initial training dataset for our last stage, which aims at obtaining a neural network control policy. We use reinforcement learning to train the policy, and show that the initial dataset is crucial for the overall success of the method.","sentences":["In this paper we present a layered approach for multi-agent control problem, decomposed into three stages, each building upon the results of the previous one.","First, a high-level plan for a coarse abstraction of the system is computed, relying on parametric timed automata augmented with stopwatches as they allow to efficiently model simplified dynamics of such systems.","In the second stage, the high-level plan, based on SMT-formulation, mainly handles the combinatorial aspects of the problem, provides a more dynamically accurate solution.","These stages are collectively referred to as the SWA-SMT solver.","They are correct by construction but lack a crucial feature: they cannot be executed in real time.","To overcome this, we use SWA-SMT solutions as the initial training dataset for our last stage, which aims at obtaining a neural network control policy.","We use reinforcement learning to train the policy, and show that the initial dataset is crucial for the overall success of the method."],"url":"http://arxiv.org/abs/2307.06758v1"}
{"created":"2023-07-13 13:52:07","title":"PREFENDER: A Prefetching Defender against Cache Side Channel Attacks as A Pretender","abstract":"Cache side channel attacks are increasingly alarming in modern processors due to the recent emergence of Spectre and Meltdown attacks. A typical attack performs intentional cache access and manipulates cache states to leak secrets by observing the victim's cache access patterns. Different countermeasures have been proposed to defend against both general and transient execution based attacks. Despite their effectiveness, they mostly trade some level of performance for security, or have restricted security scope. In this paper, we seek an approach to enforcing security while maintaining performance. We leverage the insight that attackers need to access cache in order to manipulate and observe cache state changes for information leakage. Specifically, we propose Prefender, a secure prefetcher that learns and predicts attack-related accesses for prefetching the cachelines to simultaneously help security and performance. Our results show that Prefender is effective against several cache side channel attacks while maintaining or even improving performance for SPEC CPU 2006 and 2017 benchmarks.","sentences":["Cache side channel attacks are increasingly alarming in modern processors due to the recent emergence of Spectre and Meltdown attacks.","A typical attack performs intentional cache access and manipulates cache states to leak secrets by observing the victim's cache access patterns.","Different countermeasures have been proposed to defend against both general and transient execution based attacks.","Despite their effectiveness, they mostly trade some level of performance for security, or have restricted security scope.","In this paper, we seek an approach to enforcing security while maintaining performance.","We leverage the insight that attackers need to access cache in order to manipulate and observe cache state changes for information leakage.","Specifically, we propose Prefender, a secure prefetcher that learns and predicts attack-related accesses for prefetching the cachelines to simultaneously help security and performance.","Our results show that Prefender is effective against several cache side channel attacks while maintaining or even improving performance for SPEC CPU 2006 and 2017 benchmarks."],"url":"http://arxiv.org/abs/2307.06756v1"}
{"created":"2023-07-13 13:43:02","title":"Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent","abstract":"The learning of Gaussian Mixture Models (also referred to simply as GMMs) plays an important role in machine learning. Known for their expressiveness and interpretability, Gaussian mixture models have a wide range of applications, from statistics, computer vision to distributional reinforcement learning. However, as of today, few known algorithms can fit or learn these models, some of which include Expectation-Maximization algorithms and Sliced Wasserstein Distance. Even fewer algorithms are compatible with gradient descent, the common learning process for neural networks.   In this paper, we derive a closed formula of two GMMs in the univariate, one-dimensional case, then propose a distance function called Sliced Cram\\'er 2-distance for learning general multivariate GMMs. Our approach has several advantages over many previous methods. First, it has a closed-form expression for the univariate case and is easy to compute and implement using common machine learning libraries (e.g., PyTorch and TensorFlow). Second, it is compatible with gradient descent, which enables us to integrate GMMs with neural networks seamlessly. Third, it can fit a GMM not only to a set of data points, but also to another GMM directly, without sampling from the target model. And fourth, it has some theoretical guarantees like global gradient boundedness and unbiased sampling gradient. These features are especially useful for distributional reinforcement learning and Deep Q Networks, where the goal is to learn a distribution over future rewards. We will also construct a Gaussian Mixture Distributional Deep Q Network as a toy example to demonstrate its effectiveness. Compared with previous models, this model is parameter efficient in terms of representing a distribution and possesses better interpretability.","sentences":["The learning of Gaussian Mixture Models (also referred to simply as GMMs) plays an important role in machine learning.","Known for their expressiveness and interpretability, Gaussian mixture models have a wide range of applications, from statistics, computer vision to distributional reinforcement learning.","However, as of today, few known algorithms can fit or learn these models, some of which include Expectation-Maximization algorithms and Sliced Wasserstein Distance.","Even fewer algorithms are compatible with gradient descent, the common learning process for neural networks.   ","In this paper, we derive a closed formula of two GMMs in the univariate, one-dimensional case, then propose a distance function called Sliced Cram\\'er 2-distance for learning general multivariate GMMs.","Our approach has several advantages over many previous methods.","First, it has a closed-form expression for the univariate case and is easy to compute and implement using common machine learning libraries (e.g., PyTorch and TensorFlow).","Second, it is compatible with gradient descent, which enables us to integrate GMMs with neural networks seamlessly.","Third, it can fit a GMM not only to a set of data points, but also to another GMM directly, without sampling from the target model.","And fourth, it has some theoretical guarantees like global gradient boundedness and unbiased sampling gradient.","These features are especially useful for distributional reinforcement learning and Deep Q Networks, where the goal is to learn a distribution over future rewards.","We will also construct a Gaussian Mixture Distributional Deep Q Network as a toy example to demonstrate its effectiveness.","Compared with previous models, this model is parameter efficient in terms of representing a distribution and possesses better interpretability."],"url":"http://arxiv.org/abs/2307.06753v1"}
{"created":"2023-07-13 13:42:44","title":"(k-2)-linear connected components in hypergraphs of rank k","abstract":"We define a $q$-linear path in a hypergraph $H$ as a sequence $(e_1,\\ldots,e_L)$ of edges of $H$ such that $|e_i \\cap e_{i+1}| \\in [\\![1,q]\\!]$ and $e_i \\cap e_j=\\varnothing$ if $|i-j|>1$. In this paper, we study the connected components associated to these paths when $q=k-2$ where $k$ is the rank of $H$. If $k=3$ then $q=1$ which coincides with the well-known notion of linear path or loose path. We describe the structure of the connected components, using an algorithmic proof which shows that the connected components can be computed in polynomial time. We then mention two consequences of our algorithmic result. The first one is that deciding the winner of the Maker-Breaker game on a hypergraph of rank 3 can be done in polynomial time. The second one is that tractable cases for the NP-complete problem of \"Paths Avoiding Forbidden Pairs\" in a graph can be deduced from the recognition of a special type of line graph of a hypergraph.","sentences":["We define a $q$-linear path in a hypergraph $H$ as a sequence $(e_1,\\ldots,e_L)$ of edges of $H$ such that $|e_i \\cap e_{i+1}| \\in","[\\![1,q]\\!]$ and $e_i \\cap e_j=\\varnothing$ if $|i-j|>1$.","In this paper, we study the connected components associated to these paths when $q=k-2$ where $k$ is the rank of $H$. If $k=3$ then $q=1$ which coincides with the well-known notion of linear path or loose path.","We describe the structure of the connected components, using an algorithmic proof which shows that the connected components can be computed in polynomial time.","We then mention two consequences of our algorithmic result.","The first one is that deciding the winner of the Maker-Breaker game on a hypergraph of rank 3 can be done in polynomial time.","The second one is that tractable cases for the NP-complete problem of \"Paths Avoiding Forbidden Pairs\" in a graph can be deduced from the recognition of a special type of line graph of a hypergraph."],"url":"http://arxiv.org/abs/2307.06752v1"}
{"created":"2023-07-13 13:41:32","title":"Watch Your Pose: Unsupervised Domain Adaption with Pose based Triplet Selection for Gait Recognition","abstract":"Gait Recognition is a computer vision task aiming to identify people by their walking patterns. Existing methods show impressive results on individual datasets but lack the ability to generalize to unseen scenarios. Unsupervised Domain Adaptation (UDA) tries to adapt a model, pre-trained in a supervised manner on a source domain, to an unlabelled target domain. UDA for Gait Recognition is still in its infancy and existing works proposed solutions to limited scenarios. In this paper, we reveal a fundamental phenomenon in adaptation of gait recognition models, in which the target domain is biased to pose-based features rather than identity features, causing a significant performance drop in the identification task. We suggest Gait Orientation-based method for Unsupervised Domain Adaptation (GOUDA) to reduce this bias. To this end, we present a novel Triplet Selection algorithm with a curriculum learning framework, aiming to adapt the embedding space by pushing away samples of similar poses and bringing closer samples of different poses. We provide extensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP, GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL, showing the superiority of our proposed method over prior works.","sentences":["Gait Recognition is a computer vision task aiming to identify people by their walking patterns.","Existing methods show impressive results on individual datasets but lack the ability to generalize to unseen scenarios.","Unsupervised Domain Adaptation (UDA) tries to adapt a model, pre-trained in a supervised manner on a source domain, to an unlabelled target domain.","UDA for Gait Recognition is still in its infancy and existing works proposed solutions to limited scenarios.","In this paper, we reveal a fundamental phenomenon in adaptation of gait recognition models, in which the target domain is biased to pose-based features rather than identity features, causing a significant performance drop in the identification task.","We suggest Gait Orientation-based method for Unsupervised Domain Adaptation (GOUDA) to reduce this bias.","To this end, we present a novel Triplet Selection algorithm with a curriculum learning framework, aiming to adapt the embedding space by pushing away samples of similar poses and bringing closer samples of different poses.","We provide extensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP, GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL, showing the superiority of our proposed method over prior works."],"url":"http://arxiv.org/abs/2307.06751v1"}
{"created":"2023-07-13 13:31:25","title":"Smart Cities and Digital Twins in Lower Austria","abstract":"Smart city solutions require innovative governance approaches together with the smart use of technology, such as digital twins, by city managers and policymakers to manage the big societal challenges. The project Smart Cities aNd Digital Twins in Lower Austria (SCiNDTiLA) extends the state of the art of research in several contributing disciplines and uses the foundations of complexity theory and computational social science methods to develop a digital-twin-based smart city model. The project will also apply a novel transdisciplinary process to conceptualise sustainable smart cities and validate the smart city generic model. The outcomes will be translated into a roadmap highlighting methodologies, guidelines and policy recommendations for tackling societal challenges in smart cities with a focus on rescaling the entire framework to be transferred to regions, smaller towns and non-urban environments, such as rural areas and smart villages, in ways that fit the respective local governance, ethical and operational capacity context.","sentences":["Smart city solutions require innovative governance approaches together with the smart use of technology, such as digital twins, by city managers and policymakers to manage the big societal challenges.","The project Smart Cities aNd Digital Twins in Lower Austria (SCiNDTiLA) extends the state of the art of research in several contributing disciplines and uses the foundations of complexity theory and computational social science methods to develop a digital-twin-based smart city model.","The project will also apply a novel transdisciplinary process to conceptualise sustainable smart cities and validate the smart city generic model.","The outcomes will be translated into a roadmap highlighting methodologies, guidelines and policy recommendations for tackling societal challenges in smart cities with a focus on rescaling the entire framework to be transferred to regions, smaller towns and non-urban environments, such as rural areas and smart villages, in ways that fit the respective local governance, ethical and operational capacity context."],"url":"http://arxiv.org/abs/2307.06743v1"}
{"created":"2023-07-13 13:18:38","title":"Closeness Centralities of Lollipop Graphs","abstract":"Closeness is one of the most studied characteristics of networks. Residual closeness is a very sensitive measure of graphs robustness. Additional closeness is a measure of growth potentials of networks. In this article we calculate the closeness, vertex residual closeness, link residual closeness, and additional closeness of lollipop graphs.","sentences":["Closeness is one of the most studied characteristics of networks.","Residual closeness is a very sensitive measure of graphs robustness.","Additional closeness is a measure of growth potentials of networks.","In this article we calculate the closeness, vertex residual closeness, link residual closeness, and additional closeness of lollipop graphs."],"url":"http://arxiv.org/abs/2307.06738v1"}
{"created":"2023-07-13 13:17:50","title":"Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data","abstract":"Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios. We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and poses. We introduce a new method for synthetic data generation - RePoGen, RarE POses GENerator - with comprehensive control over pose and view to augment the COCO dataset. Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top-view pose estimation and significantly improves performance on the bottom-view dataset. Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodological choices and demonstrate improved performance. The code and the datasets are available on the project website.","sentences":["Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios.","We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and poses.","We introduce a new method for synthetic data generation - RePoGen, RarE POses GENerator - with comprehensive control over pose and view to augment the COCO dataset.","Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top-view pose estimation and significantly improves performance on the bottom-view dataset.","Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodological choices and demonstrate improved performance.","The code and the datasets are available on the project website."],"url":"http://arxiv.org/abs/2307.06737v1"}
{"created":"2023-07-13 13:16:01","title":"MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting","abstract":"Time series forecasting has received wide interest from existing research due to its broad applications and inherent challenging. The research challenge lies in identifying effective patterns in historical series and applying them to future forecasting. Advanced models based on point-wise connected MLP and Transformer architectures have strong fitting power, but their secondary computational complexity limits practicality. Additionally, those structures inherently disrupt the temporal order, reducing the information utilization and making the forecasting process uninterpretable. To solve these problems, this paper proposes a forecasting model, MPR-Net. It first adaptively decomposes multi-scale historical series patterns using convolution operation, then constructs a pattern extension forecasting method based on the prior knowledge of pattern reproduction, and finally reconstructs future patterns into future series using deconvolution operation. By leveraging the temporal dependencies present in the time series, MPR-Net not only achieves linear time complexity, but also makes the forecasting process interpretable. By carrying out sufficient experiments on more than ten real data sets of both short and long term forecasting tasks, MPR-Net achieves the state of the art forecasting performance, as well as good generalization and robustness performance.","sentences":["Time series forecasting has received wide interest from existing research due to its broad applications and inherent challenging.","The research challenge lies in identifying effective patterns in historical series and applying them to future forecasting.","Advanced models based on point-wise connected MLP and Transformer architectures have strong fitting power, but their secondary computational complexity limits practicality.","Additionally, those structures inherently disrupt the temporal order, reducing the information utilization and making the forecasting process uninterpretable.","To solve these problems, this paper proposes a forecasting model, MPR-Net.","It first adaptively decomposes multi-scale historical series patterns using convolution operation, then constructs a pattern extension forecasting method based on the prior knowledge of pattern reproduction, and finally reconstructs future patterns into future series using deconvolution operation.","By leveraging the temporal dependencies present in the time series, MPR-Net not only achieves linear time complexity, but also makes the forecasting process interpretable.","By carrying out sufficient experiments on more than ten real data sets of both short and long term forecasting tasks, MPR-Net achieves the state of the art forecasting performance, as well as good generalization and robustness performance."],"url":"http://arxiv.org/abs/2307.06736v1"}
{"created":"2023-07-13 12:37:25","title":"The Price of Equity with Binary Valuations and Few Agent Types","abstract":"In fair division problems, the notion of price of fairness measures the loss in welfare due to a fairness constraint. Prior work on the price of fairness has focused primarily on envy-freeness up to one good (EF1) as the fairness constraint, and on the utilitarian and egalitarian welfare measures. Our work instead focuses on the price of equitability up to one good (EQ1) (which we term price of equity) and considers the broad class of generalized $p$-mean welfare measures (which includes utilitarian, egalitarian, and Nash welfare as special cases). We derive fine-grained bounds on the price of equity in terms of the number of agent types (i.e., the maximum number of agents with distinct valuations), which allows us to identify scenarios where the existing bounds in terms of the number of agents are overly pessimistic.   Our work focuses on the setting with binary additive valuations, and obtains upper and lower bounds on the price of equity for $p$-mean welfare for all $p \\leqslant 1$. For any fixed $p$, our bounds are tight up to constant factors. A useful insight of our work is to identify the structure of allocations that underlie the upper (respectively, the lower) bounds simultaneously for all $p$-mean welfare measures, thus providing a unified structural understanding of price of fairness in this setting. This structural understanding, in fact, extends to the more general class of binary submodular (or matroid rank) valuations. We also show that, unlike binary additive valuations, for binary submodular valuations the number of agent types does not provide bounds on the price of equity.","sentences":["In fair division problems, the notion of price of fairness measures the loss in welfare due to a fairness constraint.","Prior work on the price of fairness has focused primarily on envy-freeness up to one good (EF1) as the fairness constraint, and on the utilitarian and egalitarian welfare measures.","Our work instead focuses on the price of equitability up to one good (EQ1) (which we term price of equity) and considers the broad class of generalized $p$-mean welfare measures (which includes utilitarian, egalitarian, and Nash welfare as special cases).","We derive fine-grained bounds on the price of equity in terms of the number of agent types (i.e., the maximum number of agents with distinct valuations), which allows us to identify scenarios where the existing bounds in terms of the number of agents are overly pessimistic.   ","Our work focuses on the setting with binary additive valuations, and obtains upper and lower bounds on the price of equity for $p$-mean welfare for all $p \\leqslant 1$. For any fixed $p$, our bounds are tight up to constant factors.","A useful insight of our work is to identify the structure of allocations that underlie the upper (respectively, the lower) bounds simultaneously for all $p$-mean welfare measures, thus providing a unified structural understanding of price of fairness in this setting.","This structural understanding, in fact, extends to the more general class of binary submodular (or matroid rank) valuations.","We also show that, unlike binary additive valuations, for binary submodular valuations the number of agent types does not provide bounds on the price of equity."],"url":"http://arxiv.org/abs/2307.06726v1"}
{"created":"2023-07-13 12:37:14","title":"Multimodal Object Detection in Remote Sensing","abstract":"Object detection in remote sensing is a crucial computer vision task that has seen significant advancements with deep learning techniques. However, most existing works in this area focus on the use of generic object detection and do not leverage the potential of multimodal data fusion. In this paper, we present a comparison of methods for multimodal object detection in remote sensing, survey available multimodal datasets suitable for evaluation, and discuss future directions.","sentences":["Object detection in remote sensing is a crucial computer vision task that has seen significant advancements with deep learning techniques.","However, most existing works in this area focus on the use of generic object detection and do not leverage the potential of multimodal data fusion.","In this paper, we present a comparison of methods for multimodal object detection in remote sensing, survey available multimodal datasets suitable for evaluation, and discuss future directions."],"url":"http://arxiv.org/abs/2307.06724v1"}
{"created":"2023-07-13 12:32:49","title":"Breaking 3-Factor Approximation for Correlation Clustering in Polylogarithmic Rounds","abstract":"In this paper, we study parallel algorithms for the correlation clustering problem, where every pair of two different entities is labeled with similar or dissimilar. The goal is to partition the entities into clusters to minimize the number of disagreements with the labels. Currently, all efficient parallel algorithms have an approximation ratio of at least 3. In comparison with the $1.994+\\epsilon$ ratio achieved by polynomial-time sequential algorithms [CLN22], a significant gap exists.   We propose the first poly-logarithmic depth parallel algorithm that achieves a better approximation ratio than 3. Specifically, our algorithm computes a $(2.4+\\epsilon)$-approximate solution and uses $\\tilde{O}(m^{1.5})$ work. Additionally, it can be translated into a $\\tilde{O}(m^{1.5})$-time sequential algorithm and a poly-logarithmic rounds sublinear-memory MPC algorithm with $\\tilde{O}(m^{1.5})$ total memory.   Our approach is inspired by Awerbuch, Khandekar, and Rao's [AKR12] length-constrained multi-commodity flow algorithm, where we develop an efficient parallel algorithm to solve a truncated correlation clustering linear program of Charikar, Guruswami, and Wirth [CGW05]. Then we show the solution of the truncated linear program can be rounded with a factor of at most 2.4 loss by using the framework of [CMSY15]. Such a rounding framework can then be implemented using parallel pivot-based approaches.","sentences":["In this paper, we study parallel algorithms for the correlation clustering problem, where every pair of two different entities is labeled with similar or dissimilar.","The goal is to partition the entities into clusters to minimize the number of disagreements with the labels.","Currently, all efficient parallel algorithms have an approximation ratio of at least 3.","In comparison with the $1.994+\\epsilon$ ratio achieved by polynomial-time sequential algorithms [CLN22], a significant gap exists.   ","We propose the first poly-logarithmic depth parallel algorithm that achieves a better approximation ratio than 3.","Specifically, our algorithm computes a $(2.4+\\epsilon)$-approximate solution and uses $\\tilde{O}(m^{1.5})$ work.","Additionally, it can be translated into a $\\tilde{O}(m^{1.5})$-time sequential algorithm and a poly-logarithmic rounds sublinear-memory MPC algorithm with $\\tilde{O}(m^{1.5})$ total memory.   ","Our approach is inspired by Awerbuch, Khandekar, and Rao's [AKR12] length-constrained multi-commodity flow algorithm, where we develop an efficient parallel algorithm to solve a truncated correlation clustering linear program of Charikar, Guruswami, and","Wirth [CGW05].","Then we show the solution of the truncated linear program can be rounded with a factor of at most 2.4 loss by using the framework of [CMSY15].","Such a rounding framework can then be implemented using parallel pivot-based approaches."],"url":"http://arxiv.org/abs/2307.06723v1"}
{"created":"2023-07-13 12:29:29","title":"Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative","abstract":"Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog. In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL). In RL-based DPL, dialog policies are updated according to rewards. The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations. One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL). Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse. This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator. Next, based on these analyses, we propose a method that eliminates AL from reward estimation and DPL while retaining its advantages. We evaluate our method using MultiWOZ, a multi-domain task-oriented dialog corpus.","sentences":["Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog.","In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL).","In RL-based DPL, dialog policies are updated according to rewards.","The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations.","One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL).","Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse.","This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator.","Next, based on these analyses, we propose a method that eliminates AL from reward estimation and DPL while retaining its advantages.","We evaluate our method using MultiWOZ, a multi-domain task-oriented dialog corpus."],"url":"http://arxiv.org/abs/2307.06721v1"}
{"created":"2023-07-13 12:26:27","title":"Weakly supervised marine animal detection from remote sensing images using vector-quantized variational autoencoder","abstract":"This paper studies a reconstruction-based approach for weakly-supervised animal detection from aerial images in marine environments. Such an approach leverages an anomaly detection framework that computes metrics directly on the input space, enhancing interpretability and anomaly localization compared to feature embedding methods. Building upon the success of Vector-Quantized Variational Autoencoders in anomaly detection on computer vision datasets, we adapt them to the marine animal detection domain and address the challenge of handling noisy data. To evaluate our approach, we compare it with existing methods in the context of marine animal detection from aerial image data. Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent studies in the literature. Our framework offers improved interpretability and localization of anomalies, providing valuable insights for monitoring marine ecosystems and mitigating the impact of human activities on marine animals.","sentences":["This paper studies a reconstruction-based approach for weakly-supervised animal detection from aerial images in marine environments.","Such an approach leverages an anomaly detection framework that computes metrics directly on the input space, enhancing interpretability and anomaly localization compared to feature embedding methods.","Building upon the success of Vector-Quantized Variational Autoencoders in anomaly detection on computer vision datasets, we adapt them to the marine animal detection domain and address the challenge of handling noisy data.","To evaluate our approach, we compare it with existing methods in the context of marine animal detection from aerial image data.","Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent studies in the literature.","Our framework offers improved interpretability and localization of anomalies, providing valuable insights for monitoring marine ecosystems and mitigating the impact of human activities on marine animals."],"url":"http://arxiv.org/abs/2307.06720v1"}
