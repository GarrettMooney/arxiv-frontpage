{"created":"2023-06-22","title":"Identifying and Extracting Rare Disease Phenotypes with Large Language Models","abstract":"Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark for evaluating ChatGPT's performance in these settings. We compared its performance to the traditional fine-tuning approach and conducted an in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero- and few-shot settings, respectively). Despite this, ChatGPT achieved similar or higher accuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with appropriate prompt engineering, ChatGPT has the potential to match or outperform fine-tuned language models for certain entity types with just one labeled sample. While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.","sentences":["Rare diseases (RDs) are collectively common and affect 300 million people worldwide.","Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually.","While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training.","Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot).","Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings.","To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark for evaluating ChatGPT's performance in these settings.","We compared its performance to the traditional fine-tuning approach and conducted an in-depth error analysis.","Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero- and few-shot settings, respectively).","Despite this, ChatGPT achieved similar or higher accuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of 0.776 and 0.725).","This suggests that with appropriate prompt engineering, ChatGPT has the potential to match or outperform fine-tuned language models for certain entity types with just one labeled sample.","While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations."],"url":"http://arxiv.org/abs/2306.12656v1"}
{"created":"2023-06-22","title":"Towards Explainable Evaluation Metrics for Machine Translation","abstract":"Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language explanations. We hope that our work can help catalyze and guide future research on explainable evaluation metrics and, mediately, also contribute to better and more transparent machine translation systems.","sentences":["Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models.","They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent.","To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial.","In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties.","In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4.","Finally, we contribute a vision of next-generation approaches, including natural language explanations.","We hope that our work can help catalyze and guide future research on explainable evaluation metrics and, mediately, also contribute to better and more transparent machine translation systems."],"url":"http://arxiv.org/abs/2306.13041v1"}
{"created":"2023-06-22","title":"Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling","abstract":"ChatGPT sets a new record with the fastest-growing user base, as a chatbot powered by a large language model (LLM). While it demonstrates state-of-the-art capabilities in a variety of language-generating tasks, it also raises widespread public concerns regarding its societal impact. In this paper, we utilize natural language processing approaches to investigate the public attitudes towards ChatGPT by applying sentiment analysis and topic modeling techniques to Twitter data. Our result shows that the overall sentiment is largely neutral to positive, which also holds true across different occupation groups. Among a wide range of topics mentioned in tweets, the most popular topics are Artificial Intelligence, Search Engines, Education, Writing, and Question Answering.","sentences":["ChatGPT sets a new record with the fastest-growing user base, as a chatbot powered by a large language model (LLM).","While it demonstrates state-of-the-art capabilities in a variety of language-generating tasks, it also raises widespread public concerns regarding its societal impact.","In this paper, we utilize natural language processing approaches to investigate the public attitudes towards ChatGPT by applying sentiment analysis and topic modeling techniques to Twitter data.","Our result shows that the overall sentiment is largely neutral to positive, which also holds true across different occupation groups.","Among a wide range of topics mentioned in tweets, the most popular topics are Artificial Intelligence, Search Engines, Education, Writing, and Question Answering."],"url":"http://arxiv.org/abs/2306.12951v1"}
{"created":"2023-06-22","title":"Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation","abstract":"While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding. This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks. Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good quality outputs and as an evaluator correlates moderately with human evaluations though it is prone to giving lower scores. ChatGPT also seems to be very adept at normalizing historical text. We finally test ChatGPT in a scenario with adversarially attacked and unseen source documents and find that ChatGPT is better at omission and entity swap than negating against its prior knowledge.","sentences":["While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility, information sharing, and understanding.","This paper comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation.","We build the first CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in English and German, and examine the effectiveness of popular transformer end-to-end models with different intermediate task finetuning tasks.","Additionally, we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator.","Overall, we report evaluations from humans, ChatGPT, and several recent automatic evaluation metrics where we find our intermediate task finetuned end-to-end models generate bad to moderate quality summaries; ChatGPT as a summarizer (without any finetuning) provides moderate to good quality outputs and as an evaluator correlates moderately with human evaluations though it is prone to giving lower scores.","ChatGPT also seems to be very adept at normalizing historical text.","We finally test ChatGPT in a scenario with adversarially attacked and unseen source documents and find that ChatGPT is better at omission and entity swap than negating against its prior knowledge."],"url":"http://arxiv.org/abs/2306.12916v1"}
{"created":"2023-06-22","title":"Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models","abstract":"Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension are vital.","sentences":["Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements.","Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment.","In this paper, we introduce a simple yet effective instruction tuning approach to address these issues.","By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis.","In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension are vital."],"url":"http://arxiv.org/abs/2306.12659v1"}
{"created":"2023-06-22","title":"Identifying and Extracting Rare Disease Phenotypes with Large Language Models","abstract":"Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark for evaluating ChatGPT's performance in these settings. We compared its performance to the traditional fine-tuning approach and conducted an in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero- and few-shot settings, respectively). Despite this, ChatGPT achieved similar or higher accuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with appropriate prompt engineering, ChatGPT has the potential to match or outperform fine-tuned language models for certain entity types with just one labeled sample. While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.","sentences":["Rare diseases (RDs) are collectively common and affect 300 million people worldwide.","Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually.","While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training.","Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot).","Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings.","To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark for evaluating ChatGPT's performance in these settings.","We compared its performance to the traditional fine-tuning approach and conducted an in-depth error analysis.","Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero- and few-shot settings, respectively).","Despite this, ChatGPT achieved similar or higher accuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of 0.776 and 0.725).","This suggests that with appropriate prompt engineering, ChatGPT has the potential to match or outperform fine-tuned language models for certain entity types with just one labeled sample.","While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations."],"url":"http://arxiv.org/abs/2306.12656v1"}
{"created":"2023-06-22","title":"Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective","abstract":"We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\\times$ (ConvNet-4) and 16$\\times$ (ResNet-18) faster in speed with less memory consumption of 11.6$\\times$ and 6.4$\\times$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://zeyuanyin.github.io/projects/SRe2L/.","sentences":["We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation.","The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures.","Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets.","Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively.","Our approach also outperforms MTT by approximately 52$\\times$ (ConvNet-4) and 16$\\times$ (ResNet-18) faster in speed with less memory consumption of 11.6$\\times$ and 6.4$\\times$ during data synthesis.","Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://zeyuanyin.github.io/projects/SRe2L/."],"url":"http://arxiv.org/abs/2306.13092v1"}
{"created":"2023-06-22","title":"Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting","abstract":"Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any offline RL algorithm. We further analyze that the opportunity for performance improvement over the behavior policy correlates with the positive-sided variance of the returns of the trajectories in the dataset. We empirically show that while CQL, IQL, and TD3+BC achieve only a part of this potential policy improvement, these same algorithms combined with our reweighted sampling strategy fully exploit the dataset. Furthermore, we empirically demonstrate that, despite its theoretical limitation, the approach may still be efficient in stochastic environments. The code is available at https://github.com/Improbable-AI/harness-offline-rl.","sentences":["Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy.","It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset.","We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest.","To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return.","This re-weighted sampling strategy may be combined with any offline RL algorithm.","We further analyze that the opportunity for performance improvement over the behavior policy correlates with the positive-sided variance of the returns of the trajectories in the dataset.","We empirically show that while CQL, IQL, and TD3+BC achieve only a part of this potential policy improvement, these same algorithms combined with our reweighted sampling strategy fully exploit the dataset.","Furthermore, we empirically demonstrate that, despite its theoretical limitation, the approach may still be efficient in stochastic environments.","The code is available at https://github.com/Improbable-AI/harness-offline-rl."],"url":"http://arxiv.org/abs/2306.13085v1"}
{"created":"2023-06-22","title":"A Comparison of Time-based Models for Multimodal Emotion Recognition","abstract":"Emotion recognition has become an important research topic in the field of human-computer interaction. Studies on sound and videos to understand emotions focused mainly on analyzing facial expressions and classified 6 basic emotions. In this study, the performance of different sequence models in multi-modal emotion recognition was compared. The sound and images were first processed by multi-layered CNN models, and the outputs of these models were fed into various sequence models. The sequence model is GRU, Transformer, LSTM and Max Pooling. Accuracy, precision, and F1 Score values of all models were calculated. The multi-modal CREMA-D dataset was used in the experiments. As a result of the comparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed the best result in F1 score, LSTM-based architecture with 0.699 in precision metric, while sensitivity showed the best results over time with Max Pooling-based architecture with 0.620. As a result, it has been observed that the sequence models compare performances close to each other.","sentences":["Emotion recognition has become an important research topic in the field of human-computer interaction.","Studies on sound and videos to understand emotions focused mainly on analyzing facial expressions and classified 6 basic emotions.","In this study, the performance of different sequence models in multi-modal emotion recognition was compared.","The sound and images were first processed by multi-layered CNN models, and the outputs of these models were fed into various sequence models.","The sequence model is GRU, Transformer, LSTM and Max Pooling.","Accuracy, precision, and F1 Score values of all models were calculated.","The multi-modal CREMA-D dataset was used in the experiments.","As a result of the comparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed the best result in F1 score, LSTM-based architecture with 0.699 in precision metric, while sensitivity showed the best results over time with Max Pooling-based architecture with 0.620.","As a result, it has been observed that the sequence models compare performances close to each other."],"url":"http://arxiv.org/abs/2306.13076v1"}
{"created":"2023-06-22","title":"Semi-automated extraction of research topics and trends from NCI funding in radiological sciences from 2000-2020","abstract":"Investigators, funders, and the public desire knowledge on topics and trends in publicly funded research but current efforts in manual categorization are limited in scale and understanding. We developed a semi-automated approach to extract and name research topics, and applied this to \\$1.9B of NCI funding over 21 years in the radiological sciences to determine micro- and macro-scale research topics and funding trends. Our method relies on sequential clustering of existing biomedical-based word embeddings, naming using subject matter experts, and visualization to discover trends at a macroscopic scale above individual topics. We present results using 15 and 60 cluster topics, where we found that 2D projection of grant embeddings reveals two dominant axes: physics-biology and therapeutic-diagnostic. For our dataset, we found that funding for therapeutics- and physics-based research have outpaced diagnostics- and biology-based research, respectively. We hope these results may (1) give insight to funders on the appropriateness of their funding allocation, (2) assist investigators in contextualizing their work and explore neighboring research domains, and (3) allow the public to review where their tax dollars are being allocated.","sentences":["Investigators, funders, and the public desire knowledge on topics and trends in publicly funded research but current efforts in manual categorization are limited in scale and understanding.","We developed a semi-automated approach to extract and name research topics, and applied this to \\$1.9B of NCI funding over 21 years in the radiological sciences to determine micro- and macro-scale research topics and funding trends.","Our method relies on sequential clustering of existing biomedical-based word embeddings, naming using subject matter experts, and visualization to discover trends at a macroscopic scale above individual topics.","We present results using 15 and 60 cluster topics, where we found that 2D projection of grant embeddings reveals two dominant axes: physics-biology and therapeutic-diagnostic.","For our dataset, we found that funding for therapeutics- and physics-based research have outpaced diagnostics- and biology-based research, respectively.","We hope these results may (1) give insight to funders on the appropriateness of their funding allocation, (2) assist investigators in contextualizing their work and explore neighboring research domains, and (3) allow the public to review where their tax dollars are being allocated."],"url":"http://arxiv.org/abs/2306.13075v1"}
{"created":"2023-06-22","title":"Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports","abstract":"Multi-object tracking algorithms have made significant advancements due to the recent developments in object detection. However, most existing methods primarily focus on tracking pedestrians or vehicles, which exhibit relatively simple and regular motion patterns. Consequently, there is a scarcity of algorithms that address the tracking of targets with irregular or non-linear motion, such as multi-athlete tracking. Furthermore, popular tracking algorithms often rely on the Kalman filter for object motion modeling, which fails to track objects when their motion contradicts the linear motion assumption of the Kalman filter. Due to this reason, we proposed a novel online and robust multi-object tracking approach, named Iterative Scale-Up ExpansionIoU and Deep Features for multi-object tracking. Unlike conventional methods, we abandon the use of the Kalman filter and propose utilizing the iterative scale-up expansion IoU. This approach achieves superior tracking performance without requiring additional training data or adopting a more robust detector, all while maintaining a lower computational cost compared to other appearance-based methods. Our proposed method demonstrates remarkable effectiveness in tracking irregular motion objects, achieving a score of 75.3% in HOTA. It outperforms all state-of-the-art online tracking algorithms on the SportsMOT dataset, covering various kinds of sport scenarios.","sentences":["Multi-object tracking algorithms have made significant advancements due to the recent developments in object detection.","However, most existing methods primarily focus on tracking pedestrians or vehicles, which exhibit relatively simple and regular motion patterns.","Consequently, there is a scarcity of algorithms that address the tracking of targets with irregular or non-linear motion, such as multi-athlete tracking.","Furthermore, popular tracking algorithms often rely on the Kalman filter for object motion modeling, which fails to track objects when their motion contradicts the linear motion assumption of the Kalman filter.","Due to this reason, we proposed a novel online and robust multi-object tracking approach, named Iterative Scale-Up ExpansionIoU and Deep Features for multi-object tracking.","Unlike conventional methods, we abandon the use of the Kalman filter and propose utilizing the iterative scale-up expansion IoU.","This approach achieves superior tracking performance without requiring additional training data or adopting a more robust detector, all while maintaining a lower computational cost compared to other appearance-based methods.","Our proposed method demonstrates remarkable effectiveness in tracking irregular motion objects, achieving a score of 75.3% in HOTA.","It outperforms all state-of-the-art online tracking algorithms on the SportsMOT dataset, covering various kinds of sport scenarios."],"url":"http://arxiv.org/abs/2306.13074v1"}
{"created":"2023-06-22","title":"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs","abstract":"The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs). This leads to a growing need to explore the untapped area of \\emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs. Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when verbalizing their confidence; 2) Prompting strategies such as CoT, Top-K and Multi-step confidences improve calibration of verbalized confidence; 3) Consistency-based methods outperform the verbalized confidences in most cases, with particularly notable improvements on the arithmetic reasoning task; 4) Hybrid methods consistently deliver the best performance over their baselines, thereby emerging as a promising state-of-the-art approach; 5) Despite these advancements, all investigated methods continue to struggle with challenging tasks, such as those requiring professional knowledge, leaving significant scope for improvement of confidence elicitation.","sentences":["The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes.","Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs).","This leads to a growing need to explore the untapped area of \\emph{non-logit-based} approaches to estimate the uncertainty of LLMs.","Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information.","We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs.","Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when verbalizing their confidence; 2) Prompting strategies such as CoT, Top-K and Multi-step confidences improve calibration of verbalized confidence; 3) Consistency-based methods outperform the verbalized confidences in most cases, with particularly notable improvements on the arithmetic reasoning task; 4) Hybrid methods consistently deliver the best performance over their baselines, thereby emerging as a promising state-of-the-art approach; 5) Despite these advancements, all investigated methods continue to struggle with challenging tasks, such as those requiring professional knowledge, leaving significant scope for improvement of confidence elicitation."],"url":"http://arxiv.org/abs/2306.13063v1"}
{"created":"2023-06-22","title":"CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions","abstract":"Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be leveraged as baselines for our task. We further demonstrate that these automatic systems can be used for practical pre-test evaluation tasks such as detecting underperforming distractors, where our detection systems can automatically identify poor distractors that few candidates select. We release the data publicly for future research.","sentences":["Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks.","Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams.","This process is currently quite manual, which can lead to time lags in the question development cycle.","Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information.","In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions.","We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be leveraged as baselines for our task.","We further demonstrate that these automatic systems can be used for practical pre-test evaluation tasks such as detecting underperforming distractors, where our detection systems can automatically identify poor distractors that few candidates select.","We release the data publicly for future research."],"url":"http://arxiv.org/abs/2306.13047v1"}
{"created":"2023-06-22","title":"Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things","abstract":"This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Machine Learning (ML) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning. The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness. The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection. This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection. The approach is experimentally evaluated on public datasets and compared with well-known ML models, showing that this SSID framework is very useful and advantageous as an accurate and online learning ML-based IDS for IoT systems.","sentences":["This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Machine Learning (ML) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning.","The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness.","The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection.","This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection.","The approach is experimentally evaluated on public datasets and compared with well-known ML models, showing that this SSID framework is very useful and advantageous as an accurate and online learning ML-based IDS for IoT systems."],"url":"http://arxiv.org/abs/2306.13030v1"}
{"created":"2023-06-22","title":"Decentralized Online Federated G-Network Learning for Lightweight Intrusion Detection","abstract":"Cyberattacks are increasingly threatening networked systems, often with the emergence of new types of unknown (zero-day) attacks and the rise of vulnerable devices. While Machine Learning (ML)-based Intrusion Detection Systems (IDSs) have been shown to be extremely promising in detecting these attacks, the need to learn large amounts of labelled data often limits the applicability of ML-based IDSs to cybersystems that only have access to private local data. To address this issue, this paper proposes a novel Decentralized and Online Federated Learning Intrusion Detection (DOF-ID) architecture. DOF-ID is a collaborative learning system that allows each IDS used for a cybersystem to learn from experience gained in other cybersystems in addition to its own local data without violating the data privacy of other systems. As the performance evaluation results using public Kitsune and Bot-IoT datasets show, DOF-ID significantly improves the intrusion detection performance in all collaborating nodes simultaneously with acceptable computation time for online learning.","sentences":["Cyberattacks are increasingly threatening networked systems, often with the emergence of new types of unknown (zero-day) attacks and the rise of vulnerable devices.","While Machine Learning (ML)-based Intrusion Detection Systems (IDSs) have been shown to be extremely promising in detecting these attacks, the need to learn large amounts of labelled data often limits the applicability of ML-based IDSs to cybersystems that only have access to private local data.","To address this issue, this paper proposes a novel Decentralized and Online Federated Learning Intrusion Detection (DOF-ID) architecture.","DOF-ID is a collaborative learning system that allows each IDS used for a cybersystem to learn from experience gained in other cybersystems in addition to its own local data without violating the data privacy of other systems.","As the performance evaluation results using public Kitsune and Bot-IoT datasets show, DOF-ID significantly improves the intrusion detection performance in all collaborating nodes simultaneously with acceptable computation time for online learning."],"url":"http://arxiv.org/abs/2306.13029v1"}
{"created":"2023-06-22","title":"Enhancing ReaxFF for Molecular Dynamics Simulations of Lithium-Ion Batteries: An interactive reparameterization protocol","abstract":"Lithium-ion batteries (LIBs) are crucial for the green economy, powering portable electronics, electric vehicles, and renewable energy systems. The solid-electrolyte interphase (SEI) is vital for LIB operation, performance, and safety. SEI forms due to thermal instability at the anode-electrolyte interface, with electrolyte reduction products stabilizing it as an electrochemical buffer. This article aims to enhance the parametrization of the ReaxFF force field for accurate molecular dynamics (MD) simulations of SEI in LIBs.   Focus is on Lithium Fluoride (LiF), an inorganic salt with favorable properties in the passivation layer. The protocol heavily relies on Python libraries for atomistic simulations, enabling robust automation of reparameterization steps. The proposed configurations and dataset enable the new ReaxFF to accurately represent the solid nature of LiF and improve mass transport property prediction in MD simulations. Optimized ReaxFF surpasses previous force fields by adjusting lithium diffusivity, resulting in a significant improvement in room temperature prediction by two orders of magnitude.   However, our comprehensive investigation reveals ReaxFF's strong sensitivity to the training set, challenging its ability to interpolate the potential energy surface. Consequently, the current ReaxFF formulation is suitable for modeling specific phenomena by utilizing the proposed interactive reparameterization protocol and constructing a dataset. This work is an important step towards refining ReaxFF for precise reactive MD simulations, shedding light on challenges and limitations in force field parametrization. The demonstrated limitations underscore the potential for developing more advanced force fields through our interactive reparameterization protocol, enabling accurate and comprehensive MD simulations in the future.","sentences":["Lithium-ion batteries (LIBs) are crucial for the green economy, powering portable electronics, electric vehicles, and renewable energy systems.","The solid-electrolyte interphase (SEI) is vital for LIB operation, performance, and safety.","SEI forms due to thermal instability at the anode-electrolyte interface, with electrolyte reduction products stabilizing it as an electrochemical buffer.","This article aims to enhance the parametrization of the ReaxFF force field for accurate molecular dynamics (MD) simulations of SEI in LIBs.   ","Focus is on Lithium Fluoride (LiF), an inorganic salt with favorable properties in the passivation layer.","The protocol heavily relies on Python libraries for atomistic simulations, enabling robust automation of reparameterization steps.","The proposed configurations and dataset enable the new ReaxFF to accurately represent the solid nature of LiF and improve mass transport property prediction in MD simulations.","Optimized ReaxFF surpasses previous force fields by adjusting lithium diffusivity, resulting in a significant improvement in room temperature prediction by two orders of magnitude.   ","However, our comprehensive investigation reveals ReaxFF's strong sensitivity to the training set, challenging its ability to interpolate the potential energy surface.","Consequently, the current ReaxFF formulation is suitable for modeling specific phenomena by utilizing the proposed interactive reparameterization protocol and constructing a dataset.","This work is an important step towards refining ReaxFF for precise reactive MD simulations, shedding light on challenges and limitations in force field parametrization.","The demonstrated limitations underscore the potential for developing more advanced force fields through our interactive reparameterization protocol, enabling accurate and comprehensive MD simulations in the future."],"url":"http://arxiv.org/abs/2306.13027v1"}
{"created":"2023-06-22","title":"Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things","abstract":"This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Machine Learning (ML) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning. The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness. The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection. This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection. The approach is experimentally evaluated on public datasets and compared with well-known ML models, showing that this SSID framework is very useful and advantageous as an accurate and online learning ML-based IDS for IoT systems.","sentences":["This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Machine Learning (ML) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning.","The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness.","The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection.","This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection.","The approach is experimentally evaluated on public datasets and compared with well-known ML models, showing that this SSID framework is very useful and advantageous as an accurate and online learning ML-based IDS for IoT systems."],"url":"http://arxiv.org/abs/2306.13030v1"}
{"created":"2023-06-22","title":"Identifying and Extracting Rare Disease Phenotypes with Large Language Models","abstract":"Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark for evaluating ChatGPT's performance in these settings. We compared its performance to the traditional fine-tuning approach and conducted an in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero- and few-shot settings, respectively). Despite this, ChatGPT achieved similar or higher accuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with appropriate prompt engineering, ChatGPT has the potential to match or outperform fine-tuned language models for certain entity types with just one labeled sample. While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.","sentences":["Rare diseases (RDs) are collectively common and affect 300 million people worldwide.","Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually.","While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training.","Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot).","Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings.","To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark for evaluating ChatGPT's performance in these settings.","We compared its performance to the traditional fine-tuning approach and conducted an in-depth error analysis.","Overall, fine-tuning BioClinicalBERT resulted in higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the zero- and few-shot settings, respectively).","Despite this, ChatGPT achieved similar or higher accuracy for certain entities (i.e., rare diseases and signs) in the one-shot setting (F1 of 0.776 and 0.725).","This suggests that with appropriate prompt engineering, ChatGPT has the potential to match or outperform fine-tuned language models for certain entity types with just one labeled sample.","While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations."],"url":"http://arxiv.org/abs/2306.12656v1"}
{"created":"2023-06-22","title":"Armed Conflict and Early Human Capital Accumulation: Evidence from Cameroon's Anglophone Conflict","abstract":"This paper examines the impact of the Anglophone Conflict in Cameroon on human capital accumulation. Using high-quality individual-level data on test scores and information on conflict-related violent events, a difference-in-differences design is employed to estimate the conflict's causal effects. The results show that an increase in violent events and conflict-related deaths causes a significant decline in test scores in reading and mathematics. The conflict also leads to higher rates of teacher absenteeism and reduced access to electricity in schools. These findings highlight the adverse consequences of conflict-related violence on human capital accumulation, particularly within the Anglophone subsystem. The study emphasizes the disproportionate burden faced by Anglophone pupils due to language-rooted tensions and segregated educational systems.","sentences":["This paper examines the impact of the Anglophone Conflict in Cameroon on human capital accumulation.","Using high-quality individual-level data on test scores and information on conflict-related violent events, a difference-in-differences design is employed to estimate the conflict's causal effects.","The results show that an increase in violent events and conflict-related deaths causes a significant decline in test scores in reading and mathematics.","The conflict also leads to higher rates of teacher absenteeism and reduced access to electricity in schools.","These findings highlight the adverse consequences of conflict-related violence on human capital accumulation, particularly within the Anglophone subsystem.","The study emphasizes the disproportionate burden faced by Anglophone pupils due to language-rooted tensions and segregated educational systems."],"url":"http://arxiv.org/abs/2306.13070v1"}
{"created":"2023-06-22","title":"CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions","abstract":"Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be leveraged as baselines for our task. We further demonstrate that these automatic systems can be used for practical pre-test evaluation tasks such as detecting underperforming distractors, where our detection systems can automatically identify poor distractors that few candidates select. We release the data publicly for future research.","sentences":["Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks.","Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams.","This process is currently quite manual, which can lead to time lags in the question development cycle.","Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information.","In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions.","We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be leveraged as baselines for our task.","We further demonstrate that these automatic systems can be used for practical pre-test evaluation tasks such as detecting underperforming distractors, where our detection systems can automatically identify poor distractors that few candidates select.","We release the data publicly for future research."],"url":"http://arxiv.org/abs/2306.13047v1"}
{"created":"2023-06-22","title":"GT-TSCH: Game-Theoretic Distributed TSCH Scheduler for Low-Power IoT Networks","abstract":"Time-Slotted Channel Hopping (TSCH) is a synchronous medium access mode of the IEEE 802.15.4e standard designed for providing low-latency and highly-reliable end-to-end communication. TSCH constructs a communication schedule by combining frequency channel hopping with Time Division Multiple Access (TDMA). In recent years, IETF designed several standards to define general mechanisms for the implementation of TSCH. However, the problem of updating the TSCH schedule according to the changes of the wireless link quality and node's traffic load left unresolved. In this paper, we use non-cooperative game theory to propose GT-TSCH, a distributed TSCH scheduler designed for low-power IoT applications. By considering selfish behavior of nodes in packet forwarding, GT-TSCH updates the TSCH schedule in a distributed approach with low control overhead by monitoring the queue length, the place of the node in the Directed Acyclic Graph (DAG) topology, the quality of the wireless link, and the data packet generation rate. We prove the existence and uniqueness of Nash equilibrium in our game model and we find the optimal number of TSCH Tx timeslots to update the TSCH slotframe. To examine the performance of our contribution, we implement GT-TSCH on Zolertia Firefly IoT motes and the Contiki-NG Operating System (OS). The evaluation results reveal that GT-TSCH improves performance in terms of throughput and end-to-end delay compared to the state-of-the-art method.","sentences":["Time-Slotted Channel Hopping (TSCH) is a synchronous medium access mode of the IEEE 802.15.4e standard designed for providing low-latency and highly-reliable end-to-end communication.","TSCH constructs a communication schedule by combining frequency channel hopping with Time Division Multiple Access (TDMA).","In recent years, IETF designed several standards to define general mechanisms for the implementation of TSCH.","However, the problem of updating the TSCH schedule according to the changes of the wireless link quality and node's traffic load left unresolved.","In this paper, we use non-cooperative game theory to propose GT-TSCH, a distributed TSCH scheduler designed for low-power IoT applications.","By considering selfish behavior of nodes in packet forwarding, GT-TSCH updates the TSCH schedule in a distributed approach with low control overhead by monitoring the queue length, the place of the node in the Directed Acyclic Graph (DAG) topology, the quality of the wireless link, and the data packet generation rate.","We prove the existence and uniqueness of Nash equilibrium in our game model and we find the optimal number of TSCH Tx timeslots to update the TSCH slotframe.","To examine the performance of our contribution, we implement GT-TSCH on Zolertia Firefly IoT motes and the Contiki-NG Operating System (OS).","The evaluation results reveal that GT-TSCH improves performance in terms of throughput and end-to-end delay compared to the state-of-the-art method."],"url":"http://arxiv.org/abs/2306.13039v1"}
{"created":"2023-06-22","title":"Magnetic Dirac semimetal state of (Mn,Ge)Bi$_2$Te$_4$","abstract":"For quantum electronics, the possibility to finely tune the properties of magnetic topological insulators (TIs) is a key issue. We studied solid solutions between two isostructural Z$_2$ TIs, magnetic MnBi$_2$Te$_4$ and nonmagnetic GeBi$_2$Te$_4$, with Z$_2$ invariants of 1;000 and 1;001, respectively. For high-quality, large mixed crystals of Ge$_x$Mn$_{1-x}$Bi$_2$Te$_4$, we observed linear x-dependent magnetic properties, composition-independent pairwise exchange interactions along with an easy magnetization axis. The bulk band gap gradually decreases to zero for $x$ from 0 to 0.4, before reopening for $x>0.6$, evidencing topological phase transitions (TPTs) between topologically nontrivial phases and the semimetal state. The TPTs are driven purely by the variation of orbital contributions. By tracing the x-dependent $6p$ contribution to the states near the fundamental gap, the effective spin-orbit coupling variation is extracted. As $x$ varies, the maximum of this contribution switches from the valence to the conduction band, thereby driving two TPTs. The gapless state observed at $x=0.42$ closely resembles a Dirac semimetal above the Neel temperature and shows a magnetic gap below, which is clearly visible in raw photoemission data. The observed behavior of the Ge$_x$Mn$_{1-x}$Bi$_2$Te$_4$ system thereby demonstrates an ability to precisely control topological and magnetic properties of TIs.","sentences":["For quantum electronics, the possibility to finely tune the properties of magnetic topological insulators (TIs) is a key issue.","We studied solid solutions between two isostructural Z$_2$ TIs, magnetic MnBi$_2$Te$_4$ and nonmagnetic GeBi$_2$Te$_4$, with Z$_2$ invariants of 1;000 and 1;001, respectively.","For high-quality, large mixed crystals of Ge$_x$Mn$_{1-x}$Bi$_2$Te$_4$, we observed linear x-dependent magnetic properties, composition-independent pairwise exchange interactions along with an easy magnetization axis.","The bulk band gap gradually decreases to zero for $x$ from 0 to 0.4, before reopening for $x>0.6$, evidencing topological phase transitions (TPTs) between topologically nontrivial phases and the semimetal state.","The TPTs are driven purely by the variation of orbital contributions.","By tracing the x-dependent $6p$ contribution to the states near the fundamental gap, the effective spin-orbit coupling variation is extracted.","As $x$ varies, the maximum of this contribution switches from the valence to the conduction band, thereby driving two TPTs.","The gapless state observed at $x=0.42$ closely resembles a Dirac semimetal above the Neel temperature and shows a magnetic gap below, which is clearly visible in raw photoemission data.","The observed behavior of the Ge$_x$Mn$_{1-x}$Bi$_2$Te$_4$ system thereby demonstrates an ability to precisely control topological and magnetic properties of TIs."],"url":"http://arxiv.org/abs/2306.13024v1"}
{"created":"2023-06-22","title":"Multi-Objective Hull Form Optimization with CAD Engine-based Deep Learning Physics for 3D Flow Prediction","abstract":"In this work, we propose a built-in Deep Learning Physics Optimization (DLPO) framework to set up a shape optimization study of the Duisburg Test Case (DTC) container vessel. We present two different applications: (1) sensitivity analysis to detect the most promising generic basis hull shapes, and (2) multi-objective optimization to quantify the trade-off between optimal hull forms. DLPO framework allows for the evaluation of design iterations automatically in an end-to-end manner. We achieved these results by coupling Extrality's Deep Learning Physics (DLP) model to a CAD engine and an optimizer. Our proposed DLP model is trained on full 3D volume data coming from RANS simulations, and it can provide accurate and high-quality 3D flow predictions in real-time, which makes it a good evaluator to perform optimization of new container vessel designs w.r.t the hydrodynamic efficiency. In particular, it is able to recover the forces acting on the vessel by integration on the hull surface with a mean relative error of 3.84\\% \\pm 2.179\\% on the total resistance. Each iteration takes only 20 seconds, thus leading to a drastic saving of time and engineering efforts, while delivering valuable insight into the performance of the vessel, including RANS-like detailed flow information. We conclude that DLPO framework is a promising tool to accelerate the ship design process and lead to more efficient ships with better hydrodynamic performance.","sentences":["In this work, we propose a built-in Deep Learning Physics Optimization (DLPO) framework to set up a shape optimization study of the Duisburg Test Case (DTC) container vessel.","We present two different applications: (1) sensitivity analysis to detect the most promising generic basis hull shapes, and (2) multi-objective optimization to quantify the trade-off between optimal hull forms.","DLPO framework allows for the evaluation of design iterations automatically in an end-to-end manner.","We achieved these results by coupling Extrality's Deep Learning Physics (DLP) model to a CAD engine and an optimizer.","Our proposed DLP model is trained on full 3D volume data coming from RANS simulations, and it can provide accurate and high-quality 3D flow predictions in real-time, which makes it a good evaluator to perform optimization of new container vessel designs w.r.t the hydrodynamic efficiency.","In particular, it is able to recover the forces acting on the vessel by integration on the hull surface with a mean relative error of 3.84\\% \\pm 2.179\\% on the total resistance.","Each iteration takes only 20 seconds, thus leading to a drastic saving of time and engineering efforts, while delivering valuable insight into the performance of the vessel, including RANS-like detailed flow information.","We conclude that DLPO framework is a promising tool to accelerate the ship design process and lead to more efficient ships with better hydrodynamic performance."],"url":"http://arxiv.org/abs/2306.12915v1"}
{"created":"2023-06-22","title":"Data Architecture for Digital Object Space Management Service (DOSM) using DAT","abstract":"The Internet of Things (IoT) data and social media data are two of the fastest-growing data segments. Having high-quality data is crucial for making informed business decisions. The strategic process of leveraging insights from data is known as data-driven decision-making. To achieve this, it is necessary to collect, store, analyze, and protect data in the best ways possible. Data architecture is a complex task that involves describing the flow of data from its source to its destination and creating a blueprint for managing the data to meet business needs for information. In this paper, we utilize the Data Architecture Tool (DAT) to model data for Digital Space Management Service, which was developed as part of the VASARI project. This work focuses on describing the movement of data, data formats, data location, data processing (batch or real-time), data storage technologies, and main operations on the data.","sentences":["The Internet of Things (IoT) data and social media data are two of the fastest-growing data segments.","Having high-quality data is crucial for making informed business decisions.","The strategic process of leveraging insights from data is known as data-driven decision-making.","To achieve this, it is necessary to collect, store, analyze, and protect data in the best ways possible.","Data architecture is a complex task that involves describing the flow of data from its source to its destination and creating a blueprint for managing the data to meet business needs for information.","In this paper, we utilize the Data Architecture Tool (DAT) to model data for Digital Space Management Service, which was developed as part of the VASARI project.","This work focuses on describing the movement of data, data formats, data location, data processing (batch or real-time), data storage technologies, and main operations on the data."],"url":"http://arxiv.org/abs/2306.12909v1"}
{"created":"2023-06-22","title":"MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning","abstract":"In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.","sentences":["In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms.","Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD.","We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score).","Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family.","However, WORLD needs additional data like F0.","Finally, using perceptual loss in discriminators based on STOI could improve the quality more.","WebMUSHRA-based subjective tests also show the quality of the proposed approach."],"url":"http://arxiv.org/abs/2306.12785v1"}
{"created":"2023-06-22","title":"Fantastic Fits with fantasy of Active Galactic Nuclei Spectra -- Exploring the Fe II emission near the H$\u03b1$ line","abstract":"In this study, a refined approach for multicomponent fitting of active galactic nuclei (AGN) spectra is presented utilizing the newly developed Python code $fantasy$ (fully automated python tool for AGN spectra analysis). AGN spectra are modeled by simultaneously considering the underlying broken power-law continuum, predefined emission line lists, and an Fe II model, which is here extended to cover the wavelength range 3700 - 11000 A. The Fe II model, founded solely on atomic data, effectively describes the extensive emission of the complex iron ion in the vicinity of the H$\\gamma$ and H$\\beta$ lines, as well as near the H$\\alpha$ line, which was previously rarely studied. The proposed spectral fitting approach is tested on a sample of high-quality AGN spectra from the Sloan Digital Sky Survey (SDSS) Data Release 17. The results indicate that when Fe II emission is present near H$\\beta$, it is also detected redward from H$\\alpha$, potentially contaminating the broad H$\\alpha$ line wings and thus affecting the measurements of its flux and width. The production of Fe II emission is found to be strongly correlated with Eddington luminosity and appears to be controlled by the similar mechanism as the hydrogen Balmer lines. The study highlights the benefits of fitting AGN type 1 spectra with the $fantasy$ code, pointing that it may be used as a robust tool for analyzing a large number of AGN spectra in the coming spectral surveys.","sentences":["In this study, a refined approach for multicomponent fitting of active galactic nuclei (AGN) spectra is presented utilizing the newly developed Python code $fantasy$ (fully automated python tool for AGN spectra analysis).","AGN spectra are modeled by simultaneously considering the underlying broken power-law continuum, predefined emission line lists, and an Fe II model, which is here extended to cover the wavelength range 3700 - 11000 A.","The Fe II model, founded solely on atomic data, effectively describes the extensive emission of the complex iron ion in the vicinity of the H$\\gamma$ and H$\\beta$ lines, as well as near the H$\\alpha$ line, which was previously rarely studied.","The proposed spectral fitting approach is tested on a sample of high-quality AGN spectra from the Sloan Digital Sky Survey (SDSS) Data Release 17.","The results indicate that when Fe II emission is present near H$\\beta$, it is also detected redward from H$\\alpha$, potentially contaminating the broad H$\\alpha$ line wings and thus affecting the measurements of its flux and width.","The production of Fe II emission is found to be strongly correlated with Eddington luminosity and appears to be controlled by the similar mechanism as the hydrogen Balmer lines.","The study highlights the benefits of fitting AGN type 1 spectra with the $fantasy$ code, pointing that it may be used as a robust tool for analyzing a large number of AGN spectra in the coming spectral surveys."],"url":"http://arxiv.org/abs/2306.12782v1"}
{"created":"2023-06-22","title":"A prior regularized full waveform inversion using generative diffusion models","abstract":"Full waveform inversion (FWI) has the potential to provide high-resolution subsurface model estimations. However, due to limitations in observation, e.g., regional noise, limited shots or receivers, and band-limited data, it is hard to obtain the desired high-resolution model with FWI. To address this challenge, we propose a new paradigm for FWI regularized by generative diffusion models. Specifically, we pre-train a diffusion model in a fully unsupervised manner on a prior velocity model distribution that represents our expectations of the subsurface and then adapt it to the seismic observations by incorporating the FWI into the sampling process of the generative diffusion models. What makes diffusion models uniquely appropriate for such an implementation is that the generative process retains the form and dimensions of the velocity model. Numerical examples demonstrate that our method can outperform the conventional FWI with only negligible additional computational cost. Even in cases of very sparse observations or observations with strong noise, the proposed method could still reconstruct a high-quality subsurface model. Thus, we can incorporate our prior expectations of the solutions in an efficient manner. We further test this approach on field data, which demonstrates the effectiveness of the proposed method.","sentences":["Full waveform inversion (FWI) has the potential to provide high-resolution subsurface model estimations.","However, due to limitations in observation, e.g., regional noise, limited shots or receivers, and band-limited data, it is hard to obtain the desired high-resolution model with FWI.","To address this challenge, we propose a new paradigm for FWI regularized by generative diffusion models.","Specifically, we pre-train a diffusion model in a fully unsupervised manner on a prior velocity model distribution that represents our expectations of the subsurface and then adapt it to the seismic observations by incorporating the FWI into the sampling process of the generative diffusion models.","What makes diffusion models uniquely appropriate for such an implementation is that the generative process retains the form and dimensions of the velocity model.","Numerical examples demonstrate that our method can outperform the conventional FWI with only negligible additional computational cost.","Even in cases of very sparse observations or observations with strong noise, the proposed method could still reconstruct a high-quality subsurface model.","Thus, we can incorporate our prior expectations of the solutions in an efficient manner.","We further test this approach on field data, which demonstrates the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2306.12776v1"}
{"created":"2023-06-22","title":"ViCTORIA project: MeerKAT HI observations of the ram pressure stripped galaxy NGC 4523","abstract":"We present the first results of a 21 cm HI line pilot observation carried out with MeerKAT in preparation for the ViCTORIA project, an untargeted survey of the Virgo galaxy cluster. The extraordinary quality of the data in terms of sensitivity and angular resolution (rms~0.65 mJy beam^-1 at ~27\"x39\" and 11 km/s resolution) allowed us to detect an extended (~10 kpc projected length) low column density (N(HI) < 2.5x10^20 cm^-2) HI gas tail associated with the dwarf irregular galaxy NGC4523 at the northern edge of the cluster. The morphology of the tail and of the stellar disc suggest that the galaxy is suffering a hydrodynamic interaction with the surrounding hot intracluster medium (ICM; ram pressure stripping). The orientation of the trailing tail, the gradient in the HI gas column density at the interface between the cold ISM and the hot ICM, the velocity of the galaxy with respect to that of the cluster, and its position indicate that NGC4523 is infalling for the first time into Virgo from the NNW background of the cluster. Using a grid of hydrodynamic simulations we derive the impact parameters with the surrounding ICM, and estimate that the galaxy will be at pericentre (D~500-600 kpc) in ~1 Gyr, where ram pressure stripping will be able to remove most, if not all, of its gas. The galaxy is located on the star formation main sequence when its star formation rate is derived using Halpha images obtained during the VESTIGE survey, suggesting that NGC4523 is only at the beginning of its interaction with the surrounding environment. A few HII regions are detected in the Halpha images within the HI gas tail outside the stellar disc. Their ages, derived by comparing their Halpha, FUV, NUV, and optical colours with the predictions of SED fitting models, are <30 Myr, and suggest that these HII regions have formed within the stripped gas.","sentences":["We present the first results of a 21 cm HI line pilot observation carried out with MeerKAT in preparation for the ViCTORIA project, an untargeted survey of the Virgo galaxy cluster.","The extraordinary quality of the data in terms of sensitivity and angular resolution (rms~0.65 mJy beam^-1 at ~27\"x39\" and 11 km/s resolution) allowed us to detect an extended (~10 kpc projected length) low column density (N(HI) <","2.5x10^20 cm^-2) HI gas tail associated with the dwarf irregular galaxy NGC4523 at the northern edge of the cluster.","The morphology of the tail and of the stellar disc suggest that the galaxy is suffering a hydrodynamic interaction with the surrounding hot intracluster medium (ICM; ram pressure stripping).","The orientation of the trailing tail, the gradient in the HI gas column density at the interface between the cold ISM and the hot ICM, the velocity of the galaxy with respect to that of the cluster, and its position indicate that NGC4523 is infalling for the first time into Virgo from the NNW background of the cluster.","Using a grid of hydrodynamic simulations we derive the impact parameters with the surrounding ICM, and estimate that the galaxy will be at pericentre (D~500-600 kpc) in ~1 Gyr, where ram pressure stripping will be able to remove most, if not all, of its gas.","The galaxy is located on the star formation main sequence when its star formation rate is derived using Halpha images obtained during the VESTIGE survey, suggesting that NGC4523 is only at the beginning of its interaction with the surrounding environment.","A few HII regions are detected in the Halpha images within the HI gas tail outside the stellar disc.","Their ages, derived by comparing their Halpha, FUV, NUV, and optical colours with the predictions of SED fitting models, are <30 Myr, and suggest that these HII regions have formed within the stripped gas."],"url":"http://arxiv.org/abs/2306.12751v1"}
{"created":"2023-06-22","title":"STAR-RIS-Assisted Privacy Protection in Semantic Communication System","abstract":"Semantic communication (SemCom) has emerged as a promising architecture in the realm of intelligent communication paradigms. SemCom involves extracting and compressing the core information at the transmitter while enabling the receiver to interpret it based on established knowledge bases (KBs). This approach enhances communication efficiency greatly. However, the open nature of wireless transmission and the presence of homogeneous KBs among subscribers of identical data type pose a risk of privacy leakage in SemCom. To address this challenge, we propose to leverage the simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) to achieve privacy protection in a SemCom system. In this system, the STAR-RIS is utilized to enhance the signal transmission of the SemCom between a base station and a destination user, as well as to covert the signal to interference specifically for the eavesdropper (Eve). Simulation results demonstrate that our generated task-level disturbance outperforms other benchmarks in protecting SemCom privacy, as evidenced by the significantly lower task success rate achieved by Eve.","sentences":["Semantic communication (SemCom) has emerged as a promising architecture in the realm of intelligent communication paradigms.","SemCom involves extracting and compressing the core information at the transmitter while enabling the receiver to interpret it based on established knowledge bases (KBs).","This approach enhances communication efficiency greatly.","However, the open nature of wireless transmission and the presence of homogeneous KBs among subscribers of identical data type pose a risk of privacy leakage in SemCom.","To address this challenge, we propose to leverage the simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) to achieve privacy protection in a SemCom system.","In this system, the STAR-RIS is utilized to enhance the signal transmission of the SemCom between a base station and a destination user, as well as to covert the signal to interference specifically for the eavesdropper (Eve).","Simulation results demonstrate that our generated task-level disturbance outperforms other benchmarks in protecting SemCom privacy, as evidenced by the significantly lower task success rate achieved by Eve."],"url":"http://arxiv.org/abs/2306.12675v1"}
