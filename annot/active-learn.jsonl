{"text":"Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0938926401,"dev-research":0.2982262014,"prompt-eng":0.4467146986,"data-quality":0.0907505903,"ml-security":0.0722008352}}
{"text":"While proprietary models such as GPT-4 and Claude have demonstrated considerable advancements in handling tens of thousands of tokens of context, open-sourced models are still in the early stages of experimentation.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.1093926187,"dev-research":0.2757470669,"prompt-eng":0.4460553737,"data-quality":0.1107092384,"ml-security":0.1393816139}}
{"text":"It also remains unclear whether developing these long context models can offer substantial gains on practical downstream tasks over retrieval-based methods or models simply trained on chunked contexts.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0218150956,"dev-research":0.2385162563,"prompt-eng":0.4132056699,"data-quality":0.1446466386,"ml-security":0.0762246307}}
{"text":"To address this challenge, we propose to institute standardized evaluation for long context language models.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.1000169964,"dev-research":0.2462801895,"prompt-eng":0.4233126333,"data-quality":0.2003889756,"ml-security":0.0565991839}}
{"text":"Concretely, we develop L-Eval which contains 411 long documents and over 2,000 query-response pairs manually annotated and checked by the authors encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.4555346053,"dev-research":0.2864731867,"prompt-eng":0.4302796865,"data-quality":0.1874783034,"ml-security":0.1236791421}}
{"text":"L-Eval also adopts diverse evaluation methods and instruction styles, enabling a more reliable assessment of Long Context Language Models (LCLMs).","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0679740864,"dev-research":0.3027032424,"prompt-eng":0.4514033362,"data-quality":0.1464047071,"ml-security":0.0985100556}}
{"text":"Our findings indicate that while open-source models typically lag behind their commercial counterparts, they still exhibit impressive performance.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.0552141946,"dev-research":0.3466524081,"prompt-eng":0.3630109139,"data-quality":0.1043884596,"ml-security":0.152521664}}
{"text":"LLaMA2 achieves the best results (win 45\\% vs turbo-16k) on open-ended tasks with only 4k context length and ChatGLM2 achieves the best results on closed-ended tasks with 8k input tokens.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.1034173828,"dev-research":0.283109664,"prompt-eng":0.4113424316,"data-quality":0.0888427596,"ml-security":0.1119814212}}
{"text":"We release our new evaluation suite, code, and all generation results including predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at {\\url{https://github.com/OpenLMLab/LEval}}.","meta":{"url":"http://arxiv.org/abs/2307.11088v1"},"cats":{"new-dataset":0.3485704544,"dev-research":0.260830799,"prompt-eng":0.4277925855,"data-quality":0.1551761471,"ml-security":0.1036312703}}
{"text":"Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains a challenge in 3D representation learning.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.1224277481,"dev-research":0.2381023649,"prompt-eng":0.375671073,"data-quality":0.1397133261,"ml-security":0.1168184186}}
{"text":"Existing point-based methods often suffer from the vanishing gradient problem or require a large number of points to accurately model scene geometry and texture.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.0328447744,"dev-research":0.201926356,"prompt-eng":0.3746778727,"data-quality":0.1824440822,"ml-security":0.0817736333}}
{"text":"To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a novel method that consists of a point-based scene representation and a differentiable renderer.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.0863363424,"dev-research":0.2322858695,"prompt-eng":0.418307308,"data-quality":0.0945276405,"ml-security":0.0454872976}}
{"text":"Our scene representation uses a point cloud where each point is characterized by its spatial position, foreground score, and view-independent feature vector.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.1633218592,"dev-research":0.2732931478,"prompt-eng":0.3932494102,"data-quality":0.1241728555,"ml-security":0.0595404087}}
{"text":"The renderer selects the relevant points for each ray and produces accurate colours using their associated features.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.0645476079,"dev-research":0.3045920747,"prompt-eng":0.4156779912,"data-quality":0.1155443791,"ml-security":0.026480031}}
{"text":"PAPR effectively learns point cloud positions to represent the correct scene geometry, even when the initialization drastically differs from the target geometry.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.0450242673,"dev-research":0.2175375189,"prompt-eng":0.4250133917,"data-quality":0.1259472703,"ml-security":0.0619227892}}
{"text":"Notably, our method captures fine texture details while using only a parsimonious set of points.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.075572341,"dev-research":0.284654518,"prompt-eng":0.4003279699,"data-quality":0.2322658712,"ml-security":0.0584946318}}
{"text":"We also demonstrate four practical applications of our method: geometry editing, object manipulation, texture transfer, and exposure control.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.0222884545,"dev-research":0.2824536442,"prompt-eng":0.4237573611,"data-quality":0.0667185912,"ml-security":0.0737632727}}
{"text":"More results and code are available on our project website at https://zvict.github.io/papr/.","meta":{"url":"http://arxiv.org/abs/2307.11086v1"},"cats":{"new-dataset":0.2698969531,"dev-research":0.2321389744,"prompt-eng":0.4167884024,"data-quality":0.1030415458,"ml-security":0.0376297076}}
{"text":"In this perspective paper, we argue that the dominant paradigm in anomaly detection cannot scale indefinitely and will eventually hit fundamental limits.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0305970436,"dev-research":0.2394773694,"prompt-eng":0.3510983317,"data-quality":0.1960293036,"ml-security":0.5581784974}}
{"text":"This is due to the a no free lunch principle for anomaly detection.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0313666403,"dev-research":0.311878174,"prompt-eng":0.3644311299,"data-quality":0.1642970678,"ml-security":0.4564971809}}
{"text":"These limitations can be overcome when there are strong tasks priors, as is the case for many industrial tasks.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0065675348,"dev-research":0.2962970827,"prompt-eng":0.4215127138,"data-quality":0.0570185314,"ml-security":0.1344802151}}
{"text":"When such priors do not exists, the task is much harder for anomaly detection.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.0196124214,"dev-research":0.2689194812,"prompt-eng":0.4097068137,"data-quality":0.2211314512,"ml-security":0.3613799004}}
{"text":"We pose two such tasks as grand challenges for anomaly detection: i) scientific discovery by anomaly detection ii) a \"mini-grand\" challenge of detecting the most anomalous image in the ImageNet dataset.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.3436680001,"dev-research":0.2484618604,"prompt-eng":0.3942072045,"data-quality":0.3130388398,"ml-security":0.3827364281}}
{"text":"We believe new anomaly detection tools and ideas would need to be developed to overcome these challenges.","meta":{"url":"http://arxiv.org/abs/2307.11085v1"},"cats":{"new-dataset":0.1196801284,"dev-research":0.4179371159,"prompt-eng":0.4079273068,"data-quality":0.1909029545,"ml-security":0.428341448}}
{"text":"Automated surgical step recognition is an important task that can significantly improve patient safety and decision-making during surgeries.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0342341704,"dev-research":0.3295720398,"prompt-eng":0.4332024403,"data-quality":0.1489284264,"ml-security":0.0736436807}}
{"text":"Existing state-of-the-art methods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0486878945,"dev-research":0.2238896293,"prompt-eng":0.3928028574,"data-quality":0.09293338,"ml-security":0.0415434517}}
{"text":"However, the benefits of joint modeling of spatio-temporal features and long-range information are not taken in account.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0372380116,"dev-research":0.2348178697,"prompt-eng":0.3056289887,"data-quality":0.0977890291,"ml-security":0.0814349839}}
{"text":"In this paper, we propose a vision transformer-based approach to jointly learn spatio-temporal features directly from sequence of frame-level patches.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.2926892556,"dev-research":0.2759663544,"prompt-eng":0.3741394352,"data-quality":0.1361980215,"ml-security":0.1008370623}}
{"text":"Our method incorporates a gated-temporal attention mechanism that intelligently combines short-term and long-term spatio-temporal feature representations.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0759194052,"dev-research":0.2664908003,"prompt-eng":0.4330197266,"data-quality":0.1153822412,"ml-security":0.0787732788}}
{"text":"We extensively evaluate our approach on two cataract surgery video datasets, namely Cataract-101 and D99, and demonstrate superior performance compared to various state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.2735326742,"dev-research":0.2600294565,"prompt-eng":0.3446330473,"data-quality":0.1214349893,"ml-security":0.0570910155}}
{"text":"These results validate the suitability of our proposed approach for automated surgical step recognition.","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.0271613933,"dev-research":0.2652725104,"prompt-eng":0.4282206748,"data-quality":0.1559742115,"ml-security":0.0471468177}}
{"text":"Our code is released at: https://github.com/nisargshah1999/GLSFormer","meta":{"url":"http://arxiv.org/abs/2307.11081v1"},"cats":{"new-dataset":0.1488621839,"dev-research":0.2166513233,"prompt-eng":0.4165180138,"data-quality":0.1591472404,"ml-security":0.0611874057}}
{"text":"The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0606942891,"dev-research":0.2242500176,"prompt-eng":0.4503435233,"data-quality":0.197657659,"ml-security":0.1647858984}}
{"text":"In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0553300766,"dev-research":0.2858389687,"prompt-eng":0.4728352087,"data-quality":0.2869149277,"ml-security":0.1867776279}}
{"text":"To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.1963447439,"dev-research":0.3235706128,"prompt-eng":0.4880794401,"data-quality":0.3891667191,"ml-security":0.1155023538}}
{"text":"AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0408948741,"dev-research":0.2925777785,"prompt-eng":0.4671549287,"data-quality":0.1207848681,"ml-security":0.074977764}}
{"text":"The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0801037626,"dev-research":0.3184852106,"prompt-eng":0.492430348,"data-quality":0.1345544811,"ml-security":0.1489736273}}
{"text":"By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised paradigm.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.1018956682,"dev-research":0.2361496718,"prompt-eng":0.544623974,"data-quality":0.1944844049,"ml-security":0.1513780472}}
{"text":"As depicted in Figure 1, extensive experiments demonstrate that AlignDet can achieve significant improvements across diverse protocols, such as detection algorithm, model backbone, data setting, and training schedule.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0668212083,"dev-research":0.315560252,"prompt-eng":0.4192960584,"data-quality":0.1623479903,"ml-security":0.1025304247}}
{"text":"For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.","meta":{"url":"http://arxiv.org/abs/2307.11077v1"},"cats":{"new-dataset":0.0679210401,"dev-research":0.3256024457,"prompt-eng":0.3562724993,"data-quality":0.1310009061,"ml-security":0.0796213201}}
{"text":"Human mesh reconstruction from a single image is challenging in the presence of occlusion, which can be caused by self, objects, or other humans.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.0540667228,"dev-research":0.2084414449,"prompt-eng":0.3681808285,"data-quality":0.0830554027,"ml-security":0.1007424361}}
{"text":"Existing methods either fail to separate human features accurately or lack proper supervision for feature completion.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.0204762543,"dev-research":0.3969256153,"prompt-eng":0.4380851796,"data-quality":0.3891762573,"ml-security":0.091744875}}
{"text":"In this paper, we propose Dense Inpainting Human Mesh Recovery (DIMR), a two-stage method that leverages dense correspondence maps to handle occlusion.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.049611854,"dev-research":0.2377638365,"prompt-eng":0.3509485501,"data-quality":0.0834389762,"ml-security":0.0483739601}}
{"text":"Our method utilizes a dense correspondence map to separate visible human features and completes human features on a structured UV map dense human with an attention-based feature completion module.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.079478537,"dev-research":0.2807666446,"prompt-eng":0.4414513981,"data-quality":0.1172562822,"ml-security":0.0539070809}}
{"text":"We also design a feature inpainting training procedure that guides the network to learn from unoccluded features.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.0448250537,"dev-research":0.3842841769,"prompt-eng":0.4442055755,"data-quality":0.2205278456,"ml-security":0.1714452313}}
{"text":"We evaluate our method on several datasets and demonstrate its superior performance under heavily occluded scenarios compared to other methods.","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.4523911438,"dev-research":0.2854659437,"prompt-eng":0.3370961531,"data-quality":0.2338584954,"ml-security":0.1529240533}}
{"text":"Extensive experiments show that our method obviously outperforms prior SOTA methods on heavily occluded images and achieves comparable results on the standard benchmarks (3DPW).","meta":{"url":"http://arxiv.org/abs/2307.11074v1"},"cats":{"new-dataset":0.0373275686,"dev-research":0.2077558645,"prompt-eng":0.3902262365,"data-quality":0.1091703916,"ml-security":0.0548504848}}
{"text":"Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.0112272589,"dev-research":0.2993071911,"prompt-eng":0.3979702272,"data-quality":0.0781741917,"ml-security":0.0417313533}}
{"text":"As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.0139798692,"dev-research":0.3533800004,"prompt-eng":0.4067569149,"data-quality":0.113095922,"ml-security":0.0654101463}}
{"text":"In this work, we formulate the newt ask of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction in context of the underlying 3D scene.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.1306316786,"dev-research":0.2890240343,"prompt-eng":0.4735091276,"data-quality":0.1765090562,"ml-security":0.0395508323}}
{"text":"To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from procedurally generated 3D scenes.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.837420682,"dev-research":0.3306126645,"prompt-eng":0.371666698,"data-quality":0.1058767342,"ml-security":0.0971528763}}
{"text":"Each example consists of an input image, editing instruction in language, and the edited image.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.20403389,"dev-research":0.3565906546,"prompt-eng":0.4790370761,"data-quality":0.166439481,"ml-security":0.0497786826}}
{"text":"We also introduce 3DIT : single and multi-task models for four editing tasks.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.1040446,"dev-research":0.3217637113,"prompt-eng":0.4302027184,"data-quality":0.0930205853,"ml-security":0.0324119997}}
{"text":"Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.1544354042,"dev-research":0.2529669263,"prompt-eng":0.4067714477,"data-quality":0.0796727664,"ml-security":0.0755274916}}
{"text":"Surprisingly, training on only synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to real-world images.","meta":{"url":"http://arxiv.org/abs/2307.11073v1"},"cats":{"new-dataset":0.0942508119,"dev-research":0.2537207383,"prompt-eng":0.3781133793,"data-quality":0.1112832642,"ml-security":0.1239550601}}
{"text":"Large scientific collaborations often have multiple scientists accessing the same set of files while doing different analyses, which create repeated accesses to the large amounts of shared data located far away.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.2702747164,"dev-research":0.3781351276,"prompt-eng":0.331539,"data-quality":0.0991268148,"ml-security":0.118777927}}
{"text":"These data accesses have long latency due to distance and occupy the limited bandwidth available over the wide-area network.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0852181014,"dev-research":0.2619445528,"prompt-eng":0.2939710823,"data-quality":0.0912655748,"ml-security":0.1194719249}}
{"text":"To reduce the wide-area network traffic and the data access latency, regional data storage caches have been installed as a new networking service.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.2212536462,"dev-research":0.3432536529,"prompt-eng":0.3057952077,"data-quality":0.1519976911,"ml-security":0.0930562551}}
{"text":"To study the effectiveness of such a cache system in scientific applications, we examine the Southern California Petabyte Scale Cache for a high-energy physics experiment.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.1015990359,"dev-research":0.2492560198,"prompt-eng":0.4037384679,"data-quality":0.0825322031,"ml-security":0.0545641007}}
{"text":"By examining about 3TB of operational logs, we show that this cache removed 67.6% of file requests from the wide-area network and reduced the traffic volume on wide-area network by 12.3TB (or 35.4%) an average day.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.1194667489,"dev-research":0.2820864003,"prompt-eng":0.3108002831,"data-quality":0.1090542263,"ml-security":0.1358503746}}
{"text":"The reduction in the traffic volume (35.4%) is less than the reduction in file counts (67.6%) because the larger files are less likely to be reused.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0585624192,"dev-research":0.3139109263,"prompt-eng":0.3167343292,"data-quality":0.1189301432,"ml-security":0.0931142004}}
{"text":"Due to this difference in data access patterns, the cache system has implemented a policy to avoid evicting smaller files when processing larger files.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0322341805,"dev-research":0.3103134221,"prompt-eng":0.3083897526,"data-quality":0.0901390104,"ml-security":0.1573485774}}
{"text":"We also build a machine learning model to study the predictability of the cache behavior.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0472698917,"dev-research":0.3124707991,"prompt-eng":0.415489408,"data-quality":0.1674271369,"ml-security":0.2748435345}}
{"text":"Tests show that this model is able to accurately predict the cache accesses, cache misses, and network throughput, making the model useful for future studies on resource provisioning and planning.","meta":{"url":"http://arxiv.org/abs/2307.11069v1"},"cats":{"new-dataset":0.0248016609,"dev-research":0.283750016,"prompt-eng":0.4046945248,"data-quality":0.0803644053,"ml-security":0.0935969375}}
{"text":"We propose a simple three-stage approach to segment unseen objects in RGB images using their CAD models.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.1246940169,"dev-research":0.1774801471,"prompt-eng":0.3797608823,"data-quality":0.1676539772,"ml-security":0.0567254768}}
{"text":"Leveraging recent powerful foundation models, DINOv2 and Segment Anything, we create descriptors and generate proposals, including binary masks for a given input RGB image.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.3818236759,"dev-research":0.2537549573,"prompt-eng":0.439918697,"data-quality":0.1285196876,"ml-security":0.1061717803}}
{"text":"By matching proposals with reference descriptors created from CAD models, we achieve precise object ID assignment along with modal masks.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.1443475543,"dev-research":0.2663031283,"prompt-eng":0.4464480578,"data-quality":0.1620800625,"ml-security":0.0823265768}}
{"text":"We experimentally demonstrate that our method achieves state-of-the-art results in CAD-based novel object segmentation, surpassing existing approaches on the seven core datasets of the BOP challenge by 19.8\\% AP using the same BOP evaluation protocol.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.3376868745,"dev-research":0.2433369869,"prompt-eng":0.4081815888,"data-quality":0.1907448414,"ml-security":0.0662537451}}
{"text":"Our source code is available at https://github.com/nv-nguyen/cnos.","meta":{"url":"http://arxiv.org/abs/2307.11067v1"},"cats":{"new-dataset":0.3410880943,"dev-research":0.2839332811,"prompt-eng":0.3974581646,"data-quality":0.1026689015,"ml-security":0.0657660151}}
{"text":"In this paper, we focus on decentralized agricultural supply chains consisting of multiple non-competing distributors satisfying the demand of their respective markets.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.1382916125,"dev-research":0.2007554787,"prompt-eng":0.3605535552,"data-quality":0.0835688038,"ml-security":0.0510399948}}
{"text":"These distributors source a single product from a farmer through an agricultural cooperative, operating in a single period.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0958396489,"dev-research":0.2633405112,"prompt-eng":0.3835734858,"data-quality":0.120399925,"ml-security":0.0645929014}}
{"text":"The agents have the ability to coordinate their actions to maximize their profits, and we use cooperative game theory to analyze cooperation among them.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0443278557,"dev-research":0.2725843349,"prompt-eng":0.4186652485,"data-quality":0.0568762618,"ml-security":0.1530024985}}
{"text":"The distributors can engage in joint ordering, increasing their order size, which leads to a decrease in the price per kilogram.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0264575346,"dev-research":0.2841674437,"prompt-eng":0.3898509792,"data-quality":0.1153334556,"ml-security":0.0691385696}}
{"text":"Additionally, distributors have the opportunity to cooperate with the farmer, securing a reduced price per kilogram at the cost price, while compensating the farmer for any kilograms not acquired in the cooperation agreement.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0253031255,"dev-research":0.2520420717,"prompt-eng":0.4134769378,"data-quality":0.103221636,"ml-security":0.0838164834}}
{"text":"We introduce multidistributor-farmer games and we prove that all the agents have incentives to cooperate.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.1247669065,"dev-research":0.2508824721,"prompt-eng":0.3852447334,"data-quality":0.0846435278,"ml-security":0.1268514709}}
{"text":"We demonstrate the existence of stable allocations, where no subgroup of agents can be better off by separating.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0275581377,"dev-research":0.1738973739,"prompt-eng":0.3664421283,"data-quality":0.1067504042,"ml-security":0.213003023}}
{"text":"Moreover, we propose and characterize a distribution of the total profit that justly compensates the contribution of the farmer in any group of distributors.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.076785472,"dev-research":0.2396540134,"prompt-eng":0.4127242303,"data-quality":0.1454930723,"ml-security":0.0976489787}}
{"text":"Finally, we explore the conditions under which the farmer can be compensated in order to maximize their revenues when cooperating with all players.","meta":{"url":"http://arxiv.org/abs/2307.11065v1"},"cats":{"new-dataset":0.0906983974,"dev-research":0.2688036034,"prompt-eng":0.4029000034,"data-quality":0.0830786864,"ml-security":0.0827679949}}
{"text":"This position paper describes the Parsl open source research software project and its various phases over seven years.","meta":{"url":"http://arxiv.org/abs/2307.11060v1"},"cats":{"new-dataset":0.5321840702,"dev-research":0.347293026,"prompt-eng":0.3849915619,"data-quality":0.0606668193,"ml-security":0.0768421067}}
{"text":"It defines four types of research software engineers (RSEs) who have been important to the project in those phases; we believe this is also applicable to other research software projects.","meta":{"url":"http://arxiv.org/abs/2307.11060v1"},"cats":{"new-dataset":0.1043811773,"dev-research":0.3641074934,"prompt-eng":0.4154349927,"data-quality":0.0589998456,"ml-security":0.0476482454}}
{"text":"In this project, we implemented an end-to-end system that takes in combined visual features of video frames from a normal camera and depth information from a cloud points scanner, and predicts driving policies (vehicle speed and steering angle).","meta":{"url":"http://arxiv.org/abs/2307.11058v1"},"cats":{"new-dataset":0.3152037909,"dev-research":0.2851480257,"prompt-eng":0.3916963303,"data-quality":0.0776203923,"ml-security":0.0971058771}}
{"text":"We verified the safety of our system by comparing the predicted results with standard behaviors by real-world experienced drivers.","meta":{"url":"http://arxiv.org/abs/2307.11058v1"},"cats":{"new-dataset":0.1032934175,"dev-research":0.3604440189,"prompt-eng":0.4188126589,"data-quality":0.1457019562,"ml-security":0.3956522499}}
{"text":"Our test results show that the predictions can be considered as accurate in at lease half of the testing cases (50% 80%, depending on the model), and using combined features improved the performance in most cases than using video frames only.","meta":{"url":"http://arxiv.org/abs/2307.11058v1"},"cats":{"new-dataset":0.0367447417,"dev-research":0.3366682463,"prompt-eng":0.4238110176,"data-quality":0.1805334371,"ml-security":0.1071635535}}
{"text":"We consider a notion of planarity for two-way finite automata and transducers, inspired by Temperley-Lieb monoids of planar diagrams.","meta":{"url":"http://arxiv.org/abs/2307.11057v1"},"cats":{"new-dataset":0.0545554745,"dev-research":0.2544139949,"prompt-eng":0.4236994498,"data-quality":0.0820376742,"ml-security":0.0670108676}}
{"text":"We show that this restriction captures star-free languages and first-order transductions.","meta":{"url":"http://arxiv.org/abs/2307.11057v1"},"cats":{"new-dataset":0.0474121299,"dev-research":0.2324056959,"prompt-eng":0.4115456526,"data-quality":0.1521371261,"ml-security":0.1417838084}}
{"text":"This article presents DataXploreFines, an innovative Shiny application that revolutionizes data exploration, analysis, and visualization.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.6127581444,"dev-research":0.3486502143,"prompt-eng":0.3488382522,"data-quality":0.0727131157,"ml-security":0.08402799}}
{"text":"The application offers functionalities for data loading, management, summarization, basic graphs, advanced analysis, and contact.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.3430412686,"dev-research":0.3623814459,"prompt-eng":0.3755369682,"data-quality":0.0683516876,"ml-security":0.0485895414}}
{"text":"Users can upload their datasets in popular formats like CSV or Excel, explore the data structure, perform manipulations, and obtain statistical summaries.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.76819004,"dev-research":0.4219060591,"prompt-eng":0.3853215335,"data-quality":0.0882946216,"ml-security":0.1298889665}}
{"text":"DataXploreFines provides a wide range of interactive visualizations, including histograms, scatter plots, bar charts, and line graphs, enabling users to identify patterns and trends.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.6174139779,"dev-research":0.3930658116,"prompt-eng":0.3655056422,"data-quality":0.0744108933,"ml-security":0.0821786901}}
{"text":"Additionally, the application offers statistical tools such as time series analysis using ARIMA and SARIMA models, forecasting, and Ljung-Box statistic.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.2113075319,"dev-research":0.2865122701,"prompt-eng":0.3795176187,"data-quality":0.0409372007,"ml-security":0.0556813525}}
{"text":"Its user-friendly interface empowers individuals from various domains, including beginners in statistics, to make informed decisions.","meta":{"url":"http://arxiv.org/abs/2307.11056v1"},"cats":{"new-dataset":0.0805686183,"dev-research":0.4739953325,"prompt-eng":0.4557340631,"data-quality":0.0868265776,"ml-security":0.1156494718}}
{"text":"Existing high-resolution satellite image forgery localization methods rely on patch-based or downsampling-based training.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.1037385173,"dev-research":0.2701417945,"prompt-eng":0.4222709197,"data-quality":0.3288268975,"ml-security":0.1943207464}}
{"text":"Both of these training methods have major drawbacks, such as inaccurate boundaries between pristine and forged regions, the generation of unwanted artifacts, etc.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.0148489046,"dev-research":0.3077362091,"prompt-eng":0.3505496406,"data-quality":0.1836543153,"ml-security":0.1982604904}}
{"text":"To tackle the aforementioned challenges, inspired by the high-resolution image segmentation literature, we propose a novel model called HRFNet to enable satellite image forgery localization effectively.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.1370138552,"dev-research":0.2346619403,"prompt-eng":0.4143250848,"data-quality":0.2728931555,"ml-security":0.164949664}}
{"text":"Specifically, equipped with shallow and deep branches, our model can successfully integrate RGB and resampling features in both global and local manners to localize forgery more accurately.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.062024536,"dev-research":0.255899085,"prompt-eng":0.431967717,"data-quality":0.2917216532,"ml-security":0.1296382273}}
{"text":"We perform various experiments to demonstrate that our method achieves the best performance, while the memory requirement and processing speed are not compromised compared to existing methods.","meta":{"url":"http://arxiv.org/abs/2307.11052v1"},"cats":{"new-dataset":0.0278127502,"dev-research":0.2763306087,"prompt-eng":0.4085307729,"data-quality":0.1172459927,"ml-security":0.1031807622}}
{"text":"Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0303008272,"dev-research":0.2308299707,"prompt-eng":0.4758270868,"data-quality":0.0636391655,"ml-security":0.124894384}}
{"text":"Solving sequential decision-making tasks requiring expansive exploration requires either careful design of reward functions or the use of novelty-seeking exploration bonuses.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0104869825,"dev-research":0.2518028831,"prompt-eng":0.4372588369,"data-quality":0.0485441396,"ml-security":0.0925022336}}
{"text":"Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0369891621,"dev-research":0.3129413582,"prompt-eng":0.5053866776,"data-quality":0.0784686072,"ml-security":0.0555359611}}
{"text":"In this work, we present a technique called Human Guided Exploration (HuGE), which uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0675797181,"dev-research":0.3140109407,"prompt-eng":0.503083439,"data-quality":0.1422742029,"ml-security":0.0741997176}}
{"text":"HuGE guides exploration for reinforcement learning not only in simulation but also in the real world, all without meticulous reward specification.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0909853605,"dev-research":0.2263475852,"prompt-eng":0.4308543873,"data-quality":0.0593493641,"ml-security":0.1388451046}}
{"text":"The key concept involves bifurcating human feedback and policy learning: human feedback steers exploration, while self-supervised learning from the exploration data yields unbiased policies.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0621670548,"dev-research":0.2534107171,"prompt-eng":0.4738495215,"data-quality":0.1446005015,"ml-security":0.1179581394}}
{"text":"This procedure can leverage noisy, asynchronous human feedback to learn policies with no hand-crafted reward design or exploration bonuses.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0387003785,"dev-research":0.2784754563,"prompt-eng":0.4928620417,"data-quality":0.1375031861,"ml-security":0.201601185}}
{"text":"HuGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.2014435716,"dev-research":0.2638473869,"prompt-eng":0.4425813911,"data-quality":0.0851407225,"ml-security":0.0863861359}}
{"text":"Moreover, this paradigm can be scaled to learning directly on real-world robots, using occasional, asynchronous feedback from human supervisors.","meta":{"url":"http://arxiv.org/abs/2307.11049v1"},"cats":{"new-dataset":0.0348410528,"dev-research":0.2481128184,"prompt-eng":0.4729208441,"data-quality":0.1017892925,"ml-security":0.179932242}}
{"text":"When has an agent converged?","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0224916732,"dev-research":0.1664672276,"prompt-eng":0.3823753482,"data-quality":0.0619016894,"ml-security":0.1154603505}}
{"text":"Standard models of the reinforcement learning problem give rise to a straightforward definition of convergence: An agent converges when its behavior or performance in each environment state stops changing.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0240054994,"dev-research":0.1866982715,"prompt-eng":0.3943874458,"data-quality":0.069403474,"ml-security":0.162248057}}
{"text":"However, as we shift the focus of our learning problem from the environment's state to the agent's state, the concept of an agent's convergence becomes significantly less clear.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0417852256,"dev-research":0.214459044,"prompt-eng":0.4104922725,"data-quality":0.096576107,"ml-security":0.2166353188}}
{"text":"In this paper, we propose two complementary accounts of agent convergence in a framing of the reinforcement learning problem that centers around bounded agents.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0421739736,"dev-research":0.1686392001,"prompt-eng":0.3843116316,"data-quality":0.0954046688,"ml-security":0.2153346716}}
{"text":"The first view says that a bounded agent has converged when the minimal number of states needed to describe the agent's future behavior cannot decrease.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0240236483,"dev-research":0.1841846728,"prompt-eng":0.36768192,"data-quality":0.0547082875,"ml-security":0.1296237696}}
{"text":"The second view says that a bounded agent has converged just when the agent's performance only changes if the agent's internal state changes.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0150636211,"dev-research":0.1998101462,"prompt-eng":0.3448501723,"data-quality":0.0632426561,"ml-security":0.1016836173}}
{"text":"We establish basic properties of these two definitions, show that they accommodate typical views of convergence in standard settings, and prove several facts about their nature and relationship.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.013538014,"dev-research":0.2414251672,"prompt-eng":0.3497979634,"data-quality":0.1201055709,"ml-security":0.0796241922}}
{"text":"We take these perspectives, definitions, and analysis to bring clarity to a central idea of the field.","meta":{"url":"http://arxiv.org/abs/2307.11044v1"},"cats":{"new-dataset":0.0731385372,"dev-research":0.3850676484,"prompt-eng":0.3453346127,"data-quality":0.1286659833,"ml-security":0.0546366015}}
{"text":"The development of simple and fast hypergraph spectral methods has been hindered by the lack of numerical algorithms for simulating heat diffusions and computing fundamental objects, such as Personalized PageRank vectors, over hypergraphs.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0494196001,"dev-research":0.2240786345,"prompt-eng":0.3472385884,"data-quality":0.1065433461,"ml-security":0.0768066013}}
{"text":"In this paper, we overcome this challenge by designing two novel algorithmic primitives.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.089004587,"dev-research":0.2427840333,"prompt-eng":0.3580203407,"data-quality":0.091477674,"ml-security":0.0866299718}}
{"text":"The first is a simple, easy-to-compute discrete-time heat diffusion that enjoys the same favorable properties as the discrete-time heat diffusion over graphs.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0246524939,"dev-research":0.2252125019,"prompt-eng":0.3732693324,"data-quality":0.0705492548,"ml-security":0.086009143}}
{"text":"This diffusion can be directly applied to speed up existing hypergraph partitioning algorithms.   ","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0348184498,"dev-research":0.2382589796,"prompt-eng":0.3154803847,"data-quality":0.1247632611,"ml-security":0.0743134522}}
{"text":"Our second contribution is the novel application of mirror descent to compute resolvents of non-differentiable squared norms, which we believe to be of independent interest beyond hypergraph problems.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0410605839,"dev-research":0.1960002885,"prompt-eng":0.3319640236,"data-quality":0.1361987582,"ml-security":0.1577992259}}
{"text":"Based on this new primitive, we derive the first nearly-linear-time algorithm that simulates the discrete-time heat diffusion to approximately compute resolvents of the hypergraph Laplacian operator, which include Personalized PageRank vectors and solutions to the hypergraph analogue of Laplacian systems.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0444546278,"dev-research":0.1955876372,"prompt-eng":0.3522237787,"data-quality":0.0823728518,"ml-security":0.0989291608}}
{"text":"Our algorithm runs in time that is linear in the size of the hypergraph and inversely proportional to the hypergraph spectral gap $\\lambda_G$, matching the complexity of analogous diffusion-based algorithms for the graph version of the problem.","meta":{"url":"http://arxiv.org/abs/2307.11042v1"},"cats":{"new-dataset":0.0589868123,"dev-research":0.1524349141,"prompt-eng":0.3098747206,"data-quality":0.110787076,"ml-security":0.0759672123}}
{"text":"Cyber threats, such as advanced persistent threats (APTs), ransomware, and zero-day exploits, are rapidly evolving and demand improved security measures.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.1056044066,"dev-research":0.4097259905,"prompt-eng":0.3938843023,"data-quality":0.0668556971,"ml-security":0.6609953993}}
{"text":"Honeypots and honeynets, as deceptive systems, offer valuable insights into attacker behavior, helping researchers and practitioners develop innovative defense strategies and enhance detection mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.047826428,"dev-research":0.3704232595,"prompt-eng":0.4013699341,"data-quality":0.1654697364,"ml-security":0.7516890067}}
{"text":"However, their deployment involves significant maintenance and overhead expenses.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0278812307,"dev-research":0.3856684172,"prompt-eng":0.39205727,"data-quality":0.0997440353,"ml-security":0.0666557327}}
{"text":"At the same time, the complexity of modern computing has prompted the rise of autonomic computing, aiming for systems that can operate without human intervention.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0261838049,"dev-research":0.349941233,"prompt-eng":0.4444892835,"data-quality":0.0724187969,"ml-security":0.1145039203}}
{"text":"Recent honeypot and honeynet research claims to incorporate autonomic computing principles, often using terms like adaptive, dynamic, intelligent, and learning.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0239030344,"dev-research":0.301151392,"prompt-eng":0.4299752542,"data-quality":0.1099730337,"ml-security":0.2701454905}}
{"text":"This study investigates such claims by measuring the extent to which autonomic principles principles are expressed in honeypot and honeynet literature.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0278916547,"dev-research":0.2746707798,"prompt-eng":0.4226951887,"data-quality":0.1119999219,"ml-security":0.2012556058}}
{"text":"The findings reveal that autonomic computing keywords are present in the literature sample, suggesting an evolution from self-adaptation to autonomic computing implementations.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0205144712,"dev-research":0.3341741063,"prompt-eng":0.4571551563,"data-quality":0.1410377332,"ml-security":0.0925538745}}
{"text":"Yet, despite these findings, the analysis also shows low frequencies of self-configuration, self-healing, and self-protection keywords.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0519774636,"dev-research":0.3089828176,"prompt-eng":0.3912652119,"data-quality":0.1651658046,"ml-security":0.1940676518}}
{"text":"Interestingly, self-optimization appeared prominently in the literature.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0114507614,"dev-research":0.2696869305,"prompt-eng":0.4169917916,"data-quality":0.0865812622,"ml-security":0.132551396}}
{"text":"While this study presents a foundation for the convergence of autonomic computing and deceptive systems, future research could explore technical implementations in sample articles and test them for autonomic behavior.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.012394901,"dev-research":0.3158532488,"prompt-eng":0.4707390243,"data-quality":0.1689056098,"ml-security":0.3176706686}}
{"text":"Additionally, investigations into the design and implementation of individual autonomic computing principles in honeypots and determining the necessary ratio of these principles for a system to exhibit autonomic behavior could provide valuable insights for both researchers and practitioners.","meta":{"url":"http://arxiv.org/abs/2307.11038v1"},"cats":{"new-dataset":0.0180146205,"dev-research":0.2756130532,"prompt-eng":0.4484892203,"data-quality":0.0800025843,"ml-security":0.2085990398}}
{"text":"Object localization in general environments is a fundamental part of vision systems.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0326206914,"dev-research":0.2677873067,"prompt-eng":0.4376444048,"data-quality":0.1954962383,"ml-security":0.1241170151}}
{"text":"While dominating on the COCO benchmark, recent Transformer-based detection methods are not competitive in diverse domains.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0689053588,"dev-research":0.2356093391,"prompt-eng":0.3992863506,"data-quality":0.2367336189,"ml-security":0.1152173008}}
{"text":"Moreover, these methods still struggle to very accurately estimate the object bounding boxes in complex environments.   ","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0466755851,"dev-research":0.2561908757,"prompt-eng":0.3487741874,"data-quality":0.1216907799,"ml-security":0.1487320868}}
{"text":"We introduce Cascade-DETR for high-quality universal object detection.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.1201596451,"dev-research":0.21195948,"prompt-eng":0.4211471852,"data-quality":0.2060549578,"ml-security":0.1181869249}}
{"text":"We jointly tackle the generalization to diverse domains and localization accuracy by proposing the Cascade Attention layer, which explicitly integrates object-centric information into the detection decoder by limiting the attention to the previous box prediction.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0707067966,"dev-research":0.2505277771,"prompt-eng":0.4253972447,"data-quality":0.2609259642,"ml-security":0.133531019}}
{"text":"To further enhance accuracy, we also revisit the scoring of queries.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.057697999,"dev-research":0.3681467534,"prompt-eng":0.4624763159,"data-quality":0.2274632298,"ml-security":0.0811590792}}
{"text":"Instead of relying on classification scores, we predict the expected IoU of the query, leading to substantially more well-calibrated confidences.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0357022466,"dev-research":0.2818164143,"prompt-eng":0.4716292076,"data-quality":0.2141040045,"ml-security":0.1968280751}}
{"text":"Lastly, we introduce a universal object detection benchmark, UDB10, that contains 10 datasets from diverse domains.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.5035790728,"dev-research":0.238139989,"prompt-eng":0.3713384738,"data-quality":0.164065739,"ml-security":0.1844896712}}
{"text":"While also advancing the state-of-the-art on COCO, Cascade-DETR substantially improves DETR-based detectors on all datasets in UDB10, even by over 10 mAP in some cases.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.3977951407,"dev-research":0.3084169212,"prompt-eng":0.4042232063,"data-quality":0.2722823549,"ml-security":0.1285881031}}
{"text":"The improvements under stringent quality requirements are even more pronounced.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.0234373902,"dev-research":0.4028491063,"prompt-eng":0.3931690283,"data-quality":0.3496102859,"ml-security":0.0526392485}}
{"text":"Our code and models will be released at https://github.com/SysCV/cascade-detr.","meta":{"url":"http://arxiv.org/abs/2307.11035v1"},"cats":{"new-dataset":0.1449855892,"dev-research":0.2505824901,"prompt-eng":0.4492874619,"data-quality":0.1156618852,"ml-security":0.1065204126}}
{"text":"Recent work has shown that language models' (LMs) prompt-based learning capabilities make them well suited for automating data labeling in domains where manual annotation is expensive.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.2398726227,"dev-research":0.303899527,"prompt-eng":0.6132391782,"data-quality":0.5511699419,"ml-security":0.1811083529}}
{"text":"The challenge is that while writing an initial prompt is cheap, improving a prompt is costly -- practitioners often require significant labeled data in order to evaluate the impact of prompt modifications.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0716621265,"dev-research":0.4740055422,"prompt-eng":0.6192954654,"data-quality":0.2625774634,"ml-security":0.1983064069}}
{"text":"Our work asks whether it is possible to improve prompt-based learning without additional labeled data.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0932283519,"dev-research":0.3152411259,"prompt-eng":0.5588059117,"data-quality":0.2213287267,"ml-security":0.2358350622}}
{"text":"We approach this problem by attempting to modify the predictions of a prompt, rather than the prompt itself.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0737173434,"dev-research":0.279197362,"prompt-eng":0.6675047392,"data-quality":0.2646948418,"ml-security":0.2336828513}}
{"text":"Our intuition is that accurate predictions should also be consistent: samples which are similar under some feature representation should receive the same prompt prediction.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0142135864,"dev-research":0.3364746932,"prompt-eng":0.5285611172,"data-quality":0.3265609766,"ml-security":0.178194106}}
{"text":"We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.1473255375,"dev-research":0.257834211,"prompt-eng":0.4065342811,"data-quality":0.4027115679,"ml-security":0.1269161574}}
{"text":"Embroid then uses these neighborhoods to create additional predictions for each sample, and combines these predictions with a simple latent variable graphical model in order to generate a final corrected prediction.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0879520035,"dev-research":0.2924285436,"prompt-eng":0.4757794347,"data-quality":0.1868005886,"ml-security":0.0571740293}}
{"text":"In addition to providing a theoretical analysis of Embroid, we conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0162312249,"dev-research":0.2455392236,"prompt-eng":0.4345710761,"data-quality":0.1286708663,"ml-security":0.045645254}}
{"text":"We find that (1) Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought), and (3) can be specialized to domains like law through the embedding functions.","meta":{"url":"http://arxiv.org/abs/2307.11031v1"},"cats":{"new-dataset":0.0196347292,"dev-research":0.3245006447,"prompt-eng":0.5606554507,"data-quality":0.105751906,"ml-security":0.0917983027}}
{"text":"Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0478778462,"dev-research":0.4308594997,"prompt-eng":0.4624357576,"data-quality":0.0931243449,"ml-security":0.0578446539}}
{"text":"Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0814731028,"dev-research":0.3078667604,"prompt-eng":0.558239593,"data-quality":0.1120020997,"ml-security":0.0699547355}}
{"text":"The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0195173954,"dev-research":0.262947798,"prompt-eng":0.4754306499,"data-quality":0.1541078561,"ml-security":0.0518506177}}
{"text":"To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0954268389,"dev-research":0.3752029679,"prompt-eng":0.4788126253,"data-quality":0.0817574895,"ml-security":0.038392249}}
{"text":"The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0433308467,"dev-research":0.4650194853,"prompt-eng":0.4408220624,"data-quality":0.0881230587,"ml-security":0.047461206}}
{"text":"This collaborative process champions the human in a dominant role, in addition to mixed and shifting levels of initiative that exist between humans and LLMs.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0330222221,"dev-research":0.3255009155,"prompt-eng":0.5015391459,"data-quality":0.0443092539,"ml-security":0.0587823572}}
{"text":"This research also reports on collaboration breakdowns that occur during this process, user perceptions of using existing LLMs during Human-AI Co-creativity, and discusses design implications to support this co-creativity process.","meta":{"url":"http://arxiv.org/abs/2307.10811v1"},"cats":{"new-dataset":0.0468261253,"dev-research":0.3928546165,"prompt-eng":0.4657285559,"data-quality":0.1026022995,"ml-security":0.0905076389}}
{"text":"VTubers, or Virtual YouTubers, are live streamers who create streaming content using animated 2D or 3D virtual avatars.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.085865352,"dev-research":0.2714500012,"prompt-eng":0.4283981529,"data-quality":0.0980033482,"ml-security":0.0917837695}}
{"text":"In recent years, there has been a significant increase in the number of VTuber creators and viewers across the globe.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.1116820194,"dev-research":0.368465931,"prompt-eng":0.4328868427,"data-quality":0.0928489758,"ml-security":0.059501068}}
{"text":"This practise has drawn research attention into topics such as viewers' engagement behaviors and perceptions, however, as animated avatars offer more identity and performance flexibility than traditional live streaming where one uses their own body, little research has focused on how this flexibility influences how creators present themselves.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.0349528524,"dev-research":0.3513836495,"prompt-eng":0.3785650302,"data-quality":0.0800508265,"ml-security":0.0766033968}}
{"text":"This research thus seeks to fill this gap by presenting results from a qualitative study of 16 Chinese-speaking VTubers' streaming practices.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.2308930828,"dev-research":0.3308645888,"prompt-eng":0.3624407176,"data-quality":0.1276844808,"ml-security":0.0622520549}}
{"text":"The data revealed that the virtual avatars that were used while live streaming afforded creators opportunities to present themselves using inflated presentations and resulted in inclusive interactions with viewers.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.1011064013,"dev-research":0.3398230872,"prompt-eng":0.3926878643,"data-quality":0.0922151587,"ml-security":0.1123943834}}
{"text":"The results also unveiled the inflated, and often sexualized, gender expressions of VTubers while they were situated in misogynistic environments.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.055854772,"dev-research":0.2419263855,"prompt-eng":0.4012554713,"data-quality":0.1859588517,"ml-security":0.1093216152}}
{"text":"The socio-technical facets of VTubing were found to potentially reduce sexual harassment and sexism, whilst also raising self-objectification concerns.","meta":{"url":"http://arxiv.org/abs/2307.11025v1"},"cats":{"new-dataset":0.0099021011,"dev-research":0.3465136759,"prompt-eng":0.4141822147,"data-quality":0.184370594,"ml-security":0.2064209959}}
{"text":"There are many techniques and tools for termination of C programs, but up to now they were not very powerful for termination proofs of programs whose termination depends on recursive data structures like lists.","meta":{"url":"http://arxiv.org/abs/2307.11024v1"},"cats":{"new-dataset":0.0949609061,"dev-research":0.3705996912,"prompt-eng":0.3796636924,"data-quality":0.1535699494,"ml-security":0.1754144081}}
{"text":"We present the first approach that extends powerful techniques for termination analysis of C programs (with memory allocation and explicit pointer arithmetic) to lists.","meta":{"url":"http://arxiv.org/abs/2307.11024v1"},"cats":{"new-dataset":0.0990634522,"dev-research":0.3773109648,"prompt-eng":0.3906261326,"data-quality":0.1541278694,"ml-security":0.2183622191}}
{"text":"Over the last half century, the main application of Brain Computer Interfaces, BCIs has been controlling wheelchairs and neural prostheses or generating text or commands for people with restricted mobility.","meta":{"url":"http://arxiv.org/abs/2307.11023v1"},"cats":{"new-dataset":0.0484044911,"dev-research":0.3185060548,"prompt-eng":0.4370775734,"data-quality":0.0699080732,"ml-security":0.0998051155}}
{"text":"There has been very limited attention in the field to applications for computer aided design, despite the potential of BCIs to provide a new form of environmental interaction.","meta":{"url":"http://arxiv.org/abs/2307.11023v1"},"cats":{"new-dataset":0.045784089,"dev-research":0.3644735822,"prompt-eng":0.4418110965,"data-quality":0.0427634329,"ml-security":0.0944847859}}
{"text":"In this paper we introduce the development and application of Neuron, a novel BCI tool that enables designers with little experience in neuroscience or computer programming to gain access to neurological data, along with established metrics relevant to design, create BCI interaction prototypes, both with digital onscreen objects and physical devices, and evaluate designs based on neurological information and record measurements for further analysis.","meta":{"url":"http://arxiv.org/abs/2307.11023v1"},"cats":{"new-dataset":0.199590485,"dev-research":0.4348090667,"prompt-eng":0.4086996368,"data-quality":0.1092536766,"ml-security":0.109088946}}
{"text":"After discussing the BCI tool development, the article presents its capabilities through two case studies, along with a brief evaluation of the tool performance and a discussion of implications, limitations, and future improvement.","meta":{"url":"http://arxiv.org/abs/2307.11023v1"},"cats":{"new-dataset":0.0621927565,"dev-research":0.3899552126,"prompt-eng":0.4081630171,"data-quality":0.0855788172,"ml-security":0.0721952675}}
{"text":"Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.05914859,"dev-research":0.3611391888,"prompt-eng":0.3813057489,"data-quality":0.0808030244,"ml-security":0.1061343311}}
{"text":"Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solving a wide range of tasks with world knowledge, including knowledge-intensive tasks.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.1739571234,"dev-research":0.2009635053,"prompt-eng":0.496952175,"data-quality":0.0905740841,"ml-security":0.0853848829}}
{"text":"However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporating retrieval augmentation.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0160241944,"dev-research":0.2651460226,"prompt-eng":0.503960136,"data-quality":0.2148537667,"ml-security":0.0771430924}}
{"text":"In this study, we present an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0538579854,"dev-research":0.2751892949,"prompt-eng":0.4674117402,"data-quality":0.168023381,"ml-security":0.0746755437}}
{"text":"Specially, we focus on three primary research questions and analyze them by examining QA performance, priori judgement and posteriori judgement of LLMs.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0285526781,"dev-research":0.2340683706,"prompt-eng":0.4579901958,"data-quality":0.0767388645,"ml-security":0.0502385399}}
{"text":"We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.033985404,"dev-research":0.2508425376,"prompt-eng":0.5603770191,"data-quality":0.143146436,"ml-security":0.1481194255}}
{"text":"Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries, thereby improving their judgemental abilities.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0165744765,"dev-research":0.2728567747,"prompt-eng":0.5266619611,"data-quality":0.1565928073,"ml-security":0.074332551}}
{"text":"Additionally, we also find that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.0095541708,"dev-research":0.3193697595,"prompt-eng":0.5197585968,"data-quality":0.16255598,"ml-security":0.0759117635}}
{"text":"The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.","meta":{"url":"http://arxiv.org/abs/2307.11019v1"},"cats":{"new-dataset":0.1927192784,"dev-research":0.1754435488,"prompt-eng":0.4606812723,"data-quality":0.1644315704,"ml-security":0.076393959}}
{"text":"Myocardial infarction (MI) is one of the most common causes of death in the world.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.1343762704,"dev-research":0.3035069888,"prompt-eng":0.3818191196,"data-quality":0.1849679377,"ml-security":0.1170041335}}
{"text":"Image-based biomarkers commonly used in the clinic, such as ejection fraction, fail to capture more complex patterns in the heart's 3D anatomy and thus limit diagnostic accuracy.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0235586744,"dev-research":0.2800502948,"prompt-eng":0.362597623,"data-quality":0.213424792,"ml-security":0.1061487565}}
{"text":"In this work, we present the multi-objective point cloud autoencoder as a novel geometric deep learning approach for explainable infarction prediction, based on multi-class 3D point cloud representations of cardiac anatomy and function.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.1143474981,"dev-research":0.2953620936,"prompt-eng":0.3422643932,"data-quality":0.0945948456,"ml-security":0.1597518984}}
{"text":"Its architecture consists of multiple task-specific branches connected by a low-dimensional latent space to allow for effective multi-objective learning of both reconstruction and MI prediction, while capturing pathology-specific 3D shape information in an interpretable latent space.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0515283354,"dev-research":0.239409346,"prompt-eng":0.406337423,"data-quality":0.0799310779,"ml-security":0.0803205871}}
{"text":"Furthermore, its hierarchical branch design with point cloud-based deep learning operations enables efficient multi-scale feature learning directly on high-resolution anatomy point clouds.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0897329015,"dev-research":0.2723232702,"prompt-eng":0.3729348861,"data-quality":0.0729934313,"ml-security":0.071336308}}
{"text":"In our experiments on a large UK Biobank dataset, the multi-objective point cloud autoencoder is able to accurately reconstruct multi-temporal 3D shapes with Chamfer distances between predicted and input anatomies below the underlying images' pixel resolution.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.2607056791,"dev-research":0.2395469958,"prompt-eng":0.3893456023,"data-quality":0.1160466029,"ml-security":0.0752187676}}
{"text":"Our method outperforms multiple machine learning and deep learning benchmarks for the task of incident MI prediction by 19% in terms of Area Under the Receiver Operating Characteristic curve.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0853251913,"dev-research":0.279416028,"prompt-eng":0.4416132556,"data-quality":0.1580088986,"ml-security":0.250000284}}
{"text":"In addition, its task-specific compact latent space exhibits easily separable control and MI clusters with clinically plausible associations between subject encodings and corresponding 3D shapes, thus demonstrating the explainability of the prediction.","meta":{"url":"http://arxiv.org/abs/2307.11017v1"},"cats":{"new-dataset":0.0357344318,"dev-research":0.2397600578,"prompt-eng":0.473896667,"data-quality":0.0790220231,"ml-security":0.1040472928}}
{"text":"Flow map learning (FML), in conjunction with deep neural networks (DNNs), has shown promises for data driven modeling of unknown dynamical systems.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.3144120104,"dev-research":0.2685227499,"prompt-eng":0.3742183089,"data-quality":0.1054844292,"ml-security":0.2875121383}}
{"text":"A remarkable feature of FML is that it is capable of producing accurate predictive models for partially observed systems, even when their exact mathematical models do not exist.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.0363581135,"dev-research":0.257696355,"prompt-eng":0.436931278,"data-quality":0.1423516621,"ml-security":0.136525927}}
{"text":"In this paper, we present an overview of the FML framework, along with the important computational details for its successful implementation.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.1544566819,"dev-research":0.2616546861,"prompt-eng":0.3944865923,"data-quality":0.0884342652,"ml-security":0.0442540365}}
{"text":"We also present a set of well defined benchmark problems for learning unknown dynamical systems.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.2082469608,"dev-research":0.2059852539,"prompt-eng":0.3715886755,"data-quality":0.1378155195,"ml-security":0.3456387469}}
{"text":"All the numerical details of these problems are presented, along with their FML results, to ensure that the problems are accessible for cross-examination and the results are reproducible.","meta":{"url":"http://arxiv.org/abs/2307.11013v1"},"cats":{"new-dataset":0.1637700889,"dev-research":0.2150304102,"prompt-eng":0.3829785638,"data-quality":0.1230195174,"ml-security":0.0537189939}}
{"text":"Deep Neural Networks~(DNNs) have been widely deployed in software to address various tasks~(e.g., autonomous driving, medical diagnosis).","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.1969685189,"dev-research":0.410675331,"prompt-eng":0.3801088397,"data-quality":0.1229489453,"ml-security":0.3058880584}}
{"text":"However, they could also produce incorrect behaviors that result in financial losses and even threaten human safety.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.0198582022,"dev-research":0.3541253127,"prompt-eng":0.3762645318,"data-quality":0.3651789506,"ml-security":0.4587169089}}
{"text":"To reveal the incorrect behaviors in DNN and repair them, DNN developers often collect rich unlabeled datasets from the natural world and label them to test the DNN models.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.2365092066,"dev-research":0.4109317883,"prompt-eng":0.4191738301,"data-quality":0.6309704108,"ml-security":0.3776504182}}
{"text":"However, properly labeling a large number of unlabeled datasets is a highly expensive and time-consuming task.   ","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.2989394761,"dev-research":0.327557987,"prompt-eng":0.3849735389,"data-quality":0.4500848289,"ml-security":0.1390110442}}
{"text":"To address the above-mentioned problem, we propose NSS, Neuron Sensitivity guided test case Selection, which can reduce the labeling time by selecting valuable test cases from unlabeled datasets.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.125350011,"dev-research":0.2991092022,"prompt-eng":0.4635746293,"data-quality":0.3373636111,"ml-security":0.2168329784}}
{"text":"NSS leverages the internal neuron's information induced by test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.0073348252,"dev-research":0.3733282912,"prompt-eng":0.4481086828,"data-quality":0.190112775,"ml-security":0.3354875991}}
{"text":"We evaluate NSS with four widely used datasets and four well-designed DNN models compared to SOTA baseline methods.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.3241877211,"dev-research":0.3037777622,"prompt-eng":0.3651376025,"data-quality":0.1567428507,"ml-security":0.1336454507}}
{"text":"The results show that NSS performs well in assessing the test cases' probability of fault triggering and model improvement capabilities.","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.0556065279,"dev-research":0.4189011772,"prompt-eng":0.4768441007,"data-quality":0.2122827366,"ml-security":0.1825663005}}
{"text":"Specifically, compared with baseline approaches, NSS obtains a higher fault detection rate~(e.g., when selecting 5\\% test case from the unlabeled dataset in MNIST \\& LeNet1 experiment, NSS can obtain 81.8\\% fault detection rate, 20\\% higher than baselines).","meta":{"url":"http://arxiv.org/abs/2307.11011v1"},"cats":{"new-dataset":0.0482706813,"dev-research":0.3752812064,"prompt-eng":0.4088216865,"data-quality":0.2645629931,"ml-security":0.1562844704}}
{"text":"Complex software can be hard to read, adapt, and maintain.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0522581975,"dev-research":0.5260491607,"prompt-eng":0.3499301692,"data-quality":0.1209502249,"ml-security":0.1250082579}}
{"text":"Refactoring it can create cleaner and self-explanatory code.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0514086956,"dev-research":0.6122173756,"prompt-eng":0.4365406167,"data-quality":0.1880552471,"ml-security":0.1509696699}}
{"text":"Refactoring tools try to guide developers towards better code, with more quality.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0334179596,"dev-research":0.6049799847,"prompt-eng":0.3851202468,"data-quality":0.1666575983,"ml-security":0.1005823554}}
{"text":"However, most of them take too long to provide feedback, support, and guidance on how developers should improve their software.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0724025715,"dev-research":0.6387020552,"prompt-eng":0.4268829754,"data-quality":0.1789394048,"ml-security":0.1510562892}}
{"text":"To reduce this problem, we explored the concept of Live Refactoring, focusing on visually suggesting and applying refactorings, in real-time.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.1012421229,"dev-research":0.4957766209,"prompt-eng":0.3941043559,"data-quality":0.1172304676,"ml-security":0.0951488403}}
{"text":"With this in mind, we developed a Live Refactoring Environment that visually identifies, recommends, and applies Extract Method refactorings.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0914246212,"dev-research":0.5181457727,"prompt-eng":0.4252869924,"data-quality":0.1330828728,"ml-security":0.1064863484}}
{"text":"To validate it, we conducted an empirical experiment.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0304832144,"dev-research":0.3598355785,"prompt-eng":0.4317383752,"data-quality":0.2391851018,"ml-security":0.1169870624}}
{"text":"Early results showed that our approach improved several code quality metrics.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0945392248,"dev-research":0.5664903283,"prompt-eng":0.3980785494,"data-quality":0.3708994204,"ml-security":0.0749163633}}
{"text":"Besides, we also concluded that our results were significantly different and better than the ones from refactoring the code manually without further help.","meta":{"url":"http://arxiv.org/abs/2307.11010v1"},"cats":{"new-dataset":0.0725809045,"dev-research":0.4512054583,"prompt-eng":0.3771723956,"data-quality":0.2683835242,"ml-security":0.0485686651}}
{"text":"Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0057880932,"dev-research":0.2879829374,"prompt-eng":0.3855475426,"data-quality":0.1460385028,"ml-security":0.4552305761}}
{"text":"Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0021850639,"dev-research":0.2193202342,"prompt-eng":0.4122428362,"data-quality":0.1734780824,"ml-security":0.2561936844}}
{"text":"This work critically examines this explanation.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0123045211,"dev-research":0.4009081056,"prompt-eng":0.3542773057,"data-quality":0.1161590839,"ml-security":0.1158574654}}
{"text":"Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize, and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0093190258,"dev-research":0.2077487292,"prompt-eng":0.3588064767,"data-quality":0.1959520476,"ml-security":0.2537518446}}
{"text":"Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0104127831,"dev-research":0.3250885891,"prompt-eng":0.398514755,"data-quality":0.2383371668,"ml-security":0.1860589657}}
{"text":"This calls for the search for other explanations for the generalization of over-parameterized neural networks.","meta":{"url":"http://arxiv.org/abs/2307.11007v1"},"cats":{"new-dataset":0.0089881625,"dev-research":0.2999170054,"prompt-eng":0.4248995306,"data-quality":0.1152301258,"ml-security":0.4272152637}}
{"text":"There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.0830785825,"dev-research":0.2085491688,"prompt-eng":0.4466288057,"data-quality":0.1450733873,"ml-security":0.099684546}}
{"text":"However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.0081424638,"dev-research":0.2302499237,"prompt-eng":0.4362635173,"data-quality":0.2703794868,"ml-security":0.11211404}}
{"text":"In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.0687883232,"dev-research":0.2494620377,"prompt-eng":0.4191701987,"data-quality":0.0947706769,"ml-security":0.0623582763}}
{"text":"In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.1209821775,"dev-research":0.3279015008,"prompt-eng":0.4262178665,"data-quality":0.1808132429,"ml-security":0.1560006177}}
{"text":"This is followed by the LM subnetwork, which makes an initial SLU prediction.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.0422099072,"dev-research":0.2139154311,"prompt-eng":0.4383724448,"data-quality":0.1217548205,"ml-security":0.105859827}}
{"text":"Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.0456024343,"dev-research":0.2222895352,"prompt-eng":0.4315200001,"data-quality":0.1526190379,"ml-security":0.1256758404}}
{"text":"Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.","meta":{"url":"http://arxiv.org/abs/2307.11005v1"},"cats":{"new-dataset":0.1421819578,"dev-research":0.2170374091,"prompt-eng":0.3764142018,"data-quality":0.1685118043,"ml-security":0.1040555216}}
{"text":"We propose new techniques for reducing communication in private federated learning without the need for setting or tuning compression rates.","meta":{"url":"http://arxiv.org/abs/2307.10999v1"},"cats":{"new-dataset":0.0697977942,"dev-research":0.2261866793,"prompt-eng":0.3551287034,"data-quality":0.1542763983,"ml-security":0.337869706}}
{"text":"Our on-the-fly methods automatically adjust the compression rate based on the error induced during training, while maintaining provable privacy guarantees through the use of secure aggregation and differential privacy.","meta":{"url":"http://arxiv.org/abs/2307.10999v1"},"cats":{"new-dataset":0.0565909676,"dev-research":0.2807924219,"prompt-eng":0.3809761815,"data-quality":0.2683878338,"ml-security":0.5411156807}}
{"text":"Our techniques are provably instance-optimal for mean estimation, meaning that they can adapt to the ``hardness of the problem\" with minimal interactivity.","meta":{"url":"http://arxiv.org/abs/2307.10999v1"},"cats":{"new-dataset":0.0200422359,"dev-research":0.2211586129,"prompt-eng":0.4015319777,"data-quality":0.1693554717,"ml-security":0.1892742102}}
{"text":"We demonstrate the effectiveness of our approach on real-world datasets by achieving favorable compression rates without the need for tuning.","meta":{"url":"http://arxiv.org/abs/2307.10999v1"},"cats":{"new-dataset":0.5198863908,"dev-research":0.2644519145,"prompt-eng":0.3164368058,"data-quality":0.2347923201,"ml-security":0.1821699708}}
{"text":"Deep learning models are usually black boxes when deployed on machine learning platforms.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0449063899,"dev-research":0.2756977676,"prompt-eng":0.3525590668,"data-quality":0.1363578205,"ml-security":0.5021575935}}
{"text":"Prior works have shown that the attributes ($e.g.$, the number of convolutional layers) of a target black-box neural network can be exposed through a sequence of queries.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0336504413,"dev-research":0.2591651373,"prompt-eng":0.3993607047,"data-quality":0.0969455603,"ml-security":0.5339292834}}
{"text":"There is a crucial limitation: these works assume the dataset used for training the target model to be known beforehand and leverage this dataset for model attribute attack.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0428554615,"dev-research":0.2690355913,"prompt-eng":0.3875751008,"data-quality":0.1906498762,"ml-security":0.7152866281}}
{"text":"However, it is difficult to access the training dataset of the target black-box model in reality.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.2177713643,"dev-research":0.2244575627,"prompt-eng":0.3334552088,"data-quality":0.1308586318,"ml-security":0.3794640099}}
{"text":"Therefore, whether the attributes of a target black-box model could be still revealed in this case is doubtful.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0155792269,"dev-research":0.2436443016,"prompt-eng":0.433012151,"data-quality":0.1899738541,"ml-security":0.4017261828}}
{"text":"In this paper, we investigate a new problem of Domain-agnostic Reverse Engineering the Attributes of a black-box target Model, called DREAM, without requiring the availability of the target model's training dataset, and put forward a general and principled framework by casting this problem as an out of distribution (OOD) generalization problem.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.1106826905,"dev-research":0.2580665027,"prompt-eng":0.4502814767,"data-quality":0.2165555198,"ml-security":0.5185236389}}
{"text":"In this way, we can learn a domain-agnostic model to inversely infer the attributes of a target black-box model with unknown training data.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0535178081,"dev-research":0.2370549793,"prompt-eng":0.4342949192,"data-quality":0.2140872074,"ml-security":0.4649848794}}
{"text":"This makes our method one of the kinds that can gracefully apply to an arbitrary domain for model attribute reverse engineering with strong generalization ability.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0361569866,"dev-research":0.3315256445,"prompt-eng":0.4722659728,"data-quality":0.2456709833,"ml-security":0.3578643373}}
{"text":"Extensive experimental studies are conducted and the results validate the superiority of our proposed method over the baselines.","meta":{"url":"http://arxiv.org/abs/2307.10997v1"},"cats":{"new-dataset":0.0121910701,"dev-research":0.3017375418,"prompt-eng":0.4136072722,"data-quality":0.1302784704,"ml-security":0.06179581}}
{"text":"This paper aims to apply a new deep learning approach to the task of generating raw audio files.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.2679840418,"dev-research":0.2905884356,"prompt-eng":0.3481154743,"data-quality":0.2250774982,"ml-security":0.1440711786}}
{"text":"It is based on diffusion models, a recent type of deep generative model.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0636125737,"dev-research":0.2317925046,"prompt-eng":0.441097337,"data-quality":0.0957178203,"ml-security":0.103021634}}
{"text":"This new type of method has recently shown outstanding results with image generation.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.170752758,"dev-research":0.2450840933,"prompt-eng":0.4370915786,"data-quality":0.1303662112,"ml-security":0.0338817302}}
{"text":"A lot of focus has been given to those models by the computer vision community.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.1080363549,"dev-research":0.3240592695,"prompt-eng":0.4098836978,"data-quality":0.0898575435,"ml-security":0.0676070766}}
{"text":"On the other hand, really few have been given for other types of applications such as music generation in waveform domain.   ","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0380181798,"dev-research":0.2764574958,"prompt-eng":0.3749781813,"data-quality":0.0941915833,"ml-security":0.0535043842}}
{"text":"In this paper the model for unconditional generating applied to music is implemented: Progressive distillation diffusion with 1D U-Net.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0173484126,"dev-research":0.1842146399,"prompt-eng":0.4008752999,"data-quality":0.1217886361,"ml-security":0.065988059}}
{"text":"Then, a comparison of different parameters of diffusion and their value in a full result is presented.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0217827138,"dev-research":0.2090056687,"prompt-eng":0.4004365256,"data-quality":0.0729736552,"ml-security":0.0707840909}}
{"text":"One big advantage of the methods implemented through this work is the fact that the model is able to deal with progressing audio processing and generating , using transformation from 1-channel 128 x 384 to 3-channel 128 x 128 mel-spectrograms and looped generation.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.0665331279,"dev-research":0.2929139362,"prompt-eng":0.4047133055,"data-quality":0.0983921012,"ml-security":0.0447737909}}
{"text":"The empirical comparisons are realized across different self-collected datasets.","meta":{"url":"http://arxiv.org/abs/2307.10994v1"},"cats":{"new-dataset":0.33293317,"dev-research":0.3622360776,"prompt-eng":0.3684713106,"data-quality":0.1962811242,"ml-security":0.0947098496}}
{"text":"Deep Learning (DL) , a variant of the neural network algorithms originally proposed in the 1980s, has made surprising progress in Artificial Intelligence (AI), ranging from language translation, protein folding, autonomous cars, and more recently human-like language models (CHATbots), all that seemed intractable until very recently.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.144936377,"dev-research":0.3172684371,"prompt-eng":0.3564865482,"data-quality":0.1230217313,"ml-security":0.219699177}}
{"text":"Despite the growing use of Deep Learning (DL) networks, little is actually understood about the learning mechanisms and representations that makes these networks effective across such a diverse range of applications.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.0336956352,"dev-research":0.3247048352,"prompt-eng":0.3486293785,"data-quality":0.1482496655,"ml-security":0.2677346223}}
{"text":"Part of the answer must be the huge scale of the architecture and of course the large scale of the data, since not much has changed since 1987.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.3065947955,"dev-research":0.3471334103,"prompt-eng":0.3277920643,"data-quality":0.0635753221,"ml-security":0.0733240726}}
{"text":"But the nature of deep learned representations remain largely unknown.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.0345582824,"dev-research":0.2438775882,"prompt-eng":0.3310884616,"data-quality":0.1751079639,"ml-security":0.2380608234}}
{"text":"Unfortunately training sets with millions or billions of tokens have unknown combinatorics and Networks with millions or billions of hidden units cannot easily be visualized and their mechanisms cannot be easily revealed.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.2442404343,"dev-research":0.2517290862,"prompt-eng":0.4250544653,"data-quality":0.2405725349,"ml-security":0.3633623478}}
{"text":"In this paper, we explore these questions with a large (1.24M weights; VGG) DL in a novel high density sample task (5 unique tokens with at minimum 500 exemplars per token) which allows us to more carefully follow the emergence of category structure and feature construction.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.1057097477,"dev-research":0.2581739708,"prompt-eng":0.4673010588,"data-quality":0.1519319988,"ml-security":0.1165459453}}
{"text":"We use various visualization methods for following the emergence of the classification and the development of the coupling of feature detectors and structures that provide a type of graphical bootstrapping, From these results we harvest some basic observations of the learning dynamics of DL and propose a new theory of complex feature construction based on our results.","meta":{"url":"http://arxiv.org/abs/2307.10991v1"},"cats":{"new-dataset":0.1093412047,"dev-research":0.3580325242,"prompt-eng":0.4180285901,"data-quality":0.1569731211,"ml-security":0.1320509387}}
{"text":"Many machine learning regression methods leverage large datasets for training predictive models.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.1301016071,"dev-research":0.3148394712,"prompt-eng":0.3762218552,"data-quality":0.1164993477,"ml-security":0.4838518609}}
{"text":"However, using large datasets may not be feasible due to computational limitations or high labelling costs.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.2561464837,"dev-research":0.2389746085,"prompt-eng":0.3100270879,"data-quality":0.1831988446,"ml-security":0.2181333361}}
{"text":"Therefore, sampling small training sets from large pools of unlabelled data points is essential to maximize model performance while maintaining computational efficiency.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.0868108846,"dev-research":0.1907946404,"prompt-eng":0.3847054975,"data-quality":0.3142604581,"ml-security":0.1979680782}}
{"text":"In this work, we study a sampling approach aimed to minimize the fill distance of the selected set.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.1306657564,"dev-research":0.1782728631,"prompt-eng":0.3637790901,"data-quality":0.1521904224,"ml-security":0.0700618703}}
{"text":"We derive an upper bound for the maximum expected prediction error that linearly depends on the training set fill distance, conditional to the knowledge of data features.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.1020249579,"dev-research":0.2382421181,"prompt-eng":0.3891230659,"data-quality":0.2504322014,"ml-security":0.2852700897}}
{"text":"For empirical validation, we perform experiments using two regression models on two datasets.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.0580174173,"dev-research":0.2727793272,"prompt-eng":0.432368337,"data-quality":0.237244163,"ml-security":0.1808346288}}
{"text":"We empirically show that selecting a training set by aiming to minimize the fill distance, thereby minimizing the bound, significantly reduces the maximum prediction error of various regression models, outperforming existing sampling approaches by a large margin.","meta":{"url":"http://arxiv.org/abs/2307.10988v1"},"cats":{"new-dataset":0.0528300366,"dev-research":0.2327218157,"prompt-eng":0.3972029001,"data-quality":0.2016036382,"ml-security":0.2896523819}}
{"text":"How should my own decisions affect my beliefs about the outcomes I expect to achieve?","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0266649191,"dev-research":0.3574931478,"prompt-eng":0.3817230686,"data-quality":0.0957496702,"ml-security":0.099963258}}
{"text":"If taking a certain action makes me view myself as a certain type of person, it might affect how I think others view me, and how I view others who are similar to me.","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0089750012,"dev-research":0.3234173595,"prompt-eng":0.3938299045,"data-quality":0.1009577432,"ml-security":0.1699054083}}
{"text":"This can influence my expected utility calculations and change which action I perceive to be best.","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0071291291,"dev-research":0.3437628625,"prompt-eng":0.4515914885,"data-quality":0.0720923065,"ml-security":0.1214207455}}
{"text":"Whether and how it should is subject to debate, with contenders for how to think about it including evidential decision theory, causal decision theory, and functional decision theory.","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0127868814,"dev-research":0.28556311,"prompt-eng":0.4004753305,"data-quality":0.1304541864,"ml-security":0.1164209752}}
{"text":"In this paper, we show that mechanised causal models can be used to characterise and differentiate the most important decision theories, and generate a taxonomy of different decision theories.","meta":{"url":"http://arxiv.org/abs/2307.10987v1"},"cats":{"new-dataset":0.0233801846,"dev-research":0.3254643855,"prompt-eng":0.4448083789,"data-quality":0.0979502144,"ml-security":0.1475215494}}
{"text":"Fair resource allocation is an important problem in many real-world scenarios, where resources such as goods and chores must be allocated among agents.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0554775693,"dev-research":0.2466112733,"prompt-eng":0.3734782815,"data-quality":0.0919394818,"ml-security":0.1667072078}}
{"text":"In this survey, we delve into the intricacies of fair allocation, focusing specifically on the challenges associated with indivisible resources.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0939837475,"dev-research":0.2440985803,"prompt-eng":0.3699672382,"data-quality":0.1071216587,"ml-security":0.1802467767}}
{"text":"We define fairness and efficiency within this context and thoroughly survey existential results, algorithms, and approximations that satisfy various fairness criteria, including envyfreeness, proportionality, MMS, and their relaxations.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0495621217,"dev-research":0.2266465423,"prompt-eng":0.3722284327,"data-quality":0.0871361182,"ml-security":0.2034032214}}
{"text":"Additionally, we discuss algorithms that achieve fairness and efficiency, such as Pareto Optimality and Utilitarian Welfare.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0327455496,"dev-research":0.2436723949,"prompt-eng":0.3758938101,"data-quality":0.0815338222,"ml-security":0.1959059013}}
{"text":"We also study the computational complexity of these algorithms, the likelihood of finding fair allocations, and the price of fairness for each fairness notion.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0292982232,"dev-research":0.205015637,"prompt-eng":0.3567386588,"data-quality":0.1081218928,"ml-security":0.2585666987}}
{"text":"We also cover mixed instances of indivisible and divisible items and investigate different valuation and allocation settings.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0944579804,"dev-research":0.2119616143,"prompt-eng":0.3835525698,"data-quality":0.1079501466,"ml-security":0.0722384314}}
{"text":"By summarizing the state-of-the-art research, this survey provides valuable insights into fair resource allocation of indivisible goods and chores, highlighting computational complexities, fairness guarantees, and trade-offs between fairness and efficiency.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0450574392,"dev-research":0.2491066288,"prompt-eng":0.3688652653,"data-quality":0.0757668988,"ml-security":0.1351912149}}
{"text":"It serves as a foundation for future advancements in this vital field.","meta":{"url":"http://arxiv.org/abs/2307.10985v1"},"cats":{"new-dataset":0.0670690875,"dev-research":0.2924743395,"prompt-eng":0.3643570615,"data-quality":0.0430151196,"ml-security":0.0462219934}}
{"text":"Reconstructing accurate 3D scenes from images is a long-standing vision task.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1321859297,"dev-research":0.2204553318,"prompt-eng":0.3683612435,"data-quality":0.1175928621,"ml-security":0.0403376273}}
{"text":"Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0498677489,"dev-research":0.1984619566,"prompt-eng":0.3402613673,"data-quality":0.0959494662,"ml-security":0.0659671048}}
{"text":"State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1462016282,"dev-research":0.1796152383,"prompt-eng":0.3385081163,"data-quality":0.1136684249,"ml-security":0.0536958278}}
{"text":"Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1009012794,"dev-research":0.1889508847,"prompt-eng":0.3043420537,"data-quality":0.1457943081,"ml-security":0.1311299329}}
{"text":"In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.2963283448,"dev-research":0.1909642673,"prompt-eng":0.3287082172,"data-quality":0.1496815126,"ml-security":0.0971353253}}
{"text":"We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1041009956,"dev-research":0.2274355727,"prompt-eng":0.4180196665,"data-quality":0.1265914057,"ml-security":0.0399741827}}
{"text":"Equipped with our module, monocular models can be stably trained with over 8 million images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.1712542451,"dev-research":0.1817729135,"prompt-eng":0.4153567426,"data-quality":0.1372421569,"ml-security":0.147556972}}
{"text":"Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0669611185,"dev-research":0.1967405313,"prompt-eng":0.3415395078,"data-quality":0.1022384719,"ml-security":0.0828972636}}
{"text":"Notably, our method won the championship in the 2nd Monocular Depth Estimation Challenge.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0523763019,"dev-research":0.2292450205,"prompt-eng":0.4010483648,"data-quality":0.1207624467,"ml-security":0.0538177265}}
{"text":"Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0982013374,"dev-research":0.2199454177,"prompt-eng":0.3885890296,"data-quality":0.1914688218,"ml-security":0.051174173}}
{"text":"The potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0068595308,"dev-research":0.365078435,"prompt-eng":0.4337407971,"data-quality":0.0557370777,"ml-security":0.0743901791}}
{"text":"For example, our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to high-quality metric scale dense mapping.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.0786986415,"dev-research":0.2587364916,"prompt-eng":0.3878507955,"data-quality":0.1426759991,"ml-security":0.0502884557}}
{"text":"The code is available at https://github.com/YvanYin/Metric3D.","meta":{"url":"http://arxiv.org/abs/2307.10984v1"},"cats":{"new-dataset":0.4093952597,"dev-research":0.2650570871,"prompt-eng":0.3793561386,"data-quality":0.0945746759,"ml-security":0.0425224196}}
{"text":"In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0876628315,"dev-research":0.253318786,"prompt-eng":0.3895322791,"data-quality":0.239108254,"ml-security":0.1839751739}}
{"text":"In this paper, we propose MASR, a Metadata Aware Speech Representation learning framework, which addresses the aforementioned limitations.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.2759375977,"dev-research":0.2635981573,"prompt-eng":0.4229286624,"data-quality":0.3372735179,"ml-security":0.1028061114}}
{"text":"MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.1490060611,"dev-research":0.3490710282,"prompt-eng":0.4052301435,"data-quality":0.1399495424,"ml-security":0.0575316605}}
{"text":"The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.1128039954,"dev-research":0.3269280606,"prompt-eng":0.3694320018,"data-quality":0.1864681217,"ml-security":0.1801631229}}
{"text":"A key advantage of the MASR framework is that it can be combined with any choice of SSL method.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0077991744,"dev-research":0.2696186843,"prompt-eng":0.370123188,"data-quality":0.047469664,"ml-security":0.0932377141}}
{"text":"Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0422745173,"dev-research":0.2766306185,"prompt-eng":0.4795614774,"data-quality":0.2338353684,"ml-security":0.0641735767}}
{"text":"In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0243567503,"dev-research":0.2261505677,"prompt-eng":0.4323305204,"data-quality":0.08381364,"ml-security":0.0391356}}
{"text":"We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.","meta":{"url":"http://arxiv.org/abs/2307.10982v1"},"cats":{"new-dataset":0.0330392271,"dev-research":0.2611025204,"prompt-eng":0.437566621,"data-quality":0.4115060181,"ml-security":0.183737587}}
{"text":"Collaborative inference has been a promising solution to enable resource-constrained edge devices to perform inference using state-of-the-art deep neural networks (DNNs).","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0715482283,"dev-research":0.3042804611,"prompt-eng":0.3702118479,"data-quality":0.0775506122,"ml-security":0.1729456084}}
{"text":"In collaborative inference, the edge device first feeds the input to a partial DNN locally and then uploads the intermediate result to the cloud to complete the inference.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0282794415,"dev-research":0.3646313275,"prompt-eng":0.3965342668,"data-quality":0.0828914001,"ml-security":0.1087829081}}
{"text":"However, recent research indicates model inversion attacks (MIAs) can reconstruct input data from intermediate results, posing serious privacy concerns for collaborative inference.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0558698519,"dev-research":0.2992632058,"prompt-eng":0.4215451023,"data-quality":0.1859181948,"ml-security":0.7693686874}}
{"text":"Existing perturbation and cryptography techniques are inefficient and unreliable in defending against MIAs while performing accurate inference.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0296625917,"dev-research":0.2664474844,"prompt-eng":0.3829288479,"data-quality":0.1850954545,"ml-security":0.5005568377}}
{"text":"This paper provides a viable solution, named PATROL, which develops privacy-oriented pruning to balance privacy, efficiency, and utility of collaborative inference.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0928730712,"dev-research":0.3125015588,"prompt-eng":0.4306408509,"data-quality":0.1423436177,"ml-security":0.5085376891}}
{"text":"PATROL takes advantage of the fact that later layers in a DNN can extract more task-specific features.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.02486565,"dev-research":0.364975919,"prompt-eng":0.4567809831,"data-quality":0.1146132832,"ml-security":0.2630539668}}
{"text":"Given limited local resources for collaborative inference, PATROL intends to deploy more layers at the edge based on pruning techniques to enforce task-specific features for inference and reduce task-irrelevant but sensitive features for privacy preservation.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.044945366,"dev-research":0.3056550585,"prompt-eng":0.4543071412,"data-quality":0.1410978217,"ml-security":0.4994753966}}
{"text":"To achieve privacy-oriented pruning, PATROL introduces two key components: Lipschitz regularization and adversarial reconstruction training, which increase the reconstruction errors by reducing the stability of MIAs and enhance the target inference model by adversarial training, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10981v1"},"cats":{"new-dataset":0.0493822015,"dev-research":0.2040779721,"prompt-eng":0.393416068,"data-quality":0.2184981879,"ml-security":0.6738565672}}
{"text":"U-Net, known for its simple yet efficient architecture, is widely utilized for image processing tasks and is particularly suitable for deployment on neuromorphic chips.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0299079102,"dev-research":0.3042948301,"prompt-eng":0.4061450431,"data-quality":0.0814467469,"ml-security":0.1111332331}}
{"text":"This paper introduces the novel concept of Spiking-UNet for image processing, which combines the power of Spiking Neural Networks (SNNs) with the U-Net architecture.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.1004824642,"dev-research":0.2810039385,"prompt-eng":0.3798660165,"data-quality":0.1038752446,"ml-security":0.1483089967}}
{"text":"To achieve an efficient Spiking-UNet, we face two primary challenges: ensuring high-fidelity information propagation through the network via spikes and formulating an effective training strategy.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.047803025,"dev-research":0.2876882767,"prompt-eng":0.4022726529,"data-quality":0.1615703896,"ml-security":0.2206012177}}
{"text":"To address the issue of information loss, we introduce multi-threshold spiking neurons, which improve the efficiency of information transmission within the Spiking-UNet.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0470738797,"dev-research":0.2586933278,"prompt-eng":0.3992733248,"data-quality":0.1446944274,"ml-security":0.2168330258}}
{"text":"For the training strategy, we adopt a conversion and fine-tuning pipeline that leverage pre-trained U-Net models.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0620040398,"dev-research":0.2787817739,"prompt-eng":0.473177182,"data-quality":0.1311127298,"ml-security":0.1830200287}}
{"text":"During the conversion process, significant variability in data distribution across different parts is observed when utilizing skip connections.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0490225142,"dev-research":0.2820795499,"prompt-eng":0.4180338691,"data-quality":0.2005037154,"ml-security":0.0435630612}}
{"text":"Therefore, we propose a connection-wise normalization method to prevent inaccurate firing rates.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0168873407,"dev-research":0.2762234541,"prompt-eng":0.4057285113,"data-quality":0.2161529939,"ml-security":0.1626250757}}
{"text":"Furthermore, we adopt a flow-based training method to fine-tune the converted models, reducing time steps while preserving performance.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0345449535,"dev-research":0.2473016162,"prompt-eng":0.4225257297,"data-quality":0.1467285298,"ml-security":0.1111774345}}
{"text":"Experimental results show that, on image segmentation and denoising, our Spiking-UNet achieves comparable performance to its non-spiking counterpart, surpassing existing SNN methods.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.1114473679,"dev-research":0.2518036054,"prompt-eng":0.3621532856,"data-quality":0.1598827519,"ml-security":0.1214106086}}
{"text":"Compared with the converted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference time by approximately 90\\%.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0168882906,"dev-research":0.2832284765,"prompt-eng":0.4179315885,"data-quality":0.1006996881,"ml-security":0.1117560511}}
{"text":"This research broadens the application scope of SNNs in image processing and is expected to inspire further exploration in the field of neuromorphic engineering.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.0614440196,"dev-research":0.2551290798,"prompt-eng":0.4176898692,"data-quality":0.1151704461,"ml-security":0.1684258797}}
{"text":"The code for our Spiking-UNet implementation is available at https://github.com/SNNresearch/Spiking-UNet.","meta":{"url":"http://arxiv.org/abs/2307.10974v1"},"cats":{"new-dataset":0.2107888245,"dev-research":0.2434421558,"prompt-eng":0.4017894351,"data-quality":0.1035549791,"ml-security":0.0844734404}}
{"text":"This paper presents a novel approach to assessing human lighting adjustment behavior and preference in diverse lighting conditions through the evaluation of emotional feedback and behavioral data using VR.","meta":{"url":"http://arxiv.org/abs/2307.10969v1"},"cats":{"new-dataset":0.10077755,"dev-research":0.3540527676,"prompt-eng":0.4344549939,"data-quality":0.1014220392,"ml-security":0.0641168233}}
{"text":"Participants (n= 27) were exposed to different lighting (n=17) conditions with different levels of illuminance and correlated color temperature (CCT) with a randomized order in a virtual office environment.","meta":{"url":"http://arxiv.org/abs/2307.10969v1"},"cats":{"new-dataset":0.1255457938,"dev-research":0.2813220305,"prompt-eng":0.4095615861,"data-quality":0.0916781885,"ml-security":0.1515207942}}
{"text":"Results from this study significantly advanced our understanding of preferred lighting conditions in virtual reality environments, influenced by a variety of factors such as illuminance, color temperature, order of presentation, and participant demographics.","meta":{"url":"http://arxiv.org/abs/2307.10969v1"},"cats":{"new-dataset":0.0963577903,"dev-research":0.3500543289,"prompt-eng":0.4019208219,"data-quality":0.0683252873,"ml-security":0.0708048016}}
{"text":"Through a comprehensive analysis of user adjustment profiles, we obtained insightful data that can guide the optimization of lighting design across various settings.","meta":{"url":"http://arxiv.org/abs/2307.10969v1"},"cats":{"new-dataset":0.0578828683,"dev-research":0.3439333031,"prompt-eng":0.4307758442,"data-quality":0.0530366826,"ml-security":0.0549490258}}
{"text":"The Cyber threats exposure has created worldwide pressure on organizations to comply with cyber security standards and policies for protecting their digital assets.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.1686317275,"dev-research":0.3658238593,"prompt-eng":0.4057415363,"data-quality":0.0934102344,"ml-security":0.5978401218}}
{"text":"Vulnerability assessment (VA) and Penetration Testing (PT) are widely adopted Security Compliance (SC) methods to identify security gaps and anticipate security breaches.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.1115491027,"dev-research":0.3944055902,"prompt-eng":0.4366209534,"data-quality":0.1803533766,"ml-security":0.4454796371}}
{"text":"In the computer networks context and despite the use of autonomous tools and systems, security compliance remains highly repetitive and resources consuming.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.032123933,"dev-research":0.3930679583,"prompt-eng":0.3927502053,"data-quality":0.1468810703,"ml-security":0.4577994624}}
{"text":"In this paper, we proposed a novel method to tackle the ever-growing problem of efficiency and effectiveness in network infrastructures security auditing by formally introducing, designing, and developing an Expert-System Automated Security Compliance Framework (ESASCF) that enables industrial and open-source VA and PT tools and systems to extract, process, store and re-use the expertise in a human-expert way to allow direct application in similar scenarios or during the periodic re-testing.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.1226038329,"dev-research":0.4563938302,"prompt-eng":0.4272418651,"data-quality":0.1430768815,"ml-security":0.3505209921}}
{"text":"The implemented model was then integrated within the ESASCF and tested on different size networks and proved efficient in terms of time-efficiency and testing effectiveness allowing ESASCF to take over autonomously the SC in Re-testing and offloading Expert by automating repeated segments SC and thus enabling Experts to prioritize important tasks in Ad-Hoc compliance tests.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.0448310585,"dev-research":0.3060994542,"prompt-eng":0.4481658416,"data-quality":0.070931857,"ml-security":0.1384308809}}
{"text":"The obtained results validate the performance enhancement notably by cutting the time required for an expert to 50% in the context of typical corporate networks first SC and 20% in re-testing, representing a significant cost-cutting.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.0165228491,"dev-research":0.347729474,"prompt-eng":0.4028922277,"data-quality":0.1085491517,"ml-security":0.0795435272}}
{"text":"In addition, the framework allows a long-term impact illustrated in the knowledge extraction, generalization, and re-utilization, which enables better SC confidence independent of the human expert skills, coverage, and wrong decisions resulting in impactful false negatives.","meta":{"url":"http://arxiv.org/abs/2307.10967v1"},"cats":{"new-dataset":0.0465755033,"dev-research":0.4238564127,"prompt-eng":0.3949045925,"data-quality":0.1297660524,"ml-security":0.165828832}}
{"text":"Endoscopic surgery is currently an important treatment method in the field of spinal surgery and avoiding damage to the spinal nerves through video guidance is a key challenge.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.0307717903,"dev-research":0.2572517326,"prompt-eng":0.3722819032,"data-quality":0.0782563841,"ml-security":0.0714649538}}
{"text":"This paper presents the first real-time segmentation method for spinal nerves in endoscopic surgery, which provides crucial navigational information for surgeons.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.0732724366,"dev-research":0.239175572,"prompt-eng":0.371749184,"data-quality":0.080902729,"ml-security":0.05113374}}
{"text":"A finely annotated segmentation dataset of approximately 10,000 consec-utive frames recorded during surgery is constructed for the first time for this field, addressing the problem of semantic segmentation.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.4276894174,"dev-research":0.2365362624,"prompt-eng":0.390088247,"data-quality":0.258285224,"ml-security":0.0674794146}}
{"text":"Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-frame information and self-attention mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.5621883761,"dev-research":0.2749038282,"prompt-eng":0.4107282144,"data-quality":0.1822376831,"ml-security":0.0766807403}}
{"text":"We also conduct extended exper-iments on a similar polyp endoscopy video dataset and show that the model has good generalization ability with advantageous performance.","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.2417941058,"dev-research":0.1934761152,"prompt-eng":0.3925155926,"data-quality":0.1228117423,"ml-security":0.0719520955}}
{"text":"The dataset and code of this work are presented at: https://github.com/zzzzzzpc/FUnet .","meta":{"url":"http://arxiv.org/abs/2307.10955v1"},"cats":{"new-dataset":0.9104653371,"dev-research":0.1979611932,"prompt-eng":0.3723920276,"data-quality":0.1112139531,"ml-security":0.0608081834}}
{"text":"In CMF surgery, the planning of bony movement to achieve a desired facial outcome is a challenging task.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.013371468,"dev-research":0.217977136,"prompt-eng":0.3845973855,"data-quality":0.0448514975,"ml-security":0.0545293965}}
{"text":"Current bone driven approaches focus on normalizing the bone with the expectation that the facial appearance will be corrected accordingly.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0078820097,"dev-research":0.2827916526,"prompt-eng":0.3946772867,"data-quality":0.0952675181,"ml-security":0.09931311}}
{"text":"However, due to the complex non-linear relationship between bony structure and facial soft-tissue, such bone-driven methods are insufficient to correct facial deformities.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0043499951,"dev-research":0.2539380767,"prompt-eng":0.3150398219,"data-quality":0.1189431366,"ml-security":0.1703391483}}
{"text":"Despite efforts to simulate facial changes resulting from bony movement, surgical planning still relies on iterative revisions and educated guesses.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0110533152,"dev-research":0.3061545238,"prompt-eng":0.4170760828,"data-quality":0.0928041194,"ml-security":0.0982007108}}
{"text":"To address these issues, we propose a soft-tissue driven framework that can automatically create and verify surgical plans.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0437869956,"dev-research":0.2858122941,"prompt-eng":0.4143440959,"data-quality":0.0654297166,"ml-security":0.0543906601}}
{"text":"Our framework consists of a bony planner network that estimates the bony movements required to achieve the desired facial outcome and a facial simulator network that can simulate the possible facial changes resulting from the estimated bony movement plans.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.1211590037,"dev-research":0.2418748027,"prompt-eng":0.3811759666,"data-quality":0.0427003864,"ml-security":0.1213418432}}
{"text":"By combining these two models, we can verify and determine the final bony movement required for planning.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.027585134,"dev-research":0.1849596755,"prompt-eng":0.4185826263,"data-quality":0.0335596333,"ml-security":0.0334742429}}
{"text":"The proposed framework was evaluated using a clinical dataset, and our experimental results demonstrate that the soft-tissue driven approach greatly improves the accuracy and efficacy of surgical planning when compared to the conventional bone-driven approach.","meta":{"url":"http://arxiv.org/abs/2307.10954v1"},"cats":{"new-dataset":0.0482579352,"dev-research":0.2834710959,"prompt-eng":0.3604318376,"data-quality":0.0575417783,"ml-security":0.0577167465}}
{"text":"Current object detection models have achieved good results on many benchmark datasets, detecting objects in dark conditions remains a large challenge.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.2299913305,"dev-research":0.2085548441,"prompt-eng":0.3780773208,"data-quality":0.215580164,"ml-security":0.2364867015}}
{"text":"To address this issue, we propose a pyramid enhanced network (PENet) and joint it with YOLOv3 to build a dark object detection framework named PE-YOLO.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.1714343013,"dev-research":0.2284292821,"prompt-eng":0.3872685422,"data-quality":0.1619337604,"ml-security":0.2240273795}}
{"text":"Firstly, PENet decomposes the image into four components of different resolutions using the Laplacian pyramid.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.1705298185,"dev-research":0.2242209967,"prompt-eng":0.3797859102,"data-quality":0.1162757635,"ml-security":0.041661648}}
{"text":"Specifically we propose a detail processing module (DPM) to enhance the detail of images, which consists of context branch and edge branch.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.0990494669,"dev-research":0.3027296546,"prompt-eng":0.4226666386,"data-quality":0.1372057191,"ml-security":0.0277523637}}
{"text":"In addition, we propose a low-frequency enhancement filter (LEF) to capture low-frequency semantics and prevent high-frequency noise.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.1011357951,"dev-research":0.3541666305,"prompt-eng":0.4248537947,"data-quality":0.4371594313,"ml-security":0.0768134755}}
{"text":"PE-YOLO adopts an end-to-end joint training approach and only uses normal detection loss to simplify the training process.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.0203108378,"dev-research":0.236729164,"prompt-eng":0.4022077731,"data-quality":0.2087016866,"ml-security":0.138103761}}
{"text":"We conduct experiments on the low-light object detection dataset ExDark to demonstrate the effectiveness of ours.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.3875864087,"dev-research":0.2457787822,"prompt-eng":0.3929106409,"data-quality":0.1980976316,"ml-security":0.1543950946}}
{"text":"The results indicate that compared with other dark detectors and low-light enhancement models, PE-YOLO achieves the advanced results, achieving 78.0% in mAP and 53.6 in FPS, respectively, which can adapt to object detection under different low-light conditions.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.0884649843,"dev-research":0.2099077074,"prompt-eng":0.4251189145,"data-quality":0.1564366445,"ml-security":0.0791856952}}
{"text":"The code is available at https://github.com/XiangchenYin/PE-YOLO.","meta":{"url":"http://arxiv.org/abs/2307.10953v1"},"cats":{"new-dataset":0.3969736088,"dev-research":0.2430805873,"prompt-eng":0.4446224871,"data-quality":0.1216912728,"ml-security":0.0772461721}}
{"text":"Autonomous driving requires accurate local scene understanding information.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.0821518029,"dev-research":0.3357895726,"prompt-eng":0.4216302418,"data-quality":0.2723728228,"ml-security":0.0939289651}}
{"text":"To this end, autonomous agents deploy object detection and online BEV lane graph extraction methods as a part of their perception stack.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.1209266122,"dev-research":0.2490400731,"prompt-eng":0.3982160103,"data-quality":0.1576977515,"ml-security":0.0880483126}}
{"text":"In this work, we propose an architecture and loss formulation to improve the accuracy of local lane graph estimates by using 3D object detection outputs.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.1531777756,"dev-research":0.2581788103,"prompt-eng":0.3310363192,"data-quality":0.244485056,"ml-security":0.09468435}}
{"text":"The proposed method learns to assign the objects to centerlines by considering the centerlines as cluster centers and the objects as data points to be assigned a probability distribution over the cluster centers.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.0917186856,"dev-research":0.2521737575,"prompt-eng":0.4051679496,"data-quality":0.1796022033,"ml-security":0.0916842019}}
{"text":"This training scheme ensures direct supervision on the relationship between lanes and objects, thus leading to better performance.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.061036773,"dev-research":0.2972712457,"prompt-eng":0.4255481769,"data-quality":0.123824146,"ml-security":0.1196999129}}
{"text":"The proposed method improves lane graph estimation substantially over state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.1044173678,"dev-research":0.2747420171,"prompt-eng":0.3378042972,"data-quality":0.1608730287,"ml-security":0.0495077396}}
{"text":"The extensive ablations show that our method can achieve significant performance improvements by using the outputs of existing 3D object detection methods.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.086532023,"dev-research":0.2350746465,"prompt-eng":0.3786343225,"data-quality":0.1505440854,"ml-security":0.0531996191}}
{"text":"Since our method uses the detection outputs rather than detection method intermediate representations, a single model of our method can use any detection method at test time.","meta":{"url":"http://arxiv.org/abs/2307.10947v1"},"cats":{"new-dataset":0.0238167659,"dev-research":0.248490029,"prompt-eng":0.47994951,"data-quality":0.2719458771,"ml-security":0.2058654967}}
{"text":"Recent advances in deep learning have significantly improved the performance of various computer vision applications.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.0385465149,"dev-research":0.2771481842,"prompt-eng":0.3819742852,"data-quality":0.1338905957,"ml-security":0.1361346266}}
{"text":"However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack of prior knowledge about the number and nature of new categories.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.1357663507,"dev-research":0.3233202786,"prompt-eng":0.3982145324,"data-quality":0.1914763251,"ml-security":0.1281128542}}
{"text":"Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge about the number of novel categories and the proportion of novel samples in the batch.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.2273439212,"dev-research":0.2867702851,"prompt-eng":0.3726591044,"data-quality":0.3345625611,"ml-security":0.1684596397}}
{"text":"To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervised class incremental learning approach for discovering novel categories on unlabeled sets without prior knowledge.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.3137043309,"dev-research":0.3070352462,"prompt-eng":0.3892512444,"data-quality":0.3097982332,"ml-security":0.1541951549}}
{"text":"The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and novel categories and clusters on the unlabeled dataset.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.3409575996,"dev-research":0.3264198783,"prompt-eng":0.3731296489,"data-quality":0.5702769713,"ml-security":0.1230208664}}
{"text":"Furthermore, the proxy anchors-based exemplar generates representative category vectors to mitigate catastrophic forgetting.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.0288084918,"dev-research":0.3333186016,"prompt-eng":0.4569974959,"data-quality":0.2220114525,"ml-security":0.1742607208}}
{"text":"Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained datasets under real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10943v1"},"cats":{"new-dataset":0.7000568407,"dev-research":0.3178462385,"prompt-eng":0.3485401905,"data-quality":0.2735747888,"ml-security":0.1715675619}}
{"text":"Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0731493531,"dev-research":0.3089430325,"prompt-eng":0.4421560158,"data-quality":0.1917821341,"ml-security":0.1417572478}}
{"text":"Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0791092167,"dev-research":0.2158583225,"prompt-eng":0.4557896354,"data-quality":0.1455718009,"ml-security":0.1258827133}}
{"text":"In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0621304885,"dev-research":0.2403629175,"prompt-eng":0.4897058581,"data-quality":0.0662825368,"ml-security":0.1048430834}}
{"text":"However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0096150554,"dev-research":0.3386851224,"prompt-eng":0.4602768874,"data-quality":0.1222596829,"ml-security":0.1815317397}}
{"text":"This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA).","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0145191684,"dev-research":0.2024099989,"prompt-eng":0.4571337823,"data-quality":0.0619456296,"ml-security":0.1494464508}}
{"text":"Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0994812246,"dev-research":0.28670039,"prompt-eng":0.4660054715,"data-quality":0.1546846145,"ml-security":0.1150912583}}
{"text":"Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0443717871,"dev-research":0.3314324173,"prompt-eng":0.4199421503,"data-quality":0.1246453927,"ml-security":0.1416595198}}
{"text":"Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT).","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0288858842,"dev-research":0.2349531112,"prompt-eng":0.5080857596,"data-quality":0.1224956615,"ml-security":0.1545764885}}
{"text":"The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0177397727,"dev-research":0.1993708731,"prompt-eng":0.4464841904,"data-quality":0.0666412518,"ml-security":0.0637751651}}
{"text":"We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.","meta":{"url":"http://arxiv.org/abs/2307.10936v1"},"cats":{"new-dataset":0.0381994063,"dev-research":0.222189426,"prompt-eng":0.4176504559,"data-quality":0.0637384055,"ml-security":0.1395981391}}
{"text":"Modern approaches for vision-centric environment perception for autonomous navigation make extensive use of self-supervised monocular depth estimation algorithms that output disparity maps.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.1087304364,"dev-research":0.2339237685,"prompt-eng":0.4099060156,"data-quality":0.1376661842,"ml-security":0.0848867324}}
{"text":"However, when this disparity map is projected onto 3D space, the errors in disparity are magnified, resulting in a depth estimation error that increases quadratically as the distance from the camera increases.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.0350229736,"dev-research":0.2622578807,"prompt-eng":0.3374867082,"data-quality":0.1698202399,"ml-security":0.0563318181}}
{"text":"Though Light Detection and Ranging (LiDAR) can solve this issue, it is expensive and not feasible for many applications.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.097801449,"dev-research":0.1985023558,"prompt-eng":0.3712428622,"data-quality":0.1049831286,"ml-security":0.0681691388}}
{"text":"To address the challenge of accurate ranging with low-cost sensors, we propose, OCTraN, a transformer architecture that uses iterative-attention to convert 2D image features into 3D occupancy features and makes use of convolution and transpose convolution to efficiently operate on spatial information.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.2345453699,"dev-research":0.2940501633,"prompt-eng":0.3483677102,"data-quality":0.0781044737,"ml-security":0.0644262771}}
{"text":"We also develop a self-supervised training pipeline to generalize the model to any scene by eliminating the need for LiDAR ground truth by substituting it with pseudo-ground truth labels obtained from boosted monocular depth estimation.","meta":{"url":"http://arxiv.org/abs/2307.10934v1"},"cats":{"new-dataset":0.1610146822,"dev-research":0.2132606277,"prompt-eng":0.4247730338,"data-quality":0.2446038932,"ml-security":0.1834786692}}
{"text":"The enhancement of unsupervised learning of sentence representations has been significantly achieved by the utility of contrastive learning.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0483497386,"dev-research":0.2488626966,"prompt-eng":0.4277335116,"data-quality":0.1316241076,"ml-security":0.1087738755}}
{"text":"This approach clusters the augmented positive instance with the anchor instance to create a desired embedding space.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0525400256,"dev-research":0.2699535269,"prompt-eng":0.4328273589,"data-quality":0.2159683716,"ml-security":0.0915967005}}
{"text":"However, relying solely on the contrastive objective can result in sub-optimal outcomes due to its inability to differentiate subtle semantic variations between positive pairs.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0056880627,"dev-research":0.2691740254,"prompt-eng":0.3905972163,"data-quality":0.192800776,"ml-security":0.0678876819}}
{"text":"Specifically, common data augmentation techniques frequently introduce semantic distortion, leading to a semantic margin between the positive pair.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0393766007,"dev-research":0.3972018372,"prompt-eng":0.4254970569,"data-quality":0.480649729,"ml-security":0.1077459062}}
{"text":"While the InfoNCE loss function overlooks the semantic margin and prioritizes similarity maximization between positive pairs during training, leading to the insensitive semantic comprehension ability of the trained model.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.030308384,"dev-research":0.3339481032,"prompt-eng":0.4226669211,"data-quality":0.3314022383,"ml-security":0.2255183848}}
{"text":"In this paper, we introduce a novel Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework, capable of simultaneously adapting to various positive pairs generated by different augmentation techniques.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0532600846,"dev-research":0.2259053517,"prompt-eng":0.4282048739,"data-quality":0.1346636205,"ml-security":0.1261617229}}
{"text":"We propose a \\textit{Twins Loss} to preserve the innate margin during training and promote the potential of data enhancement in order to overcome the sub-optimal issue.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0524546212,"dev-research":0.2741596902,"prompt-eng":0.3699940062,"data-quality":0.260766508,"ml-security":0.3133175443}}
{"text":"We also present proof-of-concept experiments combined with the contrastive objective to prove the validity of the proposed Twins Loss.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.0136448763,"dev-research":0.2466258405,"prompt-eng":0.4050670124,"data-quality":0.3066512956,"ml-security":0.2116120737}}
{"text":"Furthermore, we propose a hippocampus queue mechanism to restore and reuse the negative instances without additional calculation, which further enhances the efficiency and performance of the IFCL.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.022493814,"dev-research":0.2761314116,"prompt-eng":0.4581818991,"data-quality":0.1324198903,"ml-security":0.1316619954}}
{"text":"We verify the IFCL framework on nine semantic textual similarity tasks with both English and Chinese datasets, and the experimental results show that IFCL outperforms state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.10932v1"},"cats":{"new-dataset":0.302928629,"dev-research":0.3125231079,"prompt-eng":0.4251931013,"data-quality":0.3157671442,"ml-security":0.0496989083}}
{"text":"The development of large language models (LLMs) has seen rapid progress in recent years.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.1286550038,"dev-research":0.2100467207,"prompt-eng":0.5137257187,"data-quality":0.1049969145,"ml-security":0.0769726915}}
{"text":"One of the most widely used LLMs is the Generative Pre-trained Transformer (GPT) series, which has been applied in various fields, including the media domain.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.049681904,"dev-research":0.1570063244,"prompt-eng":0.5225609204,"data-quality":0.0802295718,"ml-security":0.0609983344}}
{"text":"However, in practical applications, the differences between the media's use cases and the general-purpose applications of LLMs have become increasingly apparent, especially Chinese.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0287806418,"dev-research":0.2316415141,"prompt-eng":0.4595040238,"data-quality":0.1056261885,"ml-security":0.1024038466}}
{"text":"As a result, there is a growing need to develop LLM that are specifically tailored to the unique requirements of the media domain.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0506388609,"dev-research":0.2686558037,"prompt-eng":0.4825030177,"data-quality":0.0853597411,"ml-security":0.0778664352}}
{"text":"In this paper, we present MediaGPT, a large language model training on variety of media data and addressing the practical needs of Chinese media.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.4652031658,"dev-research":0.2231210284,"prompt-eng":0.4437468569,"data-quality":0.2523794586,"ml-security":0.069366058}}
{"text":"We have designed a diverse set of task instruction types to cater to the specific requirements of the domain.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0813625793,"dev-research":0.3276525995,"prompt-eng":0.4669174124,"data-quality":0.0530439645,"ml-security":0.0495988102}}
{"text":"To further validate the effectiveness of our proposed LLM, we have constructed unique datasets that are tailored to the media domain and have also developed verification methods that are specifically designed for generative-type tasks.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.2060049882,"dev-research":0.2320117605,"prompt-eng":0.4891367089,"data-quality":0.2554217211,"ml-security":0.0703729422}}
{"text":"By doing so, we aim to bridge the gap between the general-purpose LLM and the requirements of the media domain, and to pave the way for more effective and efficient use of LLM in this field.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.0385012788,"dev-research":0.2338352477,"prompt-eng":0.4855297083,"data-quality":0.0894647998,"ml-security":0.0586872466}}
{"text":"This paper aims to explore the challenges and opportunities of developing LLM for media applications and to propose potential solutions for addressing these challenges.","meta":{"url":"http://arxiv.org/abs/2307.10930v1"},"cats":{"new-dataset":0.078001822,"dev-research":0.2291512879,"prompt-eng":0.4850460227,"data-quality":0.1156519343,"ml-security":0.0726734563}}
{"text":"Evaluation of Large Language Models (LLMs) is challenging because aligning to human values requires the composition of multiple skills and the required set of skills varies depending on the instruction.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0340997889,"dev-research":0.2307937123,"prompt-eng":0.5208034349,"data-quality":0.1604634987,"ml-security":0.083404893}}
{"text":"Recent studies have evaluated the performance of LLMs in two ways, (1) automatic evaluation on several independent benchmarks and (2) human or machined-based evaluation giving an overall score to the response.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0215330599,"dev-research":0.2263661093,"prompt-eng":0.5507259093,"data-quality":0.140892371,"ml-security":0.0652908582}}
{"text":"However, both settings are coarse-grained evaluations, not considering the nature of user instructions that require instance-wise skill composition, which limits the interpretation of the true capabilities of LLMs.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0060757809,"dev-research":0.2762572469,"prompt-eng":0.492496046,"data-quality":0.099555751,"ml-security":0.0929249787}}
{"text":"In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets), a fine-grained evaluation protocol that can be used for both model-based and human-based evaluation which decomposes coarse-level scoring to an instance-wise skill set-level.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.2211390324,"dev-research":0.3379974771,"prompt-eng":0.4737015984,"data-quality":0.2940535071,"ml-security":0.083535643}}
{"text":"Specifically, we define 12 fine-grained skills needed for LLMs to follow open-ended user instructions and construct an evaluation set by allocating a set of skills for each instance.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0919509895,"dev-research":0.2784208959,"prompt-eng":0.5254136703,"data-quality":0.0983342224,"ml-security":0.1232083621}}
{"text":"Additionally, by annotating the target domains and difficulty level for each instance, FLASK provides a holistic view with a comprehensive analysis of a model's performance depending on skill, domain, and difficulty.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0897394429,"dev-research":0.3495934484,"prompt-eng":0.4529565247,"data-quality":0.1158088571,"ml-security":0.0795041178}}
{"text":"Through using FLASK, we compare multiple open-sourced and proprietary LLMs and observe highly-correlated findings between model-based and human-based evaluations.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.1203802763,"dev-research":0.3195242855,"prompt-eng":0.4906580241,"data-quality":0.1370346267,"ml-security":0.1413938456}}
{"text":"FLASK enables developers to more accurately measure the model performance and how it can be improved by analyzing factors that make LLMs proficient in particular skills.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0463968939,"dev-research":0.3476912461,"prompt-eng":0.4774149656,"data-quality":0.0797742635,"ml-security":0.1458424866}}
{"text":"For practitioners, FLASK can be used to recommend suitable models for particular situations through comprehensive comparison among various LLMs.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.0521602405,"dev-research":0.2249472527,"prompt-eng":0.4492017691,"data-quality":0.0483199365,"ml-security":0.085021932}}
{"text":"We release the evaluation data and code implementation at https://github.com/kaistAI/FLASK.","meta":{"url":"http://arxiv.org/abs/2307.10928v1"},"cats":{"new-dataset":0.3560183109,"dev-research":0.3039266327,"prompt-eng":0.4047194343,"data-quality":0.1047574803,"ml-security":0.0867349297}}
{"text":"Intrinsic decomposition is to infer the albedo and shading from the image.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.0211631141,"dev-research":0.2715014061,"prompt-eng":0.38413128,"data-quality":0.0974627813,"ml-security":0.0849067027}}
{"text":"Since it is a heavily ill-posed problem, previous methods rely on prior assumptions from 2D images, however, the exploration of the data representation itself is limited.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.1393758742,"dev-research":0.2050774601,"prompt-eng":0.3518370731,"data-quality":0.1149345515,"ml-security":0.1202221762}}
{"text":"The point cloud is known as a rich format of scene representation, which naturally aligns the geometric information and the color information of an image.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.1098163956,"dev-research":0.2698990338,"prompt-eng":0.3741522715,"data-quality":0.1212019629,"ml-security":0.0665449359}}
{"text":"Our proposed method, Point Intrinsic Net, in short, PoInt-Net, jointly predicts the albedo, light source direction, and shading, using point cloud representation.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.0514652119,"dev-research":0.2537211709,"prompt-eng":0.3792876864,"data-quality":0.1061779318,"ml-security":0.088340078}}
{"text":"Experiments reveal the benefits of PoInt-Net, in terms of accuracy, it outperforms 2D representation approaches on multiple metrics across datasets; in terms of efficiency, it trains on small-scale point clouds and performs stably on any-scale point clouds; in terms of robustness, it only trains on single object level dataset, and demonstrates reasonable generalization ability for unseen objects and scenes.","meta":{"url":"http://arxiv.org/abs/2307.10924v1"},"cats":{"new-dataset":0.1441396685,"dev-research":0.2496906258,"prompt-eng":0.3307932699,"data-quality":0.1275852789,"ml-security":0.1434304506}}
{"text":"Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.1890546309,"dev-research":0.241194652,"prompt-eng":0.4006449769,"data-quality":0.1159809458,"ml-security":0.2107842651}}
{"text":"However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram).","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.0673517046,"dev-research":0.2248253655,"prompt-eng":0.369475936,"data-quality":0.0677801726,"ml-security":0.1641435694}}
{"text":"These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.1695727344,"dev-research":0.177996843,"prompt-eng":0.3741016557,"data-quality":0.111770927,"ml-security":0.0781660627}}
{"text":"In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.190635097,"dev-research":0.2000685898,"prompt-eng":0.3312032069,"data-quality":0.0967534388,"ml-security":0.1755059597}}
{"text":"Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.0123117097,"dev-research":0.1657646042,"prompt-eng":0.3896936938,"data-quality":0.1075651277,"ml-security":0.141107772}}
{"text":"We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.6739618609,"dev-research":0.2763550173,"prompt-eng":0.3510714579,"data-quality":0.1281766551,"ml-security":0.1420389653}}
{"text":"Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.","meta":{"url":"http://arxiv.org/abs/2307.10923v1"},"cats":{"new-dataset":0.0648857312,"dev-research":0.2827935529,"prompt-eng":0.4487700627,"data-quality":0.2922028116,"ml-security":0.1191697563}}
{"text":"Recent contrastive language image pre-training has led to learning highly transferable and robust image representations.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.1969909123,"dev-research":0.219669884,"prompt-eng":0.4453968294,"data-quality":0.2099587668,"ml-security":0.1015120681}}
{"text":"However, adapting these models to video domains with minimal supervision remains an open problem.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.0755391963,"dev-research":0.2245277338,"prompt-eng":0.4286378356,"data-quality":0.2532663306,"ml-security":0.0970245666}}
{"text":"We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.083987092,"dev-research":0.2264449651,"prompt-eng":0.461049814,"data-quality":0.2816939239,"ml-security":0.0867332874}}
{"text":"A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.0366464072,"dev-research":0.2506187072,"prompt-eng":0.4580030843,"data-quality":0.0778650444,"ml-security":0.1518845772}}
{"text":"Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.1225461349,"dev-research":0.3799471082,"prompt-eng":0.5353713089,"data-quality":0.1943859868,"ml-security":0.1385150654}}
{"text":"We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.0389192799,"dev-research":0.3394464091,"prompt-eng":0.4386877021,"data-quality":0.2477231324,"ml-security":0.1520094885}}
{"text":"Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10922v1"},"cats":{"new-dataset":0.1351610293,"dev-research":0.2231590374,"prompt-eng":0.3530109102,"data-quality":0.1714593197,"ml-security":0.099035923}}
{"text":"Despite the rapid progress in self-supervised learning (SSL), end-to-end fine-tuning still remains the dominant fine-tuning strategy for medical imaging analysis.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0638721483,"dev-research":0.267193074,"prompt-eng":0.4262176938,"data-quality":0.225014726,"ml-security":0.1904870862}}
{"text":"However, it remains unclear whether this approach is truly optimal for effectively utilizing the pre-trained knowledge, especially considering the diverse categories of SSL that capture different types of features.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.023250187,"dev-research":0.3203846935,"prompt-eng":0.4344920504,"data-quality":0.1201741361,"ml-security":0.2596121274}}
{"text":"In this paper, we first establish strong contrastive and restorative SSL baselines that outperform SOTA methods across four diverse downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0440515385,"dev-research":0.2418889804,"prompt-eng":0.3854033147,"data-quality":0.1527914042,"ml-security":0.1039656609}}
{"text":"Building upon these strong baselines, we conduct an extensive fine-tuning analysis across multiple pre-training and fine-tuning datasets, as well as various fine-tuning dataset sizes.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.4731491337,"dev-research":0.3360139615,"prompt-eng":0.4173110023,"data-quality":0.2539223245,"ml-security":0.1437846611}}
{"text":"Contrary to the conventional wisdom of fine-tuning only the last few layers of a pre-trained network, we show that fine-tuning intermediate layers is more effective, with fine-tuning the second quarter (25-50%) of the network being optimal for contrastive SSL whereas fine-tuning the third quarter (50-75%) of the network being optimal for restorative SSL.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0134795061,"dev-research":0.2564756215,"prompt-eng":0.4158992228,"data-quality":0.1439763478,"ml-security":0.2055091734}}
{"text":"Compared to the de-facto standard of end-to-end fine-tuning, our best fine-tuning strategy, which fine-tunes a shallower network consisting of the first three quarters (0-75%) of the pre-trained network, yields improvements of as much as 5.48%.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0333661062,"dev-research":0.3389309622,"prompt-eng":0.4337264337,"data-quality":0.1970192173,"ml-security":0.1469599037}}
{"text":"Additionally, using these insights, we propose a simple yet effective method to leverage the complementary strengths of multiple SSL models, resulting in enhancements of up to 3.57% compared to using the best model alone.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0202552549,"dev-research":0.2396375975,"prompt-eng":0.4104695396,"data-quality":0.0939800335,"ml-security":0.1557536873}}
{"text":"Hence, our fine-tuning strategies not only enhance the performance of individual SSL models, but also enable effective utilization of the complementary strengths offered by multiple SSL models, leading to significant improvements in self-supervised medical imaging analysis.","meta":{"url":"http://arxiv.org/abs/2307.10915v1"},"cats":{"new-dataset":0.0311064167,"dev-research":0.2293021846,"prompt-eng":0.4312879731,"data-quality":0.1342937847,"ml-security":0.1974142183}}
{"text":"Limited by expensive pixel-level labels, polyp segmentation models are plagued by data shortage and suffer from impaired generalization.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0674940545,"dev-research":0.2269510858,"prompt-eng":0.3940439607,"data-quality":0.318111856,"ml-security":0.1639856065}}
{"text":"In contrast, polyp bounding box annotations are much cheaper and more accessible.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0272932508,"dev-research":0.3598402464,"prompt-eng":0.3820965051,"data-quality":0.1838790875,"ml-security":0.0977592118}}
{"text":"Thus, to reduce labeling cost, we propose to learn a weakly supervised polyp segmentation model (i.e., WeakPolyp) completely based on bounding box annotations.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.1819795899,"dev-research":0.2104003433,"prompt-eng":0.4140545179,"data-quality":0.3880379651,"ml-security":0.148053611}}
{"text":"However, coarse bounding boxes contain too much noise.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0490726565,"dev-research":0.2485552692,"prompt-eng":0.3552839466,"data-quality":0.2623821106,"ml-security":0.1573194261}}
{"text":"To avoid interference, we introduce the mask-to-box (M2B) transformation.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0312590164,"dev-research":0.2052467695,"prompt-eng":0.3925760926,"data-quality":0.0815319559,"ml-security":0.1103276566}}
{"text":"By supervising the outer box mask of the prediction instead of the prediction itself, M2B greatly mitigates the mismatch between the coarse label and the precise prediction.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0097549699,"dev-research":0.2464347215,"prompt-eng":0.4709944487,"data-quality":0.406441175,"ml-security":0.1599540743}}
{"text":"But, M2B only provides sparse supervision, leading to non-unique predictions.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0313274992,"dev-research":0.2086147453,"prompt-eng":0.4299474307,"data-quality":0.2027861844,"ml-security":0.1558565785}}
{"text":"Therefore, we further propose a scale consistency (SC) loss for dense supervision.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0323487264,"dev-research":0.2450246669,"prompt-eng":0.4572403866,"data-quality":0.3969769931,"ml-security":0.0777064319}}
{"text":"By explicitly aligning predictions across the same image at different scales, the SC loss largely reduces the variation of predictions.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0098155646,"dev-research":0.2552126325,"prompt-eng":0.4103329752,"data-quality":0.180728264,"ml-security":0.1024096298}}
{"text":"Note that our WeakPolyp is a plug-and-play model, which can be easily ported to other appealing backbones.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.075914878,"dev-research":0.2611094481,"prompt-eng":0.3971591348,"data-quality":0.0778803408,"ml-security":0.1687527911}}
{"text":"Besides, the proposed modules are only used during training, bringing no computation cost to inference.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.0200783756,"dev-research":0.2197930637,"prompt-eng":0.3423156117,"data-quality":0.068780178,"ml-security":0.1351598093}}
{"text":"Extensive experiments demonstrate the effectiveness of our proposed WeakPolyp, which surprisingly achieves a comparable performance with a fully supervised model, requiring no mask annotations at all.","meta":{"url":"http://arxiv.org/abs/2307.10912v1"},"cats":{"new-dataset":0.1346590529,"dev-research":0.223679404,"prompt-eng":0.4274797789,"data-quality":0.4003566112,"ml-security":0.3778026247}}
{"text":"The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.035187855,"dev-research":0.2611373214,"prompt-eng":0.4140843533,"data-quality":0.2440500131,"ml-security":0.1285280882}}
{"text":"Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI).","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0240635716,"dev-research":0.2196545108,"prompt-eng":0.3615587813,"data-quality":0.1086864635,"ml-security":0.0717124883}}
{"text":"However, the relation between other MVSSL methods and MI remains unclear.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0124386745,"dev-research":0.2608170942,"prompt-eng":0.3749617197,"data-quality":0.1225185293,"ml-security":0.0661140424}}
{"text":"We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0695430443,"dev-research":0.1612266032,"prompt-eng":0.3906893142,"data-quality":0.1139411903,"ml-security":0.0846480462}}
{"text":"Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.1030157875,"dev-research":0.2176652712,"prompt-eng":0.3795065066,"data-quality":0.131515059,"ml-security":0.1449073}}
{"text":"We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0110834318,"dev-research":0.2502675345,"prompt-eng":0.41495635,"data-quality":0.1062094387,"ml-security":0.0977997553}}
{"text":"We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   ","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.0161051996,"dev-research":0.18426991,"prompt-eng":0.3575192413,"data-quality":0.0958055817,"ml-security":0.2034064353}}
{"text":"Github repo: https://github.com/apple/ml-entropy-reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.10907v1"},"cats":{"new-dataset":0.3199716515,"dev-research":0.2061343801,"prompt-eng":0.4069459835,"data-quality":0.1639206645,"ml-security":0.0992985329}}
{"text":"Digital democracy and new forms for direct digital participation in policy making gain unprecedented momentum.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0886039513,"dev-research":0.3117099254,"prompt-eng":0.3953265379,"data-quality":0.0925187576,"ml-security":0.1265579137}}
{"text":"This is particularly the case for preferential voting methods and decision-support systems designed to promote fairer, more inclusive and legitimate collective decision-making processes in citizens assemblies, participatory budgeting and elections.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0313011116,"dev-research":0.3149906704,"prompt-eng":0.4304010894,"data-quality":0.1019848702,"ml-security":0.1215130122}}
{"text":"However, a systematic human experimentation with different voting methods is cumbersome and costly.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0234375363,"dev-research":0.3589096618,"prompt-eng":0.4293454092,"data-quality":0.1189859277,"ml-security":0.1361980212}}
{"text":"This paper introduces VoteLab, an open-source and thoroughly-documented platform for modular and adaptive design of voting experiments.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.1724522505,"dev-research":0.3339830631,"prompt-eng":0.4780968811,"data-quality":0.2187740925,"ml-security":0.1664566857}}
{"text":"It supports to visually and interactively build reusable campaigns with a choice of different voting methods, while voters can easily respond to subscribed voting questions on a smartphone.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0445647431,"dev-research":0.3604038493,"prompt-eng":0.4512438293,"data-quality":0.0746969493,"ml-security":0.1082901738}}
{"text":"A proof-of-concept with four voting methods and questions on COVID-19 in an online lab experiment have been used to study the consistency of voting outcomes.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0480533938,"dev-research":0.3114174188,"prompt-eng":0.4391201736,"data-quality":0.1694359624,"ml-security":0.1277417711}}
{"text":"It demonstrates the capability of VoteLab to support rigorous experimentation of complex voting scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10903v1"},"cats":{"new-dataset":0.0617055661,"dev-research":0.3748292919,"prompt-eng":0.4741736003,"data-quality":0.142164213,"ml-security":0.1374084785}}
{"text":"We show that computing the strongest polynomial invariant for single-path loops with polynomial assignments is at least as hard as the Skolem problem, a famous problem whose decidability has been open for almost a century.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0532465557,"dev-research":0.2407878686,"prompt-eng":0.3962690042,"data-quality":0.1697229714,"ml-security":0.1455239022}}
{"text":"While the strongest polynomial invariants are computable for affine loops, for polynomial loops the problem remained wide open.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0483685043,"dev-research":0.2314187667,"prompt-eng":0.3346696021,"data-quality":0.1449364385,"ml-security":0.1706147373}}
{"text":"As an intermediate result of independent interest, we prove that reachability for discrete polynomial dynamical systems is Skolem-hard as well.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0428281033,"dev-research":0.2066819874,"prompt-eng":0.371745809,"data-quality":0.0755785075,"ml-security":0.1430554558}}
{"text":"Furthermore, we generalize the notion of invariant ideals and introduce moment invariant ideals for probabilistic programs.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0362229277,"dev-research":0.2703983835,"prompt-eng":0.4374520623,"data-quality":0.1228483492,"ml-security":0.1626547357}}
{"text":"With this tool, we further show that the strongest polynomial moment invariant is (i) uncomputable, for probabilistic loops with branching statements, and (ii) Skolem-hard to compute for polynomial probabilistic loops without branching statements.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0369396962,"dev-research":0.215842834,"prompt-eng":0.4129290904,"data-quality":0.1166538878,"ml-security":0.1054096438}}
{"text":"Finally, we identify a class of probabilistic loops for which the strongest polynomial moment invariant is computable and provide an algorithm for it.","meta":{"url":"http://arxiv.org/abs/2307.10902v1"},"cats":{"new-dataset":0.0952619581,"dev-research":0.2134604542,"prompt-eng":0.3975091477,"data-quality":0.1152825739,"ml-security":0.1313215835}}
{"text":"As social robots see increasing deployment within the general public, improving the interaction with those robots is essential.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.0372172686,"dev-research":0.3072946123,"prompt-eng":0.444289732,"data-quality":0.0944682848,"ml-security":0.1233068354}}
{"text":"Spoken language offers an intuitive interface for the human-robot interaction (HRI), with dialogue management (DM) being a key component in those interactive systems.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.1521397195,"dev-research":0.3034344235,"prompt-eng":0.4571353225,"data-quality":0.0917163897,"ml-security":0.0588363036}}
{"text":"Yet, to overcome current challenges and manage smooth, informative and engaging interaction a more structural approach to combining HRI and DM is needed.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.0372430271,"dev-research":0.3107852128,"prompt-eng":0.4504684453,"data-quality":0.0690264928,"ml-security":0.0297576045}}
{"text":"In this systematic review, we analyse the current use of DM in HRI and focus on the type of dialogue manager used, its capabilities, evaluation methods and the challenges specific to DM in HRI.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.1150618506,"dev-research":0.2922214861,"prompt-eng":0.417610047,"data-quality":0.110965961,"ml-security":0.0499970758}}
{"text":"We identify the challenges and current scientific frontier related to the DM approach, interaction domain, robot appearance, physical situatedness and multimodality.","meta":{"url":"http://arxiv.org/abs/2307.10897v1"},"cats":{"new-dataset":0.0802721923,"dev-research":0.2673441092,"prompt-eng":0.4624415143,"data-quality":0.0566476849,"ml-security":0.0486587182}}
{"text":"For companies producing related products, a Software Product Line (SPL) is a software reuse method that improves time-to-market and software quality, achieving substantial cost reductions.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.1562670218,"dev-research":0.47231177,"prompt-eng":0.3614555538,"data-quality":0.0891331517,"ml-security":0.0833032301}}
{"text":"These benefits do not come for free.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0514823166,"dev-research":0.2893519789,"prompt-eng":0.3235728568,"data-quality":0.084109343,"ml-security":0.1851035806}}
{"text":"It often takes years to re-architect and re-engineer a codebase to support SPL and, once adopted, it must be maintained.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0619923112,"dev-research":0.4630222792,"prompt-eng":0.3549995072,"data-quality":0.0860362143,"ml-security":0.0629890053}}
{"text":"Current SPL practice relies on a collection of tools, tailored for different reengineering phases, whose output developers must coordinate and integrate.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0650468183,"dev-research":0.487225001,"prompt-eng":0.4024586261,"data-quality":0.0734249464,"ml-security":0.0634254556}}
{"text":"We present Foundry, a general automated approach for leveraging software transplantation to speed conversion to and maintenance of SPL.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.1176718938,"dev-research":0.4272301552,"prompt-eng":0.4071851245,"data-quality":0.1223064413,"ml-security":0.0746890682}}
{"text":"Foundry facilitates feature extraction and migration.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0878225095,"dev-research":0.4422604171,"prompt-eng":0.4116007747,"data-quality":0.141078815,"ml-security":0.0556033629}}
{"text":"It can efficiently, repeatedly, transplant a sequence of features, implemented in multiple files.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.1117600637,"dev-research":0.3988080006,"prompt-eng":0.400733361,"data-quality":0.1031216065,"ml-security":0.0702574904}}
{"text":"We used Foundry to create two valid product lines that integrate features from three real-world systems in an automated way.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.1159742289,"dev-research":0.4441933201,"prompt-eng":0.4455034506,"data-quality":0.1471982568,"ml-security":0.0549244167}}
{"text":"Moreover, we conducted an experiment comparing Foundry's feature migration with manual effort.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.037707198,"dev-research":0.4578563393,"prompt-eng":0.4590531049,"data-quality":0.1537879709,"ml-security":0.056414104}}
{"text":"We show that Foundry automatically migrated features across codebases 4.8 times faster, on average, than the average time a group of SPL experts took to accomplish the task.","meta":{"url":"http://arxiv.org/abs/2307.10896v1"},"cats":{"new-dataset":0.0814005874,"dev-research":0.5035958717,"prompt-eng":0.4017611224,"data-quality":0.1191157477,"ml-security":0.0760675894}}
{"text":"Digital dentistry has made significant advancements in recent years, yet numerous challenges remain to be addressed.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0238664499,"dev-research":0.317866217,"prompt-eng":0.3584942812,"data-quality":0.0892684445,"ml-security":0.1094545211}}
{"text":"In this study, we release a new extensive dataset of tooth meshes to encourage further research.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.1718445797,"dev-research":0.2808582162,"prompt-eng":0.3332331534,"data-quality":0.0777159741,"ml-security":0.095236495}}
{"text":"Additionally, we propose Variational FoldingNet (VF-Net), which extends FoldingNet to enable probabilistic learning of point cloud representations.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.1042762127,"dev-research":0.2322376505,"prompt-eng":0.3567835466,"data-quality":0.0976507094,"ml-security":0.1102011828}}
{"text":"A key challenge in existing latent variable models for point clouds is the lack of a 1-to-1 mapping between input points and output points.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.1189949535,"dev-research":0.1864804377,"prompt-eng":0.4311054009,"data-quality":0.1330081689,"ml-security":0.0742828428}}
{"text":"Instead, they must rely on optimizing Chamfer distances, a metric that does not have a normalized distributional counterpart, preventing its usage in probabilistic models.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0153394272,"dev-research":0.2001798799,"prompt-eng":0.4000032198,"data-quality":0.1250801583,"ml-security":0.1024414702}}
{"text":"We demonstrate that explicit minimization of Chamfer distances can be replaced by a suitable encoder, which allows us to increase computational efficiency while simplifying the probabilistic extension.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0673974717,"dev-research":0.2069484553,"prompt-eng":0.4190797652,"data-quality":0.1328610554,"ml-security":0.0823202872}}
{"text":"Our experimental findings present empirical evidence demonstrating the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0103588636,"dev-research":0.2358809958,"prompt-eng":0.353946483,"data-quality":0.1152577367,"ml-security":0.0974096001}}
{"text":"Additionally, our investigation highlights the robustness of VF-Net's latent representations.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0231839165,"dev-research":0.2472003517,"prompt-eng":0.39434755,"data-quality":0.2705631831,"ml-security":0.1892956428}}
{"text":"These results underscore the promising prospects of VF-Net as an effective and reliable method for point cloud reconstruction and analysis.","meta":{"url":"http://arxiv.org/abs/2307.10895v1"},"cats":{"new-dataset":0.0686393242,"dev-research":0.2384422847,"prompt-eng":0.3154489392,"data-quality":0.1330765881,"ml-security":0.0731923383}}
{"text":"Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.3007909268,"dev-research":0.2705476106,"prompt-eng":0.4081728609,"data-quality":0.0508380459,"ml-security":0.0505529699}}
{"text":"Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.6343227321,"dev-research":0.2405498924,"prompt-eng":0.3935171409,"data-quality":0.0565318717,"ml-security":0.0259822924}}
{"text":"Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.1874536062,"dev-research":0.2511444155,"prompt-eng":0.4554126316,"data-quality":0.0821001737,"ml-security":0.0373583521}}
{"text":"While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.0467870556,"dev-research":0.288066532,"prompt-eng":0.4531199239,"data-quality":0.0615288099,"ml-security":0.0791019299}}
{"text":"In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.3252013679,"dev-research":0.2234375557,"prompt-eng":0.4179274921,"data-quality":0.0580654998,"ml-security":0.0322133779}}
{"text":"We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.1952520661,"dev-research":0.2134840824,"prompt-eng":0.4780205887,"data-quality":0.1149638824,"ml-security":0.0326387879}}
{"text":"Additionally, we provide an overview of common datasets and evaluation metrics.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.7804732154,"dev-research":0.3485812529,"prompt-eng":0.3785236534,"data-quality":0.1610828629,"ml-security":0.0811720058}}
{"text":"Lastly, we discuss open problems and outline potential future research directions.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.218282447,"dev-research":0.3283234939,"prompt-eng":0.3623256245,"data-quality":0.0840140372,"ml-security":0.0931566076}}
{"text":"We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.","meta":{"url":"http://arxiv.org/abs/2307.10894v1"},"cats":{"new-dataset":0.3204030491,"dev-research":0.2887525019,"prompt-eng":0.4246304537,"data-quality":0.066461513,"ml-security":0.0846101514}}
{"text":"The ability to learn polynomials and generalize out-of-distribution is essential for simulation metamodels in many disciplines of engineering, where the time step updates are described by polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0499665578,"dev-research":0.2416456459,"prompt-eng":0.4462078718,"data-quality":0.0862829552,"ml-security":0.1592295698}}
{"text":"While feed forward neural networks can fit any function, they cannot generalize out-of-distribution for higher-order polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0149274753,"dev-research":0.1849645397,"prompt-eng":0.3769837575,"data-quality":0.1232579124,"ml-security":0.3621920562}}
{"text":"Therefore, this paper collects and proposes multiplicative neural network (MNN) architectures that are used as recursive building blocks for approximating higher-order polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0375676794,"dev-research":0.2039362416,"prompt-eng":0.3639583681,"data-quality":0.0727867305,"ml-security":0.182704116}}
{"text":"Our experiments show that MNNs are better than baseline models at generalizing, and their performance in validation is true to their performance in out-of-distribution tests.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0344801781,"dev-research":0.2391030007,"prompt-eng":0.4442089623,"data-quality":0.2199082953,"ml-security":0.1211887669}}
{"text":"In addition to MNN architectures, a simulation metamodeling approach is proposed for simulations with polynomial time step updates.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0345260445,"dev-research":0.2231901669,"prompt-eng":0.392620297,"data-quality":0.0686796675,"ml-security":0.0627494311}}
{"text":"For these simulations, simulating a time interval can be performed in fewer steps by increasing the step size, which entails approximating higher-order polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0326165288,"dev-research":0.180584139,"prompt-eng":0.377996997,"data-quality":0.0439867331,"ml-security":0.0924214974}}
{"text":"While our approach is compatible with any simulation with polynomial time step updates, a demonstration is shown for an epidemiology simulation model, which also shows the inductive bias in MNNs for learning and generalizing higher-order polynomials.","meta":{"url":"http://arxiv.org/abs/2307.10892v1"},"cats":{"new-dataset":0.0391849593,"dev-research":0.2032505158,"prompt-eng":0.4258405254,"data-quality":0.0930165104,"ml-security":0.3011042897}}
{"text":"Abstraction is a key verification technique to improve scalability.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.007850056,"dev-research":0.4325594582,"prompt-eng":0.3807796823,"data-quality":0.0973683503,"ml-security":0.1070613084}}
{"text":"However, its use for neural networks is so far extremely limited.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0149215768,"dev-research":0.2481803646,"prompt-eng":0.3317931717,"data-quality":0.091356898,"ml-security":0.2382905086}}
{"text":"Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0299196582,"dev-research":0.2986949205,"prompt-eng":0.3721943667,"data-quality":0.2439468904,"ml-security":0.1891429433}}
{"text":"We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs).","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.027785417,"dev-research":0.3301269678,"prompt-eng":0.410826228,"data-quality":0.2176720139,"ml-security":0.1118220415}}
{"text":"Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0078325498,"dev-research":0.2407828278,"prompt-eng":0.3595045753,"data-quality":0.1195888782,"ml-security":0.0811732843}}
{"text":"In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0170409301,"dev-research":0.2481467178,"prompt-eng":0.3678345516,"data-quality":0.1002003069,"ml-security":0.1604311813}}
{"text":"We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.022606711,"dev-research":0.3778407582,"prompt-eng":0.4493292559,"data-quality":0.2531632815,"ml-security":0.0572099505}}
{"text":"Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.","meta":{"url":"http://arxiv.org/abs/2307.10891v1"},"cats":{"new-dataset":0.0223903564,"dev-research":0.3952499138,"prompt-eng":0.3911307916,"data-quality":0.1748103195,"ml-security":0.0760910343}}
{"text":"The problem of matching markets has been studied for a long time in the literature due to its wide range of applications.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0376530954,"dev-research":0.1685335808,"prompt-eng":0.3563381137,"data-quality":0.1111644528,"ml-security":0.0910408991}}
{"text":"Finding a stable matching is a common equilibrium objective in this problem.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0451219693,"dev-research":0.1613410776,"prompt-eng":0.3731365186,"data-quality":0.1084365779,"ml-security":0.0748763118}}
{"text":"Since market participants are usually uncertain of their preferences, a rich line of recent works study the online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms).","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0406579355,"dev-research":0.292294221,"prompt-eng":0.4158015122,"data-quality":0.0757912813,"ml-security":0.1914762802}}
{"text":"Most previous works in this line are only able to derive theoretical guarantees for player-pessimal stable regret, which is defined compared with the players' least-preferred stable matching.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0089192704,"dev-research":0.2046273664,"prompt-eng":0.3743157492,"data-quality":0.0815812535,"ml-security":0.1397973739}}
{"text":"However, under the pessimal stable matching, players only obtain the least reward among all stable matchings.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0075790192,"dev-research":0.1871580472,"prompt-eng":0.3693778117,"data-quality":0.0992940647,"ml-security":0.1013046067}}
{"text":"To maximize players' profits, player-optimal stable matching would be the most desirable.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0187549721,"dev-research":0.2444148036,"prompt-eng":0.3769774182,"data-quality":0.063288052,"ml-security":0.0820458846}}
{"text":"Though \\citet{basu21beyond} successfully bring an upper bound for player-optimal stable regret, their result can be exponentially large if players' preference gap is small.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0237164066,"dev-research":0.2485770588,"prompt-eng":0.3829856605,"data-quality":0.0638209319,"ml-security":0.0723951436}}
{"text":"Whether a polynomial guarantee for this regret exists is a significant but still open problem.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0214276446,"dev-research":0.2379874456,"prompt-eng":0.3850459399,"data-quality":0.1144062303,"ml-security":0.1725731112}}
{"text":"In this work, we provide a new algorithm named explore-then-Gale-Shapley (ETGS) and show that the optimal stable regret of each player can be upper bounded by $O(K\\log T/\\Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $\\Delta$ is the players' minimum preference gap among the first $N+1$-ranked arms.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0576422364,"dev-research":0.1904415495,"prompt-eng":0.3936339896,"data-quality":0.0463009208,"ml-security":0.1429700459}}
{"text":"This result significantly improves previous works which either have a weaker player-pessimal stable matching objective or apply only to markets with special assumptions.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0188347741,"dev-research":0.2064323786,"prompt-eng":0.3485357332,"data-quality":0.116603465,"ml-security":0.096229283}}
{"text":"When the preferences of participants satisfy some special conditions, our regret upper bound also matches the previously derived lower bound.","meta":{"url":"http://arxiv.org/abs/2307.10890v1"},"cats":{"new-dataset":0.0144630693,"dev-research":0.2577119275,"prompt-eng":0.4312253569,"data-quality":0.0588405523,"ml-security":0.1383622052}}
{"text":"In multi-agent system design, a crucial aspect is to ensure robustness, meaning that for a coalition of agents A, small violations of adversarial assumptions only lead to small violations of A's goals.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.011575536,"dev-research":0.2953686949,"prompt-eng":0.4044783145,"data-quality":0.1152329936,"ml-security":0.4358475899}}
{"text":"In this paper we introduce a logical framework for robust strategic reasoning about multi-agent systems.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.0412893523,"dev-research":0.2978961202,"prompt-eng":0.435173197,"data-quality":0.1057872189,"ml-security":0.1390149897}}
{"text":"Specifically, inspired by recent works on robust temporal logics, we introduce and study rATL and rATL*, logics that extend the well-known Alternating-time Temporal Logic ATL and ATL* by means of an opportune multi-valued semantics for the strategy quantifiers and temporal operators.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.0569679209,"dev-research":0.2678344321,"prompt-eng":0.4137333838,"data-quality":0.1099567703,"ml-security":0.0981963827}}
{"text":"We study the model-checking and satisfiability problems for rATL and rATL* and show that dealing with robustness comes at no additional computational cost.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.0307977718,"dev-research":0.2407263968,"prompt-eng":0.4320010781,"data-quality":0.1917139544,"ml-security":0.1491703709}}
{"text":"Indeed, we show that these problems are PTime-complete and ExpTime-complete for rATL, respectively, while both are 2ExpTime-complete for rATL*.","meta":{"url":"http://arxiv.org/abs/2307.10885v1"},"cats":{"new-dataset":0.1259678984,"dev-research":0.2482395475,"prompt-eng":0.4171146287,"data-quality":0.1489256086,"ml-security":0.0553321022}}
{"text":"Trajectory and control secrecy is an important issue in robotics security.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0258562729,"dev-research":0.2465181443,"prompt-eng":0.4155922057,"data-quality":0.0616375859,"ml-security":0.4321431726}}
{"text":"This paper proposes a novel algorithm for the control input inference of a mobile agent without knowing its control objective.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0380818008,"dev-research":0.2612089206,"prompt-eng":0.4535980013,"data-quality":0.0795537519,"ml-security":0.2282684917}}
{"text":"Specifically, the algorithm first estimates the target state by applying external perturbations.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0115004449,"dev-research":0.2019592498,"prompt-eng":0.4013482842,"data-quality":0.0925266824,"ml-security":0.1406869371}}
{"text":"Then we identify the objective function based on the inverse optimal control, providing the well-posedness proof and the identifiability analysis.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0667744415,"dev-research":0.2266759153,"prompt-eng":0.406483138,"data-quality":0.0982493701,"ml-security":0.1550220957}}
{"text":"Next, we obtain the optimal estimate of the control horizon using binary search.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0651560379,"dev-research":0.1561046755,"prompt-eng":0.4029674191,"data-quality":0.0670843396,"ml-security":0.1298454025}}
{"text":"Finally, the agent's control optimization problem is reconstructed and solved to predict its input.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0407949038,"dev-research":0.2318641848,"prompt-eng":0.4576240705,"data-quality":0.0779388555,"ml-security":0.2110228032}}
{"text":"Simulation illustrates the efficiency and the performance of the algorithm.","meta":{"url":"http://arxiv.org/abs/2307.10883v1"},"cats":{"new-dataset":0.0272898264,"dev-research":0.2379613659,"prompt-eng":0.3868367214,"data-quality":0.0900091691,"ml-security":0.0491048956}}
{"text":"Encrypted mempools are a class of solutions aimed at preventing or reducing negative externalities of MEV extraction using cryptographic privacy.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.1158139175,"dev-research":0.2525836855,"prompt-eng":0.3990682484,"data-quality":0.1396636339,"ml-security":0.2969558101}}
{"text":"Mempool encryption aims to hide information related to pending transactions until a block including the transactions is committed, targeting the prevention of frontrunning and similar behaviour.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0377394809,"dev-research":0.2496841936,"prompt-eng":0.4203316003,"data-quality":0.0977974791,"ml-security":0.2441992426}}
{"text":"Among the various methods of encryption, threshold schemes are particularly interesting for the design of MEV mitigation mechanisms, as their distributed nature and minimal hardware requirements harmonize with a broader goal of decentralization.   ","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0293490498,"dev-research":0.2331926165,"prompt-eng":0.3838942224,"data-quality":0.0670617594,"ml-security":0.2998525014}}
{"text":"This work looks beyond the formal and technical cryptographic aspects of threshold encryption schemes to focus on the market and incentive implications of implementing encrypted mempools as MEV mitigation techniques.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0433829807,"dev-research":0.2310838213,"prompt-eng":0.4180983719,"data-quality":0.1087203267,"ml-security":0.3806377597}}
{"text":"In particular, this paper argues that the deployment of such protocols without proper consideration and understanding of market impact invites several undesired outcomes, with the ultimate goal of stimulating further analysis of this class of solutions outside of pure cryptograhic considerations.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0401630499,"dev-research":0.2583430907,"prompt-eng":0.3719906467,"data-quality":0.0978901055,"ml-security":0.3514917514}}
{"text":"Included in the paper is an overview of a series of problems, various candidate solutions in the form of mempool encryption techniques with a focus on threshold encryption, potential drawbacks to these solutions, and Osmosis as a case study.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0625599308,"dev-research":0.1855359784,"prompt-eng":0.4069549022,"data-quality":0.1075592672,"ml-security":0.3009174989}}
{"text":"The paper targets a broad audience and remains agnostic to blockchain design where possible while drawing from mostly financial examples.","meta":{"url":"http://arxiv.org/abs/2307.10878v1"},"cats":{"new-dataset":0.0333230947,"dev-research":0.2857050004,"prompt-eng":0.3535359953,"data-quality":0.0746833688,"ml-security":0.1215602768}}
{"text":"Industry standard frameworks are now widespread for labeling the high-level stages and granular actions of attacker and defender behavior in cyberspace.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.1218614206,"dev-research":0.3568246813,"prompt-eng":0.4536703316,"data-quality":0.1703435804,"ml-security":0.5816107317}}
{"text":"While these labels are used for atomic actions, and to some extent for sequences of actions, there remains a need for labeled data from realistic full-scale attacks.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.1163116785,"dev-research":0.3352751532,"prompt-eng":0.4264276465,"data-quality":0.3699912216,"ml-security":0.4796901012}}
{"text":"This data is valuable for better understanding human actors' decisions, behaviors, and individual attributes.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.383448596,"dev-research":0.3515919424,"prompt-eng":0.366050912,"data-quality":0.0851293694,"ml-security":0.1388400326}}
{"text":"The analysis could lead to more effective attribution and disruption of attackers.   ","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.0341967614,"dev-research":0.4370873069,"prompt-eng":0.4279464506,"data-quality":0.161507402,"ml-security":0.7807617556}}
{"text":"We present a methodological approach and exploratory case study for systematically analyzing human behavior during a cyber offense/defense capture-the-flag (CTF) game.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.2902866731,"dev-research":0.4402242884,"prompt-eng":0.4284842742,"data-quality":0.1244932275,"ml-security":0.3526661398}}
{"text":"We describe the data collection and analysis to derive a metric called keystroke accuracy.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.1942030918,"dev-research":0.4000887666,"prompt-eng":0.4250956877,"data-quality":0.2092914281,"ml-security":0.1054712279}}
{"text":"After collecting players' commands, we label them using the MITRE ATT&CK framework using a new tool called Pathfinder.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.3087460692,"dev-research":0.3351979773,"prompt-eng":0.5259795839,"data-quality":0.2138868197,"ml-security":0.0531881747}}
{"text":"We present results from preliminary analysis of participants' keystroke accuracy and its relation to score outcome in CTF games.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.1164169748,"dev-research":0.3850734655,"prompt-eng":0.4425299096,"data-quality":0.1685219432,"ml-security":0.1073235581}}
{"text":"We describe frequency of action classification within the MITRE ATT&CK framework and discuss some of the mathematical trends suggested by our observations.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.0974876772,"dev-research":0.2094992616,"prompt-eng":0.4189546725,"data-quality":0.1201966245,"ml-security":0.1102544099}}
{"text":"We conclude with a discussion of extensions for the methodology, including performance evaluation during games and the potential use of this methodology for training artificial intelligence.","meta":{"url":"http://arxiv.org/abs/2307.10877v1"},"cats":{"new-dataset":0.0429730284,"dev-research":0.342200859,"prompt-eng":0.3984794684,"data-quality":0.101279072,"ml-security":0.17812914}}
{"text":"The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.1405068474,"dev-research":0.2495207513,"prompt-eng":0.3808795292,"data-quality":0.2757113922,"ml-security":0.3916036617}}
{"text":"To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.2694045007,"dev-research":0.2875695801,"prompt-eng":0.3580064407,"data-quality":0.3085098536,"ml-security":0.2805783791}}
{"text":"Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.0382062291,"dev-research":0.2880219098,"prompt-eng":0.4685787032,"data-quality":0.221284459,"ml-security":0.2645397871}}
{"text":"We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.0475044292,"dev-research":0.2683281791,"prompt-eng":0.4210857249,"data-quality":0.127382922,"ml-security":0.2132469983}}
{"text":"The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.0319226411,"dev-research":0.2619916951,"prompt-eng":0.3733174665,"data-quality":0.3250884409,"ml-security":0.4221600848}}
{"text":"Despite requiring no additional training effort, our method produces exceptional results in various removal-and-classification experiments for noisy point clouds, which are corrupted by random noise, adversarial noise, and backdoor trigger noise.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.1079854794,"dev-research":0.216644885,"prompt-eng":0.3547328281,"data-quality":0.440686619,"ml-security":0.426855096}}
{"text":"Impressively, it achieves 87% accuracy in defense against the backdoor attack by removing triggers.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.0233092853,"dev-research":0.3772313729,"prompt-eng":0.433106349,"data-quality":0.1650922672,"ml-security":0.4009483067}}
{"text":"Overall, the proposed PointCVaR effectively eliminates noise points and enhances point cloud classification, making it a promising plug-in module for various models in different scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10875v1"},"cats":{"new-dataset":0.1650520125,"dev-research":0.2798953804,"prompt-eng":0.4000092799,"data-quality":0.2279603287,"ml-security":0.1212822685}}
{"text":"Having efficient testing strategies is a core challenge that needs to be overcome for the release of automated driving.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0289439062,"dev-research":0.3980827278,"prompt-eng":0.4727748946,"data-quality":0.1184107294,"ml-security":0.2035336975}}
{"text":"This necessitates clear requirements as well as suitable methods for testing.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0234039985,"dev-research":0.3280960009,"prompt-eng":0.461454112,"data-quality":0.1500668606,"ml-security":0.1120840114}}
{"text":"In this work, the requirements for perception modules are considered with respect to relevance.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0508479057,"dev-research":0.2573784018,"prompt-eng":0.4542663487,"data-quality":0.1791446459,"ml-security":0.0456933246}}
{"text":"The concept of relevance currently remains insufficiently defined and specified.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0256943817,"dev-research":0.3125623557,"prompt-eng":0.3314797439,"data-quality":0.267963097,"ml-security":0.0833699097}}
{"text":"In this paper, we propose a novel methodology to overcome this challenge by exemplary application to collision safety in the highway domain.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.1655799923,"dev-research":0.2920158957,"prompt-eng":0.3600992839,"data-quality":0.1486921503,"ml-security":0.2497099334}}
{"text":"Using this general system and use case specification, a corresponding concept for relevance is derived.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0274903739,"dev-research":0.3817735477,"prompt-eng":0.4619508734,"data-quality":0.1236212675,"ml-security":0.0888049987}}
{"text":"Irrelevant objects are thus defined as objects which do not limit the set of safe actions available to the ego vehicle under consideration of all uncertainties.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0328679634,"dev-research":0.276171085,"prompt-eng":0.4094450338,"data-quality":0.1594482633,"ml-security":0.1853239827}}
{"text":"As an initial step, the use case is decomposed into functional scenarios with respect to collision relevance.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0154414358,"dev-research":0.3738921384,"prompt-eng":0.4242820754,"data-quality":0.0718505229,"ml-security":0.1610126871}}
{"text":"For each functional scenario, possible actions of both the ego vehicle and any other dynamic object are formalized as equations.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0195428411,"dev-research":0.2604685267,"prompt-eng":0.4568715774,"data-quality":0.0773426734,"ml-security":0.1107284705}}
{"text":"This set of possible actions is constrained by traffic rules, yielding relevance criteria.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0265370361,"dev-research":0.2475506518,"prompt-eng":0.4030179328,"data-quality":0.0931205962,"ml-security":0.1599289905}}
{"text":"As a result, we present a conservative estimation which dynamic objects are relevant for perception and need to be considered for a complete evaluation.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.1170667219,"dev-research":0.2057307689,"prompt-eng":0.4272046559,"data-quality":0.1429036216,"ml-security":0.0713922881}}
{"text":"The estimation provides requirements which are applicable for offline testing and validation of perception components.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0535563441,"dev-research":0.2508201093,"prompt-eng":0.4555735482,"data-quality":0.1502209528,"ml-security":0.0644488549}}
{"text":"A visualization is presented for examples from the highD dataset, showing the plausibility of the results.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.7663408891,"dev-research":0.3059659937,"prompt-eng":0.380516958,"data-quality":0.1336333129,"ml-security":0.1292173819}}
{"text":"Finally, a possibility for a future validation of the presented relevance concept is outlined.","meta":{"url":"http://arxiv.org/abs/2307.10873v1"},"cats":{"new-dataset":0.0219323846,"dev-research":0.3593265707,"prompt-eng":0.4664044542,"data-quality":0.1964602915,"ml-security":0.070454962}}
{"text":"Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0228425276,"dev-research":0.2929527709,"prompt-eng":0.365261429,"data-quality":0.1399159081,"ml-security":0.2363845498}}
{"text":"To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0539160392,"dev-research":0.3729054033,"prompt-eng":0.4322976277,"data-quality":0.382546302,"ml-security":0.089692241}}
{"text":"Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.1531746436,"dev-research":0.2810211989,"prompt-eng":0.3818055154,"data-quality":0.0527633306,"ml-security":0.1082053827}}
{"text":"Some existing methods tackle this problem by analyzing each metric independently to detect anomalies.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.1202888655,"dev-research":0.2966111862,"prompt-eng":0.4056364644,"data-quality":0.3992073007,"ml-security":0.1665488538}}
{"text":"However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0494540238,"dev-research":0.4266400293,"prompt-eng":0.5107843044,"data-quality":0.2713587383,"ml-security":0.2400109785}}
{"text":"To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.1039540945,"dev-research":0.3320642266,"prompt-eng":0.375425317,"data-quality":0.1729997161,"ml-security":0.1341580092}}
{"text":"However, most of the studies fall short of extracting these two types of features explicitly.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0364454984,"dev-research":0.3307485825,"prompt-eng":0.3636434346,"data-quality":0.1941839238,"ml-security":0.1011054578}}
{"text":"Moreover, there exist some unlabeled anomalies mixed in the training data, which may hinder the detection performance.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0408529263,"dev-research":0.2718652582,"prompt-eng":0.4016479,"data-quality":0.5446580625,"ml-security":0.3742671085}}
{"text":"To address these limitations, we propose the Relational- Temporal Anomaly Detection Model (RTAnomaly) that combines the relational and temporal information of metrics.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.1521744983,"dev-research":0.3071785645,"prompt-eng":0.3742779097,"data-quality":0.2237086761,"ml-security":0.3251536824}}
{"text":"RTAnomaly employs a graph attention layer to learn the dependencies among metrics, which will further help pinpoint the anomalous metrics that may cause the anomaly effectively.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.1332643063,"dev-research":0.3430293749,"prompt-eng":0.4075064455,"data-quality":0.2685822826,"ml-security":0.2148862182}}
{"text":"In addition, we exploit the concept of positive unlabeled learning to address the issue of potential anomalies in the training data.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0685849516,"dev-research":0.2676825525,"prompt-eng":0.442905349,"data-quality":0.6015160812,"ml-security":0.3691507633}}
{"text":"To evaluate our method, we conduct experiments on a public dataset and two industrial datasets.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.5400447769,"dev-research":0.2794894457,"prompt-eng":0.376934397,"data-quality":0.1789336869,"ml-security":0.2004172493}}
{"text":"RTAnomaly outperforms all the baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920, demonstrating its superiority.","meta":{"url":"http://arxiv.org/abs/2307.10869v1"},"cats":{"new-dataset":0.0799850519,"dev-research":0.301533906,"prompt-eng":0.407933782,"data-quality":0.1596325062,"ml-security":0.1174991337}}
{"text":"Captions are crucial for understanding scientific visualizations and documents.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.2027368236,"dev-research":0.3831954138,"prompt-eng":0.4023106262,"data-quality":0.2243137115,"ml-security":0.0508518243}}
{"text":"Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.2072920406,"dev-research":0.3329679329,"prompt-eng":0.4408857593,"data-quality":0.4008867972,"ml-security":0.0567810204}}
{"text":"To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.3086855692,"dev-research":0.3539305556,"prompt-eng":0.5140813032,"data-quality":0.2808363411,"ml-security":0.0523133939}}
{"text":"Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.1512522646,"dev-research":0.2569886869,"prompt-eng":0.4994352356,"data-quality":0.2170101836,"ml-security":0.0468626521}}
{"text":"We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.0480526537,"dev-research":0.2744777093,"prompt-eng":0.4354508016,"data-quality":0.2208935306,"ml-security":0.2043548384}}
{"text":"In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.0802389734,"dev-research":0.2146112763,"prompt-eng":0.3981215812,"data-quality":0.0821323421,"ml-security":0.0772233536}}
{"text":"Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.","meta":{"url":"http://arxiv.org/abs/2307.10867v1"},"cats":{"new-dataset":0.5960174785,"dev-research":0.2462909633,"prompt-eng":0.4077281414,"data-quality":0.2457195836,"ml-security":0.0456309922}}
{"text":"Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.1354964026,"dev-research":0.2997839562,"prompt-eng":0.2953003863,"data-quality":0.1328142799,"ml-security":0.1981978896}}
{"text":"In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.020729716,"dev-research":0.2151865973,"prompt-eng":0.3591083537,"data-quality":0.1729241113,"ml-security":0.2613132865}}
{"text":"Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.0759676538,"dev-research":0.2406577793,"prompt-eng":0.3382887735,"data-quality":0.2410959606,"ml-security":0.3390559619}}
{"text":"Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.0096261594,"dev-research":0.2123831963,"prompt-eng":0.3254110149,"data-quality":0.1724892436,"ml-security":0.2131123476}}
{"text":"Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.0369387377,"dev-research":0.2156887486,"prompt-eng":0.3340086473,"data-quality":0.1312323583,"ml-security":0.247681136}}
{"text":"This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation.","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.1140663673,"dev-research":0.2984756303,"prompt-eng":0.3364193132,"data-quality":0.230572143,"ml-security":0.142159571}}
{"text":"Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence .","meta":{"url":"http://arxiv.org/abs/2307.10865v1"},"cats":{"new-dataset":0.2352060812,"dev-research":0.2733861584,"prompt-eng":0.3485840176,"data-quality":0.1583359863,"ml-security":0.0804012887}}
{"text":"Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0971550219,"dev-research":0.199051934,"prompt-eng":0.4657412343,"data-quality":0.1616564214,"ml-security":0.0723167654}}
{"text":"Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.1371214542,"dev-research":0.2344612351,"prompt-eng":0.5816179578,"data-quality":0.1315033006,"ml-security":0.1013070692}}
{"text":"Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0437396982,"dev-research":0.2224269789,"prompt-eng":0.4620299456,"data-quality":0.121592833,"ml-security":0.0481624987}}
{"text":"It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0795403212,"dev-research":0.3786282763,"prompt-eng":0.6230814274,"data-quality":0.1689955718,"ml-security":0.1277277863}}
{"text":"However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0033789264,"dev-research":0.2952339562,"prompt-eng":0.508915366,"data-quality":0.19213631,"ml-security":0.1729425502}}
{"text":"To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0666469546,"dev-research":0.3591780347,"prompt-eng":0.5131062513,"data-quality":0.1269071798,"ml-security":0.1093165758}}
{"text":"We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.0376467249,"dev-research":0.188083355,"prompt-eng":0.3717953063,"data-quality":0.1494599067,"ml-security":0.1627986324}}
{"text":"Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.1182496098,"dev-research":0.3756782689,"prompt-eng":0.5045998923,"data-quality":0.1621657095,"ml-security":0.0677580715}}
{"text":"More videos and updates can be found on the project page \\url{https://sites.google.com/view/divide-and-bind}.","meta":{"url":"http://arxiv.org/abs/2307.10864v1"},"cats":{"new-dataset":0.3375760176,"dev-research":0.3107617135,"prompt-eng":0.3217917138,"data-quality":0.0936092024,"ml-security":0.0722985406}}
{"text":"The great advancements of generative adversarial networks and face recognition models in computer vision have made it possible to swap identities on images from single sources.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0300082575,"dev-research":0.2318940695,"prompt-eng":0.405656846,"data-quality":0.21279404,"ml-security":0.3297448006}}
{"text":"Although a lot of studies seems to have proposed almost satisfactory solutions, we notice previous methods still suffer from an identity-attribute entanglement that causes undesired attributes swapping because widely used identity encoders, eg, ArcFace, have some crucial attribute biases owing to their pretraining on face recognition tasks.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0089486341,"dev-research":0.2905742027,"prompt-eng":0.400907724,"data-quality":0.1925144328,"ml-security":0.2573711789}}
{"text":"To address this issue, we design BlendFace, a novel identity encoder for face-swapping.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0463441899,"dev-research":0.3031515099,"prompt-eng":0.3873201097,"data-quality":0.1498214988,"ml-security":0.1919095858}}
{"text":"The key idea behind BlendFace is training face recognition models on blended images whose attributes are replaced with those of another mitigates inter-personal biases such as hairsyles.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0230750456,"dev-research":0.290260472,"prompt-eng":0.3820010876,"data-quality":0.1350862346,"ml-security":0.2028649498}}
{"text":"BlendFace feeds disentangled identity features into generators and guides generators properly as an identity loss function.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0235565007,"dev-research":0.275389099,"prompt-eng":0.3898591684,"data-quality":0.2116468611,"ml-security":0.1524304553}}
{"text":"Extensive experiments demonstrate that BlendFace improves the identity-attribute disentanglement in face-swapping models, maintaining a comparable quantitative performance to previous methods.","meta":{"url":"http://arxiv.org/abs/2307.10854v1"},"cats":{"new-dataset":0.0104241685,"dev-research":0.2653033937,"prompt-eng":0.3559895743,"data-quality":0.1295050338,"ml-security":0.1340627023}}
{"text":"Weakly-supervised change detection (WSCD) aims to detect pixel-level changes with only image-level annotations.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.1693780195,"dev-research":0.3228498361,"prompt-eng":0.470189956,"data-quality":0.3884564461,"ml-security":0.1139860646}}
{"text":"Owing to its label efficiency, WSCD is drawing increasing attention recently.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.117929409,"dev-research":0.3936815523,"prompt-eng":0.4433421252,"data-quality":0.2786931061,"ml-security":0.0635226376}}
{"text":"However, current WSCD methods often encounter the challenge of change missing and fabricating, i.e., the inconsistency between image-level annotations and pixel-level predictions.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0906410152,"dev-research":0.3323107872,"prompt-eng":0.4649558903,"data-quality":0.4132457533,"ml-security":0.0681876124}}
{"text":"Specifically, change missing refer to the situation that the WSCD model fails to predict any changed pixels, even though the image-level label indicates changed, and vice versa for change fabricating.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0677663919,"dev-research":0.2714993028,"prompt-eng":0.4430399783,"data-quality":0.3908746051,"ml-security":0.0620186499}}
{"text":"To address this challenge, in this work, we leverage global-scale and local-scale priors in WSCD and propose two components: a Dilated Prior (DP) decoder and a Label Gated (LG) constraint.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0790706072,"dev-research":0.2190939663,"prompt-eng":0.4697490921,"data-quality":0.1954713735,"ml-security":0.0995325312}}
{"text":"The DP decoder decodes samples with the changed image-level label, skips samples with the unchanged label, and replaces them with an all-unchanged pixel-level label.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0676146851,"dev-research":0.278583871,"prompt-eng":0.4608190805,"data-quality":0.3299405645,"ml-security":0.0760036172}}
{"text":"The LG constraint is derived from the correspondence between changed representations and image-level labels, penalizing the model when it mispredicts the change status.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0165023999,"dev-research":0.2035022947,"prompt-eng":0.462103809,"data-quality":0.3016221373,"ml-security":0.0971858341}}
{"text":"Additionally, we develop TransWCD, a simple yet powerful transformer-based model, showcasing the potential of weakly-supervised learning in change detection.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.1115625884,"dev-research":0.2936174656,"prompt-eng":0.4471195881,"data-quality":0.2680406173,"ml-security":0.2214501955}}
{"text":"By integrating the DP decoder and LG constraint into TransWCD, we form TransWCD-DL.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.0520942786,"dev-research":0.2149267478,"prompt-eng":0.4652697679,"data-quality":0.1032582264,"ml-security":0.0593523316}}
{"text":"Our proposed TransWCD and TransWCD-DL achieve significant +6.33% and +9.55% F1 score improvements over the state-of-the-art methods on the WHU-CD dataset, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.2788293086,"dev-research":0.3183921843,"prompt-eng":0.4141175536,"data-quality":0.1872811768,"ml-security":0.0471955111}}
{"text":"Some performance metrics even exceed several fully-supervised change detection (FSCD) competitors.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.1037222548,"dev-research":0.3083091709,"prompt-eng":0.4384631366,"data-quality":0.2982238075,"ml-security":0.2006151957}}
{"text":"Code will be available at https://github.com/zhenghuizhao/TransWCD.","meta":{"url":"http://arxiv.org/abs/2307.10853v1"},"cats":{"new-dataset":0.2061094408,"dev-research":0.2787661686,"prompt-eng":0.4564441491,"data-quality":0.0966719006,"ml-security":0.0375307472}}
{"text":"In this paper, we present novel algorithms that efficiently compute a shortest reconfiguration sequence between two given dominating sets in trees and interval graphs under the Token Sliding model.","meta":{"url":"http://arxiv.org/abs/2307.10847v1"},"cats":{"new-dataset":0.2103893718,"dev-research":0.2279037689,"prompt-eng":0.3478230386,"data-quality":0.0786584675,"ml-security":0.0863100416}}
{"text":"In this problem, a graph is provided along with its two dominating sets, which can be imagined as tokens placed on vertices.","meta":{"url":"http://arxiv.org/abs/2307.10847v1"},"cats":{"new-dataset":0.0636552246,"dev-research":0.2450707466,"prompt-eng":0.4125011864,"data-quality":0.1972686984,"ml-security":0.1088654681}}
{"text":"The objective is to find a shortest sequence of dominating sets that transforms one set into the other, with each set in the sequence resulting from sliding a single token in the previous set.","meta":{"url":"http://arxiv.org/abs/2307.10847v1"},"cats":{"new-dataset":0.1846866544,"dev-research":0.2117607777,"prompt-eng":0.3937263732,"data-quality":0.0771309889,"ml-security":0.0874454603}}
{"text":"While identifying any sequence has been well studied, our work presents the first polynomial algorithms for this optimization variant in the context of dominating sets.","meta":{"url":"http://arxiv.org/abs/2307.10847v1"},"cats":{"new-dataset":0.1648172311,"dev-research":0.201528363,"prompt-eng":0.3627301615,"data-quality":0.08881016,"ml-security":0.1611262211}}
{"text":"Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to spontaneously set diverse goals to learn a set of skills.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0282803635,"dev-research":0.2081538018,"prompt-eng":0.4703538781,"data-quality":0.056321907,"ml-security":0.1519962738}}
{"text":"Despite the excellent works proposed in various fields, reaching distant goals in temporally extended tasks remains a challenge for GCRL.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0338317857,"dev-research":0.2093909732,"prompt-eng":0.418439344,"data-quality":0.0470749166,"ml-security":0.0526090013}}
{"text":"Current works tackled this problem by leveraging planning algorithms to plan intermediate subgoals to augment GCRL.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.059919579,"dev-research":0.1990159593,"prompt-eng":0.4010925077,"data-quality":0.0468068707,"ml-security":0.0245232071}}
{"text":"Their methods need two crucial requirements: (i) a state representation space to search valid subgoals, and (ii) a distance function to measure the reachability of subgoals.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0298893564,"dev-research":0.2181746805,"prompt-eng":0.3968689132,"data-quality":0.0825586478,"ml-security":0.0422240894}}
{"text":"However, they struggle to scale to high-dimensional state space due to their non-compact representations.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0256377578,"dev-research":0.1926283595,"prompt-eng":0.3722439309,"data-quality":0.0725001475,"ml-security":0.0984586133}}
{"text":"Moreover, they cannot collect high-quality training data through standard GC policies, which results in an inaccurate distance function.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0408949818,"dev-research":0.2039567453,"prompt-eng":0.3274592086,"data-quality":0.2724215265,"ml-security":0.190013644}}
{"text":"Both affect the efficiency and performance of planning and policy learning.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0140738658,"dev-research":0.3133534761,"prompt-eng":0.3975272971,"data-quality":0.0636684217,"ml-security":0.0938645564}}
{"text":"In the paper, we propose a goal-conditioned RL algorithm combined with Disentanglement-based Reachability Planning (REPlan) to solve temporally extended tasks.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0583444794,"dev-research":0.2140822734,"prompt-eng":0.3771685246,"data-quality":0.0336340863,"ml-security":0.041098823}}
{"text":"In REPlan, a Disentangled Representation Module (DRM) is proposed to learn compact representations which disentangle robot poses and object positions from high-dimensional observations in a self-supervised manner.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.1386885989,"dev-research":0.204962203,"prompt-eng":0.3828562449,"data-quality":0.1002180493,"ml-security":0.0976931919}}
{"text":"A simple REachability discrimination Module (REM) is also designed to determine the temporal distance of subgoals.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0593465393,"dev-research":0.2113847671,"prompt-eng":0.4455134609,"data-quality":0.0776234154,"ml-security":0.0527631174}}
{"text":"Moreover, REM computes intrinsic bonuses to encourage the collection of novel states for training.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0108080251,"dev-research":0.2335061685,"prompt-eng":0.4513549868,"data-quality":0.0797497401,"ml-security":0.1430914689}}
{"text":"We evaluate our REPlan in three vision-based simulation tasks and one real-world task.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0979515151,"dev-research":0.2493310733,"prompt-eng":0.4272328497,"data-quality":0.0725902864,"ml-security":0.0474217685}}
{"text":"The experiments demonstrate that our REPlan significantly outperforms the prior state-of-the-art methods in solving temporally extended tasks.","meta":{"url":"http://arxiv.org/abs/2307.10846v1"},"cats":{"new-dataset":0.0155897991,"dev-research":0.2832273672,"prompt-eng":0.4150950126,"data-quality":0.0487419988,"ml-security":0.0603404009}}
{"text":"Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0479680169,"dev-research":0.232433265,"prompt-eng":0.4285040886,"data-quality":0.1780177758,"ml-security":0.2140468854}}
{"text":"However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0096776448,"dev-research":0.2748703626,"prompt-eng":0.3944906368,"data-quality":0.1363763621,"ml-security":0.2073978914}}
{"text":"To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0435431381,"dev-research":0.2317812186,"prompt-eng":0.4161592926,"data-quality":0.1386882984,"ml-security":0.1009368521}}
{"text":"To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy).","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0421556206,"dev-research":0.3683497059,"prompt-eng":0.4698983469,"data-quality":0.140564901,"ml-security":0.0623781503}}
{"text":"When encountering a new task, all previous tasks are sorted from \"difficult\" to \"easy\" based on the priorities.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.023006368,"dev-research":0.3929386792,"prompt-eng":0.4332689497,"data-quality":0.081495375,"ml-security":0.0514963954}}
{"text":"Then the parameters of the new continual learner will be learned via selectively maintaining the knowledge amongst more difficult past tasks, which could well overcome catastrophic forgetting with less computational cost.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0180130768,"dev-research":0.2990366149,"prompt-eng":0.4315118288,"data-quality":0.1282890472,"ml-security":0.2087749226}}
{"text":"We adopt an alternative convex search to iteratively update the model parameters and priority weights in the bi-convex formulation.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.0108079837,"dev-research":0.1589112215,"prompt-eng":0.4293548449,"data-quality":0.078240378,"ml-security":0.0918325783}}
{"text":"The proposed spWC framework is plug-and-play, which is applicable to most continual learning algorithms (e.g., EWC, MAS and RCIL) in different directions (e.g., classification and segmentation).","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.1228033065,"dev-research":0.2367438144,"prompt-eng":0.3649744085,"data-quality":0.1470001814,"ml-security":0.1276642047}}
{"text":"Experimental results on several public benchmark datasets demonstrate that our proposed framework can effectively improve performance when compared with other popular continual learning algorithms.","meta":{"url":"http://arxiv.org/abs/2307.10845v1"},"cats":{"new-dataset":0.1742612116,"dev-research":0.2254562977,"prompt-eng":0.3599655439,"data-quality":0.2135189736,"ml-security":0.1800776314}}
{"text":"This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.3254113093,"dev-research":0.218989705,"prompt-eng":0.4183710809,"data-quality":0.1274645812,"ml-security":0.140485081}}
{"text":"The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS).","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.2223997588,"dev-research":0.1748287811,"prompt-eng":0.4133014756,"data-quality":0.0798324113,"ml-security":0.1164331072}}
{"text":"The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.0472607374,"dev-research":0.2548621485,"prompt-eng":0.4068185236,"data-quality":0.2519720489,"ml-security":0.211158873}}
{"text":"The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (>8 mm/hr), in terms of the critical success index (CSI)..","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.1584771574,"dev-research":0.2431811681,"prompt-eng":0.4100760291,"data-quality":0.1840090856,"ml-security":0.1476707066}}
{"text":"Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probability distribution to the IMERG than the regression network.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.1227826974,"dev-research":0.2036427145,"prompt-eng":0.4193095083,"data-quality":0.2076465297,"ml-security":0.1655920469}}
{"text":"It is uncovered that the inclusion of the physical variables can improve precipitation nowcasting, especially at longer lead times in both networks.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.0402982993,"dev-research":0.2790577475,"prompt-eng":0.4277825329,"data-quality":0.1050599606,"ml-security":0.1032995506}}
{"text":"Taking IMERG as a relative reference, a multi-scale analysis in terms of fractions skill score (FSS), shows that the nowcasting machine remains skillful (FSS > 0.5) at the resolution of 10 km compared to 50 km for GFS.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.092085663,"dev-research":0.2277212014,"prompt-eng":0.4148246132,"data-quality":0.1019550377,"ml-security":0.0403010699}}
{"text":"For precipitation rates greater than 4~mm/hr, only the classification network remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.","meta":{"url":"http://arxiv.org/abs/2307.10843v1"},"cats":{"new-dataset":0.1337992131,"dev-research":0.2160055027,"prompt-eng":0.4029424515,"data-quality":0.1686228058,"ml-security":0.1090754957}}
{"text":"Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain.","meta":{"url":"http://arxiv.org/abs/2307.10842v1"},"cats":{"new-dataset":0.1493230887,"dev-research":0.2578302747,"prompt-eng":0.4548739565,"data-quality":0.223872401,"ml-security":0.1093569801}}
{"text":"We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities.","meta":{"url":"http://arxiv.org/abs/2307.10842v1"},"cats":{"new-dataset":0.1643077127,"dev-research":0.2590194056,"prompt-eng":0.5182055984,"data-quality":0.430075744,"ml-security":0.20827359}}
{"text":"The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements.","meta":{"url":"http://arxiv.org/abs/2307.10842v1"},"cats":{"new-dataset":0.0286718089,"dev-research":0.2593556316,"prompt-eng":0.4171703004,"data-quality":0.1079837329,"ml-security":0.0558963792}}
{"text":"We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.","meta":{"url":"http://arxiv.org/abs/2307.10842v1"},"cats":{"new-dataset":0.1957170878,"dev-research":0.2474551127,"prompt-eng":0.4110699778,"data-quality":0.5370026911,"ml-security":0.136852232}}
{"text":"Soft robots have been leveraged in considerable areas like surgery, rehabilitation, and bionics due to their softness, flexibility, and safety.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0281390161,"dev-research":0.2480163745,"prompt-eng":0.3627031167,"data-quality":0.0674101481,"ml-security":0.0930584885}}
{"text":"However, it is challenging to produce two same soft robots even with the same mold and manufacturing process owing to the complexity of soft materials.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0149317875,"dev-research":0.2521774683,"prompt-eng":0.4041002172,"data-quality":0.0629631936,"ml-security":0.0508944603}}
{"text":"Meanwhile, widespread usage of a system requires the ability to fabricate replaceable components, which is interchangeability.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0075007416,"dev-research":0.3594233193,"prompt-eng":0.4075179197,"data-quality":0.0852263839,"ml-security":0.074923585}}
{"text":"Due to the necessity of this property, a hybrid adaptive controller is introduced to achieve interchangeability from the perspective of control approaches.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0088396648,"dev-research":0.2066281538,"prompt-eng":0.3777712165,"data-quality":0.0688178067,"ml-security":0.0601446384}}
{"text":"This method utilizes an offline trained recurrent neural network controller to cope with the nonlinear and delayed response from soft robots.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0715077283,"dev-research":0.1770672773,"prompt-eng":0.4112985413,"data-quality":0.0841727302,"ml-security":0.096060684}}
{"text":"Furthermore, an online optimizing kinematics controller is applied to decrease the error caused by the above neural network controller.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0191452964,"dev-research":0.2707371676,"prompt-eng":0.3773315727,"data-quality":0.1203403788,"ml-security":0.0930096629}}
{"text":"Soft pneumatic robots with different deformation properties but the same mold have been included for validation experiments.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.019188799,"dev-research":0.2309797552,"prompt-eng":0.3876212211,"data-quality":0.1033164724,"ml-security":0.0788370023}}
{"text":"In the experiments, the systems with different actuation configurations and the different robots follow the desired trajectory with errors of 0.040 and 0.030 compared with the working space length, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0281793077,"dev-research":0.220493813,"prompt-eng":0.3718519033,"data-quality":0.1067888337,"ml-security":0.0580574515}}
{"text":"Such an adaptive controller also shows good performance on different control frequencies and desired velocities.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.019967408,"dev-research":0.2396509848,"prompt-eng":0.4106580526,"data-quality":0.0583015598,"ml-security":0.0414141555}}
{"text":"This controller endows soft robots with the potential for wide application, and future work may include different offline and online controllers.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0396270354,"dev-research":0.2410215604,"prompt-eng":0.418758014,"data-quality":0.0483240995,"ml-security":0.066515854}}
{"text":"A weight parameter adjusting strategy may also be proposed in the future.","meta":{"url":"http://arxiv.org/abs/2307.10838v1"},"cats":{"new-dataset":0.0041087484,"dev-research":0.222241845,"prompt-eng":0.4647235247,"data-quality":0.059739679,"ml-security":0.0931993493}}
{"text":"This paper proposes a grant-free massive access scheme based on the millimeter wave (mmWave) extra-large-scale multiple-input multiple-output (XL-MIMO) to support massive Internet-of-Things (IoT) devices with low latency, high data rate, and high localization accuracy in the upcoming sixth-generation (6G) networks.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0606118128,"dev-research":0.2336863956,"prompt-eng":0.3628343425,"data-quality":0.0526905513,"ml-security":0.0741728086}}
{"text":"The XL-MIMO consists of multiple antenna subarrays that are widely spaced over the service area to ensure line-of-sight (LoS) transmissions.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0586360874,"dev-research":0.2268753327,"prompt-eng":0.3828311236,"data-quality":0.0797376865,"ml-security":0.0735793921}}
{"text":"First, we establish the XL-MIMO-based massive access model considering the near-field spatial non-stationary (SNS) property.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0393293394,"dev-research":0.1408307483,"prompt-eng":0.3607379937,"data-quality":0.0564567542,"ml-security":0.09427771}}
{"text":"Then, by exploiting the block sparsity of subarrays and the SNS property, we propose a structured block orthogonal matching pursuit algorithm for efficient active user detection (AUD) and channel estimation (CE).","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0594777837,"dev-research":0.238705308,"prompt-eng":0.3875778027,"data-quality":0.1509582092,"ml-security":0.2004314632}}
{"text":"Furthermore, different sensing matrices are applied in different pilot subcarriers for exploiting the diversity gains.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0172345808,"dev-research":0.217323918,"prompt-eng":0.3853050861,"data-quality":0.1196574283,"ml-security":0.1165541567}}
{"text":"Additionally, a multi-subarray collaborative localization algorithm is designed for localization.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.0775484081,"dev-research":0.3205536503,"prompt-eng":0.3906642073,"data-quality":0.1357222461,"ml-security":0.059455695}}
{"text":"In particular, the angle of arrival (AoA) and time difference of arrival (TDoA) of the LoS links between active users and related subarrays are extracted from the estimated XL-MIMO channels, and then the coordinates of active users are acquired by jointly utilizing the AoAs and TDoAs.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.044757914,"dev-research":0.2528193756,"prompt-eng":0.4152429798,"data-quality":0.065451215,"ml-security":0.0485045704}}
{"text":"Simulation results show that the proposed algorithms outperform existing algorithms in terms of AUD and CE performance and can achieve centimeter-level localization accuracy.","meta":{"url":"http://arxiv.org/abs/2307.10837v1"},"cats":{"new-dataset":0.1049990226,"dev-research":0.2416432407,"prompt-eng":0.4065153857,"data-quality":0.1971795062,"ml-security":0.0495770646}}
{"text":"Miller recently proposed a definition of contrastive (counterfactual) explanations based on the well-known Halpern-Pearl (HP) definitions of causes and (non-contrastive) explanations.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0342624613,"dev-research":0.3844219596,"prompt-eng":0.4375540524,"data-quality":0.2557976586,"ml-security":0.2070397687}}
{"text":"Crucially, the Miller definition was based on the original HP definition of explanations, but this has since been modified by Halpern; presumably because the original yields counterintuitive results in many standard examples.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0078463708,"dev-research":0.3243842823,"prompt-eng":0.3960455904,"data-quality":0.1791110364,"ml-security":0.1486227699}}
{"text":"More recently Borner has proposed a third definition, observing that this modified HP definition may also yield counterintuitive results.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0160557646,"dev-research":0.3506999832,"prompt-eng":0.4458851938,"data-quality":0.2038041213,"ml-security":0.0795234781}}
{"text":"In this paper we show that the Miller definition inherits issues found in the original HP definition.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0300568656,"dev-research":0.2542622395,"prompt-eng":0.3844537631,"data-quality":0.1942333823,"ml-security":0.105663765}}
{"text":"We address these issues by proposing two improved variants based on the more robust modified HP and Borner definitions.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0835184488,"dev-research":0.3238315475,"prompt-eng":0.4421991974,"data-quality":0.2367603183,"ml-security":0.079943907}}
{"text":"We analyse our new definitions and show that they retain the spirit of the Miller definition where all three variants satisfy an alternative unified definition that is modular with respect to an underlying definition of non-contrastive explanations.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0145637936,"dev-research":0.2803139376,"prompt-eng":0.3909884483,"data-quality":0.1971758028,"ml-security":0.0940050558}}
{"text":"To the best of our knowledge this paper also provides the first explicit comparison between the original and modified HP definitions.","meta":{"url":"http://arxiv.org/abs/2307.10832v1"},"cats":{"new-dataset":0.0273506422,"dev-research":0.2811091132,"prompt-eng":0.3999155082,"data-quality":0.124832104,"ml-security":0.058419802}}
{"text":"This study examines the relationship between Yelp reviews and food types, investigating how ratings, sentiments, and topics vary across different types of food.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.0361432634,"dev-research":0.3205661688,"prompt-eng":0.4104455841,"data-quality":0.1553685633,"ml-security":0.0537094022}}
{"text":"Specifically, we analyze how ratings and sentiments of reviews vary across food types, cluster food types based on ratings and sentiments, infer review topics using machine learning models, and compare topic distributions among different food types.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.0410132335,"dev-research":0.2908584258,"prompt-eng":0.4250855835,"data-quality":0.2006419576,"ml-security":0.0955145073}}
{"text":"Our analyses reveal that some food types have similar ratings, sentiments, and topics distributions, while others have distinct patterns.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.0718941337,"dev-research":0.2854259925,"prompt-eng":0.4457296856,"data-quality":0.168841665,"ml-security":0.0668172563}}
{"text":"We identify four clusters of food types based on ratings and sentiments and find that reviewers tend to focus on different topics when reviewing certain food types.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.034985901,"dev-research":0.3310582907,"prompt-eng":0.455086403,"data-quality":0.1977678755,"ml-security":0.0496488989}}
{"text":"These findings have important implications for understanding user behavior and cultural influence on digital media platforms and promoting cross-cultural understanding and appreciation.","meta":{"url":"http://arxiv.org/abs/2307.10826v1"},"cats":{"new-dataset":0.0779906769,"dev-research":0.3934736732,"prompt-eng":0.4188346152,"data-quality":0.218475687,"ml-security":0.0858079598}}
{"text":"Incremental semantic segmentation aims to continually learn the segmentation of new coming classes without accessing the training data of previously learned classes.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.2565985411,"dev-research":0.2599258268,"prompt-eng":0.3895435177,"data-quality":0.2566248869,"ml-security":0.1623812702}}
{"text":"However, most current methods fail to address catastrophic forgetting and background shift since they 1) treat all previous classes equally without considering different forgetting paces caused by imbalanced gradient back-propagation; 2) lack strong semantic guidance between classes.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.0226533563,"dev-research":0.249463897,"prompt-eng":0.4034235733,"data-quality":0.2989671387,"ml-security":0.2429607558}}
{"text":"To tackle the above challenges, in this paper, we propose a Gradient-Semantic Compensation (GSC) model, which surmounts incremental semantic segmentation from both gradient and semantic perspectives.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.1663385784,"dev-research":0.2111140957,"prompt-eng":0.3879218263,"data-quality":0.3139568313,"ml-security":0.101884039}}
{"text":"Specifically, to address catastrophic forgetting from the gradient aspect, we develop a step-aware gradient compensation that can balance forgetting paces of previously seen classes via re-weighting gradient backpropagation.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.0452361031,"dev-research":0.2571769971,"prompt-eng":0.4300959309,"data-quality":0.2618085859,"ml-security":0.2973102156}}
{"text":"Meanwhile, we propose a soft-sharp semantic relation distillation to distill consistent inter-class semantic relations via soft labels for alleviating catastrophic forgetting from the semantic aspect.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.0753068998,"dev-research":0.3284720524,"prompt-eng":0.438362739,"data-quality":0.4827252783,"ml-security":0.1141845761}}
{"text":"In addition, we develop a prototypical pseudo re-labeling that provides strong semantic guidance to mitigate background shift.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.069059788,"dev-research":0.3660478106,"prompt-eng":0.4870079526,"data-quality":0.3926812086,"ml-security":0.1457167136}}
{"text":"It produces high-quality pseudo labels for old classes in the background by measuring distances between pixels and class-wise prototypes.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.2092226873,"dev-research":0.324957306,"prompt-eng":0.4590531106,"data-quality":0.3305934366,"ml-security":0.0822512856}}
{"text":"Extensive experiments on three public datasets, i.e., Pascal VOC 2012, ADE20K, and Cityscapes, demonstrate the effectiveness of our proposed GSC model.","meta":{"url":"http://arxiv.org/abs/2307.10822v1"},"cats":{"new-dataset":0.3484340228,"dev-research":0.2248067613,"prompt-eng":0.3734141302,"data-quality":0.169186172,"ml-security":0.2163741375}}
{"text":"A physical simulation engine (PSE) is a software system that simulates physical environments and objects.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0577219125,"dev-research":0.3021424376,"prompt-eng":0.485517791,"data-quality":0.0665119102,"ml-security":0.0867915064}}
{"text":"Modern PSEs feature both forward and backward simulations, where the forward phase predicts the behavior of a simulated system, and the backward phase provides gradients (guidance) for learning-based control tasks, such as a robot arm learning to fetch items.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0346654732,"dev-research":0.2136221859,"prompt-eng":0.5140597955,"data-quality":0.0711010563,"ml-security":0.2295145053}}
{"text":"This way, modern PSEs show promising support for learning-based control methods.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0348796653,"dev-research":0.2814076845,"prompt-eng":0.4992582217,"data-quality":0.0852567487,"ml-security":0.2271555263}}
{"text":"To date, PSEs have been largely used in various high-profitable, commercial applications, such as games, movies, virtual reality (VR), and robotics.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0292969564,"dev-research":0.3248474423,"prompt-eng":0.4371482671,"data-quality":0.0478590649,"ml-security":0.0942578079}}
{"text":"Despite the prosperous development and usage of PSEs by academia and industrial manufacturers such as Google and NVIDIA, PSEs may produce incorrect simulations, which may lead to negative results, from poor user experience in entertainment to accidents in robotics-involved manufacturing and surgical operations.   ","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0248905661,"dev-research":0.3550193806,"prompt-eng":0.4545305224,"data-quality":0.173911323,"ml-security":0.1546069262}}
{"text":"This paper introduces PHYFU, a fuzzing framework designed specifically for PSEs to uncover errors in both forward and backward simulation phases.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0817256626,"dev-research":0.3228710733,"prompt-eng":0.5082372374,"data-quality":0.2538437947,"ml-security":0.2179047341}}
{"text":"PHYFU mutates initial states and asserts if the PSE under test behaves consistently with respect to basic Physics Laws (PLs).","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0175097489,"dev-research":0.2155549577,"prompt-eng":0.4844782337,"data-quality":0.1446520104,"ml-security":0.1292951941}}
{"text":"We further use feedback-driven test input scheduling to guide and accelerate the search for errors.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.0623975626,"dev-research":0.3757211809,"prompt-eng":0.5235229258,"data-quality":0.313512042,"ml-security":0.1360120717}}
{"text":"Our study of four PSEs covers mainstream industrial vendors (Google and NVIDIA) as well as academic products.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.2388366062,"dev-research":0.2958469912,"prompt-eng":0.4196368899,"data-quality":0.0904309826,"ml-security":0.0966631841}}
{"text":"We successfully uncover over 5K error-triggering inputs that generate incorrect simulation results spanning across the whole software stack of PSEs.","meta":{"url":"http://arxiv.org/abs/2307.10818v1"},"cats":{"new-dataset":0.1546058471,"dev-research":0.3699855069,"prompt-eng":0.4720626475,"data-quality":0.3867768179,"ml-security":0.2732433826}}
{"text":"Recent text-to-image diffusion models have demonstrated an astonishing capacity to generate high-quality images.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.1029526349,"dev-research":0.2134079304,"prompt-eng":0.4474137843,"data-quality":0.1648312822,"ml-security":0.0838948661}}
{"text":"However, researchers mainly studied the way of synthesizing images with only text prompts.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.044775297,"dev-research":0.3141174866,"prompt-eng":0.5337931269,"data-quality":0.1593147034,"ml-security":0.1361114458}}
{"text":"While some works have explored using other modalities as conditions, considerable paired data, e.g., box/mask-image pairs, and fine-tuning time are required for nurturing models.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0408132683,"dev-research":0.192544351,"prompt-eng":0.412362216,"data-quality":0.0645478691,"ml-security":0.0944384316}}
{"text":"As such paired data is time-consuming and labor-intensive to acquire and restricted to a closed set, this potentially becomes the bottleneck for applications in an open world.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.3394897448,"dev-research":0.2957871729,"prompt-eng":0.2870665306,"data-quality":0.0767357315,"ml-security":0.1342255713}}
{"text":"This paper focuses on the simplest form of user-provided conditions, e.g., box or scribble.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0387073076,"dev-research":0.3421402776,"prompt-eng":0.4681702006,"data-quality":0.0849703832,"ml-security":0.0908562775}}
{"text":"To mitigate the aforementioned problem, we propose a training-free method to control objects and contexts in the synthesized images adhering to the given spatial conditions.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.1332991188,"dev-research":0.249270043,"prompt-eng":0.4414807703,"data-quality":0.1104213361,"ml-security":0.1625381079}}
{"text":"Specifically, three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints, are designed and seamlessly integrated into the denoising step of diffusion models, requiring no additional training and massive annotated layout data.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0601492726,"dev-research":0.2371239229,"prompt-eng":0.3741846389,"data-quality":0.0750153616,"ml-security":0.1161201976}}
{"text":"Extensive results show that the proposed constraints can control what and where to present in the images while retaining the ability of the Stable Diffusion model to synthesize with high fidelity and diverse concept coverage.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.0283852164,"dev-research":0.20595521,"prompt-eng":0.4382108484,"data-quality":0.1297751569,"ml-security":0.134190955}}
{"text":"The code is publicly available at https://github.com/Sierkinhane/BoxDiff.","meta":{"url":"http://arxiv.org/abs/2307.10816v1"},"cats":{"new-dataset":0.2857766551,"dev-research":0.2626043946,"prompt-eng":0.3865649557,"data-quality":0.1131769762,"ml-security":0.0927430145}}
{"text":"In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.2183594589,"dev-research":0.280401382,"prompt-eng":0.4115765434,"data-quality":0.2385719783,"ml-security":0.1734293943}}
{"text":"However, where training data for a language does not exist, data from other languages can be used instead.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.2528258481,"dev-research":0.2514604173,"prompt-eng":0.3697281094,"data-quality":0.3669777394,"ml-security":0.2033112708}}
{"text":"We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.1324106788,"dev-research":0.2402502891,"prompt-eng":0.4753995434,"data-quality":0.1449446736,"ml-security":0.0484122939}}
{"text":"For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED).","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.7246888062,"dev-research":0.2376081351,"prompt-eng":0.3551646611,"data-quality":0.1699117421,"ml-security":0.0780148183}}
{"text":"For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.8476992758,"dev-research":0.2649468844,"prompt-eng":0.3955518766,"data-quality":0.2094011411,"ml-security":0.0835745726}}
{"text":"We followed previous research in mapping labels for all datasets to just two classes, positive and negative.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.3958322884,"dev-research":0.2506909292,"prompt-eng":0.4017762475,"data-quality":0.4816949135,"ml-security":0.1169749188}}
{"text":"Thus we can compare performance on different languages directly, and combine languages for training and testing.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.034473632,"dev-research":0.3589679248,"prompt-eng":0.4403968799,"data-quality":0.1703962154,"ml-security":0.0753484689}}
{"text":"In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0579872658,"dev-research":0.2732932949,"prompt-eng":0.4576379304,"data-quality":0.2946042239,"ml-security":0.1326948026}}
{"text":"Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER are equally difficult.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0351229119,"dev-research":0.195403589,"prompt-eng":0.4381392061,"data-quality":0.102914918,"ml-security":0.0279134342}}
{"text":"Similarly, German SER is more difficult, and Urdu SER is easier.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.009064489,"dev-research":0.2709647284,"prompt-eng":0.4205168035,"data-quality":0.1042748415,"ml-security":0.0553060531}}
{"text":"In Experiment 2, we trained on one language and tested on another, in both directions for each pair:","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0407279643,"dev-research":0.2587937012,"prompt-eng":0.4694688063,"data-quality":0.2205591611,"ml-security":0.0943140457}}
{"text":"Amharic<->German, Amharic<->English, and Amharic<->Urdu.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.3115507593,"dev-research":0.2226568307,"prompt-eng":0.4076091267,"data-quality":0.2199492744,"ml-security":0.0430012101}}
{"text":"Results with Amharic as target suggested that using English or German as source will give the best result.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.5192933227,"dev-research":0.2000059661,"prompt-eng":0.4174960454,"data-quality":0.241279462,"ml-security":0.0292531654}}
{"text":"In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.071265407,"dev-research":0.2041451671,"prompt-eng":0.4285309489,"data-quality":0.2167416122,"ml-security":0.0650812352}}
{"text":"The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better result can be obtained when using two or three non-Amharic languages for training than when using just one non-Amharic language.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.0369619099,"dev-research":0.2340666359,"prompt-eng":0.3986646543,"data-quality":0.3246630566,"ml-security":0.0563160541}}
{"text":"Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SER classifier when resources for a language are scarce.","meta":{"url":"http://arxiv.org/abs/2307.10814v1"},"cats":{"new-dataset":0.066670571,"dev-research":0.2593390694,"prompt-eng":0.4326998363,"data-quality":0.2827154852,"ml-security":0.1574000351}}
{"text":"Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, advertising, tourism, etc.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.1222687001,"dev-research":0.2483572893,"prompt-eng":0.3794096758,"data-quality":0.1142718884,"ml-security":0.0646007172}}
{"text":"Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE).","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.0476309706,"dev-research":0.3865556324,"prompt-eng":0.3847366671,"data-quality":0.1446465823,"ml-security":0.0636881589}}
{"text":"However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignoring that the overall QoE also depends on the accompanying audio signals.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.0388880359,"dev-research":0.3088002868,"prompt-eng":0.3522274061,"data-quality":0.2326644131,"ml-security":0.0454303763}}
{"text":"In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, which includes 375 distorted omnidirectional audio-visual (A/V) sequences generated from 15 high-quality pristine omnidirectional A/V contents, and the corresponding perceptual audio-visual quality scores.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.275456861,"dev-research":0.303374234,"prompt-eng":0.3505675196,"data-quality":0.2891934373,"ml-security":0.0544236709}}
{"text":"Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which combine existing state-of-the-art single-mode audio and video QA models via multimodal fusion strategies.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.0890637821,"dev-research":0.2842652558,"prompt-eng":0.3588507014,"data-quality":0.2119308451,"ml-security":0.0301426375}}
{"text":"We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchmark for omnidirectional QoE evaluation.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.1585695594,"dev-research":0.2590731955,"prompt-eng":0.3782522582,"data-quality":0.1235884201,"ml-security":0.0475528149}}
{"text":"Our dataset is available at https://github.com/iamazxl/OAVQA.","meta":{"url":"http://arxiv.org/abs/2307.10813v1"},"cats":{"new-dataset":0.9471352669,"dev-research":0.2231779909,"prompt-eng":0.3281856638,"data-quality":0.0997181044,"ml-security":0.0886992729}}
{"text":"Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0407487756,"dev-research":0.2334965164,"prompt-eng":0.4706971522,"data-quality":0.0809774819,"ml-security":0.1275855545}}
{"text":"One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.019802434,"dev-research":0.2821841866,"prompt-eng":0.3773892738,"data-quality":0.0702273788,"ml-security":0.1304472406}}
{"text":"Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0269901759,"dev-research":0.2047974349,"prompt-eng":0.4248104025,"data-quality":0.0631716491,"ml-security":0.0854009068}}
{"text":"However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0187619098,"dev-research":0.2474989056,"prompt-eng":0.4437486773,"data-quality":0.0980478893,"ml-security":0.1210437231}}
{"text":"The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0227843772,"dev-research":0.176627741,"prompt-eng":0.4152990245,"data-quality":0.0947006788,"ml-security":0.0385820267}}
{"text":"We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0290572278,"dev-research":0.1589023183,"prompt-eng":0.4099837139,"data-quality":0.0364556779,"ml-security":0.0374774205}}
{"text":"Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.","meta":{"url":"http://arxiv.org/abs/2307.10810v1"},"cats":{"new-dataset":0.0708383993,"dev-research":0.2727907213,"prompt-eng":0.3885020492,"data-quality":0.0777262632,"ml-security":0.2336721423}}
{"text":"This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0709061435,"dev-research":0.2470784414,"prompt-eng":0.3688352325,"data-quality":0.1641390469,"ml-security":0.2182349099}}
{"text":"The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0327556528,"dev-research":0.2236899248,"prompt-eng":0.3461988316,"data-quality":0.066913874,"ml-security":0.0618123719}}
{"text":"SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0578987854,"dev-research":0.2668720993,"prompt-eng":0.3424235796,"data-quality":0.2305573219,"ml-security":0.085723671}}
{"text":"In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0179528584,"dev-research":0.2485487854,"prompt-eng":0.4130334437,"data-quality":0.1734361951,"ml-security":0.1311905547}}
{"text":"Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0109680999,"dev-research":0.2848795511,"prompt-eng":0.3881725743,"data-quality":0.2088506459,"ml-security":0.144677603}}
{"text":"In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0112038568,"dev-research":0.2102727676,"prompt-eng":0.3713686094,"data-quality":0.1099829595,"ml-security":0.1507195071}}
{"text":"To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.0069113861,"dev-research":0.2220850556,"prompt-eng":0.3730255695,"data-quality":0.1719636522,"ml-security":0.1599029947}}
{"text":"Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.","meta":{"url":"http://arxiv.org/abs/2307.10805v1"},"cats":{"new-dataset":0.1217635422,"dev-research":0.2678190404,"prompt-eng":0.3622425981,"data-quality":0.2203178908,"ml-security":0.150584195}}
{"text":"With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.3083791846,"dev-research":0.2996266122,"prompt-eng":0.3751688025,"data-quality":0.1320699518,"ml-security":0.1143241235}}
{"text":"Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.1315007578,"dev-research":0.3019542794,"prompt-eng":0.3074606458,"data-quality":0.0971333232,"ml-security":0.0866156327}}
{"text":"These characteristics make it difficult to design and train STDM models.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.0089289973,"dev-research":0.2835910607,"prompt-eng":0.4425063943,"data-quality":0.0835068985,"ml-security":0.1547710654}}
{"text":"Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.0825695521,"dev-research":0.3049177871,"prompt-eng":0.3603099577,"data-quality":0.0944926114,"ml-security":0.064039298}}
{"text":"To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.1293065035,"dev-research":0.2533783469,"prompt-eng":0.4004826787,"data-quality":0.1071339195,"ml-security":0.0708936643}}
{"text":"Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.7223448874,"dev-research":0.2818034449,"prompt-eng":0.3315941335,"data-quality":0.1325629358,"ml-security":0.0745272996}}
{"text":"Then, typical ST ocean data quality enhancement techniques are discussed.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.1350587021,"dev-research":0.3179048759,"prompt-eng":0.3459478584,"data-quality":0.2165627672,"ml-security":0.0847043384}}
{"text":"Next, we classify existing STDM studies for ocean into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate the techniques for these tasks.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.1457214464,"dev-research":0.27275407,"prompt-eng":0.4247171239,"data-quality":0.1276647269,"ml-security":0.1227789738}}
{"text":"Finally, promising research opportunities are highlighted.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.0529402304,"dev-research":0.3183698831,"prompt-eng":0.4001338382,"data-quality":0.094700184,"ml-security":0.0687797077}}
{"text":"This survey will help scientists from the fields of both computer science and ocean science have a better understanding of the fundamental concepts, key techniques, and open challenges of STDM in ocean.","meta":{"url":"http://arxiv.org/abs/2307.10803v1"},"cats":{"new-dataset":0.2002683811,"dev-research":0.2685973822,"prompt-eng":0.4093365889,"data-quality":0.0911397854,"ml-security":0.0689836403}}
{"text":"Multimodal learning aims to build models that can process and relate information from multiple modalities.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.0930413796,"dev-research":0.260762949,"prompt-eng":0.4439012819,"data-quality":0.1175765947,"ml-security":0.0881616494}}
{"text":"Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.1710441384,"dev-research":0.2616074674,"prompt-eng":0.3325889656,"data-quality":0.1261236207,"ml-security":0.0547953247}}
{"text":"In this work, we propose a framework, named Meta-Transformer, that leverages a $\\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.1334752118,"dev-research":0.1966767796,"prompt-eng":0.3848844078,"data-quality":0.1254899096,"ml-security":0.0891828575}}
{"text":"In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.0623525744,"dev-research":0.3023764626,"prompt-eng":0.4606468047,"data-quality":0.2017439709,"ml-security":0.10556128}}
{"text":"Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.0595664983,"dev-research":0.2527307619,"prompt-eng":0.421259977,"data-quality":0.1402138248,"ml-security":0.0943519687}}
{"text":"Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series).","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.0629442926,"dev-research":0.2298946733,"prompt-eng":0.4000352488,"data-quality":0.0938331002,"ml-security":0.0412163122}}
{"text":"Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers.","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.117341758,"dev-research":0.2884405254,"prompt-eng":0.4456676263,"data-quality":0.0859594442,"ml-security":0.0619036468}}
{"text":"Code will be available at https://github.com/invictus717/MetaTransformer","meta":{"url":"http://arxiv.org/abs/2307.10802v1"},"cats":{"new-dataset":0.1654473635,"dev-research":0.2478713137,"prompt-eng":0.418914521,"data-quality":0.1692648611,"ml-security":0.038331065}}
{"text":"Despite successes across a broad range of applications, sequence-to-sequence models' construct of solutions are argued to be less compositional than human-like generalization.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.032038829,"dev-research":0.2563796894,"prompt-eng":0.4039211444,"data-quality":0.0766254855,"ml-security":0.1128181578}}
{"text":"There is mounting evidence that one of the reasons hindering compositional generalization is representations of the encoder and decoder uppermost layer are entangled.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0038867741,"dev-research":0.2664101769,"prompt-eng":0.3865591395,"data-quality":0.0903971898,"ml-security":0.142453612}}
{"text":"In other words, the syntactic and semantic representations of sequences are twisted inappropriately.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0321670219,"dev-research":0.3060523545,"prompt-eng":0.3753805217,"data-quality":0.2471226533,"ml-security":0.1302131079}}
{"text":"However, most previous studies mainly concentrate on enhancing token-level semantic information to alleviate the representations entanglement problem, rather than composing and using the syntactic and semantic representations of sequences appropriately as humans do.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0120770531,"dev-research":0.3212019501,"prompt-eng":0.4322846575,"data-quality":0.18118283,"ml-security":0.09986871}}
{"text":"In addition, we explain why the entanglement problem exists from the perspective of recent studies about training deeper Transformer, mainly owing to the ``shallow'' residual connections and its simple, one-step operations, which fails to fuse previous layers' information effectively.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0148752575,"dev-research":0.2190715397,"prompt-eng":0.3670082187,"data-quality":0.1343252973,"ml-security":0.2368219746}}
{"text":"Starting from this finding and inspired by humans' strategies, we propose \\textsc{FuSion} (\\textbf{Fu}sing \\textbf{S}yntactic and Semant\\textbf{i}c Representati\\textbf{on}s), an extension to sequence-to-sequence models to learn to fuse previous layers' information back into the encoding and decoding process appropriately through introducing a \\emph{fuse-attention module} at each encoder and decoder layer.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.2186643384,"dev-research":0.2663861004,"prompt-eng":0.464712585,"data-quality":0.1479962547,"ml-security":0.1548089642}}
{"text":"\\textsc{FuSion} achieves competitive and even \\textbf{state-of-the-art} results on two realistic benchmarks, which empirically demonstrates the effectiveness of our proposal.","meta":{"url":"http://arxiv.org/abs/2307.10799v1"},"cats":{"new-dataset":0.0230884208,"dev-research":0.3067401291,"prompt-eng":0.4061869446,"data-quality":0.1515775699,"ml-security":0.0874201538}}
{"text":"In this paper, we present our method for neural face reenactment, called HyperReenact, that aims to generate realistic talking head images of a source identity, driven by a target facial pose.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.0774782638,"dev-research":0.2323575108,"prompt-eng":0.3754638146,"data-quality":0.1174179249,"ml-security":0.1455355299}}
{"text":"Existing state-of-the-art face reenactment methods train controllable generative models that learn to synthesize realistic facial images, yet producing reenacted faces that are prone to significant visual artifacts, especially under the challenging condition of extreme head pose changes, or requiring expensive few-shot fine-tuning to better preserve the source identity characteristics.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.0382799811,"dev-research":0.2330420842,"prompt-eng":0.4462635312,"data-quality":0.122952233,"ml-security":0.1519185644}}
{"text":"We propose to address these limitations by leveraging the photorealistic generation ability and the disentangled properties of a pretrained StyleGAN2 generator, by first inverting the real images into its latent space and then using a hypernetwork to perform: (i) refinement of the source identity characteristics and (ii) facial pose re-targeting, eliminating this way the dependence on external editing methods that typically produce artifacts.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.0518427776,"dev-research":0.3042552088,"prompt-eng":0.38828452,"data-quality":0.190337425,"ml-security":0.1640699228}}
{"text":"Our method operates under the one-shot setting (i.e., using a single source frame) and allows for cross-subject reenactment, without requiring any subject-specific fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.0245526943,"dev-research":0.1920187611,"prompt-eng":0.4174978953,"data-quality":0.0861645832,"ml-security":0.050478784}}
{"text":"We compare our method both quantitatively and qualitatively against several state-of-the-art techniques on the standard benchmarks of VoxCeleb1 and VoxCeleb2, demonstrating the superiority of our approach in producing artifact-free images, exhibiting remarkable robustness even under extreme head pose changes.","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.0770273216,"dev-research":0.2702164503,"prompt-eng":0.3590231459,"data-quality":0.1497699717,"ml-security":0.0873606636}}
{"text":"We make the code and the pretrained models publicly available at: https://github.com/StelaBou/HyperReenact .","meta":{"url":"http://arxiv.org/abs/2307.10797v1"},"cats":{"new-dataset":0.2034201881,"dev-research":0.2405693207,"prompt-eng":0.4396651479,"data-quality":0.0890655415,"ml-security":0.1002359319}}
{"text":"Compiler error messages serve as an initial resource for programmers dealing with compilation errors.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.1402166516,"dev-research":0.5229782107,"prompt-eng":0.452462723,"data-quality":0.4839153403,"ml-security":0.1708244045}}
{"text":"However, previous studies indicate that they often lack sufficient targeted information to resolve code issues.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0254484986,"dev-research":0.4863172592,"prompt-eng":0.3895021221,"data-quality":0.3044324676,"ml-security":0.2492709389}}
{"text":"Consequently, programmers typically rely on their own research to fix errors.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0302286484,"dev-research":0.6025402473,"prompt-eng":0.4131009641,"data-quality":0.4458857036,"ml-security":0.1376677149}}
{"text":"Historically, Stack Overflow has been the primary resource for such information, but recent advances in large language models offer alternatives.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.2826658029,"dev-research":0.2999254997,"prompt-eng":0.4482727123,"data-quality":0.1453879127,"ml-security":0.0880638449}}
{"text":"This study systematically examines 100 compiler error messages from three sources to determine the most effective approach for programmers encountering compiler errors.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.1538122997,"dev-research":0.5893824975,"prompt-eng":0.4490387839,"data-quality":0.5579991459,"ml-security":0.1815488164}}
{"text":"Factors considered include Stack Overflow search methods and the impact of model version and prompt phrasing when using large language models.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0177573342,"dev-research":0.3163550218,"prompt-eng":0.4803182678,"data-quality":0.1414419954,"ml-security":0.147899771}}
{"text":"The results reveal that GPT-4 outperforms Stack Overflow in explaining compiler error messages, the effectiveness of adding code snippets to Stack Overflow searches depends on the search method, and results for Stack Overflow differ significantly between Google and StackExchange API searches.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0335023643,"dev-research":0.3923939774,"prompt-eng":0.3871389856,"data-quality":0.1731144827,"ml-security":0.1157776179}}
{"text":"Furthermore, GPT-4 surpasses GPT-3.5, with \"How to fix\" prompts yielding superior outcomes to \"What does this error mean\" prompts.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.0617737996,"dev-research":0.319492176,"prompt-eng":0.4851544563,"data-quality":0.3225511084,"ml-security":0.108016015}}
{"text":"These results offer valuable guidance for programmers seeking assistance with compiler error messages, underscoring the transformative potential of advanced large language models like GPT-4 in debugging and opening new avenues of exploration for researchers in AI-assisted programming.","meta":{"url":"http://arxiv.org/abs/2307.10793v1"},"cats":{"new-dataset":0.1327442502,"dev-research":0.4680456144,"prompt-eng":0.4598811791,"data-quality":0.2530276043,"ml-security":0.1649951916}}
{"text":"Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.1601333916,"dev-research":0.2834363615,"prompt-eng":0.3783778789,"data-quality":0.257093919,"ml-security":0.3239296843}}
{"text":"While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0533877513,"dev-research":0.2726270274,"prompt-eng":0.3458348141,"data-quality":0.146478191,"ml-security":0.0923537158}}
{"text":"It thus remains unclear if the performance of such pre-existing algorithms can be further improved.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0312385787,"dev-research":0.2280831043,"prompt-eng":0.3528779605,"data-quality":0.1215072685,"ml-security":0.0624019603}}
{"text":"We address said question in this work.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.186857342,"dev-research":0.3395612378,"prompt-eng":0.3968425191,"data-quality":0.1341763931,"ml-security":0.0958672503}}
{"text":"Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.1951801158,"dev-research":0.3638076678,"prompt-eng":0.3621192531,"data-quality":0.2533312105,"ml-security":0.2443276917}}
{"text":"We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0506045704,"dev-research":0.2538491243,"prompt-eng":0.4258981486,"data-quality":0.2436301275,"ml-security":0.2010030743}}
{"text":"Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) significant performance improvements can be realized by optimizing hyperparameters such as the underlying feature extractor, and that (II) image-level augmentations can, but are not guaranteed, to improve performance.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0587692772,"dev-research":0.3167515861,"prompt-eng":0.4195790094,"data-quality":0.2156124893,"ml-security":0.1593950279}}
{"text":"Based on these findings, we achieve a new state of the art in few-shot AD on VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to the few-shot setting.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.1406423356,"dev-research":0.3121652509,"prompt-eng":0.3929403285,"data-quality":0.1715509042,"ml-security":0.0812149494}}
{"text":"Last, we identify the investigation of feature extractors with a strong inductive bias as a potential future research direction for (few-shot) AD/AS.","meta":{"url":"http://arxiv.org/abs/2307.10792v1"},"cats":{"new-dataset":0.0331819285,"dev-research":0.3300000601,"prompt-eng":0.416930417,"data-quality":0.2792231064,"ml-security":0.1795282145}}
{"text":"To be successful, Vision-and-Language Navigation (VLN) agents must be able to ground instructions to actions based on their surroundings.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.0993685157,"dev-research":0.3343739628,"prompt-eng":0.4517750874,"data-quality":0.1028371336,"ml-security":0.1015166349}}
{"text":"In this work, we develop a methodology to study agent behavior on a skill-specific basis -- examining how well existing agents ground instructions about stopping, turning, and moving towards specified objects or rooms.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.1477894693,"dev-research":0.3249345591,"prompt-eng":0.4645231898,"data-quality":0.056351311,"ml-security":0.1650595409}}
{"text":"Our approach is based on generating skill-specific interventions and measuring changes in agent predictions.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.0270119617,"dev-research":0.3391138752,"prompt-eng":0.4788695062,"data-quality":0.087684641,"ml-security":0.1947543105}}
{"text":"We present a detailed case study analyzing the behavior of a recent agent and then compare multiple agents in terms of skill-specific competency scores.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.047076483,"dev-research":0.2840771562,"prompt-eng":0.4255994145,"data-quality":0.0737612178,"ml-security":0.100038401}}
{"text":"This analysis suggests that biases from training have lasting effects on agent behavior and that existing models are able to ground simple referring expressions.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.0308804645,"dev-research":0.3084388884,"prompt-eng":0.4454382837,"data-quality":0.2567721132,"ml-security":0.2401404448}}
{"text":"Our comparisons between models show that skill-specific scores correlate with improvements in overall VLN task performance.","meta":{"url":"http://arxiv.org/abs/2307.10790v1"},"cats":{"new-dataset":0.021104084,"dev-research":0.4079496791,"prompt-eng":0.450424613,"data-quality":0.1143862224,"ml-security":0.1667126839}}
{"text":"Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.0319985493,"dev-research":0.2700358708,"prompt-eng":0.397013943,"data-quality":0.3639089402,"ml-security":0.8280318211}}
{"text":"However, it has been shown that existing attacks are not well suited for this kind of classifiers.","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.0208063689,"dev-research":0.2877652733,"prompt-eng":0.4030763754,"data-quality":0.3153418467,"ml-security":0.7958683094}}
{"text":"In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality).","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.0234108126,"dev-research":0.2646839121,"prompt-eng":0.3720910995,"data-quality":0.1369636057,"ml-security":0.6163191005}}
{"text":"We then show that existing attacks do not meet both of these properties.","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.0251328743,"dev-research":0.2805040678,"prompt-eng":0.3795890019,"data-quality":0.142883249,"ml-security":0.5456987494}}
{"text":"Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets.","meta":{"url":"http://arxiv.org/abs/2307.10788v1"},"cats":{"new-dataset":0.1578490939,"dev-research":0.2371349034,"prompt-eng":0.3818042013,"data-quality":0.1223074614,"ml-security":0.6366181358}}
{"text":"Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.1792441134,"dev-research":0.3397902692,"prompt-eng":0.3656291828,"data-quality":0.1905730164,"ml-security":0.1864933852}}
{"text":"However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.0054595296,"dev-research":0.2261511113,"prompt-eng":0.4001282964,"data-quality":0.0577137943,"ml-security":0.0835938768}}
{"text":"In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.058360136,"dev-research":0.2553548373,"prompt-eng":0.4503537393,"data-quality":0.1845569395,"ml-security":0.1168878971}}
{"text":"Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.1305752376,"dev-research":0.2791368735,"prompt-eng":0.4972296233,"data-quality":0.1688783504,"ml-security":0.1312255831}}
{"text":"It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.","meta":{"url":"http://arxiv.org/abs/2307.10787v1"},"cats":{"new-dataset":0.0323267989,"dev-research":0.2780316776,"prompt-eng":0.4276211759,"data-quality":0.165500561,"ml-security":0.1010373142}}
{"text":"The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle sensing due to its cost-effectiveness and operability in adverse weather conditions.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0765455173,"dev-research":0.2560581225,"prompt-eng":0.385636023,"data-quality":0.0719420081,"ml-security":0.0892475978}}
{"text":"However, the adoption of this technology has been hindered by sparsity and noise issues in radar point cloud data.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0583028573,"dev-research":0.2789648508,"prompt-eng":0.3430953365,"data-quality":0.2008015445,"ml-security":0.1207063239}}
{"text":"This paper introduces spatial multi-representation fusion (SMURF), a novel approach to 3D object detection using a single 4D imaging radar.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0738496198,"dev-research":0.2338402466,"prompt-eng":0.3673794973,"data-quality":0.09707811,"ml-security":0.0672767885}}
{"text":"SMURF leverages multiple representations of radar detection points, including pillarization and density features of a multi-dimensional Gaussian mixture distribution through kernel density estimation (KDE).","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0582512105,"dev-research":0.235542023,"prompt-eng":0.3976931048,"data-quality":0.1471620605,"ml-security":0.0992071333}}
{"text":"KDE effectively mitigates measurement inaccuracy caused by limited angular resolution and multi-path propagation of radar signals.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0250972264,"dev-research":0.2966172935,"prompt-eng":0.3627648337,"data-quality":0.1433840302,"ml-security":0.1077197294}}
{"text":"Additionally, KDE helps alleviate point cloud sparsity by capturing density features.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.0463385172,"dev-research":0.3166126564,"prompt-eng":0.3898729353,"data-quality":0.1093322277,"ml-security":0.0747584894}}
{"text":"Experimental evaluations on View-of-Delft (VoD) and TJ4DRadSet datasets demonstrate the effectiveness and generalization ability of SMURF, outperforming recently proposed 4D imaging radar-based single-representation models.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.1392321728,"dev-research":0.2160031459,"prompt-eng":0.3620175481,"data-quality":0.0949613367,"ml-security":0.0737535184}}
{"text":"Moreover, while using 4D imaging radar only, SMURF still achieves comparable performance to the state-of-the-art 4D imaging radar and camera fusion-based method, with an increase of 1.22% in the mean average precision on bird's-eye view of TJ4DRadSet dataset and 1.32% in the 3D mean average precision on the entire annotated area of VoD dataset.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.1243545915,"dev-research":0.2429957112,"prompt-eng":0.3515415697,"data-quality":0.1121291661,"ml-security":0.0395836208}}
{"text":"Our proposed method demonstrates impressive inference time and addresses the challenges of real-time detection, with the inference time no more than 0.05 seconds for most scans on both datasets.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.3333902457,"dev-research":0.2647265082,"prompt-eng":0.3804200365,"data-quality":0.1861912496,"ml-security":0.1860922258}}
{"text":"This research highlights the benefits of 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D object detection with 4D imaging radar.","meta":{"url":"http://arxiv.org/abs/2307.10784v1"},"cats":{"new-dataset":0.067309878,"dev-research":0.2436085647,"prompt-eng":0.3701223523,"data-quality":0.0733542227,"ml-security":0.0689815997}}
{"text":"Zero-shot point cloud segmentation aims to make deep models capable of recognizing novel objects in point cloud that are unseen in the training phase.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.1725526541,"dev-research":0.1895270494,"prompt-eng":0.3469324056,"data-quality":0.2146979775,"ml-security":0.2026061263}}
{"text":"Recent trends favor the pipeline which transfers knowledge from seen classes with labels to unseen classes without labels.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.1527735247,"dev-research":0.2623358628,"prompt-eng":0.4142380118,"data-quality":0.3305376214,"ml-security":0.1905077481}}
{"text":"They typically align visual features with semantic features obtained from word embedding by the supervision of seen classes' annotations.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.1028231115,"dev-research":0.3736187453,"prompt-eng":0.4395347988,"data-quality":0.3473998896,"ml-security":0.0564510351}}
{"text":"However, point cloud contains limited information to fully match with semantic features.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.0586723711,"dev-research":0.2765014738,"prompt-eng":0.3392875509,"data-quality":0.1561463191,"ml-security":0.0952419586}}
{"text":"In fact, the rich appearance information of images is a natural complement to the textureless point cloud, which is not well explored in previous literature.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.0737017414,"dev-research":0.2060249434,"prompt-eng":0.3572207855,"data-quality":0.1538878593,"ml-security":0.1065832625}}
{"text":"Motivated by this, we propose a novel multi-modal zero-shot learning method to better utilize the complementary information of point clouds and images for more accurate visual-semantic alignment.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.1994686808,"dev-research":0.2159028635,"prompt-eng":0.3466480587,"data-quality":0.23434527,"ml-security":0.071336776}}
{"text":"Extensive experiments are performed in two popular benchmarks, i.e., SemanticKITTI and nuScenes, and our method outperforms current SOTA methods with 52% and 49% improvement on average for unseen class mIoU, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10782v1"},"cats":{"new-dataset":0.0956667483,"dev-research":0.2314080477,"prompt-eng":0.3857322614,"data-quality":0.2489377367,"ml-security":0.0665291431}}
{"text":"5G non-public networks (NPNs) play a key role in enabling critical Industrial Internet of Things (IoT) applications in various vertical industries.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.1122273516,"dev-research":0.2321446671,"prompt-eng":0.3003231155,"data-quality":0.048837539,"ml-security":0.117681793}}
{"text":"Among other features, 5G NPNs enable novel operation models, where the roles and responsibilities for setting up and operating the network can be distributed among several stakeholders, i.e., among the public mobile network operators (MNOs), the industrial party who uses the 5G NPN services and 3rd parties.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.0344344006,"dev-research":0.2854678413,"prompt-eng":0.3349408857,"data-quality":0.0555372145,"ml-security":0.1075778169}}
{"text":"This results in many theoretically feasible operation models for 5G NPN, each with its own advantages and disadvantages.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.0110419473,"dev-research":0.189326121,"prompt-eng":0.3523375093,"data-quality":0.0419422078,"ml-security":0.1110734759}}
{"text":"We investigate the resulting operation models and identify a set of nine prime models taking into account today's practical considerations.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.0276055644,"dev-research":0.19636013,"prompt-eng":0.4340081964,"data-quality":0.0532926987,"ml-security":0.1236653766}}
{"text":"Additionally, we define a framework to qualitatively analyze the operation models and use it to evaluate and compare the identified operation models.","meta":{"url":"http://arxiv.org/abs/2307.10781v1"},"cats":{"new-dataset":0.0243635107,"dev-research":0.3618271117,"prompt-eng":0.4246770981,"data-quality":0.089997417,"ml-security":0.1134750736}}
{"text":"Vision transformers have demonstrated remarkable success in a wide range of computer vision tasks over the last years.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0470551577,"dev-research":0.2730433932,"prompt-eng":0.4155142429,"data-quality":0.0757057247,"ml-security":0.063674917}}
{"text":"However, their high computational costs remain a significant barrier to their practical deployment.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0657530888,"dev-research":0.3499529836,"prompt-eng":0.3707489997,"data-quality":0.0443926986,"ml-security":0.1352204034}}
{"text":"In particular, the complexity of transformer models is quadratic with respect to the number of input tokens.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0081667904,"dev-research":0.2210268238,"prompt-eng":0.3818609847,"data-quality":0.0843316709,"ml-security":0.1143057284}}
{"text":"Therefore techniques that reduce the number of input tokens that need to be processed have been proposed.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.017943019,"dev-research":0.3051205778,"prompt-eng":0.4610028884,"data-quality":0.1597346274,"ml-security":0.1456335652}}
{"text":"This paper introduces Learned Thresholds token Merging and Pruning (LTMP), a novel approach that leverages the strengths of both token merging and token pruning.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0802810777,"dev-research":0.282403305,"prompt-eng":0.4501292575,"data-quality":0.2909722092,"ml-security":0.1901199961}}
{"text":"LTMP uses learned threshold masking modules that dynamically determine which tokens to merge and which to prune.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0376528071,"dev-research":0.2477873909,"prompt-eng":0.4323759401,"data-quality":0.2239484291,"ml-security":0.1299193088}}
{"text":"We demonstrate our approach with extensive experiments on vision transformers on the ImageNet classification task.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0495817257,"dev-research":0.266059665,"prompt-eng":0.4150134929,"data-quality":0.2421659998,"ml-security":0.1914668513}}
{"text":"Our results demonstrate that LTMP achieves state-of-the-art accuracy across reduction rates while requiring only a single fine-tuning epoch, which is an order of magnitude faster than previous methods.","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.0598147206,"dev-research":0.2610190483,"prompt-eng":0.3830203173,"data-quality":0.1849908054,"ml-security":0.0761532228}}
{"text":"Code is available at https://github.com/Mxbonn/ltmp .","meta":{"url":"http://arxiv.org/abs/2307.10780v1"},"cats":{"new-dataset":0.2563653081,"dev-research":0.2411691303,"prompt-eng":0.4316125288,"data-quality":0.1136796384,"ml-security":0.070128783}}
{"text":"Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.0871995662,"dev-research":0.2343497978,"prompt-eng":0.3569442607,"data-quality":0.0929329335,"ml-security":0.0765987535}}
{"text":"However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.0591639591,"dev-research":0.3082156706,"prompt-eng":0.3765960136,"data-quality":0.1299896678,"ml-security":0.1188334027}}
{"text":"In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.0605766418,"dev-research":0.2241660446,"prompt-eng":0.3938553178,"data-quality":0.0729251355,"ml-security":0.1093320185}}
{"text":"We propose strategies to remove this bottleneck and further simplify its memory usage.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.0795090619,"dev-research":0.362595433,"prompt-eng":0.3767958816,"data-quality":0.1157070625,"ml-security":0.133647098}}
{"text":"Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.1125377715,"dev-research":0.3142523753,"prompt-eng":0.377770558,"data-quality":0.0758187943,"ml-security":0.0836815776}}
{"text":"In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.090718913,"dev-research":0.2329538869,"prompt-eng":0.3664174152,"data-quality":0.1713430006,"ml-security":0.1013077441}}
{"text":"but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.","meta":{"url":"http://arxiv.org/abs/2307.10779v1"},"cats":{"new-dataset":0.0723682675,"dev-research":0.237723089,"prompt-eng":0.3882437865,"data-quality":0.0798933605,"ml-security":0.1584358514}}
{"text":"Online job ads serve as a valuable source of information for skill requirements, playing a crucial role in labor market analysis and e-recruitment processes.","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.0200879133,"dev-research":0.2919095254,"prompt-eng":0.3810533219,"data-quality":0.1255295835,"ml-security":0.1623627775}}
{"text":"Since such ads are typically formatted in free text, natural language processing (NLP) technologies are required to automatically process them.","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.0620469563,"dev-research":0.3689126678,"prompt-eng":0.4460395345,"data-quality":0.3659724356,"ml-security":0.1226527791}}
{"text":"We specifically focus on the task of detecting skills (mentioned literally, or implicitly described) and linking them to a large skill ontology, making it a challenging case of extreme multi-label classification (XMLC).","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.099608804,"dev-research":0.3129698565,"prompt-eng":0.4475006251,"data-quality":0.3176920992,"ml-security":0.1388329246}}
{"text":"Given that there is no sizable labeled (training) dataset are available for this specific XMLC task, we propose techniques to leverage general Large Language Models (LLMs).","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.4394513286,"dev-research":0.2320871977,"prompt-eng":0.454406189,"data-quality":0.2773124113,"ml-security":0.0990524454}}
{"text":"We describe a cost-effective approach to generate an accurate, fully synthetic labeled dataset for skill extraction, and present a contrastive learning strategy that proves effective in the task.","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.4202871918,"dev-research":0.3014998641,"prompt-eng":0.4025905776,"data-quality":0.3264701481,"ml-security":0.1453965839}}
{"text":"Our results across three skill extraction benchmarks show a consistent increase of between 15 to 25 percentage points in \\textit{R-Precision@5} compared to previously published results that relied solely on distant supervision through literal matches.","meta":{"url":"http://arxiv.org/abs/2307.10778v1"},"cats":{"new-dataset":0.094654731,"dev-research":0.3435142496,"prompt-eng":0.4289126717,"data-quality":0.3177673466,"ml-security":0.075520786}}
{"text":"Neural Radiance Fields (NeRFs) have achieved great success in the past few years.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0687230703,"dev-research":0.2694713082,"prompt-eng":0.381182113,"data-quality":0.1314232595,"ml-security":0.1350461943}}
{"text":"However, most current methods still require intensive resources due to ray marching-based rendering.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.03984379,"dev-research":0.2567058344,"prompt-eng":0.3600129119,"data-quality":0.0475852082,"ml-security":0.0429686036}}
{"text":"To construct urban-level radiance fields efficiently, we design Deformable Neural Mesh Primitive~(DNMP), and propose to parameterize the entire scene with such primitives.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.2735027884,"dev-research":0.2221442877,"prompt-eng":0.4029060134,"data-quality":0.064413767,"ml-security":0.1117463785}}
{"text":"The DNMP is a flexible and compact neural variant of classic mesh representation, which enjoys both the efficiency of rasterization-based rendering and the powerful neural representation capability for photo-realistic image synthesis.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0718003816,"dev-research":0.2518631519,"prompt-eng":0.3884128099,"data-quality":0.076989553,"ml-security":0.0971055037}}
{"text":"Specifically, a DNMP consists of a set of connected deformable mesh vertices with paired vertex features to parameterize the geometry and radiance information of a local area.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0737583553,"dev-research":0.28845218,"prompt-eng":0.4071822449,"data-quality":0.0748500331,"ml-security":0.0933339167}}
{"text":"To constrain the degree of freedom for optimization and lower the storage budgets, we enforce the shape of each primitive to be decoded from a relatively low-dimensional latent space.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0281129044,"dev-research":0.2071470601,"prompt-eng":0.3946657994,"data-quality":0.104440214,"ml-security":0.185784674}}
{"text":"The rendering colors are decoded from the vertex features (interpolated with rasterization) by a view-dependent MLP.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.078367818,"dev-research":0.267034407,"prompt-eng":0.3748068742,"data-quality":0.1147676621,"ml-security":0.1091240589}}
{"text":"The DNMP provides a new paradigm for urban-level scene representation with appealing properties: $(1)$ High-quality rendering.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.2228006482,"dev-research":0.2913210087,"prompt-eng":0.4149848558,"data-quality":0.1057049097,"ml-security":0.0738031324}}
{"text":"Our method achieves leading performance for novel view synthesis in urban scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.2364591629,"dev-research":0.3409065707,"prompt-eng":0.3889456343,"data-quality":0.0984375775,"ml-security":0.0640206238}}
{"text":"$(2)$ Low computational costs.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.0211600275,"dev-research":0.2887334202,"prompt-eng":0.3537551389,"data-quality":0.0681193131,"ml-security":0.0880685791}}
{"text":"Our representation enables fast rendering (2.07ms/1k pixels) and low peak memory usage (110MB/1k pixels).","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.1822593223,"dev-research":0.3231603721,"prompt-eng":0.4017691878,"data-quality":0.0666847017,"ml-security":0.0482101568}}
{"text":"We also present a lightweight version that can run 33$\\times$ faster than vanilla NeRFs, and comparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.1055862593,"dev-research":0.2719906588,"prompt-eng":0.3546031055,"data-quality":0.1089846667,"ml-security":0.0970670234}}
{"text":"Project page: \\href{https://dnmp.github.io/}{https://dnmp.github.io/}.","meta":{"url":"http://arxiv.org/abs/2307.10776v1"},"cats":{"new-dataset":0.155160599,"dev-research":0.3390293024,"prompt-eng":0.3798340138,"data-quality":0.1121097748,"ml-security":0.0794656808}}
{"text":"Background.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.2201146089,"dev-research":0.2701232712,"prompt-eng":0.4087147309,"data-quality":0.1221936601,"ml-security":0.1240009317}}
{"text":"Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0655304583,"dev-research":0.4855942277,"prompt-eng":0.4194827386,"data-quality":0.1252131601,"ml-security":0.3054280045}}
{"text":"In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0383087308,"dev-research":0.3581408258,"prompt-eng":0.4672197685,"data-quality":0.1424631472,"ml-security":0.136851248}}
{"text":"Aims.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.1559384874,"dev-research":0.2789113201,"prompt-eng":0.4689634103,"data-quality":0.082924854,"ml-security":0.0629672636}}
{"text":"Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0709703811,"dev-research":0.4287961661,"prompt-eng":0.4734209037,"data-quality":0.139799772,"ml-security":0.1494932459}}
{"text":"Method.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0357467563,"dev-research":0.3399177064,"prompt-eng":0.3975480928,"data-quality":0.1458015442,"ml-security":0.1120188571}}
{"text":"To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.1975153057,"dev-research":0.4358985808,"prompt-eng":0.458375956,"data-quality":0.2324060262,"ml-security":0.0566298366}}
{"text":"Results.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.1028885059,"dev-research":0.3083195407,"prompt-eng":0.4038500352,"data-quality":0.1938673736,"ml-security":0.0745611074}}
{"text":"We found that AutoML solutions can generate models that outperform those trained and optimized by researchers to perform classification tasks in the SE domain.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0616386978,"dev-research":0.3639353116,"prompt-eng":0.5058676307,"data-quality":0.2911055313,"ml-security":0.1615366251}}
{"text":"Also, our findings show that the currently available AutoML solutions do not live up to their names as they do not equally support automation across the stages of the ML development workflow and for all the team members.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.1125017287,"dev-research":0.4017576708,"prompt-eng":0.4827815203,"data-quality":0.1638093458,"ml-security":0.094615278}}
{"text":"Conclusions.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0491206439,"dev-research":0.3936333751,"prompt-eng":0.4081125936,"data-quality":0.1562034933,"ml-security":0.0872252915}}
{"text":"We derive insights to inform the SE research community on how AutoML can facilitate their activities and tool builders on how to design the next generation of AutoML technologies.","meta":{"url":"http://arxiv.org/abs/2307.10774v1"},"cats":{"new-dataset":0.0662310527,"dev-research":0.4669684832,"prompt-eng":0.4853014784,"data-quality":0.1292282016,"ml-security":0.0612492351}}
{"text":"Music recommendation systems have emerged as a vital component to enhance user experience and satisfaction for the music streaming services, which dominates music consumption.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.0305046636,"dev-research":0.3510484361,"prompt-eng":0.3870488545,"data-quality":0.1389032723,"ml-security":0.087315739}}
{"text":"The key challenge in improving these recommender systems lies in comprehending the complexity of music data, specifically for the underpinning music genre classification.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.0526862279,"dev-research":0.3413290755,"prompt-eng":0.3526693251,"data-quality":0.2607673663,"ml-security":0.1454782278}}
{"text":"The limitations of manual genre classification have highlighted the need for a more advanced system, namely the Automatic Music Genre Classification (AMGC) system.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.0896149868,"dev-research":0.294440325,"prompt-eng":0.3953534625,"data-quality":0.2617581399,"ml-security":0.0672176227}}
{"text":"While traditional machine learning techniques have shown potential in genre classification, they heavily rely on manually engineered features and feature selection, failing to capture the full complexity of music data.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.0381474122,"dev-research":0.3661616581,"prompt-eng":0.3816041409,"data-quality":0.2945926413,"ml-security":0.1529298899}}
{"text":"On the other hand, deep learning classification architectures like the traditional Convolutional Neural Networks (CNN) are effective in capturing the spatial hierarchies but struggle to capture the temporal dynamics inherent in music data.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.1495934426,"dev-research":0.2324399723,"prompt-eng":0.3177167547,"data-quality":0.1787694754,"ml-security":0.1521394865}}
{"text":"To address these challenges, this study proposes a novel approach using visual spectrograms as input, and propose a hybrid model that combines the strength of the Residual neural Network (ResNet) and the Gated Recurrent Unit (GRU).","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.2186419847,"dev-research":0.2955532194,"prompt-eng":0.4125985561,"data-quality":0.1683696836,"ml-security":0.1198253214}}
{"text":"This model is designed to provide a more comprehensive analysis of music data, offering the potential to improve the music recommender systems through achieving a more comprehensive analysis of music data and hence potentially more accurate genre classification.","meta":{"url":"http://arxiv.org/abs/2307.10773v1"},"cats":{"new-dataset":0.1484419224,"dev-research":0.3033253395,"prompt-eng":0.3655016961,"data-quality":0.2502793435,"ml-security":0.1067671937}}
{"text":"Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0260058845,"dev-research":0.2465924089,"prompt-eng":0.3404160752,"data-quality":0.1159607867,"ml-security":0.0750916414}}
{"text":"This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0458537781,"dev-research":0.2252222539,"prompt-eng":0.3753939117,"data-quality":0.0445950815,"ml-security":0.0699357646}}
{"text":"Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.038816001,"dev-research":0.3091654487,"prompt-eng":0.4033238109,"data-quality":0.296002538,"ml-security":0.1366043947}}
{"text":"To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.1070368279,"dev-research":0.2005929555,"prompt-eng":0.3921589095,"data-quality":0.2241286424,"ml-security":0.0635269614}}
{"text":"We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0470086709,"dev-research":0.256317892,"prompt-eng":0.4284682529,"data-quality":0.1781734578,"ml-security":0.1733673487}}
{"text":"The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0150293172,"dev-research":0.2705197059,"prompt-eng":0.3451907104,"data-quality":0.0442438323,"ml-security":0.1195860396}}
{"text":"Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.0732914887,"dev-research":0.2538271582,"prompt-eng":0.4043422057,"data-quality":0.1957874871,"ml-security":0.0886229107}}
{"text":"Code will be released at https://github.com/mondalanindya/MSQNet.","meta":{"url":"http://arxiv.org/abs/2307.10763v1"},"cats":{"new-dataset":0.2800248699,"dev-research":0.3311465874,"prompt-eng":0.4026879528,"data-quality":0.1187017932,"ml-security":0.1507273239}}
{"text":"This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.162439984,"dev-research":0.1984939663,"prompt-eng":0.4508983529,"data-quality":0.1135352624,"ml-security":0.143018204}}
{"text":"Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.0311891063,"dev-research":0.2686457198,"prompt-eng":0.5020841786,"data-quality":0.0927776923,"ml-security":0.1426442963}}
{"text":"Additionally, employing PTMs in practical applications can be challenging due to their considerable size.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.0210524854,"dev-research":0.2711404615,"prompt-eng":0.4637314323,"data-quality":0.0696375518,"ml-security":0.106771779}}
{"text":"Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.0451992007,"dev-research":0.2274610787,"prompt-eng":0.4707947619,"data-quality":0.0489644265,"ml-security":0.0718822256}}
{"text":"In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.2182805305,"dev-research":0.2588695586,"prompt-eng":0.4165241895,"data-quality":0.1090680614,"ml-security":0.1063049666}}
{"text":"Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.3440916475,"dev-research":0.2967365621,"prompt-eng":0.413408183,"data-quality":0.122456681,"ml-security":0.1039675704}}
{"text":"To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.0396104058,"dev-research":0.3126588395,"prompt-eng":0.4454420028,"data-quality":0.1168882448,"ml-security":0.1783019429}}
{"text":"Subsequently, Vesper employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, both of which are crucial for emotion recognition.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.1415737149,"dev-research":0.2867732739,"prompt-eng":0.4406045382,"data-quality":0.1359095177,"ml-security":0.1006341444}}
{"text":"Experimental results on the IEMOCAP, MELD, and CREMA-D datasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers.","meta":{"url":"http://arxiv.org/abs/2307.10757v1"},"cats":{"new-dataset":0.3104908102,"dev-research":0.2647700941,"prompt-eng":0.3620665391,"data-quality":0.0846354429,"ml-security":0.0912180298}}
{"text":"One-class classification (OCC) aims to train a classifier only with the target class data and attracts great attention for its strong applicability in real-world application.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.122758414,"dev-research":0.2726328997,"prompt-eng":0.4030121043,"data-quality":0.2386402433,"ml-security":0.239249063}}
{"text":"Despite a lot of advances have been made in OCC, it still lacks the effective OCC loss functions for deep learning.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0672026169,"dev-research":0.2564499433,"prompt-eng":0.319488685,"data-quality":0.1725828476,"ml-security":0.1796291189}}
{"text":"In this paper, a novel logarithmic barrier function based OCC loss (LBL) that assigns large gradients to the margin samples and thus derives more compact hypersphere, is first proposed by approximating the OCC objective smoothly.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0538389166,"dev-research":0.1782741833,"prompt-eng":0.3435976676,"data-quality":0.1219472857,"ml-security":0.137022537}}
{"text":"But the optimization of LBL may be instability especially when samples lie on the boundary leading to the infinity loss.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0085558619,"dev-research":0.1562852032,"prompt-eng":0.3679446423,"data-quality":0.1844157754,"ml-security":0.1844223766}}
{"text":"To address this issue, then, a unilateral relaxation Sigmoid function is introduced into LBL and a novel OCC loss named LBLSig is proposed.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0115183045,"dev-research":0.1762711868,"prompt-eng":0.4099261193,"data-quality":0.1500132948,"ml-security":0.0942082216}}
{"text":"The LBLSig can be seen as the fusion of the mean square error (MSE) and the cross entropy (CE) and the optimization of LBLSig is smoother owing to the unilateral relaxation Sigmoid function.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0060466051,"dev-research":0.1904193283,"prompt-eng":0.4011099734,"data-quality":0.0997003557,"ml-security":0.0675308788}}
{"text":"The effectiveness of the proposed LBL and LBLSig is experimentally demonstrated in comparisons with several state-of-the-art OCC algorithms on different network structures.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.0299504721,"dev-research":0.2184337019,"prompt-eng":0.3421202833,"data-quality":0.1083542872,"ml-security":0.0851110651}}
{"text":"The source code can be found at https://github.com/ML-HDU/LBL_LBLSig.","meta":{"url":"http://arxiv.org/abs/2307.10753v1"},"cats":{"new-dataset":0.209850833,"dev-research":0.2397122879,"prompt-eng":0.4146465027,"data-quality":0.1414686625,"ml-security":0.0761827522}}
{"text":"Artificial Intelligence (AI), and in particular generative models, are transformative tools for knowledge work.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0459357346,"dev-research":0.3107734948,"prompt-eng":0.43384892,"data-quality":0.0814002278,"ml-security":0.0817419624}}
{"text":"They problematise notions of creativity, originality, plagiarism, the attribution of credit, and copyright ownership.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.146354227,"dev-research":0.3606492938,"prompt-eng":0.3877110419,"data-quality":0.3168367798,"ml-security":0.1550208757}}
{"text":"Critics of generative models emphasise the reliance on large amounts of training data, and view the output of these models as no more than randomised plagiarism, remix, or collage of the source data.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0788980998,"dev-research":0.2841840299,"prompt-eng":0.4220645566,"data-quality":0.2504185579,"ml-security":0.1972664378}}
{"text":"On these grounds, many have argued for stronger regulations on the deployment, use, and attribution of the output of these models.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0376263878,"dev-research":0.3764301809,"prompt-eng":0.4004570712,"data-quality":0.1310016359,"ml-security":0.2644730149}}
{"text":"However, these issues are not new or unique to artificial intelligence.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0672863407,"dev-research":0.3685452717,"prompt-eng":0.3326324149,"data-quality":0.2026255503,"ml-security":0.1888792779}}
{"text":"In this position paper, using examples from literary criticism, the history of art, and copyright law, I show how creativity and originality resist definition as a notatable or information-theoretic property of an object, and instead can be seen as the property of a process, an author, or a viewer.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0905145974,"dev-research":0.3267124386,"prompt-eng":0.3700615299,"data-quality":0.2064897101,"ml-security":0.1303405065}}
{"text":"Further alternative views hold that all creative work is essentially reuse (mostly without attribution), or that randomness itself can be creative.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0546120186,"dev-research":0.3501810815,"prompt-eng":0.3842047871,"data-quality":0.1915568983,"ml-security":0.1302460152}}
{"text":"I suggest that creativity is ultimately defined by communities of creators and receivers, and the deemed sources of creativity in a workflow often depend on which parts of the workflow can be automated.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.057140101,"dev-research":0.4705383209,"prompt-eng":0.4405347922,"data-quality":0.1926168535,"ml-security":0.0581979472}}
{"text":"Using examples from recent studies of AI in creative knowledge work, I suggest that AI shifts knowledge work from material production to critical integration.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.054840554,"dev-research":0.359891184,"prompt-eng":0.3744095773,"data-quality":0.1406103238,"ml-security":0.0766384425}}
{"text":"This position paper aims to begin a conversation around a more nuanced approach to the problems of creativity and credit assignment for generative models, one which more fully recognises the importance of the creative and curatorial voice of the users of these models and moves away from simpler notational or information-theoretic views.","meta":{"url":"http://arxiv.org/abs/2307.10751v1"},"cats":{"new-dataset":0.0616447711,"dev-research":0.2827292237,"prompt-eng":0.4602779532,"data-quality":0.2384621903,"ml-security":0.0860876796}}
{"text":"The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0349049651,"dev-research":0.3392853775,"prompt-eng":0.4077226258,"data-quality":0.3904903079,"ml-security":0.0612954429}}
{"text":"Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0251535563,"dev-research":0.3421830937,"prompt-eng":0.4246568494,"data-quality":0.3225076769,"ml-security":0.1181398905}}
{"text":"In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0117706775,"dev-research":0.322343428,"prompt-eng":0.3850273844,"data-quality":0.1996485984,"ml-security":0.0997229073}}
{"text":"A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.018920475,"dev-research":0.2460114234,"prompt-eng":0.3959704739,"data-quality":0.1166855805,"ml-security":0.0992175521}}
{"text":"In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0658006735,"dev-research":0.3104825357,"prompt-eng":0.3867227429,"data-quality":0.2417911285,"ml-security":0.116107081}}
{"text":"To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0586261959,"dev-research":0.2555081144,"prompt-eng":0.4158872682,"data-quality":0.2373673726,"ml-security":0.1428988622}}
{"text":"To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0474913668,"dev-research":0.2791689901,"prompt-eng":0.4116697157,"data-quality":0.5242728552,"ml-security":0.1709919315}}
{"text":"First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&S model.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.1335068658,"dev-research":0.250770766,"prompt-eng":0.427434967,"data-quality":0.5774988336,"ml-security":0.1511573422}}
{"text":"To address these limitations, we propose a new Soft D&S model with improved accuracy in estimating soft labels.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.121729462,"dev-research":0.2570584692,"prompt-eng":0.4334333668,"data-quality":0.4988814007,"ml-security":0.0884568963}}
{"text":"Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&S, in combination with different fairness options using synthetic and semi-synthetic data.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0761678612,"dev-research":0.2682865438,"prompt-eng":0.3769379177,"data-quality":0.2115299491,"ml-security":0.1704836013}}
{"text":"The experimental results suggest that the combination of Soft D&S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.0265149599,"dev-research":0.245091215,"prompt-eng":0.3626908502,"data-quality":0.1679473249,"ml-security":0.2154365976}}
{"text":"These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.","meta":{"url":"http://arxiv.org/abs/2307.10749v1"},"cats":{"new-dataset":0.031122394,"dev-research":0.3504552179,"prompt-eng":0.4103207132,"data-quality":0.223760402,"ml-security":0.1486982483}}
{"text":"Recommending suitable jobs to users is a critical task in online recruitment platforms, as it can enhance users' satisfaction and the platforms' profitability.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0152513924,"dev-research":0.3118500315,"prompt-eng":0.4254634427,"data-quality":0.0893134487,"ml-security":0.1211233908}}
{"text":"While existing job recommendation methods encounter challenges such as the low quality of users' resumes, which hampers their accuracy and practical effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0189981464,"dev-research":0.3624230395,"prompt-eng":0.3855188898,"data-quality":0.182982548,"ml-security":0.1041572489}}
{"text":"With the rapid development of large language models (LLMs), utilizing the rich external knowledge encapsulated within them, as well as their powerful capabilities of text processing and reasoning, is a promising way to complete users' resumes for more accurate recommendations.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.1150173337,"dev-research":0.2580327253,"prompt-eng":0.5139140815,"data-quality":0.1339706697,"ml-security":0.0719761271}}
{"text":"However, directly leveraging LLMs to enhance recommendation results is not a one-size-fits-all solution, as LLMs may suffer from fabricated generation and few-shot problems, which degrade the quality of resume completion.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0349765947,"dev-research":0.23175785,"prompt-eng":0.4895294572,"data-quality":0.1260488584,"ml-security":0.0618817051}}
{"text":"In this paper, we propose a novel LLM-based approach for job recommendation.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0523929529,"dev-research":0.2195493363,"prompt-eng":0.4538874562,"data-quality":0.1016108312,"ml-security":0.0916402994}}
{"text":"To alleviate the limitation of fabricated generation for LLMs, we extract accurate and valuable information beyond users' self-description, which helps the LLMs better profile users for resume completion.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.1372286866,"dev-research":0.3018669389,"prompt-eng":0.5540264987,"data-quality":0.2130733023,"ml-security":0.0883999209}}
{"text":"Specifically, we not only extract users' explicit properties (e.g., skills, interests) from their self-description but also infer users' implicit characteristics from their behaviors for more accurate and meaningful resume completion.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.024856775,"dev-research":0.3427959893,"prompt-eng":0.4885949683,"data-quality":0.1779306915,"ml-security":0.1243153901}}
{"text":"Nevertheless, some users still suffer from few-shot problems, which arise due to scarce interaction records, leading to limited guidance for the models in generating high-quality resumes.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0684347613,"dev-research":0.2925271908,"prompt-eng":0.4072354506,"data-quality":0.1241630339,"ml-security":0.0969618262}}
{"text":"To address this issue, we propose aligning unpaired low-quality with high-quality generated resumes by Generative Adversarial Networks (GANs), which can refine the resume representations for better recommendation results.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.0877462414,"dev-research":0.2632761783,"prompt-eng":0.3901151536,"data-quality":0.208042799,"ml-security":0.0979768944}}
{"text":"Extensive experiments on three large real-world recruitment datasets demonstrate the effectiveness of our proposed method.","meta":{"url":"http://arxiv.org/abs/2307.10747v1"},"cats":{"new-dataset":0.1849089675,"dev-research":0.2348271423,"prompt-eng":0.4179907293,"data-quality":0.1143560528,"ml-security":0.2456568429}}
{"text":"Active learning algorithms have become increasingly popular for training models with limited data.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.0951559235,"dev-research":0.2063790182,"prompt-eng":0.3600034586,"data-quality":0.1038670853,"ml-security":0.3335168801}}
{"text":"However, selecting data for annotation remains a challenging problem due to the limited information available on unseen data.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.2968486131,"dev-research":0.289630101,"prompt-eng":0.3988354012,"data-quality":0.4140564319,"ml-security":0.15075692}}
{"text":"To address this issue, we propose EdgeAL, which utilizes the edge information of unseen images as {\\it a priori} information for measuring uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.0808957197,"dev-research":0.2689911644,"prompt-eng":0.3911616761,"data-quality":0.237204285,"ml-security":0.0857122292}}
{"text":"The uncertainty is quantified by analyzing the divergence and entropy in model predictions across edges.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.0166252978,"dev-research":0.2433185376,"prompt-eng":0.3829926449,"data-quality":0.1844462925,"ml-security":0.1097476466}}
{"text":"This measure is then used to select superpixels for annotation.","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.1104702575,"dev-research":0.2854402454,"prompt-eng":0.4288760769,"data-quality":0.146214276,"ml-security":0.0331158355}}
{"text":"We demonstrate the effectiveness of EdgeAL on multi-class Optical Coherence Tomography (OCT) segmentation tasks, where we achieved a 99% dice score while reducing the annotation label cost to 12%, 2.3%, and 3%, respectively, on three publicly available datasets (Duke, AROI, and UMN).","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.1463583439,"dev-research":0.2662775504,"prompt-eng":0.3665439921,"data-quality":0.2104151816,"ml-security":0.0576774523}}
{"text":"The source code is available at \\url{https://github.com/Mak-Ta-Reque/EdgeAL}","meta":{"url":"http://arxiv.org/abs/2307.10745v1"},"cats":{"new-dataset":0.1607692971,"dev-research":0.2684626043,"prompt-eng":0.357678895,"data-quality":0.1027259902,"ml-security":0.0873859761}}
{"text":"This work addresses human intention identification during physical Human-Robot Interaction (pHRI) tasks to include this information in an assistive controller.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0301737779,"dev-research":0.3037098715,"prompt-eng":0.4781942694,"data-quality":0.1271097656,"ml-security":0.0817779527}}
{"text":"To this purpose, human intention is defined as the desired trajectory that the human wants to follow over a finite rolling prediction horizon so that the robot can assist in pursuing it.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.024020759,"dev-research":0.2860334663,"prompt-eng":0.4333574824,"data-quality":0.0532511891,"ml-security":0.0851947058}}
{"text":"This work investigates a Recurrent Neural Network (RNN), specifically, Long-Short Term Memory (LSTM) cascaded with a Fully Connected layer.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.180604372,"dev-research":0.1998108835,"prompt-eng":0.3868967659,"data-quality":0.1039378127,"ml-security":0.1705626777}}
{"text":"In particular, we propose an iterative training procedure to adapt the model.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0398500465,"dev-research":0.2178050352,"prompt-eng":0.4555390686,"data-quality":0.1464085437,"ml-security":0.1020702977}}
{"text":"Such an iterative procedure is powerful in reducing the prediction error.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0173059282,"dev-research":0.2634038814,"prompt-eng":0.4306474043,"data-quality":0.1607809437,"ml-security":0.1279702903}}
{"text":"Still, it has the drawback that it is time-consuming and does not generalize to different users or different co-manipulated objects.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0076663768,"dev-research":0.4227890326,"prompt-eng":0.3752907703,"data-quality":0.0612295092,"ml-security":0.1288917808}}
{"text":"To overcome this issue, Transfer Learning (TL) adapts the pre-trained model to new trajectories, users, and co-manipulated objects by freezing the LSTM layer and fine-tuning the last FC layer, which makes the procedure faster.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0969582839,"dev-research":0.2399782305,"prompt-eng":0.4570367598,"data-quality":0.070576312,"ml-security":0.1214264823}}
{"text":"Experiments show that the iterative procedure adapts the model and reduces prediction error.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.007519108,"dev-research":0.2914925068,"prompt-eng":0.4673929721,"data-quality":0.1427891669,"ml-security":0.1080952299}}
{"text":"Experiments also show that TL adapts to different users and to the co-manipulation of a large object.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.0156230668,"dev-research":0.3362205789,"prompt-eng":0.4573156498,"data-quality":0.0473606369,"ml-security":0.0748077462}}
{"text":"Finally, to check the utility of adopting the proposed method, we compare the proposed controller enhanced by the intention prediction with the other two standard controllers of pHRI.","meta":{"url":"http://arxiv.org/abs/2307.10743v1"},"cats":{"new-dataset":0.012190385,"dev-research":0.2672765047,"prompt-eng":0.4540758027,"data-quality":0.0646911338,"ml-security":0.0694728351}}
{"text":"Applications involving humans and robots working together are spreading nowadays.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0579025067,"dev-research":0.3322812785,"prompt-eng":0.3972943757,"data-quality":0.0509847004,"ml-security":0.1091522701}}
{"text":"Alongside, modeling and control techniques that allow physical Human-Robot Interaction (pHRI) are widely investigated.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0602214631,"dev-research":0.2780606512,"prompt-eng":0.4275614656,"data-quality":0.0368270427,"ml-security":0.0642095199}}
{"text":"To better understand its potential application in pHRI, this work investigates the Cooperative Differential Game Theory modeling of pHRI in a cooperative reaching task, specifically for reference tracking.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0748147478,"dev-research":0.2794526061,"prompt-eng":0.4200437291,"data-quality":0.0498185398,"ml-security":0.084404935}}
{"text":"The proposed controller based on Collaborative Game Theory is deeply analyzed and compared in simulations with two other techniques, Linear Quadratic Regulator (LQR) and Non-Cooperative Game-Theoretic Controller.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0283866154,"dev-research":0.2797030343,"prompt-eng":0.3776438983,"data-quality":0.0429592669,"ml-security":0.0859562838}}
{"text":"The set of simulations shows how different tuning of control parameters affects the system response and control efforts of both the players for the three controllers, suggesting the use of Cooperative GT in the case the robot should assist the human, while Non-Cooperative GT represents a better choice in the case the robot should lead the action.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0157647878,"dev-research":0.2243897499,"prompt-eng":0.4311877451,"data-quality":0.0424978291,"ml-security":0.0630847335}}
{"text":"Finally, preliminary tests with a trained human are performed to extract useful information on the real applicability and limitations of the proposed method.","meta":{"url":"http://arxiv.org/abs/2307.10739v1"},"cats":{"new-dataset":0.0097588979,"dev-research":0.3292039184,"prompt-eng":0.4345832251,"data-quality":0.173407506,"ml-security":0.1485700021}}
{"text":"Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.1472539654,"dev-research":0.2589887172,"prompt-eng":0.3753139029,"data-quality":0.1120432159,"ml-security":0.36148709}}
{"text":"Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0304538831,"dev-research":0.2534201768,"prompt-eng":0.3941467686,"data-quality":0.0814241494,"ml-security":0.2021542188}}
{"text":"Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0113515647,"dev-research":0.3012901343,"prompt-eng":0.3909653212,"data-quality":0.0914561895,"ml-security":0.1308493056}}
{"text":"The problem of balancing performance and fairness considerations when selecting FL clients remains open.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0222329522,"dev-research":0.2502477156,"prompt-eng":0.3602027052,"data-quality":0.0740384296,"ml-security":0.1505031992}}
{"text":"To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0430027214,"dev-research":0.2313704471,"prompt-eng":0.3946350051,"data-quality":0.1103548057,"ml-security":0.177784126}}
{"text":"Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0146354694,"dev-research":0.2498606139,"prompt-eng":0.4447239401,"data-quality":0.0613086346,"ml-security":0.1796470905}}
{"text":"By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceived poor performance, thereby further enhancing fair client treatment.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.0209796947,"dev-research":0.3239155204,"prompt-eng":0.3960865718,"data-quality":0.1827553228,"ml-security":0.3345535581}}
{"text":"Extensive experiments based on real-world multimedia datasets show that FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on average than the best-performing state-of-the-art approach.","meta":{"url":"http://arxiv.org/abs/2307.10738v1"},"cats":{"new-dataset":0.3222165988,"dev-research":0.289593529,"prompt-eng":0.3823730345,"data-quality":0.2501280429,"ml-security":0.1734021499}}
{"text":"We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020).","meta":{"url":"http://arxiv.org/abs/2307.10736v1"},"cats":{"new-dataset":0.2938550591,"dev-research":0.1675297349,"prompt-eng":0.4481183361,"data-quality":0.0965521929,"ml-security":0.0708648192}}
{"text":"We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can.","meta":{"url":"http://arxiv.org/abs/2307.10736v1"},"cats":{"new-dataset":0.027867582,"dev-research":0.2264655378,"prompt-eng":0.3963175904,"data-quality":0.3812319463,"ml-security":0.3455758555}}
{"text":"This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data.","meta":{"url":"http://arxiv.org/abs/2307.10736v1"},"cats":{"new-dataset":0.0676126347,"dev-research":0.1869287633,"prompt-eng":0.4116058491,"data-quality":0.1903764013,"ml-security":0.3248557871}}
{"text":"Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.","meta":{"url":"http://arxiv.org/abs/2307.10736v1"},"cats":{"new-dataset":0.0680370251,"dev-research":0.1971871617,"prompt-eng":0.391043618,"data-quality":0.1203521985,"ml-security":0.1381914534}}
{"text":"Managing shared mutable states in high concurrency state access operations is a persistent challenge in Network Functions Virtualization (NFV).","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0434407495,"dev-research":0.2814799211,"prompt-eng":0.3493226845,"data-quality":0.092715104,"ml-security":0.1484828721}}
{"text":"This is particularly true when striving to meet chain output equivalence (COE) requirements.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0041579431,"dev-research":0.241434413,"prompt-eng":0.4124415308,"data-quality":0.1536199981,"ml-security":0.0757907108}}
{"text":"This paper presents TransNFV, an innovative NFV framework that incorporates transactional semantics to optimize NFV state management.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.1132463647,"dev-research":0.2934868778,"prompt-eng":0.3906024869,"data-quality":0.1269255844,"ml-security":0.0543611019}}
{"text":"The TransNFV integrates VNF state access operations as transactions, resolves transaction dependencies, schedules transactions dynamically, and executes transactions efficiently.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0503697623,"dev-research":0.2888265817,"prompt-eng":0.3752278861,"data-quality":0.0628023681,"ml-security":0.0831703277}}
{"text":"Initial findings suggest that TransNFV maintains shared VNF state consistency, meets COE requirements, and skillfully handles complex cross-flow states in dynamic network conditions.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0310257137,"dev-research":0.2507267189,"prompt-eng":0.3803072681,"data-quality":0.0963903612,"ml-security":0.0828192321}}
{"text":"TransNFV thus provides a promising solution to enhance state management and overall performance in future NFV platforms.","meta":{"url":"http://arxiv.org/abs/2307.10732v1"},"cats":{"new-dataset":0.0823835685,"dev-research":0.2710076798,"prompt-eng":0.3996401561,"data-quality":0.0713876459,"ml-security":0.0660733666}}
{"text":"In frequency division duplexing (FDD) cell-free massive MIMO, the acquisition of the channel state information (CSI) is very challenging because of the large overhead required for the training and feedback of the downlink channels of multiple cooperating base stations (BSs).","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0644839229,"dev-research":0.2366703595,"prompt-eng":0.4107316671,"data-quality":0.0819327113,"ml-security":0.0794252287}}
{"text":"In this paper, for systems with partial uplink-downlink channel reciprocity, and a general spatial domain channel model with variations in the average port power and correlation among port coefficients, we propose a joint-port-selection-based CSI acquisition and feedback scheme for the downlink transmission with zero-forcing precoding.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0480023698,"dev-research":0.2390287125,"prompt-eng":0.4410793776,"data-quality":0.10270199,"ml-security":0.1307250265}}
{"text":"The scheme uses an eigenvalue-decomposition-based transformation to reduce the feedback overhead by exploring the port correlation.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0205159985,"dev-research":0.298970441,"prompt-eng":0.3980679058,"data-quality":0.0670062506,"ml-security":0.0912529151}}
{"text":"We derive the sum-rate of the system for any port selection.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.026257785,"dev-research":0.2173346428,"prompt-eng":0.3701738202,"data-quality":0.0420791921,"ml-security":0.0866461582}}
{"text":"Based on the sum-rate result, we propose a low-complexity greedy-search-based joint port selection (GS-JPS) algorithm.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0286315676,"dev-research":0.2221015119,"prompt-eng":0.342913383,"data-quality":0.0489972539,"ml-security":0.064805082}}
{"text":"Moreover, to adapt to fast time-varying scenarios, a supervised deep learning-enhanced joint port selection (DL-JPS) algorithm is proposed.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0764201016,"dev-research":0.277898953,"prompt-eng":0.3529489505,"data-quality":0.0835505543,"ml-security":0.1485954416}}
{"text":"Simulations verify the effectiveness of our proposed schemes and their advantage over existing port-selection channel acquisition schemes.","meta":{"url":"http://arxiv.org/abs/2307.10730v1"},"cats":{"new-dataset":0.0398164777,"dev-research":0.2506578425,"prompt-eng":0.4113393835,"data-quality":0.0788996317,"ml-security":0.0788752663}}
{"text":"The development of an electronic voting system that would replace traditional election procedures is a research topic of great interest for many years.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0401134326,"dev-research":0.3266166122,"prompt-eng":0.4419134656,"data-quality":0.0874560742,"ml-security":0.1006782504}}
{"text":"Blockchain technology could provide some guarantees and fulfill strong requirements for electronic voting platforms, such as transparency, immutability, and confidentiality.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0169655886,"dev-research":0.2604979998,"prompt-eng":0.3636392157,"data-quality":0.0925946584,"ml-security":0.2095016233}}
{"text":"From time to time research is conducted to address problems in voting systems.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0448284064,"dev-research":0.3193554941,"prompt-eng":0.4034155584,"data-quality":0.1753006227,"ml-security":0.2004263649}}
{"text":"Many research works attempt to implement secure and reliable voting systems, which address known security, anonymity, and fraud issues that might threaten such systems.   ","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0425667759,"dev-research":0.3035639612,"prompt-eng":0.3845471543,"data-quality":0.1949397127,"ml-security":0.4216925263}}
{"text":"This paper presents a proposal of a secure electronic voting system, the EtherVote, using the Ethereum Blockchain network that focuses deeply on the field of identification of eligible citizens.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0978864728,"dev-research":0.2864319441,"prompt-eng":0.4000156533,"data-quality":0.1204482702,"ml-security":0.1936554426}}
{"text":"The proposed system will be entirely based on Blockchain without any central authority servers or databases, thus improving security, privacy, and election cost.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.055388833,"dev-research":0.2501344213,"prompt-eng":0.3260175685,"data-quality":0.061894602,"ml-security":0.2708749185}}
{"text":"Limitations, problems, and solutions are discussed, in order to make the proposed electronic voting system ideal and ready to use for national elections.","meta":{"url":"http://arxiv.org/abs/2307.10726v1"},"cats":{"new-dataset":0.0512402062,"dev-research":0.2819577,"prompt-eng":0.4020542278,"data-quality":0.0816359109,"ml-security":0.0992502009}}
{"text":"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0674270632,"dev-research":0.2655507356,"prompt-eng":0.5401179924,"data-quality":0.0983464036,"ml-security":0.088621549}}
{"text":"However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0360936634,"dev-research":0.3970752376,"prompt-eng":0.4144242949,"data-quality":0.25435493,"ml-security":0.5494283094}}
{"text":"Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0237896407,"dev-research":0.2533228389,"prompt-eng":0.5004497883,"data-quality":0.2456796076,"ml-security":0.6931181455}}
{"text":"Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0458138798,"dev-research":0.2283797757,"prompt-eng":0.4121308674,"data-quality":0.404759783,"ml-security":0.4906051194}}
{"text":"In this paper, we present the theoretical limitations of such semantic censorship approaches.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0325359389,"dev-research":0.2580717848,"prompt-eng":0.3547526375,"data-quality":0.3626619163,"ml-security":0.248107966}}
{"text":"Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0504734307,"dev-research":0.3004785273,"prompt-eng":0.4256505928,"data-quality":0.2877066159,"ml-security":0.3646450187}}
{"text":"Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.134024327,"dev-research":0.3179276074,"prompt-eng":0.3926091889,"data-quality":0.286154576,"ml-security":0.7624143806}}
{"text":"As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.","meta":{"url":"http://arxiv.org/abs/2307.10719v1"},"cats":{"new-dataset":0.0893126937,"dev-research":0.2794069333,"prompt-eng":0.35948737,"data-quality":0.2604177655,"ml-security":0.6125449217}}
{"text":"Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.3175745675,"dev-research":0.2910995286,"prompt-eng":0.4064428695,"data-quality":0.8312271984,"ml-security":0.1904380724}}
{"text":"Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.0606943195,"dev-research":0.3101205955,"prompt-eng":0.4039361442,"data-quality":0.5772702503,"ml-security":0.0885972508}}
{"text":"However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.0103815459,"dev-research":0.2898044252,"prompt-eng":0.3406459983,"data-quality":0.118554075,"ml-security":0.0862664757}}
{"text":"In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.665104235,"dev-research":0.217968122,"prompt-eng":0.337263793,"data-quality":0.2377714388,"ml-security":0.2007390972}}
{"text":"Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.1307667105,"dev-research":0.3929744571,"prompt-eng":0.4309097434,"data-quality":0.598550625,"ml-security":0.1131382948}}
{"text":"These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.0346323197,"dev-research":0.2973495266,"prompt-eng":0.4529529847,"data-quality":0.2904549389,"ml-security":0.1330621752}}
{"text":"Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.1834643467,"dev-research":0.2843650251,"prompt-eng":0.4191901349,"data-quality":0.6412712825,"ml-security":0.1423589116}}
{"text":"We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.2730006605,"dev-research":0.2717539515,"prompt-eng":0.3611113946,"data-quality":0.6404450856,"ml-security":0.2105748184}}
{"text":"We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.8372535256,"dev-research":0.249827739,"prompt-eng":0.3553428265,"data-quality":0.6561780784,"ml-security":0.2490738723}}
{"text":"Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.","meta":{"url":"http://arxiv.org/abs/2307.10718v1"},"cats":{"new-dataset":0.1184416788,"dev-research":0.2601862302,"prompt-eng":0.384202864,"data-quality":0.2819348071,"ml-security":0.1476075344}}
{"text":"Self-supervised monocular depth estimation (SS-MDE) has the potential to scale to vast quantities of data.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.1082612833,"dev-research":0.1852793433,"prompt-eng":0.4042435123,"data-quality":0.1146945051,"ml-security":0.0653546481}}
{"text":"Unfortunately, existing approaches limit themselves to the automotive domain, resulting in models incapable of generalizing to complex environments such as natural or indoor settings.   ","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.0281654332,"dev-research":0.2867203497,"prompt-eng":0.384724127,"data-quality":0.0858830589,"ml-security":0.1512716708}}
{"text":"To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more data than existing automotive datasets.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.7731075457,"dev-research":0.3092246945,"prompt-eng":0.3315293202,"data-quality":0.1161389978,"ml-security":0.1057015253}}
{"text":"SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and scuba diving.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.4305334565,"dev-research":0.2646410958,"prompt-eng":0.338939273,"data-quality":0.0742775212,"ml-security":0.0553453747}}
{"text":"Using this dataset, we train an SS-MDE model that provides zero-shot generalization to a large collection of indoor/outdoor datasets.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.4364621639,"dev-research":0.2000081068,"prompt-eng":0.37268789,"data-quality":0.1221411435,"ml-security":0.1949868154}}
{"text":"The resulting model outperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more efficient architecture.   ","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.0710802444,"dev-research":0.2249184007,"prompt-eng":0.3863699174,"data-quality":0.1612628751,"ml-security":0.2322851345}}
{"text":"We additionally introduce a collection of best-practices to further maximize performance and zero-shot generalization.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.0556541387,"dev-research":0.2879217115,"prompt-eng":0.3870318086,"data-quality":0.1742631276,"ml-security":0.1675360067}}
{"text":"This includes 1) aspect ratio augmentation, 2) camera intrinsic estimation, 3) support frame randomization and 4) flexible motion estimation.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.1787646251,"dev-research":0.2272693331,"prompt-eng":0.3800375334,"data-quality":0.095345775,"ml-security":0.0338995499}}
{"text":"Code is available at https://github.com/jspenmar/slowtv_monodepth.","meta":{"url":"http://arxiv.org/abs/2307.10713v1"},"cats":{"new-dataset":0.1902537746,"dev-research":0.2937448252,"prompt-eng":0.3940613872,"data-quality":0.0892436431,"ml-security":0.0605923127}}
{"text":"Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.0792430721,"dev-research":0.2387162022,"prompt-eng":0.4713168968,"data-quality":0.1196152618,"ml-security":0.1584695987}}
{"text":"This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.1584255,"dev-research":0.294971177,"prompt-eng":0.4799891523,"data-quality":0.2028981045,"ml-security":0.1275512904}}
{"text":"Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\\\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.0435225259,"dev-research":0.2185840716,"prompt-eng":0.360691304,"data-quality":0.1012821027,"ml-security":0.1963371256}}
{"text":"To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.061210372,"dev-research":0.1944260594,"prompt-eng":0.4139845578,"data-quality":0.1226679323,"ml-security":0.1300407258}}
{"text":"It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.0052956592,"dev-research":0.2481925362,"prompt-eng":0.4424840575,"data-quality":0.1619546977,"ml-security":0.2462350553}}
{"text":"To reduce numerical errors in both the forward generation and gradient backpropagation processes, we further reparameterize the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.027123766,"dev-research":0.2223005285,"prompt-eng":0.4022483049,"data-quality":0.1280127951,"ml-security":0.1804697038}}
{"text":"Finally, we demonstrate the effectiveness of AdjointDPM on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.","meta":{"url":"http://arxiv.org/abs/2307.10711v1"},"cats":{"new-dataset":0.1202191187,"dev-research":0.2979605289,"prompt-eng":0.4428007392,"data-quality":0.3655164149,"ml-security":0.6397231361}}
{"text":"We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0863196084,"dev-research":0.1880947582,"prompt-eng":0.3956553089,"data-quality":0.0525163845,"ml-security":0.2124831322}}
{"text":"Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0353372138,"dev-research":0.1917941946,"prompt-eng":0.4327947866,"data-quality":0.0857794992,"ml-security":0.1060146237}}
{"text":"To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0479666513,"dev-research":0.166906714,"prompt-eng":0.4192534303,"data-quality":0.0577427008,"ml-security":0.0652383634}}
{"text":"By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0261958507,"dev-research":0.2015556308,"prompt-eng":0.4527223623,"data-quality":0.0644329721,"ml-security":0.1495081951}}
{"text":"We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.1499874449,"dev-research":0.1909977089,"prompt-eng":0.4393614369,"data-quality":0.0568468845,"ml-security":0.0956502836}}
{"text":"Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0218053696,"dev-research":0.224491088,"prompt-eng":0.4314875685,"data-quality":0.0857678406,"ml-security":0.153365246}}
{"text":"Our method consistently outperforms previous approaches across a range of tasks.","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.0149052408,"dev-research":0.387441455,"prompt-eng":0.4001525843,"data-quality":0.1693057189,"ml-security":0.0576260924}}
{"text":"Code and supplementary materials are available on the project page https://haosulab.github.io/RPG/","meta":{"url":"http://arxiv.org/abs/2307.10710v1"},"cats":{"new-dataset":0.5035278957,"dev-research":0.279296131,"prompt-eng":0.413949217,"data-quality":0.0576563894,"ml-security":0.0556037749}}
{"text":"Reconfigurable intelligent surface (RIS) architectures not limited to diagonal phase shift matrices have recently been considered to increase their flexibility in shaping the wireless channel.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0153533,"dev-research":0.2198761703,"prompt-eng":0.3991929803,"data-quality":0.0479179739,"ml-security":0.0682080932}}
{"text":"One of these beyond-diagonal RIS or BD-RIS architectures leads to a unitary and symmetric RIS matrix.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0306417153,"dev-research":0.2021135523,"prompt-eng":0.3788803802,"data-quality":0.0684917456,"ml-security":0.1026303104}}
{"text":"In this letter, we consider the problem of maximizing the signal-to-noise ratio (SNR) in single and multiple antenna links assisted by a BD-RIS.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0268229984,"dev-research":0.1824126358,"prompt-eng":0.3886914386,"data-quality":0.1899959955,"ml-security":0.1055437983}}
{"text":"The Max-SNR problem admits a closed-form solution based on the Takagi factorization of a certain complex and symmetric matrix.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0779311472,"dev-research":0.1884608008,"prompt-eng":0.3693303156,"data-quality":0.1153823932,"ml-security":0.1040602552}}
{"text":"This allows us to solve the max-SNR problem for SISO, SIMO, and MISO channels.","meta":{"url":"http://arxiv.org/abs/2307.10707v1"},"cats":{"new-dataset":0.0498217845,"dev-research":0.2463318173,"prompt-eng":0.3881705958,"data-quality":0.1354701268,"ml-security":0.0982874237}}
{"text":"Semantic segmentation is a common task in autonomous driving to understand the surrounding environment.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.1397299123,"dev-research":0.3246492379,"prompt-eng":0.4300652184,"data-quality":0.212967109,"ml-security":0.1223738611}}
{"text":"Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0587598177,"dev-research":0.2504621173,"prompt-eng":0.365554146,"data-quality":0.1273081534,"ml-security":0.0954439693}}
{"text":"However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.1127165208,"dev-research":0.2424202394,"prompt-eng":0.3675924983,"data-quality":0.1578819904,"ml-security":0.1206214425}}
{"text":"This paper proposes a lightweight model for the driveable area and lane line segmentation.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0953089073,"dev-research":0.2126898315,"prompt-eng":0.3669596678,"data-quality":0.1086902815,"ml-security":0.0600361347}}
{"text":"TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0723431739,"dev-research":0.2456971687,"prompt-eng":0.3773122715,"data-quality":0.163842451,"ml-security":0.0676721838}}
{"text":"We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.4275324905,"dev-research":0.2372041314,"prompt-eng":0.3739160732,"data-quality":0.1656328976,"ml-security":0.1554031379}}
{"text":"Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0288613859,"dev-research":0.284232638,"prompt-eng":0.3808502817,"data-quality":0.1053944431,"ml-security":0.0918983031}}
{"text":"Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0866604298,"dev-research":0.2391838651,"prompt-eng":0.3991548048,"data-quality":0.1131824782,"ml-security":0.0697198731}}
{"text":"Furthermore, TwinLiteNet can run in real-time on embedded devices with limited computing power, especially since it achieves 60FPS on Jetson Xavier NX, making it an ideal solution for self-driving vehicles.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.063640549,"dev-research":0.3440716467,"prompt-eng":0.3727644253,"data-quality":0.0911343198,"ml-security":0.1363530944}}
{"text":"Code is available: url{https://github.com/chequanghuy/TwinLiteNet}.","meta":{"url":"http://arxiv.org/abs/2307.10705v1"},"cats":{"new-dataset":0.0806018478,"dev-research":0.2599912949,"prompt-eng":0.3932203563,"data-quality":0.1450693718,"ml-security":0.0817745968}}
{"text":"The drastic growth of electric vehicles and photovoltaics can introduce new challenges, such as electrical current congestion and voltage limit violations due to peak load demands.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0413847678,"dev-research":0.2685259638,"prompt-eng":0.3848239891,"data-quality":0.1471379191,"ml-security":0.1503309758}}
{"text":"These issues can be mitigated by controlling the operation of electric vehicles i.e., smart charging.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.02311038,"dev-research":0.3431701959,"prompt-eng":0.4337079824,"data-quality":0.2253719993,"ml-security":0.2390278753}}
{"text":"Centralized smart charging solutions have already been proposed in the literature.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.041529249,"dev-research":0.2795857171,"prompt-eng":0.4002891427,"data-quality":0.1002466725,"ml-security":0.1310756686}}
{"text":"But such solutions may lack scalability and suffer from inherent drawbacks of centralization, such as a single point of failure, and data privacy concerns.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0179515111,"dev-research":0.318580484,"prompt-eng":0.3220488905,"data-quality":0.1756912898,"ml-security":0.3485599571}}
{"text":"Decentralization can help tackle these challenges.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0438398793,"dev-research":0.3215819312,"prompt-eng":0.3810395669,"data-quality":0.0821638247,"ml-security":0.1530722308}}
{"text":"In this paper, a fully decentralized smart charging system is proposed using the philosophy of adaptive multi-agent systems.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0401485748,"dev-research":0.2302406135,"prompt-eng":0.4009126227,"data-quality":0.0981436227,"ml-security":0.1459390463}}
{"text":"The proposed system utilizes multi-armed bandit learning to handle uncertainties in the system.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0380138512,"dev-research":0.2391370921,"prompt-eng":0.4558191799,"data-quality":0.1584793367,"ml-security":0.2154106606}}
{"text":"The presented system is decentralized, scalable, real-time, model-free, and takes fairness among different players into account.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0802966855,"dev-research":0.3185165544,"prompt-eng":0.3576490314,"data-quality":0.0623938562,"ml-security":0.1942587831}}
{"text":"A detailed case study is also presented for performance evaluation.","meta":{"url":"http://arxiv.org/abs/2307.10704v1"},"cats":{"new-dataset":0.0317795903,"dev-research":0.369753912,"prompt-eng":0.4119354085,"data-quality":0.0848408142,"ml-security":0.0613198002}}
{"text":"Granger causality (GC) is often considered not an actual form of causality.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.009054916,"dev-research":0.2107946054,"prompt-eng":0.3627051535,"data-quality":0.1413430879,"ml-security":0.1045106766}}
{"text":"Still, it is arguably the most widely used method to assess the predictability of a time series from another one.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0186197971,"dev-research":0.3181117928,"prompt-eng":0.3797362325,"data-quality":0.129048902,"ml-security":0.1023999663}}
{"text":"Granger causality has been widely used in many applied disciplines, from neuroscience and econometrics to Earth sciences.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0245805338,"dev-research":0.2562234311,"prompt-eng":0.4350700327,"data-quality":0.0825867916,"ml-security":0.0959266666}}
{"text":"We revisit GC under a graphical perspective of state-space models.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0248978533,"dev-research":0.1686760167,"prompt-eng":0.4173986251,"data-quality":0.0952850422,"ml-security":0.1113378434}}
{"text":"For that, we use GraphEM, a recently presented expectation-maximisation algorithm for estimating the linear matrix operator in the state equation of a linear-Gaussian state-space model.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0451254769,"dev-research":0.1784054375,"prompt-eng":0.3947928075,"data-quality":0.0962738264,"ml-security":0.1420720901}}
{"text":"Lasso regularisation is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0464432631,"dev-research":0.1733589479,"prompt-eng":0.4065352684,"data-quality":0.1304170525,"ml-security":0.1010139647}}
{"text":"Experiments in toy examples and challenging climate problems illustrate the benefits of the proposed model and inference technique over standard Granger causality methods.","meta":{"url":"http://arxiv.org/abs/2307.10703v1"},"cats":{"new-dataset":0.0363875181,"dev-research":0.2594887903,"prompt-eng":0.4394377098,"data-quality":0.1068208074,"ml-security":0.1185179728}}
{"text":"Knowledge graphs, represented in RDF, are able to model entities and their relations by means of ontologies.","meta":{"url":"http://arxiv.org/abs/2307.10702v1"},"cats":{"new-dataset":0.3520865708,"dev-research":0.2841382593,"prompt-eng":0.3864487365,"data-quality":0.1187014989,"ml-security":0.0522141722}}
{"text":"The use of knowledge graphs for information modeling has attracted interest in recent years.","meta":{"url":"http://arxiv.org/abs/2307.10702v1"},"cats":{"new-dataset":0.1370017218,"dev-research":0.3446532825,"prompt-eng":0.3979247987,"data-quality":0.0913279565,"ml-security":0.0707345904}}
{"text":"In recommender systems, items and users can be mapped and integrated into the knowledge graph, which can represent more links and relationships between users and items.","meta":{"url":"http://arxiv.org/abs/2307.10702v1"},"cats":{"new-dataset":0.0733427562,"dev-research":0.3808167782,"prompt-eng":0.4257214822,"data-quality":0.0909400973,"ml-security":0.0631244854}}
{"text":"Constraint-based recommender systems are based on the idea of explicitly exploiting deep recommendation knowledge through constraints to identify relevant recommendations.","meta":{"url":"http://arxiv.org/abs/2307.10702v1"},"cats":{"new-dataset":0.0528331132,"dev-research":0.3246129802,"prompt-eng":0.4008453531,"data-quality":0.1411272841,"ml-security":0.1993561615}}
{"text":"When combined with knowledge graphs, a constraint-based recommender system gains several benefits in terms of constraint sets.","meta":{"url":"http://arxiv.org/abs/2307.10702v1"},"cats":{"new-dataset":0.0391590849,"dev-research":0.3371306995,"prompt-eng":0.3929297317,"data-quality":0.0823351461,"ml-security":0.1190287529}}
{"text":"In this paper, we investigate and propose the construction of a constraint-based recommender system via RDF knowledge graphs applied to the vehicle purchase/sale domain.","meta":{"url":"http://arxiv.org/abs/2307.10702v1"},"cats":{"new-dataset":0.2210835394,"dev-research":0.2813443613,"prompt-eng":0.3811779967,"data-quality":0.1038337536,"ml-security":0.0633655354}}
{"text":"The results of our experiments show that the proposed approach is able to efficiently identify recommendations in accordance with user preferences.","meta":{"url":"http://arxiv.org/abs/2307.10702v1"},"cats":{"new-dataset":0.0275014583,"dev-research":0.3385542244,"prompt-eng":0.4549244564,"data-quality":0.1479664409,"ml-security":0.0914314864}}
{"text":"There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis.","meta":{"url":"http://arxiv.org/abs/2307.10700v1"},"cats":{"new-dataset":0.1268647596,"dev-research":0.2204161846,"prompt-eng":0.4180596854,"data-quality":0.1572542393,"ml-security":0.0569483385}}
{"text":"Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022.","meta":{"url":"http://arxiv.org/abs/2307.10700v1"},"cats":{"new-dataset":0.5293856792,"dev-research":0.2921662688,"prompt-eng":0.3527010694,"data-quality":0.1011939277,"ml-security":0.0793558567}}
{"text":"We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration.","meta":{"url":"http://arxiv.org/abs/2307.10700v1"},"cats":{"new-dataset":0.0819605553,"dev-research":0.2515083704,"prompt-eng":0.4495975901,"data-quality":0.1222442729,"ml-security":0.0500067305}}
{"text":"We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors.","meta":{"url":"http://arxiv.org/abs/2307.10700v1"},"cats":{"new-dataset":0.0755461073,"dev-research":0.2918818783,"prompt-eng":0.4630784576,"data-quality":0.0831800755,"ml-security":0.1316972653}}
{"text":"LLM research is also shaped by social dynamics: we document gender and academic/industry disparities in the topics LLM authors focus on, and a US/China schism in the collaboration network.","meta":{"url":"http://arxiv.org/abs/2307.10700v1"},"cats":{"new-dataset":0.0567122934,"dev-research":0.2222872868,"prompt-eng":0.4143321844,"data-quality":0.08017728,"ml-security":0.0680974286}}
{"text":"Overall, our analysis documents the profound ways in which LLM research both shapes and is shaped by society, attesting to the necessity of sociotechnical lenses.","meta":{"url":"http://arxiv.org/abs/2307.10700v1"},"cats":{"new-dataset":0.0375864174,"dev-research":0.297661245,"prompt-eng":0.4390748131,"data-quality":0.0496431808,"ml-security":0.1165742455}}
{"text":"Retinal image matching plays a crucial role in monitoring disease progression and treatment response.","meta":{"url":"http://arxiv.org/abs/2307.10698v1"},"cats":{"new-dataset":0.044557891,"dev-research":0.288361009,"prompt-eng":0.3976907554,"data-quality":0.1280646612,"ml-security":0.0558942647}}
{"text":"However, datasets with matched keypoints between temporally separated pairs of images are not available in abundance to train transformer-based model.","meta":{"url":"http://arxiv.org/abs/2307.10698v1"},"cats":{"new-dataset":0.2897653163,"dev-research":0.1470744709,"prompt-eng":0.3570499907,"data-quality":0.1161227193,"ml-security":0.1146586943}}
{"text":"We propose a novel approach based on reverse knowledge distillation to train large models with limited data while preventing overfitting.","meta":{"url":"http://arxiv.org/abs/2307.10698v1"},"cats":{"new-dataset":0.1310033081,"dev-research":0.25662339,"prompt-eng":0.4136649333,"data-quality":0.1670079851,"ml-security":0.2820459849}}
{"text":"Firstly, we propose architectural modifications to a CNN-based semi-supervised method called SuperRetina that help us improve its results on a publicly available dataset.","meta":{"url":"http://arxiv.org/abs/2307.10698v1"},"cats":{"new-dataset":0.2810193473,"dev-research":0.2578339516,"prompt-eng":0.3840456524,"data-quality":0.1790252963,"ml-security":0.1501262276}}
{"text":"Then, we train a computationally heavier model based on a vision transformer encoder using the lighter CNN-based model, which is counter-intuitive in the field knowledge-distillation research where training lighter models based on heavier ones is the norm.","meta":{"url":"http://arxiv.org/abs/2307.10698v1"},"cats":{"new-dataset":0.0153896712,"dev-research":0.2667354168,"prompt-eng":0.3840319005,"data-quality":0.12570456,"ml-security":0.1845345588}}
{"text":"Surprisingly, such reverse knowledge distillation improves generalization even further.","meta":{"url":"http://arxiv.org/abs/2307.10698v1"},"cats":{"new-dataset":0.0150119881,"dev-research":0.3287773441,"prompt-eng":0.441201313,"data-quality":0.1729148403,"ml-security":0.168630697}}
{"text":"Our experiments suggest that high-dimensional fitting in representation space may prevent overfitting unlike training directly to match the final output.","meta":{"url":"http://arxiv.org/abs/2307.10698v1"},"cats":{"new-dataset":0.035786704,"dev-research":0.2904150337,"prompt-eng":0.437830977,"data-quality":0.2502518968,"ml-security":0.2767044472}}
{"text":"We also provide a public dataset with annotations for retinal image keypoint detection and matching to help the research community develop algorithms for retinal image applications.","meta":{"url":"http://arxiv.org/abs/2307.10698v1"},"cats":{"new-dataset":0.4518302389,"dev-research":0.2848437116,"prompt-eng":0.3907849377,"data-quality":0.2106407798,"ml-security":0.1057565528}}
{"text":"The widespread use of mobile devices for various digital services has created a need for reliable and real-time person authentication.","meta":{"url":"http://arxiv.org/abs/2307.10697v1"},"cats":{"new-dataset":0.0379900617,"dev-research":0.3563886331,"prompt-eng":0.4186322326,"data-quality":0.1011204027,"ml-security":0.1927740161}}
{"text":"In this context, facial recognition technologies have emerged as a dependable method for verifying users due to the prevalence of cameras in mobile devices and their integration into everyday applications.","meta":{"url":"http://arxiv.org/abs/2307.10697v1"},"cats":{"new-dataset":0.0671932497,"dev-research":0.3186363505,"prompt-eng":0.4040342904,"data-quality":0.1805220355,"ml-security":0.2350155965}}
{"text":"The rapid advancement of deep Convolutional Neural Networks (CNNs) has led to numerous face verification architectures.","meta":{"url":"http://arxiv.org/abs/2307.10697v1"},"cats":{"new-dataset":0.0646181211,"dev-research":0.2953353791,"prompt-eng":0.3864289029,"data-quality":0.1116977264,"ml-security":0.2497875998}}
{"text":"However, these models are often large and impractical for mobile applications, reaching sizes of hundreds of megabytes with millions of parameters.","meta":{"url":"http://arxiv.org/abs/2307.10697v1"},"cats":{"new-dataset":0.0491650153,"dev-research":0.2662034548,"prompt-eng":0.3976136812,"data-quality":0.0497484831,"ml-security":0.138777492}}
{"text":"We address this issue by developing SqueezerFaceNet, a light face recognition network which less than 1M parameters.","meta":{"url":"http://arxiv.org/abs/2307.10697v1"},"cats":{"new-dataset":0.1927996492,"dev-research":0.213827808,"prompt-eng":0.3462717395,"data-quality":0.1180952608,"ml-security":0.1766021101}}
{"text":"This is achieved by applying a network pruning method based on Taylor scores, where filters with small importance scores are removed iteratively.","meta":{"url":"http://arxiv.org/abs/2307.10697v1"},"cats":{"new-dataset":0.0306458179,"dev-research":0.283071082,"prompt-eng":0.3498617582,"data-quality":0.2200876322,"ml-security":0.1012234352}}
{"text":"Starting from an already small network (of 1.24M) based on SqueezeNet, we show that it can be further reduced (up to 40%) without an appreciable loss in performance.","meta":{"url":"http://arxiv.org/abs/2307.10697v1"},"cats":{"new-dataset":0.0196081452,"dev-research":0.2401470889,"prompt-eng":0.3340393149,"data-quality":0.0975605083,"ml-security":0.1391439608}}
{"text":"To the best of our knowledge, we are the first to evaluate network pruning methods for the task of face recognition.","meta":{"url":"http://arxiv.org/abs/2307.10697v1"},"cats":{"new-dataset":0.0275870403,"dev-research":0.2782300015,"prompt-eng":0.3576344657,"data-quality":0.199071039,"ml-security":0.15825412}}
{"text":"Improving the feature representation ability is the foundation of many whole slide pathological image (WSIs) tasks.","meta":{"url":"http://arxiv.org/abs/2307.10696v1"},"cats":{"new-dataset":0.0628712665,"dev-research":0.3451011133,"prompt-eng":0.4684263157,"data-quality":0.1474719374,"ml-security":0.0998111682}}
{"text":"Recent works have achieved great success in pathological-specific self-supervised learning (SSL).","meta":{"url":"http://arxiv.org/abs/2307.10696v1"},"cats":{"new-dataset":0.1107756593,"dev-research":0.2515057553,"prompt-eng":0.4467348246,"data-quality":0.2688553116,"ml-security":0.2827216347}}
{"text":"However, most of them only focus on learning patch-level representations, thus there is still a gap between pretext and slide-level downstream tasks, e.g., subtyping, grading and staging.","meta":{"url":"http://arxiv.org/abs/2307.10696v1"},"cats":{"new-dataset":0.0513289809,"dev-research":0.3346045854,"prompt-eng":0.4628835824,"data-quality":0.1260898642,"ml-security":0.0684056962}}
{"text":"Aiming towards slide-level representations, we propose Slide-Level Prototypical Distillation (SLPD) to explore intra- and inter-slide semantic structures for context modeling on WSIs.","meta":{"url":"http://arxiv.org/abs/2307.10696v1"},"cats":{"new-dataset":0.0795899596,"dev-research":0.2739341329,"prompt-eng":0.4736096469,"data-quality":0.1388277863,"ml-security":0.0586664749}}
{"text":"Specifically, we iteratively perform intra-slide clustering for the regions (4096x4096 patches) within each WSI to yield the prototypes and encourage the region representations to be closer to the assigned prototypes.","meta":{"url":"http://arxiv.org/abs/2307.10696v1"},"cats":{"new-dataset":0.2467706353,"dev-research":0.3363479534,"prompt-eng":0.4609544691,"data-quality":0.0789840804,"ml-security":0.0393107523}}
{"text":"By representing each slide with its prototypes, we further select similar slides by the set distance of prototypes and assign the regions by cross-slide prototypes for distillation.","meta":{"url":"http://arxiv.org/abs/2307.10696v1"},"cats":{"new-dataset":0.0676988071,"dev-research":0.2602764835,"prompt-eng":0.4469966164,"data-quality":0.0742550278,"ml-security":0.0353445908}}
{"text":"SLPD achieves state-of-the-art results on multiple slide-level benchmarks and demonstrates that representation learning of semantic structures of slides can make a suitable proxy task for WSI analysis.","meta":{"url":"http://arxiv.org/abs/2307.10696v1"},"cats":{"new-dataset":0.1380989527,"dev-research":0.340579207,"prompt-eng":0.4466051192,"data-quality":0.1554643769,"ml-security":0.0508840334}}
{"text":"Code will be available at https://github.com/Carboxy/SLPD.","meta":{"url":"http://arxiv.org/abs/2307.10696v1"},"cats":{"new-dataset":0.1212221344,"dev-research":0.2596106942,"prompt-eng":0.436259175,"data-quality":0.0965401067,"ml-security":0.0603765314}}
{"text":"Recently, denoising methods based on supervised learning have exhibited promising performance.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.0538645719,"dev-research":0.2891964602,"prompt-eng":0.3699594253,"data-quality":0.2767296577,"ml-security":0.1871182632}}
{"text":"However, their reliance on external datasets containing noisy-clean image pairs restricts their applicability.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.1763678539,"dev-research":0.2870938512,"prompt-eng":0.3212240564,"data-quality":0.3725162036,"ml-security":0.2792572606}}
{"text":"To address this limitation, researchers have focused on training denoising networks using solely a set of noisy inputs.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.0318850163,"dev-research":0.280050536,"prompt-eng":0.3155146891,"data-quality":0.3201246664,"ml-security":0.3676048669}}
{"text":"To improve the feasibility of denoising procedures, in this study, we proposed a single-image self-supervised learning method in which only the noisy input image is used for network training.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.0627897942,"dev-research":0.2503269258,"prompt-eng":0.380737108,"data-quality":0.4340569667,"ml-security":0.2522977742}}
{"text":"Gated convolution was used for feature extraction and no-reference image quality assessment was used for guiding the training process.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.0531267727,"dev-research":0.2723611505,"prompt-eng":0.3816842564,"data-quality":0.2931663287,"ml-security":0.0874638423}}
{"text":"Moreover, the proposed method sampled instances from the input image dataset using Bernoulli sampling with a certain dropout rate for training.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.1746610995,"dev-research":0.1960060692,"prompt-eng":0.388413113,"data-quality":0.2773993856,"ml-security":0.1601560783}}
{"text":"The corresponding result was produced by averaging the generated predictions from various instances of the trained network with dropouts.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.0451548362,"dev-research":0.2666372332,"prompt-eng":0.4115087957,"data-quality":0.3086690288,"ml-security":0.1803799283}}
{"text":"The experimental results indicated that the proposed method achieved state-of-the-art denoising performance on both synthetic and real-world datasets.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.2689997799,"dev-research":0.2905248367,"prompt-eng":0.2974199242,"data-quality":0.2546217479,"ml-security":0.1613901505}}
{"text":"This highlights the effectiveness and practicality of our method as a potential solution for various noise removal tasks.","meta":{"url":"http://arxiv.org/abs/2307.10695v1"},"cats":{"new-dataset":0.028743902,"dev-research":0.2602316025,"prompt-eng":0.3709037214,"data-quality":0.3226027796,"ml-security":0.0795222026}}
{"text":"We present a new framework called KorraAI for conceiving and building embodied conversational agents (ECAs).","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.2623977551,"dev-research":0.2707540032,"prompt-eng":0.4079089883,"data-quality":0.0680218324,"ml-security":0.1041612189}}
{"text":"Our framework models ECAs' behavior considering contextual information, for example, about environment and interaction time, and uncertain information provided by the human interaction partner.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.0748191131,"dev-research":0.2887056178,"prompt-eng":0.4424324668,"data-quality":0.0885064827,"ml-security":0.1283970413}}
{"text":"Moreover, agents built with KorraAI can show proactive behavior, as they can initiate interactions with human partners.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.109219312,"dev-research":0.306200642,"prompt-eng":0.3749939911,"data-quality":0.0531850111,"ml-security":0.1324121829}}
{"text":"For these purposes, KorraAI exploits probabilistic programming.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.1373647495,"dev-research":0.2600779523,"prompt-eng":0.4105574149,"data-quality":0.1068802595,"ml-security":0.2365749658}}
{"text":"Probabilistic models in KorraAI are used to model its behavior and interactions with the user.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.1462391455,"dev-research":0.267567996,"prompt-eng":0.4527582189,"data-quality":0.0737775613,"ml-security":0.1454928553}}
{"text":"They enable adaptation to the user's preferences and a certain degree of indeterminism in the ECAs to achieve more natural behavior.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.0094205526,"dev-research":0.33322715,"prompt-eng":0.4496625387,"data-quality":0.070356654,"ml-security":0.1725360671}}
{"text":"Human-like internal states, such as moods, preferences, and emotions (e.g., surprise), can be modeled in KorraAI with distributions and Bayesian networks.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.1277151822,"dev-research":0.2405923799,"prompt-eng":0.4404766016,"data-quality":0.0756567331,"ml-security":0.1446438775}}
{"text":"These models can evolve over time, even without interaction with the user.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.0301720862,"dev-research":0.253621813,"prompt-eng":0.4286652386,"data-quality":0.0530049645,"ml-security":0.2143081208}}
{"text":"ECA models are implemented as plugins and share a common interface.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.0746491154,"dev-research":0.2899248309,"prompt-eng":0.3925261975,"data-quality":0.0692848555,"ml-security":0.0772049845}}
{"text":"This enables ECA designers to focus more on the character they are modeling and less on the technical details, as well as to store and exchange ECA models.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.0465372209,"dev-research":0.3817596325,"prompt-eng":0.4360275625,"data-quality":0.0766889522,"ml-security":0.0840643174}}
{"text":"Several applications of KorraAI ECAs are possible, such as virtual sales agents, customer service agents, virtual companions, entertainers, or tutors.","meta":{"url":"http://arxiv.org/abs/2307.10693v1"},"cats":{"new-dataset":0.110735528,"dev-research":0.2405607504,"prompt-eng":0.3427927316,"data-quality":0.0715917115,"ml-security":0.1025009358}}
{"text":"As the advent of artificial general intelligence (AGI) progresses at a breathtaking pace, the application of large language models (LLMs) as AI Agents in robotics remains in its nascent stage.","meta":{"url":"http://arxiv.org/abs/2307.10690v1"},"cats":{"new-dataset":0.1207220784,"dev-research":0.2122474799,"prompt-eng":0.4683657338,"data-quality":0.0915434863,"ml-security":0.1081019601}}
{"text":"A significant concern that hampers the seamless integration of these AI Agents into robotics is the unpredictability of the content they generate, a phenomena known as ``hallucination''.","meta":{"url":"http://arxiv.org/abs/2307.10690v1"},"cats":{"new-dataset":0.0170424445,"dev-research":0.3037384566,"prompt-eng":0.4340123889,"data-quality":0.1427377013,"ml-security":0.1229734898}}
{"text":"Drawing inspiration from biological neural systems, we propose a novel, layered architecture for autonomous robotics, bridging AI agent intelligence and robot instinct.","meta":{"url":"http://arxiv.org/abs/2307.10690v1"},"cats":{"new-dataset":0.0825882184,"dev-research":0.257651001,"prompt-eng":0.4321775777,"data-quality":0.0787717188,"ml-security":0.1872199662}}
{"text":"In this context, we define Robot Instinct as the innate or learned set of responses and priorities in an autonomous robotic system that ensures survival-essential tasks, such as safety assurance and obstacle avoidance, are carried out in a timely and effective manner.","meta":{"url":"http://arxiv.org/abs/2307.10690v1"},"cats":{"new-dataset":0.1047960657,"dev-research":0.2959763095,"prompt-eng":0.464001287,"data-quality":0.0755408316,"ml-security":0.2699861844}}
{"text":"This paradigm harmoniously combines the intelligence of LLMs with the instinct of robotic behaviors, contributing to a more safe and versatile autonomous robotic system.","meta":{"url":"http://arxiv.org/abs/2307.10690v1"},"cats":{"new-dataset":0.0344794688,"dev-research":0.212210753,"prompt-eng":0.5243603301,"data-quality":0.0753744461,"ml-security":0.2054335091}}
{"text":"As a case study, we illustrate this paradigm within the context of a mobile robot, demonstrating its potential to significantly enhance autonomous robotics and enabling a future where robots can operate independently and safely across diverse environments.","meta":{"url":"http://arxiv.org/abs/2307.10690v1"},"cats":{"new-dataset":0.0635859693,"dev-research":0.2914345456,"prompt-eng":0.4026360414,"data-quality":0.0628606521,"ml-security":0.1460977434}}
{"text":"We develop an approach called bounded combinatorial reconfiguration for solving combinatorial reconfiguration problems based on Answer Set Programming (ASP).","meta":{"url":"http://arxiv.org/abs/2307.10688v1"},"cats":{"new-dataset":0.3342819466,"dev-research":0.2668109755,"prompt-eng":0.3727981575,"data-quality":0.0645682981,"ml-security":0.0764266915}}
{"text":"The general task is to study the solution spaces of source combinatorial problems and to decide whether or not there are sequences of feasible solutions that have special properties.","meta":{"url":"http://arxiv.org/abs/2307.10688v1"},"cats":{"new-dataset":0.1530151352,"dev-research":0.2632518603,"prompt-eng":0.3854914066,"data-quality":0.0844618302,"ml-security":0.1158815936}}
{"text":"The resulting recongo solver covers all metrics of the solver track in the most recent international competition on combinatorial reconfiguration (CoRe Challenge 2022).","meta":{"url":"http://arxiv.org/abs/2307.10688v1"},"cats":{"new-dataset":0.3974750806,"dev-research":0.2666826593,"prompt-eng":0.3859445073,"data-quality":0.080591367,"ml-security":0.051457172}}
{"text":"recongo ranked first in the shortest metric of the single-engine solvers track.","meta":{"url":"http://arxiv.org/abs/2307.10688v1"},"cats":{"new-dataset":0.076350296,"dev-research":0.2654613549,"prompt-eng":0.3757551661,"data-quality":0.0742734778,"ml-security":0.0627141345}}
{"text":"In this paper, we present the design and implementation of bounded combinatorial reconfiguration, and present an ASP encoding of the independent set reconfiguration problem that is one of the most studied combinatorial reconfiguration problems.","meta":{"url":"http://arxiv.org/abs/2307.10688v1"},"cats":{"new-dataset":0.2445660907,"dev-research":0.2476990555,"prompt-eng":0.3484440024,"data-quality":0.0771393259,"ml-security":0.0722830511}}
{"text":"Finally, we present empirical analysis considering all instances of CoRe Challenge 2022.","meta":{"url":"http://arxiv.org/abs/2307.10688v1"},"cats":{"new-dataset":0.2073277621,"dev-research":0.3667830954,"prompt-eng":0.4381212909,"data-quality":0.1730453305,"ml-security":0.1741453366}}
{"text":"Camouflaged object detection (COD), aiming to segment camouflaged objects which exhibit similar patterns with the background, is a challenging task.","meta":{"url":"http://arxiv.org/abs/2307.10685v1"},"cats":{"new-dataset":0.1405206693,"dev-research":0.2830984985,"prompt-eng":0.4155557158,"data-quality":0.2041872442,"ml-security":0.2163226799}}
{"text":"Most existing works are dedicated to establishing specialized modules to identify camouflaged objects with complete and fine details, while the boundary can not be well located for the lack of object-related semantics.","meta":{"url":"http://arxiv.org/abs/2307.10685v1"},"cats":{"new-dataset":0.0993862368,"dev-research":0.2798517902,"prompt-eng":0.4313842911,"data-quality":0.2809358376,"ml-security":0.171672665}}
{"text":"In this paper, we propose a novel ``pre-train, adapt and detect\" paradigm to detect camouflaged objects.","meta":{"url":"http://arxiv.org/abs/2307.10685v1"},"cats":{"new-dataset":0.1509970626,"dev-research":0.2209491529,"prompt-eng":0.4802634666,"data-quality":0.2217944706,"ml-security":0.2306757714}}
{"text":"By introducing a large pre-trained model, abundant knowledge learned from massive multi-modal data can be directly transferred to COD.","meta":{"url":"http://arxiv.org/abs/2307.10685v1"},"cats":{"new-dataset":0.1211707428,"dev-research":0.2990238694,"prompt-eng":0.4496682818,"data-quality":0.0797404721,"ml-security":0.1169278434}}
{"text":"A lightweight parallel adapter is inserted to adjust the features suitable for the downstream COD task.","meta":{"url":"http://arxiv.org/abs/2307.10685v1"},"cats":{"new-dataset":0.0345206717,"dev-research":0.3849820532,"prompt-eng":0.4177380464,"data-quality":0.0702172756,"ml-security":0.064836697}}
{"text":"Extensive experiments on four challenging benchmark datasets demonstrate that our method outperforms existing state-of-the-art COD models by large margins.","meta":{"url":"http://arxiv.org/abs/2307.10685v1"},"cats":{"new-dataset":0.4319293874,"dev-research":0.3236481937,"prompt-eng":0.3628099348,"data-quality":0.1695305936,"ml-security":0.1486199317}}
{"text":"Moreover, we design a multi-task learning scheme for tuning the adapter to exploit the shareable knowledge across different semantic classes.","meta":{"url":"http://arxiv.org/abs/2307.10685v1"},"cats":{"new-dataset":0.0627263195,"dev-research":0.2699819788,"prompt-eng":0.4215738078,"data-quality":0.1574980085,"ml-security":0.1202721363}}
{"text":"Comprehensive experimental results showed that the generalization ability of our model can be substantially improved with multi-task adapter initialization on source tasks and multi-task adaptation on target tasks.","meta":{"url":"http://arxiv.org/abs/2307.10685v1"},"cats":{"new-dataset":0.0325595194,"dev-research":0.2654589501,"prompt-eng":0.4856425454,"data-quality":0.0932929918,"ml-security":0.0899098642}}
{"text":"Knowledge graphs have proven to be effective for modeling entities and their relationships through the use of ontologies.","meta":{"url":"http://arxiv.org/abs/2307.10680v1"},"cats":{"new-dataset":0.1745835243,"dev-research":0.3370473399,"prompt-eng":0.4129395886,"data-quality":0.1414649955,"ml-security":0.0521262368}}
{"text":"The recent emergence in interest for using knowledge graphs as a form of information modeling has led to their increased adoption in recommender systems.","meta":{"url":"http://arxiv.org/abs/2307.10680v1"},"cats":{"new-dataset":0.0699316643,"dev-research":0.3511033108,"prompt-eng":0.3934248236,"data-quality":0.0802002234,"ml-security":0.0973350318}}
{"text":"By incorporating users and items into the knowledge graph, these systems can better capture the implicit connections between them and provide more accurate recommendations.","meta":{"url":"http://arxiv.org/abs/2307.10680v1"},"cats":{"new-dataset":0.053843961,"dev-research":0.4706319621,"prompt-eng":0.4320760275,"data-quality":0.1498303135,"ml-security":0.0722212462}}
{"text":"In this paper, we investigate and propose the construction of a personalized recommender system via knowledge graphs embedding applied to the vehicle purchase/sale domain.","meta":{"url":"http://arxiv.org/abs/2307.10680v1"},"cats":{"new-dataset":0.161568562,"dev-research":0.3044698659,"prompt-eng":0.3848092727,"data-quality":0.1171511974,"ml-security":0.0808577071}}
{"text":"The results of our experimentation demonstrate the efficacy of the proposed method in providing relevant recommendations that are consistent with individual users.","meta":{"url":"http://arxiv.org/abs/2307.10680v1"},"cats":{"new-dataset":0.0112909756,"dev-research":0.3972072465,"prompt-eng":0.4743216714,"data-quality":0.1639623686,"ml-security":0.0782810169}}
{"text":"We wish to define the limits of a classical classification model based on deep learning when applied to abstract images, which do not represent visually identifiable objects.","meta":{"url":"http://arxiv.org/abs/2307.10677v1"},"cats":{"new-dataset":0.1099376708,"dev-research":0.189702357,"prompt-eng":0.3551647841,"data-quality":0.2310849651,"ml-security":0.3062454668}}
{"text":"QR codes (Quick Response codes) fall into this category of abstract images: one bit corresponding to one encoded character, QR codes were not designed to be decoded manually.","meta":{"url":"http://arxiv.org/abs/2307.10677v1"},"cats":{"new-dataset":0.0806902094,"dev-research":0.2940327752,"prompt-eng":0.4507389414,"data-quality":0.1597240232,"ml-security":0.0781400538}}
{"text":"To understand the limitations of a deep learning-based model for abstract image classification, we train an image classification model on QR codes generated from information obtained when reading a health pass.","meta":{"url":"http://arxiv.org/abs/2307.10677v1"},"cats":{"new-dataset":0.2117397246,"dev-research":0.2744816435,"prompt-eng":0.4007440158,"data-quality":0.1718639293,"ml-security":0.220010001}}
{"text":"We compare a classification model with a classical (deterministic) decoding method in the presence of noise.","meta":{"url":"http://arxiv.org/abs/2307.10677v1"},"cats":{"new-dataset":0.0648466071,"dev-research":0.199185164,"prompt-eng":0.4157407229,"data-quality":0.4385654009,"ml-security":0.2805285567}}
{"text":"This study allows us to conclude that a model based on deep learning can be relevant for the understanding of abstract images.","meta":{"url":"http://arxiv.org/abs/2307.10677v1"},"cats":{"new-dataset":0.0819396876,"dev-research":0.3056376878,"prompt-eng":0.4039220254,"data-quality":0.1843388514,"ml-security":0.1147877895}}
{"text":"Relational information between different types of entities is often modelled by a multilayer network (MLN) -- a network with subnetworks represented by layers.","meta":{"url":"http://arxiv.org/abs/2307.10674v1"},"cats":{"new-dataset":0.0537239125,"dev-research":0.2793847562,"prompt-eng":0.3864914485,"data-quality":0.1209618212,"ml-security":0.1326549629}}
{"text":"The layers of an MLN can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question.","meta":{"url":"http://arxiv.org/abs/2307.10674v1"},"cats":{"new-dataset":0.042514421,"dev-research":0.2706290483,"prompt-eng":0.3731338486,"data-quality":0.1103285327,"ml-security":0.1381757656}}
{"text":"Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis.","meta":{"url":"http://arxiv.org/abs/2307.10674v1"},"cats":{"new-dataset":0.0147137037,"dev-research":0.2602973467,"prompt-eng":0.4018376834,"data-quality":0.1723892508,"ml-security":0.3074537659}}
{"text":"Additionally, layer arrangements with a dimensionality beyond 2D, which are common in this scenario, motivate the use of stereoscopic displays.","meta":{"url":"http://arxiv.org/abs/2307.10674v1"},"cats":{"new-dataset":0.0312915249,"dev-research":0.3108215261,"prompt-eng":0.382277725,"data-quality":0.0482823649,"ml-security":0.0561285584}}
{"text":"We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements.","meta":{"url":"http://arxiv.org/abs/2307.10674v1"},"cats":{"new-dataset":0.1554195976,"dev-research":0.3018896182,"prompt-eng":0.3521867225,"data-quality":0.0565653467,"ml-security":0.0609407829}}
{"text":"The study employs six analysis tasks that cover the spectrum of an MLN task taxonomy, from path finding and pattern identification to comparisons between and across layers.","meta":{"url":"http://arxiv.org/abs/2307.10674v1"},"cats":{"new-dataset":0.0556544127,"dev-research":0.3325941611,"prompt-eng":0.4387705383,"data-quality":0.102944418,"ml-security":0.1028855651}}
{"text":"We found no clear overall winner.","meta":{"url":"http://arxiv.org/abs/2307.10674v1"},"cats":{"new-dataset":0.0448752701,"dev-research":0.2308832419,"prompt-eng":0.3314613437,"data-quality":0.2505943651,"ml-security":0.0696190859}}
{"text":"However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs.","meta":{"url":"http://arxiv.org/abs/2307.10674v1"},"cats":{"new-dataset":0.1310307982,"dev-research":0.2873184262,"prompt-eng":0.3881518452,"data-quality":0.0625520339,"ml-security":0.0619836191}}
{"text":"We study a natural geometric variant of the classic Knapsack problem called 2D-Knapsack: we are given a set of axis-parallel rectangles and a rectangular bounding box, and the goal is to pack as many of these rectangles inside the box without overlap.","meta":{"url":"http://arxiv.org/abs/2307.10672v1"},"cats":{"new-dataset":0.2070910672,"dev-research":0.2351509155,"prompt-eng":0.3305188263,"data-quality":0.0709167949,"ml-security":0.1441717869}}
{"text":"Naturally, this problem is NP-complete.","meta":{"url":"http://arxiv.org/abs/2307.10672v1"},"cats":{"new-dataset":0.2610758872,"dev-research":0.2513480702,"prompt-eng":0.3704916158,"data-quality":0.1790838509,"ml-security":0.0810279045}}
{"text":"Recently, Grandoni et al.","meta":{"url":"http://arxiv.org/abs/2307.10672v1"},"cats":{"new-dataset":0.3053670115,"dev-research":0.2351186006,"prompt-eng":0.3523739603,"data-quality":0.0992359513,"ml-security":0.0523561299}}
{"text":"[ESA'19] showed that it is also W[1]-hard when parameterized by the size $k$ of the sought packing, and they presented a parameterized approximation scheme (PAS) for the variant where we are allowed to rotate the rectangles by 90{\\textdegree} before packing them into the box.","meta":{"url":"http://arxiv.org/abs/2307.10672v1"},"cats":{"new-dataset":0.05731897,"dev-research":0.2423689612,"prompt-eng":0.3784207991,"data-quality":0.0930678887,"ml-security":0.0606236123}}
{"text":"Obtaining a PAS for the original 2D-Knapsack problem, without rotation, appears to be a challenging open question.","meta":{"url":"http://arxiv.org/abs/2307.10672v1"},"cats":{"new-dataset":0.0768488501,"dev-research":0.1804853608,"prompt-eng":0.3674028483,"data-quality":0.0754891726,"ml-security":0.0711566126}}
{"text":"In this work, we make progress towards this goal by showing a PAS under the following assumptions: - both the box and all the input rectangles have integral, polynomially bounded sidelengths; - every input rectangle is wide -- its width is greater than its height; and - the aspect ratio of the box is bounded by a constant.","meta":{"url":"http://arxiv.org/abs/2307.10672v1"},"cats":{"new-dataset":0.1236271144,"dev-research":0.2408806203,"prompt-eng":0.3645604877,"data-quality":0.0743859746,"ml-security":0.1064687734}}
{"text":"Our approximation scheme relies on a mix of various parameterized and approximation techniques, including color coding, rounding, and searching for a structured near-optimum packing using dynamic programming.","meta":{"url":"http://arxiv.org/abs/2307.10672v1"},"cats":{"new-dataset":0.06496146,"dev-research":0.2352091023,"prompt-eng":0.397769555,"data-quality":0.1144332541,"ml-security":0.0582745348}}
{"text":"Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, parsing, NER) and relatively simple classification tasks such as sentiment classification or article classification from a single news source.","meta":{"url":"http://arxiv.org/abs/2307.10666v1"},"cats":{"new-dataset":0.0631231652,"dev-research":0.2773432867,"prompt-eng":0.4500852263,"data-quality":0.3527258489,"ml-security":0.1171917962}}
{"text":"As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datasets, composed of news articles from various sources spanning over twenty years, which allows a more rigorous evaluation of such models.","meta":{"url":"http://arxiv.org/abs/2307.10666v1"},"cats":{"new-dataset":0.7126943532,"dev-research":0.2785415308,"prompt-eng":0.326470418,"data-quality":0.2700448892,"ml-security":0.1701676023}}
{"text":"We define four classification tasks: news source, news category, inferred author's gender, and day of the week.","meta":{"url":"http://arxiv.org/abs/2307.10666v1"},"cats":{"new-dataset":0.1335377987,"dev-research":0.2927819089,"prompt-eng":0.3994599706,"data-quality":0.2473205562,"ml-security":0.0831154228}}
{"text":"To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong machine-learning baselines built upon pre-trained transformer models.","meta":{"url":"http://arxiv.org/abs/2307.10666v1"},"cats":{"new-dataset":0.0350843514,"dev-research":0.3345038879,"prompt-eng":0.4858675404,"data-quality":0.1674276336,"ml-security":0.2028997689}}
{"text":"Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models.","meta":{"url":"http://arxiv.org/abs/2307.10666v1"},"cats":{"new-dataset":0.2016535195,"dev-research":0.2572515783,"prompt-eng":0.5046331445,"data-quality":0.1799354606,"ml-security":0.092559718}}
{"text":"Neural Radiance Field (NeRF) is a promising approach for synthesizing novel views, given a set of images and the corresponding camera poses of a scene.","meta":{"url":"http://arxiv.org/abs/2307.10664v1"},"cats":{"new-dataset":0.2438843786,"dev-research":0.2434477703,"prompt-eng":0.3728499599,"data-quality":0.1370553312,"ml-security":0.1276980077}}
{"text":"However, images photographed from a low-light scene can hardly be used to train a NeRF model to produce high-quality results, due to their low pixel intensities, heavy noise, and color distortion.","meta":{"url":"http://arxiv.org/abs/2307.10664v1"},"cats":{"new-dataset":0.0692115594,"dev-research":0.2472031032,"prompt-eng":0.3974930441,"data-quality":0.3116377278,"ml-security":0.1342434801}}
{"text":"Combining existing low-light image enhancement methods with NeRF methods also does not work well due to the view inconsistency caused by the individual 2D enhancement process.","meta":{"url":"http://arxiv.org/abs/2307.10664v1"},"cats":{"new-dataset":0.0155570582,"dev-research":0.258137753,"prompt-eng":0.3344062794,"data-quality":0.1616474954,"ml-security":0.0620116326}}
{"text":"In this paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to enhance the scene representation and synthesize normal-light novel views directly from sRGB low-light images in an unsupervised manner.","meta":{"url":"http://arxiv.org/abs/2307.10664v1"},"cats":{"new-dataset":0.1335681159,"dev-research":0.2242955289,"prompt-eng":0.3932339612,"data-quality":0.1779788005,"ml-security":0.0535909956}}
{"text":"The core of our approach is a decomposition of radiance field learning, which allows us to enhance the illumination, reduce noise and correct the distorted colors jointly with the NeRF optimization process.","meta":{"url":"http://arxiv.org/abs/2307.10664v1"},"cats":{"new-dataset":0.0725785002,"dev-research":0.2210082973,"prompt-eng":0.3835740435,"data-quality":0.1870343514,"ml-security":0.1471444747}}
{"text":"Our method is able to produce novel view images with proper lighting and vivid colors and details, given a collection of camera-finished low dynamic range (8-bits/channel) images from a low-light scene.","meta":{"url":"http://arxiv.org/abs/2307.10664v1"},"cats":{"new-dataset":0.448352874,"dev-research":0.2422857592,"prompt-eng":0.3701844182,"data-quality":0.091208838,"ml-security":0.0420155885}}
{"text":"Experiments demonstrate that our method outperforms existing low-light enhancement methods and NeRF methods.","meta":{"url":"http://arxiv.org/abs/2307.10664v1"},"cats":{"new-dataset":0.0174266556,"dev-research":0.264143253,"prompt-eng":0.3837564039,"data-quality":0.1673811788,"ml-security":0.0804156534}}
{"text":"Modern scientific workflows require hybrid infrastructures combining numerous decentralized resources on the IoT/Edge interconnected to Cloud/HPC systems (aka the Computing Continuum) to enable their optimized execution.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.072792985,"dev-research":0.2877068982,"prompt-eng":0.3633539741,"data-quality":0.0484871638,"ml-security":0.0598075842}}
{"text":"Understanding and optimizing the performance of such complex Edge-to-Cloud workflows is challenging.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.0681929198,"dev-research":0.3191096182,"prompt-eng":0.338924353,"data-quality":0.0516051683,"ml-security":0.0669246036}}
{"text":"Capturing the provenance of key performance indicators, with their related data and processes, may assist in understanding and optimizing workflow executions.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.1383858494,"dev-research":0.4791890515,"prompt-eng":0.4012768889,"data-quality":0.0933854706,"ml-security":0.0613384546}}
{"text":"However, the capture overhead can be prohibitive, particularly in resource-constrained devices, such as the ones on the IoT/Edge.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.0202587665,"dev-research":0.2868942604,"prompt-eng":0.3615208798,"data-quality":0.042364198,"ml-security":0.169753748}}
{"text":"To address this challenge, based on a performance analysis of existing systems, we propose ProvLight, a tool to enable efficient provenance capture on the IoT/Edge.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.2490736354,"dev-research":0.3659477368,"prompt-eng":0.3795336072,"data-quality":0.1063476943,"ml-security":0.1201834491}}
{"text":"We leverage simplified data models, data compression and grouping, and lightweight transmission protocols to reduce overheads.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.1084848201,"dev-research":0.2745709263,"prompt-eng":0.383720618,"data-quality":0.0894598848,"ml-security":0.1881874886}}
{"text":"We further integrate ProvLight into the E2Clab framework to enable workflow provenance capture across the Edge-to-Cloud Continuum.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.2812649117,"dev-research":0.3519926349,"prompt-eng":0.3690905853,"data-quality":0.1125627252,"ml-security":0.080603666}}
{"text":"This integration makes E2Clab a promising platform for the performance optimization of applications through reproducible experiments.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.0335974949,"dev-research":0.3718230796,"prompt-eng":0.4205516326,"data-quality":0.0839770138,"ml-security":0.0825614676}}
{"text":"We validate ProvLight at a large scale with synthetic workloads on 64 real-life IoT/Edge devices in the FIT IoT LAB testbed.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.1089576817,"dev-research":0.35474812,"prompt-eng":0.4114780437,"data-quality":0.0874267365,"ml-security":0.1065437858}}
{"text":"Evaluations show that ProvLight outperforms state-of-the-art systems like ProvLake and DfAnalyzer in resource-constrained devices.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.0228910661,"dev-research":0.3137478266,"prompt-eng":0.4547956233,"data-quality":0.1036191812,"ml-security":0.0709446453}}
{"text":"ProvLight is 26 -- 37x faster to capture and transmit provenance data; uses 5 -- 7x less CPU; 2x less memory; transmits 2x less data; and consumes 2 -- 2.5x less energy.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.1048662576,"dev-research":0.3397081685,"prompt-eng":0.3723792319,"data-quality":0.077753613,"ml-security":0.0583807694}}
{"text":"ProvLight and E2Clab are available as open-source tools.","meta":{"url":"http://arxiv.org/abs/2307.10658v1"},"cats":{"new-dataset":0.2319258815,"dev-research":0.3925430621,"prompt-eng":0.417915154,"data-quality":0.0816557985,"ml-security":0.0556924201}}
{"text":"Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.0968604397,"dev-research":0.2368222011,"prompt-eng":0.3576512342,"data-quality":0.1261585944,"ml-security":0.3841863434}}
{"text":"Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.2034845831,"dev-research":0.2772516392,"prompt-eng":0.3464701271,"data-quality":0.1330949101,"ml-security":0.4773713237}}
{"text":"This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.0327775022,"dev-research":0.3682262846,"prompt-eng":0.3636177418,"data-quality":0.0581257228,"ml-security":0.3426394259}}
{"text":"Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.1618242578,"dev-research":0.3176180307,"prompt-eng":0.3810351059,"data-quality":0.0849792408,"ml-security":0.0546294607}}
{"text":"However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.037369971,"dev-research":0.2890898647,"prompt-eng":0.4326167432,"data-quality":0.1588642059,"ml-security":0.2225939044}}
{"text":"In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.1657788911,"dev-research":0.2894291762,"prompt-eng":0.406071808,"data-quality":0.1341896832,"ml-security":0.331493251}}
{"text":"This survey differs from previous ones due to four distinct contributions.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.2907245815,"dev-research":0.2553294823,"prompt-eng":0.3681998366,"data-quality":0.1051900181,"ml-security":0.0535193374}}
{"text":"First, we present a new taxonomy of FL methods in terms of the sharing methods, which includes three categories of shared information: model sharing, synthetic data sharing, and knowledge sharing.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.1717112625,"dev-research":0.3348026927,"prompt-eng":0.3517322907,"data-quality":0.1034638808,"ml-security":0.1542166279}}
{"text":"Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms that provide certain privacy guarantees.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.0936544852,"dev-research":0.3344257165,"prompt-eng":0.351530801,"data-quality":0.1542176628,"ml-security":0.7951631533}}
{"text":"Third, we conduct extensive experiments to compare the performance and communication overhead of various sharing methods in FL.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.0694224637,"dev-research":0.301400724,"prompt-eng":0.3721851541,"data-quality":0.070034912,"ml-security":0.0993632348}}
{"text":"Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.0514823083,"dev-research":0.2790624184,"prompt-eng":0.402437472,"data-quality":0.1977948809,"ml-security":0.8494345102}}
{"text":"Finally, we discuss potential deficiencies in current methods and outline future directions for improvement.","meta":{"url":"http://arxiv.org/abs/2307.10655v1"},"cats":{"new-dataset":0.0261929155,"dev-research":0.4204259166,"prompt-eng":0.380599809,"data-quality":0.2722678991,"ml-security":0.0622950675}}
{"text":"A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP).","meta":{"url":"http://arxiv.org/abs/2307.10654v1"},"cats":{"new-dataset":0.014109338,"dev-research":0.346670761,"prompt-eng":0.4549912974,"data-quality":0.1346594693,"ml-security":0.2873819186}}
{"text":"The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP).","meta":{"url":"http://arxiv.org/abs/2307.10654v1"},"cats":{"new-dataset":0.0182452182,"dev-research":0.2306981179,"prompt-eng":0.430473253,"data-quality":0.0916168187,"ml-security":0.0954819285}}
{"text":"Except for tree-based methods, usually the unconditional version is used (for computational reasons).","meta":{"url":"http://arxiv.org/abs/2307.10654v1"},"cats":{"new-dataset":0.0124891004,"dev-research":0.3083747813,"prompt-eng":0.3394210657,"data-quality":0.1316546707,"ml-security":0.0725879149}}
{"text":"We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components.","meta":{"url":"http://arxiv.org/abs/2307.10654v1"},"cats":{"new-dataset":0.0605476712,"dev-research":0.3104319936,"prompt-eng":0.4087296952,"data-quality":0.1374211172,"ml-security":0.2552830684}}
{"text":"This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.","meta":{"url":"http://arxiv.org/abs/2307.10654v1"},"cats":{"new-dataset":0.0995217515,"dev-research":0.3306180532,"prompt-eng":0.3867806226,"data-quality":0.1098530246,"ml-security":0.1917200875}}
{"text":"Time series anomaly detection is crucial for industrial monitoring services that handle a large volume of data, aiming to ensure reliability and optimize system performance.","meta":{"url":"http://arxiv.org/abs/2307.10653v1"},"cats":{"new-dataset":0.1271696767,"dev-research":0.2934536344,"prompt-eng":0.3736287394,"data-quality":0.2328173837,"ml-security":0.2653460839}}
{"text":"Existing methods often require extensive labeled resources and manual parameter selection, highlighting the need for automation.","meta":{"url":"http://arxiv.org/abs/2307.10653v1"},"cats":{"new-dataset":0.0278097138,"dev-research":0.4334588337,"prompt-eng":0.5302612144,"data-quality":0.1429413247,"ml-security":0.0639614204}}
{"text":"This paper proposes a comprehensive framework for automatic parameter optimization in time series anomaly detection models.","meta":{"url":"http://arxiv.org/abs/2307.10653v1"},"cats":{"new-dataset":0.0766624741,"dev-research":0.2573255976,"prompt-eng":0.434474058,"data-quality":0.2098173244,"ml-security":0.3688654852}}
{"text":"The framework introduces three optimization targets: prediction score, shape score, and sensitivity score, which can be easily adapted to different model backbones without prior knowledge or manual labeling efforts.","meta":{"url":"http://arxiv.org/abs/2307.10653v1"},"cats":{"new-dataset":0.0337152908,"dev-research":0.2721368415,"prompt-eng":0.4898023612,"data-quality":0.1303313864,"ml-security":0.1532303086}}
{"text":"The proposed framework has been successfully applied online for over six months, serving more than 50,000 time series every minute.","meta":{"url":"http://arxiv.org/abs/2307.10653v1"},"cats":{"new-dataset":0.1429336613,"dev-research":0.2287631037,"prompt-eng":0.3273034197,"data-quality":0.0449249759,"ml-security":0.070535874}}
{"text":"It simplifies the user's experience by requiring only an expected sensitive value, offering a user-friendly interface, and achieving desired detection results.","meta":{"url":"http://arxiv.org/abs/2307.10653v1"},"cats":{"new-dataset":0.0139729556,"dev-research":0.4165519808,"prompt-eng":0.4746236187,"data-quality":0.1969527381,"ml-security":0.3867262327}}
{"text":"Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness of the proposed framework.","meta":{"url":"http://arxiv.org/abs/2307.10653v1"},"cats":{"new-dataset":0.4167590527,"dev-research":0.3023807602,"prompt-eng":0.3331011647,"data-quality":0.1919492484,"ml-security":0.1876798078}}
{"text":"As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years.","meta":{"url":"http://arxiv.org/abs/2307.10652v1"},"cats":{"new-dataset":0.1606168808,"dev-research":0.3659575796,"prompt-eng":0.4188043624,"data-quality":0.2406470382,"ml-security":0.0590819585}}
{"text":"Given the increasing amount of research work in this area, several NLP-related approaches have been surveyed in the research community.","meta":{"url":"http://arxiv.org/abs/2307.10652v1"},"cats":{"new-dataset":0.1200212548,"dev-research":0.3362174284,"prompt-eng":0.3913081029,"data-quality":0.2312900395,"ml-security":0.0413214042}}
{"text":"However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent to this day.","meta":{"url":"http://arxiv.org/abs/2307.10652v1"},"cats":{"new-dataset":0.0478926307,"dev-research":0.2939260826,"prompt-eng":0.3536166361,"data-quality":0.1693551011,"ml-security":0.0709265045}}
{"text":"Contributing to closing this gap, we have systematically classified and analyzed research papers included in the ACL Anthology.","meta":{"url":"http://arxiv.org/abs/2307.10652v1"},"cats":{"new-dataset":0.1688527727,"dev-research":0.2850063862,"prompt-eng":0.3706493395,"data-quality":0.1499491398,"ml-security":0.0837457605}}
{"text":"As a result, we present a structured overview of the research landscape, provide a taxonomy of fields-of-study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.","meta":{"url":"http://arxiv.org/abs/2307.10652v1"},"cats":{"new-dataset":0.2895572301,"dev-research":0.3636097888,"prompt-eng":0.4194707629,"data-quality":0.1854145017,"ml-security":0.0363707233}}
{"text":"Session-based recommendation techniques aim to capture dynamic user behavior by analyzing past interactions.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.0291432762,"dev-research":0.3435559448,"prompt-eng":0.4458974707,"data-quality":0.0991881981,"ml-security":0.1582546946}}
{"text":"However, existing methods heavily rely on historical item ID sequences to extract user preferences, leading to challenges such as popular bias and cold-start problems.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.1272562985,"dev-research":0.3226716437,"prompt-eng":0.4413219695,"data-quality":0.1502297439,"ml-security":0.1562137954}}
{"text":"In this paper, we propose a hybrid multimodal approach for session-based recommendation to address these challenges.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.099039314,"dev-research":0.2914896514,"prompt-eng":0.4295357802,"data-quality":0.1098997974,"ml-security":0.057563762}}
{"text":"Our approach combines different modalities, including textual content and item IDs, leveraging the complementary nature of these modalities using CatBoost.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.0708322031,"dev-research":0.2763959243,"prompt-eng":0.4268979159,"data-quality":0.3246601341,"ml-security":0.0663831614}}
{"text":"To learn universal item representations, we design a language representation-based item retrieval architecture that extracts features from the textual content utilizing pre-trained language models.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.1454508984,"dev-research":0.2821504262,"prompt-eng":0.4644286285,"data-quality":0.2232146225,"ml-security":0.0932058832}}
{"text":"Furthermore, we introduce a novel Decoupled Contrastive Learning method to enhance the effectiveness of the language representation.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.1071978589,"dev-research":0.2735236791,"prompt-eng":0.4090713096,"data-quality":0.2236457807,"ml-security":0.0925501423}}
{"text":"This technique decouples the sequence representation and item representation space, facilitating bidirectional alignment through dual-queue contrastive learning.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.1604320708,"dev-research":0.2345989744,"prompt-eng":0.3773456908,"data-quality":0.128151686,"ml-security":0.0526241135}}
{"text":"Simultaneously, the momentum queue provides a large number of negative samples, effectively enhancing the effectiveness of contrastive learning.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.045593022,"dev-research":0.2077815305,"prompt-eng":0.4038963753,"data-quality":0.0857153179,"ml-security":0.087606209}}
{"text":"Our approach yielded competitive results, securing a 5th place ranking in KDD CUP 2023 Task 1.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.0597242869,"dev-research":0.2934422657,"prompt-eng":0.4357950599,"data-quality":0.1469734063,"ml-security":0.1087599317}}
{"text":"We have released the source code and pre-trained models associated with this work.","meta":{"url":"http://arxiv.org/abs/2307.10650v1"},"cats":{"new-dataset":0.4439159349,"dev-research":0.3100479474,"prompt-eng":0.4723498137,"data-quality":0.128459935,"ml-security":0.1300507769}}
{"text":"With the emergence of new application areas, such as cyber-physical systems and human-in-the-loop applications, there is a need to guarantee a certain level of end-to-end network latency with extremely high reliability, e.g., 99.999%.","meta":{"url":"http://arxiv.org/abs/2307.10648v1"},"cats":{"new-dataset":0.0298500144,"dev-research":0.3388689128,"prompt-eng":0.3837410229,"data-quality":0.1508009294,"ml-security":0.1265705512}}
{"text":"While mechanisms specified under IEEE 802.1as time-sensitive networking (TSN) can be used to achieve these requirements for switched Ethernet networks, implementing TSN mechanisms in wireless networks is challenging due to their stochastic nature.","meta":{"url":"http://arxiv.org/abs/2307.10648v1"},"cats":{"new-dataset":0.0306044963,"dev-research":0.2690867885,"prompt-eng":0.3901593158,"data-quality":0.0745701778,"ml-security":0.1285573045}}
{"text":"To conform the wireless link to a reliability level of 99.999%, the behavior of extremely rare outliers in the latency probability distribution, or the tail of the distribution, must be analyzed and controlled.","meta":{"url":"http://arxiv.org/abs/2307.10648v1"},"cats":{"new-dataset":0.0325569553,"dev-research":0.2671778428,"prompt-eng":0.4578553275,"data-quality":0.2595097421,"ml-security":0.1446423942}}
{"text":"This work proposes predicting the tail of the latency distribution using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to estimate the likelihood of rare latencies conditioned on the network parameters, which can be used to make more informed decisions in wireless transmission.","meta":{"url":"http://arxiv.org/abs/2307.10648v1"},"cats":{"new-dataset":0.0689291121,"dev-research":0.2267393216,"prompt-eng":0.4523237894,"data-quality":0.1231418822,"ml-security":0.1350844817}}
{"text":"Actual latency measurements of IEEE 802.11g (WiFi), commercial private and a software-defined 5G network are used to benchmark the proposed approaches and evaluate their sensitivities concerning the tail probabilities.","meta":{"url":"http://arxiv.org/abs/2307.10648v1"},"cats":{"new-dataset":0.0632693735,"dev-research":0.3001224733,"prompt-eng":0.3949844238,"data-quality":0.1000425911,"ml-security":0.0563753893}}
{"text":"Non-Terrestrial Networks (NTNs) can be used to provide ubiquitous 5G and beyond services to un(der)served areas.","meta":{"url":"http://arxiv.org/abs/2307.10646v1"},"cats":{"new-dataset":0.169482403,"dev-research":0.2485579131,"prompt-eng":0.3296333093,"data-quality":0.0852407316,"ml-security":0.098864158}}
{"text":"To ensure reliable communication in such networks, packet duplication (PD) through multi-connectivity is a promising solution.","meta":{"url":"http://arxiv.org/abs/2307.10646v1"},"cats":{"new-dataset":0.0545509572,"dev-research":0.2802323856,"prompt-eng":0.3456820951,"data-quality":0.1860438038,"ml-security":0.1200170441}}
{"text":"However, the existing PD schemes developed for terrestrial environments may not be reactive enough for the NTN environment where propagation delays are significantly longer.","meta":{"url":"http://arxiv.org/abs/2307.10646v1"},"cats":{"new-dataset":0.043677718,"dev-research":0.2239867038,"prompt-eng":0.3723534073,"data-quality":0.0533713438,"ml-security":0.1205463145}}
{"text":"This paper proposes a dynamic PD activation scheme for NTNs based on hybrid automatic repeat request feedback.","meta":{"url":"http://arxiv.org/abs/2307.10646v1"},"cats":{"new-dataset":0.0460361097,"dev-research":0.2423172194,"prompt-eng":0.440018441,"data-quality":0.1605099187,"ml-security":0.1396203423}}
{"text":"The scheme aims to reduce the number of duplicated packets while maintaining high reliability.","meta":{"url":"http://arxiv.org/abs/2307.10646v1"},"cats":{"new-dataset":0.0542140771,"dev-research":0.2852490822,"prompt-eng":0.3783255355,"data-quality":0.1367796472,"ml-security":0.1327854531}}
{"text":"To evaluate the proposed scheme, simulations are conducted in a scenario with two transparent payload lowearth orbit satellites.","meta":{"url":"http://arxiv.org/abs/2307.10646v1"},"cats":{"new-dataset":0.0842490919,"dev-research":0.1646706861,"prompt-eng":0.4202134014,"data-quality":0.059454921,"ml-security":0.0837044111}}
{"text":"The results show a significant reduction of 87.2% in the number of duplicated packets compared to blind duplication, with only marginal compromise in reliability.","meta":{"url":"http://arxiv.org/abs/2307.10646v1"},"cats":{"new-dataset":0.1445468578,"dev-research":0.314634033,"prompt-eng":0.3599683668,"data-quality":0.2110684643,"ml-security":0.1291785857}}
{"text":"Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few.","meta":{"url":"http://arxiv.org/abs/2307.10644v1"},"cats":{"new-dataset":0.3513311892,"dev-research":0.1821176083,"prompt-eng":0.3574387561,"data-quality":0.0893866906,"ml-security":0.1723944356}}
{"text":"In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them.","meta":{"url":"http://arxiv.org/abs/2307.10644v1"},"cats":{"new-dataset":0.0616184848,"dev-research":0.2624704337,"prompt-eng":0.3449178298,"data-quality":0.1388552056,"ml-security":0.0882052804}}
{"text":"The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases.","meta":{"url":"http://arxiv.org/abs/2307.10644v1"},"cats":{"new-dataset":0.0685650321,"dev-research":0.1959285889,"prompt-eng":0.3698391208,"data-quality":0.1566558124,"ml-security":0.09721449}}
{"text":"In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions.","meta":{"url":"http://arxiv.org/abs/2307.10644v1"},"cats":{"new-dataset":0.1412149179,"dev-research":0.1937645805,"prompt-eng":0.3951109362,"data-quality":0.1512256381,"ml-security":0.0815974126}}
{"text":"Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of centered normal distributions.","meta":{"url":"http://arxiv.org/abs/2307.10644v1"},"cats":{"new-dataset":0.1382385774,"dev-research":0.2604956192,"prompt-eng":0.3693844757,"data-quality":0.1097397225,"ml-security":0.1061856817}}
{"text":"We show that the projective Hilbert distance on the cone yields a metric on the embedded normal submanifold and we pullback that cone distance with its associated straight line Hilbert cone geodesics to obtain a distance and smooth paths between normal distributions.","meta":{"url":"http://arxiv.org/abs/2307.10644v1"},"cats":{"new-dataset":0.1234353484,"dev-research":0.2460665143,"prompt-eng":0.4007051431,"data-quality":0.1169418344,"ml-security":0.065377341}}
{"text":"Compared to the Fisher-Rao distance approximation, the pullback Hilbert cone distance is computationally light since it requires to compute only the extreme minimal and maximal eigenvalues of matrices.","meta":{"url":"http://arxiv.org/abs/2307.10644v1"},"cats":{"new-dataset":0.0242644443,"dev-research":0.2134208683,"prompt-eng":0.3838696155,"data-quality":0.0700838362,"ml-security":0.0584076118}}
{"text":"Finally, we show how to use those distances in clustering tasks.","meta":{"url":"http://arxiv.org/abs/2307.10644v1"},"cats":{"new-dataset":0.1922292035,"dev-research":0.2818090327,"prompt-eng":0.4173459866,"data-quality":0.1547146347,"ml-security":0.0415142013}}
{"text":"The widespread use of face retouching filters on short-video platforms has raised concerns about the authenticity of digital appearances and the impact of deceptive advertising.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.0413901298,"dev-research":0.3183845921,"prompt-eng":0.3769849728,"data-quality":0.2697132759,"ml-security":0.2954164847}}
{"text":"To address these issues, there is a pressing need to develop advanced face retouching techniques.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.0127298127,"dev-research":0.3510789773,"prompt-eng":0.404938823,"data-quality":0.0781921038,"ml-security":0.1095181504}}
{"text":"However, the lack of large-scale and fine-grained face retouching datasets has been a major obstacle to progress in this field.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.2357348723,"dev-research":0.2570601308,"prompt-eng":0.3651555735,"data-quality":0.0870251138,"ml-security":0.139739031}}
{"text":"In this paper, we introduce RetouchingFFHQ, a large-scale and fine-grained face retouching dataset that contains over half a million conditionally-retouched images.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.5744409631,"dev-research":0.2326247851,"prompt-eng":0.3934115524,"data-quality":0.0947407006,"ml-security":0.142297753}}
{"text":"RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customization.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.5037747938,"dev-research":0.2899637158,"prompt-eng":0.3569328912,"data-quality":0.1240636293,"ml-security":0.0662700656}}
{"text":"By including four typical types of face retouching operations and different retouching levels, we extend the binary face retouching detection into a fine-grained, multi-retouching type, and multi-retouching level estimation problem.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.04866998,"dev-research":0.2333667737,"prompt-eng":0.4388604344,"data-quality":0.1050012915,"ml-security":0.1184603707}}
{"text":"Additionally, we propose a Multi-granularity Attention Module (MAM) as a plugin for CNN backbones for enhanced cross-scale representation learning.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.1229120942,"dev-research":0.2197087699,"prompt-eng":0.4231632976,"data-quality":0.1058736962,"ml-security":0.1036910692}}
{"text":"Extensive experiments using different baselines as well as our proposed method on RetouchingFFHQ show decent performance on face retouching detection.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.1010286848,"dev-research":0.2471475698,"prompt-eng":0.3927784881,"data-quality":0.0988530613,"ml-security":0.090290453}}
{"text":"With the proposed new dataset, we believe there is great potential for future work to tackle the challenging problem of real-world fine-grained face retouching detection.","meta":{"url":"http://arxiv.org/abs/2307.10642v1"},"cats":{"new-dataset":0.4331861362,"dev-research":0.2455713804,"prompt-eng":0.3777953555,"data-quality":0.1809591068,"ml-security":0.2333089814}}
{"text":"In today's era of information explosion, more users are becoming more reliant upon recommender systems to have better advice, suggestions, or inspire them.","meta":{"url":"http://arxiv.org/abs/2307.10639v1"},"cats":{"new-dataset":0.0337547254,"dev-research":0.4351962395,"prompt-eng":0.4365148424,"data-quality":0.0943787448,"ml-security":0.1147743601}}
{"text":"The measure of the semantic relatedness or likeness between terms, words, or text data plays an important role in different applications dealing with textual data, as in a recommender system.","meta":{"url":"http://arxiv.org/abs/2307.10639v1"},"cats":{"new-dataset":0.0551375297,"dev-research":0.3614330863,"prompt-eng":0.396012578,"data-quality":0.2604634441,"ml-security":0.063355804}}
{"text":"Over the past few years, many ontologies have been developed and used as a form of structured representation of knowledge bases for information systems.","meta":{"url":"http://arxiv.org/abs/2307.10639v1"},"cats":{"new-dataset":0.2553876725,"dev-research":0.3616236645,"prompt-eng":0.4106903305,"data-quality":0.0913812955,"ml-security":0.0617971674}}
{"text":"The measure of semantic similarity from ontology has developed by several methods.","meta":{"url":"http://arxiv.org/abs/2307.10639v1"},"cats":{"new-dataset":0.0947543397,"dev-research":0.3190051591,"prompt-eng":0.3985034418,"data-quality":0.2053783698,"ml-security":0.0315852595}}
{"text":"In this paper, we propose and carry on an approach for the improvement of semantic similarity calculations within a recommender system based-on RDF graphs.","meta":{"url":"http://arxiv.org/abs/2307.10639v1"},"cats":{"new-dataset":0.1139011397,"dev-research":0.326902145,"prompt-eng":0.375215862,"data-quality":0.1693970815,"ml-security":0.048895428}}
{"text":"Neural network quantization aims to accelerate and trim full-precision neural network models by using low bit approximations.","meta":{"url":"http://arxiv.org/abs/2307.10638v1"},"cats":{"new-dataset":0.0234614875,"dev-research":0.2692537714,"prompt-eng":0.3458991919,"data-quality":0.1663681442,"ml-security":0.2351880869}}
{"text":"Methods adopting the quantization aware training (QAT) paradigm have recently seen a rapid growth, but are often conceptually complicated.","meta":{"url":"http://arxiv.org/abs/2307.10638v1"},"cats":{"new-dataset":0.0206875385,"dev-research":0.2682857016,"prompt-eng":0.4194048317,"data-quality":0.1752027703,"ml-security":0.1925025779}}
{"text":"This paper proposes a novel and highly effective QAT method, quantized feature distillation (QFD).","meta":{"url":"http://arxiv.org/abs/2307.10638v1"},"cats":{"new-dataset":0.0155144607,"dev-research":0.2707767774,"prompt-eng":0.3832784111,"data-quality":0.12076723,"ml-security":0.0810104825}}
{"text":"QFD first trains a quantized (or binarized) representation as the teacher, then quantize the network using knowledge distillation (KD).","meta":{"url":"http://arxiv.org/abs/2307.10638v1"},"cats":{"new-dataset":0.093103581,"dev-research":0.2109088274,"prompt-eng":0.4113401511,"data-quality":0.1109882334,"ml-security":0.109053414}}
{"text":"Quantitative results show that QFD is more flexible and effective (i.e., quantization friendly) than previous quantization methods.","meta":{"url":"http://arxiv.org/abs/2307.10638v1"},"cats":{"new-dataset":0.0117840803,"dev-research":0.2550637029,"prompt-eng":0.3632040213,"data-quality":0.109341393,"ml-security":0.0877641111}}
{"text":"QFD surpasses existing methods by a noticeable margin on not only image classification but also object detection, albeit being much simpler.","meta":{"url":"http://arxiv.org/abs/2307.10638v1"},"cats":{"new-dataset":0.0606074745,"dev-research":0.2391985809,"prompt-eng":0.3934453171,"data-quality":0.2063009608,"ml-security":0.1338333605}}
{"text":"Furthermore, QFD quantizes ViT and Swin-Transformer on MS-COCO detection and segmentation, which verifies its potential in real world deployment.","meta":{"url":"http://arxiv.org/abs/2307.10638v1"},"cats":{"new-dataset":0.1262810529,"dev-research":0.2370566113,"prompt-eng":0.4182451652,"data-quality":0.1443451744,"ml-security":0.0794101964}}
{"text":"To the best of our knowledge, this is the first time that vision transformers have been quantized in object detection and image segmentation tasks.","meta":{"url":"http://arxiv.org/abs/2307.10638v1"},"cats":{"new-dataset":0.0972618927,"dev-research":0.2480469189,"prompt-eng":0.4188495765,"data-quality":0.1248654203,"ml-security":0.1074172513}}
{"text":"A reliable and comprehensive evaluation metric that aligns with manual preference assessments is crucial for conversational head video synthesis method development.","meta":{"url":"http://arxiv.org/abs/2307.10636v1"},"cats":{"new-dataset":0.0730865796,"dev-research":0.3674396563,"prompt-eng":0.4323930194,"data-quality":0.1711900915,"ml-security":0.0322859931}}
{"text":"Existing quantitative evaluations often fail to capture the full complexity of human preference, as they only consider limited evaluation dimensions.","meta":{"url":"http://arxiv.org/abs/2307.10636v1"},"cats":{"new-dataset":0.0109306802,"dev-research":0.3265385316,"prompt-eng":0.3871537312,"data-quality":0.1757784586,"ml-security":0.1084966342}}
{"text":"Qualitative evaluations and user studies offer a solution but are time-consuming and labor-intensive.","meta":{"url":"http://arxiv.org/abs/2307.10636v1"},"cats":{"new-dataset":0.0489001668,"dev-research":0.4603778616,"prompt-eng":0.3778252346,"data-quality":0.0611351188,"ml-security":0.0553822416}}
{"text":"This limitation hinders the advancement of conversational head generation algorithms and systems.","meta":{"url":"http://arxiv.org/abs/2307.10636v1"},"cats":{"new-dataset":0.0514519423,"dev-research":0.2440027507,"prompt-eng":0.4254392967,"data-quality":0.0991430157,"ml-security":0.0769373703}}
{"text":"In this paper, we propose a novel learning-based evaluation metric named Preference Score (PS) for fitting human preference according to the quantitative evaluations across different dimensions.","meta":{"url":"http://arxiv.org/abs/2307.10636v1"},"cats":{"new-dataset":0.083821729,"dev-research":0.3080000771,"prompt-eng":0.4632926444,"data-quality":0.154423328,"ml-security":0.0931227011}}
{"text":"PS can serve as a quantitative evaluation without the need for human annotation.","meta":{"url":"http://arxiv.org/abs/2307.10636v1"},"cats":{"new-dataset":0.0515791423,"dev-research":0.3600525372,"prompt-eng":0.484963811,"data-quality":0.2405351203,"ml-security":0.1004073625}}
{"text":"Experimental results validate the superiority of Preference Score in aligning with human perception, and also demonstrates robustness and generalizability to unseen data, making it a valuable tool for advancing conversation head generation.","meta":{"url":"http://arxiv.org/abs/2307.10636v1"},"cats":{"new-dataset":0.0620181474,"dev-research":0.3152442691,"prompt-eng":0.4788834564,"data-quality":0.1927996534,"ml-security":0.0895482092}}
{"text":"We expect this metric could facilitate new advances in conversational head generation.","meta":{"url":"http://arxiv.org/abs/2307.10636v1"},"cats":{"new-dataset":0.051432999,"dev-research":0.3215436169,"prompt-eng":0.4495961932,"data-quality":0.1342305473,"ml-security":0.0542026357}}
{"text":"Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.0673001418,"dev-research":0.1657415812,"prompt-eng":0.4704566022,"data-quality":0.1685665232,"ml-security":0.0842717235}}
{"text":"However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.0236012945,"dev-research":0.314385458,"prompt-eng":0.3750395903,"data-quality":0.1024199497,"ml-security":0.1490057909}}
{"text":"To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.095354896,"dev-research":0.3605006214,"prompt-eng":0.3790919645,"data-quality":0.0661521413,"ml-security":0.0605104094}}
{"text":"SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.7941437138,"dev-research":0.2536089184,"prompt-eng":0.364281905,"data-quality":0.137372516,"ml-security":0.151451038}}
{"text":"Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.1065899499,"dev-research":0.1929076381,"prompt-eng":0.5996832048,"data-quality":0.1283756301,"ml-security":0.1172940437}}
{"text":"The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80%.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.0332250775,"dev-research":0.2365369312,"prompt-eng":0.4650884685,"data-quality":0.1780947611,"ml-security":0.0846656245}}
{"text":"Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.028690501,"dev-research":0.3774395075,"prompt-eng":0.5188652554,"data-quality":0.2660885096,"ml-security":0.1391109106}}
{"text":"Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.0130188827,"dev-research":0.3590358766,"prompt-eng":0.5248361062,"data-quality":0.1165281787,"ml-security":0.1613512214}}
{"text":"We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.","meta":{"url":"http://arxiv.org/abs/2307.10635v1"},"cats":{"new-dataset":0.0269890842,"dev-research":0.2802888922,"prompt-eng":0.4909664787,"data-quality":0.0902659727,"ml-security":0.1080568043}}
{"text":"Large Language Models have many methods for solving the same problem.","meta":{"url":"http://arxiv.org/abs/2307.10633v1"},"cats":{"new-dataset":0.1016545923,"dev-research":0.2391818891,"prompt-eng":0.4266885664,"data-quality":0.2134115451,"ml-security":0.0761916496}}
{"text":"This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use).","meta":{"url":"http://arxiv.org/abs/2307.10633v1"},"cats":{"new-dataset":0.0116450878,"dev-research":0.4862227347,"prompt-eng":0.4105195308,"data-quality":0.1048784786,"ml-security":0.1213326735}}
{"text":"In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method.","meta":{"url":"http://arxiv.org/abs/2307.10633v1"},"cats":{"new-dataset":0.0331004133,"dev-research":0.2805321956,"prompt-eng":0.3936211933,"data-quality":0.2796956877,"ml-security":0.20827788}}
{"text":"Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales.","meta":{"url":"http://arxiv.org/abs/2307.10633v1"},"cats":{"new-dataset":0.0322855309,"dev-research":0.3462065389,"prompt-eng":0.4765344095,"data-quality":0.1392223577,"ml-security":0.0803065162}}
{"text":"We then conduct ablation analyses to explore why MMST works.","meta":{"url":"http://arxiv.org/abs/2307.10633v1"},"cats":{"new-dataset":0.0226324921,"dev-research":0.2744730242,"prompt-eng":0.4055861054,"data-quality":0.0997748567,"ml-security":0.0503143519}}
{"text":"We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods.","meta":{"url":"http://arxiv.org/abs/2307.10633v1"},"cats":{"new-dataset":0.0760101941,"dev-research":0.3165456994,"prompt-eng":0.414481911,"data-quality":0.1970908015,"ml-security":0.142786914}}
{"text":"We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective.","meta":{"url":"http://arxiv.org/abs/2307.10633v1"},"cats":{"new-dataset":0.0067424009,"dev-research":0.4181457509,"prompt-eng":0.5531879144,"data-quality":0.1302704815,"ml-security":0.1448665071}}
{"text":"We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training.","meta":{"url":"http://arxiv.org/abs/2307.10633v1"},"cats":{"new-dataset":0.0242597609,"dev-research":0.3248372795,"prompt-eng":0.445540903,"data-quality":0.2543114071,"ml-security":0.3079783572}}
{"text":"This article presents the methods used to parallelize a new computer vision application.","meta":{"url":"http://arxiv.org/abs/2307.10632v1"},"cats":{"new-dataset":0.0946729636,"dev-research":0.2741471208,"prompt-eng":0.3685886218,"data-quality":0.0643693119,"ml-security":0.0566051641}}
{"text":"The system is able to automatically detect meteor from non-stabilized cameras and noisy video sequences.","meta":{"url":"http://arxiv.org/abs/2307.10632v1"},"cats":{"new-dataset":0.2127469951,"dev-research":0.2543436081,"prompt-eng":0.4308099346,"data-quality":0.1872898554,"ml-security":0.0929329615}}
{"text":"The application is designed to be embedded in weather balloons or for airborne observation campaigns.","meta":{"url":"http://arxiv.org/abs/2307.10632v1"},"cats":{"new-dataset":0.133708536,"dev-research":0.2785485743,"prompt-eng":0.4210174534,"data-quality":0.0819044168,"ml-security":0.0785460236}}
{"text":"Thus, the final target is a low power system-on-chip (< 10 Watts) while the software needs to compute a stream of frames in real-time (> 25 frames per second).","meta":{"url":"http://arxiv.org/abs/2307.10632v1"},"cats":{"new-dataset":0.0432953352,"dev-research":0.3788293632,"prompt-eng":0.3903643166,"data-quality":0.085681758,"ml-security":0.0799700108}}
{"text":"For this, first the application is split in a tasks graph, then different parallelization techniques are applied.","meta":{"url":"http://arxiv.org/abs/2307.10632v1"},"cats":{"new-dataset":0.0304018592,"dev-research":0.2556277449,"prompt-eng":0.3750281955,"data-quality":0.057123165,"ml-security":0.0501999512}}
{"text":"Experiment results demonstrate the efficiency of the parallelization methods.","meta":{"url":"http://arxiv.org/abs/2307.10632v1"},"cats":{"new-dataset":0.0209267694,"dev-research":0.2673280142,"prompt-eng":0.3624317433,"data-quality":0.061295944,"ml-security":0.0654203938}}
{"text":"For instance, on the Raspberry Pi 4 and on a HD video sequence, the processing chain reaches 42 frames per second while it only consumes 6 Watts.","meta":{"url":"http://arxiv.org/abs/2307.10632v1"},"cats":{"new-dataset":0.0695858194,"dev-research":0.2488062817,"prompt-eng":0.3427344299,"data-quality":0.0670935607,"ml-security":0.0293789773}}
{"text":"The practice of code reuse is crucial in software development for a faster and more efficient development lifecycle.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.0844912249,"dev-research":0.6049526603,"prompt-eng":0.3766041893,"data-quality":0.0919462023,"ml-security":0.1306568346}}
{"text":"In reality, however, code reuse practices lack proper control, resulting in issues such as vulnerability propagation and intellectual property infringements.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.0648126043,"dev-research":0.5032813392,"prompt-eng":0.3564069657,"data-quality":0.293791745,"ml-security":0.4422994753}}
{"text":"Assembly clone search, a critical shift-right defence mechanism, has been effective in identifying vulnerable code resulting from reuse in released executables.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.0952577103,"dev-research":0.4458430551,"prompt-eng":0.4394395036,"data-quality":0.1516762042,"ml-security":0.3681019338}}
{"text":"Recent studies on assembly clone search demonstrate a trend towards using machine learning-based methods to match assembly code variants produced by different toolchains.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.1218192275,"dev-research":0.3454469416,"prompt-eng":0.4147328646,"data-quality":0.150880931,"ml-security":0.08907654}}
{"text":"However, these methods are limited to what they learn from a small number of toolchain variants used in training, rendering them inapplicable to unseen architectures and their corresponding compilation toolchain variants.   ","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.0681228123,"dev-research":0.4017451342,"prompt-eng":0.3843280305,"data-quality":0.1271205915,"ml-security":0.2581041548}}
{"text":"This paper presents the first study on the problem of assembly clone search with unseen architectures and libraries.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.1255404541,"dev-research":0.2767552687,"prompt-eng":0.3951523369,"data-quality":0.0961554779,"ml-security":0.0655272015}}
{"text":"We propose incorporating human common knowledge through large-scale pre-trained natural language models, in the form of transfer learning, into current learning-based approaches for assembly clone search.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.2951612102,"dev-research":0.2746932566,"prompt-eng":0.4572277514,"data-quality":0.1565484416,"ml-security":0.0920615348}}
{"text":"Transfer learning can aid in addressing the limitations of the existing approaches, as it can bring in broader knowledge from human experts in assembly code.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.0553411598,"dev-research":0.3997533906,"prompt-eng":0.4385891801,"data-quality":0.0976764278,"ml-security":0.1034557849}}
{"text":"We further address the sequence limit issue by proposing a reinforcement learning agent to remove unnecessary and redundant tokens.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.0629150268,"dev-research":0.1979131362,"prompt-eng":0.4151846434,"data-quality":0.1623168588,"ml-security":0.2002081462}}
{"text":"Coupled with a new Variational Information Bottleneck learning strategy, the proposed system minimizes the reliance on potential indicators of architectures and optimization settings, for a better generalization of unseen architectures.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.0291708217,"dev-research":0.2808788257,"prompt-eng":0.3654722825,"data-quality":0.1304466436,"ml-security":0.2654259282}}
{"text":"We simulate the unseen architecture clone search scenarios and the experimental results show the effectiveness of the proposed approach against the state-of-the-art solutions.","meta":{"url":"http://arxiv.org/abs/2307.10631v1"},"cats":{"new-dataset":0.0676904728,"dev-research":0.2926584658,"prompt-eng":0.4170798211,"data-quality":0.1153172582,"ml-security":0.1456276599}}
{"text":"Underlying the theory of inferences, a primary task of logic is language analysis.","meta":{"url":"http://arxiv.org/abs/2307.10629v1"},"cats":{"new-dataset":0.0132853651,"dev-research":0.3728624909,"prompt-eng":0.4556795592,"data-quality":0.1300849049,"ml-security":0.0783237084}}
{"text":"Such a task can be understood as depending on a general theory of representation, taking as a starting point the idea that some entities (`` representations '') can present some entites (`` contents '').","meta":{"url":"http://arxiv.org/abs/2307.10629v1"},"cats":{"new-dataset":0.0452767757,"dev-research":0.2765349657,"prompt-eng":0.4702194154,"data-quality":0.1093890904,"ml-security":0.0755902996}}
{"text":"We outline a theory of representation accounting for the capacity of representational systems to access universes that extend beyond an immediate presence.","meta":{"url":"http://arxiv.org/abs/2307.10629v1"},"cats":{"new-dataset":0.0388561798,"dev-research":0.2249761055,"prompt-eng":0.4031150507,"data-quality":0.0764768965,"ml-security":0.1211383262}}
{"text":"We define three logical properties that any adequate representational system should have: completeness, faithfulness, coherence.","meta":{"url":"http://arxiv.org/abs/2307.10629v1"},"cats":{"new-dataset":0.031054063,"dev-research":0.2511368387,"prompt-eng":0.4153720332,"data-quality":0.160908908,"ml-security":0.0675492921}}
{"text":"We show that logical laws are laws of representation.","meta":{"url":"http://arxiv.org/abs/2307.10629v1"},"cats":{"new-dataset":0.0755059555,"dev-research":0.3573892636,"prompt-eng":0.4120110145,"data-quality":0.1524269443,"ml-security":0.136041211}}
{"text":"Finally, it appears that logic can be considered as the abstract theory of representation.","meta":{"url":"http://arxiv.org/abs/2307.10629v1"},"cats":{"new-dataset":0.0190230492,"dev-research":0.3251273921,"prompt-eng":0.3963437905,"data-quality":0.0884055467,"ml-security":0.0795910737}}
{"text":"Colonoscopic Polyp Re-Identification aims to match a specific polyp in a large gallery with different cameras and views, which plays a key role for the prevention and treatment of colorectal cancer in the computer-aided diagnosis.","meta":{"url":"http://arxiv.org/abs/2307.10625v1"},"cats":{"new-dataset":0.0964744545,"dev-research":0.2204805057,"prompt-eng":0.3897769388,"data-quality":0.1351656235,"ml-security":0.0667320016}}
{"text":"However, traditional methods mainly focus on the visual representation learning, while neglect to explore the potential of semantic features during training, which may easily leads to poor generalization capability when adapted the pretrained model into the new scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10625v1"},"cats":{"new-dataset":0.0239409422,"dev-research":0.3301722853,"prompt-eng":0.4280714518,"data-quality":0.1909338365,"ml-security":0.1405020669}}
{"text":"To relieve this dilemma, we propose a simple but effective training method named VT-ReID, which can remarkably enrich the representation of polyp videos with the interchange of high-level semantic information.","meta":{"url":"http://arxiv.org/abs/2307.10625v1"},"cats":{"new-dataset":0.0691496721,"dev-research":0.2866186154,"prompt-eng":0.4105511553,"data-quality":0.2824292456,"ml-security":0.0682422882}}
{"text":"Moreover, we elaborately design a novel clustering mechanism to introduce prior knowledge from textual data, which leverages contrastive learning to promote better separation from abundant unlabeled text data.","meta":{"url":"http://arxiv.org/abs/2307.10625v1"},"cats":{"new-dataset":0.1560162576,"dev-research":0.2955578515,"prompt-eng":0.3855979951,"data-quality":0.29093415,"ml-security":0.1113963641}}
{"text":"To the best of our knowledge, this is the first attempt to employ the visual-text feature with clustering mechanism for the colonoscopic polyp re-identification.","meta":{"url":"http://arxiv.org/abs/2307.10625v1"},"cats":{"new-dataset":0.079447757,"dev-research":0.2573916649,"prompt-eng":0.4158193841,"data-quality":0.2310955794,"ml-security":0.0639859764}}
{"text":"Empirical results show that our method significantly outperforms current state-of-the art methods with a clear margin.","meta":{"url":"http://arxiv.org/abs/2307.10625v1"},"cats":{"new-dataset":0.0310142752,"dev-research":0.3437153571,"prompt-eng":0.3839800671,"data-quality":0.2589091498,"ml-security":0.0626842327}}
{"text":"In this paper, we briefly introduce the solution of our team HFUT-VUT for the Micros-gesture Classification in the MiGA challenge at IJCAI 2023.","meta":{"url":"http://arxiv.org/abs/2307.10624v1"},"cats":{"new-dataset":0.2923923211,"dev-research":0.2459586908,"prompt-eng":0.4164131078,"data-quality":0.1733746046,"ml-security":0.0936534842}}
{"text":"The micro-gesture classification task aims at recognizing the action category of a given video based on the skeleton data.","meta":{"url":"http://arxiv.org/abs/2307.10624v1"},"cats":{"new-dataset":0.1692567941,"dev-research":0.2649984134,"prompt-eng":0.3744479628,"data-quality":0.1470909589,"ml-security":0.0708496709}}
{"text":"For this task, we propose a 3D-CNNs-based micro-gesture recognition network, which incorporates a skeletal and semantic embedding loss to improve action classification performance.","meta":{"url":"http://arxiv.org/abs/2307.10624v1"},"cats":{"new-dataset":0.2020561479,"dev-research":0.2909410197,"prompt-eng":0.3425988179,"data-quality":0.1312902197,"ml-security":0.1043489825}}
{"text":"Finally, we rank 1st in the Micro-gesture Classification Challenge, surpassing the second-place team in terms of Top-1 accuracy by 1.10%.","meta":{"url":"http://arxiv.org/abs/2307.10624v1"},"cats":{"new-dataset":0.2476422083,"dev-research":0.280647423,"prompt-eng":0.4248051499,"data-quality":0.186194244,"ml-security":0.0837411659}}
{"text":"In recent years, tensor networks have emerged as powerful tools for solving large-scale optimization problems.","meta":{"url":"http://arxiv.org/abs/2307.10620v1"},"cats":{"new-dataset":0.0372225542,"dev-research":0.201034079,"prompt-eng":0.3715125272,"data-quality":0.0722919256,"ml-security":0.1823880102}}
{"text":"One of the most promising tensor networks is the tensor ring (TR) decomposition, which achieves circular dimensional permutation invariance in the model through the utilization of the trace operation and equitable treatment of the latent cores.","meta":{"url":"http://arxiv.org/abs/2307.10620v1"},"cats":{"new-dataset":0.0297935681,"dev-research":0.2063954587,"prompt-eng":0.3735289584,"data-quality":0.1124869039,"ml-security":0.1051494145}}
{"text":"On the other hand, more recently, quaternions have gained significant attention and have been widely utilized in color image processing tasks due to their effectiveness in encoding color pixels.","meta":{"url":"http://arxiv.org/abs/2307.10620v1"},"cats":{"new-dataset":0.0228562324,"dev-research":0.3021495269,"prompt-eng":0.3711953619,"data-quality":0.0609283599,"ml-security":0.0650128924}}
{"text":"Therefore, in this paper, we propose the quaternion tensor ring (QTR) decomposition, which inherits the powerful and generalized representation abilities of the TR decomposition while leveraging the advantages of quaternions for color pixel representation.","meta":{"url":"http://arxiv.org/abs/2307.10620v1"},"cats":{"new-dataset":0.040166115,"dev-research":0.2725722175,"prompt-eng":0.3551476744,"data-quality":0.0778221486,"ml-security":0.069509185}}
{"text":"In addition to providing the definition of QTR decomposition and an algorithm for learning the QTR format, this paper also proposes a low-rank quaternion tensor completion (LRQTC) model and its algorithm for color image inpainting based on the QTR decomposition.","meta":{"url":"http://arxiv.org/abs/2307.10620v1"},"cats":{"new-dataset":0.0367994479,"dev-research":0.2141565438,"prompt-eng":0.3859937159,"data-quality":0.0953288588,"ml-security":0.0535318738}}
{"text":"Finally, extensive experiments on color image inpainting demonstrate that the proposed QTLRC method is highly competitive.","meta":{"url":"http://arxiv.org/abs/2307.10620v1"},"cats":{"new-dataset":0.0268024812,"dev-research":0.2313199228,"prompt-eng":0.3826981866,"data-quality":0.1013531843,"ml-security":0.0430371561}}
{"text":"As more data-intensive tasks with large footprints are deployed in virtual machines (VMs), huge pages are widely used to eliminate the increasing address translation overhead.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.1392770096,"dev-research":0.3428698853,"prompt-eng":0.4003513897,"data-quality":0.0908731593,"ml-security":0.0997241796}}
{"text":"However, once the huge page mapping is established, all the base page regions in the huge page share a single extended page table (EPT) entry, so that the hypervisor loses awareness of accesses to base page regions.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0341751953,"dev-research":0.2631581023,"prompt-eng":0.4056199814,"data-quality":0.0813595337,"ml-security":0.0911717807}}
{"text":"None of the state-of-the-art solutions can obtain access information at base page granularity for huge pages.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0470600455,"dev-research":0.2240583688,"prompt-eng":0.3363764086,"data-quality":0.1113544148,"ml-security":0.129472358}}
{"text":"We observe that this can lead to incorrect decisions by the hypervisor, such as incorrect data placement in a tiered memory system and unshared base page regions when sharing pages.   ","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0201423813,"dev-research":0.3459759207,"prompt-eng":0.3717348726,"data-quality":0.2015594196,"ml-security":0.1771540874}}
{"text":"This paper proposes FHPM, a fine-grained huge page management for virtualization without hardware and guest OS modification.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0875273996,"dev-research":0.2849683862,"prompt-eng":0.4115585107,"data-quality":0.0609132694,"ml-security":0.1074073197}}
{"text":"FHPM can identify access information at base page granularity, and dynamically promote and demote pages.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0434644781,"dev-research":0.2787570209,"prompt-eng":0.5023385885,"data-quality":0.1007993296,"ml-security":0.1925882289}}
{"text":"A key insight of FHPM is to redirect the EPT huge page directory entries (PDEs) to new companion pages so that the MMU can track access information within huge pages.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.063518512,"dev-research":0.2048672483,"prompt-eng":0.4801695245,"data-quality":0.0698910884,"ml-security":0.1092518311}}
{"text":"Then, FHPM can promote and demote pages according to the current hot page pressure to balance address translation overhead and memory usage.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0234675338,"dev-research":0.2719846922,"prompt-eng":0.479240166,"data-quality":0.1124558532,"ml-security":0.1265854469}}
{"text":"At the same time, FHPM proposes a VM-friendly page splitting and collapsing mechanism to avoid extra VM-exits.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0245117918,"dev-research":0.2621255403,"prompt-eng":0.4325640928,"data-quality":0.0891712921,"ml-security":0.1322866257}}
{"text":"In combination, FHPM minimizes the monitoring and management overhead and ensures that the hypervisor gets fine-grained VM memory accesses to make the proper decision.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0192846658,"dev-research":0.3049378957,"prompt-eng":0.4159102276,"data-quality":0.0762184388,"ml-security":0.1118161224}}
{"text":"We apply FHPM to improve tiered memory management (FHPM-TMM) and to promote page sharing (FHPM-Share).","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0687603229,"dev-research":0.2269134325,"prompt-eng":0.4534674914,"data-quality":0.0880160906,"ml-security":0.1210851303}}
{"text":"FHPM-TMM achieves a performance improvement of up to 33% and 61% over the pure huge page and base page management.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0372948191,"dev-research":0.2558489705,"prompt-eng":0.4559513361,"data-quality":0.0617646195,"ml-security":0.0580672138}}
{"text":"FHPM-Share can save 41% more memory than Ingens, a state-of-the-art page sharing solution, with comparable performance.","meta":{"url":"http://arxiv.org/abs/2307.10618v1"},"cats":{"new-dataset":0.0700588736,"dev-research":0.2848610188,"prompt-eng":0.4186411062,"data-quality":0.0765449279,"ml-security":0.100164475}}
{"text":"In recent years, online reviews play a vital role for promoting any kind of product or services.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.029898699,"dev-research":0.3569945426,"prompt-eng":0.3743590597,"data-quality":0.1348829188,"ml-security":0.0721989542}}
{"text":"Businesses may embed fake reviews in order to attract customers to purchase their products.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.0401505784,"dev-research":0.3696162253,"prompt-eng":0.4111490195,"data-quality":0.3421838545,"ml-security":0.1679692494}}
{"text":"They may even highlight the benefits of their own product or criticize the competition's product.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.0145281625,"dev-research":0.416947082,"prompt-eng":0.4195568385,"data-quality":0.2224765495,"ml-security":0.1891456235}}
{"text":"Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.0417844504,"dev-research":0.361563292,"prompt-eng":0.3917187733,"data-quality":0.3274702556,"ml-security":0.2273794101}}
{"text":"So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.0379075622,"dev-research":0.3879133962,"prompt-eng":0.376994515,"data-quality":0.2230794375,"ml-security":0.2604375209}}
{"text":"Thus, identifying deceptive reviews is an intense and on-going research area.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.0385212726,"dev-research":0.4193845357,"prompt-eng":0.3873646489,"data-quality":0.2962458724,"ml-security":0.2279612744}}
{"text":"This research paper proposes machine learning model approach to identify deceptive reviews.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.0580894275,"dev-research":0.3728332414,"prompt-eng":0.3913933723,"data-quality":0.3827960086,"ml-security":0.4350011733}}
{"text":"The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.1676619762,"dev-research":0.2983803429,"prompt-eng":0.3906126206,"data-quality":0.4366506385,"ml-security":0.313785316}}
{"text":"We developed a n-gram model and max features to identify deceptive contents with a particular focus on fake reviews.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.102060692,"dev-research":0.3401754032,"prompt-eng":0.417555991,"data-quality":0.4675430718,"ml-security":0.2864959488}}
{"text":"Further, we conduct a benchmark study to investigate the performance of two different features extraction techniques and apply five machine learning classification techniques.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.104909023,"dev-research":0.35847535,"prompt-eng":0.3696081971,"data-quality":0.2387619972,"ml-security":0.1730261499}}
{"text":"The experimental results show that passive aggressive classifier outperforms other algorithms, and it reaches the highest accuracy not only in text classification but also to fake reviews.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.0563827796,"dev-research":0.3299932647,"prompt-eng":0.3909054517,"data-quality":0.5136367022,"ml-security":0.3687705311}}
{"text":"We also study the data augmentation and implement different deep learning techniques.","meta":{"url":"http://arxiv.org/abs/2307.10617v1"},"cats":{"new-dataset":0.1600486537,"dev-research":0.3016103937,"prompt-eng":0.3949088175,"data-quality":0.2063685123,"ml-security":0.1287278054}}
{"text":"Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.0594024214,"dev-research":0.2654042254,"prompt-eng":0.4001315508,"data-quality":0.1153103249,"ml-security":0.1886142639}}
{"text":"Existing federated learning works mainly focus on model homogeneous settings.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.0299532409,"dev-research":0.2178621305,"prompt-eng":0.4078872441,"data-quality":0.0964477606,"ml-security":0.1468797046}}
{"text":"However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.0484992376,"dev-research":0.2695818591,"prompt-eng":0.3698872452,"data-quality":0.0990643603,"ml-security":0.2635846822}}
{"text":"Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.0705462444,"dev-research":0.2498581818,"prompt-eng":0.3703539147,"data-quality":0.0810414947,"ml-security":0.1001911355}}
{"text":"Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.0953429463,"dev-research":0.2994273774,"prompt-eng":0.410906994,"data-quality":0.1275871446,"ml-security":0.0597919365}}
{"text":"In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.1024800424,"dev-research":0.2369938411,"prompt-eng":0.3955993799,"data-quality":0.1182651515,"ml-security":0.0580721899}}
{"text":"In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.0227757017,"dev-research":0.2540814689,"prompt-eng":0.3719434811,"data-quality":0.1221905165,"ml-security":0.0480499686}}
{"text":"We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.0670305452,"dev-research":0.2424477358,"prompt-eng":0.4261138575,"data-quality":0.1295155899,"ml-security":0.068555319}}
{"text":"Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.0532894033,"dev-research":0.2578728756,"prompt-eng":0.410300548,"data-quality":0.0712929387,"ml-security":0.0384462637}}
{"text":"A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.","meta":{"url":"http://arxiv.org/abs/2307.10616v1"},"cats":{"new-dataset":0.7140262687,"dev-research":0.2271052726,"prompt-eng":0.4055697538,"data-quality":0.0904719436,"ml-security":0.0327102197}}
{"text":"Indian Judiciary is suffering from burden of millions of cases that are lying pending in its courts at all the levels.","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.0544122307,"dev-research":0.2904776209,"prompt-eng":0.3187403737,"data-quality":0.1050159945,"ml-security":0.1783262746}}
{"text":"In this paper, we analyze the data that we have collected on the pendency of 24 high courts in the Republic of India as they were made available on High Court NJDG (HC-NJDG).","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.3110699035,"dev-research":0.2398449907,"prompt-eng":0.3695565035,"data-quality":0.0793633515,"ml-security":0.1019430035}}
{"text":"We collected data on 73 days beginning August 31, 2017 to December 26, 2018, including these days.","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.820409434,"dev-research":0.2664341832,"prompt-eng":0.3616010344,"data-quality":0.099934282,"ml-security":0.113628102}}
{"text":"Thus, the data collected by us spans a period of almost sixteen months.","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.6261528558,"dev-research":0.2529088094,"prompt-eng":0.3337568933,"data-quality":0.0971494542,"ml-security":0.0697838655}}
{"text":"We have analyzed various statistics available on the NJDG portal for High Courts, including but not limited to the number of judges in each high court, the number of cases pending in each high court, cases that have been pending for more than 10 years, cases filed, listed and disposed, cases filed by women and senior citizens, etc.","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.1776953933,"dev-research":0.2295008264,"prompt-eng":0.3837972634,"data-quality":0.0717739797,"ml-security":0.0716515948}}
{"text":"Our results show that: 1) statistics as important as the number of judges in high courts have serious errors on NJDG (Fig. 1, 2, 10, 11, Table V).","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.0758172365,"dev-research":0.324311616,"prompt-eng":0.3848067975,"data-quality":0.2829415697,"ml-security":0.0825832684}}
{"text":"2) pending cases in most of the high courts are increasing rather than decreasing (Fig. 3, 13).","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.0732381317,"dev-research":0.2575556628,"prompt-eng":0.3723575508,"data-quality":0.0855560933,"ml-security":0.1042187205}}
{"text":"3) regular update of HC-NJDG is required for it to be useful.","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.0351412243,"dev-research":0.3132969192,"prompt-eng":0.4119618021,"data-quality":0.1106037518,"ml-security":0.075967035}}
{"text":"Data related to some high courts is not being updated regularly or is updated erroneously on the portal (Fig. 14).","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.4087106858,"dev-research":0.2599379987,"prompt-eng":0.3282954346,"data-quality":0.1370158467,"ml-security":0.1311170957}}
{"text":"4) there is a huge difference in terms of average load of cases on judges of different high courts (Fig. 6).","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.0397550033,"dev-research":0.2353853034,"prompt-eng":0.3549365558,"data-quality":0.0855951993,"ml-security":0.0827209937}}
{"text":"5) if all the high courts operate at their approved strength of judges, then for most of the high courts pendency can be nullified within 20 years from now (Fig. 21, 22).","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.0801258593,"dev-research":0.2290778187,"prompt-eng":0.351546888,"data-quality":0.1787291201,"ml-security":0.1505488862}}
{"text":"6) the pending cases filed by women and senior citizens are disproportionately low, they together constitute less than 10% of the total pending cases (Fig. 23 - 27) 7) a better scheduling process for preparing causelists in courts can help reducing the number of pending cases in the High Courts (Fig. 29).","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.0524454733,"dev-research":0.3145394379,"prompt-eng":0.3818558467,"data-quality":0.1232839565,"ml-security":0.1391555856}}
{"text":"8) some statistics are not well defined (Fig. 31).","meta":{"url":"http://arxiv.org/abs/2307.10615v1"},"cats":{"new-dataset":0.2454625932,"dev-research":0.3011294708,"prompt-eng":0.3426143408,"data-quality":0.2066114726,"ml-security":0.0957076596}}
{"text":"Relative localization is crucial for multi-robot systems to perform cooperative tasks, especially in GPS-denied environments.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.0255985715,"dev-research":0.2602913622,"prompt-eng":0.42228986,"data-quality":0.1225093334,"ml-security":0.1101249874}}
{"text":"Current techniques for multi-robot relative localization rely on expensive or short-range sensors such as cameras and LIDARs.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.058501735,"dev-research":0.2300772636,"prompt-eng":0.3934190549,"data-quality":0.0912789097,"ml-security":0.0641374085}}
{"text":"As a result, these algorithms face challenges such as high computational complexity, dependencies on well-structured environments, etc.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.0706719349,"dev-research":0.3027684394,"prompt-eng":0.3675379971,"data-quality":0.0946672028,"ml-security":0.1274826692}}
{"text":"To overcome these limitations, we propose a new distributed approach to perform relative localization using a Gaussian Processes map of the Radio Signal Strength Indicator (RSSI) values from a single wireless Access Point (AP) to which the robots are connected.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.0782982248,"dev-research":0.2360166867,"prompt-eng":0.4220515488,"data-quality":0.1110784971,"ml-security":0.0679977013}}
{"text":"Our approach, Gaussian Processes-based Relative Localization (GPRL), combines two pillars.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.1562086455,"dev-research":0.2446425575,"prompt-eng":0.4248621373,"data-quality":0.1598638557,"ml-security":0.0639486741}}
{"text":"First, the robots locate the AP w.r.t.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.1055676903,"dev-research":0.2430518638,"prompt-eng":0.4372151946,"data-quality":0.1008252307,"ml-security":0.0781890033}}
{"text":"their local reference frames using novel hierarchical inferencing that significantly reduces computational complexity.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.1740847892,"dev-research":0.3010618211,"prompt-eng":0.4052301874,"data-quality":0.1635076059,"ml-security":0.0538309815}}
{"text":"Secondly, the robots obtain relative positions of neighbor robots with an AP-oriented vector transformation.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.0364081921,"dev-research":0.2829391252,"prompt-eng":0.4213717405,"data-quality":0.0706161562,"ml-security":0.0601546867}}
{"text":"The approach readily applies to resource-constrained devices and relies only on the ubiquitously-available RSSI measurement.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.035775928,"dev-research":0.2506021936,"prompt-eng":0.4075153539,"data-quality":0.0685535963,"ml-security":0.0846305119}}
{"text":"We extensively validate the performance of the two pillars of the proposed GRPL in Robotarium simulations.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.0905129146,"dev-research":0.2112144814,"prompt-eng":0.4203592645,"data-quality":0.0524102528,"ml-security":0.02689367}}
{"text":"We also demonstrate the applicability of GPRL through a multi-robot rendezvous task with a team of three real-world robots.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.0762135891,"dev-research":0.2411981532,"prompt-eng":0.4132381811,"data-quality":0.0623210487,"ml-security":0.0723687047}}
{"text":"The results demonstrate that GPRL outperformed state-of-the-art approaches regarding accuracy, computation, and real-time performance.","meta":{"url":"http://arxiv.org/abs/2307.10614v1"},"cats":{"new-dataset":0.0797991828,"dev-research":0.3419452508,"prompt-eng":0.3435239628,"data-quality":0.1028272191,"ml-security":0.0659108221}}
{"text":"We present a near-linear time approximation algorithm for the subtrajectory cluster problem of $c$-packed trajectories.","meta":{"url":"http://arxiv.org/abs/2307.10610v1"},"cats":{"new-dataset":0.1256672823,"dev-research":0.1852303088,"prompt-eng":0.3219088028,"data-quality":0.1055440896,"ml-security":0.0517583536}}
{"text":"The problem involves finding $m$ subtrajectories within a given trajectory $T$ such that their Fr\\'echet distances are at most $(1 + \\varepsilon)d$, and at least one subtrajectory must be of length~$l$ or longer.","meta":{"url":"http://arxiv.org/abs/2307.10610v1"},"cats":{"new-dataset":0.1744875633,"dev-research":0.1309841262,"prompt-eng":0.3544481582,"data-quality":0.1165654412,"ml-security":0.07285087}}
{"text":"A trajectory $T$ is $c$-packed if the intersection of $T$ and any ball $B$ with radius $r$ is at most $c \\cdot r$ in length.   ","meta":{"url":"http://arxiv.org/abs/2307.10610v1"},"cats":{"new-dataset":0.1837128371,"dev-research":0.2626506779,"prompt-eng":0.3638527507,"data-quality":0.0951222678,"ml-security":0.055626743}}
{"text":"Previous results by Gudmundsson and Wong \\cite{GudmundssonWong2022Cubicupperlower} established an $\\Omega(n^3)$ lower bound unless the Strong Exponential Time Hypothesis fails, and they presented an $O(n^3 \\log^2","meta":{"url":"http://arxiv.org/abs/2307.10610v1"},"cats":{"new-dataset":0.0637181674,"dev-research":0.1921722048,"prompt-eng":0.3004232556,"data-quality":0.0849421823,"ml-security":0.085139385}}
{"text":"n)$ time algorithm.","meta":{"url":"http://arxiv.org/abs/2307.10610v1"},"cats":{"new-dataset":0.1571618442,"dev-research":0.2481953876,"prompt-eng":0.3558459511,"data-quality":0.107005866,"ml-security":0.0977702825}}
{"text":"We circumvent this conditional lower bound by studying subtrajectory cluster on $c$-packed trajectories, resulting in an algorithm with an $O((c^2 n/\\varepsilon^2)\\log(c/\\varepsilon)\\log(n/\\varepsilon))$ time complexity.","meta":{"url":"http://arxiv.org/abs/2307.10610v1"},"cats":{"new-dataset":0.0853271109,"dev-research":0.1928299507,"prompt-eng":0.297081552,"data-quality":0.0960773229,"ml-security":0.0840478451}}
{"text":"Building outline extracted from high-resolution aerial images can be used in various application fields such as change detection and disaster assessment.","meta":{"url":"http://arxiv.org/abs/2307.10609v1"},"cats":{"new-dataset":0.2175878166,"dev-research":0.3503832746,"prompt-eng":0.4460792544,"data-quality":0.130073367,"ml-security":0.063787004}}
{"text":"However, traditional CNN model cannot recognize contours very precisely from original images.","meta":{"url":"http://arxiv.org/abs/2307.10609v1"},"cats":{"new-dataset":0.0219382309,"dev-research":0.1959806916,"prompt-eng":0.3539072939,"data-quality":0.3089168967,"ml-security":0.1171376912}}
{"text":"In this paper, we proposed a CNN and Transformer based model together with active contour model to deal with this problem.","meta":{"url":"http://arxiv.org/abs/2307.10609v1"},"cats":{"new-dataset":0.0735644779,"dev-research":0.1782681476,"prompt-eng":0.3911970974,"data-quality":0.111613197,"ml-security":0.0786495478}}
{"text":"We also designed a triple-branch decoder structure to handle different features generated by encoder.","meta":{"url":"http://arxiv.org/abs/2307.10609v1"},"cats":{"new-dataset":0.0767054244,"dev-research":0.320475304,"prompt-eng":0.4496991026,"data-quality":0.1047824404,"ml-security":0.050234124}}
{"text":"Experiment results show that our model outperforms other baseline model on two datasets, achieving 91.1% mIoU on Vaihingen and 83.8% on Bing huts.","meta":{"url":"http://arxiv.org/abs/2307.10609v1"},"cats":{"new-dataset":0.2070468873,"dev-research":0.2382948284,"prompt-eng":0.3903354777,"data-quality":0.1053430218,"ml-security":0.0641446485}}
{"text":"In this work, we initiate the complexity study of Biclique Contraction and Balanced Biclique Contraction.","meta":{"url":"http://arxiv.org/abs/2307.10607v1"},"cats":{"new-dataset":0.0855438504,"dev-research":0.2550297177,"prompt-eng":0.3053918897,"data-quality":0.0872375289,"ml-security":0.0408926194}}
{"text":"In these problems, given as input a graph G and an integer k, the objective is to determine whether one can contract at most k edges in G to obtain a biclique and a balanced biclique, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10607v1"},"cats":{"new-dataset":0.1066427209,"dev-research":0.2168795691,"prompt-eng":0.3071084248,"data-quality":0.1019311833,"ml-security":0.0802304407}}
{"text":"We first prove that these problems are NP-complete even when the input graph is bipartite.","meta":{"url":"http://arxiv.org/abs/2307.10607v1"},"cats":{"new-dataset":0.1442161063,"dev-research":0.2187955732,"prompt-eng":0.3424579798,"data-quality":0.2257628648,"ml-security":0.1010825797}}
{"text":"Next, we study the parameterized complexity of these problems and show that they admit single exponential-time FPT algorithms when parameterized by the number k of edge contractions.","meta":{"url":"http://arxiv.org/abs/2307.10607v1"},"cats":{"new-dataset":0.0375324775,"dev-research":0.187542938,"prompt-eng":0.3072954125,"data-quality":0.0950665213,"ml-security":0.1126181519}}
{"text":"Then, we show that Balanced Biclique Contraction admits a quadratic vertex kernel while Biclique Contraction does not admit any polynomial compression (or kernel) under standard complexity-theoretic assumptions.","meta":{"url":"http://arxiv.org/abs/2307.10607v1"},"cats":{"new-dataset":0.0258145671,"dev-research":0.246416475,"prompt-eng":0.2964929402,"data-quality":0.1412113288,"ml-security":0.094333193}}
{"text":"To address 3D object retrieval, substantial efforts have been made to generate highly discriminative descriptors of 3D objects represented by a single modality, e.g., voxels, point clouds or multi-view images.","meta":{"url":"http://arxiv.org/abs/2307.10601v1"},"cats":{"new-dataset":0.1029473623,"dev-research":0.216973324,"prompt-eng":0.4020956852,"data-quality":0.0934863337,"ml-security":0.0798233051}}
{"text":"It is promising to leverage the complementary information from multi-modality representations of 3D objects to further improve retrieval performance.","meta":{"url":"http://arxiv.org/abs/2307.10601v1"},"cats":{"new-dataset":0.0330224529,"dev-research":0.2397452055,"prompt-eng":0.3950830578,"data-quality":0.0880025447,"ml-security":0.0480761963}}
{"text":"However, multi-modality 3D object retrieval is rarely developed and analyzed on large-scale datasets.","meta":{"url":"http://arxiv.org/abs/2307.10601v1"},"cats":{"new-dataset":0.1573632581,"dev-research":0.1812674904,"prompt-eng":0.341701993,"data-quality":0.0792816378,"ml-security":0.0644554498}}
{"text":"In this paper, we propose self-and-cross attention based aggregation of point cloud and multi-view images (SCA-PVNet) for 3D object retrieval.","meta":{"url":"http://arxiv.org/abs/2307.10601v1"},"cats":{"new-dataset":0.1070207751,"dev-research":0.1880115577,"prompt-eng":0.3771371414,"data-quality":0.1038898018,"ml-security":0.0580982168}}
{"text":"With deep features extracted from point clouds and multi-view images, we design two types of feature aggregation modules, namely the In-Modality Aggregation Module (IMAM) and the Cross-Modality Aggregation Module (CMAM), for effective feature fusion.","meta":{"url":"http://arxiv.org/abs/2307.10601v1"},"cats":{"new-dataset":0.1665224105,"dev-research":0.2582781828,"prompt-eng":0.3661235134,"data-quality":0.1324804627,"ml-security":0.078424846}}
{"text":"IMAM leverages a self-attention mechanism to aggregate multi-view features while CMAM exploits a cross-attention mechanism to interact point cloud features with multi-view features.","meta":{"url":"http://arxiv.org/abs/2307.10601v1"},"cats":{"new-dataset":0.0821750934,"dev-research":0.2847837099,"prompt-eng":0.4338480257,"data-quality":0.1144001757,"ml-security":0.1018246646}}
{"text":"The final descriptor of a 3D object for object retrieval can be obtained via concatenating the aggregated features from both modules.","meta":{"url":"http://arxiv.org/abs/2307.10601v1"},"cats":{"new-dataset":0.1140054738,"dev-research":0.2304554592,"prompt-eng":0.3854679591,"data-quality":0.0927104091,"ml-security":0.0465484334}}
{"text":"Extensive experiments and analysis are conducted on three datasets, ranging from small to large scale, to show the superiority of the proposed SCA-PVNet over the state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.10601v1"},"cats":{"new-dataset":0.0623698192,"dev-research":0.2238709862,"prompt-eng":0.3750001111,"data-quality":0.1262916047,"ml-security":0.0846871485}}
{"text":"Artificial Intelligence (AI)'s pervasive presence and variety necessitate diversity and inclusivity (D&I) principles in its design for fairness, trust, and transparency.","meta":{"url":"http://arxiv.org/abs/2307.10600v1"},"cats":{"new-dataset":0.0484855855,"dev-research":0.3144833796,"prompt-eng":0.3562823677,"data-quality":0.0998367756,"ml-security":0.3140164886}}
{"text":"Yet, these considerations are often overlooked, leading to issues of bias, discrimination, and perceived untrustworthiness.","meta":{"url":"http://arxiv.org/abs/2307.10600v1"},"cats":{"new-dataset":0.0099304197,"dev-research":0.3493877179,"prompt-eng":0.3614446098,"data-quality":0.2848526377,"ml-security":0.2612079966}}
{"text":"In response, we conducted a Systematic Review to unearth challenges and solutions relating to D&I in AI.","meta":{"url":"http://arxiv.org/abs/2307.10600v1"},"cats":{"new-dataset":0.0789028402,"dev-research":0.4363766675,"prompt-eng":0.381820373,"data-quality":0.1380295422,"ml-security":0.1193602275}}
{"text":"Our rigorous search yielded 48 research articles published between 2017 and 2022.","meta":{"url":"http://arxiv.org/abs/2307.10600v1"},"cats":{"new-dataset":0.1724329165,"dev-research":0.2473610989,"prompt-eng":0.3566520044,"data-quality":0.1021235918,"ml-security":0.0740088074}}
{"text":"Open coding of these papers revealed 55 unique challenges and 33 solutions for D&I in AI, as well as 24 unique challenges and 23 solutions for enhancing such practices using AI.","meta":{"url":"http://arxiv.org/abs/2307.10600v1"},"cats":{"new-dataset":0.1797984097,"dev-research":0.4381314961,"prompt-eng":0.375366972,"data-quality":0.1334087663,"ml-security":0.1488211393}}
{"text":"This study, by offering a deeper understanding of these issues, will enlighten researchers and practitioners seeking to integrate these principles into future AI systems.","meta":{"url":"http://arxiv.org/abs/2307.10600v1"},"cats":{"new-dataset":0.0377806225,"dev-research":0.3776498959,"prompt-eng":0.3783579787,"data-quality":0.0969004693,"ml-security":0.2337132479}}
{"text":"The Internet of Things (IoT) integrates more than billions of intelligent devices over the globe with the capability of communicating with other connected devices with little to no human intervention.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.055901955,"dev-research":0.3246932775,"prompt-eng":0.4115738974,"data-quality":0.0581013651,"ml-security":0.1031478142}}
{"text":"IoT enables data aggregation and analysis on a large scale to improve life quality in many domains.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.2222365219,"dev-research":0.3670771862,"prompt-eng":0.3545800431,"data-quality":0.0705332782,"ml-security":0.0730342746}}
{"text":"In particular, data collected by IoT contain a tremendous amount of information for anomaly detection.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.1599568498,"dev-research":0.340661838,"prompt-eng":0.4098007153,"data-quality":0.137385259,"ml-security":0.3385210759}}
{"text":"The heterogeneous nature of IoT is both a challenge and an opportunity for cybersecurity.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.0329663518,"dev-research":0.2810040085,"prompt-eng":0.3842947958,"data-quality":0.0489446646,"ml-security":0.2435375412}}
{"text":"Traditional approaches in cybersecurity monitoring often require different kinds of data pre-processing and handling for various data types, which might be problematic for datasets that contain heterogeneous features.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.1382767982,"dev-research":0.3881238584,"prompt-eng":0.3519129695,"data-quality":0.1333149785,"ml-security":0.4460937617}}
{"text":"However, heterogeneous types of network devices can often capture a more diverse set of signals than a single type of device readings, which is particularly useful for anomaly detection.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.025227266,"dev-research":0.3354200682,"prompt-eng":0.3750452385,"data-quality":0.1684089939,"ml-security":0.2311275279}}
{"text":"In this paper, we present a comprehensive study on using ensemble machine learning methods for enhancing IoT cybersecurity via anomaly detection.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.0767599485,"dev-research":0.3014022867,"prompt-eng":0.4058692693,"data-quality":0.1635858548,"ml-security":0.481263637}}
{"text":"Rather than using one single machine learning model, ensemble learning combines the predictive power from multiple models, enhancing their predictive accuracy in heterogeneous datasets rather than using one single machine learning model.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.0242415913,"dev-research":0.2661962251,"prompt-eng":0.3491383153,"data-quality":0.1413380074,"ml-security":0.212555537}}
{"text":"We propose a unified framework with ensemble learning that utilises Bayesian hyperparameter optimisation to adapt to a network environment that contains multiple IoT sensor readings.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.0923200148,"dev-research":0.2407274524,"prompt-eng":0.4452154676,"data-quality":0.1205459793,"ml-security":0.1791279014}}
{"text":"Experimentally, we illustrate their high predictive power when compared to traditional methods.","meta":{"url":"http://arxiv.org/abs/2307.10596v1"},"cats":{"new-dataset":0.015016298,"dev-research":0.2971645412,"prompt-eng":0.4638020359,"data-quality":0.0964220239,"ml-security":0.1830549041}}
{"text":"A key challenge in Bayesian decentralized data fusion is the `rumor propagation' or `double counting' phenomenon, where previously sent data circulates back to its sender.","meta":{"url":"http://arxiv.org/abs/2307.10594v1"},"cats":{"new-dataset":0.1631888967,"dev-research":0.241898447,"prompt-eng":0.4051125981,"data-quality":0.2024723848,"ml-security":0.2228273744}}
{"text":"It is often addressed by approximate methods like covariance intersection (CI) which takes a weighted average of the estimates to compute the bound.","meta":{"url":"http://arxiv.org/abs/2307.10594v1"},"cats":{"new-dataset":0.034732255,"dev-research":0.2256853723,"prompt-eng":0.347215911,"data-quality":0.1237040049,"ml-security":0.081191955}}
{"text":"The problem is that this bound is not tight, i.e. the estimate is often over-conservative.","meta":{"url":"http://arxiv.org/abs/2307.10594v1"},"cats":{"new-dataset":0.0561564469,"dev-research":0.1924757831,"prompt-eng":0.3257776546,"data-quality":0.1307897245,"ml-security":0.1403242546}}
{"text":"In this paper, we show that by exploiting the probabilistic independence structure in multi-agent decentralized fusion problems a tighter bound can be found using (i) an expansion to the CI algorithm that uses multiple (non-monolithic) weighting factors instead of one (monolithic) factor in the original CI and (ii) a general optimization scheme that is able to compute optimal bounds and fully exploit an arbitrary dependency structure.","meta":{"url":"http://arxiv.org/abs/2307.10594v1"},"cats":{"new-dataset":0.0168052105,"dev-research":0.2028787525,"prompt-eng":0.3647573531,"data-quality":0.0805004428,"ml-security":0.0868946527}}
{"text":"We compare our methods and show that on a simple problem, they converge to the same solution.","meta":{"url":"http://arxiv.org/abs/2307.10594v1"},"cats":{"new-dataset":0.0272709573,"dev-research":0.2979654571,"prompt-eng":0.337292891,"data-quality":0.157067612,"ml-security":0.103508231}}
{"text":"We then test our new non-monolithic CI algorithm on a large-scale target tracking simulation and show that it achieves a tighter bound and a more accurate estimate compared to the original monolithic CI.","meta":{"url":"http://arxiv.org/abs/2307.10594v1"},"cats":{"new-dataset":0.084421329,"dev-research":0.243338029,"prompt-eng":0.3726995911,"data-quality":0.1045945443,"ml-security":0.0919066568}}
{"text":"Event-based cameras have become increasingly popular for tracking fast-moving objects due to their high temporal resolution, low latency, and high dynamic range.","meta":{"url":"http://arxiv.org/abs/2307.10593v1"},"cats":{"new-dataset":0.1937851759,"dev-research":0.2598460668,"prompt-eng":0.3752175193,"data-quality":0.0574810641,"ml-security":0.0415175712}}
{"text":"In this paper, we propose a novel algorithm for tracking event blobs using raw events asynchronously in real time.","meta":{"url":"http://arxiv.org/abs/2307.10593v1"},"cats":{"new-dataset":0.2829065706,"dev-research":0.2765539467,"prompt-eng":0.3540245508,"data-quality":0.1398914991,"ml-security":0.0784086868}}
{"text":"We introduce the concept of an event blob as a spatio-temporal likelihood of event occurrence where the conditional spatial likelihood is blob-like.","meta":{"url":"http://arxiv.org/abs/2307.10593v1"},"cats":{"new-dataset":0.1835536049,"dev-research":0.252615525,"prompt-eng":0.4419587933,"data-quality":0.1273834831,"ml-security":0.0920659277}}
{"text":"Many real-world objects generate event blob data, for example, flickering LEDs such as car headlights or any small foreground object moving against a static or slowly varying background.","meta":{"url":"http://arxiv.org/abs/2307.10593v1"},"cats":{"new-dataset":0.2726401437,"dev-research":0.3447224884,"prompt-eng":0.4214057296,"data-quality":0.1499433517,"ml-security":0.1935415738}}
{"text":"The proposed algorithm uses a nearest neighbour classifier with a dynamic threshold criteria for data association coupled with a Kalman filter to track the event blob state.","meta":{"url":"http://arxiv.org/abs/2307.10593v1"},"cats":{"new-dataset":0.1478046258,"dev-research":0.2471240288,"prompt-eng":0.3955423163,"data-quality":0.1761422183,"ml-security":0.1056905137}}
{"text":"Our algorithm achieves highly accurate tracking and event blob shape estimation even under challenging lighting conditions and high-speed motions.","meta":{"url":"http://arxiv.org/abs/2307.10593v1"},"cats":{"new-dataset":0.1171527046,"dev-research":0.2596551693,"prompt-eng":0.3705674861,"data-quality":0.0894375459,"ml-security":0.0490025054}}
{"text":"The microsecond time resolution achieved means that the filter output can be used to derive secondary information such as time-to-contact or range estimation, that will enable applications to real-world problems such as collision avoidance in autonomous driving.","meta":{"url":"http://arxiv.org/abs/2307.10593v1"},"cats":{"new-dataset":0.0472444062,"dev-research":0.2661932744,"prompt-eng":0.3865213369,"data-quality":0.0696389243,"ml-security":0.0715384596}}
{"text":"Recent advances in Deep Neural Networks (DNNs) and sensor technologies are enabling autonomous driving systems (ADSs) with an ever-increasing level of autonomy.","meta":{"url":"http://arxiv.org/abs/2307.10590v1"},"cats":{"new-dataset":0.1125988222,"dev-research":0.300243575,"prompt-eng":0.3533499522,"data-quality":0.1157024392,"ml-security":0.2378254767}}
{"text":"However, assessing their dependability remains a critical concern.","meta":{"url":"http://arxiv.org/abs/2307.10590v1"},"cats":{"new-dataset":0.0670126121,"dev-research":0.3445163847,"prompt-eng":0.3762476302,"data-quality":0.221267542,"ml-security":0.1341999675}}
{"text":"State-of-the-art ADS testing approaches modify the controllable attributes of a simulated driving environment until the ADS misbehaves.","meta":{"url":"http://arxiv.org/abs/2307.10590v1"},"cats":{"new-dataset":0.0312073527,"dev-research":0.3225169602,"prompt-eng":0.4345962811,"data-quality":0.218887992,"ml-security":0.268182793}}
{"text":"Such approaches have two main drawbacks: (1) modifications to the simulated environment might not be easily transferable to the in-field test setting (e.g., changing the road shape); (2) environment instances in which the ADS is successful are discarded, despite the possibility that they could contain hidden driving conditions in which the ADS may misbehave.   ","meta":{"url":"http://arxiv.org/abs/2307.10590v1"},"cats":{"new-dataset":0.0245163944,"dev-research":0.2672356326,"prompt-eng":0.381702556,"data-quality":0.1303585537,"ml-security":0.2232357483}}
{"text":"In this paper, we present GenBo (GENerator of BOundary state pairs), a novel test generator for ADS testing.","meta":{"url":"http://arxiv.org/abs/2307.10590v1"},"cats":{"new-dataset":0.0652303304,"dev-research":0.2335040057,"prompt-eng":0.4250810438,"data-quality":0.171180759,"ml-security":0.1625726476}}
{"text":"GenBo mutates the driving conditions of the ego vehicle (position, velocity and orientation), collected in a failure-free environment instance, and efficiently generates challenging driving conditions at the behavior boundary (i.e., where the model starts to misbehave) in the same environment.","meta":{"url":"http://arxiv.org/abs/2307.10590v1"},"cats":{"new-dataset":0.0316321035,"dev-research":0.2541784388,"prompt-eng":0.4479437702,"data-quality":0.131276488,"ml-security":0.1652525738}}
{"text":"We use such boundary conditions to augment the initial training dataset and retrain the DNN model under test.","meta":{"url":"http://arxiv.org/abs/2307.10590v1"},"cats":{"new-dataset":0.187696893,"dev-research":0.2670775822,"prompt-eng":0.4095152913,"data-quality":0.1968712755,"ml-security":0.2418575534}}
{"text":"Our evaluation results show that the retrained model has up to 16 higher success rate on a separate set of evaluation tracks with respect to the original DNN model.","meta":{"url":"http://arxiv.org/abs/2307.10590v1"},"cats":{"new-dataset":0.0954041913,"dev-research":0.3089947159,"prompt-eng":0.4196883298,"data-quality":0.1899749606,"ml-security":0.0772401931}}
{"text":"Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation.","meta":{"url":"http://arxiv.org/abs/2307.10588v1"},"cats":{"new-dataset":0.0251264605,"dev-research":0.3218743916,"prompt-eng":0.3849706137,"data-quality":0.1196201453,"ml-security":0.1552617887}}
{"text":"Transportation electrification is being promoted worldwide to reduce emissions.","meta":{"url":"http://arxiv.org/abs/2307.10588v1"},"cats":{"new-dataset":0.0473510441,"dev-research":0.2864067727,"prompt-eng":0.3920962765,"data-quality":0.1209183386,"ml-security":0.0956071729}}
{"text":"As a result, many automakers will soon start making only battery electric vehicles (BEVs).","meta":{"url":"http://arxiv.org/abs/2307.10588v1"},"cats":{"new-dataset":0.0473830685,"dev-research":0.2752036852,"prompt-eng":0.4009851666,"data-quality":0.1207783889,"ml-security":0.072985797}}
{"text":"BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns.","meta":{"url":"http://arxiv.org/abs/2307.10588v1"},"cats":{"new-dataset":0.096696361,"dev-research":0.3354262931,"prompt-eng":0.4055812048,"data-quality":0.1005415344,"ml-security":0.0667354805}}
{"text":"While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages.","meta":{"url":"http://arxiv.org/abs/2307.10588v1"},"cats":{"new-dataset":0.0486502361,"dev-research":0.2962339083,"prompt-eng":0.3834960651,"data-quality":0.1534141705,"ml-security":0.1699303597}}
{"text":"This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively.","meta":{"url":"http://arxiv.org/abs/2307.10588v1"},"cats":{"new-dataset":0.1740458259,"dev-research":0.2750945185,"prompt-eng":0.3916351784,"data-quality":0.1609627076,"ml-security":0.1740135488}}
{"text":"The MCDNN is configured using a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a total of 1570167 vehicle miles traveled.","meta":{"url":"http://arxiv.org/abs/2307.10588v1"},"cats":{"new-dataset":0.4109648194,"dev-research":0.2486401539,"prompt-eng":0.3780647844,"data-quality":0.1299150717,"ml-security":0.1210935882}}
{"text":"The numerical findings revealed that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.","meta":{"url":"http://arxiv.org/abs/2307.10588v1"},"cats":{"new-dataset":0.0799895289,"dev-research":0.2666345549,"prompt-eng":0.4129971857,"data-quality":0.1782153213,"ml-security":0.2510620937}}
{"text":"Automatic speech recognition (ASR) systems are designed to transcribe spoken language into written text and find utility in a variety of applications including voice assistants and transcription services.","meta":{"url":"http://arxiv.org/abs/2307.10587v1"},"cats":{"new-dataset":0.11072628,"dev-research":0.26602304,"prompt-eng":0.4343196221,"data-quality":0.2399557357,"ml-security":0.0817018734}}
{"text":"However, it has been observed that state-of-the-art ASR systems which deliver impressive benchmark results, struggle with speakers of certain regions or demographics due to variation in their speech properties.","meta":{"url":"http://arxiv.org/abs/2307.10587v1"},"cats":{"new-dataset":0.1003134571,"dev-research":0.2892666909,"prompt-eng":0.407755621,"data-quality":0.2297736106,"ml-security":0.071918054}}
{"text":"In this work, we describe the curation of a massive speech dataset of 8740 hours consisting of $\\sim9.8$K technical lectures in the English language along with their transcripts delivered by instructors representing various parts of Indian demography.","meta":{"url":"http://arxiv.org/abs/2307.10587v1"},"cats":{"new-dataset":0.8217181757,"dev-research":0.2380769868,"prompt-eng":0.3455006347,"data-quality":0.1889185048,"ml-security":0.1341692244}}
{"text":"The dataset is sourced from the very popular NPTEL MOOC platform.","meta":{"url":"http://arxiv.org/abs/2307.10587v1"},"cats":{"new-dataset":0.9198513753,"dev-research":0.2506331177,"prompt-eng":0.3393798005,"data-quality":0.0980658493,"ml-security":0.1371334313}}
{"text":"We use the curated dataset to measure the existing disparity in YouTube Automatic Captions and OpenAI Whisper model performance across the diverse demographic traits of speakers in India.","meta":{"url":"http://arxiv.org/abs/2307.10587v1"},"cats":{"new-dataset":0.3593040599,"dev-research":0.2898624117,"prompt-eng":0.3750041611,"data-quality":0.2808444995,"ml-security":0.0838598604}}
{"text":"While there exists disparity due to gender, native region, age and speech rate of speakers, disparity based on caste is non-existent.","meta":{"url":"http://arxiv.org/abs/2307.10587v1"},"cats":{"new-dataset":0.0632760994,"dev-research":0.268416462,"prompt-eng":0.2803937577,"data-quality":0.181295398,"ml-security":0.0785597144}}
{"text":"We also observe statistically significant disparity across the disciplines of the lectures.","meta":{"url":"http://arxiv.org/abs/2307.10587v1"},"cats":{"new-dataset":0.0645524986,"dev-research":0.2644954285,"prompt-eng":0.390274934,"data-quality":0.1856008555,"ml-security":0.065264533}}
{"text":"These results indicate the need of more inclusive and robust ASR systems and more representational datasets for disparity evaluation in them.","meta":{"url":"http://arxiv.org/abs/2307.10587v1"},"cats":{"new-dataset":0.1532130847,"dev-research":0.2902546078,"prompt-eng":0.3634539697,"data-quality":0.2442099118,"ml-security":0.1012308472}}
{"text":"As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged.","meta":{"url":"http://arxiv.org/abs/2307.10586v1"},"cats":{"new-dataset":0.0496349798,"dev-research":0.3419721387,"prompt-eng":0.4016475486,"data-quality":0.2675817441,"ml-security":0.6191909892}}
{"text":"Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10586v1"},"cats":{"new-dataset":0.0452480503,"dev-research":0.4163080647,"prompt-eng":0.431888443,"data-quality":0.2899256437,"ml-security":0.7782716513}}
{"text":"This paper proposes a holistic assessment methodology for the reliability of ML systems.","meta":{"url":"http://arxiv.org/abs/2307.10586v1"},"cats":{"new-dataset":0.035415039,"dev-research":0.32717361,"prompt-eng":0.4454504352,"data-quality":0.3010143988,"ml-security":0.1696578768}}
{"text":"Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection.","meta":{"url":"http://arxiv.org/abs/2307.10586v1"},"cats":{"new-dataset":0.1163232058,"dev-research":0.2296828581,"prompt-eng":0.4015614201,"data-quality":0.3629497158,"ml-security":0.5259366288}}
{"text":"A reliability score is also introduced and used to assess the overall system reliability.","meta":{"url":"http://arxiv.org/abs/2307.10586v1"},"cats":{"new-dataset":0.043692476,"dev-research":0.3815575138,"prompt-eng":0.4361556792,"data-quality":0.1976008873,"ml-security":0.0508590512}}
{"text":"To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using our proposed reliability metrics and reliability score.","meta":{"url":"http://arxiv.org/abs/2307.10586v1"},"cats":{"new-dataset":0.0207058892,"dev-research":0.3189563847,"prompt-eng":0.3805687285,"data-quality":0.1769513838,"ml-security":0.0591961267}}
{"text":"Our analysis of over 500 models reveals that designing for one metric does not necessarily constrain others but certain algorithmic techniques can improve reliability across multiple metrics simultaneously.","meta":{"url":"http://arxiv.org/abs/2307.10586v1"},"cats":{"new-dataset":0.0277312748,"dev-research":0.2912731093,"prompt-eng":0.392627962,"data-quality":0.1548857697,"ml-security":0.1024035925}}
{"text":"This study contributes to a more comprehensive understanding of ML reliability and provides a roadmap for future research and development.","meta":{"url":"http://arxiv.org/abs/2307.10586v1"},"cats":{"new-dataset":0.0632574619,"dev-research":0.3105265,"prompt-eng":0.4161155773,"data-quality":0.3326986427,"ml-security":0.1723631986}}
{"text":"Have you ever imagined how it would look if we placed new objects into paintings?","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.0247619535,"dev-research":0.2467024789,"prompt-eng":0.4270807723,"data-quality":0.1075054399,"ml-security":0.0872153493}}
{"text":"For example, what would it look like if we placed a basketball into Claude Monet's ``Water Lilies, Evening Effect''?","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.0384326695,"dev-research":0.2265806043,"prompt-eng":0.4191125602,"data-quality":0.1395265736,"ml-security":0.0814450195}}
{"text":"We propose Reference-based Painterly Inpainting, a novel task that crosses the wild reference domain gap and implants novel objects into artworks.","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.0282423084,"dev-research":0.2595180537,"prompt-eng":0.3887138848,"data-quality":0.1698638847,"ml-security":0.048437279}}
{"text":"Although previous works have examined reference-based inpainting, they are not designed for large domain discrepancies between the target and the reference, such as inpainting an artistic image using a photorealistic reference.","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.0199612847,"dev-research":0.2539897052,"prompt-eng":0.3636350552,"data-quality":0.2498299107,"ml-security":0.0450255832}}
{"text":"This paper proposes a novel diffusion framework, dubbed RefPaint, to ``inpaint more wildly'' by taking such references with large domain gaps.","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.0421719896,"dev-research":0.2370651121,"prompt-eng":0.3341708835,"data-quality":0.1325584684,"ml-security":0.0689851275}}
{"text":"Built with an image-conditioned diffusion model, we introduce a ladder-side branch and a masked fusion mechanism to work with the inpainting mask.","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.0104176007,"dev-research":0.1861871688,"prompt-eng":0.4068710973,"data-quality":0.0790156424,"ml-security":0.0939321188}}
{"text":"By decomposing the CLIP image embeddings at inference time, one can manipulate the strength of semantic and style information with ease.","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.0275599772,"dev-research":0.3053721988,"prompt-eng":0.4569612268,"data-quality":0.1851641725,"ml-security":0.0978755476}}
{"text":"Experiments demonstrate that our proposed RefPaint framework produces significantly better results than existing methods.","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.0402667431,"dev-research":0.297969737,"prompt-eng":0.3677811654,"data-quality":0.1715367769,"ml-security":0.0392791599}}
{"text":"Our method enables creative painterly image inpainting with reference objects that would otherwise be difficult to achieve.","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.033929938,"dev-research":0.2856745675,"prompt-eng":0.4086293483,"data-quality":0.1897760147,"ml-security":0.0524870004}}
{"text":"Project page: https://vita-group.github.io/RefPaint/","meta":{"url":"http://arxiv.org/abs/2307.10584v1"},"cats":{"new-dataset":0.2958882458,"dev-research":0.3480416767,"prompt-eng":0.3926444236,"data-quality":0.1102504846,"ml-security":0.0600647346}}
{"text":"Nowadays, botnets have become one of the major threats to cyber security.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.0452667746,"dev-research":0.3615939222,"prompt-eng":0.3941597306,"data-quality":0.1471605392,"ml-security":0.6188560451}}
{"text":"The characteristics of botnets are mainly reflected in bots network behavior and their intercommunication relationships.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.0397551117,"dev-research":0.3416608834,"prompt-eng":0.4035980215,"data-quality":0.1438909582,"ml-security":0.3277948688}}
{"text":"Existing botnet detection methods use flow features or topological features of the communication graph individually and overlook the other type of feature, which affects model performance.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.03087189,"dev-research":0.3808605657,"prompt-eng":0.3603325203,"data-quality":0.1525229221,"ml-security":0.3215109909}}
{"text":"In this paper, we propose a botnet detection model which uses graph convolutional network (GCN) to deeply fuse flow features and topological features for the first time.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.1158682388,"dev-research":0.3239264221,"prompt-eng":0.3292367736,"data-quality":0.1785669097,"ml-security":0.460937761}}
{"text":"We construct communication graphs from network traffic and represent nodes with flow features.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.1699782077,"dev-research":0.334777203,"prompt-eng":0.3690509171,"data-quality":0.1357385591,"ml-security":0.1561660961}}
{"text":"Due to the imbalance of existing public traffic flow datasets, it is impossible to train a GCN model on these datasets.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.1821486943,"dev-research":0.1424253729,"prompt-eng":0.3205874552,"data-quality":0.1608887901,"ml-security":0.268568089}}
{"text":"Therefore, we use a balanced public communication graph dataset to pretrain a GCN model, thereby guaranteeing its capacity for recognizing topological features.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.3352427715,"dev-research":0.2492395463,"prompt-eng":0.364663172,"data-quality":0.1864205539,"ml-security":0.1673158679}}
{"text":"We then feed the communication graph with flow features into the pretrained GCN.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.1415255233,"dev-research":0.2578866548,"prompt-eng":0.4248758255,"data-quality":0.1307724674,"ml-security":0.1168436832}}
{"text":"The output from the last hidden layer is treated as the fusion of flow and topological features.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.0696709803,"dev-research":0.3057587512,"prompt-eng":0.3672537559,"data-quality":0.0916277452,"ml-security":0.1281867882}}
{"text":"Additionally, by adjusting the number of layers in the GCN network, the model can effectively detect botnets operating under both C2 and P2P structures.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.0297118928,"dev-research":0.2605471602,"prompt-eng":0.3782827028,"data-quality":0.1447964916,"ml-security":0.3386839343}}
{"text":"Validated on the public ISCX2014 dataset, our approach achieves a remarkable accuracy of 98.85% and a recall rate of 92.90% for C2 botnets, alongside an accuracy of 99.10% and a recall rate of 94.66% for P2P botnets.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.2651215229,"dev-research":0.3033674118,"prompt-eng":0.3959419954,"data-quality":0.2700104721,"ml-security":0.2888629666}}
{"text":"These results not only demonstrate the efficacy of our method, but also surpass the performance of the currently leading detection models.","meta":{"url":"http://arxiv.org/abs/2307.10583v1"},"cats":{"new-dataset":0.0234615515,"dev-research":0.2404693919,"prompt-eng":0.4224496251,"data-quality":0.2946216609,"ml-security":0.2576749844}}
{"text":"Accurate and timely prediction of sea fog is very important for effectively managing maritime and coastal economic activities.","meta":{"url":"http://arxiv.org/abs/2307.10580v1"},"cats":{"new-dataset":0.0806079263,"dev-research":0.3373762178,"prompt-eng":0.3606225621,"data-quality":0.0802700492,"ml-security":0.1056414305}}
{"text":"Given the intricate nature and inherent variability of sea fog, traditional numerical and statistical forecasting methods are often proven inadequate.","meta":{"url":"http://arxiv.org/abs/2307.10580v1"},"cats":{"new-dataset":0.0625731189,"dev-research":0.2819934724,"prompt-eng":0.3435217333,"data-quality":0.1438246742,"ml-security":0.151413556}}
{"text":"This study aims to develop an advanced sea fog forecasting method embedded in a numerical weather prediction model using the Yangtze River Estuary (YRE) coastal area as a case study.","meta":{"url":"http://arxiv.org/abs/2307.10580v1"},"cats":{"new-dataset":0.1110112069,"dev-research":0.3051077563,"prompt-eng":0.3759327011,"data-quality":0.0670504974,"ml-security":0.111444498}}
{"text":"Prior to training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and decipher the underlying mechanisms driving sea fog occurrence.","meta":{"url":"http://arxiv.org/abs/2307.10580v1"},"cats":{"new-dataset":0.1557299529,"dev-research":0.3310236296,"prompt-eng":0.3580157787,"data-quality":0.1040304525,"ml-security":0.218758589}}
{"text":"In addition, we implement ensemble learning and a focal loss function to address the issue of imbalanced data, thereby enhancing the predictive ability of our model.","meta":{"url":"http://arxiv.org/abs/2307.10580v1"},"cats":{"new-dataset":0.0523487361,"dev-research":0.22095598,"prompt-eng":0.4099144806,"data-quality":0.2171844817,"ml-security":0.3299685493}}
{"text":"To verify the accuracy of our method, we evaluate its performance using a comprehensive dataset spanning one year, which encompasses both weather station observations and historical forecasts.","meta":{"url":"http://arxiv.org/abs/2307.10580v1"},"cats":{"new-dataset":0.5309090881,"dev-research":0.3176383323,"prompt-eng":0.3599746108,"data-quality":0.1395157019,"ml-security":0.0632790021}}
{"text":"Remarkably, our machine learning-based approach surpasses the predictive performance of two conventional methods, the weather research and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL).","meta":{"url":"http://arxiv.org/abs/2307.10580v1"},"cats":{"new-dataset":0.0851396815,"dev-research":0.2356782792,"prompt-eng":0.3869045223,"data-quality":0.0833089693,"ml-security":0.1464481343}}
{"text":"Specifically, in regard to predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, our methodology achieves superior results by increasing the probability of detection (POD) while simultaneously reducing the false alarm ratio (FAR).","meta":{"url":"http://arxiv.org/abs/2307.10580v1"},"cats":{"new-dataset":0.131778541,"dev-research":0.2250354079,"prompt-eng":0.4028072963,"data-quality":0.1371164686,"ml-security":0.1839258591}}
{"text":"SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.066738286,"dev-research":0.2225716053,"prompt-eng":0.3891547292,"data-quality":0.1123393704,"ml-security":0.4849989179}}
{"text":"It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.031468091,"dev-research":0.3347794523,"prompt-eng":0.3469399928,"data-quality":0.0995944154,"ml-security":0.188412447}}
{"text":"However, SecureBoost suffers from high computational complexity and risk of label leakage.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.0341367299,"dev-research":0.2774726181,"prompt-eng":0.4030849628,"data-quality":0.3467825693,"ml-security":0.5611352223}}
{"text":"To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.0154908045,"dev-research":0.2286111643,"prompt-eng":0.4599364001,"data-quality":0.1069775626,"ml-security":0.4527129913}}
{"text":"Existing methods either set hyperparameters empirically or heuristically, which are far from optimal.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.0173977872,"dev-research":0.2407367944,"prompt-eng":0.4494828844,"data-quality":0.1324126382,"ml-security":0.1177167187}}
{"text":"To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.0329652786,"dev-research":0.1865604156,"prompt-eng":0.410104224,"data-quality":0.1190914535,"ml-security":0.5232981452}}
{"text":"We design measurements of the three objectives.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.1157132203,"dev-research":0.2865645994,"prompt-eng":0.4368320988,"data-quality":0.0557161661,"ml-security":0.0490378425}}
{"text":"In particular, the privacy leakage is measured using our proposed instance clustering attack.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.0900389911,"dev-research":0.3136563882,"prompt-eng":0.3886197444,"data-quality":0.2285246067,"ml-security":0.7184099566}}
{"text":"Experimental results demonstrate that the CMOSB yields not only hyperparameters superior to the baseline but also optimal sets of hyperparameters that can support the flexible requirements of FL participants.","meta":{"url":"http://arxiv.org/abs/2307.10579v1"},"cats":{"new-dataset":0.0210522669,"dev-research":0.2555213709,"prompt-eng":0.469140089,"data-quality":0.076623647,"ml-security":0.1038498619}}
{"text":"Traditional computer vision models often require extensive manual effort for data acquisition and validation, particularly when detecting subtle behavioral nuances or events.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.0292859955,"dev-research":0.3947891077,"prompt-eng":0.4759316469,"data-quality":0.2310843749,"ml-security":0.1281874287}}
{"text":"The difficulty in distinguishing routine behaviors from potential risks in real-world applications, like differentiating routine shopping from potential shoplifting, further complicates the process.   ","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.0261531246,"dev-research":0.3803905059,"prompt-eng":0.4049359177,"data-quality":0.1455769795,"ml-security":0.4421655588}}
{"text":"We present Ethosight, a novel zero-shot computer vision algorithm.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.2179908492,"dev-research":0.2078045162,"prompt-eng":0.3132494395,"data-quality":0.1424178558,"ml-security":0.1090890058}}
{"text":"Ethosight eradicates the need for pre-existing symbolic knowledge, initiating from a clean slate based on user requirements and semantic knowledge of interest.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.2072686969,"dev-research":0.4859008507,"prompt-eng":0.4656811364,"data-quality":0.1246595888,"ml-security":0.0650121658}}
{"text":"Using localized label affinity calculations and a reasoning-guided iterative learning loop, Ethosight infers scene details and iteratively refines the label set.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.2298117869,"dev-research":0.2903955477,"prompt-eng":0.4551526804,"data-quality":0.4299103984,"ml-security":0.066266003}}
{"text":"Reasoning mechanisms can be derived from large language models like GPT4, symbolic reasoners like OpenNARS, or hybrid systems.   ","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.0468083078,"dev-research":0.4305923785,"prompt-eng":0.485945763,"data-quality":0.1086274308,"ml-security":0.1129118901}}
{"text":"Ethosight further capitalizes on the capabilities of a pre-trained multi-modal model, ImageBind, generating accurate semantic knowledge of images within a few cycles.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.1524976507,"dev-research":0.2806733688,"prompt-eng":0.4797500274,"data-quality":0.1606771083,"ml-security":0.0498400542}}
{"text":"It successfully captures both explicit and nuanced elements efficiently.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.016803038,"dev-research":0.3808670764,"prompt-eng":0.4147310702,"data-quality":0.2414365809,"ml-security":0.0688637398}}
{"text":"We also introduce the implementation of Korzybski's \"time-binding\" concept in machines, which allows for generational learning and knowledge sharing across deployments.   ","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.2440499021,"dev-research":0.3813237608,"prompt-eng":0.4658990101,"data-quality":0.0828601471,"ml-security":0.1466971501}}
{"text":"Our evaluations demonstrate Ethosight's efficacy across 40 complex use cases.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.0611510723,"dev-research":0.4009676215,"prompt-eng":0.3903759187,"data-quality":0.0804274466,"ml-security":0.1234397854}}
{"text":"It has exhibited an exceptional ability to discern new areas of interest, consistently generating high-affinity scores within the top five labels from a set of a thousand.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.0809021861,"dev-research":0.326161589,"prompt-eng":0.4662438223,"data-quality":0.2232355789,"ml-security":0.0897286995}}
{"text":"Tests conducted across diverse environments attest to Ethosight's robust performance.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.0857198737,"dev-research":0.3743064948,"prompt-eng":0.4214059975,"data-quality":0.1708856601,"ml-security":0.1921937353}}
{"text":"Detailed results and case studies within the main body of this paper and an appendix underscore a promising trajectory towards enhancing the adaptability and resilience of computer vision models in detecting and extracting subtle and nuanced behaviors.","meta":{"url":"http://arxiv.org/abs/2307.10577v1"},"cats":{"new-dataset":0.084337135,"dev-research":0.2988848843,"prompt-eng":0.4223449613,"data-quality":0.2939957368,"ml-security":0.2478938394}}
{"text":"As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data.","meta":{"url":"http://arxiv.org/abs/2307.10575v1"},"cats":{"new-dataset":0.0766083701,"dev-research":0.251114011,"prompt-eng":0.3714535543,"data-quality":0.1323599169,"ml-security":0.3570259241}}
{"text":"However, the heterogeneous data distribution among clients often leads to a decrease in model performance.","meta":{"url":"http://arxiv.org/abs/2307.10575v1"},"cats":{"new-dataset":0.0188229767,"dev-research":0.2855098531,"prompt-eng":0.3407569263,"data-quality":0.1061012664,"ml-security":0.1442279929}}
{"text":"To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution.","meta":{"url":"http://arxiv.org/abs/2307.10575v1"},"cats":{"new-dataset":0.0655302667,"dev-research":0.2339367137,"prompt-eng":0.39140946,"data-quality":0.2917980399,"ml-security":0.1904436749}}
{"text":"Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training.","meta":{"url":"http://arxiv.org/abs/2307.10575v1"},"cats":{"new-dataset":0.015503473,"dev-research":0.3009778723,"prompt-eng":0.4605759156,"data-quality":0.147095498,"ml-security":0.1413106081}}
{"text":"The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg.","meta":{"url":"http://arxiv.org/abs/2307.10575v1"},"cats":{"new-dataset":0.0277463574,"dev-research":0.3036008709,"prompt-eng":0.4620710992,"data-quality":0.234661731,"ml-security":0.0729518841}}
{"text":"Furthermore, our approach has a fast convergence rate in heterogeneous settings.","meta":{"url":"http://arxiv.org/abs/2307.10575v1"},"cats":{"new-dataset":0.0448699453,"dev-research":0.1537781523,"prompt-eng":0.3368811436,"data-quality":0.0794139144,"ml-security":0.0480545993}}
{"text":"Due to complexity and dynamics of construction work, resource, and cash flows, poor management of them usually leads to time and cost overruns, bankruptcy, even project failure.","meta":{"url":"http://arxiv.org/abs/2307.10574v1"},"cats":{"new-dataset":0.0452182268,"dev-research":0.4183428298,"prompt-eng":0.3669201718,"data-quality":0.1704367332,"ml-security":0.115859839}}
{"text":"Existing approaches in construction failed to achieve optimal control of resource flow in a dynamic environment with uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.10574v1"},"cats":{"new-dataset":0.0543883225,"dev-research":0.2933015528,"prompt-eng":0.3674940379,"data-quality":0.1187036801,"ml-security":0.1038232786}}
{"text":"Therefore, this paper introducess a model and method to adaptive control the resource flows to optimize the work and cash flows of construction projects.","meta":{"url":"http://arxiv.org/abs/2307.10574v1"},"cats":{"new-dataset":0.0387277289,"dev-research":0.3301856397,"prompt-eng":0.3724690745,"data-quality":0.0938992491,"ml-security":0.0665692995}}
{"text":"First, a mathematical model based on a partially observable Markov decision process is established to formulate the complex interactions of construction work, resource, and cash flows as well as uncertainty and variability of diverse influence factors.","meta":{"url":"http://arxiv.org/abs/2307.10574v1"},"cats":{"new-dataset":0.0631479809,"dev-research":0.2964488957,"prompt-eng":0.3932569156,"data-quality":0.0866691868,"ml-security":0.0667100796}}
{"text":"Meanwhile, to efficiently find the optimal solutions, a deep reinforcement learning (DRL) based method is introduced to realize the continuous adaptive optimal control of labor and material flows, thereby optimizing the work and cash flows.","meta":{"url":"http://arxiv.org/abs/2307.10574v1"},"cats":{"new-dataset":0.0374391056,"dev-research":0.2251324353,"prompt-eng":0.341967984,"data-quality":0.0706856878,"ml-security":0.102501204}}
{"text":"To assist the training process of DRL, a simulator based on discrete event simulation is also developed to mimic the dynamic features and external environments of a project.","meta":{"url":"http://arxiv.org/abs/2307.10574v1"},"cats":{"new-dataset":0.202251695,"dev-research":0.312021873,"prompt-eng":0.4280154752,"data-quality":0.064735658,"ml-security":0.1252503749}}
{"text":"Experiments in simulated scenarios illustrate that our method outperforms the vanilla empirical method and genetic algorithm, possesses remarkable capability in diverse projects and external environments, and a hybrid agent of DRL and empirical method leads to the best result.","meta":{"url":"http://arxiv.org/abs/2307.10574v1"},"cats":{"new-dataset":0.0502365628,"dev-research":0.3210142015,"prompt-eng":0.4260116435,"data-quality":0.0875199174,"ml-security":0.1111868675}}
{"text":"This paper contributes to adaptive control and optimization of coupled work, resource, and cash flows, and may serve as a step stone for adopting DRL technology in construction project management.","meta":{"url":"http://arxiv.org/abs/2307.10574v1"},"cats":{"new-dataset":0.0603224241,"dev-research":0.3658922563,"prompt-eng":0.3942860574,"data-quality":0.0649155553,"ml-security":0.0464336558}}
{"text":"Language models can be prompted to reason through problems in a manner that significantly improves performance.","meta":{"url":"http://arxiv.org/abs/2307.10573v1"},"cats":{"new-dataset":0.0233041578,"dev-research":0.4819217186,"prompt-eng":0.5190386537,"data-quality":0.2819381234,"ml-security":0.1102151246}}
{"text":"However, \\textit{why} such prompting improves performance is unclear.","meta":{"url":"http://arxiv.org/abs/2307.10573v1"},"cats":{"new-dataset":0.0132861864,"dev-research":0.4404355509,"prompt-eng":0.5469863432,"data-quality":0.2449829624,"ml-security":0.1143706457}}
{"text":"Recent work showed that using logically \\textit{invalid} Chain-of-Thought (CoT) prompting improves performance almost as much as logically \\textit{valid} CoT prompting, and that editing CoT prompts to replace problem-specific information with abstract information or out-of-distribution information typically doesn't harm performance.","meta":{"url":"http://arxiv.org/abs/2307.10573v1"},"cats":{"new-dataset":0.0095584094,"dev-research":0.4446420462,"prompt-eng":0.5169602847,"data-quality":0.329052191,"ml-security":0.1438581469}}
{"text":"Critics have responded that these findings are based on too few and too easy tasks to draw meaningful conclusions.","meta":{"url":"http://arxiv.org/abs/2307.10573v1"},"cats":{"new-dataset":0.0274167868,"dev-research":0.4963257054,"prompt-eng":0.3798600872,"data-quality":0.2550720305,"ml-security":0.0993445403}}
{"text":"To resolve this dispute, we test whether logically invalid CoT prompts offer the same level of performance gains as logically valid prompts on the hardest tasks in the BIG-Bench benchmark, termed BIG-Bench Hard (BBH).","meta":{"url":"http://arxiv.org/abs/2307.10573v1"},"cats":{"new-dataset":0.0214121069,"dev-research":0.3175045301,"prompt-eng":0.4763116557,"data-quality":0.1388885975,"ml-security":0.1178376977}}
{"text":"We find that the logically \\textit{invalid} reasoning prompts do indeed achieve similar performance gains on BBH tasks as logically valid reasoning prompts.","meta":{"url":"http://arxiv.org/abs/2307.10573v1"},"cats":{"new-dataset":0.0113507723,"dev-research":0.3951167081,"prompt-eng":0.5460377213,"data-quality":0.2891506639,"ml-security":0.1127280201}}
{"text":"We also discover that some CoT prompts used by previous works contain logical errors.","meta":{"url":"http://arxiv.org/abs/2307.10573v1"},"cats":{"new-dataset":0.0322676907,"dev-research":0.4503599287,"prompt-eng":0.504942992,"data-quality":0.3286734415,"ml-security":0.1338563885}}
{"text":"This suggests that covariates beyond logically valid reasoning are responsible for performance improvements.","meta":{"url":"http://arxiv.org/abs/2307.10573v1"},"cats":{"new-dataset":0.004959265,"dev-research":0.4534017633,"prompt-eng":0.4161542137,"data-quality":0.1081030755,"ml-security":0.0818624827}}
{"text":"How do people internalize visualizations: as images or information?","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.1575381161,"dev-research":0.4842133291,"prompt-eng":0.4120709861,"data-quality":0.1506774193,"ml-security":0.1013670304}}
{"text":"In this study, we investigate the nature of internalization for visualizations (i.e., how the mind encodes visualizations in memory) and how memory encoding affects its retrieval.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.0782012333,"dev-research":0.3996402377,"prompt-eng":0.4266978122,"data-quality":0.1313779231,"ml-security":0.1176740426}}
{"text":"This exploratory work examines the influence of various design elements on a user's perception of a chart.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.1151048142,"dev-research":0.4465405629,"prompt-eng":0.4147480767,"data-quality":0.1169967458,"ml-security":0.0663589697}}
{"text":"Specifically, which design elements lead to perceptions of visualization as an image or as information?","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.0702299021,"dev-research":0.3992724203,"prompt-eng":0.4112856156,"data-quality":0.0924890573,"ml-security":0.0782410723}}
{"text":"Understanding how design elements contribute to viewers perceiving a visualization more as an image or information will help designers decide which elements to include to achieve their communication goals.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.0485211853,"dev-research":0.4748756444,"prompt-eng":0.4665622266,"data-quality":0.1114703094,"ml-security":0.0783397824}}
{"text":"For this study, we annotated 500 visualizations and analyzed the responses of 250 online participants, who rated the visualizations on a bilinear scale as image or information.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.2704715039,"dev-research":0.4192449218,"prompt-eng":0.3926589565,"data-quality":0.1142969156,"ml-security":0.0589951145}}
{"text":"We then conducted an in-person study (n = 101) using a free recall task to examine how the image/information ratings and design elements impact memory.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.128407937,"dev-research":0.3976797584,"prompt-eng":0.4717991215,"data-quality":0.1292329128,"ml-security":0.0931138328}}
{"text":"The results revealed several interesting findings: Image-rated visualizations were perceived as more aesthetically appealing, enjoyable, and pleasing.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.0796867879,"dev-research":0.4043514448,"prompt-eng":0.421475203,"data-quality":0.1548311109,"ml-security":0.0515699335}}
{"text":"Information-rated visualizations were perceived as less difficult to understand and more aesthetically likable and nice, though participants expressed higher positive sentiment when viewing image-rated visualizations and felt less guided to a conclusion.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.0353621226,"dev-research":0.4592502752,"prompt-eng":0.4030664905,"data-quality":0.2217526175,"ml-security":0.0814418599}}
{"text":"We also found different patterns among participants that were older.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.0547526557,"dev-research":0.3397718163,"prompt-eng":0.3698177088,"data-quality":0.114588926,"ml-security":0.0808896647}}
{"text":"Importantly, we show that visualizations internalized as images are less effective in conveying trends and messages, though they elicit a more positive emotional judgment, while informative visualizations exhibit annotation focused recall and elicit a more positive design judgment.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.1460926804,"dev-research":0.5306656665,"prompt-eng":0.4478889417,"data-quality":0.2500180317,"ml-security":0.0946921214}}
{"text":"We discuss the implications of this dissociation between aesthetic pleasure and perceived ease of use in visualization design.","meta":{"url":"http://arxiv.org/abs/2307.10571v1"},"cats":{"new-dataset":0.0291343448,"dev-research":0.4256332778,"prompt-eng":0.4031000278,"data-quality":0.0902455883,"ml-security":0.0742125477}}
{"text":"Realistic human-centric rendering plays a key role in both computer vision and computer graphics.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.0435188667,"dev-research":0.2937680467,"prompt-eng":0.3863379592,"data-quality":0.0912413887,"ml-security":0.0625753207}}
{"text":"Rapid progress has been made in the algorithm aspect over the years, yet existing human-centric rendering datasets and benchmarks are rather impoverished in terms of diversity, which are crucial for rendering effect.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.3011581979,"dev-research":0.2618917801,"prompt-eng":0.3329880743,"data-quality":0.0739090722,"ml-security":0.0694452528}}
{"text":"Researchers are usually constrained to explore and evaluate a small set of rendering problems on current datasets, while real-world applications require methods to be robust across different scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.1655874019,"dev-research":0.3283658455,"prompt-eng":0.3390105089,"data-quality":0.1508586428,"ml-security":0.1360877246}}
{"text":"In this work, we present DNA-Rendering, a large-scale, high-fidelity repository of human performance data for neural actor rendering.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.2281171697,"dev-research":0.262134823,"prompt-eng":0.3904892721,"data-quality":0.1031427798,"ml-security":0.0811994075}}
{"text":"DNA-Rendering presents several alluring attributes.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.0614175363,"dev-research":0.2891014449,"prompt-eng":0.4263972135,"data-quality":0.0992293153,"ml-security":0.041349088}}
{"text":"First, our dataset contains over 1500 human subjects, 5000 motion sequences, and 67.5M frames' data volume.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.8983535878,"dev-research":0.2263579319,"prompt-eng":0.3454115008,"data-quality":0.0782201217,"ml-security":0.0669642141}}
{"text":"Second, we provide rich assets for each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models, cloth/accessory materials, multi-view images, and videos.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.4278349191,"dev-research":0.216053999,"prompt-eng":0.3930299777,"data-quality":0.0372453276,"ml-security":0.0736702889}}
{"text":"These assets boost the current method's accuracy on downstream rendering tasks.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.0180269649,"dev-research":0.3565157236,"prompt-eng":0.3816489434,"data-quality":0.1035987112,"ml-security":0.0526212531}}
{"text":"Third, we construct a professional multi-view system to capture data, which contains 60 synchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and stern camera calibration steps, ensuring high-quality resources for task training and evaluation.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.5989370136,"dev-research":0.2774548618,"prompt-eng":0.3871097897,"data-quality":0.0769107708,"ml-security":0.0451928287}}
{"text":"Along with the dataset, we provide a large-scale and quantitative benchmark in full-scale, with multiple tasks to evaluate the existing progress of novel view synthesis, novel pose animation synthesis, and novel identity rendering methods.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.3219644755,"dev-research":0.303765312,"prompt-eng":0.3537708733,"data-quality":0.0902232833,"ml-security":0.0981505107}}
{"text":"In this manuscript, we describe our DNA-Rendering effort as a revealing of new observations, challenges, and future directions to human-centric rendering.","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.1364759593,"dev-research":0.3155914842,"prompt-eng":0.4305472865,"data-quality":0.1209809746,"ml-security":0.0426049235}}
{"text":"The dataset, code, and benchmarks will be publicly available at https://dna-rendering.github.io/","meta":{"url":"http://arxiv.org/abs/2307.10173v1"},"cats":{"new-dataset":0.7354165292,"dev-research":0.2647951111,"prompt-eng":0.3699238131,"data-quality":0.1049775542,"ml-security":0.055345388}}
{"text":"Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness.","meta":{"url":"http://arxiv.org/abs/2307.10172v1"},"cats":{"new-dataset":0.416020934,"dev-research":0.2461479779,"prompt-eng":0.3338912553,"data-quality":0.1963645975,"ml-security":0.1291904842}}
{"text":"To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information.","meta":{"url":"http://arxiv.org/abs/2307.10172v1"},"cats":{"new-dataset":0.9074682203,"dev-research":0.3159241064,"prompt-eng":0.3156161504,"data-quality":0.2794985697,"ml-security":0.1156191141}}
{"text":"Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training.","meta":{"url":"http://arxiv.org/abs/2307.10172v1"},"cats":{"new-dataset":0.7286441706,"dev-research":0.2884825452,"prompt-eng":0.3479352767,"data-quality":0.1128012858,"ml-security":0.0831949404}}
{"text":"To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.10172v1"},"cats":{"new-dataset":0.4566058315,"dev-research":0.4399143186,"prompt-eng":0.4362642437,"data-quality":0.1646484558,"ml-security":0.1262412253}}
{"text":"Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-shot and few-shot learning scenarios demonstrate the superiority of DialogStudio.","meta":{"url":"http://arxiv.org/abs/2307.10172v1"},"cats":{"new-dataset":0.4581721203,"dev-research":0.2311286389,"prompt-eng":0.3270037471,"data-quality":0.1882707103,"ml-security":0.2213967982}}
{"text":"To improve transparency and support dataset and task-based research, as well as language model pre-training, all datasets, licenses, codes, and models associated with DialogStudio are made publicly accessible at https://github.com/salesforce/DialogStudio","meta":{"url":"http://arxiv.org/abs/2307.10172v1"},"cats":{"new-dataset":0.6402291627,"dev-research":0.335575587,"prompt-eng":0.3507372446,"data-quality":0.1464196611,"ml-security":0.2035109779}}
{"text":"Movement paths are used widely in intelligent transportation and smart city applications.","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.0442081492,"dev-research":0.2874046873,"prompt-eng":0.3790910773,"data-quality":0.0586099995,"ml-security":0.0607339212}}
{"text":"To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation.","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.05132312,"dev-research":0.2560875649,"prompt-eng":0.4185464049,"data-quality":0.0981780577,"ml-security":0.0722293854}}
{"text":"In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential.","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.0726285278,"dev-research":0.2631143419,"prompt-eng":0.407702659,"data-quality":0.0830806487,"ml-security":0.0992488331}}
{"text":"Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.   ","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.0398599653,"dev-research":0.3149421978,"prompt-eng":0.3989011242,"data-quality":0.1568942267,"ml-security":0.1115735636}}
{"text":"We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability.","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.1265387891,"dev-research":0.3277782009,"prompt-eng":0.4011260445,"data-quality":0.1079326009,"ml-security":0.1088889693}}
{"text":"More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length.","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.0504170514,"dev-research":0.2587623015,"prompt-eng":0.4093754266,"data-quality":0.1320803452,"ml-security":0.1408636335}}
{"text":"Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders.","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.1248130194,"dev-research":0.2920121023,"prompt-eng":0.4156301245,"data-quality":0.1298624363,"ml-security":0.1257311883}}
{"text":"We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders.","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.0492362102,"dev-research":0.2705336767,"prompt-eng":0.3983342257,"data-quality":0.1406774207,"ml-security":0.0897648308}}
{"text":"Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.","meta":{"url":"http://arxiv.org/abs/2307.10171v1"},"cats":{"new-dataset":0.6450219113,"dev-research":0.284981849,"prompt-eng":0.3207729059,"data-quality":0.1236376781,"ml-security":0.1516636603}}
{"text":"Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years.","meta":{"url":"http://arxiv.org/abs/2307.10169v1"},"cats":{"new-dataset":0.1453785554,"dev-research":0.185011501,"prompt-eng":0.4434514886,"data-quality":0.1637356307,"ml-security":0.1793214063}}
{"text":"Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas.","meta":{"url":"http://arxiv.org/abs/2307.10169v1"},"cats":{"new-dataset":0.0982776752,"dev-research":0.3548776208,"prompt-eng":0.3905557981,"data-quality":0.1025152381,"ml-security":0.0864856879}}
{"text":"In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.","meta":{"url":"http://arxiv.org/abs/2307.10169v1"},"cats":{"new-dataset":0.2719609108,"dev-research":0.3294315043,"prompt-eng":0.384112288,"data-quality":0.1488621448,"ml-security":0.1524336321}}
{"text":"LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities.","meta":{"url":"http://arxiv.org/abs/2307.10168v1"},"cats":{"new-dataset":0.0441740554,"dev-research":0.2145382222,"prompt-eng":0.4876942589,"data-quality":0.1232024972,"ml-security":0.1762394018}}
{"text":"However, current efforts focus mainly on simple atomic tasks.","meta":{"url":"http://arxiv.org/abs/2307.10168v1"},"cats":{"new-dataset":0.0101398764,"dev-research":0.3443117237,"prompt-eng":0.4083036111,"data-quality":0.0663187499,"ml-security":0.0481462532}}
{"text":"We explore whether LLMs can replicate more complex crowdsourcing pipelines.","meta":{"url":"http://arxiv.org/abs/2307.10168v1"},"cats":{"new-dataset":0.176526785,"dev-research":0.2523230515,"prompt-eng":0.4756874094,"data-quality":0.1449862284,"ml-security":0.1106148864}}
{"text":"We find that modern LLMs can simulate some of crowdworkers' abilities in these \"human computation algorithms,\" but the level of success is variable and influenced by requesters' understanding of LLM capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks.","meta":{"url":"http://arxiv.org/abs/2307.10168v1"},"cats":{"new-dataset":0.0365234128,"dev-research":0.2009753922,"prompt-eng":0.487470782,"data-quality":0.0510270843,"ml-security":0.1441730679}}
{"text":"We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets.","meta":{"url":"http://arxiv.org/abs/2307.10168v1"},"cats":{"new-dataset":0.040326022,"dev-research":0.2895780539,"prompt-eng":0.5275050056,"data-quality":0.0984974191,"ml-security":0.2672928968}}
{"text":"Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of LLMs on different tasks (by cross-comparing their performances on sub-tasks) and (2) LLMs' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.","meta":{"url":"http://arxiv.org/abs/2307.10168v1"},"cats":{"new-dataset":0.1270024777,"dev-research":0.242496452,"prompt-eng":0.4506896761,"data-quality":0.1197302613,"ml-security":0.1068155723}}
{"text":"Generative Engineering Design approaches driven by Deep Generative Models (DGM) have been proposed to facilitate industrial engineering processes.","meta":{"url":"http://arxiv.org/abs/2307.10166v1"},"cats":{"new-dataset":0.0633290538,"dev-research":0.3139472771,"prompt-eng":0.4941623055,"data-quality":0.0982281161,"ml-security":0.0990426608}}
{"text":"In such processes, designs often come in the form of images, such as blueprints, engineering drawings, and CAD models depending on the level of detail.","meta":{"url":"http://arxiv.org/abs/2307.10166v1"},"cats":{"new-dataset":0.0720495851,"dev-research":0.3347640457,"prompt-eng":0.4554443026,"data-quality":0.0768473277,"ml-security":0.0496703252}}
{"text":"DGMs have been successfully employed for synthesis of natural images, e.g., displaying animals, human faces and landscapes.","meta":{"url":"http://arxiv.org/abs/2307.10166v1"},"cats":{"new-dataset":0.1326049803,"dev-research":0.2912856399,"prompt-eng":0.4493641105,"data-quality":0.10199067,"ml-security":0.071733804}}
{"text":"However, industrial design images are fundamentally different from natural scenes in that they contain rich structural patterns and long-range dependencies, which are challenging for convolution-based DGMs to generate.","meta":{"url":"http://arxiv.org/abs/2307.10166v1"},"cats":{"new-dataset":0.1775549115,"dev-research":0.2863918617,"prompt-eng":0.4122058883,"data-quality":0.1538609164,"ml-security":0.0887407867}}
{"text":"Moreover, DGM-driven generation process is typically triggered based on random noisy inputs, which outputs unpredictable samples and thus cannot perform an efficient industrial design exploration.","meta":{"url":"http://arxiv.org/abs/2307.10166v1"},"cats":{"new-dataset":0.0322504193,"dev-research":0.3003105858,"prompt-eng":0.4906222053,"data-quality":0.1400601743,"ml-security":0.1266548281}}
{"text":"We tackle these challenges by proposing a novel model Self-Attention Adversarial Latent Autoencoder (SA-ALAE), which allows generating feasible design images of complex engineering parts.","meta":{"url":"http://arxiv.org/abs/2307.10166v1"},"cats":{"new-dataset":0.0953288185,"dev-research":0.281464841,"prompt-eng":0.4916467997,"data-quality":0.1368080633,"ml-security":0.2455674366}}
{"text":"With SA-ALAE, users can not only explore novel variants of an existing design, but also control the generation process by operating in latent space.","meta":{"url":"http://arxiv.org/abs/2307.10166v1"},"cats":{"new-dataset":0.0315785439,"dev-research":0.3257199785,"prompt-eng":0.463771513,"data-quality":0.0517021288,"ml-security":0.0763821097}}
{"text":"The potential of SA-ALAE is shown by generating engineering blueprints in a real automotive design task.","meta":{"url":"http://arxiv.org/abs/2307.10166v1"},"cats":{"new-dataset":0.0607873368,"dev-research":0.3414971291,"prompt-eng":0.4327270501,"data-quality":0.0646460565,"ml-security":0.0563218363}}
{"text":"Millions of vehicles are transported every year, tightly parked in vessels or boats.","meta":{"url":"http://arxiv.org/abs/2307.10165v1"},"cats":{"new-dataset":0.2476559263,"dev-research":0.2881241814,"prompt-eng":0.3682062421,"data-quality":0.0977215483,"ml-security":0.1310510096}}
{"text":"To reduce the risks of associated safety issues like fires, knowing the location of vehicles is essential, since different vehicles may need different mitigation measures, e.g. electric cars.","meta":{"url":"http://arxiv.org/abs/2307.10165v1"},"cats":{"new-dataset":0.0317958413,"dev-research":0.3641194577,"prompt-eng":0.3827984286,"data-quality":0.1092536034,"ml-security":0.2351412016}}
{"text":"This work is aimed at creating a solution based on a nano-drone that navigates across rows of parked vehicles and detects their license plates.","meta":{"url":"http://arxiv.org/abs/2307.10165v1"},"cats":{"new-dataset":0.1122762231,"dev-research":0.2422352582,"prompt-eng":0.4113866731,"data-quality":0.1098298181,"ml-security":0.0926471898}}
{"text":"We do so via a wall-following algorithm, and a CNN trained to detect license plates.","meta":{"url":"http://arxiv.org/abs/2307.10165v1"},"cats":{"new-dataset":0.0877551431,"dev-research":0.2608906853,"prompt-eng":0.4179141418,"data-quality":0.2143482462,"ml-security":0.2117396507}}
{"text":"All computations are done in real-time on the drone, which just sends position and detected images that allow the creation of a 2D map with the position of the plates.","meta":{"url":"http://arxiv.org/abs/2307.10165v1"},"cats":{"new-dataset":0.0901091382,"dev-research":0.2708160823,"prompt-eng":0.3993734547,"data-quality":0.0573654199,"ml-security":0.0484049627}}
{"text":"Our solution is capable of reading all plates across eight test cases (with several rows of plates, different drone speeds, or low light) by aggregation of measurements across several drone journeys.","meta":{"url":"http://arxiv.org/abs/2307.10165v1"},"cats":{"new-dataset":0.2268536743,"dev-research":0.2010980502,"prompt-eng":0.4284261214,"data-quality":0.0797284075,"ml-security":0.0547190823}}
{"text":"Most studies of reflecting intelligent surfaces (RISs)-assisted visible light communication (VLC) systems have focused on the integration of RISs in the channel to combat the line-of-sight (LoS) blockage and to enhance the corresponding achievable data rate.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.0777378807,"dev-research":0.2556342112,"prompt-eng":0.3883818859,"data-quality":0.0773429226,"ml-security":0.0608384528}}
{"text":"Some recent efforts have investigated the integration of liquid crystal (LC)-RIS in the VLC receiver to also improve the corresponding achievable data rate.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.0882413896,"dev-research":0.2698341347,"prompt-eng":0.4090401038,"data-quality":0.1314779677,"ml-security":0.0568484272}}
{"text":"To jointly benefit from the previously mentioned appealing capabilities of the RIS technology in both the channel and the receiver, in this work, we propose a novel indoor VLC system that is jointly assisted by a mirror array-based RIS in the channel and an LC-based RIS aided-VLC receiver.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.047400599,"dev-research":0.290127536,"prompt-eng":0.4053114451,"data-quality":0.1097595473,"ml-security":0.0504760555}}
{"text":"To illustrate the performance of the proposed system, a rate maximization problem is formulated, solved, and evaluated.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.0659156462,"dev-research":0.1917710267,"prompt-eng":0.4020464065,"data-quality":0.0939871031,"ml-security":0.0743607361}}
{"text":"This maximization problem jointly optimizes the roll and yaw angles of the mirror array-based RIS as well as the refractive index of the LC-based RIS VLC receiver.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.0341559644,"dev-research":0.2042725797,"prompt-eng":0.3965814746,"data-quality":0.0654864786,"ml-security":0.0425149671}}
{"text":"Moreover, this maximization problem considers practical assumptions, such as the presence of non-users blockers in the LoS path between the transmitter-receiver pair and the user's random device orientation (i.e., the user's self-blockage).","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.0240289703,"dev-research":0.2117040062,"prompt-eng":0.4217827676,"data-quality":0.0907617272,"ml-security":0.2781348726}}
{"text":"Due to the non-convexity of the formulated optimization problem, a low-complexity algorithm is utilized to get the global optimal solution.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.0191852783,"dev-research":0.1781967739,"prompt-eng":0.3311775602,"data-quality":0.0640868756,"ml-security":0.062185991}}
{"text":"A multi-user scenario of the proposed scheme is also presented.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.0383043335,"dev-research":0.2542602066,"prompt-eng":0.4203773429,"data-quality":0.0538581598,"ml-security":0.1188303406}}
{"text":"Furthermore, the energy efficiency of the proposed system is also investigated.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.024770455,"dev-research":0.2052619935,"prompt-eng":0.3732352063,"data-quality":0.0586438578,"ml-security":0.0395504543}}
{"text":"Simulation results are provided, confirming that the proposed system yields a noteworthy improvement in data rate and energy efficiency performances compared to several baseline schemes.","meta":{"url":"http://arxiv.org/abs/2307.10164v1"},"cats":{"new-dataset":0.1323136726,"dev-research":0.222046443,"prompt-eng":0.3715813285,"data-quality":0.0880187928,"ml-security":0.0501465699}}
{"text":"In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation.","meta":{"url":"http://arxiv.org/abs/2307.10163v1"},"cats":{"new-dataset":0.0607720203,"dev-research":0.3306575256,"prompt-eng":0.4628263766,"data-quality":0.1920553273,"ml-security":0.8489886384}}
{"text":"Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.   ","meta":{"url":"http://arxiv.org/abs/2307.10163v1"},"cats":{"new-dataset":0.1032132264,"dev-research":0.4005378104,"prompt-eng":0.4085414603,"data-quality":0.4526237039,"ml-security":0.8289880311}}
{"text":"In this work, we present a different approach to the backdoor attack problem.","meta":{"url":"http://arxiv.org/abs/2307.10163v1"},"cats":{"new-dataset":0.1056441456,"dev-research":0.2356180193,"prompt-eng":0.4019719839,"data-quality":0.1305566632,"ml-security":0.6550404783}}
{"text":"Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to \"detect\" in a general sense.","meta":{"url":"http://arxiv.org/abs/2307.10163v1"},"cats":{"new-dataset":0.0694261337,"dev-research":0.3174288001,"prompt-eng":0.4029219049,"data-quality":0.3245302838,"ml-security":0.8870604372}}
{"text":"Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend.","meta":{"url":"http://arxiv.org/abs/2307.10163v1"},"cats":{"new-dataset":0.0281851844,"dev-research":0.3695234163,"prompt-eng":0.4254367826,"data-quality":0.153660426,"ml-security":0.7911798776}}
{"text":"Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data.","meta":{"url":"http://arxiv.org/abs/2307.10163v1"},"cats":{"new-dataset":0.0794169477,"dev-research":0.3424985212,"prompt-eng":0.4259940959,"data-quality":0.228674135,"ml-security":0.8841690326}}
{"text":"Under this assumption (which we make formal) we develop a new primitive for detecting backdoor attacks.","meta":{"url":"http://arxiv.org/abs/2307.10163v1"},"cats":{"new-dataset":0.0322546832,"dev-research":0.3668320214,"prompt-eng":0.4528067421,"data-quality":0.1684357148,"ml-security":0.638381894}}
{"text":"Our primitive naturally gives rise to a detection algorithm that comes with theoretical guarantees and is effective in practice.","meta":{"url":"http://arxiv.org/abs/2307.10163v1"},"cats":{"new-dataset":0.0359452316,"dev-research":0.322945051,"prompt-eng":0.4057506261,"data-quality":0.179794705,"ml-security":0.2821997252}}
{"text":"When researchers and practitioners are about to start a new project or have just entered a new research field, choosing a proper research topic is always challenging.","meta":{"url":"http://arxiv.org/abs/2307.10162v1"},"cats":{"new-dataset":0.0553265972,"dev-research":0.3190007675,"prompt-eng":0.3851130773,"data-quality":0.0997457395,"ml-security":0.0553634588}}
{"text":"To help them have an overall understanding of the research trend in real-time and find out the research topic they are interested in, we develop the Research Trend Visualization toolkit (RTVis) to analyze and visualize the research paper information.","meta":{"url":"http://arxiv.org/abs/2307.10162v1"},"cats":{"new-dataset":0.3404879471,"dev-research":0.453468363,"prompt-eng":0.3817330045,"data-quality":0.0745836797,"ml-security":0.0565241373}}
{"text":"RTVis consists of a field theme river, a co-occurrence network, a specialized citation bar chart, and a word frequency race diagram, showing the field change through time respectively, cooperating relationship among authors, paper citation numbers in different venues, and the most common words in the abstract part.","meta":{"url":"http://arxiv.org/abs/2307.10162v1"},"cats":{"new-dataset":0.5018807402,"dev-research":0.3441841027,"prompt-eng":0.3970472286,"data-quality":0.1215292464,"ml-security":0.0366947872}}
{"text":"Moreover, RTVis is open source and easy to deploy.","meta":{"url":"http://arxiv.org/abs/2307.10162v1"},"cats":{"new-dataset":0.356911234,"dev-research":0.4131706121,"prompt-eng":0.389665863,"data-quality":0.0592565701,"ml-security":0.0715133997}}
{"text":"The demo of our toolkit and code with detailed documentation are both available online.","meta":{"url":"http://arxiv.org/abs/2307.10162v1"},"cats":{"new-dataset":0.3787073617,"dev-research":0.4792035019,"prompt-eng":0.4262735465,"data-quality":0.1161241844,"ml-security":0.0697156274}}
{"text":"Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment.","meta":{"url":"http://arxiv.org/abs/2307.10160v1"},"cats":{"new-dataset":0.1650346384,"dev-research":0.2227778537,"prompt-eng":0.4152038224,"data-quality":0.1001157965,"ml-security":0.2624207522}}
{"text":"This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors.","meta":{"url":"http://arxiv.org/abs/2307.10160v1"},"cats":{"new-dataset":0.0301307025,"dev-research":0.3485045947,"prompt-eng":0.4011961605,"data-quality":0.129247707,"ml-security":0.3562960642}}
{"text":"In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy.","meta":{"url":"http://arxiv.org/abs/2307.10160v1"},"cats":{"new-dataset":0.0513262189,"dev-research":0.2509356625,"prompt-eng":0.3966926843,"data-quality":0.0975000718,"ml-security":0.1412768607}}
{"text":"By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives.","meta":{"url":"http://arxiv.org/abs/2307.10160v1"},"cats":{"new-dataset":0.0762056175,"dev-research":0.2461556445,"prompt-eng":0.4691394833,"data-quality":0.0893153783,"ml-security":0.1245930395}}
{"text":"We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy.","meta":{"url":"http://arxiv.org/abs/2307.10160v1"},"cats":{"new-dataset":0.0592131657,"dev-research":0.2775338103,"prompt-eng":0.4364673192,"data-quality":0.1551038471,"ml-security":0.2676266823}}
{"text":"Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.","meta":{"url":"http://arxiv.org/abs/2307.10160v1"},"cats":{"new-dataset":0.0536979419,"dev-research":0.2080995519,"prompt-eng":0.427932092,"data-quality":0.126882639,"ml-security":0.3210949585}}
{"text":"In an era where visual content generation is increasingly driven by machine learning, the integration of human feedback into generative models presents significant opportunities for enhancing user experience and output quality.","meta":{"url":"http://arxiv.org/abs/2307.10159v1"},"cats":{"new-dataset":0.0758764016,"dev-research":0.3572053952,"prompt-eng":0.5147762809,"data-quality":0.1810515008,"ml-security":0.0571525527}}
{"text":"This study explores strategies for incorporating iterative human feedback into the generative process of diffusion-based text-to-image models.","meta":{"url":"http://arxiv.org/abs/2307.10159v1"},"cats":{"new-dataset":0.0396555004,"dev-research":0.2676170454,"prompt-eng":0.527228071,"data-quality":0.1696096068,"ml-security":0.054413737}}
{"text":"We propose FABRIC, a training-free approach applicable to a wide range of popular diffusion models, which exploits the self-attention layer present in the most widely used architectures to condition the diffusion process on a set of feedback images.","meta":{"url":"http://arxiv.org/abs/2307.10159v1"},"cats":{"new-dataset":0.0498762212,"dev-research":0.2074957884,"prompt-eng":0.4926802278,"data-quality":0.1314089757,"ml-security":0.1685372544}}
{"text":"To ensure a rigorous assessment of our approach, we introduce a comprehensive evaluation methodology, offering a robust mechanism to quantify the performance of generative visual models that integrate human feedback.","meta":{"url":"http://arxiv.org/abs/2307.10159v1"},"cats":{"new-dataset":0.0327921667,"dev-research":0.3168289375,"prompt-eng":0.5213843043,"data-quality":0.167378812,"ml-security":0.0533117715}}
{"text":"We show that generation results improve over multiple rounds of iterative feedback through exhaustive analysis, implicitly optimizing arbitrary user preferences.","meta":{"url":"http://arxiv.org/abs/2307.10159v1"},"cats":{"new-dataset":0.05047402,"dev-research":0.421635201,"prompt-eng":0.5264931683,"data-quality":0.1137505574,"ml-security":0.0721191947}}
{"text":"The potential applications of these findings extend to fields such as personalized content creation and customization.","meta":{"url":"http://arxiv.org/abs/2307.10159v1"},"cats":{"new-dataset":0.0295072348,"dev-research":0.4149943824,"prompt-eng":0.4490671958,"data-quality":0.1362926212,"ml-security":0.1050934838}}
{"text":"Lip reading is a challenging task that has many potential applications in speech recognition, human-computer interaction, and security systems.","meta":{"url":"http://arxiv.org/abs/2307.10157v1"},"cats":{"new-dataset":0.0997301164,"dev-research":0.2532852705,"prompt-eng":0.3805986928,"data-quality":0.1593983426,"ml-security":0.215664934}}
{"text":"However, existing lip reading systems often suffer from low accuracy due to the limitations of video features.","meta":{"url":"http://arxiv.org/abs/2307.10157v1"},"cats":{"new-dataset":0.055590598,"dev-research":0.2784785232,"prompt-eng":0.3322232474,"data-quality":0.1960171086,"ml-security":0.1092364922}}
{"text":"In this paper, we propose a novel approach that leverages visemes, which are groups of phonetically similar lip shapes, to extract more discriminative and robust video features for lip reading.","meta":{"url":"http://arxiv.org/abs/2307.10157v1"},"cats":{"new-dataset":0.1854647148,"dev-research":0.2511414355,"prompt-eng":0.3800422685,"data-quality":0.1881756447,"ml-security":0.1221744745}}
{"text":"We evaluate our approach on various tasks, including word-level and sentence-level lip reading, and audiovisual speech recognition using the Arman-AV dataset, a largescale Persian corpus.","meta":{"url":"http://arxiv.org/abs/2307.10157v1"},"cats":{"new-dataset":0.5225988587,"dev-research":0.2001574405,"prompt-eng":0.3500458375,"data-quality":0.147618516,"ml-security":0.0572010242}}
{"text":"Our experimental results show that our viseme based approach consistently outperforms the state-of-theart methods in all these tasks.","meta":{"url":"http://arxiv.org/abs/2307.10157v1"},"cats":{"new-dataset":0.0116878813,"dev-research":0.2782072323,"prompt-eng":0.4308120279,"data-quality":0.1065772954,"ml-security":0.0431831754}}
{"text":"The proposed method reduces the lip-reading word error rate (WER) by 9.1% relative to the best previous method.","meta":{"url":"http://arxiv.org/abs/2307.10157v1"},"cats":{"new-dataset":0.0327250112,"dev-research":0.3269149857,"prompt-eng":0.4021383654,"data-quality":0.3221576438,"ml-security":0.0669038795}}
{"text":"Length extrapolation has attracted considerable attention recently since it allows transformers to be tested on longer sequences than those used in training.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.0558606411,"dev-research":0.2545008053,"prompt-eng":0.3912575298,"data-quality":0.111445127,"ml-security":0.115971302}}
{"text":"Previous research has shown that this property can be attained by using carefully designed Relative Positional Encodings (RPEs).","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.0609430709,"dev-research":0.1936229256,"prompt-eng":0.4344346819,"data-quality":0.1735341495,"ml-security":0.0758747586}}
{"text":"While these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.0788958069,"dev-research":0.1947009044,"prompt-eng":0.3526022814,"data-quality":0.2449117488,"ml-security":0.0604445486}}
{"text":"This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.136226639,"dev-research":0.2051288475,"prompt-eng":0.3358267739,"data-quality":0.142491218,"ml-security":0.0692072815}}
{"text":"We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.0189683584,"dev-research":0.1951902515,"prompt-eng":0.3626269774,"data-quality":0.0884079609,"ml-security":0.163738131}}
{"text":"Two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.0466613206,"dev-research":0.3226538016,"prompt-eng":0.4811665471,"data-quality":0.2464610332,"ml-security":0.046637968}}
{"text":"As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.0782083259,"dev-research":0.2060322117,"prompt-eng":0.4251723357,"data-quality":0.1500195535,"ml-security":0.1020882124}}
{"text":"Extensive experiments are conducted on the Wikitext-103, Books, Github, and WikiBook datasets to demonstrate the viability of our discovered conditions.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.2452576334,"dev-research":0.2653597863,"prompt-eng":0.4251711055,"data-quality":0.1400973872,"ml-security":0.1948314677}}
{"text":"We also compare TRF to Empirical Receptive Field (ERF) across different models, showing consistently matched trends on the aforementioned datasets.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.1921182793,"dev-research":0.3101559687,"prompt-eng":0.4217176799,"data-quality":0.1969461073,"ml-security":0.1278202345}}
{"text":"The code is available at https://github.com/OpenNLPLab/Rpe.","meta":{"url":"http://arxiv.org/abs/2307.10156v1"},"cats":{"new-dataset":0.2761074916,"dev-research":0.2283455786,"prompt-eng":0.4081549123,"data-quality":0.1371250883,"ml-security":0.0451420241}}
{"text":"Unsupervised node clustering (or community detection) is a classical graph learning task.","meta":{"url":"http://arxiv.org/abs/2307.10155v1"},"cats":{"new-dataset":0.0409406597,"dev-research":0.2927175487,"prompt-eng":0.3964471753,"data-quality":0.2093591215,"ml-security":0.1567031114}}
{"text":"In this paper, we study algorithms, which exploit the geometry of the graph to identify densely connected substructures, which form clusters or communities.","meta":{"url":"http://arxiv.org/abs/2307.10155v1"},"cats":{"new-dataset":0.1117267573,"dev-research":0.3004518985,"prompt-eng":0.3239042232,"data-quality":0.1616905903,"ml-security":0.0785813981}}
{"text":"Our method implements discrete Ricci curvatures and their associated geometric flows, under which the edge weights of the graph evolve to reveal its community structure.","meta":{"url":"http://arxiv.org/abs/2307.10155v1"},"cats":{"new-dataset":0.0666195384,"dev-research":0.232923151,"prompt-eng":0.348230044,"data-quality":0.1011767483,"ml-security":0.0945950745}}
{"text":"We consider several discrete curvature notions and analyze the utility of the resulting algorithms.","meta":{"url":"http://arxiv.org/abs/2307.10155v1"},"cats":{"new-dataset":0.0282398887,"dev-research":0.1648650387,"prompt-eng":0.3478488765,"data-quality":0.0973032813,"ml-security":0.0706933375}}
{"text":"In contrast to prior literature, we study not only single-membership community detection, where each node belongs to exactly one community, but also mixed-membership community detection, where communities may overlap.","meta":{"url":"http://arxiv.org/abs/2307.10155v1"},"cats":{"new-dataset":0.0314015953,"dev-research":0.2652939425,"prompt-eng":0.3804130541,"data-quality":0.2420600202,"ml-security":0.1257531173}}
{"text":"For the latter, we argue that it is beneficial to perform community detection on the line graph, i.e., the graph's dual.","meta":{"url":"http://arxiv.org/abs/2307.10155v1"},"cats":{"new-dataset":0.0529037665,"dev-research":0.3289821386,"prompt-eng":0.354602057,"data-quality":0.2067356676,"ml-security":0.1460994588}}
{"text":"We provide both theoretical and empirical evidence for the utility of our curvature-based clustering algorithms.","meta":{"url":"http://arxiv.org/abs/2307.10155v1"},"cats":{"new-dataset":0.0232938055,"dev-research":0.2492101516,"prompt-eng":0.3919214474,"data-quality":0.1732382444,"ml-security":0.123085322}}
{"text":"In addition, we give several results on the relationship between the curvature of a graph and that of its dual, which enable the efficient implementation of our proposed mixed-membership community detection approach and which may be of independent interest for curvature-based network analysis.","meta":{"url":"http://arxiv.org/abs/2307.10155v1"},"cats":{"new-dataset":0.0402875337,"dev-research":0.294487353,"prompt-eng":0.3219268786,"data-quality":0.2259142929,"ml-security":0.113063849}}
{"text":"Studying the manipulation of deformable linear objects has significant practical applications in industry, including car manufacturing, textile production, and electronics automation.","meta":{"url":"http://arxiv.org/abs/2307.10153v1"},"cats":{"new-dataset":0.0141406049,"dev-research":0.2340959399,"prompt-eng":0.3830456666,"data-quality":0.0698770497,"ml-security":0.0932889012}}
{"text":"However, deformable linear object manipulation poses a significant challenge in developing planning and control algorithms, due to the precise and continuous control required to effectively manipulate the deformable nature of these objects.","meta":{"url":"http://arxiv.org/abs/2307.10153v1"},"cats":{"new-dataset":0.037264494,"dev-research":0.2461093811,"prompt-eng":0.3735950171,"data-quality":0.0497041144,"ml-security":0.1023455089}}
{"text":"In this paper, we propose a new framework to control and maintain the shape of deformable linear objects with two robot manipulators utilizing environmental contacts.","meta":{"url":"http://arxiv.org/abs/2307.10153v1"},"cats":{"new-dataset":0.0783738787,"dev-research":0.2440264404,"prompt-eng":0.3753685742,"data-quality":0.0594863086,"ml-security":0.106582003}}
{"text":"The framework is composed of a shape planning algorithm which automatically generates appropriate positions to place fixtures, and an object-centered skill engine which includes task and motion planning to control the motion and force of both robots based on the object status.","meta":{"url":"http://arxiv.org/abs/2307.10153v1"},"cats":{"new-dataset":0.0576350752,"dev-research":0.2681503507,"prompt-eng":0.4462182531,"data-quality":0.0357386167,"ml-security":0.0469983931}}
{"text":"The status of the deformable linear object is estimated online utilizing visual as well as force information.","meta":{"url":"http://arxiv.org/abs/2307.10153v1"},"cats":{"new-dataset":0.092501282,"dev-research":0.2499928619,"prompt-eng":0.3911370388,"data-quality":0.0936999948,"ml-security":0.1097743745}}
{"text":"The framework manages to handle a cable routing task in real-world experiments with two Panda robots and especially achieves contact-aware and flexible clip fixing with challenging fixtures.","meta":{"url":"http://arxiv.org/abs/2307.10153v1"},"cats":{"new-dataset":0.065882019,"dev-research":0.2552681016,"prompt-eng":0.4064698533,"data-quality":0.0957854679,"ml-security":0.0639925105}}
{"text":"The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions.","meta":{"url":"http://arxiv.org/abs/2307.10142v1"},"cats":{"new-dataset":0.0264567549,"dev-research":0.2430218085,"prompt-eng":0.4537444763,"data-quality":0.0827389495,"ml-security":0.1261037865}}
{"text":"Well-designed shaping reward can lead to significantly faster learning.","meta":{"url":"http://arxiv.org/abs/2307.10142v1"},"cats":{"new-dataset":0.0126357128,"dev-research":0.3212255768,"prompt-eng":0.4545870384,"data-quality":0.0685913953,"ml-security":0.1576958171}}
{"text":"Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned.","meta":{"url":"http://arxiv.org/abs/2307.10142v1"},"cats":{"new-dataset":0.0053047207,"dev-research":0.3100282969,"prompt-eng":0.4573628436,"data-quality":0.2204239548,"ml-security":0.3283819049}}
{"text":"In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy.","meta":{"url":"http://arxiv.org/abs/2307.10142v1"},"cats":{"new-dataset":0.0130689634,"dev-research":0.2226716928,"prompt-eng":0.4482209543,"data-quality":0.0612632092,"ml-security":0.1464496048}}
{"text":"Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping.","meta":{"url":"http://arxiv.org/abs/2307.10142v1"},"cats":{"new-dataset":0.0141411393,"dev-research":0.2058072601,"prompt-eng":0.4355618958,"data-quality":0.047130694,"ml-security":0.1411981636}}
{"text":"In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot.","meta":{"url":"http://arxiv.org/abs/2307.10142v1"},"cats":{"new-dataset":0.0322528958,"dev-research":0.1967944868,"prompt-eng":0.4103798088,"data-quality":0.0626587414,"ml-security":0.0567442982}}
{"text":"We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed.","meta":{"url":"http://arxiv.org/abs/2307.10142v1"},"cats":{"new-dataset":0.0079112513,"dev-research":0.1720943696,"prompt-eng":0.3494009711,"data-quality":0.0405922946,"ml-security":0.0936359914}}
{"text":"However, the PBRS reward terms are significantly more robust to scaling than typical reward shaping approaches, and thus easier to tune.","meta":{"url":"http://arxiv.org/abs/2307.10142v1"},"cats":{"new-dataset":0.0019769444,"dev-research":0.2599595055,"prompt-eng":0.4353246441,"data-quality":0.0834063175,"ml-security":0.0760014596}}
{"text":"Neural reflectance models are capable of accurately reproducing the spatially-varying appearance of many real-world materials at different scales.","meta":{"url":"http://arxiv.org/abs/2307.10135v1"},"cats":{"new-dataset":0.0670260744,"dev-research":0.2127053038,"prompt-eng":0.432431954,"data-quality":0.1424224746,"ml-security":0.0992062143}}
{"text":"However, existing methods have difficulties handling highly glossy materials.","meta":{"url":"http://arxiv.org/abs/2307.10135v1"},"cats":{"new-dataset":0.0073549379,"dev-research":0.2832669202,"prompt-eng":0.3909406184,"data-quality":0.1744593628,"ml-security":0.0755602263}}
{"text":"To address this problem, we introduce a new neural reflectance model which, compared with existing methods, better preserves not only specular highlights but also fine-grained details.","meta":{"url":"http://arxiv.org/abs/2307.10135v1"},"cats":{"new-dataset":0.1178957159,"dev-research":0.2473021127,"prompt-eng":0.4190768268,"data-quality":0.2352258702,"ml-security":0.0779145351}}
{"text":"To this end, we enhance the neural network performance by encoding input data to frequency space, inspired by NeRF, to better preserve the details.","meta":{"url":"http://arxiv.org/abs/2307.10135v1"},"cats":{"new-dataset":0.0620108095,"dev-research":0.3111180387,"prompt-eng":0.3936282986,"data-quality":0.2034371861,"ml-security":0.2693735622}}
{"text":"Furthermore, we introduce a gradient-based loss and employ it in multiple stages, adaptive to the progress of the learning phase.","meta":{"url":"http://arxiv.org/abs/2307.10135v1"},"cats":{"new-dataset":0.023470443,"dev-research":0.2386526611,"prompt-eng":0.4140275202,"data-quality":0.16680957,"ml-security":0.2182156022}}
{"text":"Lastly, we utilize an optional extension to the decoder network using the Inception module for more accurate yet costly performance.","meta":{"url":"http://arxiv.org/abs/2307.10135v1"},"cats":{"new-dataset":0.0873618297,"dev-research":0.2898057144,"prompt-eng":0.3925711204,"data-quality":0.1842447982,"ml-security":0.1186761215}}
{"text":"We demonstrate the effectiveness of our method using a variety of synthetic and real examples.","meta":{"url":"http://arxiv.org/abs/2307.10135v1"},"cats":{"new-dataset":0.0228129271,"dev-research":0.2489675372,"prompt-eng":0.3811431405,"data-quality":0.1815219347,"ml-security":0.1005558986}}
{"text":"Previous work on Dynamic Complexity has established that there exist dynamic constant-time parallel algorithms for regular tree languages and context-free languages under label or symbol changes.","meta":{"url":"http://arxiv.org/abs/2307.10131v1"},"cats":{"new-dataset":0.075662253,"dev-research":0.2258375979,"prompt-eng":0.3675698544,"data-quality":0.1450002482,"ml-security":0.1082138633}}
{"text":"However, these algorithms were not developed with the goal to minimise work (or, equivalently, the number of processors).","meta":{"url":"http://arxiv.org/abs/2307.10131v1"},"cats":{"new-dataset":0.0138803704,"dev-research":0.2395417902,"prompt-eng":0.2973312228,"data-quality":0.0873264673,"ml-security":0.0876286337}}
{"text":"In fact, their inspection yields the work bounds $O(n^2)$ and $O(n^7)$ per change operation, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10131v1"},"cats":{"new-dataset":0.0238923816,"dev-research":0.2904000522,"prompt-eng":0.3402544338,"data-quality":0.0787829717,"ml-security":0.1282850899}}
{"text":"In this paper, dynamic algorithms for regular tree languages are proposed that generalise the previous algorithms in that they allow unbounded node rank and leaf insertions, while improving the work bound from $O(n^2)$ to $O(n^{\\epsilon})$, for arbitrary $\\epsilon > 0$.","meta":{"url":"http://arxiv.org/abs/2307.10131v1"},"cats":{"new-dataset":0.0675272666,"dev-research":0.2308169188,"prompt-eng":0.352445634,"data-quality":0.1683671792,"ml-security":0.1075101652}}
{"text":"For context-free languages, algorithms with better work bounds (compared with $O(n^7)$) for restricted classes are proposed: for every $\\epsilon > 0$ there are such algorithms for deterministic context-free languages with work bound $O(n^{3+\\epsilon})$ and for visibly pushdown languages with work bound $O(n^{2+\\epsilon})$.","meta":{"url":"http://arxiv.org/abs/2307.10131v1"},"cats":{"new-dataset":0.1197459365,"dev-research":0.2399672967,"prompt-eng":0.3513802047,"data-quality":0.1142090439,"ml-security":0.2396709064}}
{"text":"Facial age estimation has received a lot of attention for its diverse application scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.0822245334,"dev-research":0.3076225371,"prompt-eng":0.364195315,"data-quality":0.0842775066,"ml-security":0.1304798511}}
{"text":"Most existing studies treat each sample equally and aim to reduce the average estimation error for the entire dataset, which can be summarized as General Age Estimation.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.0938400832,"dev-research":0.2803914382,"prompt-eng":0.3287955704,"data-quality":0.1710348474,"ml-security":0.0951561887}}
{"text":"However, due to the long-tailed distribution prevalent in the dataset, treating all samples equally will inevitably bias the model toward the head classes (usually the adult with a majority of samples).","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.0179618222,"dev-research":0.1993852617,"prompt-eng":0.3819962642,"data-quality":0.1533194689,"ml-security":0.244684658}}
{"text":"Driven by this, some works suggest that each class should be treated equally to improve performance in tail classes (with a minority of samples), which can be summarized as Long-tailed Age Estimation.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.130015547,"dev-research":0.233042098,"prompt-eng":0.4025172165,"data-quality":0.1219746256,"ml-security":0.135833638}}
{"text":"However, Long-tailed Age Estimation usually faces a performance trade-off, i.e., achieving improvement in tail classes by sacrificing the head classes.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.0272305601,"dev-research":0.2598904668,"prompt-eng":0.3830185524,"data-quality":0.0816335837,"ml-security":0.1442784249}}
{"text":"In this paper, our goal is to design a unified framework to perform well on both tasks, killing two birds with one stone.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.063407169,"dev-research":0.2738133593,"prompt-eng":0.404987211,"data-quality":0.0686234696,"ml-security":0.1276368611}}
{"text":"To this end, we propose a simple, effective, and flexible training paradigm named GLAE, which is two-fold.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.0328786849,"dev-research":0.2235084724,"prompt-eng":0.4153954909,"data-quality":0.1132249494,"ml-security":0.1172023859}}
{"text":"Our GLAE provides a surprising improvement on Morph II, reaching the lowest MAE and CMAE of 1.14 and 1.27 years, respectively.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.0753386681,"dev-research":0.2346032657,"prompt-eng":0.401480227,"data-quality":0.0976019627,"ml-security":0.0517131964}}
{"text":"Compared to the previous best method, MAE dropped by up to 34%, which is an unprecedented improvement, and for the first time, MAE is close to 1 year old.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.0138257541,"dev-research":0.3003637185,"prompt-eng":0.3845047585,"data-quality":0.1331641303,"ml-security":0.0898051509}}
{"text":"Extensive experiments on other age benchmark datasets, including CACD, MIVIA, and Chalearn LAP 2015, also indicate that GLAE outperforms the state-of-the-art approaches significantly.","meta":{"url":"http://arxiv.org/abs/2307.10129v1"},"cats":{"new-dataset":0.3715766212,"dev-research":0.2660177812,"prompt-eng":0.340901861,"data-quality":0.1252691702,"ml-security":0.0688691079}}
{"text":"Though performed almost effortlessly by humans, segmenting 2D gray-scale or color images in terms of their constituent regions of interest (e.g.~background, objects or portions of objects) constitutes one of the greatest challenges in science and technology as a consequence of the involved dimensionality reduction(3D to 2D), noise, reflections, shades, and occlusions, among many other possible effects.","meta":{"url":"http://arxiv.org/abs/2307.10123v1"},"cats":{"new-dataset":0.0390529153,"dev-research":0.2522106985,"prompt-eng":0.3695561873,"data-quality":0.0948926145,"ml-security":0.0727597932}}
{"text":"While a large number of interesting approaches have been respectively suggested along the last decades, it was mainly with the more recent development of deep learning that more effective and general solutions have been obtained, currently constituting the basic comparison reference for this type of operation.","meta":{"url":"http://arxiv.org/abs/2307.10123v1"},"cats":{"new-dataset":0.0360327308,"dev-research":0.2347428776,"prompt-eng":0.3542691807,"data-quality":0.1318233334,"ml-security":0.2061095408}}
{"text":"Also developed recently, a multiset-based methodology has been described that is capable of encouraging performance that combines spatial accuracy, stability, and robustness while requiring minimal computational resources (hardware and/or training and recognition time).","meta":{"url":"http://arxiv.org/abs/2307.10123v1"},"cats":{"new-dataset":0.3969502095,"dev-research":0.2534330939,"prompt-eng":0.3476476201,"data-quality":0.1024329472,"ml-security":0.0886928335}}
{"text":"The interesting features of the latter methodology mostly follow from the enhanced selectivity and sensitivity, as well as good robustness to data perturbations and outliers, allowed by the coincidence similarity index on which the multiset approach to supervised image segmentation is based.","meta":{"url":"http://arxiv.org/abs/2307.10123v1"},"cats":{"new-dataset":0.2047346717,"dev-research":0.253255961,"prompt-eng":0.3926569488,"data-quality":0.2881623636,"ml-security":0.1615745929}}
{"text":"After describing the deep learning and multiset approaches, the present work develops two comparison experiments between them which are primarily aimed at illustrating their respective main interesting features when applied to the adopted specific type of data and parameter configurations.","meta":{"url":"http://arxiv.org/abs/2307.10123v1"},"cats":{"new-dataset":0.1856865437,"dev-research":0.2581479816,"prompt-eng":0.36049175,"data-quality":0.1001081603,"ml-security":0.1505782368}}
{"text":"While the deep learning approach confirmed its potential for performing image segmentation, the alternative multiset methodology allowed for encouraging accuracy while requiring little computational resources.","meta":{"url":"http://arxiv.org/abs/2307.10123v1"},"cats":{"new-dataset":0.1384506885,"dev-research":0.2654398193,"prompt-eng":0.3370592335,"data-quality":0.1734836878,"ml-security":0.1047644926}}
{"text":"The paper proposes dynamic parallel algorithms for connectivity and bipartiteness of undirected graphs that require constant time and $O(n^{1/2+\\epsilon})$ work on the CRCW PRAM model.","meta":{"url":"http://arxiv.org/abs/2307.10107v1"},"cats":{"new-dataset":0.1119478006,"dev-research":0.1865641027,"prompt-eng":0.3103406301,"data-quality":0.0805014015,"ml-security":0.0852231233}}
{"text":"The work of these algorithms almost matches the work of the $O(\\log n)$ time algorithm for connectivity by Kopelowitz et al.","meta":{"url":"http://arxiv.org/abs/2307.10107v1"},"cats":{"new-dataset":0.0865363852,"dev-research":0.1896007642,"prompt-eng":0.3016057524,"data-quality":0.0912734783,"ml-security":0.0852495537}}
{"text":"(2018) on the EREW PRAM model and the time of the sequential algorithm for bipartiteness by Eppstein et al. (1997).","meta":{"url":"http://arxiv.org/abs/2307.10107v1"},"cats":{"new-dataset":0.0718624098,"dev-research":0.1361077241,"prompt-eng":0.3671224508,"data-quality":0.0822587404,"ml-security":0.0785811312}}
{"text":"In particular, we show that the sparsification technique, which has been used in both mentioned papers, can in principle also be used for constant time algorithms in the CRCW PRAM model, despite the logarithmic depth of sparsification trees.","meta":{"url":"http://arxiv.org/abs/2307.10107v1"},"cats":{"new-dataset":0.0344764247,"dev-research":0.1686658067,"prompt-eng":0.3761376992,"data-quality":0.1332363788,"ml-security":0.1508679261}}
{"text":"Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.0687612912,"dev-research":0.2443806279,"prompt-eng":0.5167985525,"data-quality":0.3134967465,"ml-security":0.0966385467}}
{"text":"Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.0218026659,"dev-research":0.2591614394,"prompt-eng":0.4286060872,"data-quality":0.187111988,"ml-security":0.229808082}}
{"text":"Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.0097483713,"dev-research":0.2200437677,"prompt-eng":0.3400904174,"data-quality":0.069928465,"ml-security":0.1817061794}}
{"text":"This has been an effective strategy to trade-off between storage and training speed with generalization performance.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.0323474007,"dev-research":0.3003067159,"prompt-eng":0.3829763257,"data-quality":0.1006346489,"ml-security":0.1556118733}}
{"text":"However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.0210900383,"dev-research":0.2121361195,"prompt-eng":0.3606813344,"data-quality":0.0740688739,"ml-security":0.2428302063}}
{"text":"In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.0508330013,"dev-research":0.2314639763,"prompt-eng":0.469751693,"data-quality":0.3316549543,"ml-security":0.2301312622}}
{"text":"We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the backward pass, acting as gradient noise.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.0181289551,"dev-research":0.2596238967,"prompt-eng":0.4149225281,"data-quality":0.2559652083,"ml-security":0.2731898039}}
{"text":"GradDrop is sparse and stochastic unlike gradual freezing.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.0703499289,"dev-research":0.2376844294,"prompt-eng":0.3915226499,"data-quality":0.0640724112,"ml-security":0.1058898708}}
{"text":"Extensive experiments on the multilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitive against methods that use additional translated data for intermediate pretraining and outperforms standard fine-tuning and gradual unfreezing.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.1231435383,"dev-research":0.2865076505,"prompt-eng":0.4270413906,"data-quality":0.1440400935,"ml-security":0.0794300129}}
{"text":"A post-analysis shows how GradDrop improves performance with languages it was not trained on, such as under-resourced languages.","meta":{"url":"http://arxiv.org/abs/2307.10098v1"},"cats":{"new-dataset":0.042843978,"dev-research":0.3516400856,"prompt-eng":0.3984262393,"data-quality":0.1424289277,"ml-security":0.1307228711}}
{"text":"Prototype-based classification is a classical method in machine learning, and recently it has achieved remarkable success in semi-supervised semantic segmentation.","meta":{"url":"http://arxiv.org/abs/2307.10097v1"},"cats":{"new-dataset":0.1972823392,"dev-research":0.2457343597,"prompt-eng":0.4296143061,"data-quality":0.2633640228,"ml-security":0.0974337684}}
{"text":"However, the current approach isolates the prototype initialization process from the main training framework, which appears to be unnecessary.","meta":{"url":"http://arxiv.org/abs/2307.10097v1"},"cats":{"new-dataset":0.0397187436,"dev-research":0.3439683572,"prompt-eng":0.4213624915,"data-quality":0.1323916547,"ml-security":0.2076449391}}
{"text":"Furthermore, while the direct use of K-Means algorithm for prototype generation has considered rich intra-class variance, it may not be the optimal solution for the classification task.","meta":{"url":"http://arxiv.org/abs/2307.10097v1"},"cats":{"new-dataset":0.0801627691,"dev-research":0.2743973368,"prompt-eng":0.4263480366,"data-quality":0.1835975173,"ml-security":0.14459315}}
{"text":"To tackle these problems, we propose a novel boundary-refined prototype generation (BRPG) method, which is incorporated into the whole training framework.","meta":{"url":"http://arxiv.org/abs/2307.10097v1"},"cats":{"new-dataset":0.1285031842,"dev-research":0.2956772697,"prompt-eng":0.4615351615,"data-quality":0.0893271662,"ml-security":0.0851406451}}
{"text":"Specifically, our approach samples and clusters high- and low-confidence features separately based on a confidence threshold, aiming to generate prototypes closer to the class boundaries.","meta":{"url":"http://arxiv.org/abs/2307.10097v1"},"cats":{"new-dataset":0.1382052184,"dev-research":0.3260556348,"prompt-eng":0.4943905745,"data-quality":0.215355727,"ml-security":0.1978669327}}
{"text":"Moreover, an adaptive prototype optimization strategy is introduced to make prototype augmentation for categories with scattered feature distributions.","meta":{"url":"http://arxiv.org/abs/2307.10097v1"},"cats":{"new-dataset":0.0223103163,"dev-research":0.2748915619,"prompt-eng":0.464386836,"data-quality":0.1841965985,"ml-security":0.0943555917}}
{"text":"Extensive experiments on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the superiority and scalability of the proposed method, outperforming the current state-of-the-art approaches.","meta":{"url":"http://arxiv.org/abs/2307.10097v1"},"cats":{"new-dataset":0.4061421147,"dev-research":0.2940103592,"prompt-eng":0.35070199,"data-quality":0.1767126264,"ml-security":0.0904124315}}
{"text":"The code is available at xxxxxxxxxxxxxx.","meta":{"url":"http://arxiv.org/abs/2307.10097v1"},"cats":{"new-dataset":0.3842970045,"dev-research":0.3301168155,"prompt-eng":0.4277366452,"data-quality":0.1215860967,"ml-security":0.127752633}}
{"text":"Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations.","meta":{"url":"http://arxiv.org/abs/2307.10093v1"},"cats":{"new-dataset":0.0746867564,"dev-research":0.2376959186,"prompt-eng":0.3613983971,"data-quality":0.152015221,"ml-security":0.1288781125}}
{"text":"However, in certain applications, this invariance property can be too flexible, thus undesirable.","meta":{"url":"http://arxiv.org/abs/2307.10093v1"},"cats":{"new-dataset":0.0048001433,"dev-research":0.2221482951,"prompt-eng":0.3796396164,"data-quality":0.1840814502,"ml-security":0.1544790134}}
{"text":"Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations.","meta":{"url":"http://arxiv.org/abs/2307.10093v1"},"cats":{"new-dataset":0.1260521188,"dev-research":0.2464384098,"prompt-eng":0.3382419448,"data-quality":0.1508871139,"ml-security":0.1045025827}}
{"text":"We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations.","meta":{"url":"http://arxiv.org/abs/2307.10093v1"},"cats":{"new-dataset":0.022994348,"dev-research":0.1778870498,"prompt-eng":0.3601871501,"data-quality":0.0546001152,"ml-security":0.0642978955}}
{"text":"It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance.","meta":{"url":"http://arxiv.org/abs/2307.10093v1"},"cats":{"new-dataset":0.0292711722,"dev-research":0.5193003574,"prompt-eng":0.4525710932,"data-quality":0.1425203238,"ml-security":0.0937805526}}
{"text":"We present theoretical insights into the proposed metric.","meta":{"url":"http://arxiv.org/abs/2307.10093v1"},"cats":{"new-dataset":0.0412557864,"dev-research":0.2472240867,"prompt-eng":0.3701586798,"data-quality":0.1263646724,"ml-security":0.073436431}}
{"text":"We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.","meta":{"url":"http://arxiv.org/abs/2307.10093v1"},"cats":{"new-dataset":0.0589484602,"dev-research":0.2277087458,"prompt-eng":0.3985392769,"data-quality":0.1412132965,"ml-security":0.0736348914}}
{"text":"We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures.","meta":{"url":"http://arxiv.org/abs/2307.10089v1"},"cats":{"new-dataset":0.1810357921,"dev-research":0.3247656342,"prompt-eng":0.3949682207,"data-quality":0.112692672,"ml-security":0.0946923969}}
{"text":"Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings.","meta":{"url":"http://arxiv.org/abs/2307.10089v1"},"cats":{"new-dataset":0.0626034447,"dev-research":0.3105211618,"prompt-eng":0.3782016411,"data-quality":0.1251487124,"ml-security":0.0841021423}}
{"text":"We specifically study how to use what we call geometric and iconic textures.","meta":{"url":"http://arxiv.org/abs/2307.10089v1"},"cats":{"new-dataset":0.0581783749,"dev-research":0.322627183,"prompt-eng":0.4421191021,"data-quality":0.1292917215,"ml-security":0.0514595426}}
{"text":"Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories.","meta":{"url":"http://arxiv.org/abs/2307.10089v1"},"cats":{"new-dataset":0.0301114934,"dev-research":0.3231951604,"prompt-eng":0.4141050809,"data-quality":0.1605219909,"ml-security":0.0727872742}}
{"text":"We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters.","meta":{"url":"http://arxiv.org/abs/2307.10089v1"},"cats":{"new-dataset":0.1078086157,"dev-research":0.3465673709,"prompt-eng":0.4435003411,"data-quality":0.1272240286,"ml-security":0.0525047667}}
{"text":"30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps.","meta":{"url":"http://arxiv.org/abs/2307.10089v1"},"cats":{"new-dataset":0.6220269171,"dev-research":0.4144628011,"prompt-eng":0.4133085098,"data-quality":0.0973070132,"ml-security":0.0581010335}}
{"text":"We then had 150 participants rate these designs for aesthetics.","meta":{"url":"http://arxiv.org/abs/2307.10089v1"},"cats":{"new-dataset":0.1063655287,"dev-research":0.3125413472,"prompt-eng":0.4323894892,"data-quality":0.0977959729,"ml-security":0.0506804048}}
{"text":"Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.","meta":{"url":"http://arxiv.org/abs/2307.10089v1"},"cats":{"new-dataset":0.1024579589,"dev-research":0.2875755772,"prompt-eng":0.4181515901,"data-quality":0.1656369378,"ml-security":0.0569264334}}
{"text":"There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.12771338,"dev-research":0.4544880327,"prompt-eng":0.5479476126,"data-quality":0.1583454977,"ml-security":0.1390843529}}
{"text":"We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.7575417797,"dev-research":0.3623011762,"prompt-eng":0.4000746265,"data-quality":0.085156332,"ml-security":0.1780093653}}
{"text":"The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.7439987566,"dev-research":0.3796410696,"prompt-eng":0.4901509219,"data-quality":0.0996654867,"ml-security":0.126332974}}
{"text":"It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.6370139149,"dev-research":0.3210940481,"prompt-eng":0.3678113408,"data-quality":0.0764075971,"ml-security":0.0770991699}}
{"text":"It contains multi-step tasks that require semantic understanding of language and visual context.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.1038334637,"dev-research":0.428916266,"prompt-eng":0.4598615343,"data-quality":0.1074841872,"ml-security":0.0352285877}}
{"text":"This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.6453733183,"dev-research":0.3074650715,"prompt-eng":0.4082359961,"data-quality":0.1746193884,"ml-security":0.2217821358}}
{"text":"And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls to operate carousel widgets).","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.0279421726,"dev-research":0.3427893483,"prompt-eng":0.385097712,"data-quality":0.0499561392,"ml-security":0.0586021715}}
{"text":"We organize our dataset to encourage robustness analysis of device-control systems, i.e., how well a system performs in the presence of new task descriptions, new applications, or new platform versions.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.5428850216,"dev-research":0.4449045549,"prompt-eng":0.4569416422,"data-quality":0.2288571409,"ml-security":0.3577390101}}
{"text":"We develop two agents and report performance across the dataset.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.3815286002,"dev-research":0.2981302649,"prompt-eng":0.3919626835,"data-quality":0.1326936171,"ml-security":0.0845710906}}
{"text":"The dataset is available at https://github.com/google-research/google-research/tree/master/android_in_the_wild.","meta":{"url":"http://arxiv.org/abs/2307.10088v1"},"cats":{"new-dataset":0.9036611857,"dev-research":0.2307664432,"prompt-eng":0.320870163,"data-quality":0.0850441908,"ml-security":0.1069587378}}
{"text":"With the rapid development of global road transportation, countries worldwide have completed the construction of road networks.","meta":{"url":"http://arxiv.org/abs/2307.10085v1"},"cats":{"new-dataset":0.1118135981,"dev-research":0.2875618988,"prompt-eng":0.3834918982,"data-quality":0.0671256648,"ml-security":0.054461827}}
{"text":"However, the ensuing challenge lies in the maintenance of existing roads.","meta":{"url":"http://arxiv.org/abs/2307.10085v1"},"cats":{"new-dataset":0.0605499398,"dev-research":0.3629513652,"prompt-eng":0.3839532547,"data-quality":0.1662631547,"ml-security":0.112530524}}
{"text":"It is well-known that countries allocate limited budgets to road maintenance projects, and road management departments face difficulties in making scientifically informed maintenance decisions.","meta":{"url":"http://arxiv.org/abs/2307.10085v1"},"cats":{"new-dataset":0.1364327806,"dev-research":0.3906421918,"prompt-eng":0.4036658746,"data-quality":0.1786461902,"ml-security":0.1114190026}}
{"text":"Therefore, integrating various artificial intelligence decision-making techniques to thoroughly explore historical maintenance data and adapt them to the context of road maintenance scientific decision-making has become an urgent issue.","meta":{"url":"http://arxiv.org/abs/2307.10085v1"},"cats":{"new-dataset":0.2959173347,"dev-research":0.3894978907,"prompt-eng":0.3701705476,"data-quality":0.1539299619,"ml-security":0.0770905906}}
{"text":"This integration aims to provide road management departments with more scientific tools and evidence for decision-making.","meta":{"url":"http://arxiv.org/abs/2307.10085v1"},"cats":{"new-dataset":0.0900518607,"dev-research":0.3802322302,"prompt-eng":0.4333744736,"data-quality":0.0858605348,"ml-security":0.0401144841}}
{"text":"The framework proposed in this paper primarily addresses the following four issues: 1) predicting the pavement performance of various routes, 2) determining the prioritization of maintenance routes, 3) making maintenance decisions based on the evaluation of the effects of past maintenance, and considering comprehensive technical and management indicators, and 4) determining the prioritization of maintenance sections based on the maintenance effectiveness and recommended maintenance effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.10085v1"},"cats":{"new-dataset":0.0798778238,"dev-research":0.3383147583,"prompt-eng":0.4118200155,"data-quality":0.0984310898,"ml-security":0.0650278186}}
{"text":"By tackling these four problems, the framework enables intelligent decision-making for the optimal maintenance plan and maintenance sections, taking into account limited funding and historical maintenance management experience.","meta":{"url":"http://arxiv.org/abs/2307.10085v1"},"cats":{"new-dataset":0.1052044563,"dev-research":0.3642243927,"prompt-eng":0.4155230785,"data-quality":0.0974960866,"ml-security":0.0726269867}}
{"text":"A system and testing rig were designed and built to simulate the use of an eversion robot equipped with a radiation sensor to characterise an irradiated pipe prior to decommissioning.","meta":{"url":"http://arxiv.org/abs/2307.10084v1"},"cats":{"new-dataset":0.0649876127,"dev-research":0.2532360626,"prompt-eng":0.4925154691,"data-quality":0.1097466365,"ml-security":0.0883765032}}
{"text":"The magnets were used as dummy radiation sources which were detected by a hall effect sensor mounted in the interior of the robot.","meta":{"url":"http://arxiv.org/abs/2307.10084v1"},"cats":{"new-dataset":0.0327734881,"dev-research":0.2559206057,"prompt-eng":0.4778549993,"data-quality":0.1199383751,"ml-security":0.121208956}}
{"text":"The robot successfully navigated a simple structure with sharp 45{\\deg} and 90{\\deg} swept bends as well as constrictions that were used to model partial blockages.","meta":{"url":"http://arxiv.org/abs/2307.10084v1"},"cats":{"new-dataset":0.0426561161,"dev-research":0.2078177404,"prompt-eng":0.4149301896,"data-quality":0.0525823978,"ml-security":0.0438243929}}
{"text":"Similar subtrajectory search is a finer-grained operator that can better capture the similarities between one query trajectory and a portion of a data trajectory than the traditional similar trajectory search, which requires the two checked trajectories are similar to each other in whole.","meta":{"url":"http://arxiv.org/abs/2307.10082v1"},"cats":{"new-dataset":0.0184665446,"dev-research":0.2448639985,"prompt-eng":0.3765100706,"data-quality":0.0879518368,"ml-security":0.048832107}}
{"text":"Many real applications (e.g., trajectory clustering and trajectory join) utilize similar subtrajectory search as a basic operator.","meta":{"url":"http://arxiv.org/abs/2307.10082v1"},"cats":{"new-dataset":0.0135754536,"dev-research":0.2275983162,"prompt-eng":0.3592166122,"data-quality":0.0643981159,"ml-security":0.0635521375}}
{"text":"It is considered that the time complexity is O(mn^2) for exact algorithms to solve the similar subtrajectory search problem under most trajectory distance functions in the existing studies, where m is the length of the query trajectory and n is the length of the data trajectory.","meta":{"url":"http://arxiv.org/abs/2307.10082v1"},"cats":{"new-dataset":0.0493520165,"dev-research":0.1622625881,"prompt-eng":0.302244895,"data-quality":0.0557561912,"ml-security":0.0611953599}}
{"text":"In this paper, to the best of our knowledge, we are the first to propose an exact algorithm to solve the similar subtrajectory search problem in O(mn) time for most of widely used trajectory distance functions (e.g., WED, DTW, ERP, EDR and Frechet distance).","meta":{"url":"http://arxiv.org/abs/2307.10082v1"},"cats":{"new-dataset":0.0705286562,"dev-research":0.1577328583,"prompt-eng":0.3192003695,"data-quality":0.0585721208,"ml-security":0.0525374919}}
{"text":"Through extensive experiments on three real datasets, we demonstrate the efficiency and effectiveness of our proposed algorithms.","meta":{"url":"http://arxiv.org/abs/2307.10082v1"},"cats":{"new-dataset":0.3824331172,"dev-research":0.2567194979,"prompt-eng":0.3122480792,"data-quality":0.1933869352,"ml-security":0.1305102819}}
{"text":"The problem of reconstructing a sequence of independent and identically distributed symbols from a set of equal size, consecutive, fragments, as well as a dependent reference sequence, is considered.","meta":{"url":"http://arxiv.org/abs/2307.10080v1"},"cats":{"new-dataset":0.217598095,"dev-research":0.1795968074,"prompt-eng":0.3828976006,"data-quality":0.1776503119,"ml-security":0.0898753962}}
{"text":"First, in the regime in which the fragments are relatively long, and typically no fragment appears more than once, the scaling of the failure probability of maximum likelihood reconstruction algorithm is exactly determined for perfect reconstruction and bounded for partial reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.10080v1"},"cats":{"new-dataset":0.0641064261,"dev-research":0.1194846923,"prompt-eng":0.3716775878,"data-quality":0.1351794001,"ml-security":0.0702331005}}
{"text":"Second, the regime in which the fragments are relatively short and repeating fragments abound is characterized.","meta":{"url":"http://arxiv.org/abs/2307.10080v1"},"cats":{"new-dataset":0.0683562979,"dev-research":0.1878773463,"prompt-eng":0.3403798841,"data-quality":0.1084118367,"ml-security":0.0476968619}}
{"text":"A trade-off is stated between the fraction of fragments that cannot be adequately reconstructed vs. the distortion level allowed for the reconstruction of each fragment, while still allowing vanishing failure probability","meta":{"url":"http://arxiv.org/abs/2307.10080v1"},"cats":{"new-dataset":0.0283287014,"dev-research":0.1691264899,"prompt-eng":0.3419149057,"data-quality":0.1665768791,"ml-security":0.0545284347}}
{"text":"The interest in exploring planetary bodies for scientific investigation and in-situ resource utilization is ever-rising.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.2024906721,"dev-research":0.2524478418,"prompt-eng":0.3536626586,"data-quality":0.0732406691,"ml-security":0.0352379094}}
{"text":"Yet, many sites of interest are inaccessible to state-of-the-art planetary exploration robots because of the robots' inability to traverse steep slopes, unstructured terrain, and loose soil.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.0491877352,"dev-research":0.2254014424,"prompt-eng":0.3775030054,"data-quality":0.0867673695,"ml-security":0.0811873377}}
{"text":"Additionally, current single-robot approaches only allow a limited exploration speed and a single set of skills.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.0089230247,"dev-research":0.1980885427,"prompt-eng":0.3472686903,"data-quality":0.032276469,"ml-security":0.0829919825}}
{"text":"Here, we present a team of legged robots with complementary skills for exploration missions in challenging planetary analog environments.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.1446858385,"dev-research":0.230014166,"prompt-eng":0.4325749271,"data-quality":0.0686451968,"ml-security":0.0827202989}}
{"text":"We equipped the robots with an efficient locomotion controller, a mapping pipeline for online and post-mission visualization, instance segmentation to highlight scientific targets, and scientific instruments for remote and in-situ investigation.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.189390314,"dev-research":0.2684345169,"prompt-eng":0.4428078988,"data-quality":0.0842572265,"ml-security":0.0421913506}}
{"text":"Furthermore, we integrated a robotic arm on one of the robots to enable high-precision measurements.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.0611428768,"dev-research":0.2574152261,"prompt-eng":0.4141406935,"data-quality":0.1163747686,"ml-security":0.0403259913}}
{"text":"Legged robots can swiftly navigate representative terrains, such as granular slopes beyond 25 degrees, loose soil, and unstructured terrain, highlighting their advantages compared to wheeled rover systems.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.0623085072,"dev-research":0.2735048066,"prompt-eng":0.3942137012,"data-quality":0.0482374171,"ml-security":0.0483954414}}
{"text":"We successfully verified the approach in analog deployments at the BeyondGravity ExoMars rover testbed, in a quarry in Switzerland, and at the Space Resources Challenge in Luxembourg.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.1352551639,"dev-research":0.2575785122,"prompt-eng":0.4175532908,"data-quality":0.1227674996,"ml-security":0.0808051876}}
{"text":"Our results show that a team of legged robots with advanced locomotion, perception, and measurement skills, as well as task-level autonomy, can conduct successful, effective missions in a short time.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.0742359421,"dev-research":0.2510344251,"prompt-eng":0.4461552409,"data-quality":0.0541218827,"ml-security":0.0679194111}}
{"text":"Our approach enables the scientific exploration of planetary target sites that are currently out of human and robotic reach.","meta":{"url":"http://arxiv.org/abs/2307.10079v1"},"cats":{"new-dataset":0.0750131459,"dev-research":0.2532146434,"prompt-eng":0.4126617069,"data-quality":0.0793142891,"ml-security":0.0698878272}}
{"text":"In this paper, we characterize Probabilistic Principal Component Analysis in Hilbert spaces and demonstrate how the optimal solution admits a representation in dual space.","meta":{"url":"http://arxiv.org/abs/2307.10078v1"},"cats":{"new-dataset":0.0461268512,"dev-research":0.2387397025,"prompt-eng":0.4036423202,"data-quality":0.130835236,"ml-security":0.149326168}}
{"text":"This allows us to develop a generative framework for kernel methods.","meta":{"url":"http://arxiv.org/abs/2307.10078v1"},"cats":{"new-dataset":0.0358925939,"dev-research":0.3409220659,"prompt-eng":0.4632222468,"data-quality":0.1330352191,"ml-security":0.1182646441}}
{"text":"Furthermore, we show how it englobes Kernel Principal Component Analysis and illustrate its working on a toy and a real dataset.","meta":{"url":"http://arxiv.org/abs/2307.10078v1"},"cats":{"new-dataset":0.3890264107,"dev-research":0.3061851162,"prompt-eng":0.3590682911,"data-quality":0.1617991598,"ml-security":0.1725582514}}
{"text":"Formal verification of intelligent agents is often computationally infeasible due to state-space explosion.","meta":{"url":"http://arxiv.org/abs/2307.10068v1"},"cats":{"new-dataset":0.0235602826,"dev-research":0.288650635,"prompt-eng":0.4508344439,"data-quality":0.1350830532,"ml-security":0.257682494}}
{"text":"We present a tool for reducing the impact of the explosion by means of state abstraction that is (a) easy to use and understand by non-experts, and (b) agent-based in the sense that it operates on a modular representation of the system, rather than on its huge explicit state model.","meta":{"url":"http://arxiv.org/abs/2307.10068v1"},"cats":{"new-dataset":0.0385995933,"dev-research":0.3411649276,"prompt-eng":0.4288472894,"data-quality":0.1082812471,"ml-security":0.2639100517}}
{"text":"Large offline learning-based models have enabled robots to successfully interact with objects for a wide variety of tasks.","meta":{"url":"http://arxiv.org/abs/2307.10063v1"},"cats":{"new-dataset":0.0852962193,"dev-research":0.1835983267,"prompt-eng":0.4429899726,"data-quality":0.0491182836,"ml-security":0.1264493651}}
{"text":"However, these models rely on fairly consistent structured environments.","meta":{"url":"http://arxiv.org/abs/2307.10063v1"},"cats":{"new-dataset":0.0309812311,"dev-research":0.2569365067,"prompt-eng":0.4163300894,"data-quality":0.1316463073,"ml-security":0.0791940349}}
{"text":"For more unstructured environments, an online learning component is necessary to gather and estimate information about objects in the environment in order to successfully interact with them.","meta":{"url":"http://arxiv.org/abs/2307.10063v1"},"cats":{"new-dataset":0.2105007499,"dev-research":0.2873252584,"prompt-eng":0.4051973043,"data-quality":0.0659474447,"ml-security":0.1159305269}}
{"text":"Unfortunately, online learning methods like Bayesian non-parametric models struggle with changes in the environment, which is often the desired outcome of interaction-based tasks.","meta":{"url":"http://arxiv.org/abs/2307.10063v1"},"cats":{"new-dataset":0.0434439266,"dev-research":0.2656819736,"prompt-eng":0.4258342973,"data-quality":0.0515344646,"ml-security":0.1344984896}}
{"text":"We propose using an object-centric representation for interactive online learning.","meta":{"url":"http://arxiv.org/abs/2307.10063v1"},"cats":{"new-dataset":0.1153999715,"dev-research":0.2818275349,"prompt-eng":0.4560932017,"data-quality":0.1298968991,"ml-security":0.1255248676}}
{"text":"This representation is generated by transforming the robot's actions into the object's coordinate frame.","meta":{"url":"http://arxiv.org/abs/2307.10063v1"},"cats":{"new-dataset":0.0974242048,"dev-research":0.2878376019,"prompt-eng":0.4592000185,"data-quality":0.0748271539,"ml-security":0.0773505765}}
{"text":"We demonstrate how switching to this task-relevant space improves our ability to reason with the training data collected online, enabling scalable online learning of robot-object interactions.","meta":{"url":"http://arxiv.org/abs/2307.10063v1"},"cats":{"new-dataset":0.184340395,"dev-research":0.3500206734,"prompt-eng":0.447319339,"data-quality":0.0973598546,"ml-security":0.1035689295}}
{"text":"We showcase our method by successfully navigating a manipulator arm through an environment with multiple unknown objects without violating interaction-based constraints.","meta":{"url":"http://arxiv.org/abs/2307.10063v1"},"cats":{"new-dataset":0.0513523082,"dev-research":0.2469306869,"prompt-eng":0.4043530126,"data-quality":0.0670097357,"ml-security":0.0845939416}}
{"text":"Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions.","meta":{"url":"http://arxiv.org/abs/2307.10062v1"},"cats":{"new-dataset":0.0551029864,"dev-research":0.3148867513,"prompt-eng":0.4205892014,"data-quality":0.1736631434,"ml-security":0.1575750241}}
{"text":"Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices.","meta":{"url":"http://arxiv.org/abs/2307.10062v1"},"cats":{"new-dataset":0.1885443081,"dev-research":0.3605872686,"prompt-eng":0.3907260124,"data-quality":0.3991535501,"ml-security":0.2695733089}}
{"text":"Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data.","meta":{"url":"http://arxiv.org/abs/2307.10062v1"},"cats":{"new-dataset":0.2572398698,"dev-research":0.2591512123,"prompt-eng":0.4049628884,"data-quality":0.4512996201,"ml-security":0.264749585}}
{"text":"We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms.","meta":{"url":"http://arxiv.org/abs/2307.10062v1"},"cats":{"new-dataset":0.1279303525,"dev-research":0.2892085197,"prompt-eng":0.421835411,"data-quality":0.4998801808,"ml-security":0.1364093193}}
{"text":"Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis.","meta":{"url":"http://arxiv.org/abs/2307.10062v1"},"cats":{"new-dataset":0.0509464976,"dev-research":0.2947194068,"prompt-eng":0.4836155143,"data-quality":0.6358806008,"ml-security":0.1159206458}}
{"text":"We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model.","meta":{"url":"http://arxiv.org/abs/2307.10062v1"},"cats":{"new-dataset":0.0314671414,"dev-research":0.2412829639,"prompt-eng":0.4603917521,"data-quality":0.6953511416,"ml-security":0.5998806933}}
{"text":"Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training.","meta":{"url":"http://arxiv.org/abs/2307.10062v1"},"cats":{"new-dataset":0.318718342,"dev-research":0.311491057,"prompt-eng":0.404184642,"data-quality":0.2504744274,"ml-security":0.2283943342}}
{"text":"There exist several results on deciding termination and computing runtime bounds for triangular weakly non-linear loops (twn-loops).","meta":{"url":"http://arxiv.org/abs/2307.10061v1"},"cats":{"new-dataset":0.0647037278,"dev-research":0.2429003781,"prompt-eng":0.335991451,"data-quality":0.1062457514,"ml-security":0.1168464252}}
{"text":"We show how to use results on such subclasses of programs where complexity bounds are computable within incomplete approaches for complexity analysis of full integer programs.","meta":{"url":"http://arxiv.org/abs/2307.10061v1"},"cats":{"new-dataset":0.0668279756,"dev-research":0.2729286903,"prompt-eng":0.3383144956,"data-quality":0.1159446525,"ml-security":0.1615499311}}
{"text":"To this end, we present a novel modular approach which computes local runtime bounds for subprograms which can be transformed into twn-loops.","meta":{"url":"http://arxiv.org/abs/2307.10061v1"},"cats":{"new-dataset":0.1295937087,"dev-research":0.357428431,"prompt-eng":0.3962759949,"data-quality":0.0735585179,"ml-security":0.1213933047}}
{"text":"These local runtime bounds are then lifted to global runtime bounds for the whole program.","meta":{"url":"http://arxiv.org/abs/2307.10061v1"},"cats":{"new-dataset":0.0790994529,"dev-research":0.3480387383,"prompt-eng":0.3953818535,"data-quality":0.0920656663,"ml-security":0.1537096806}}
{"text":"The power of our approach is shown by our implementation in the tool KoAT which analyzes complexity of programs where all other state-of-the-art tools fail.","meta":{"url":"http://arxiv.org/abs/2307.10061v1"},"cats":{"new-dataset":0.0909341572,"dev-research":0.3887609412,"prompt-eng":0.3352758604,"data-quality":0.1077301415,"ml-security":0.1009986564}}
{"text":"We study the operator norm discrepancy of i.i.d. random matrices, initiating the matrix-valued analog of a long line of work on the $\\ell^{\\infty}$ norm discrepancy of i.i.d. random vectors.","meta":{"url":"http://arxiv.org/abs/2307.10055v1"},"cats":{"new-dataset":0.0518088358,"dev-research":0.2390103774,"prompt-eng":0.3480457187,"data-quality":0.1510893055,"ml-security":0.1325046858}}
{"text":"First, we give a new analysis of the matrix hyperbolic cosine algorithm of Zouzias (2011), a matrix version of an online vector discrepancy algorithm of Spencer (1977) studied for average-case inputs by Bansal and Spencer (2020), for the case of i.i.d. random matrix inputs.","meta":{"url":"http://arxiv.org/abs/2307.10055v1"},"cats":{"new-dataset":0.0375190842,"dev-research":0.2148488757,"prompt-eng":0.3405497894,"data-quality":0.1257861764,"ml-security":0.1200872604}}
{"text":"We both give a general analysis and extract concrete bounds on the discrepancy achieved by this algorithm for matrices with independent entries and positive semidefinite matrices drawn from Wishart distributions.","meta":{"url":"http://arxiv.org/abs/2307.10055v1"},"cats":{"new-dataset":0.0564599919,"dev-research":0.19456684,"prompt-eng":0.3752711783,"data-quality":0.1457360505,"ml-security":0.1035896444}}
{"text":"Second, using the first moment method, we give lower bounds on the discrepancy of random matrices, in particular showing that the matrix hyperbolic cosine algorithm achieves optimal discrepancy up to logarithmic terms in several cases.","meta":{"url":"http://arxiv.org/abs/2307.10055v1"},"cats":{"new-dataset":0.0303495977,"dev-research":0.19576676,"prompt-eng":0.3306595339,"data-quality":0.1064813408,"ml-security":0.0936013008}}
{"text":"We both treat the special case of the Gaussian orthogonal ensemble and give a general result for low-rank matrix distributions that we apply to orthogonally invariant random projections.","meta":{"url":"http://arxiv.org/abs/2307.10055v1"},"cats":{"new-dataset":0.0627206265,"dev-research":0.1913072333,"prompt-eng":0.3916993165,"data-quality":0.166372607,"ml-security":0.1827300851}}
{"text":"How do we assess a new Internet congestion control (CC) design?","meta":{"url":"http://arxiv.org/abs/2307.10054v1"},"cats":{"new-dataset":0.1101527548,"dev-research":0.3075755252,"prompt-eng":0.388671579,"data-quality":0.0932946049,"ml-security":0.1156714127}}
{"text":"How do we compare it with other existing schemes?","meta":{"url":"http://arxiv.org/abs/2307.10054v1"},"cats":{"new-dataset":0.0095780459,"dev-research":0.3042796451,"prompt-eng":0.3523241611,"data-quality":0.0679410216,"ml-security":0.0776120834}}
{"text":"Under what scenarios and using what network parameters?","meta":{"url":"http://arxiv.org/abs/2307.10054v1"},"cats":{"new-dataset":0.0353701631,"dev-research":0.230215059,"prompt-eng":0.3905681201,"data-quality":0.0808605882,"ml-security":0.130272491}}
{"text":"These are just a handful of simple questions coming up every time a new CC design is going to be evaluated.","meta":{"url":"http://arxiv.org/abs/2307.10054v1"},"cats":{"new-dataset":0.0151797414,"dev-research":0.400705538,"prompt-eng":0.4547849183,"data-quality":0.115120329,"ml-security":0.0732862929}}
{"text":"Interestingly, the number of specific answers to these questions can be as large as the number of CC designers.","meta":{"url":"http://arxiv.org/abs/2307.10054v1"},"cats":{"new-dataset":0.1556863273,"dev-research":0.3626472939,"prompt-eng":0.4339246688,"data-quality":0.0825390344,"ml-security":0.0566080865}}
{"text":"In this work, we aim to highlight that the network congestion control, as a hot and active research topic, requires a crystal clear set(s) of \\textit{CC Benchmarks} to form a common ground for quantitatively comparing and unambiguously assessing the strengths and weaknesses of a design with respect to the existing ones.","meta":{"url":"http://arxiv.org/abs/2307.10054v1"},"cats":{"new-dataset":0.0979774685,"dev-research":0.3479340844,"prompt-eng":0.3657824471,"data-quality":0.1363243321,"ml-security":0.138595688}}
{"text":"As a first step toward that goal, we introduce general benchmarks that can capture the different performance of the existing Internet CC schemes.","meta":{"url":"http://arxiv.org/abs/2307.10054v1"},"cats":{"new-dataset":0.0397396369,"dev-research":0.2897842312,"prompt-eng":0.3740415275,"data-quality":0.0897918404,"ml-security":0.1053948975}}
{"text":"Using these benchmarks, we rank the Internet CC algorithms and illustrate that there is still lots of room for more innovations and improvements in this topic.","meta":{"url":"http://arxiv.org/abs/2307.10054v1"},"cats":{"new-dataset":0.0738676569,"dev-research":0.2880836706,"prompt-eng":0.3516446495,"data-quality":0.1125806305,"ml-security":0.0869790555}}
{"text":"Multimodal vision-language (VL) learning has noticeably pushed the tendency toward generic intelligence owing to emerging large foundation models.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.183298437,"dev-research":0.2934497526,"prompt-eng":0.4281066515,"data-quality":0.1374784522,"ml-security":0.087122249}}
{"text":"However, tracking, as a fundamental vision problem, surprisingly enjoys less bonus from recent flourishing VL learning.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.0250161168,"dev-research":0.3205989853,"prompt-eng":0.3871970238,"data-quality":0.1505538122,"ml-security":0.0931892133}}
{"text":"We argue that the reasons are two-fold: the lack of large-scale vision-language annotated videos and ineffective vision-language interaction learning of current works.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.0906087013,"dev-research":0.3981682801,"prompt-eng":0.3837148097,"data-quality":0.2755982327,"ml-security":0.0767986031}}
{"text":"These nuisances motivate us to design more effective vision-language representation for tracking, meanwhile constructing a large database with language annotation for model learning.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.2283665625,"dev-research":0.3582547988,"prompt-eng":0.4722366299,"data-quality":0.3302476016,"ml-security":0.1356375001}}
{"text":"Particularly, in this paper, we first propose a general attribute annotation strategy to decorate videos in six popular tracking benchmarks, which contributes a large-scale vision-language tracking database with more than 23,000 videos.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.4688097059,"dev-research":0.3461077373,"prompt-eng":0.4220586806,"data-quality":0.4272355313,"ml-security":0.075598325}}
{"text":"We then introduce a novel framework to improve tracking by learning a unified-adaptive VL representation, where the cores are the proposed asymmetric architecture search and modality mixer (ModaMixer).","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.0385516874,"dev-research":0.2301871648,"prompt-eng":0.3866693454,"data-quality":0.1267781616,"ml-security":0.0881807607}}
{"text":"To further improve VL representation, we introduce a contrastive loss to align different modalities.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.0253454803,"dev-research":0.2700999603,"prompt-eng":0.3985271121,"data-quality":0.2344982216,"ml-security":0.0755403708}}
{"text":"To thoroughly evidence the effectiveness of our method, we integrate the proposed framework on three tracking methods with different designs, i.e., the CNN-based SiamCAR, the Transformer-based OSTrack, and the hybrid structure TransT.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.0612058467,"dev-research":0.2396596309,"prompt-eng":0.3522863446,"data-quality":0.0850348615,"ml-security":0.0644916788}}
{"text":"The experiments demonstrate that our framework can significantly improve all baselines on six benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.1315842329,"dev-research":0.3694580816,"prompt-eng":0.3977652822,"data-quality":0.1286884617,"ml-security":0.0670925663}}
{"text":"Besides empirical results, we theoretically analyze our approach to show its rationality.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.0324426671,"dev-research":0.3838770502,"prompt-eng":0.3907247349,"data-quality":0.1927251531,"ml-security":0.1779103134}}
{"text":"By revealing the potential of VL representation, we expect the community to divert more attention to VL tracking and hope to open more possibilities for future tracking with diversified multimodal messages.","meta":{"url":"http://arxiv.org/abs/2307.10046v1"},"cats":{"new-dataset":0.1266398098,"dev-research":0.3743766714,"prompt-eng":0.4661441366,"data-quality":0.1606777767,"ml-security":0.1079149368}}
{"text":"In relational verification, judicious alignment of computational steps facilitates proof of relations between programs using simple relational assertions.","meta":{"url":"http://arxiv.org/abs/2307.10045v1"},"cats":{"new-dataset":0.0300804718,"dev-research":0.4300179908,"prompt-eng":0.4355904045,"data-quality":0.1933977531,"ml-security":0.1057292953}}
{"text":"Relational Hoare logics (RHL) provide compositional rules that embody various alignments of executions.","meta":{"url":"http://arxiv.org/abs/2307.10045v1"},"cats":{"new-dataset":0.0416661642,"dev-research":0.3405969502,"prompt-eng":0.4291203974,"data-quality":0.0977878059,"ml-security":0.0526193664}}
{"text":"Seemingly more flexible alignments can be expressed in terms of product automata based on program transition relations.","meta":{"url":"http://arxiv.org/abs/2307.10045v1"},"cats":{"new-dataset":0.0575936861,"dev-research":0.3639421748,"prompt-eng":0.4771428141,"data-quality":0.1309502953,"ml-security":0.0685891582}}
{"text":"A single degenerate alignment rule (self-composition), atop a complete Hoare logic, comprises a RHL for $\\forall\\forall$ properties that is complete in the ordinary logical sense.","meta":{"url":"http://arxiv.org/abs/2307.10045v1"},"cats":{"new-dataset":0.0760562881,"dev-research":0.279778683,"prompt-eng":0.3820365888,"data-quality":0.1002742412,"ml-security":0.0497323383}}
{"text":"The notion of alignment completeness was previously proposed as a more satisfactory measure, and some rules were shown to be alignment complete with respect to a few ad hoc forms of alignment automata.","meta":{"url":"http://arxiv.org/abs/2307.10045v1"},"cats":{"new-dataset":0.0484688789,"dev-research":0.3091739255,"prompt-eng":0.4572269074,"data-quality":0.2026577401,"ml-security":0.0400697707}}
{"text":"This paper proves alignment completeness with respect to a general class of $\\forall\\forall$ alignment automata, for a RHL comprised of standard rules together with a rule of semantics-preserving rewrites based on Kleene algebra with tests.","meta":{"url":"http://arxiv.org/abs/2307.10045v1"},"cats":{"new-dataset":0.1086045501,"dev-research":0.2766751171,"prompt-eng":0.4132452845,"data-quality":0.1804639483,"ml-security":0.0576695839}}
{"text":"We also give a new logic for $\\forall\\exists$ properties and prove its alignment completeness.","meta":{"url":"http://arxiv.org/abs/2307.10045v1"},"cats":{"new-dataset":0.1625257931,"dev-research":0.3259552445,"prompt-eng":0.3981082296,"data-quality":0.1573694329,"ml-security":0.0621137099}}
{"text":"Autonomous systems, such as Unmanned Aerial Vehicles (UAVs), are expected to run complex reinforcement learning (RL) models to execute fully autonomous position-navigation-time tasks within stringent onboard weight and power constraints.","meta":{"url":"http://arxiv.org/abs/2307.10041v1"},"cats":{"new-dataset":0.0702150874,"dev-research":0.20931302,"prompt-eng":0.4110732487,"data-quality":0.0626910565,"ml-security":0.1957222676}}
{"text":"We observe that reducing onboard operating voltage can benefit the energy efficiency of both the computation and flight mission, however, it can also result in on-chip bit failures that are detrimental to mission safety and performance.","meta":{"url":"http://arxiv.org/abs/2307.10041v1"},"cats":{"new-dataset":0.0355667283,"dev-research":0.3709114429,"prompt-eng":0.3858451228,"data-quality":0.1378380858,"ml-security":0.2297008951}}
{"text":"To this end, we propose BERRY, a robust learning framework to improve bit error robustness and energy efficiency for RL-enabled autonomous systems.","meta":{"url":"http://arxiv.org/abs/2307.10041v1"},"cats":{"new-dataset":0.0904462033,"dev-research":0.2939277266,"prompt-eng":0.3913975414,"data-quality":0.2527504569,"ml-security":0.2914253593}}
{"text":"BERRY supports robust learning, both offline and on-board the UAV, and for the first time, demonstrates the practicality of robust low-voltage operation on UAVs that leads to high energy savings in both compute-level operation and system-level quality-of-flight.","meta":{"url":"http://arxiv.org/abs/2307.10041v1"},"cats":{"new-dataset":0.0816266137,"dev-research":0.2627636154,"prompt-eng":0.40998933,"data-quality":0.1069879679,"ml-security":0.2723249947}}
{"text":"We perform extensive experiments on 72 autonomous navigation scenarios and demonstrate that BERRY generalizes well across environments, UAVs, autonomy policies, operating voltages and fault patterns, and consistently improves robustness, efficiency and mission performance, achieving up to 15.62% reduction in flight energy, 18.51% increase in the number of successful missions, and 3.43x processing energy reduction.","meta":{"url":"http://arxiv.org/abs/2307.10041v1"},"cats":{"new-dataset":0.1158227723,"dev-research":0.303418909,"prompt-eng":0.4303462258,"data-quality":0.0851653991,"ml-security":0.1271288018}}
{"text":"Automated medical image classification is the key component in intelligent diagnosis systems.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.0791747959,"dev-research":0.3191630133,"prompt-eng":0.4409332976,"data-quality":0.250824859,"ml-security":0.1500198398}}
{"text":"However, most medical image datasets contain plenty of samples of common diseases and just a handful of rare ones, leading to major class imbalances.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.2477087171,"dev-research":0.2750622413,"prompt-eng":0.353907317,"data-quality":0.2431906114,"ml-security":0.2752330698}}
{"text":"Currently, it is an open problem in intelligent diagnosis to effectively learn from imbalanced training data.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.0909514506,"dev-research":0.3217804388,"prompt-eng":0.4084829041,"data-quality":0.339146162,"ml-security":0.4034187717}}
{"text":"In this paper, we propose a simple yet effective framework, named \\textbf{C}lass \\textbf{A}ttention to \\textbf{RE}gions of the lesion (CARE), to handle data imbalance issues by embedding attention into the training process of \\textbf{C}onvolutional \\textbf{N}eural \\textbf{N}etworks (CNNs).","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.0850119038,"dev-research":0.2683076629,"prompt-eng":0.4101754737,"data-quality":0.3584740808,"ml-security":0.1930278815}}
{"text":"The proposed attention module helps CNNs attend to lesion regions of rare diseases, therefore helping CNNs to learn their characteristics more effectively.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.1123717016,"dev-research":0.2824995811,"prompt-eng":0.4393049251,"data-quality":0.2131885614,"ml-security":0.1540722939}}
{"text":"In addition, this attention module works only during the training phase and does not change the architecture of the original network, so it can be directly combined with any existing CNN architecture.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.049216229,"dev-research":0.19182573,"prompt-eng":0.4125324754,"data-quality":0.1342061993,"ml-security":0.0906057988}}
{"text":"The CARE framework needs bounding boxes to represent the lesion regions of rare diseases.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.1139790966,"dev-research":0.284039263,"prompt-eng":0.3635558497,"data-quality":0.1027811505,"ml-security":0.1185090899}}
{"text":"To alleviate the need for manual annotation, we further developed variants of CARE by leveraging the traditional saliency methods or a pretrained segmentation model for bounding box generation.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.168920745,"dev-research":0.2860493857,"prompt-eng":0.4582064981,"data-quality":0.1904627522,"ml-security":0.0446334504}}
{"text":"Results show that the CARE variants with automated bounding box generation are comparable to the original CARE framework with \\textit{manual} bounding box annotations.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.1209734487,"dev-research":0.3449311672,"prompt-eng":0.4464635697,"data-quality":0.1423292247,"ml-security":0.0671398113}}
{"text":"A series of experiments on an imbalanced skin image dataset and a pneumonia dataset indicates that our method can effectively help the network focus on the lesion regions of rare diseases and remarkably improves the classification performance of rare diseases.","meta":{"url":"http://arxiv.org/abs/2307.10036v1"},"cats":{"new-dataset":0.2817982217,"dev-research":0.2486217669,"prompt-eng":0.3584334429,"data-quality":0.27036198,"ml-security":0.2318478474}}
{"text":"JSON Schema is the de-facto standard schema language for JSON data.","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.2029650581,"dev-research":0.3654610046,"prompt-eng":0.3772748159,"data-quality":0.1050829055,"ml-security":0.0921514628}}
{"text":"The language went through many minor revisions, but the most recent versions of the language added two novel features, dynamic references and annotation-dependent validation, that change the evaluation model.","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.0715771383,"dev-research":0.4294807823,"prompt-eng":0.4309696978,"data-quality":0.3120240276,"ml-security":0.0686335431}}
{"text":"Modern JSON Schema is the name used to indicate all versions from Draft 2019-09, which are characterized by these new features, while Classical JSON Schema is used to indicate the previous versions.   ","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.1554090637,"dev-research":0.3601049889,"prompt-eng":0.3908960737,"data-quality":0.1096937763,"ml-security":0.0713116176}}
{"text":"These new \"modern\" features make the schema language quite difficult to understand, and have generated many discussions about the correct interpretation of their official specifications; for this reason we undertook the task of their formalization.","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.0571465776,"dev-research":0.4557751171,"prompt-eng":0.4619281375,"data-quality":0.1497361403,"ml-security":0.0451815644}}
{"text":"During this process, we also analyzed the complexity of data validation in Modern JSON Schema, with the idea of confirming the PTIME complexity of Classical JSON Schema validation, and we were surprised to discover a completely different truth: data validation, that is expected to be an extremely efficient process, acquires, with Modern JSON Schema features, a PSPACE complexity.   ","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.0629270863,"dev-research":0.3085445,"prompt-eng":0.3554098468,"data-quality":0.1340150152,"ml-security":0.1202790327}}
{"text":"In this paper, we give the first formal description of Modern JSON Schema, which we consider a central contribution of the work that we present here.","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.1462905719,"dev-research":0.2777125453,"prompt-eng":0.4189974145,"data-quality":0.1215924446,"ml-security":0.0991753651}}
{"text":"We then prove that its data validation problem is PSPACE-complete.","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.133316653,"dev-research":0.2839131222,"prompt-eng":0.4107496894,"data-quality":0.3397149948,"ml-security":0.1624256829}}
{"text":"We prove that the origin of the problem lies in dynamic references, and not in annotation-dependent validation.","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.0613136426,"dev-research":0.3004633206,"prompt-eng":0.4013849943,"data-quality":0.5630347339,"ml-security":0.1633357478}}
{"text":"We study the schema and data complexities, showing that the problem is PSPACE-complete with respect to the schema size even with a fixed instance, but is in PTIME when the schema is fixed and only the instance size is allowed to vary.","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.2312048684,"dev-research":0.2564557689,"prompt-eng":0.3684129411,"data-quality":0.1508166421,"ml-security":0.0936510233}}
{"text":"Finally, we run experiments that show that there are families of schemas where the difference in asymptotic complexity between dynamic and static references is extremely visible, even with small schemas.","meta":{"url":"http://arxiv.org/abs/2307.10034v1"},"cats":{"new-dataset":0.0770688559,"dev-research":0.2931756668,"prompt-eng":0.3516426559,"data-quality":0.112905584,"ml-security":0.1042210608}}
{"text":"Obtaining Quadratic Unconstrained Binary Optimisation models for various optimisation problems, in order to solve those on physical quantum computers (such as the the DWave annealers) is nowadays a lengthy and tedious process that requires one to remodel all problem variables as binary variables and squeeze the target function and the constraints into a single quadratic polynomial into these new variables.   ","meta":{"url":"http://arxiv.org/abs/2307.10032v1"},"cats":{"new-dataset":0.0213049644,"dev-research":0.2238607655,"prompt-eng":0.4369231439,"data-quality":0.0532501544,"ml-security":0.0869294803}}
{"text":"We report here on the basis of our automatic converter from MiniZinc to QUBO, which is able to process a large set of constraint optimisation and constraint satisfaction problems and turn them into equivalent QUBOs, effectively optimising the whole process.","meta":{"url":"http://arxiv.org/abs/2307.10032v1"},"cats":{"new-dataset":0.0236329604,"dev-research":0.2563809593,"prompt-eng":0.4529626017,"data-quality":0.1298300137,"ml-security":0.0497007547}}
{"text":"Deep neural networks often fail catastrophically by relying on spurious correlations.","meta":{"url":"http://arxiv.org/abs/2307.10026v1"},"cats":{"new-dataset":0.0213095664,"dev-research":0.325623301,"prompt-eng":0.3838223938,"data-quality":0.4310981946,"ml-security":0.4441754629}}
{"text":"Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic.","meta":{"url":"http://arxiv.org/abs/2307.10026v1"},"cats":{"new-dataset":0.006406291,"dev-research":0.4942283208,"prompt-eng":0.4329671074,"data-quality":0.2935927496,"ml-security":0.1503468759}}
{"text":"For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so.","meta":{"url":"http://arxiv.org/abs/2307.10026v1"},"cats":{"new-dataset":0.0151256746,"dev-research":0.3084187153,"prompt-eng":0.39613055,"data-quality":0.0872217005,"ml-security":0.2618823169}}
{"text":"However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk.","meta":{"url":"http://arxiv.org/abs/2307.10026v1"},"cats":{"new-dataset":0.0296168422,"dev-research":0.2638230831,"prompt-eng":0.3979338947,"data-quality":0.1351628195,"ml-security":0.1655906555}}
{"text":"Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance.","meta":{"url":"http://arxiv.org/abs/2307.10026v1"},"cats":{"new-dataset":0.0068485854,"dev-research":0.4128575401,"prompt-eng":0.3901585167,"data-quality":0.375679769,"ml-security":0.2541131235}}
{"text":"We formalize a new setting called contextual reliability which accounts for the fact that the \"right\" features to use may vary depending on the context.","meta":{"url":"http://arxiv.org/abs/2307.10026v1"},"cats":{"new-dataset":0.0354744079,"dev-research":0.4012236179,"prompt-eng":0.460709074,"data-quality":0.3803077675,"ml-security":0.0922615909}}
{"text":"We propose and analyze a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a model to rely exclusively on these features.","meta":{"url":"http://arxiv.org/abs/2307.10026v1"},"cats":{"new-dataset":0.0440745635,"dev-research":0.3646599709,"prompt-eng":0.4670403618,"data-quality":0.3127675481,"ml-security":0.2912149572}}
{"text":"Our work theoretically and empirically demonstrates the advantages of ENP over existing methods and provides new benchmarks for contextual reliability.","meta":{"url":"http://arxiv.org/abs/2307.10026v1"},"cats":{"new-dataset":0.0399153449,"dev-research":0.3183211552,"prompt-eng":0.4423082081,"data-quality":0.2335178861,"ml-security":0.0705628987}}
{"text":"Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community.","meta":{"url":"http://arxiv.org/abs/2307.10025v1"},"cats":{"new-dataset":0.069873428,"dev-research":0.3010993526,"prompt-eng":0.3757142532,"data-quality":0.1103598355,"ml-security":0.1025056027}}
{"text":"2023 ``two sessions\" proposal ``suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction\" This topic was once a hot topic on the Internet, and ``unbundling\" the relationship between birth registration and marriage has become the focus of social debate.","meta":{"url":"http://arxiv.org/abs/2307.10025v1"},"cats":{"new-dataset":0.0653167666,"dev-research":0.2666574406,"prompt-eng":0.3668799709,"data-quality":0.1168018234,"ml-security":0.0495610242}}
{"text":"In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments.","meta":{"url":"http://arxiv.org/abs/2307.10025v1"},"cats":{"new-dataset":0.0805473133,"dev-research":0.3401870567,"prompt-eng":0.3700689288,"data-quality":0.2076200234,"ml-security":0.0395017641}}
{"text":"It is found that the discussion on the proposal of ``removing marriage restrictions from birth registration\" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's sentiment inclined to be negative in most of the topics.","meta":{"url":"http://arxiv.org/abs/2307.10025v1"},"cats":{"new-dataset":0.043201402,"dev-research":0.2949021875,"prompt-eng":0.3270778336,"data-quality":0.1452120395,"ml-security":0.0912545556}}
{"text":"Based on this, eight proposals were made to provide a reference for governmental decision making and to form a reference method for researching public opinion on political issues.","meta":{"url":"http://arxiv.org/abs/2307.10025v1"},"cats":{"new-dataset":0.2059275058,"dev-research":0.3372244365,"prompt-eng":0.3809001299,"data-quality":0.1348762659,"ml-security":0.0497731697}}
{"text":"I propose an open dataset of country-level historical opinion polling data for the European Union and the UK.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.8217283647,"dev-research":0.2715572443,"prompt-eng":0.3476097702,"data-quality":0.1459031697,"ml-security":0.1030925193}}
{"text":"The dataset aims to fill a gap in available opinion polling data for the European Union.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.6111258718,"dev-research":0.3101172597,"prompt-eng":0.3287083028,"data-quality":0.1824629155,"ml-security":0.0993210122}}
{"text":"Some existing datasets are restricted to the past five years, limiting research opportunities.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.4098207224,"dev-research":0.236604488,"prompt-eng":0.2640080604,"data-quality":0.0729781963,"ml-security":0.233364042}}
{"text":"At the same time, some larger proprietary datasets exist but are available only in a visual preprocessed time series format.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.8355040624,"dev-research":0.2723113723,"prompt-eng":0.2963716842,"data-quality":0.0576642161,"ml-security":0.1359162508}}
{"text":"Finally, while other large datasets for individual countries might exist, these could be inaccessible due to language barriers.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.6670165678,"dev-research":0.2344094769,"prompt-eng":0.3219299006,"data-quality":0.2465929792,"ml-security":0.2168170659}}
{"text":"The data was gathered from Wikipedia, and preprocessed using the pandas library.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.8230433722,"dev-research":0.292109758,"prompt-eng":0.3606586972,"data-quality":0.1078455594,"ml-security":0.0594362828}}
{"text":"Both the raw and the preprocessed data are in the .csv format.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.7937839296,"dev-research":0.3067067114,"prompt-eng":0.3391918264,"data-quality":0.1302220927,"ml-security":0.096760974}}
{"text":"I hope that given the recent advances in LLMs and deep learning in general, this large dataset will enable researchers to uncover complex interactions between multimodal data (news articles, economic indicators, social media) and voting behavior.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.552636693,"dev-research":0.213429979,"prompt-eng":0.406613115,"data-quality":0.1077980619,"ml-security":0.1986328888}}
{"text":"The raw data, the preprocessed data, and the preprocessing scripts are available on GitHub.","meta":{"url":"http://arxiv.org/abs/2307.10022v1"},"cats":{"new-dataset":0.8559098964,"dev-research":0.3648230976,"prompt-eng":0.4016251604,"data-quality":0.0792846956,"ml-security":0.0862009523}}
{"text":"Rob\\^oCIn has participated in RoboCup Small Size League since 2019, won its first world title in 2022 (Division B), and is currently a three-times Latin-American champion.","meta":{"url":"http://arxiv.org/abs/2307.10018v1"},"cats":{"new-dataset":0.131999967,"dev-research":0.2431566026,"prompt-eng":0.3479133717,"data-quality":0.1009953578,"ml-security":0.068850336}}
{"text":"This paper presents our improvements to defend the Small Size League (SSL) division B title in RoboCup 2023 in Bordeaux, France.","meta":{"url":"http://arxiv.org/abs/2307.10018v1"},"cats":{"new-dataset":0.1473878376,"dev-research":0.2762750594,"prompt-eng":0.3737796331,"data-quality":0.0905349908,"ml-security":0.1462327504}}
{"text":"This paper aims to share some of the academic research that our team developed over the past year.","meta":{"url":"http://arxiv.org/abs/2307.10018v1"},"cats":{"new-dataset":0.2834576602,"dev-research":0.3470241064,"prompt-eng":0.3839301108,"data-quality":0.0606552465,"ml-security":0.0897037647}}
{"text":"Our team has successfully published 2 articles related to SSL at two high-impact conferences: the 25th RoboCup International Symposium and the 19th IEEE Latin American Robotics Symposium (LARS 2022).","meta":{"url":"http://arxiv.org/abs/2307.10018v1"},"cats":{"new-dataset":0.1191652144,"dev-research":0.233797368,"prompt-eng":0.4094081094,"data-quality":0.102310292,"ml-security":0.1439789273}}
{"text":"Over the last year, we have been continuously migrating from our past codebase to Unification.","meta":{"url":"http://arxiv.org/abs/2307.10018v1"},"cats":{"new-dataset":0.202912143,"dev-research":0.490893909,"prompt-eng":0.4583738204,"data-quality":0.1143634531,"ml-security":0.07882217}}
{"text":"We will describe the new architecture implemented and some points of software and AI refactoring.","meta":{"url":"http://arxiv.org/abs/2307.10018v1"},"cats":{"new-dataset":0.2025616978,"dev-research":0.4268156491,"prompt-eng":0.3935115274,"data-quality":0.1093184039,"ml-security":0.1310328335}}
{"text":"In addition, we discuss the process of integrating machined components into the mechanical system, our development for participating in the vision blackout challenge last year and what we are preparing for this year.","meta":{"url":"http://arxiv.org/abs/2307.10018v1"},"cats":{"new-dataset":0.0962783977,"dev-research":0.2726596853,"prompt-eng":0.4353955166,"data-quality":0.1061339946,"ml-security":0.0929918439}}
{"text":"With the increasing application of robots, stable and efficient Visual Odometry (VO) algorithms are becoming more and more important.","meta":{"url":"http://arxiv.org/abs/2307.10015v1"},"cats":{"new-dataset":0.0473230307,"dev-research":0.2449785395,"prompt-eng":0.3273291801,"data-quality":0.0695475518,"ml-security":0.0471694707}}
{"text":"Based on the Fourier Mellin Transformation (FMT) algorithm, the extended Fourier Mellin Transformation (eFMT) is an image registration approach that can be applied to downward-looking cameras, for example on aerial and underwater vehicles.","meta":{"url":"http://arxiv.org/abs/2307.10015v1"},"cats":{"new-dataset":0.0469675363,"dev-research":0.2178180127,"prompt-eng":0.4142547638,"data-quality":0.0720818213,"ml-security":0.0379727919}}
{"text":"eFMT extends FMT to multi-depth scenes and thus more application scenarios.","meta":{"url":"http://arxiv.org/abs/2307.10015v1"},"cats":{"new-dataset":0.090966137,"dev-research":0.2912716363,"prompt-eng":0.4101575,"data-quality":0.070215592,"ml-security":0.0376093741}}
{"text":"It is a visual odometry method which estimates the pose transformation between three overlapping images.","meta":{"url":"http://arxiv.org/abs/2307.10015v1"},"cats":{"new-dataset":0.1141696346,"dev-research":0.2828206264,"prompt-eng":0.3608090233,"data-quality":0.0854694931,"ml-security":0.0341989137}}
{"text":"On this basis, we develop an optimized eFMT algorithm that improves certain aspects of the method and combines it with back-end optimization for the small loop of three consecutive frames.","meta":{"url":"http://arxiv.org/abs/2307.10015v1"},"cats":{"new-dataset":0.0980559358,"dev-research":0.226930491,"prompt-eng":0.3803681519,"data-quality":0.0793887096,"ml-security":0.0210845508}}
{"text":"For this we investigate the extraction of uncertainty information from the eFMT registration, the related objective function and the graph-based optimization.","meta":{"url":"http://arxiv.org/abs/2307.10015v1"},"cats":{"new-dataset":0.0436953462,"dev-research":0.245219421,"prompt-eng":0.4533294003,"data-quality":0.2181876807,"ml-security":0.0431022476}}
{"text":"Finally, we design a series of experiments to investigate the properties of this approach and compare it with other VO and SLAM (Simultaneous Localization and Mapping) algorithms.","meta":{"url":"http://arxiv.org/abs/2307.10015v1"},"cats":{"new-dataset":0.0730617502,"dev-research":0.2087576643,"prompt-eng":0.361832483,"data-quality":0.1133614159,"ml-security":0.0374692251}}
{"text":"The results show the superior accuracy and speed of our o-eFMT approach, which is published as open source.","meta":{"url":"http://arxiv.org/abs/2307.10015v1"},"cats":{"new-dataset":0.109216588,"dev-research":0.2535477147,"prompt-eng":0.3628888015,"data-quality":0.1075048313,"ml-security":0.0413687163}}
{"text":"Deep learning-based person identification and verification systems have remarkably improved in terms of accuracy in recent years; however, such systems, including widely popular cloud-based solutions, have been found to exhibit significant biases related to race, age, and gender, a problem that requires in-depth exploration and solutions.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.1362035799,"dev-research":0.2873060803,"prompt-eng":0.343569068,"data-quality":0.149038416,"ml-security":0.2941913114}}
{"text":"This paper presents an in-depth analysis, with a particular emphasis on the intersectionality of these demographic factors.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.1793674432,"dev-research":0.3278039879,"prompt-eng":0.3687493779,"data-quality":0.0893572339,"ml-security":0.0869817983}}
{"text":"Intersectional bias refers to the performance discrepancies w.r.t.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.0166621724,"dev-research":0.2846875018,"prompt-eng":0.3625864213,"data-quality":0.1793650282,"ml-security":0.0749803339}}
{"text":"the different combinations of race, age, and gender groups, an area relatively unexplored in current literature.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.2501900095,"dev-research":0.2934128023,"prompt-eng":0.3174574591,"data-quality":0.0902709792,"ml-security":0.0872370863}}
{"text":"Furthermore, the reliance of most state-of-the-art approaches on accuracy as the principal evaluation metric often masks significant demographic disparities in performance.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.0249872729,"dev-research":0.3690521127,"prompt-eng":0.3673568369,"data-quality":0.1849234599,"ml-security":0.0853517624}}
{"text":"To counter this crucial limitation, we incorporate five additional metrics in our quantitative analysis, including disparate impact and mistreatment metrics, which are typically ignored by the relevant fairness-aware approaches.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.0447255285,"dev-research":0.3337489196,"prompt-eng":0.3446461723,"data-quality":0.1920531296,"ml-security":0.379621699}}
{"text":"Results on the Racial Faces in-the-Wild (RFW) benchmark indicate pervasive biases in face recognition systems, extending beyond race, with different demographic factors yielding significantly disparate outcomes.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.0442292741,"dev-research":0.2699815527,"prompt-eng":0.3722070614,"data-quality":0.1626224538,"ml-security":0.1973759748}}
{"text":"In particular, Africans demonstrate an 11.25% lower True Positive Rate (TPR) compared to Caucasians, while only a 3.51% accuracy drop is observed.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.0386962615,"dev-research":0.2602349322,"prompt-eng":0.4114756047,"data-quality":0.1983375396,"ml-security":0.1007931666}}
{"text":"Even more concerning, the intersections of multiple protected groups, such as African females over 60 years old, demonstrate a +39.89% disparate mistreatment rate compared to the highest Caucasians rate.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.0965114177,"dev-research":0.3161806544,"prompt-eng":0.3508453938,"data-quality":0.1509004464,"ml-security":0.345945157}}
{"text":"By shedding light on these biases and their implications, this paper aims to stimulate further research towards developing fairer, more equitable face recognition and verification systems.","meta":{"url":"http://arxiv.org/abs/2307.10011v1"},"cats":{"new-dataset":0.0425973506,"dev-research":0.2774238089,"prompt-eng":0.3775475105,"data-quality":0.1656621006,"ml-security":0.264536371}}
{"text":"Background and Context: Few instruments exist to measure students' CS engagement and learning especially in areas where coding happens with creative, project-based learning and in regard to students' self-beliefs about computing.","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.192142041,"dev-research":0.5246356903,"prompt-eng":0.4261048053,"data-quality":0.1237887851,"ml-security":0.1203289327}}
{"text":"Objective: We introduce the CS Interests and Beliefs Inventory (CSIBI), an instrument designed for novice secondary students learning by designing projects (particularly with physical computing).","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.3543996984,"dev-research":0.4498349173,"prompt-eng":0.4567351144,"data-quality":0.0759285819,"ml-security":0.1226528077}}
{"text":"The inventory contains subscales on beliefs on problem solving competency, fascination in design, value of CS, creative expression, and beliefs about context-specific CS abilities alongside programming mindsets and outcomes.","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.1663708056,"dev-research":0.4744281654,"prompt-eng":0.4358594361,"data-quality":0.0761642221,"ml-security":0.077507241}}
{"text":"We explain the creation of the instrument and attend to the role of mindsets as mediators of self-beliefs and how CSIBI may be adapted to other K-12 project-based learning settings.","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.3068864387,"dev-research":0.4069080968,"prompt-eng":0.4329345926,"data-quality":0.1049467078,"ml-security":0.1567058599}}
{"text":"Method: We administered the instrument to 303 novice CS secondary students who largely came from historically marginalized backgrounds (gender, ethnicity, and socioeconomic status).","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.1787725344,"dev-research":0.3468419508,"prompt-eng":0.3908909174,"data-quality":0.0960685849,"ml-security":0.0959244468}}
{"text":"We assessed the nine-factor structure for the 32-item instrument using confirmatory factor analysis and tested the hypothesized model of mindsets as mediators with structural equation modeling.","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.0593092224,"dev-research":0.3053737457,"prompt-eng":0.4490156743,"data-quality":0.1070302244,"ml-security":0.0654191759}}
{"text":"Findings:","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.2005913024,"dev-research":0.3553674677,"prompt-eng":0.3993966325,"data-quality":0.2207083378,"ml-security":0.0692466786}}
{"text":"We confirmed the nine factor structure of CSIBI and found significant positive correlations across factors.","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.1498138852,"dev-research":0.2968886159,"prompt-eng":0.4303839324,"data-quality":0.0926836431,"ml-security":0.0693614619}}
{"text":"The structural model results showed that problem solving competency beliefs and CS creative expression promoted programming growth mindset, which subsequently fostered students' programming self-concept.","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.0733646971,"dev-research":0.453238856,"prompt-eng":0.4103989976,"data-quality":0.1022481908,"ml-security":0.1108033992}}
{"text":"Implications: We validated an instrument to measure secondary students' self-beliefs in CS that fills several gaps in K-12 CS measurement tools by focusing on contexts of learning by designing.","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.1660719193,"dev-research":0.3959983206,"prompt-eng":0.4082526923,"data-quality":0.1427180973,"ml-security":0.1215889594}}
{"text":"CSIBI can be easily adapted to other learning by designing computing education contexts.","meta":{"url":"http://arxiv.org/abs/2307.10010v1"},"cats":{"new-dataset":0.1408850531,"dev-research":0.4431934009,"prompt-eng":0.4453195877,"data-quality":0.071539555,"ml-security":0.1933272604}}
{"text":"Audio-driven portrait animation aims to synthesize portrait videos that are conditioned by given audio.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.074722764,"dev-research":0.2723423592,"prompt-eng":0.3890982902,"data-quality":0.080506683,"ml-security":0.1129816906}}
{"text":"Animating high-fidelity and multimodal video portraits has a variety of applications.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.067737019,"dev-research":0.2331074013,"prompt-eng":0.3788507226,"data-quality":0.058308749,"ml-security":0.0743351279}}
{"text":"Previous methods have attempted to capture different motion modes and generate high-fidelity portrait videos by training different models or sampling signals from given videos.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.0804439205,"dev-research":0.2064224866,"prompt-eng":0.3987301858,"data-quality":0.0825306582,"ml-security":0.0885135783}}
{"text":"However, lacking correlation learning between lip-sync and other movements (e.g., head pose/eye blinking) usually leads to unnatural results.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.0147787542,"dev-research":0.273466762,"prompt-eng":0.3568285237,"data-quality":0.1800112287,"ml-security":0.1329366218}}
{"text":"In this paper, we propose a unified system for multi-person, diverse, and high-fidelity talking portrait generation.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.1455554585,"dev-research":0.2557815655,"prompt-eng":0.3946357954,"data-quality":0.0943700792,"ml-security":0.0710971221}}
{"text":"Our method contains three stages, i.e., 1) Mapping-Once network with Dual Attentions (MODA) generates talking representation from given audio.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.1092015897,"dev-research":0.2201585931,"prompt-eng":0.4147911594,"data-quality":0.1833122353,"ml-security":0.0464116333}}
{"text":"In MODA, we design a dual-attention module to encode accurate mouth movements and diverse modalities.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.0811313815,"dev-research":0.1849616522,"prompt-eng":0.4273761782,"data-quality":0.1602065046,"ml-security":0.0919526676}}
{"text":"2) Facial composer network generates dense and detailed face landmarks, and 3) temporal-guided renderer syntheses stable videos.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.0759875819,"dev-research":0.2923817875,"prompt-eng":0.3802857284,"data-quality":0.0808349031,"ml-security":0.0858370771}}
{"text":"Extensive evaluations demonstrate that the proposed system produces more natural and realistic video portraits compared to previous methods.","meta":{"url":"http://arxiv.org/abs/2307.10008v1"},"cats":{"new-dataset":0.0913203172,"dev-research":0.2622181101,"prompt-eng":0.3757485442,"data-quality":0.0841489526,"ml-security":0.0668152043}}
{"text":"6G is the next-generation intelligent and integrated digital information infrastructure, characterized by ubiquitous interconnection, native intelligence, multi-dimensional perception, global coverage, green and low-carbon, native network security, etc. 6G will realize the transition from serving people and people-things communication to supporting the efficient connection of intelligent agents, and comprehensively leading the digital, intelligent and green transformation of the economy and the society.","meta":{"url":"http://arxiv.org/abs/2307.10004v1"},"cats":{"new-dataset":0.2186635143,"dev-research":0.2768186058,"prompt-eng":0.3817026854,"data-quality":0.107176712,"ml-security":0.0692072148}}
{"text":"As the core support system for mobile communication network, 6 6G BSS need to integrate with new business models brought about by the development of the next-generation Internet and IT, upgrade from \"network-centric\" to \"business and service centric\" and \"customer-centric\".","meta":{"url":"http://arxiv.org/abs/2307.10004v1"},"cats":{"new-dataset":0.1137417188,"dev-research":0.283581106,"prompt-eng":0.3631310411,"data-quality":0.124913366,"ml-security":0.0652034899}}
{"text":"6G OSS and BSS systems need to strengthen their integration to improve the operational efficiency and benefits of customers by connecting the digital intelligence support capabilities on both sides of supply and demand.","meta":{"url":"http://arxiv.org/abs/2307.10004v1"},"cats":{"new-dataset":0.0614173823,"dev-research":0.393972881,"prompt-eng":0.4058567285,"data-quality":0.0910099611,"ml-security":0.1152457864}}
{"text":"This paper provides a detailed introduction to the overall vision, potential key technologies, and functional architecture of 6G BSS systems.","meta":{"url":"http://arxiv.org/abs/2307.10004v1"},"cats":{"new-dataset":0.0521332424,"dev-research":0.224978187,"prompt-eng":0.3819421816,"data-quality":0.0716611803,"ml-security":0.0429613218}}
{"text":"It also presents an evolutionary roadmap and technological prospects for the BSS systems from 5G to 6G.","meta":{"url":"http://arxiv.org/abs/2307.10004v1"},"cats":{"new-dataset":0.057298786,"dev-research":0.257177765,"prompt-eng":0.3653790733,"data-quality":0.0612243055,"ml-security":0.0813340387}}
{"text":"The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models.","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.0493081719,"dev-research":0.3799660417,"prompt-eng":0.3833842631,"data-quality":0.1896391956,"ml-security":0.3829921919}}
{"text":"Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions.","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.0679119698,"dev-research":0.3207352778,"prompt-eng":0.4610314081,"data-quality":0.1212492922,"ml-security":0.1581381309}}
{"text":"Heatmaps are almost understandable to humans, yet they are not without flaws.","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.1050228951,"dev-research":0.3610852376,"prompt-eng":0.4005032704,"data-quality":0.2294982901,"ml-security":0.1182941536}}
{"text":"Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors).","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.0654965269,"dev-research":0.3500193527,"prompt-eng":0.4512746181,"data-quality":0.1790948323,"ml-security":0.1171761588}}
{"text":"Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps.","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.053589268,"dev-research":0.2164685668,"prompt-eng":0.3903149599,"data-quality":0.177745708,"ml-security":0.1280907251}}
{"text":"In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models.","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.3769419549,"dev-research":0.3001489228,"prompt-eng":0.4480316409,"data-quality":0.3261700655,"ml-security":0.163917766}}
{"text":"Moreover, TbExplain incorporates a novel method to correct predictions and textually explain them based on the statistics of objects in the input image when the initial prediction is unreliable.","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.1142075874,"dev-research":0.3786896663,"prompt-eng":0.4599650129,"data-quality":0.2906761493,"ml-security":0.1069176232}}
{"text":"To assess the trustworthiness and validity of the text-based explanations, we conducted a qualitative experiment, and the findings indicated that these explanations are sufficiently reliable.","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.0233045755,"dev-research":0.4724964479,"prompt-eng":0.4355002609,"data-quality":0.308901352,"ml-security":0.1969892704}}
{"text":"Furthermore, our quantitative and qualitative experiments on TbExplain with scene classification datasets reveal an improvement in classification accuracy over ResNet variants.","meta":{"url":"http://arxiv.org/abs/2307.10003v1"},"cats":{"new-dataset":0.2413335091,"dev-research":0.2792705261,"prompt-eng":0.3915383203,"data-quality":0.2323605361,"ml-security":0.1225925468}}
{"text":"Motivated by the recent trend towards the usage of larger receptive fields for more context-aware neural networks in vision applications, we aim to investigate how large these receptive fields really need to be.","meta":{"url":"http://arxiv.org/abs/2307.10001v1"},"cats":{"new-dataset":0.0877499814,"dev-research":0.2262175364,"prompt-eng":0.3922843822,"data-quality":0.1144020065,"ml-security":0.1495604637}}
{"text":"To facilitate such study, several challenges need to be addressed, most importantly: (i) We need to provide an effective way for models to learn large filters (potentially as large as the input data) without increasing their memory consumption during training or inference, (ii) the study of filter sizes has to be decoupled from other effects such as the network width or number of learnable parameters, and (iii) the employed convolution operation should be a plug-and-play module that can replace any conventional convolution in a Convolutional Neural Network (CNN) and allow for an efficient implementation in current frameworks.","meta":{"url":"http://arxiv.org/abs/2307.10001v1"},"cats":{"new-dataset":0.0760763541,"dev-research":0.230301464,"prompt-eng":0.3424497687,"data-quality":0.1299603589,"ml-security":0.2093807794}}
{"text":"To facilitate such models, we propose to learn not spatial but frequency representations of filter weights as neural implicit functions, such that even infinitely large filters can be parameterized by only a few learnable weights.","meta":{"url":"http://arxiv.org/abs/2307.10001v1"},"cats":{"new-dataset":0.0480918182,"dev-research":0.186735799,"prompt-eng":0.3876831937,"data-quality":0.1639119096,"ml-security":0.2606291972}}
{"text":"The resulting neural implicit frequency CNNs are the first models to achieve results on par with the state-of-the-art on large image classification benchmarks while executing convolutions solely in the frequency domain and can be employed within any CNN architecture.","meta":{"url":"http://arxiv.org/abs/2307.10001v1"},"cats":{"new-dataset":0.1686328062,"dev-research":0.2574417139,"prompt-eng":0.4042591804,"data-quality":0.2474286112,"ml-security":0.1900688346}}
{"text":"They allow us to provide an extensive analysis of the learned receptive fields.","meta":{"url":"http://arxiv.org/abs/2307.10001v1"},"cats":{"new-dataset":0.0872464922,"dev-research":0.3327496351,"prompt-eng":0.4438419902,"data-quality":0.1230326908,"ml-security":0.1312280485}}
{"text":"Interestingly, our analysis shows that, although the proposed networks could learn very large convolution kernels, the learned filters practically translate into well-localized and relatively small convolution kernels in the spatial domain.","meta":{"url":"http://arxiv.org/abs/2307.10001v1"},"cats":{"new-dataset":0.1179260972,"dev-research":0.2588086517,"prompt-eng":0.346914178,"data-quality":0.1938536276,"ml-security":0.2275705788}}
{"text":"The derivation of mathematical results in specialised fields using Large Language Models (LLMs) is an emerging research direction that can help identify models' limitations, and potentially support mathematical discovery.","meta":{"url":"http://arxiv.org/abs/2307.09998v1"},"cats":{"new-dataset":0.0355137874,"dev-research":0.1976916627,"prompt-eng":0.477291947,"data-quality":0.1407042683,"ml-security":0.1099935205}}
{"text":"In this paper, we leverage a symbolic engine to generate derivations of equations at scale, and investigate the capabilities of LLMs when deriving goal equations from premises.","meta":{"url":"http://arxiv.org/abs/2307.09998v1"},"cats":{"new-dataset":0.0318755673,"dev-research":0.2797501214,"prompt-eng":0.5301003783,"data-quality":0.0775640838,"ml-security":0.1217617459}}
{"text":"Specifically, we employ in-context learning for GPT and fine-tune a range of T5 models to compare the robustness and generalisation of pre-training strategies to specialised models.","meta":{"url":"http://arxiv.org/abs/2307.09998v1"},"cats":{"new-dataset":0.0578386031,"dev-research":0.2180930354,"prompt-eng":0.4456336904,"data-quality":0.1158532368,"ml-security":0.1715012146}}
{"text":"Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in terms of absolute performance.","meta":{"url":"http://arxiv.org/abs/2307.09998v1"},"cats":{"new-dataset":0.108002994,"dev-research":0.2263202809,"prompt-eng":0.4080103357,"data-quality":0.0878139079,"ml-security":0.1304726448}}
{"text":"However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure.","meta":{"url":"http://arxiv.org/abs/2307.09998v1"},"cats":{"new-dataset":0.0062590442,"dev-research":0.2852917933,"prompt-eng":0.4301113261,"data-quality":0.1673023561,"ml-security":0.2210446238}}
{"text":"In addition, we analyse 1.7K equations and over 200 derivations to highlight common reasoning errors such as the inclusion of incorrect, irrelevant, and redundant equations, along with the tendency to skip derivation steps.","meta":{"url":"http://arxiv.org/abs/2307.09998v1"},"cats":{"new-dataset":0.0721594562,"dev-research":0.4019981656,"prompt-eng":0.425363683,"data-quality":0.24733725,"ml-security":0.1394114892}}
{"text":"Finally, we explore the suitability of existing metrics for evaluating mathematical derivations finding evidence that, while they capture general properties such as sensitivity to perturbations, they fail to highlight fine-grained reasoning errors and essential differences between models.","meta":{"url":"http://arxiv.org/abs/2307.09998v1"},"cats":{"new-dataset":0.0177257334,"dev-research":0.3842996787,"prompt-eng":0.4278100705,"data-quality":0.2936660672,"ml-security":0.2273622101}}
{"text":"Overall, this work demonstrates that training models on synthetic data can improve their mathematical capabilities beyond larger architectures.","meta":{"url":"http://arxiv.org/abs/2307.09998v1"},"cats":{"new-dataset":0.1371857583,"dev-research":0.2720044367,"prompt-eng":0.368449276,"data-quality":0.1182246323,"ml-security":0.2638042205}}
{"text":"To enable context-aware computer assistance in the operating room of the future, cognitive systems need to understand automatically which surgical phase is being performed by the medical team.","meta":{"url":"http://arxiv.org/abs/2307.09997v1"},"cats":{"new-dataset":0.0313785918,"dev-research":0.3916550384,"prompt-eng":0.4981394574,"data-quality":0.0887975839,"ml-security":0.0948287718}}
{"text":"The primary source of information for surgical phase recognition is typically video, which presents two challenges: extracting meaningful features from the video stream and effectively modeling temporal information in the sequence of visual features.","meta":{"url":"http://arxiv.org/abs/2307.09997v1"},"cats":{"new-dataset":0.1088445713,"dev-research":0.2110810116,"prompt-eng":0.3746484612,"data-quality":0.1170146207,"ml-security":0.0633054851}}
{"text":"For temporal modeling, attention mechanisms have gained popularity due to their ability to capture long-range dependencies.","meta":{"url":"http://arxiv.org/abs/2307.09997v1"},"cats":{"new-dataset":0.0577610882,"dev-research":0.2592813101,"prompt-eng":0.4660664906,"data-quality":0.0790077441,"ml-security":0.1261261739}}
{"text":"In this paper, we explore design choices for attention in existing temporal models for surgical phase recognition and propose a novel approach that does not resort to local attention or regularization of attention weights: TUNeS is an efficient and simple temporal model that incorporates self-attention at the coarsest stage of a U-Net-like structure.","meta":{"url":"http://arxiv.org/abs/2307.09997v1"},"cats":{"new-dataset":0.0299259938,"dev-research":0.1858223076,"prompt-eng":0.4303079761,"data-quality":0.1072611447,"ml-security":0.0775432895}}
{"text":"In addition, we propose to train the feature extractor, a standard CNN, together with an LSTM on preferably long video segments, i.e., with long temporal context.","meta":{"url":"http://arxiv.org/abs/2307.09997v1"},"cats":{"new-dataset":0.3101025941,"dev-research":0.2219865662,"prompt-eng":0.3832194535,"data-quality":0.1568619884,"ml-security":0.1180438883}}
{"text":"In our experiments, all temporal models performed better on top of feature extractors that were trained with longer temporal context.","meta":{"url":"http://arxiv.org/abs/2307.09997v1"},"cats":{"new-dataset":0.0472394998,"dev-research":0.2820547709,"prompt-eng":0.4275060578,"data-quality":0.153021089,"ml-security":0.1537607901}}
{"text":"On top of these contextualized features, TUNeS achieves state-of-the-art results on Cholec80.","meta":{"url":"http://arxiv.org/abs/2307.09997v1"},"cats":{"new-dataset":0.1350762603,"dev-research":0.3935991061,"prompt-eng":0.4383051988,"data-quality":0.1703739781,"ml-security":0.0406959302}}
{"text":"Deploying deep learning neural networks on edge devices, to accomplish task specific objectives in the real-world, requires a reduction in their memory footprint, power consumption, and latency.","meta":{"url":"http://arxiv.org/abs/2307.09994v1"},"cats":{"new-dataset":0.0405126231,"dev-research":0.3259554618,"prompt-eng":0.3465392375,"data-quality":0.0783126217,"ml-security":0.2379558338}}
{"text":"This can be realized via efficient model compression.","meta":{"url":"http://arxiv.org/abs/2307.09994v1"},"cats":{"new-dataset":0.0661642021,"dev-research":0.2112938571,"prompt-eng":0.4095517376,"data-quality":0.1729265841,"ml-security":0.1117067196}}
{"text":"Disentangled latent representations produced by variational autoencoder (VAE) networks are a promising approach for achieving model compression because they mainly retain task-specific information, discarding useless information for the task at hand.","meta":{"url":"http://arxiv.org/abs/2307.09994v1"},"cats":{"new-dataset":0.0128751118,"dev-research":0.259480356,"prompt-eng":0.391477537,"data-quality":0.1312914348,"ml-security":0.13747038}}
{"text":"We make use of the Beta-VAE framework combined with a standard criterion for pruning to investigate the impact of forcing the network to learn disentangled representations on the pruning process for the task of classification.","meta":{"url":"http://arxiv.org/abs/2307.09994v1"},"cats":{"new-dataset":0.009565191,"dev-research":0.2517633106,"prompt-eng":0.3672538687,"data-quality":0.24259671,"ml-security":0.2500936652}}
{"text":"In particular, we perform experiments on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose a path forward for future works.","meta":{"url":"http://arxiv.org/abs/2307.09994v1"},"cats":{"new-dataset":0.1222596799,"dev-research":0.2702313796,"prompt-eng":0.3754885229,"data-quality":0.1607131297,"ml-security":0.1860005699}}
{"text":"When doing private domain marketing with cloud services, the merchants usually have to purchase different machine learning models for the multiple marketing purposes, leading to a very high cost.","meta":{"url":"http://arxiv.org/abs/2307.09989v1"},"cats":{"new-dataset":0.0296984574,"dev-research":0.2411508276,"prompt-eng":0.3593553412,"data-quality":0.106139454,"ml-security":0.3441809043}}
{"text":"We present a unified user-item matching framework to simultaneously conduct item recommendation and user targeting with just one model.","meta":{"url":"http://arxiv.org/abs/2307.09989v1"},"cats":{"new-dataset":0.0306936984,"dev-research":0.259008643,"prompt-eng":0.4720936204,"data-quality":0.1031224197,"ml-security":0.0862863543}}
{"text":"We empirically demonstrate that the above concurrent modeling is viable via modeling the user-item interaction matrix with the multinomial distribution, and propose a bidirectional bias-corrected NCE loss for the implementation.","meta":{"url":"http://arxiv.org/abs/2307.09989v1"},"cats":{"new-dataset":0.0283434772,"dev-research":0.2320489831,"prompt-eng":0.4191874251,"data-quality":0.0787811981,"ml-security":0.0755477222}}
{"text":"The proposed loss function guides the model to learn the user-item joint probability $p(u,i)$ instead of the conditional probability $p(i|u)$ or $p(u|i)$ through correcting both the users and items' biases caused by the in-batch negative sampling.","meta":{"url":"http://arxiv.org/abs/2307.09989v1"},"cats":{"new-dataset":0.0386169187,"dev-research":0.2386877817,"prompt-eng":0.4445138083,"data-quality":0.2611498937,"ml-security":0.1809838193}}
{"text":"In addition, our framework is model-agnostic enabling a flexible adaptation of different model architectures.","meta":{"url":"http://arxiv.org/abs/2307.09989v1"},"cats":{"new-dataset":0.0193032796,"dev-research":0.2395182536,"prompt-eng":0.4160751383,"data-quality":0.0833576176,"ml-security":0.1019777596}}
{"text":"Extensive experiments demonstrate that our framework results in significant performance gains in comparison with the state-of-the-art methods, with greatly reduced cost on computing resources and daily maintenance.","meta":{"url":"http://arxiv.org/abs/2307.09989v1"},"cats":{"new-dataset":0.0657235285,"dev-research":0.3506749432,"prompt-eng":0.3697615997,"data-quality":0.0655798741,"ml-security":0.0553622514}}
{"text":"On-device training is essential for user personalisation and privacy.","meta":{"url":"http://arxiv.org/abs/2307.09988v1"},"cats":{"new-dataset":0.1128893926,"dev-research":0.3603048565,"prompt-eng":0.4192630561,"data-quality":0.1039717678,"ml-security":0.3285180815}}
{"text":"With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data.","meta":{"url":"http://arxiv.org/abs/2307.09988v1"},"cats":{"new-dataset":0.2308286144,"dev-research":0.2601971982,"prompt-eng":0.4353080565,"data-quality":0.0892693925,"ml-security":0.1154593765}}
{"text":"Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\\geq$10\\%).","meta":{"url":"http://arxiv.org/abs/2307.09988v1"},"cats":{"new-dataset":0.0211579516,"dev-research":0.3094796917,"prompt-eng":0.3620631418,"data-quality":0.218705839,"ml-security":0.1923844593}}
{"text":"We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity.","meta":{"url":"http://arxiv.org/abs/2307.09988v1"},"cats":{"new-dataset":0.0935578735,"dev-research":0.3061218106,"prompt-eng":0.372864932,"data-quality":0.1513590944,"ml-security":0.2439295109}}
{"text":"TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint.","meta":{"url":"http://arxiv.org/abs/2307.09988v1"},"cats":{"new-dataset":0.0299502308,"dev-research":0.2946885723,"prompt-eng":0.4023569542,"data-quality":0.116045919,"ml-security":0.1066039041}}
{"text":"TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0\\% in accuracy, while reducing the backward-pass memory and computation cost by up to 2,286$\\times$ and 7.68$\\times$, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09988v1"},"cats":{"new-dataset":0.0313028282,"dev-research":0.3123443952,"prompt-eng":0.3513661049,"data-quality":0.1571265782,"ml-security":0.1061970743}}
{"text":"Targeting broadly used real-world edge devices, TinyTrain achieves 9.5$\\times$ faster and 3.5$\\times$ more energy-efficient training over status-quo approaches, and 2.8$\\times$ smaller memory footprint than SOTA approaches, while remaining within the 1 MB memory envelope of MCU-grade platforms.","meta":{"url":"http://arxiv.org/abs/2307.09988v1"},"cats":{"new-dataset":0.0263900995,"dev-research":0.2628014368,"prompt-eng":0.3404617952,"data-quality":0.0864780006,"ml-security":0.0930118763}}
{"text":"Numerous discussions have advocated the presence of a so called rabbit-hole (RH) phenomenon on social media, interested in advanced personalization to their users.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.0177579926,"dev-research":0.3255752913,"prompt-eng":0.4574564706,"data-quality":0.1247066176,"ml-security":0.1045372571}}
{"text":"This phenomenon is loosely understood as a collapse of mainstream recommendations, in favor of ultra personalized ones that lock users into narrow and specialized feeds.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.0189498104,"dev-research":0.3224912798,"prompt-eng":0.4174864537,"data-quality":0.1173074424,"ml-security":0.2049806701}}
{"text":"Yet quantitative studies are often ignoring personalization, are of limited scale, and rely on manual tagging to track this collapse.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.0283096043,"dev-research":0.3486476679,"prompt-eng":0.417245193,"data-quality":0.3188491573,"ml-security":0.1071165887}}
{"text":"This precludes a precise understanding of the phenomenon based on reproducible observations, and thus the continuous audits of platforms.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.1021382393,"dev-research":0.3837169927,"prompt-eng":0.4038085272,"data-quality":0.1912912881,"ml-security":0.1656611214}}
{"text":"In this paper, we first tackle the scale issue by proposing a user-sided bot-centric approach that enables large scale data collection, through autoplay walks on recommendations.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.1919518152,"dev-research":0.3277090784,"prompt-eng":0.3978151424,"data-quality":0.1059850721,"ml-security":0.1023762277}}
{"text":"We then propose a simple theory that explains the appearance of these RHs.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.0093056359,"dev-research":0.2759989115,"prompt-eng":0.4029619554,"data-quality":0.1084243704,"ml-security":0.1089807698}}
{"text":"While this theory is a simplifying viewpoint on a complex and planet-wide phenomenon, it carries multiple advantages: it can be analytically modeled, and provides a general yet rigorous definition of RHs.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.0052616283,"dev-research":0.2467992766,"prompt-eng":0.3345162611,"data-quality":0.0387198137,"ml-security":0.0941709773}}
{"text":"We define them as an interplay between i) user interaction with personalization and ii) the attraction strength of certain video categories, which cause users to quickly step apart of mainstream recommendations made to fresh user profiles.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.0292725246,"dev-research":0.3503997323,"prompt-eng":0.4364894809,"data-quality":0.1505096791,"ml-security":0.1071059909}}
{"text":"We illustrate these concepts by highlighting some RHs found after collecting more than 16 million personalized recommendations on YouTube.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.1700958546,"dev-research":0.3376971604,"prompt-eng":0.4289332408,"data-quality":0.1532355021,"ml-security":0.080706413}}
{"text":"A final validation step compares our automatically-identified RHs against manually-identified RHs from a previous research work.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.0234622703,"dev-research":0.3002546922,"prompt-eng":0.4696235047,"data-quality":0.2026785961,"ml-security":0.0858511159}}
{"text":"Together, those results pave the way for large scale and automated audits of the RH effect in recommendation systems.","meta":{"url":"http://arxiv.org/abs/2307.09986v1"},"cats":{"new-dataset":0.0402072526,"dev-research":0.3063558383,"prompt-eng":0.4143830719,"data-quality":0.1499437631,"ml-security":0.1097557423}}
{"text":"A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.3131950623,"dev-research":0.3609254027,"prompt-eng":0.4143053077,"data-quality":0.1001180784,"ml-security":0.1055545728}}
{"text":"The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.0270982165,"dev-research":0.3958265488,"prompt-eng":0.5058820981,"data-quality":0.0911411664,"ml-security":0.0714251133}}
{"text":"In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.3149680717,"dev-research":0.3504405792,"prompt-eng":0.3492312426,"data-quality":0.1707650608,"ml-security":0.1376865029}}
{"text":"We make a few main findings from our analysis.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.1765423889,"dev-research":0.4136232106,"prompt-eng":0.3765317547,"data-quality":0.1865614385,"ml-security":0.1189041347}}
{"text":"First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.0578460785,"dev-research":0.3608184273,"prompt-eng":0.4084162647,"data-quality":0.0723384697,"ml-security":0.0658073331}}
{"text":"The early interactions largely define the user portrait which affect the subsequent interactions.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.0378239627,"dev-research":0.3563801923,"prompt-eng":0.4274406723,"data-quality":0.0644943449,"ml-security":0.1424009233}}
{"text":"Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s).","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.0335201881,"dev-research":0.372761676,"prompt-eng":0.4060946216,"data-quality":0.1445327394,"ml-security":0.2041570391}}
{"text":"Removal of interactions that happen nearer to the last few interactions of a user leads to increasing difficulty in learning user preference, thus deteriorating recommendation accuracy.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.0123160506,"dev-research":0.3614683202,"prompt-eng":0.4212838569,"data-quality":0.1731459046,"ml-security":0.1470618277}}
{"text":"Third, changing the order of user interactions makes it more difficult for sequential algorithms to capture the progressive interaction process.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.0142293752,"dev-research":0.3361398664,"prompt-eng":0.4149055155,"data-quality":0.0537411067,"ml-security":0.0932676318}}
{"text":"Based on these findings, we further discuss the discrepancy between the interaction generation mechanism that is employed by the MovieLens system and that of typical real world recommendation scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.0571338757,"dev-research":0.3280009709,"prompt-eng":0.4705881134,"data-quality":0.1141299245,"ml-security":0.0893374239}}
{"text":"In summary, models that achieve excellent recommendation accuracy on the MovieLens dataset may not demonstrate superior performance in practice for at least two kinds of differences: (i) the differences in the contexts of user-item interaction generation, and (ii) the differences in user knowledge about the item collections.","meta":{"url":"http://arxiv.org/abs/2307.09985v1"},"cats":{"new-dataset":0.0805324703,"dev-research":0.3369171306,"prompt-eng":0.3905305738,"data-quality":0.1507555172,"ml-security":0.0572985534}}
{"text":"Visual (re)localization is critical for various applications in computer vision and robotics.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.0363284958,"dev-research":0.2921533767,"prompt-eng":0.423556881,"data-quality":0.1977249247,"ml-security":0.0595372008}}
{"text":"Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.5582385793,"dev-research":0.1895541451,"prompt-eng":0.36165589,"data-quality":0.0634396811,"ml-security":0.0645189339}}
{"text":"Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.2720086507,"dev-research":0.2459325609,"prompt-eng":0.3920525349,"data-quality":0.0884108986,"ml-security":0.0538196993}}
{"text":"On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.0344001278,"dev-research":0.3130041209,"prompt-eng":0.3564728297,"data-quality":0.127662244,"ml-security":0.0868044523}}
{"text":"It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.0119170327,"dev-research":0.431982701,"prompt-eng":0.3880727162,"data-quality":0.0984091479,"ml-security":0.0935670256}}
{"text":"In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.2803660066,"dev-research":0.2388542273,"prompt-eng":0.4253203777,"data-quality":0.2956486883,"ml-security":0.0595304238}}
{"text":"The key to achieving this owes to a tailored motion averaging over database-query pairs.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.0958150569,"dev-research":0.2042093023,"prompt-eng":0.4023675214,"data-quality":0.0883726775,"ml-security":0.0347384029}}
{"text":"Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.0843159968,"dev-research":0.2994311131,"prompt-eng":0.4360541643,"data-quality":0.138668983,"ml-security":0.0363239144}}
{"text":"Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.","meta":{"url":"http://arxiv.org/abs/2307.09981v1"},"cats":{"new-dataset":0.1290014964,"dev-research":0.2788350701,"prompt-eng":0.4331450526,"data-quality":0.1042692394,"ml-security":0.0362765887}}
{"text":"Highly skilled professionals' forced migration from Ukraine was triggered by the conflict in Ukraine in 2014 and amplified by the Russian invasion in 2022.","meta":{"url":"http://arxiv.org/abs/2307.09979v1"},"cats":{"new-dataset":0.1384785776,"dev-research":0.3609693761,"prompt-eng":0.4009033342,"data-quality":0.0908232875,"ml-security":0.2181697374}}
{"text":"Here, we utilize LinkedIn estimates and official refugee data from the World Bank and the United Nations Refugee Agency, to understand which are the main pull factors that drive the decision-making process of the host country.","meta":{"url":"http://arxiv.org/abs/2307.09979v1"},"cats":{"new-dataset":0.2013937852,"dev-research":0.3542236075,"prompt-eng":0.4033315332,"data-quality":0.0849473287,"ml-security":0.1120319898}}
{"text":"We identify an ongoing and escalating exodus of educated individuals, largely drawn to Poland and Germany, and underscore the crucial role of pre-existing networks in shaping these migration flows.","meta":{"url":"http://arxiv.org/abs/2307.09979v1"},"cats":{"new-dataset":0.0552379552,"dev-research":0.2986895285,"prompt-eng":0.3792060324,"data-quality":0.1351330366,"ml-security":0.1090097295}}
{"text":"Key findings include a strong correlation between LinkedIn's estimates of highly educated Ukrainian displaced people and official UN refugee statistics, pointing to the significance of prior relationships with Ukraine in determining migration destinations.","meta":{"url":"http://arxiv.org/abs/2307.09979v1"},"cats":{"new-dataset":0.1402114678,"dev-research":0.3683199493,"prompt-eng":0.37919668,"data-quality":0.1365667481,"ml-security":0.1190423339}}
{"text":"We train a series of multilinear regression models and the SHAP method revealing that the existence of a support network is the most critical factor in choosing a destination country, while distance is less important.","meta":{"url":"http://arxiv.org/abs/2307.09979v1"},"cats":{"new-dataset":0.0451140805,"dev-research":0.2254739229,"prompt-eng":0.3842207619,"data-quality":0.0962918328,"ml-security":0.1009499672}}
{"text":"Our main findings show that the migration patterns of Ukraine's highly skilled workforce, and their impact on both the origin and host countries, are largely influenced by preexisting networks and communities.","meta":{"url":"http://arxiv.org/abs/2307.09979v1"},"cats":{"new-dataset":0.0610051995,"dev-research":0.3497314253,"prompt-eng":0.40254676,"data-quality":0.0960942718,"ml-security":0.1193928708}}
{"text":"This insight can inform strategies to tackle the economic challenges posed by this loss of talent and maximize the benefits of such migration for both Ukraine and the receiving nations.","meta":{"url":"http://arxiv.org/abs/2307.09979v1"},"cats":{"new-dataset":0.0446230087,"dev-research":0.3058056962,"prompt-eng":0.3718590554,"data-quality":0.0885219501,"ml-security":0.1600351443}}
{"text":"The paradigm of federated learning (FL) to address data privacy concerns by locally training parameters on resource-constrained clients in a distributed manner has garnered significant attention.","meta":{"url":"http://arxiv.org/abs/2307.09977v1"},"cats":{"new-dataset":0.1209791312,"dev-research":0.228621758,"prompt-eng":0.3782472605,"data-quality":0.1494541421,"ml-security":0.5162265867}}
{"text":"Nonetheless, FL is not applicable when not all clients within the coverage of the FL server are registered with the FL network.","meta":{"url":"http://arxiv.org/abs/2307.09977v1"},"cats":{"new-dataset":0.0484247667,"dev-research":0.2350601718,"prompt-eng":0.3279272173,"data-quality":0.1633481551,"ml-security":0.133041422}}
{"text":"To bridge this gap, this paper proposes joint learner referral aided federated client selection (LRef-FedCS), along with communications and computing resource scheduling, and local model accuracy optimization (LMAO) methods.","meta":{"url":"http://arxiv.org/abs/2307.09977v1"},"cats":{"new-dataset":0.0530308041,"dev-research":0.2494537351,"prompt-eng":0.4132337743,"data-quality":0.1254255629,"ml-security":0.1192770665}}
{"text":"These methods are designed to minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things (HieIoT) networks.","meta":{"url":"http://arxiv.org/abs/2307.09977v1"},"cats":{"new-dataset":0.0254079851,"dev-research":0.2508739943,"prompt-eng":0.3979469086,"data-quality":0.0701969228,"ml-security":0.1474354284}}
{"text":"Utilizing the Lyapunov optimization technique, we reformulate the original problem into a stepwise joint optimization problem (JOP).","meta":{"url":"http://arxiv.org/abs/2307.09977v1"},"cats":{"new-dataset":0.0635814828,"dev-research":0.1977725834,"prompt-eng":0.3463009806,"data-quality":0.074131177,"ml-security":0.0827312938}}
{"text":"Subsequently, to tackle the mixed-integer non-convex JOP, we separatively and iteratively address LRef-FedCS and LMAO through the centralized method and self-adaptive global best harmony search (SGHS) algorithm, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09977v1"},"cats":{"new-dataset":0.0437102791,"dev-research":0.1341739034,"prompt-eng":0.3602057952,"data-quality":0.0884890589,"ml-security":0.0525233046}}
{"text":"To enhance scalability, we further propose a distributed LRef-FedCS approach based on a matching game to replace the centralized method described above.","meta":{"url":"http://arxiv.org/abs/2307.09977v1"},"cats":{"new-dataset":0.1162031766,"dev-research":0.2091690263,"prompt-eng":0.3815423079,"data-quality":0.0875483745,"ml-security":0.0797040694}}
{"text":"Numerical simulations and experimental results on the MNIST/CIFAR-10 datasets demonstrate that our proposed LRef-FedCS approach could achieve a good balance between pursuing high global accuracy and reducing cost.","meta":{"url":"http://arxiv.org/abs/2307.09977v1"},"cats":{"new-dataset":0.1400464737,"dev-research":0.1955941038,"prompt-eng":0.3832592608,"data-quality":0.1262194258,"ml-security":0.0666025269}}
{"text":"Physical unclonable functions (PUFs) are hardware-oriented primitives that exploit manufacturing variations to generate a unique identity for a physical system.","meta":{"url":"http://arxiv.org/abs/2307.09968v1"},"cats":{"new-dataset":0.0721102861,"dev-research":0.3298957492,"prompt-eng":0.3778021144,"data-quality":0.1448229872,"ml-security":0.2269696496}}
{"text":"Recent advancements showed how DRAM can be exploited to implement PUFs.","meta":{"url":"http://arxiv.org/abs/2307.09968v1"},"cats":{"new-dataset":0.0709819484,"dev-research":0.2777519668,"prompt-eng":0.3940488663,"data-quality":0.0958791199,"ml-security":0.2468984176}}
{"text":"DRAM PUFs require no additional circuits for PUF operations and can be used in most of the applications with resource-constrained nodes such as Internet of Things (IoT) networks.","meta":{"url":"http://arxiv.org/abs/2307.09968v1"},"cats":{"new-dataset":0.0342172437,"dev-research":0.2340617284,"prompt-eng":0.3549750479,"data-quality":0.0649324201,"ml-security":0.1065821641}}
{"text":"However, the existing DRAM PUF solutions either require to interrupt other functions in the host system, or provide unreliable responses due to their sensitiveness to the environmental conditions.   ","meta":{"url":"http://arxiv.org/abs/2307.09968v1"},"cats":{"new-dataset":0.0394747626,"dev-research":0.269902655,"prompt-eng":0.4300071819,"data-quality":0.1074259051,"ml-security":0.2024725929}}
{"text":"In this paper, we propose EPUF, a novel strategy to extract random and unique features from DRAM cells to generate reliable PUF responses.","meta":{"url":"http://arxiv.org/abs/2307.09968v1"},"cats":{"new-dataset":0.1260186473,"dev-research":0.248275696,"prompt-eng":0.4417323872,"data-quality":0.1669548888,"ml-security":0.1484452915}}
{"text":"In particular, we use the bitmap images of the binary DRAM values and their entropy features.","meta":{"url":"http://arxiv.org/abs/2307.09968v1"},"cats":{"new-dataset":0.1662331456,"dev-research":0.2853587418,"prompt-eng":0.4397352487,"data-quality":0.1275900907,"ml-security":0.1393783583}}
{"text":"We show via real device experiments that EPUF is approximately $1.7$ times faster than other state of the art solutions, achieves $100\\%$ reliability, generates features with $47.79\\%$ uniqueness, and supports a large set of CRP that leads to new potentials for DRAM PUF-based authentication.","meta":{"url":"http://arxiv.org/abs/2307.09968v1"},"cats":{"new-dataset":0.0974797875,"dev-research":0.3087287713,"prompt-eng":0.446659231,"data-quality":0.125190992,"ml-security":0.2167076234}}
{"text":"We also propose a lightweight authentication protocol based on EPUF, which not only provides far better security guarantees but also outperforms the state-of-the-art in terms of communication overhead and computational cost.","meta":{"url":"http://arxiv.org/abs/2307.09968v1"},"cats":{"new-dataset":0.0453829418,"dev-research":0.3063382833,"prompt-eng":0.4204770654,"data-quality":0.0716697712,"ml-security":0.295216864}}
{"text":"Nowadays, AI-based systems have achieved outstanding results and have outperformed humans in different domains.","meta":{"url":"http://arxiv.org/abs/2307.09964v1"},"cats":{"new-dataset":0.0501624302,"dev-research":0.3598221893,"prompt-eng":0.3900976043,"data-quality":0.1347909667,"ml-security":0.151013394}}
{"text":"However, the processes of training AI models and inferring from them require high computational resources, which pose a significant challenge in the current energy efficiency societal demand.","meta":{"url":"http://arxiv.org/abs/2307.09964v1"},"cats":{"new-dataset":0.0509176449,"dev-research":0.2547351274,"prompt-eng":0.3996493868,"data-quality":0.0973111454,"ml-security":0.2128375448}}
{"text":"To cope with this challenge, this research project paper describes the main vision, goals, and expected outcomes of the GAISSA project.","meta":{"url":"http://arxiv.org/abs/2307.09964v1"},"cats":{"new-dataset":0.1742288418,"dev-research":0.2974026856,"prompt-eng":0.3731715368,"data-quality":0.0651119689,"ml-security":0.060868226}}
{"text":"The GAISSA project aims at providing data scientists and software engineers tool-supported, architecture-centric methods for the modelling and development of green AI-based systems.","meta":{"url":"http://arxiv.org/abs/2307.09964v1"},"cats":{"new-dataset":0.2238848377,"dev-research":0.3627334905,"prompt-eng":0.3677244211,"data-quality":0.0698362323,"ml-security":0.1085590239}}
{"text":"Although the project is in an initial stage, we describe the current research results, which illustrate the potential to achieve GAISSA objectives.","meta":{"url":"http://arxiv.org/abs/2307.09964v1"},"cats":{"new-dataset":0.201432893,"dev-research":0.2889100893,"prompt-eng":0.3616731013,"data-quality":0.054459543,"ml-security":0.0421764204}}
{"text":"We study dynamic algorithms in the model of algorithms with predictions.","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.0288729973,"dev-research":0.2338445181,"prompt-eng":0.3987795213,"data-quality":0.108821232,"ml-security":0.2610400634}}
{"text":"We assume the algorithm is given imperfect predictions regarding future updates, and we ask how such predictions can be used to improve the running time.","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.06650905,"dev-research":0.3373938413,"prompt-eng":0.4005861908,"data-quality":0.1161661365,"ml-security":0.1917753814}}
{"text":"This can be seen as a model interpolating between classic online and offline dynamic algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.0414806731,"dev-research":0.2096122068,"prompt-eng":0.3735838405,"data-quality":0.0558234007,"ml-security":0.0976968132}}
{"text":"Our results give smooth tradeoffs between these two extreme settings.   ","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.0303141015,"dev-research":0.2620616503,"prompt-eng":0.3841529793,"data-quality":0.0766001057,"ml-security":0.0754126472}}
{"text":"First, we give algorithms for incremental and decremental transitive closure and approximate APSP that take as an additional input a predicted sequence of updates (edge insertions, or edge deletions, respectively).","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.1134142519,"dev-research":0.270505532,"prompt-eng":0.4001632184,"data-quality":0.1173961029,"ml-security":0.0771974582}}
{"text":"They preprocess it in $\\tilde{O}(n^{(3+\\omega)/2})$ time, and then handle updates in $\\tilde{O}(1)$ worst-case time and queries in $\\tilde{O}(\\eta^2)$ worst-case time.","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.0263629231,"dev-research":0.2727583047,"prompt-eng":0.3480144338,"data-quality":0.0947444168,"ml-security":0.1146690386}}
{"text":"Here $\\eta$ is an error measure that can be bounded by the maximum difference between the predicted and actual insertion (deletion) time of an edge, i.e., by the $\\ell_\\infty$-error of the predictions.   ","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.0556350459,"dev-research":0.2526499494,"prompt-eng":0.3626543235,"data-quality":0.2125941321,"ml-security":0.1123748153}}
{"text":"The second group of results concerns fully dynamic problems with vertex updates, where the algorithm has access to a predicted sequence of the next $n$ updates.","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.0624187808,"dev-research":0.2490954496,"prompt-eng":0.3675333742,"data-quality":0.1289424253,"ml-security":0.2067923759}}
{"text":"We show how to solve fully dynamic triangle detection, maximum matching, single-source reachability, and more, in $O(n^{\\omega-1}+n\\eta_i)$ worst-case update time.","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.1316968529,"dev-research":0.2318653631,"prompt-eng":0.3298193936,"data-quality":0.1169316927,"ml-security":0.1444231645}}
{"text":"Here $\\eta_i$ denotes how much earlier the $i$-th update occurs than predicted.   ","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.159062493,"dev-research":0.3196944384,"prompt-eng":0.4137862385,"data-quality":0.1636997561,"ml-security":0.1345757868}}
{"text":"Our last result is a reduction that transforms a worst-case incremental algorithm without predictions into a fully dynamic algorithm which is given a predicted deletion time for each element at the time of its insertion.","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.0668819699,"dev-research":0.2564975017,"prompt-eng":0.3477590895,"data-quality":0.1843148876,"ml-security":0.17431719}}
{"text":"As a consequence we can, e.g., maintain fully dynamic exact APSP with such predictions in $\\tilde{O}(n^2)$ worst-case vertex insertion time and $\\tilde{O}(n^2 (1+\\eta_i))$ worst-case vertex deletion time (for the prediction error $\\eta_i$ defined as above).","meta":{"url":"http://arxiv.org/abs/2307.09961v1"},"cats":{"new-dataset":0.0250762874,"dev-research":0.2354033834,"prompt-eng":0.3812716041,"data-quality":0.156898607,"ml-security":0.1781628724}}
{"text":"Extracting workflow nets from textual descriptions can be used to simplify guidelines or formalize textual descriptions of formal processes like business processes and algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09959v1"},"cats":{"new-dataset":0.051632112,"dev-research":0.4345887909,"prompt-eng":0.426338287,"data-quality":0.1652570576,"ml-security":0.0549196089}}
{"text":"The task of manually extracting processes, however, requires domain expertise and effort.","meta":{"url":"http://arxiv.org/abs/2307.09959v1"},"cats":{"new-dataset":0.0300794868,"dev-research":0.454670922,"prompt-eng":0.4582596762,"data-quality":0.1448365398,"ml-security":0.0830506413}}
{"text":"While automatic process model extraction is desirable, annotating texts with formalized process models is expensive.","meta":{"url":"http://arxiv.org/abs/2307.09959v1"},"cats":{"new-dataset":0.0694186057,"dev-research":0.3698256221,"prompt-eng":0.5061855146,"data-quality":0.3095175288,"ml-security":0.060364008}}
{"text":"Therefore, there are only a few machine-learning-based extraction approaches.","meta":{"url":"http://arxiv.org/abs/2307.09959v1"},"cats":{"new-dataset":0.0538932313,"dev-research":0.2344870728,"prompt-eng":0.3464901213,"data-quality":0.1997938205,"ml-security":0.1359856719}}
{"text":"Rule-based approaches, in turn, require domain specificity to work well and can rarely distinguish relevant and irrelevant information in textual descriptions.","meta":{"url":"http://arxiv.org/abs/2307.09959v1"},"cats":{"new-dataset":0.016664324,"dev-research":0.4059987534,"prompt-eng":0.4127330074,"data-quality":0.3749186269,"ml-security":0.0826766356}}
{"text":"In this paper, we present GUIDO, a hybrid approach to the process model extraction task that first, classifies sentences regarding their relevance to the process model, using a BERT-based sentence classifier, and second, extracts a process model from the sentences classified as relevant, using dependency parsing.","meta":{"url":"http://arxiv.org/abs/2307.09959v1"},"cats":{"new-dataset":0.0612666758,"dev-research":0.3075101593,"prompt-eng":0.4648475618,"data-quality":0.1625964163,"ml-security":0.0652042826}}
{"text":"The presented approach achieves significantly better results than a pure rule-based approach.","meta":{"url":"http://arxiv.org/abs/2307.09959v1"},"cats":{"new-dataset":0.0195336281,"dev-research":0.3815493416,"prompt-eng":0.3904966663,"data-quality":0.1837975874,"ml-security":0.0820220047}}
{"text":"GUIDO achieves an average behavioral similarity score of $0.93$. Still, in comparison to purely machine-learning-based approaches, the annotation costs stay low.","meta":{"url":"http://arxiv.org/abs/2307.09959v1"},"cats":{"new-dataset":0.1224249122,"dev-research":0.3214515696,"prompt-eng":0.4507047222,"data-quality":0.3017784836,"ml-security":0.1525481295}}
{"text":"Network operators and researchers frequently use Internet measurement platforms (IMPs), such as RIPE Atlas, RIPE RIS, or RouteViews for, e.g., monitoring network performance, detecting routing events, topology discovery, or route optimization.","meta":{"url":"http://arxiv.org/abs/2307.09958v1"},"cats":{"new-dataset":0.0830601519,"dev-research":0.3456591835,"prompt-eng":0.4204191281,"data-quality":0.0979004665,"ml-security":0.1254952938}}
{"text":"To interpret the results of their measurements and avoid pitfalls or wrong generalizations, users must understand a platform's limitations.","meta":{"url":"http://arxiv.org/abs/2307.09958v1"},"cats":{"new-dataset":0.0487380666,"dev-research":0.4803504646,"prompt-eng":0.3866319806,"data-quality":0.1421394607,"ml-security":0.158690463}}
{"text":"To this end, this paper studies an important limitation of IMPs, the \\textit{bias}, which exists due to the non-uniform deployment of the vantage points.","meta":{"url":"http://arxiv.org/abs/2307.09958v1"},"cats":{"new-dataset":0.0206164608,"dev-research":0.2218182247,"prompt-eng":0.3759535138,"data-quality":0.0996980405,"ml-security":0.1453402324}}
{"text":"Specifically, we introduce a generic framework to systematically and comprehensively quantify the multi-dimensional (e.g., across location, topology, network types, etc.)","meta":{"url":"http://arxiv.org/abs/2307.09958v1"},"cats":{"new-dataset":0.1057465224,"dev-research":0.2951308644,"prompt-eng":0.3698338837,"data-quality":0.1022791033,"ml-security":0.0999120767}}
{"text":"biases of IMPs.","meta":{"url":"http://arxiv.org/abs/2307.09958v1"},"cats":{"new-dataset":0.0181219206,"dev-research":0.2901587493,"prompt-eng":0.4175290927,"data-quality":0.188578409,"ml-security":0.1735548001}}
{"text":"Using the framework and open datasets, we perform a detailed analysis of biases in IMPs that confirms well-known (to the domain experts) biases and sheds light on less-known or unexplored biases.","meta":{"url":"http://arxiv.org/abs/2307.09958v1"},"cats":{"new-dataset":0.0936647562,"dev-research":0.3865740066,"prompt-eng":0.3827504504,"data-quality":0.2561525968,"ml-security":0.3182858647}}
{"text":"To facilitate IMP users to obtain awareness of and explore bias in their measurements, as well as further research and analyses (e.g., methods for mitigating bias), we publicly share our code and data, and provide online tools (API, Web app, etc.) that calculate and visualize the bias in measurement setups.","meta":{"url":"http://arxiv.org/abs/2307.09958v1"},"cats":{"new-dataset":0.1097533103,"dev-research":0.5208886896,"prompt-eng":0.4290205732,"data-quality":0.1563610895,"ml-security":0.1873824201}}
{"text":"Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior.","meta":{"url":"http://arxiv.org/abs/2307.09955v1"},"cats":{"new-dataset":0.3205426425,"dev-research":0.3252777535,"prompt-eng":0.4631149729,"data-quality":0.1207213368,"ml-security":0.1539485948}}
{"text":"However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters.","meta":{"url":"http://arxiv.org/abs/2307.09955v1"},"cats":{"new-dataset":0.0423824612,"dev-research":0.2287790191,"prompt-eng":0.3887642584,"data-quality":0.0780037167,"ml-security":0.1086802949}}
{"text":"To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video.","meta":{"url":"http://arxiv.org/abs/2307.09955v1"},"cats":{"new-dataset":0.0698780578,"dev-research":0.2349394205,"prompt-eng":0.4447687271,"data-quality":0.0933767138,"ml-security":0.1217937652}}
{"text":"Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework.","meta":{"url":"http://arxiv.org/abs/2307.09955v1"},"cats":{"new-dataset":0.0739026198,"dev-research":0.2575793364,"prompt-eng":0.4029884652,"data-quality":0.0660695582,"ml-security":0.1518669869}}
{"text":"The performance of XSkill is best understood from the anonymous website: https://xskillcorl.github.io.","meta":{"url":"http://arxiv.org/abs/2307.09955v1"},"cats":{"new-dataset":0.0513401707,"dev-research":0.2598045082,"prompt-eng":0.3668043162,"data-quality":0.1075787775,"ml-security":0.0716069944}}
{"text":"We present a new symbolic execution semantics of probabilistic programs that include observe statements and sampling from continuous distributions.","meta":{"url":"http://arxiv.org/abs/2307.09951v1"},"cats":{"new-dataset":0.0547501906,"dev-research":0.3793671813,"prompt-eng":0.4915598643,"data-quality":0.1357897438,"ml-security":0.1242035592}}
{"text":"Building on Kozen's seminal work, this symbolic semantics consists of a countable collection of measurable functions, along with a partition of the state space.","meta":{"url":"http://arxiv.org/abs/2307.09951v1"},"cats":{"new-dataset":0.1398551867,"dev-research":0.2561055665,"prompt-eng":0.4074341046,"data-quality":0.1006600666,"ml-security":0.0689502095}}
{"text":"We use the new semantics to provide a full correctness proof of symbolic execution for probabilistic programs.","meta":{"url":"http://arxiv.org/abs/2307.09951v1"},"cats":{"new-dataset":0.0291242075,"dev-research":0.4061299416,"prompt-eng":0.482523438,"data-quality":0.2283723356,"ml-security":0.1617528022}}
{"text":"We also implement this semantics in the tool symProb, and illustrate its use on examples.","meta":{"url":"http://arxiv.org/abs/2307.09951v1"},"cats":{"new-dataset":0.0354683535,"dev-research":0.4069791001,"prompt-eng":0.4626117486,"data-quality":0.1663328859,"ml-security":0.0929883726}}
{"text":"Log parsing, the initial and vital stage in automated log analysis, involves extracting log templates from semi-structured logs to generate structured logs.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.0832821248,"dev-research":0.3799911743,"prompt-eng":0.4571335273,"data-quality":0.19566969,"ml-security":0.0942839941}}
{"text":"Nonetheless, current log parsers are limited in effectiveness due to two primary reasons.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.0241514774,"dev-research":0.3077707675,"prompt-eng":0.3529146802,"data-quality":0.1581882133,"ml-security":0.1527355792}}
{"text":"Firstly, traditional data-driven log parsers heavily rely on heuristics or manually crafted features provided by domain experts, which may not consistently yield optimal performance when applied to diverse log systems.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.051757782,"dev-research":0.3561517683,"prompt-eng":0.4198883446,"data-quality":0.1616476795,"ml-security":0.1304529128}}
{"text":"Secondly, existing deep learning-based log parsers necessitate model tuning, which is typically confined to training samples and leads to suboptimal performance across the entire log source.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.0395622909,"dev-research":0.2806322556,"prompt-eng":0.3681981308,"data-quality":0.1814826753,"ml-security":0.2116260064}}
{"text":"To overcome these limitations, we propose a precise log parsing framework named LogDiv, which leverages the in-context inference capability of large language models.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.205958053,"dev-research":0.2886427875,"prompt-eng":0.4385006298,"data-quality":0.2269469895,"ml-security":0.1149843985}}
{"text":"Specifically, LogDiv extracts the hidden semantics from multiple log examples through prompt demonstrations.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.1493874003,"dev-research":0.3973982464,"prompt-eng":0.4951346029,"data-quality":0.217576022,"ml-security":0.164260186}}
{"text":"Without the need for model tuning, LogDiv can directly generate a log template for the target log message by leveraging the semantics provided in the prompt context.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.0783769193,"dev-research":0.3438060054,"prompt-eng":0.5588972642,"data-quality":0.152771963,"ml-security":0.1268642713}}
{"text":"Additionally, we introduce a simple yet effective prompt format for extracting the output and enhancing the quality of the generated log templates.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.2405237132,"dev-research":0.4119438536,"prompt-eng":0.5458018941,"data-quality":0.2608681795,"ml-security":0.1220131658}}
{"text":"To validate the performance of LogDiv, we conducted experiments using 16 widely-used public datasets.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.5233302472,"dev-research":0.3220203929,"prompt-eng":0.3811908286,"data-quality":0.1388847258,"ml-security":0.2139561985}}
{"text":"The results show that LogDiv achieves state-of-the-art performance with an average parsing accuracy of 97.7%, precision template accuracy of 88.1%, and recall template accuracy of 90.8%.","meta":{"url":"http://arxiv.org/abs/2307.09950v1"},"cats":{"new-dataset":0.177644014,"dev-research":0.3438593591,"prompt-eng":0.4405218002,"data-quality":0.2009978434,"ml-security":0.0705712611}}
{"text":"Deep neural networks have shown exceptional performance in various tasks, but their lack of robustness, reliability, and tendency to be overconfident pose challenges for their deployment in safety-critical applications like autonomous driving.","meta":{"url":"http://arxiv.org/abs/2307.09947v1"},"cats":{"new-dataset":0.1062559769,"dev-research":0.3109799866,"prompt-eng":0.3514486673,"data-quality":0.2721534775,"ml-security":0.5168280466}}
{"text":"In this regard, quantifying the uncertainty inherent to a model's prediction is a promising endeavour to address these shortcomings.","meta":{"url":"http://arxiv.org/abs/2307.09947v1"},"cats":{"new-dataset":0.0166273353,"dev-research":0.3331949895,"prompt-eng":0.4228654868,"data-quality":0.2977704887,"ml-security":0.2698294475}}
{"text":"In this work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that incorporates dynamic predictive uncertainties into the training process by pixel-wise weighting of the well-known cross-entropy loss (CE).","meta":{"url":"http://arxiv.org/abs/2307.09947v1"},"cats":{"new-dataset":0.0734723874,"dev-research":0.2191293469,"prompt-eng":0.4412966772,"data-quality":0.2093762094,"ml-security":0.1699735949}}
{"text":"Through extensive experimentation, we demonstrate the superiority of U-CE over regular CE training on two benchmark datasets, Cityscapes and ACDC, using two common backbone architectures, ResNet-18 and ResNet-101.","meta":{"url":"http://arxiv.org/abs/2307.09947v1"},"cats":{"new-dataset":0.4452826864,"dev-research":0.3043886951,"prompt-eng":0.4165981721,"data-quality":0.1643486478,"ml-security":0.1128323312}}
{"text":"With U-CE, we manage to train models that not only improve their segmentation performance but also provide meaningful uncertainties after training.","meta":{"url":"http://arxiv.org/abs/2307.09947v1"},"cats":{"new-dataset":0.0482352761,"dev-research":0.2565315545,"prompt-eng":0.4722693474,"data-quality":0.2958380037,"ml-security":0.1459886668}}
{"text":"Consequently, we contribute to the development of more robust and reliable segmentation models, ultimately advancing the state-of-the-art in safety-critical applications and beyond.","meta":{"url":"http://arxiv.org/abs/2307.09947v1"},"cats":{"new-dataset":0.1929689567,"dev-research":0.2155769978,"prompt-eng":0.374968048,"data-quality":0.3176193296,"ml-security":0.2263629886}}
{"text":"Capsule Networks have emerged as a powerful class of deep learning architectures, known for robust performance with relatively few parameters compared to Convolutional Neural Networks (CNNs).","meta":{"url":"http://arxiv.org/abs/2307.09944v1"},"cats":{"new-dataset":0.0758361549,"dev-research":0.2012004852,"prompt-eng":0.36579683,"data-quality":0.1827170314,"ml-security":0.2458896747}}
{"text":"However, their inherent efficiency is often overshadowed by their slow, iterative routing mechanisms which establish connections between Capsule layers, posing computational challenges resulting in an inability to scale.","meta":{"url":"http://arxiv.org/abs/2307.09944v1"},"cats":{"new-dataset":0.012341272,"dev-research":0.2328554258,"prompt-eng":0.3732967917,"data-quality":0.072535669,"ml-security":0.0727653499}}
{"text":"In this paper, we introduce a novel, non-iterative routing mechanism, inspired by trainable prototype clustering.","meta":{"url":"http://arxiv.org/abs/2307.09944v1"},"cats":{"new-dataset":0.051275861,"dev-research":0.2278013744,"prompt-eng":0.39040832,"data-quality":0.1149215958,"ml-security":0.0859661321}}
{"text":"This innovative approach aims to mitigate computational complexity, while retaining, if not enhancing, performance efficacy.","meta":{"url":"http://arxiv.org/abs/2307.09944v1"},"cats":{"new-dataset":0.0077855118,"dev-research":0.3959644575,"prompt-eng":0.3434171256,"data-quality":0.0535609822,"ml-security":0.1157609467}}
{"text":"Furthermore, we harness a shared Capsule subspace, negating the need to project each lower-level Capsule to each higher-level Capsule, thereby significantly reducing memory requisites during training.","meta":{"url":"http://arxiv.org/abs/2307.09944v1"},"cats":{"new-dataset":0.0403877611,"dev-research":0.2327956742,"prompt-eng":0.4327683865,"data-quality":0.1417823522,"ml-security":0.1396736052}}
{"text":"Our approach demonstrates superior results compared to the current best non-iterative Capsule Network and tests on the Imagewoof dataset, which is too computationally demanding to handle efficiently by iterative approaches.","meta":{"url":"http://arxiv.org/abs/2307.09944v1"},"cats":{"new-dataset":0.3122431512,"dev-research":0.2054591764,"prompt-eng":0.3651023593,"data-quality":0.1728300725,"ml-security":0.0692988033}}
{"text":"Our findings underscore the potential of our proposed methodology in enhancing the operational efficiency and performance of Capsule Networks, paving the way for their application in increasingly complex computational scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09944v1"},"cats":{"new-dataset":0.0338647074,"dev-research":0.2223897728,"prompt-eng":0.3783078285,"data-quality":0.0941096444,"ml-security":0.0962859915}}
{"text":"Recommender systems are a ubiquitous feature of online platforms.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0574460812,"dev-research":0.3754533041,"prompt-eng":0.4289981531,"data-quality":0.0806896701,"ml-security":0.1437364784}}
{"text":"Increasingly, they are explicitly tasked with increasing users' long-term satisfaction.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0189404141,"dev-research":0.4101606974,"prompt-eng":0.4273015507,"data-quality":0.0870288035,"ml-security":0.0694562163}}
{"text":"In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0429668667,"dev-research":0.1978328591,"prompt-eng":0.4710100237,"data-quality":0.104941388,"ml-security":0.1177526588}}
{"text":"We observe that there is an apparent trade-off in choosing the learning signal:","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0109312022,"dev-research":0.2884504497,"prompt-eng":0.4236452417,"data-quality":0.0952894948,"ml-security":0.1993217985}}
{"text":"Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.011251196,"dev-research":0.1983794339,"prompt-eng":0.4134867296,"data-quality":0.0710115153,"ml-security":0.1447439389}}
{"text":"We address this challenge in two steps.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.080058104,"dev-research":0.2915161715,"prompt-eng":0.4309990056,"data-quality":0.16362517,"ml-security":0.0863123317}}
{"text":"First, we develop a predictive model of delayed rewards that incorporates all information obtained to date.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0572262846,"dev-research":0.2242593,"prompt-eng":0.4774911168,"data-quality":0.0935432568,"ml-security":0.169769909}}
{"text":"Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0561419637,"dev-research":0.2445328205,"prompt-eng":0.4636772018,"data-quality":0.1074868653,"ml-security":0.0828561551}}
{"text":"Second, we devise a bandit algorithm that takes advantage of this new predictive model.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0323643843,"dev-research":0.204381273,"prompt-eng":0.4411453812,"data-quality":0.1049730142,"ml-security":0.3138823127}}
{"text":"The algorithm quickly learns to identify content aligned with long-term success by carefully balancing exploration and exploitation.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0338910586,"dev-research":0.2831480117,"prompt-eng":0.4390553723,"data-quality":0.1356502287,"ml-security":0.0922749048}}
{"text":"We apply our approach to a podcast recommendation problem, where we seek to identify shows that users engage with repeatedly over two months.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.1846825439,"dev-research":0.3381321637,"prompt-eng":0.4296844703,"data-quality":0.2103784701,"ml-security":0.1416920821}}
{"text":"We empirically validate that our approach results in substantially better performance compared to approaches that either optimize for short-term proxies, or wait for the long-term outcome to be fully realized.","meta":{"url":"http://arxiv.org/abs/2307.09943v1"},"cats":{"new-dataset":0.0102725472,"dev-research":0.2698826903,"prompt-eng":0.4037449975,"data-quality":0.0903325161,"ml-security":0.1221488081}}
{"text":"Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment.","meta":{"url":"http://arxiv.org/abs/2307.09942v1"},"cats":{"new-dataset":0.0390739338,"dev-research":0.311012074,"prompt-eng":0.3788834653,"data-quality":0.0953956847,"ml-security":0.1647117331}}
{"text":"In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials.","meta":{"url":"http://arxiv.org/abs/2307.09942v1"},"cats":{"new-dataset":0.0475418096,"dev-research":0.253591216,"prompt-eng":0.4092781876,"data-quality":0.1016801193,"ml-security":0.1894876601}}
{"text":"However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.   ","meta":{"url":"http://arxiv.org/abs/2307.09942v1"},"cats":{"new-dataset":0.0036810252,"dev-research":0.3098094699,"prompt-eng":0.3741299401,"data-quality":0.1746236979,"ml-security":0.1855834971}}
{"text":"To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT.","meta":{"url":"http://arxiv.org/abs/2307.09942v1"},"cats":{"new-dataset":0.1164723695,"dev-research":0.264512447,"prompt-eng":0.3834289812,"data-quality":0.1357628323,"ml-security":0.1115555088}}
{"text":"It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to offer a granular level of alignment for improved performance and interpretability.","meta":{"url":"http://arxiv.org/abs/2307.09942v1"},"cats":{"new-dataset":0.0910671433,"dev-research":0.2828239551,"prompt-eng":0.4414416689,"data-quality":0.0812903742,"ml-security":0.0571502974}}
{"text":"We evaluated TREEMENT against existing models on real-world datasets and demonstrated that TREEMENT outperforms the best baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results in its trial-level matching ability.","meta":{"url":"http://arxiv.org/abs/2307.09942v1"},"cats":{"new-dataset":0.1338183025,"dev-research":0.2793361991,"prompt-eng":0.3746759716,"data-quality":0.2118838533,"ml-security":0.0906533351}}
{"text":"Furthermore, we also show TREEMENT can offer good interpretability to make the model results easier for adoption.","meta":{"url":"http://arxiv.org/abs/2307.09942v1"},"cats":{"new-dataset":0.0248759652,"dev-research":0.3685058369,"prompt-eng":0.4650263014,"data-quality":0.1960607402,"ml-security":0.0801289878}}
{"text":"This paper focuses on motion prediction for point cloud sequences in the challenging case of deformable 3D objects, such as human body motion.","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.2846317143,"dev-research":0.2073721278,"prompt-eng":0.3424449628,"data-quality":0.0547155534,"ml-security":0.1099601423}}
{"text":"First, we investigate the challenges caused by deformable shapes and complex motions present in this type of representation, with the ultimate goal of understanding the technical limitations of state-of-the-art models.","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.0304899507,"dev-research":0.2183717062,"prompt-eng":0.3741499916,"data-quality":0.0513049655,"ml-security":0.0908720845}}
{"text":"From this understanding, we propose an improved architecture for point cloud prediction of deformable 3D objects.","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.1416441238,"dev-research":0.23368545,"prompt-eng":0.3530913247,"data-quality":0.0789662265,"ml-security":0.1265605272}}
{"text":"Specifically, to handle deformable shapes, we propose a graph-based approach that learns and exploits the spatial structure of point clouds to extract more representative features.","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.164519888,"dev-research":0.2807110043,"prompt-eng":0.3536589493,"data-quality":0.1222336524,"ml-security":0.1405785648}}
{"text":"Then we propose a module able to combine the learned features in an adaptative manner according to the point cloud movements.","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.108356638,"dev-research":0.2540684411,"prompt-eng":0.4092730479,"data-quality":0.0899480688,"ml-security":0.0846746118}}
{"text":"The proposed adaptative module controls the composition of local and global motions for each point, enabling the network to model complex motions in deformable 3D objects more effectively.","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.0424510545,"dev-research":0.230382271,"prompt-eng":0.361429199,"data-quality":0.0526922358,"ml-security":0.0468733185}}
{"text":"We tested the proposed method on the following datasets: MNIST moving digits, the Mixamo human bodies motions, JPEG and CWIPC-SXR real-world dynamic bodies.","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.5443877601,"dev-research":0.1947869951,"prompt-eng":0.3712199317,"data-quality":0.0802328026,"ml-security":0.0627734598}}
{"text":"Simulation results demonstrate that our method outperforms the current baseline methods given its improved ability to model complex movements as well as preserve point cloud shape.","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.086045111,"dev-research":0.2282258473,"prompt-eng":0.3483041716,"data-quality":0.044516521,"ml-security":0.0470841477}}
{"text":"Furthermore, we demonstrate the generalizability of the proposed framework for dynamic feature learning, by testing the framework for action recognition on the MSRAction3D dataset and achieving results on-par with state-of-the-art methods","meta":{"url":"http://arxiv.org/abs/2307.09936v1"},"cats":{"new-dataset":0.181159053,"dev-research":0.2591227935,"prompt-eng":0.3714496774,"data-quality":0.1209155729,"ml-security":0.1918646348}}
{"text":"To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the \"spurious\" or unstable features whose relationship with the label changes across domains.","meta":{"url":"http://arxiv.org/abs/2307.09933v1"},"cats":{"new-dataset":0.139282273,"dev-research":0.3035816722,"prompt-eng":0.4411513514,"data-quality":0.6552431073,"ml-security":0.20816092}}
{"text":"However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain.","meta":{"url":"http://arxiv.org/abs/2307.09933v1"},"cats":{"new-dataset":0.0146738425,"dev-research":0.4019625635,"prompt-eng":0.4569335564,"data-quality":0.5556738514,"ml-security":0.2240610859}}
{"text":"Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels.","meta":{"url":"http://arxiv.org/abs/2307.09933v1"},"cats":{"new-dataset":0.0559515401,"dev-research":0.3546227302,"prompt-eng":0.4855165663,"data-quality":0.5518969403,"ml-security":0.2322810849}}
{"text":"In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label.","meta":{"url":"http://arxiv.org/abs/2307.09933v1"},"cats":{"new-dataset":0.1052179492,"dev-research":0.2725031781,"prompt-eng":0.4766159373,"data-quality":0.5086196932,"ml-security":0.1810671454}}
{"text":"Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain.","meta":{"url":"http://arxiv.org/abs/2307.09933v1"},"cats":{"new-dataset":0.0598186641,"dev-research":0.3362887567,"prompt-eng":0.4523640904,"data-quality":0.3433679998,"ml-security":0.303600014}}
{"text":"Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels.","meta":{"url":"http://arxiv.org/abs/2307.09933v1"},"cats":{"new-dataset":0.0493806097,"dev-research":0.1894266736,"prompt-eng":0.4472371753,"data-quality":0.2534645318,"ml-security":0.2510228028}}
{"text":"Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.","meta":{"url":"http://arxiv.org/abs/2307.09933v1"},"cats":{"new-dataset":0.2460797996,"dev-research":0.3684049615,"prompt-eng":0.3794693942,"data-quality":0.20890795,"ml-security":0.1636922915}}
{"text":"Multimodal image registration is a challenging but essential step for numerous image-guided procedures.","meta":{"url":"http://arxiv.org/abs/2307.09931v1"},"cats":{"new-dataset":0.0518008566,"dev-research":0.2246184558,"prompt-eng":0.4664615482,"data-quality":0.1090479203,"ml-security":0.0366142376}}
{"text":"Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities.","meta":{"url":"http://arxiv.org/abs/2307.09931v1"},"cats":{"new-dataset":0.0204464114,"dev-research":0.2210215785,"prompt-eng":0.3684649859,"data-quality":0.1487504554,"ml-security":0.063887265}}
{"text":"Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings.","meta":{"url":"http://arxiv.org/abs/2307.09931v1"},"cats":{"new-dataset":0.0284219288,"dev-research":0.2321077409,"prompt-eng":0.333584975,"data-quality":0.1002242722,"ml-security":0.1643012784}}
{"text":"We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration.","meta":{"url":"http://arxiv.org/abs/2307.09931v1"},"cats":{"new-dataset":0.1059051101,"dev-research":0.2624171553,"prompt-eng":0.4290303151,"data-quality":0.0806506126,"ml-security":0.0892951146}}
{"text":"We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data.","meta":{"url":"http://arxiv.org/abs/2307.09931v1"},"cats":{"new-dataset":0.148626741,"dev-research":0.298442787,"prompt-eng":0.3601288558,"data-quality":0.2333437691,"ml-security":0.2698885573}}
{"text":"Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one.","meta":{"url":"http://arxiv.org/abs/2307.09931v1"},"cats":{"new-dataset":0.0721644185,"dev-research":0.2946383635,"prompt-eng":0.3673717695,"data-quality":0.1612455752,"ml-security":0.051763722}}
{"text":"Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining.","meta":{"url":"http://arxiv.org/abs/2307.09931v1"},"cats":{"new-dataset":0.2304576187,"dev-research":0.2319129768,"prompt-eng":0.3565542999,"data-quality":0.1659372948,"ml-security":0.1545092572}}
{"text":"We make our training code and data publicly available.","meta":{"url":"http://arxiv.org/abs/2307.09931v1"},"cats":{"new-dataset":0.5966901944,"dev-research":0.3522595763,"prompt-eng":0.4106285985,"data-quality":0.1527561699,"ml-security":0.4224436665}}
{"text":"Effectively measuring and modeling the reliability of a trained model is essential to the real-world deployment of monocular depth estimation (MDE) models.","meta":{"url":"http://arxiv.org/abs/2307.09929v1"},"cats":{"new-dataset":0.0500604972,"dev-research":0.2048158735,"prompt-eng":0.4047801429,"data-quality":0.1821422415,"ml-security":0.0812500657}}
{"text":"However, the intrinsic ill-posedness and ordinal-sensitive nature of MDE pose major challenges to the estimation of uncertainty degree of the trained models.","meta":{"url":"http://arxiv.org/abs/2307.09929v1"},"cats":{"new-dataset":0.0252711704,"dev-research":0.1750283689,"prompt-eng":0.4356979573,"data-quality":0.2379248105,"ml-security":0.2553617842}}
{"text":"On the one hand, utilizing current uncertainty modeling methods may increase memory consumption and are usually time-consuming.","meta":{"url":"http://arxiv.org/abs/2307.09929v1"},"cats":{"new-dataset":0.0112469925,"dev-research":0.3233178082,"prompt-eng":0.3970923712,"data-quality":0.1281323937,"ml-security":0.1344909567}}
{"text":"On the other hand, measuring the uncertainty based on model accuracy can also be problematic, where uncertainty reliability and prediction accuracy are not well decoupled.","meta":{"url":"http://arxiv.org/abs/2307.09929v1"},"cats":{"new-dataset":0.0098451028,"dev-research":0.3261290895,"prompt-eng":0.369453299,"data-quality":0.3445233423,"ml-security":0.1568274403}}
{"text":"In this paper, we propose to model the uncertainty of MDE models from the perspective of the inherent probability distributions originating from the depth probability volume and its extensions, and to assess it more fairly with more comprehensive metrics.","meta":{"url":"http://arxiv.org/abs/2307.09929v1"},"cats":{"new-dataset":0.043700171,"dev-research":0.1845296112,"prompt-eng":0.4128566997,"data-quality":0.15317966,"ml-security":0.089426934}}
{"text":"By simply introducing additional training regularization terms, our model, with surprisingly simple formations and without requiring extra modules or multiple inferences, can provide uncertainty estimations with state-of-the-art reliability, and can be further improved when combined with ensemble or sampling methods.","meta":{"url":"http://arxiv.org/abs/2307.09929v1"},"cats":{"new-dataset":0.075474224,"dev-research":0.2047942622,"prompt-eng":0.4203238083,"data-quality":0.4285535805,"ml-security":0.1804942226}}
{"text":"A series of experiments demonstrate the effectiveness of our methods.","meta":{"url":"http://arxiv.org/abs/2307.09929v1"},"cats":{"new-dataset":0.0090859305,"dev-research":0.3413970706,"prompt-eng":0.457715481,"data-quality":0.1565383811,"ml-security":0.1370929268}}
{"text":"Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes.","meta":{"url":"http://arxiv.org/abs/2307.09923v1"},"cats":{"new-dataset":0.0554741352,"dev-research":0.3307859782,"prompt-eng":0.4226571408,"data-quality":0.0753406797,"ml-security":0.0541088463}}
{"text":"To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents.","meta":{"url":"http://arxiv.org/abs/2307.09923v1"},"cats":{"new-dataset":0.0766242061,"dev-research":0.3683375499,"prompt-eng":0.4124362997,"data-quality":0.1934007033,"ml-security":0.0725926327}}
{"text":"Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques.","meta":{"url":"http://arxiv.org/abs/2307.09923v1"},"cats":{"new-dataset":0.1129253203,"dev-research":0.3509118905,"prompt-eng":0.429345611,"data-quality":0.2198596842,"ml-security":0.0597727847}}
{"text":"These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument.","meta":{"url":"http://arxiv.org/abs/2307.09923v1"},"cats":{"new-dataset":0.0234341194,"dev-research":0.2460208046,"prompt-eng":0.3506056154,"data-quality":0.0863146562,"ml-security":0.0813975187}}
{"text":"However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable.","meta":{"url":"http://arxiv.org/abs/2307.09923v1"},"cats":{"new-dataset":0.1061056419,"dev-research":0.2190168323,"prompt-eng":0.499676168,"data-quality":0.1127549053,"ml-security":0.0767793401}}
{"text":"In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process models from textual descriptions, and assessing the suitability of process tasks from textual descriptions for robotic process automation.","meta":{"url":"http://arxiv.org/abs/2307.09923v1"},"cats":{"new-dataset":0.0911531992,"dev-research":0.3075272785,"prompt-eng":0.5197673985,"data-quality":0.1560329444,"ml-security":0.0538578432}}
{"text":"We show that, without extensive configuration or prompt engineering, LLMs perform comparably to or better than existing solutions and discuss implications for future BPM research as well as practical usage.","meta":{"url":"http://arxiv.org/abs/2307.09923v1"},"cats":{"new-dataset":0.0592759419,"dev-research":0.2548749101,"prompt-eng":0.5392335606,"data-quality":0.0997501939,"ml-security":0.0891830856}}
{"text":"Users and businesses are increasingly deploying Internet of Things (IoT) devices at home, at work, and in factories.","meta":{"url":"http://arxiv.org/abs/2307.09918v1"},"cats":{"new-dataset":0.0825754095,"dev-research":0.4037744969,"prompt-eng":0.4259755347,"data-quality":0.0610969098,"ml-security":0.0972382761}}
{"text":"At the same time, we see an increase in the use of IPv6 for Internet connectivity.","meta":{"url":"http://arxiv.org/abs/2307.09918v1"},"cats":{"new-dataset":0.0566080752,"dev-research":0.3770923279,"prompt-eng":0.3590330478,"data-quality":0.0734777352,"ml-security":0.1192694019}}
{"text":"Even though the IoT ecosystem has been the focus of recent studies, there is no comprehensive analysis of IoT end-hosts in the IPv6 Internet to date.","meta":{"url":"http://arxiv.org/abs/2307.09918v1"},"cats":{"new-dataset":0.24407321,"dev-research":0.2694615981,"prompt-eng":0.3326591693,"data-quality":0.067582002,"ml-security":0.0991451185}}
{"text":"In this paper we perform an in-depth analysis of IPv6-reachable IoT hosts using active measurements.","meta":{"url":"http://arxiv.org/abs/2307.09918v1"},"cats":{"new-dataset":0.2488489348,"dev-research":0.2815237888,"prompt-eng":0.375875084,"data-quality":0.0640863708,"ml-security":0.0836111209}}
{"text":"We run measurements targeting 530M IPv6 addresses on six popular IoT-related protocols.","meta":{"url":"http://arxiv.org/abs/2307.09918v1"},"cats":{"new-dataset":0.3112994217,"dev-research":0.2779465643,"prompt-eng":0.3952567328,"data-quality":0.0655773308,"ml-security":0.1031646773}}
{"text":"With 36.4K hosts in 156 countries we find 380x fewer IoT-speaking end-hosts compared to IPv4.","meta":{"url":"http://arxiv.org/abs/2307.09918v1"},"cats":{"new-dataset":0.3127445933,"dev-research":0.2536536521,"prompt-eng":0.3652497505,"data-quality":0.0732982936,"ml-security":0.0668111959}}
{"text":"Moreover, we conduct a security analysis for TLS-enabled IoT-speaking hosts identifying up to 57% untrusted certificates, with up to 32% being self-signed and 25% being expired.","meta":{"url":"http://arxiv.org/abs/2307.09918v1"},"cats":{"new-dataset":0.1996694937,"dev-research":0.2926371933,"prompt-eng":0.4106658041,"data-quality":0.1471616382,"ml-security":0.3555573909}}
{"text":"Finally, we plan to publish our measurement results, tools, and a website dashboard to foster further research in the area.","meta":{"url":"http://arxiv.org/abs/2307.09918v1"},"cats":{"new-dataset":0.3807365667,"dev-research":0.3959180665,"prompt-eng":0.4532244107,"data-quality":0.0929791327,"ml-security":0.0442543906}}
{"text":"Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models.","meta":{"url":"http://arxiv.org/abs/2307.09916v1"},"cats":{"new-dataset":0.0888283386,"dev-research":0.2556440394,"prompt-eng":0.3547487104,"data-quality":0.0595646488,"ml-security":0.2030649406}}
{"text":"Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning.","meta":{"url":"http://arxiv.org/abs/2307.09916v1"},"cats":{"new-dataset":0.0456889299,"dev-research":0.4507140581,"prompt-eng":0.467152392,"data-quality":0.2463147725,"ml-security":0.1675069147}}
{"text":"However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable.","meta":{"url":"http://arxiv.org/abs/2307.09916v1"},"cats":{"new-dataset":0.0297407621,"dev-research":0.4288322099,"prompt-eng":0.4780098952,"data-quality":0.2276047703,"ml-security":0.1743899423}}
{"text":"To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations.","meta":{"url":"http://arxiv.org/abs/2307.09916v1"},"cats":{"new-dataset":0.1184240957,"dev-research":0.3411120347,"prompt-eng":0.398088772,"data-quality":0.0782952691,"ml-security":0.0914189532}}
{"text":"The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions.","meta":{"url":"http://arxiv.org/abs/2307.09916v1"},"cats":{"new-dataset":0.0883153201,"dev-research":0.3422625552,"prompt-eng":0.463323368,"data-quality":0.1500096978,"ml-security":0.1997955218}}
{"text":"Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance.","meta":{"url":"http://arxiv.org/abs/2307.09916v1"},"cats":{"new-dataset":0.0835077838,"dev-research":0.2944345095,"prompt-eng":0.3928436124,"data-quality":0.0688208989,"ml-security":0.0390071531}}
{"text":"We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants.","meta":{"url":"http://arxiv.org/abs/2307.09916v1"},"cats":{"new-dataset":0.1581943352,"dev-research":0.229267181,"prompt-eng":0.3727467078,"data-quality":0.067371451,"ml-security":0.0897050096}}
{"text":"Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.","meta":{"url":"http://arxiv.org/abs/2307.09916v1"},"cats":{"new-dataset":0.0490765278,"dev-research":0.4788559053,"prompt-eng":0.4715076096,"data-quality":0.1153465229,"ml-security":0.1492486381}}
{"text":"Cross-lingual image captioning is confronted with both cross-lingual and cross-modal challenges for multimedia analysis.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.1891537874,"dev-research":0.2647865089,"prompt-eng":0.4428515144,"data-quality":0.3371717327,"ml-security":0.0680055816}}
{"text":"The crucial issue in this task is to model the global and local matching between the image and different languages.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.2139178869,"dev-research":0.2027499578,"prompt-eng":0.4577784099,"data-quality":0.2177662286,"ml-security":0.0377256232}}
{"text":"Existing cross-modal embedding methods based on Transformer architecture oversight the local matching between the image region and monolingual words, not to mention in the face of a variety of differentiated languages.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.0672653071,"dev-research":0.2354763211,"prompt-eng":0.4358382326,"data-quality":0.227405681,"ml-security":0.066385839}}
{"text":"Due to the heterogeneous property of the cross-modal and cross-lingual task, we utilize the heterogeneous network to establish cross-domain relationships and the local correspondences between the image and different languages.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.1140821869,"dev-research":0.2462201274,"prompt-eng":0.4394875288,"data-quality":0.21476101,"ml-security":0.0392576116}}
{"text":"In this paper, we propose an Embedded Heterogeneous Attention Transformer (EHAT) to build reasoning paths bridging cross-domain for cross-lingual image captioning and integrate into transformer.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.1746305465,"dev-research":0.2752590378,"prompt-eng":0.4701311031,"data-quality":0.231730629,"ml-security":0.0420766176}}
{"text":"The proposed EHAT consists of a Masked Heterogeneous Cross-attention (MHCA), Heterogeneous Attention Reasoning Network (HARN) and Heterogeneous Co-attention (HCA).","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.127413937,"dev-research":0.2294062739,"prompt-eng":0.4155063049,"data-quality":0.1257523256,"ml-security":0.0722032947}}
{"text":"HARN as the core network, models and infers cross-domain relationship anchored by vision bounding box representation features to connect two languages word features and learn the heterogeneous maps.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.1989237212,"dev-research":0.2991066909,"prompt-eng":0.4140368795,"data-quality":0.2069946983,"ml-security":0.080387037}}
{"text":"MHCA and HCA implement cross-domain integration in the encoder through the special heterogeneous attention and enable single model to generate two language captioning.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.1508012049,"dev-research":0.2190572442,"prompt-eng":0.48552926,"data-quality":0.2178093145,"ml-security":0.0463753151}}
{"text":"We test on MSCOCO dataset to generate English and Chinese, which are most widely used and have obvious difference between their language families.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.6722209719,"dev-research":0.2912594907,"prompt-eng":0.4524393046,"data-quality":0.2979827343,"ml-security":0.0911700201}}
{"text":"Our experiments show that our method even achieve better than advanced monolingual methods.","meta":{"url":"http://arxiv.org/abs/2307.09915v1"},"cats":{"new-dataset":0.031894075,"dev-research":0.325662204,"prompt-eng":0.4366552441,"data-quality":0.2983321006,"ml-security":0.0498185944}}
{"text":"We investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC.","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.0226458125,"dev-research":0.2846766332,"prompt-eng":0.4371622851,"data-quality":0.197111689,"ml-security":0.0947466709}}
{"text":"Our primary objects of interest are ALCreg and ALCvpl, the extensions of with path expressions employing, respectively, regular and visibly-pushdown languages.","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.0826034821,"dev-research":0.3317964966,"prompt-eng":0.4650157124,"data-quality":0.1482957321,"ml-security":0.115834648}}
{"text":"The first one, ALCreg, is a notational variant of the well-known Propositional Dynamic Logic of Fischer and Ladner.","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.0533697783,"dev-research":0.3232860565,"prompt-eng":0.4396492675,"data-quality":0.0916815027,"ml-security":0.0728536787}}
{"text":"The second one, ALCvpl, was introduced and investigated by Loding and Serre in 2007.","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.2362446533,"dev-research":0.3203628981,"prompt-eng":0.3965608874,"data-quality":0.0762140876,"ml-security":0.0824843373}}
{"text":"The logic ALCvpl generalises many known decidable non-regular extensions of ALCreg.   ","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.05687974,"dev-research":0.2655563006,"prompt-eng":0.3790467445,"data-quality":0.1263274118,"ml-security":0.11301322}}
{"text":"We provide a series of undecidability results.","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.0703600926,"dev-research":0.2434769963,"prompt-eng":0.3751750777,"data-quality":0.2412144947,"ml-security":0.2612849664}}
{"text":"First, we show that decidability of the concept satisfiability problem for ALCvpl is lost upon adding the seemingly innocent Self operator.","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.0538240593,"dev-research":0.2675305157,"prompt-eng":0.3938368157,"data-quality":0.1675890494,"ml-security":0.1920380311}}
{"text":"Second, we establish undecidability for the concept satisfiability problem for ALCvpl extended with nominals.","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.0876729005,"dev-research":0.2471689441,"prompt-eng":0.3899407891,"data-quality":0.1995443774,"ml-security":0.1330539993}}
{"text":"Interestingly, our undecidability proof relies only on one single non-regular (visibly-pushdown) language, namely on r#s# := { r^n s^n | n in N } for fixed role names r and s. Finally, in contrast to the classical database setting, we establish undecidability of query entailment for queries involving non-regular atoms from r#s#, already in the case of ALC-TBoxes.","meta":{"url":"http://arxiv.org/abs/2307.09913v1"},"cats":{"new-dataset":0.0397263494,"dev-research":0.2220736376,"prompt-eng":0.3732745778,"data-quality":0.1972725385,"ml-security":0.1735184472}}
{"text":"We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data.","meta":{"url":"http://arxiv.org/abs/2307.09912v1"},"cats":{"new-dataset":0.1127686928,"dev-research":0.1818206252,"prompt-eng":0.3791977406,"data-quality":0.1127742706,"ml-security":0.2056080852}}
{"text":"This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables.","meta":{"url":"http://arxiv.org/abs/2307.09912v1"},"cats":{"new-dataset":0.030027206,"dev-research":0.2276708525,"prompt-eng":0.4693083413,"data-quality":0.0609250086,"ml-security":0.179585024}}
{"text":"The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA).","meta":{"url":"http://arxiv.org/abs/2307.09912v1"},"cats":{"new-dataset":0.0145678084,"dev-research":0.2809597514,"prompt-eng":0.4329305323,"data-quality":0.097848969,"ml-security":0.1029939821}}
{"text":"However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09912v1"},"cats":{"new-dataset":0.0185641679,"dev-research":0.2770010529,"prompt-eng":0.3740114398,"data-quality":0.0867699841,"ml-security":0.1463952453}}
{"text":"Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation.","meta":{"url":"http://arxiv.org/abs/2307.09912v1"},"cats":{"new-dataset":0.0317929929,"dev-research":0.1686247563,"prompt-eng":0.3859617964,"data-quality":0.1878402945,"ml-security":0.138906863}}
{"text":"We apply our method to challenging discrete dynamical systems, discussing improvements over previous methods, as well as to continuous dynamical systems.","meta":{"url":"http://arxiv.org/abs/2307.09912v1"},"cats":{"new-dataset":0.0284335111,"dev-research":0.2350815725,"prompt-eng":0.3846934402,"data-quality":0.0916880454,"ml-security":0.1232868969}}
{"text":"This research investigates the application of Large Language Models (LLMs) to augment conversational agents in process mining, aiming to tackle its inherent complexity and diverse skill requirements.","meta":{"url":"http://arxiv.org/abs/2307.09909v1"},"cats":{"new-dataset":0.0880118496,"dev-research":0.297993855,"prompt-eng":0.4563652297,"data-quality":0.122384764,"ml-security":0.0882029547}}
{"text":"While LLM advancements present novel opportunities for conversational process mining, generating efficient outputs is still a hurdle.","meta":{"url":"http://arxiv.org/abs/2307.09909v1"},"cats":{"new-dataset":0.1053852062,"dev-research":0.3094689342,"prompt-eng":0.4386944607,"data-quality":0.1547245471,"ml-security":0.0735764634}}
{"text":"We propose an innovative approach that amend many issues in existing solutions, informed by prior research on Natural Language Processing (NLP) for conversational agents.","meta":{"url":"http://arxiv.org/abs/2307.09909v1"},"cats":{"new-dataset":0.1244211945,"dev-research":0.344984711,"prompt-eng":0.406466432,"data-quality":0.2771616149,"ml-security":0.0794519322}}
{"text":"Leveraging LLMs, our framework improves both accessibility and agent performance, as demonstrated by experiments on public question and data sets.","meta":{"url":"http://arxiv.org/abs/2307.09909v1"},"cats":{"new-dataset":0.1134418042,"dev-research":0.2323291574,"prompt-eng":0.4787388264,"data-quality":0.1167446115,"ml-security":0.1782320089}}
{"text":"Our research sets the stage for future explorations into LLMs' role in process mining and concludes with propositions for enhancing LLM memory, implementing real-time user testing, and examining diverse data sets.","meta":{"url":"http://arxiv.org/abs/2307.09909v1"},"cats":{"new-dataset":0.1230943202,"dev-research":0.3015980867,"prompt-eng":0.5226213426,"data-quality":0.0967555194,"ml-security":0.1092914883}}
{"text":"Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions using motion information derived from a target-driving video, while maintaining the person's identity in the source image.","meta":{"url":"http://arxiv.org/abs/2307.09906v1"},"cats":{"new-dataset":0.0789597479,"dev-research":0.2855983415,"prompt-eng":0.3836844023,"data-quality":0.0788091196,"ml-security":0.0585176107}}
{"text":"However, dramatic and complex motions in the driving video cause ambiguous generation, because the still source image cannot provide sufficient appearance information for occluded regions or delicate expression variations, which produces severe artifacts and significantly degrades the generation quality.","meta":{"url":"http://arxiv.org/abs/2307.09906v1"},"cats":{"new-dataset":0.0265317402,"dev-research":0.3441298098,"prompt-eng":0.3891145048,"data-quality":0.3076474718,"ml-security":0.0561374895}}
{"text":"To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation conditioned memory compensation network, coined as MCNet, for high-fidelity talking head generation.~Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to compensate warped source facial features for the generation.","meta":{"url":"http://arxiv.org/abs/2307.09906v1"},"cats":{"new-dataset":0.1176080828,"dev-research":0.2564127104,"prompt-eng":0.4165176432,"data-quality":0.1766288916,"ml-security":0.205215054}}
{"text":"Furthermore, we propose an effective query mechanism based on implicit identity representations learned from the discrete keypoints of the source image.","meta":{"url":"http://arxiv.org/abs/2307.09906v1"},"cats":{"new-dataset":0.0590146703,"dev-research":0.2678405828,"prompt-eng":0.4212706258,"data-quality":0.1895123199,"ml-security":0.1350446803}}
{"text":"It can greatly facilitate the retrieval of more correlated information from the memory bank for the compensation.","meta":{"url":"http://arxiv.org/abs/2307.09906v1"},"cats":{"new-dataset":0.0189748878,"dev-research":0.3249626855,"prompt-eng":0.4478657287,"data-quality":0.1397921937,"ml-security":0.1232960829}}
{"text":"Extensive experiments demonstrate that MCNet can learn representative and complementary facial memory, and can clearly outperform previous state-of-the-art talking head generation methods on VoxCeleb1 and CelebV datasets.","meta":{"url":"http://arxiv.org/abs/2307.09906v1"},"cats":{"new-dataset":0.1187937363,"dev-research":0.2920572037,"prompt-eng":0.4027740365,"data-quality":0.1653620537,"ml-security":0.1221739857}}
{"text":"Please check our \\href{https://github.com/harlanhong/ICCV2023-MCNET}{Project}.","meta":{"url":"http://arxiv.org/abs/2307.09906v1"},"cats":{"new-dataset":0.1138765204,"dev-research":0.2911336405,"prompt-eng":0.397439434,"data-quality":0.1954177186,"ml-security":0.070723098}}
{"text":"In recent years, Game AI research has made important breakthroughs using Reinforcement Learning (RL).","meta":{"url":"http://arxiv.org/abs/2307.09905v1"},"cats":{"new-dataset":0.0954264196,"dev-research":0.2627269689,"prompt-eng":0.369258823,"data-quality":0.0677942349,"ml-security":0.1552800853}}
{"text":"Despite this, RL for modern tabletop games has gained little to no attention, even when they offer a range of unique challenges compared to video games.","meta":{"url":"http://arxiv.org/abs/2307.09905v1"},"cats":{"new-dataset":0.075663022,"dev-research":0.3539515805,"prompt-eng":0.371084752,"data-quality":0.1018034445,"ml-security":0.1375099998}}
{"text":"To bridge this gap, we introduce PyTAG, a Python API for interacting with the Tabletop Games framework (TAG).","meta":{"url":"http://arxiv.org/abs/2307.09905v1"},"cats":{"new-dataset":0.3935041331,"dev-research":0.4008532742,"prompt-eng":0.4313287513,"data-quality":0.0781450776,"ml-security":0.0961173699}}
{"text":"TAG contains a growing set of more than 20 modern tabletop games, with a common API for AI agents.","meta":{"url":"http://arxiv.org/abs/2307.09905v1"},"cats":{"new-dataset":0.2716993196,"dev-research":0.3340073918,"prompt-eng":0.406048831,"data-quality":0.0826648525,"ml-security":0.1400955416}}
{"text":"We present techniques for training RL agents in these games and introduce baseline results after training Proximal Policy Optimisation algorithms on a subset of games.","meta":{"url":"http://arxiv.org/abs/2307.09905v1"},"cats":{"new-dataset":0.0767059305,"dev-research":0.195696318,"prompt-eng":0.3678338851,"data-quality":0.0860107477,"ml-security":0.2156206442}}
{"text":"Finally, we discuss the unique challenges complex modern tabletop games provide, now open to RL research through PyTAG.","meta":{"url":"http://arxiv.org/abs/2307.09905v1"},"cats":{"new-dataset":0.3396643463,"dev-research":0.33010824,"prompt-eng":0.3938260331,"data-quality":0.054884766,"ml-security":0.0951187199}}
{"text":"We study the problem nonparametric classification with repeated observations.","meta":{"url":"http://arxiv.org/abs/2307.09896v1"},"cats":{"new-dataset":0.177697598,"dev-research":0.1846070002,"prompt-eng":0.3841327655,"data-quality":0.3084428529,"ml-security":0.1777436655}}
{"text":"Let $\\bX$ be the $d$ dimensional feature vector and let $Y$ denote the label taking values in $\\{1,\\dots ,M\\}$.","meta":{"url":"http://arxiv.org/abs/2307.09896v1"},"cats":{"new-dataset":0.0871084008,"dev-research":0.2462674238,"prompt-eng":0.4832335736,"data-quality":0.3769917805,"ml-security":0.1243103603}}
{"text":"In contrast to usual setup with large sample size $n$ and relatively low dimension $d$, this paper deals with the situation, when instead of observing a single feature vector $\\bX$ we are given $t$ repeated feature vectors $\\bV_1,\\dots ,\\bV_t $.","meta":{"url":"http://arxiv.org/abs/2307.09896v1"},"cats":{"new-dataset":0.0474233549,"dev-research":0.2392433694,"prompt-eng":0.3789265824,"data-quality":0.1491820044,"ml-security":0.1734437928}}
{"text":"Some simple classification rules are presented such that the conditional error probabilities have exponential convergence rate of convergence as $t\\to\\infty$. In the analysis, we investigate particular models like robust detection by nominal densities, prototype classification, linear transformation, linear classification, scaling.","meta":{"url":"http://arxiv.org/abs/2307.09896v1"},"cats":{"new-dataset":0.0331182577,"dev-research":0.2208722556,"prompt-eng":0.4321071749,"data-quality":0.4663193782,"ml-security":0.3430423764}}
{"text":"Existing 2D-to-3D pose lifting networks suffer from poor performance in cross-dataset benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.09893v1"},"cats":{"new-dataset":0.2345371237,"dev-research":0.2322292891,"prompt-eng":0.338046434,"data-quality":0.0772137683,"ml-security":0.1386102074}}
{"text":"Although the use of 2D keypoints joined by \"stick-figure\" limbs has shown promise as an intermediate step, stick-figures do not account for occlusion information that is often inherent in an image.","meta":{"url":"http://arxiv.org/abs/2307.09893v1"},"cats":{"new-dataset":0.0630981793,"dev-research":0.2901442707,"prompt-eng":0.3472485189,"data-quality":0.1029502112,"ml-security":0.0590036524}}
{"text":"In this paper, we propose a novel representation using opaque 3D limbs that preserves occlusion information while implicitly encoding joint locations.","meta":{"url":"http://arxiv.org/abs/2307.09893v1"},"cats":{"new-dataset":0.1047964246,"dev-research":0.2404891926,"prompt-eng":0.3375797744,"data-quality":0.0817499439,"ml-security":0.0656064452}}
{"text":"Crucially, when training on data with accurate three-dimensional keypoints and without part-maps, this representation allows training on abstract synthetic images, with occlusion, from as many synthetic viewpoints as desired.","meta":{"url":"http://arxiv.org/abs/2307.09893v1"},"cats":{"new-dataset":0.1710415764,"dev-research":0.2283676961,"prompt-eng":0.3510195062,"data-quality":0.1231650444,"ml-security":0.1133211227}}
{"text":"The result is a pose defined by limb angles rather than joint positions $\\unicode{x2013}$ because poses are, in the real world, independent of cameras $\\unicode{x2013}$ allowing us to predict poses that are completely independent of camera viewpoint.","meta":{"url":"http://arxiv.org/abs/2307.09893v1"},"cats":{"new-dataset":0.2962998536,"dev-research":0.2938880347,"prompt-eng":0.3549635909,"data-quality":0.1166603028,"ml-security":0.1204908387}}
{"text":"The result provides not only an improvement in same-dataset benchmarks, but a \"quantum leap\" in cross-dataset benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.09893v1"},"cats":{"new-dataset":0.268787253,"dev-research":0.2786690082,"prompt-eng":0.3075039847,"data-quality":0.1471577458,"ml-security":0.0727099764}}
{"text":"We propose 3Deformer, a general-purpose framework for interactive 3D shape editing.","meta":{"url":"http://arxiv.org/abs/2307.09892v1"},"cats":{"new-dataset":0.0618958713,"dev-research":0.3102586522,"prompt-eng":0.4237701765,"data-quality":0.0737287157,"ml-security":0.0375797627}}
{"text":"Given a source 3D mesh with semantic materials, and a user-specified semantic image, 3Deformer can accurately edit the source mesh following the shape guidance of the semantic image, while preserving the source topology as rigid as possible.","meta":{"url":"http://arxiv.org/abs/2307.09892v1"},"cats":{"new-dataset":0.0149424259,"dev-research":0.2868642999,"prompt-eng":0.4162894065,"data-quality":0.1698460449,"ml-security":0.051304415}}
{"text":"Recent studies of 3D shape editing mostly focus on learning neural networks to predict 3D shapes, which requires high-cost 3D training datasets and is limited to handling objects involved in the datasets.","meta":{"url":"http://arxiv.org/abs/2307.09892v1"},"cats":{"new-dataset":0.1030144075,"dev-research":0.2610803956,"prompt-eng":0.3597446813,"data-quality":0.0812337257,"ml-security":0.1569877496}}
{"text":"Unlike these studies, our 3Deformer is a non-training and common framework, which only requires supervision of readily-available semantic images, and is compatible with editing various objects unlimited by datasets.","meta":{"url":"http://arxiv.org/abs/2307.09892v1"},"cats":{"new-dataset":0.1461665434,"dev-research":0.2746373965,"prompt-eng":0.4059000686,"data-quality":0.1887140985,"ml-security":0.068425555}}
{"text":"In 3Deformer, the source mesh is deformed utilizing the differentiable renderer technique, according to the correspondences between semantic images and mesh materials.","meta":{"url":"http://arxiv.org/abs/2307.09892v1"},"cats":{"new-dataset":0.0261914097,"dev-research":0.2851754515,"prompt-eng":0.3816675345,"data-quality":0.1333641119,"ml-security":0.1077947185}}
{"text":"However, guiding complex 3D shapes with a simple 2D image incurs extra challenges, that is, the deform accuracy, surface smoothness, geometric rigidity, and global synchronization of the edited mesh should be guaranteed.","meta":{"url":"http://arxiv.org/abs/2307.09892v1"},"cats":{"new-dataset":0.0232360116,"dev-research":0.2724726765,"prompt-eng":0.3728172323,"data-quality":0.0712455415,"ml-security":0.0706213037}}
{"text":"To address these challenges, we propose a hierarchical optimization architecture to balance the global and local shape features, and propose further various strategies and losses to improve properties of accuracy, smoothness, rigidity, and so on.","meta":{"url":"http://arxiv.org/abs/2307.09892v1"},"cats":{"new-dataset":0.0348169933,"dev-research":0.2394776798,"prompt-eng":0.3686335347,"data-quality":0.0871288386,"ml-security":0.0841641766}}
{"text":"Extensive experiments show that our 3Deformer is able to produce impressive results and reaches the state-of-the-art level.","meta":{"url":"http://arxiv.org/abs/2307.09892v1"},"cats":{"new-dataset":0.0456948604,"dev-research":0.3013888003,"prompt-eng":0.432537031,"data-quality":0.0821311179,"ml-security":0.0452060486}}
{"text":"Item Response Theory (IRT) is a well known method for assessing responses from humans in education and psychology.","meta":{"url":"http://arxiv.org/abs/2307.09891v1"},"cats":{"new-dataset":0.0454072719,"dev-research":0.3100367948,"prompt-eng":0.5117569013,"data-quality":0.1022262945,"ml-security":0.0844931992}}
{"text":"In education, IRT is used to infer student abilities and characteristics of test items from student responses.","meta":{"url":"http://arxiv.org/abs/2307.09891v1"},"cats":{"new-dataset":0.0366584456,"dev-research":0.3459162043,"prompt-eng":0.5409413105,"data-quality":0.1166907617,"ml-security":0.1146565029}}
{"text":"Interactions with students are expensive, calling for methods that efficiently gather information for inferring student abilities.","meta":{"url":"http://arxiv.org/abs/2307.09891v1"},"cats":{"new-dataset":0.0309007654,"dev-research":0.3870819605,"prompt-eng":0.4566284456,"data-quality":0.069012191,"ml-security":0.2127039152}}
{"text":"Methods based on Optimal Experimental Design (OED) are computationally costly, making them inapplicable for interactive applications.","meta":{"url":"http://arxiv.org/abs/2307.09891v1"},"cats":{"new-dataset":0.0342752789,"dev-research":0.2929603597,"prompt-eng":0.4244392581,"data-quality":0.0659145687,"ml-security":0.0988652091}}
{"text":"In response, we propose incorporating amortised experimental design into IRT.","meta":{"url":"http://arxiv.org/abs/2307.09891v1"},"cats":{"new-dataset":0.0328695988,"dev-research":0.2711230291,"prompt-eng":0.4761944207,"data-quality":0.0822204855,"ml-security":0.0780927661}}
{"text":"Here, the computational cost is shifted to a precomputing phase by training a Deep Reinforcement Learning (DRL) agent with synthetic data.","meta":{"url":"http://arxiv.org/abs/2307.09891v1"},"cats":{"new-dataset":0.1182437385,"dev-research":0.2441497738,"prompt-eng":0.398456315,"data-quality":0.0514882862,"ml-security":0.1801768331}}
{"text":"The agent is trained to select optimally informative test items for the distribution of students, and to conduct amortised inference conditioned on the experiment outcomes.","meta":{"url":"http://arxiv.org/abs/2307.09891v1"},"cats":{"new-dataset":0.0242660695,"dev-research":0.2440606658,"prompt-eng":0.5129830656,"data-quality":0.1090695601,"ml-security":0.184458225}}
{"text":"During deployment the agent estimates parameters from data, and suggests the next test item for the student, in close to real-time, by taking into account the history of experiments and outcomes.","meta":{"url":"http://arxiv.org/abs/2307.09891v1"},"cats":{"new-dataset":0.0898622933,"dev-research":0.3084301149,"prompt-eng":0.4897799912,"data-quality":0.0648491993,"ml-security":0.1374994893}}
{"text":"In this paper, we present a novel learning-based shared control framework.","meta":{"url":"http://arxiv.org/abs/2307.09887v1"},"cats":{"new-dataset":0.1322118974,"dev-research":0.2989339582,"prompt-eng":0.4117331189,"data-quality":0.0922763651,"ml-security":0.2131799638}}
{"text":"This framework deploys first-order Dynamical Systems (DS) as motion generators providing the desired reference motion, and a Variable Stiffness Dynamical Systems (VSDS) \\cite{chen2021closed} for haptic guidance.","meta":{"url":"http://arxiv.org/abs/2307.09887v1"},"cats":{"new-dataset":0.1031406106,"dev-research":0.2389564177,"prompt-eng":0.4104436519,"data-quality":0.0502806722,"ml-security":0.0595294928}}
{"text":"We show how to shape several features of our controller in order to achieve authority allocation, local motion refinement, in addition to the inherent ability of the controller to automatically synchronize with the human state during joint task execution.","meta":{"url":"http://arxiv.org/abs/2307.09887v1"},"cats":{"new-dataset":0.0387262108,"dev-research":0.3146330339,"prompt-eng":0.475504053,"data-quality":0.0636941956,"ml-security":0.1032589916}}
{"text":"We validate our approach in a teleoperated task scenario, where we also showcase the ability of our framework to deal with situations that require updating task knowledge due to possible changes in the task scenario, or changes in the environment.","meta":{"url":"http://arxiv.org/abs/2307.09887v1"},"cats":{"new-dataset":0.1493202566,"dev-research":0.3903293457,"prompt-eng":0.4743109333,"data-quality":0.113841352,"ml-security":0.1472672094}}
{"text":"Finally, we conduct a user study to compare the performance of our VSDS controller for guidance generation to two state-of-the-art controllers in a target reaching task.","meta":{"url":"http://arxiv.org/abs/2307.09887v1"},"cats":{"new-dataset":0.0477845917,"dev-research":0.3489627956,"prompt-eng":0.5039210923,"data-quality":0.0522829219,"ml-security":0.0496528432}}
{"text":"The result shows that our VSDS controller has the highest successful rate of task execution among all conditions.","meta":{"url":"http://arxiv.org/abs/2307.09887v1"},"cats":{"new-dataset":0.0652917003,"dev-research":0.3011061053,"prompt-eng":0.440992199,"data-quality":0.0860673273,"ml-security":0.0642450401}}
{"text":"Besides, our VSDS controller helps reduce the execution time and task load significantly, and was selected as the most favorable controller by participants.","meta":{"url":"http://arxiv.org/abs/2307.09887v1"},"cats":{"new-dataset":0.0107971961,"dev-research":0.3932126606,"prompt-eng":0.4448952726,"data-quality":0.0477601692,"ml-security":0.0770634177}}
{"text":"Recent advances in machine learning models have greatly increased the performance of automated methods in medical image analysis.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.0222783412,"dev-research":0.2621681192,"prompt-eng":0.3872035115,"data-quality":0.1402241575,"ml-security":0.1916271146}}
{"text":"However, the internal functioning of such models is largely hidden, which hinders their integration in clinical practice.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.0031241294,"dev-research":0.2834739656,"prompt-eng":0.351340233,"data-quality":0.1147233911,"ml-security":0.2172204295}}
{"text":"Explainability and trust are viewed as important aspects of modern methods, for the latter's widespread use in clinical communities.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.0091322144,"dev-research":0.4675736648,"prompt-eng":0.3758973137,"data-quality":0.1836516713,"ml-security":0.155634747}}
{"text":"As such, validation of machine learning models represents an important aspect and yet, most methods are only validated in a limited way.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.004532043,"dev-research":0.2787563015,"prompt-eng":0.3370060654,"data-quality":0.1687161078,"ml-security":0.3255275483}}
{"text":"In this work, we focus on providing a richer and more appropriate validation approach for highly powerful Visual Question Answering (VQA) algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.0785183422,"dev-research":0.3175314718,"prompt-eng":0.4039916861,"data-quality":0.1437662753,"ml-security":0.1027936356}}
{"text":"To better understand the performance of these methods, which answer arbitrary questions related to images, this work focuses on an automatic visual Turing test (VTT).","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.0995627366,"dev-research":0.2759374745,"prompt-eng":0.482526054,"data-quality":0.1665100365,"ml-security":0.0608414849}}
{"text":"That is, we propose an automatic adaptive questioning method, that aims to expose the reasoning behavior of a VQA algorithm.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.0572574903,"dev-research":0.3521327612,"prompt-eng":0.4771667329,"data-quality":0.1663160898,"ml-security":0.0855741913}}
{"text":"Specifically, we introduce a reinforcement learning (RL) agent that observes the history of previously asked questions, and uses it to select the next question to pose.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.1207380899,"dev-research":0.2461893854,"prompt-eng":0.4811428409,"data-quality":0.0788871559,"ml-security":0.0851614097}}
{"text":"We demonstrate our approach in the context of evaluating algorithms that automatically answer questions related to diabetic macular edema (DME) grading.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.0420101798,"dev-research":0.3969995362,"prompt-eng":0.4718267057,"data-quality":0.2246672841,"ml-security":0.0730852295}}
{"text":"The experiments show that such an agent has similar behavior to a clinician, whereby asking questions that are relevant to key clinical concepts.","meta":{"url":"http://arxiv.org/abs/2307.09886v1"},"cats":{"new-dataset":0.0247626091,"dev-research":0.3117027216,"prompt-eng":0.5000603236,"data-quality":0.09390724,"ml-security":0.1751473329}}
{"text":"Language tests measure a person's ability to use a language in terms of listening, speaking, reading, or writing.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0400427229,"dev-research":0.3300197172,"prompt-eng":0.4580402763,"data-quality":0.187439462,"ml-security":0.0574156633}}
{"text":"Such tests play an integral role in academic, professional, and immigration domains, with entities such as educational institutions, professional accreditation bodies, and governments using them to assess candidate language proficiency.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0343526906,"dev-research":0.3177480206,"prompt-eng":0.4675098791,"data-quality":0.2394496226,"ml-security":0.0792570869}}
{"text":"Recent advances in Artificial Intelligence (AI) and the discipline of Natural Language Processing have prompted language test providers to explore AI's potential applicability within language testing, leading to transformative activity patterns surrounding language instruction and learning.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0561321116,"dev-research":0.3759166513,"prompt-eng":0.468891204,"data-quality":0.2365598045,"ml-security":0.1584033018}}
{"text":"However, with concerns over AI's trustworthiness, it is imperative to understand the implications of integrating AI into language testing.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0165875124,"dev-research":0.4214337598,"prompt-eng":0.4645899733,"data-quality":0.3077322336,"ml-security":0.2444206442}}
{"text":"This knowledge will enable stakeholders to make well-informed decisions, thus safeguarding community well-being and testing integrity.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.1913304115,"dev-research":0.4658923432,"prompt-eng":0.4225682472,"data-quality":0.1498079405,"ml-security":0.1988577107}}
{"text":"To understand the concerns and effects of AI usage in language tests, we conducted interviews and surveys with English test-takers.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0527055492,"dev-research":0.3876957383,"prompt-eng":0.4398537387,"data-quality":0.2372057143,"ml-security":0.1767076916}}
{"text":"To the best of our knowledge, this is the first empirical study aimed at identifying the implications of AI adoption in language tests from a test-taker perspective.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0257399883,"dev-research":0.4085812855,"prompt-eng":0.4608311814,"data-quality":0.2192780337,"ml-security":0.1335586561}}
{"text":"Our study reveals test-taker perceptions and behavioral patterns.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.053856897,"dev-research":0.4112343479,"prompt-eng":0.4951585502,"data-quality":0.1386951759,"ml-security":0.1510049404}}
{"text":"Specifically, we identify that AI integration may enhance perceptions of fairness, consistency, and availability.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0127070259,"dev-research":0.380358975,"prompt-eng":0.3883701436,"data-quality":0.1832842569,"ml-security":0.179566872}}
{"text":"Conversely, it might incite mistrust regarding reliability and interactivity aspects, subsequently influencing the behaviors and well-being of test-takers.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0105409053,"dev-research":0.3869168008,"prompt-eng":0.4588715136,"data-quality":0.1700340545,"ml-security":0.2045601016}}
{"text":"These insights provide a better understanding of potential societal implications and assist stakeholders in making informed decisions concerning AI usage in language testing.","meta":{"url":"http://arxiv.org/abs/2307.09885v1"},"cats":{"new-dataset":0.0441939172,"dev-research":0.4582934429,"prompt-eng":0.4233464334,"data-quality":0.2722007777,"ml-security":0.2066298927}}
{"text":"We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa.","meta":{"url":"http://arxiv.org/abs/2307.09883v1"},"cats":{"new-dataset":0.043333894,"dev-research":0.2502835392,"prompt-eng":0.4388287262,"data-quality":0.1467052211,"ml-security":0.1122378525}}
{"text":"The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect.","meta":{"url":"http://arxiv.org/abs/2307.09883v1"},"cats":{"new-dataset":0.0103582849,"dev-research":0.200398669,"prompt-eng":0.3609021906,"data-quality":0.1783056215,"ml-security":0.2423414381}}
{"text":"Moreover, it requires a closed form a-priori latent distribution.","meta":{"url":"http://arxiv.org/abs/2307.09883v1"},"cats":{"new-dataset":0.0183593226,"dev-research":0.179041262,"prompt-eng":0.4266141314,"data-quality":0.1786189822,"ml-security":0.1384819007}}
{"text":"This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors.","meta":{"url":"http://arxiv.org/abs/2307.09883v1"},"cats":{"new-dataset":0.0231259752,"dev-research":0.1948734877,"prompt-eng":0.4409421383,"data-quality":0.1459774087,"ml-security":0.1614646459}}
{"text":"We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling.","meta":{"url":"http://arxiv.org/abs/2307.09883v1"},"cats":{"new-dataset":0.0445789072,"dev-research":0.155684621,"prompt-eng":0.3848884503,"data-quality":0.1405730508,"ml-security":0.2455951931}}
{"text":"The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.09883v1"},"cats":{"new-dataset":0.03942198,"dev-research":0.3511334053,"prompt-eng":0.4347935451,"data-quality":0.0586027294,"ml-security":0.1357556473}}
{"text":"We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability for tasks that are not accessible by standard VAE learning.","meta":{"url":"http://arxiv.org/abs/2307.09883v1"},"cats":{"new-dataset":0.0117087405,"dev-research":0.1742988413,"prompt-eng":0.4163380316,"data-quality":0.1349021887,"ml-security":0.2661203966}}
{"text":"Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples.","meta":{"url":"http://arxiv.org/abs/2307.09882v1"},"cats":{"new-dataset":0.0969969333,"dev-research":0.2435041349,"prompt-eng":0.4081914711,"data-quality":0.2550200666,"ml-security":0.2489264384}}
{"text":"However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy).","meta":{"url":"http://arxiv.org/abs/2307.09882v1"},"cats":{"new-dataset":0.029404282,"dev-research":0.2250132953,"prompt-eng":0.4124315567,"data-quality":0.1734804737,"ml-security":0.4800152539}}
{"text":"We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy.","meta":{"url":"http://arxiv.org/abs/2307.09882v1"},"cats":{"new-dataset":0.0547591713,"dev-research":0.2035786897,"prompt-eng":0.4096098992,"data-quality":0.1550922179,"ml-security":0.1364696214}}
{"text":"This is hypothesized to provide a better mode coverage.","meta":{"url":"http://arxiv.org/abs/2307.09882v1"},"cats":{"new-dataset":0.0119232224,"dev-research":0.2997777578,"prompt-eng":0.4175472213,"data-quality":0.1027070807,"ml-security":0.1628390972}}
{"text":"Different from previous works, we explicitly compute the density of the generated samples.","meta":{"url":"http://arxiv.org/abs/2307.09882v1"},"cats":{"new-dataset":0.2703791339,"dev-research":0.1780890723,"prompt-eng":0.3805008173,"data-quality":0.2321284415,"ml-security":0.0622060962}}
{"text":"This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term.","meta":{"url":"http://arxiv.org/abs/2307.09882v1"},"cats":{"new-dataset":0.0714679115,"dev-research":0.1751276979,"prompt-eng":0.4229478179,"data-quality":0.1297253764,"ml-security":0.0633495196}}
{"text":"The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require to have a tractable inverse function.","meta":{"url":"http://arxiv.org/abs/2307.09882v1"},"cats":{"new-dataset":0.0303089565,"dev-research":0.2070174496,"prompt-eng":0.3614563852,"data-quality":0.0758968521,"ml-security":0.0774507144}}
{"text":"Our experimental results show that we converge faster, produce comparable sample quality to GANs with similar architecture, successfully avoid over-fitting to commonly used datasets and produce smooth low-dimensional latent representations of the training data.","meta":{"url":"http://arxiv.org/abs/2307.09882v1"},"cats":{"new-dataset":0.1144610996,"dev-research":0.2577228838,"prompt-eng":0.3785048755,"data-quality":0.2037415287,"ml-security":0.1364384644}}
{"text":"Accurate navigation is of paramount importance to ensure flight safety and efficiency for autonomous drones.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.036942239,"dev-research":0.2955107679,"prompt-eng":0.3895969487,"data-quality":0.1013831708,"ml-security":0.1077224}}
{"text":"Recent research starts to use Deep Neural Networks to enhance drone navigation given their remarkable predictive capability for visual perception.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.099258364,"dev-research":0.2638999521,"prompt-eng":0.4100583524,"data-quality":0.0898407846,"ml-security":0.128097904}}
{"text":"However, existing solutions either run DNN inference tasks on drones in situ, impeded by the limited onboard resource, or offload the computation to external servers which may incur large network latency.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.0855346221,"dev-research":0.2888276343,"prompt-eng":0.3554753054,"data-quality":0.0890607749,"ml-security":0.1660017131}}
{"text":"Few works consider jointly optimizing the offloading decisions along with image transmission configurations and adapting them on the fly.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.022055917,"dev-research":0.2257705155,"prompt-eng":0.4142715803,"data-quality":0.0685394323,"ml-security":0.071717104}}
{"text":"In this paper, we propose A3D, an edge server assisted drone navigation framework that can dynamically adjust task execution location, input resolution, and image compression ratio in order to achieve low inference latency, high prediction accuracy, and long flight distances.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.0867198634,"dev-research":0.3044431963,"prompt-eng":0.3814228042,"data-quality":0.0570176781,"ml-security":0.0846437208}}
{"text":"Specifically, we first augment state-of-the-art convolutional neural networks for drone navigation and define a novel metric called Quality of Navigation as our optimization objective which can effectively capture the above goals.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.1200068585,"dev-research":0.2813202322,"prompt-eng":0.3917336557,"data-quality":0.1201997428,"ml-security":0.0922575232}}
{"text":"We then design a deep reinforcement learning based neural scheduler at the drone side for which an information encoder is devised to reshape the state features and thus improve its learning ability.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.1247642962,"dev-research":0.2500666929,"prompt-eng":0.4071352252,"data-quality":0.0651005389,"ml-security":0.1503909552}}
{"text":"To further support simultaneous multi-drone serving, we extend the edge server design by developing a network-aware resource allocation algorithm, which allows provisioning containerized resources aligned with drones' demand.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.0499680425,"dev-research":0.2502345484,"prompt-eng":0.3451181006,"data-quality":0.065668362,"ml-security":0.091185324}}
{"text":"We finally implement a proof-of-concept prototype with realistic devices and validate its performance in a real-world campus scene, as well as a simulation environment for thorough evaluation upon AirSim.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.1284176455,"dev-research":0.3060479583,"prompt-eng":0.4346560259,"data-quality":0.0913749483,"ml-security":0.1581031293}}
{"text":"Extensive experimental results show that A3D can reduce end-to-end latency by 28.06% and extend the flight distance by up to 27.28% compared with non-adaptive solutions.","meta":{"url":"http://arxiv.org/abs/2307.09880v1"},"cats":{"new-dataset":0.041475702,"dev-research":0.2678611321,"prompt-eng":0.334480347,"data-quality":0.0549993887,"ml-security":0.0510112997}}
{"text":"User models play an important role in interaction design, supporting automation of interaction design choices.","meta":{"url":"http://arxiv.org/abs/2307.09878v1"},"cats":{"new-dataset":0.0230417916,"dev-research":0.381453868,"prompt-eng":0.5120874348,"data-quality":0.0470201414,"ml-security":0.0827410256}}
{"text":"In order to do so, model parameters must be estimated from user data.","meta":{"url":"http://arxiv.org/abs/2307.09878v1"},"cats":{"new-dataset":0.0419784015,"dev-research":0.2489773454,"prompt-eng":0.4581036641,"data-quality":0.1027274876,"ml-security":0.1313987617}}
{"text":"While very large amounts of user data are sometimes required, recent research has shown how experiments can be designed so as to gather data and infer parameters as efficiently as possible, thereby minimising the data requirement.","meta":{"url":"http://arxiv.org/abs/2307.09878v1"},"cats":{"new-dataset":0.1172740583,"dev-research":0.3163618236,"prompt-eng":0.4463542241,"data-quality":0.0730896584,"ml-security":0.1989819742}}
{"text":"In the current article, we investigate a variant of these methods that amortises the computational cost of designing experiments by training a policy for choosing experimental designs with simulated participants.","meta":{"url":"http://arxiv.org/abs/2307.09878v1"},"cats":{"new-dataset":0.0418980379,"dev-research":0.2883058844,"prompt-eng":0.4236644186,"data-quality":0.078540498,"ml-security":0.2105942607}}
{"text":"Our solution learns which experiments provide the most useful data for parameter estimation by interacting with in-silico agents sampled from the model space thereby using synthetic data rather than vast amounts of human data.","meta":{"url":"http://arxiv.org/abs/2307.09878v1"},"cats":{"new-dataset":0.1407176214,"dev-research":0.2329117008,"prompt-eng":0.4514189957,"data-quality":0.1165986253,"ml-security":0.2116239678}}
{"text":"The approach is demonstrated for three progressively complex models of pointing.","meta":{"url":"http://arxiv.org/abs/2307.09878v1"},"cats":{"new-dataset":0.021376216,"dev-research":0.2559898752,"prompt-eng":0.4242997614,"data-quality":0.0749315045,"ml-security":0.047751294}}
{"text":"Agriculture industries often face challenges in manual tasks such as planting, harvesting, fertilizing, and detection, which can be time consuming and prone to errors.","meta":{"url":"http://arxiv.org/abs/2307.09874v1"},"cats":{"new-dataset":0.0425329387,"dev-research":0.3519508837,"prompt-eng":0.4289354365,"data-quality":0.2011526753,"ml-security":0.1029708245}}
{"text":"The \"Agricultural Robotic System\" project addresses these issues through a modular design that integrates advanced visual, speech recognition, and robotic technologies.","meta":{"url":"http://arxiv.org/abs/2307.09874v1"},"cats":{"new-dataset":0.1283939388,"dev-research":0.2395675514,"prompt-eng":0.4261058955,"data-quality":0.1034195148,"ml-security":0.0633308454}}
{"text":"This system is comprised of separate but interconnected modules for vision detection and speech recognition, creating a flexible and adaptable solution.","meta":{"url":"http://arxiv.org/abs/2307.09874v1"},"cats":{"new-dataset":0.1880546284,"dev-research":0.2481441976,"prompt-eng":0.371815281,"data-quality":0.141614422,"ml-security":0.0514652397}}
{"text":"The vision detection module uses computer vision techniques, trained on YOLOv5 and deployed on the Jetson Nano in TensorRT format, to accurately detect and identify different items.","meta":{"url":"http://arxiv.org/abs/2307.09874v1"},"cats":{"new-dataset":0.1060174855,"dev-research":0.2754458035,"prompt-eng":0.4126116818,"data-quality":0.1514942575,"ml-security":0.0892975261}}
{"text":"A robotic arm module then precisely controls the picking up of seedlings or seeds, and arranges them in specific locations.","meta":{"url":"http://arxiv.org/abs/2307.09874v1"},"cats":{"new-dataset":0.0706064198,"dev-research":0.2331848329,"prompt-eng":0.4457990211,"data-quality":0.0820961055,"ml-security":0.043596294}}
{"text":"The speech recognition module enhances intelligent human robot interaction, allowing for efficient and intuitive control of the system.","meta":{"url":"http://arxiv.org/abs/2307.09874v1"},"cats":{"new-dataset":0.071176985,"dev-research":0.2844434438,"prompt-eng":0.411177723,"data-quality":0.124989013,"ml-security":0.0926825174}}
{"text":"This modular approach improves the efficiency and accuracy of agricultural tasks, demonstrating the potential of robotics in the agricultural industry.","meta":{"url":"http://arxiv.org/abs/2307.09874v1"},"cats":{"new-dataset":0.0234648542,"dev-research":0.2982360076,"prompt-eng":0.4260762223,"data-quality":0.0742370205,"ml-security":0.0345349986}}
{"text":"Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us.","meta":{"url":"http://arxiv.org/abs/2307.09866v1"},"cats":{"new-dataset":0.1836115296,"dev-research":0.4003004492,"prompt-eng":0.4119536317,"data-quality":0.1488859331,"ml-security":0.4090634548}}
{"text":"Potential applications include protecting fragile facilities and designing robust topologies, etc.","meta":{"url":"http://arxiv.org/abs/2307.09866v1"},"cats":{"new-dataset":0.0571736872,"dev-research":0.3371724083,"prompt-eng":0.4015887073,"data-quality":0.0883356945,"ml-security":0.2926363086}}
{"text":"Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario.","meta":{"url":"http://arxiv.org/abs/2307.09866v1"},"cats":{"new-dataset":0.0975028894,"dev-research":0.3884437543,"prompt-eng":0.4037061946,"data-quality":0.0908181566,"ml-security":0.2603752507}}
{"text":"In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately.","meta":{"url":"http://arxiv.org/abs/2307.09866v1"},"cats":{"new-dataset":0.1751420265,"dev-research":0.262415893,"prompt-eng":0.3338585516,"data-quality":0.1690679035,"ml-security":0.2870389739}}
{"text":"The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failure and discover vulnerable infrastructures of cities.","meta":{"url":"http://arxiv.org/abs/2307.09866v1"},"cats":{"new-dataset":0.2593867221,"dev-research":0.3799943959,"prompt-eng":0.343415279,"data-quality":0.2131276415,"ml-security":0.4447433883}}
{"text":"Extensive experiments with various requests demonstrate not only the expressive power of our system but also transferring ability and necessity of the specific components.","meta":{"url":"http://arxiv.org/abs/2307.09866v1"},"cats":{"new-dataset":0.0067103082,"dev-research":0.3066825453,"prompt-eng":0.4621082378,"data-quality":0.0574855747,"ml-security":0.165411123}}
{"text":"Machine learning has affected the way in which many phenomena for various domains are modelled, one of these domains being that of structural dynamics.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0179974133,"dev-research":0.2729048659,"prompt-eng":0.4114977839,"data-quality":0.1171588997,"ml-security":0.3717790807}}
{"text":"However, because machine-learning algorithms are problem-specific, they often fail to perform efficiently in cases of data scarcity.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.023059884,"dev-research":0.3193024876,"prompt-eng":0.3207529318,"data-quality":0.2655973309,"ml-security":0.5099490446}}
{"text":"To deal with such issues, combination of physics-based approaches and machine learning algorithms have been developed.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0461747432,"dev-research":0.2328593663,"prompt-eng":0.3615923668,"data-quality":0.0944996478,"ml-security":0.1453945347}}
{"text":"Although such methods are effective, they also require the analyser's understanding of the underlying physics of the problem.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0132154226,"dev-research":0.3411421154,"prompt-eng":0.3887206813,"data-quality":0.1142744749,"ml-security":0.0808254496}}
{"text":"The current work is aimed at motivating the use of models which learn such relationships from a population of phenomena, whose underlying physics are similar.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0296367934,"dev-research":0.3031322728,"prompt-eng":0.4586783781,"data-quality":0.0810945862,"ml-security":0.1334211837}}
{"text":"The development of such models is motivated by the way that physics-based models, and more specifically finite element models, work.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.01044549,"dev-research":0.2109376299,"prompt-eng":0.4170562656,"data-quality":0.0363014338,"ml-security":0.0729230869}}
{"text":"Such models are considered transferrable, explainable and trustworthy, attributes which are not trivially imposed or achieved for machine-learning models.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0224949808,"dev-research":0.2367673096,"prompt-eng":0.4084080405,"data-quality":0.1758702805,"ml-security":0.4373558733}}
{"text":"For this reason, machine-learning approaches are less trusted by industry and often considered more difficult to form validated models.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0069007273,"dev-research":0.3430886158,"prompt-eng":0.3538437099,"data-quality":0.1991469256,"ml-security":0.4182721288}}
{"text":"To achieve such data-driven models, a population-based scheme is followed here and two different machine-learning algorithms from the meta-learning domain are used.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.233560196,"dev-research":0.2235185132,"prompt-eng":0.44246234,"data-quality":0.110478807,"ml-security":0.1746617638}}
{"text":"The two algorithms are the model-agnostic meta-learning (MAML) algorithm and the conditional neural processes (CNP) model.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0124004918,"dev-research":0.1847070051,"prompt-eng":0.4053658188,"data-quality":0.0968399893,"ml-security":0.0791053792}}
{"text":"The algorithms seem to perform as intended and outperform a traditional machine-learning algorithm at approximating the quantities of interest.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0186560034,"dev-research":0.2701502113,"prompt-eng":0.3393226301,"data-quality":0.1653841863,"ml-security":0.143752067}}
{"text":"Moreover, they exhibit behaviour similar to traditional machine learning algorithms (e.g. neural networks or Gaussian processes), concerning their performance as a function of the available structures in the training population.","meta":{"url":"http://arxiv.org/abs/2307.09862v1"},"cats":{"new-dataset":0.0174780417,"dev-research":0.2406148005,"prompt-eng":0.393130812,"data-quality":0.0939842695,"ml-security":0.3098981108}}
{"text":"Hyperspectral anomaly detection (HAD) is widely used in Earth observation and deep space exploration.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.1033919094,"dev-research":0.2707975711,"prompt-eng":0.4133755894,"data-quality":0.2034825696,"ml-security":0.1440854373}}
{"text":"A major challenge for HAD is the complex background of the input hyperspectral images (HSIs), resulting in anomalies confused in the background.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.1164873847,"dev-research":0.2551297234,"prompt-eng":0.4314271953,"data-quality":0.1590180078,"ml-security":0.1101721513}}
{"text":"On the other hand, the lack of labeled samples for HSIs leads to poor generalization of existing HAD methods.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.0163292996,"dev-research":0.3043667822,"prompt-eng":0.4039210212,"data-quality":0.2578031263,"ml-security":0.1356858367}}
{"text":"This paper starts the first attempt to study a new and generalizable background learning problem without labeled samples.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.1704309373,"dev-research":0.2137074409,"prompt-eng":0.3973250778,"data-quality":0.3117111925,"ml-security":0.3089239814}}
{"text":"We present a novel solution BSDM (background suppression diffusion model) for HAD, which can simultaneously learn latent background distributions and generalize to different datasets for suppressing complex background.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.1178439129,"dev-research":0.2289038723,"prompt-eng":0.407455287,"data-quality":0.1375940698,"ml-security":0.2549329535}}
{"text":"It is featured in three aspects: (1) For the complex background of HSIs, we design pseudo background noise and learn the potential background distribution in it with a diffusion model (DM).","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.2403996488,"dev-research":0.1996378855,"prompt-eng":0.4435396064,"data-quality":0.1214018266,"ml-security":0.1726180225}}
{"text":"(2) For the generalizability problem, we apply a statistical offset module so that the BSDM adapts to datasets of different domains without labeling samples.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.2043511262,"dev-research":0.207349377,"prompt-eng":0.408409251,"data-quality":0.323645163,"ml-security":0.1493784282}}
{"text":"(3) For achieving background suppression, we innovatively improve the inference process of DM by feeding the original HSIs into the denoising network, which removes the background as noise.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.0501536735,"dev-research":0.2603641299,"prompt-eng":0.4472519919,"data-quality":0.1768990913,"ml-security":0.1653690808}}
{"text":"Our work paves a new background suppression way for HAD that can improve HAD performance without the prerequisite of manually labeled data.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.0669183175,"dev-research":0.3349983973,"prompt-eng":0.4329905854,"data-quality":0.3005145585,"ml-security":0.1876556572}}
{"text":"Assessments and generalization experiments of four HAD methods on several real HSI datasets demonstrate the above three unique properties of the proposed method.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.115906605,"dev-research":0.2388160156,"prompt-eng":0.3975888496,"data-quality":0.1438777808,"ml-security":0.074085331}}
{"text":"The code is available at https://github.com/majitao-xd/BSDM-HAD.","meta":{"url":"http://arxiv.org/abs/2307.09861v1"},"cats":{"new-dataset":0.2463678029,"dev-research":0.2493456639,"prompt-eng":0.4240491061,"data-quality":0.0871627257,"ml-security":0.0654657307}}
{"text":"Large industrial facilities such as particle accelerators and nuclear power plants are critical infrastructures for scientific research and industrial processes.","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.1387052,"dev-research":0.288823998,"prompt-eng":0.3932967077,"data-quality":0.1072586091,"ml-security":0.0895109718}}
{"text":"These facilities are complex systems that not only require regular maintenance and upgrades but are often inaccessible to humans due to various safety hazards.","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.1778393146,"dev-research":0.3139116819,"prompt-eng":0.373366009,"data-quality":0.0819108295,"ml-security":0.1928130952}}
{"text":"Therefore, a virtual reality (VR) system that can quickly replicate real-world remote environments to provide users with a high level of spatial and situational awareness is crucial for facility maintenance planning.","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.1874903547,"dev-research":0.3324412264,"prompt-eng":0.4049945763,"data-quality":0.0838981811,"ml-security":0.0707422006}}
{"text":"However, the exact 3D shapes of these facilities are often too complex to be accurately modeled with geometric primitives through the traditional rasterization pipeline.   ","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.0279460108,"dev-research":0.282216702,"prompt-eng":0.3610712854,"data-quality":0.0775397356,"ml-security":0.064415623}}
{"text":"In this work, we develop Magic NeRF Lens, an interactive framework to support facility inspection in immersive VR using neural radiance fields (NeRF) and volumetric rendering.","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.1569223267,"dev-research":0.2595105473,"prompt-eng":0.3931013704,"data-quality":0.1065461342,"ml-security":0.1058976558}}
{"text":"We introduce a novel data fusion approach that combines the complementary strengths of volumetric rendering and geometric rasterization, allowing a NeRF model to be merged with other conventional 3D data, such as a computer-aided design model.","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.1161767363,"dev-research":0.2613132287,"prompt-eng":0.3367773027,"data-quality":0.104869031,"ml-security":0.1041157768}}
{"text":"We develop two novel 3D magic lens effects to optimize NeRF rendering by exploiting the properties of human vision and context-aware visualization.","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.0482016682,"dev-research":0.306742511,"prompt-eng":0.3788861823,"data-quality":0.1123722505,"ml-security":0.1120226961}}
{"text":"We demonstrate the high usability of our framework and methods through a technical benchmark, a visual search user study, and expert reviews.","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.1235244969,"dev-research":0.4213339955,"prompt-eng":0.4206989206,"data-quality":0.1062615611,"ml-security":0.0663088126}}
{"text":"In addition, the source code of our VR NeRF framework is made publicly available for future research and development.","meta":{"url":"http://arxiv.org/abs/2307.09860v1"},"cats":{"new-dataset":0.3540723517,"dev-research":0.37236295,"prompt-eng":0.33316264,"data-quality":0.1300330436,"ml-security":0.2494774388}}
{"text":"Rare categories abound in a number of real-world networks and play a pivotal role in a variety of high-stakes applications, including financial fraud detection, network intrusion detection, and rare disease diagnosis.","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.0502687071,"dev-research":0.2713496144,"prompt-eng":0.3724080149,"data-quality":0.2326861035,"ml-security":0.4655696676}}
{"text":"Rare category analysis (RCA) refers to the task of detecting, characterizing, and comprehending the behaviors of minority classes in a highly-imbalanced data distribution.","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.0782855257,"dev-research":0.2884347494,"prompt-eng":0.4012338069,"data-quality":0.2653159733,"ml-security":0.2298811994}}
{"text":"While the vast majority of existing work on RCA has focused on improving the prediction performance, a few fundamental research questions heretofore have received little attention and are less explored: How confident or uncertain is a prediction model in rare category analysis?","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.0377864166,"dev-research":0.2760982792,"prompt-eng":0.4171598095,"data-quality":0.2201805472,"ml-security":0.1602205121}}
{"text":"How can we quantify the uncertainty in the learning process and enable reliable rare category analysis?   ","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.0436005115,"dev-research":0.260951123,"prompt-eng":0.3976363206,"data-quality":0.3276980599,"ml-security":0.2278303279}}
{"text":"To answer these questions, we start by investigating miscalibration in existing RCA methods.","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.0425576016,"dev-research":0.2807577342,"prompt-eng":0.4241334499,"data-quality":0.3466012239,"ml-security":0.0562742591}}
{"text":"Empirical results reveal that state-of-the-art RCA methods are mainly over-confident in predicting minority classes and under-confident in predicting majority classes.","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.0404358893,"dev-research":0.3259856788,"prompt-eng":0.4291135976,"data-quality":0.2914668113,"ml-security":0.314279617}}
{"text":"Motivated by the observation, we propose a novel individual calibration framework, named CALIRARE, for alleviating the unique challenges of RCA, thus enabling reliable rare category analysis.","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.2688085278,"dev-research":0.271132688,"prompt-eng":0.4216024952,"data-quality":0.3154576762,"ml-security":0.1258030699}}
{"text":"In particular, to quantify the uncertainties in RCA, we develop a node-level uncertainty quantification algorithm to model the overlapping support regions with high uncertainty; to handle the rarity of minority classes in miscalibration calculation, we generalize the distribution-based calibration metric to the instance level and propose the first individual calibration measurement on graphs named Expected Individual Calibration Error (EICE).","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.1218598372,"dev-research":0.3003714098,"prompt-eng":0.4353528659,"data-quality":0.483328247,"ml-security":0.1320232627}}
{"text":"We perform extensive experimental evaluations on real-world datasets, including rare category characterization and model calibration tasks, which demonstrate the significance of our proposed framework.","meta":{"url":"http://arxiv.org/abs/2307.09858v1"},"cats":{"new-dataset":0.3156730429,"dev-research":0.2517022461,"prompt-eng":0.4154369896,"data-quality":0.3258366787,"ml-security":0.2039711727}}
{"text":"BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically.","meta":{"url":"http://arxiv.org/abs/2307.09857v1"},"cats":{"new-dataset":0.1201424655,"dev-research":0.2861512929,"prompt-eng":0.3505812462,"data-quality":0.1793735996,"ml-security":0.0446150153}}
{"text":"Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions.","meta":{"url":"http://arxiv.org/abs/2307.09857v1"},"cats":{"new-dataset":0.0382464459,"dev-research":0.2762710516,"prompt-eng":0.3816059264,"data-quality":0.2962127313,"ml-security":0.053850083}}
{"text":"Most algorithms generate quality without emphasizing the important region of interest.","meta":{"url":"http://arxiv.org/abs/2307.09857v1"},"cats":{"new-dataset":0.0176671228,"dev-research":0.3347351659,"prompt-eng":0.3534009831,"data-quality":0.2508836756,"ml-security":0.095891503}}
{"text":"In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed.","meta":{"url":"http://arxiv.org/abs/2307.09857v1"},"cats":{"new-dataset":0.0723756769,"dev-research":0.1930046674,"prompt-eng":0.4173807049,"data-quality":0.1060205892,"ml-security":0.0403220256}}
{"text":"This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest.","meta":{"url":"http://arxiv.org/abs/2307.09857v1"},"cats":{"new-dataset":0.0389439151,"dev-research":0.2731563854,"prompt-eng":0.4737173265,"data-quality":0.1061357066,"ml-security":0.0549970076}}
{"text":"Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach.","meta":{"url":"http://arxiv.org/abs/2307.09857v1"},"cats":{"new-dataset":0.2978667627,"dev-research":0.3260076874,"prompt-eng":0.3676243432,"data-quality":0.2820714023,"ml-security":0.0767756957}}
{"text":"Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.","meta":{"url":"http://arxiv.org/abs/2307.09857v1"},"cats":{"new-dataset":0.100262309,"dev-research":0.2259169943,"prompt-eng":0.3850673497,"data-quality":0.2724203092,"ml-security":0.1589895818}}
{"text":"Gait recognition is a biometric technique that identifies individuals by their unique walking styles, which is suitable for unconstrained environments and has a wide range of applications.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.0827627451,"dev-research":0.2671322704,"prompt-eng":0.3832405669,"data-quality":0.1188351287,"ml-security":0.0859756228}}
{"text":"While current methods focus on exploiting body part-based representations, they often neglect the hierarchical dependencies between local motion patterns.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.0400330394,"dev-research":0.2307227086,"prompt-eng":0.3903151803,"data-quality":0.0778759666,"ml-security":0.082074909}}
{"text":"In this paper, we propose a hierarchical spatio-temporal representation learning (HSTL) framework for extracting gait features from coarse to fine.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.2198637714,"dev-research":0.2459770377,"prompt-eng":0.3705966338,"data-quality":0.0914023704,"ml-security":0.0988020068}}
{"text":"Our framework starts with a hierarchical clustering analysis to recover multi-level body structures from the whole body to local details.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.1277975282,"dev-research":0.2095619511,"prompt-eng":0.3867182177,"data-quality":0.1212573081,"ml-security":0.0801019831}}
{"text":"Next, an adaptive region-based motion extractor (ARME) is designed to learn region-independent motion features.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.1049790415,"dev-research":0.2520599444,"prompt-eng":0.3881043296,"data-quality":0.0946562265,"ml-security":0.0560835122}}
{"text":"The proposed HSTL then stacks multiple ARMEs in a top-down manner, with each ARME corresponding to a specific partition level of the hierarchy.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.0589485808,"dev-research":0.2316812473,"prompt-eng":0.4254915536,"data-quality":0.0525568727,"ml-security":0.0491568143}}
{"text":"An adaptive spatio-temporal pooling (ASTP) module is used to capture gait features at different levels of detail to perform hierarchical feature mapping.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.116774257,"dev-research":0.2671219018,"prompt-eng":0.3887202,"data-quality":0.0678217812,"ml-security":0.0494330697}}
{"text":"Finally, a frame-level temporal aggregation (FTA) module is employed to reduce redundant information in gait sequences through multi-scale temporal downsampling.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.1599972226,"dev-research":0.2347627626,"prompt-eng":0.3598123105,"data-quality":0.0872898719,"ml-security":0.0482995465}}
{"text":"Extensive experiments on CASIA-B, OUMVLP, GREW, and Gait3D datasets demonstrate that our method outperforms the state-of-the-art while maintaining a reasonable balance between model accuracy and complexity.","meta":{"url":"http://arxiv.org/abs/2307.09856v1"},"cats":{"new-dataset":0.232540924,"dev-research":0.2743140282,"prompt-eng":0.3495709865,"data-quality":0.0887662521,"ml-security":0.080132424}}
{"text":"The lock set method and the partial order method are two main approaches to guarantee that dynamic data race prediction remains efficient.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0983302185,"dev-research":0.2887843487,"prompt-eng":0.3561545763,"data-quality":0.0996416562,"ml-security":0.3094824736}}
{"text":"There are many variations of these ideas.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0325901658,"dev-research":0.2929753735,"prompt-eng":0.4102522963,"data-quality":0.1124032778,"ml-security":0.1404811626}}
{"text":"Common to all of them is the assumption that the events in a critical section belong to the same thread.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0691125341,"dev-research":0.2967370764,"prompt-eng":0.4185814751,"data-quality":0.193186722,"ml-security":0.1104098228}}
{"text":"We have evidence that critical sections in the wild do extend across thread boundaries even if the surrounding acquire and release events occur in the same thread.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0501669202,"dev-research":0.2604999374,"prompt-eng":0.4226259318,"data-quality":0.1566836431,"ml-security":0.1678973878}}
{"text":"We introduce the novel concept of a cross-thread critical section to capture such situations, offer a theoretical comprehensive framework, and study their impact on state-of-the-art data race analyses.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.2014488452,"dev-research":0.3219966955,"prompt-eng":0.3611856216,"data-quality":0.1267543708,"ml-security":0.2023294385}}
{"text":"For sound partial order relations such as WCP, SDP, and DCtp, the occurrence of cross-thread critical sections negatively impacts their precision.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0290852991,"dev-research":0.2652468208,"prompt-eng":0.413340203,"data-quality":0.2090449891,"ml-security":0.0779860892}}
{"text":"For complete partial order relations such as WDP and PWR, cross-thread critical sections help to eliminate more false positives.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0422110426,"dev-research":0.2299803706,"prompt-eng":0.4574761488,"data-quality":0.1563240839,"ml-security":0.0606680386}}
{"text":"The same (positive) impact applies to the lock set construction.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0298584678,"dev-research":0.353175743,"prompt-eng":0.3680971051,"data-quality":0.1523764063,"ml-security":0.2110104921}}
{"text":"Our experimental evaluation confirms that cross-thread critical sections arise in practice.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0230042471,"dev-research":0.2808765427,"prompt-eng":0.4305438227,"data-quality":0.1264862541,"ml-security":0.1149778602}}
{"text":"For the complete relation PWR, we are able to reduce the number of false positives.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0347175521,"dev-research":0.2233178217,"prompt-eng":0.4703319844,"data-quality":0.1873379425,"ml-security":0.1080386344}}
{"text":"The performance overhead incurred by tracking cross-thread critical sections slows down the analysis by 10\\%-20\\%, on average.","meta":{"url":"http://arxiv.org/abs/2307.09855v1"},"cats":{"new-dataset":0.0259552379,"dev-research":0.2906329067,"prompt-eng":0.3941929381,"data-quality":0.096679574,"ml-security":0.0994723439}}
{"text":"This letter focuses on a transmitter or base station (BS) side beyond-diagonal reflecting intelligent surface (BD-RIS) deployment strategy to enhance the spectral efficiency (SE) of a time-division-duplex massive multiple-input multiple-output (MaMIMO) network.","meta":{"url":"http://arxiv.org/abs/2307.09848v1"},"cats":{"new-dataset":0.0357652364,"dev-research":0.2351332803,"prompt-eng":0.4235596398,"data-quality":0.0725387506,"ml-security":0.0582968888}}
{"text":"In this strategy, the active antenna array utilizes a BD-RIS at the BS to serve multiple users in the downlink.","meta":{"url":"http://arxiv.org/abs/2307.09848v1"},"cats":{"new-dataset":0.0247833654,"dev-research":0.2109683259,"prompt-eng":0.3978596172,"data-quality":0.0782399027,"ml-security":0.0739547074}}
{"text":"Based on the knowledge of statistical channel state information (CSI), the BD-RIS coefficients matrix is optimized by employing a novel manifold algorithm, and the power control coefficients are then optimized with the objective of maximizing the minimum SE.","meta":{"url":"http://arxiv.org/abs/2307.09848v1"},"cats":{"new-dataset":0.0337625841,"dev-research":0.2074468331,"prompt-eng":0.4180520973,"data-quality":0.079874259,"ml-security":0.1041705339}}
{"text":"Through numerical results we illustrate the SE performance of the proposed transmission framework and compare it with that of a conventional MaMIMO transmission for different network settings.","meta":{"url":"http://arxiv.org/abs/2307.09848v1"},"cats":{"new-dataset":0.0312709059,"dev-research":0.1687363037,"prompt-eng":0.3760891372,"data-quality":0.0746428089,"ml-security":0.0605576768}}
{"text":"In this paper, we describe the development process of autonomous navigation capabilities of a small cruise boat operating in a canal environment and present the results of a field experiment conducted in the Pohang Canal, South Korea.","meta":{"url":"http://arxiv.org/abs/2307.09845v1"},"cats":{"new-dataset":0.0783988863,"dev-research":0.2508217179,"prompt-eng":0.4163968289,"data-quality":0.0798783154,"ml-security":0.0521809666}}
{"text":"Nonlinear model predictive control (NMPC) was used for the online trajectory planning and tracking control of the cruise boat in a narrow passage in the canal.","meta":{"url":"http://arxiv.org/abs/2307.09845v1"},"cats":{"new-dataset":0.0333246821,"dev-research":0.2122727601,"prompt-eng":0.4097505066,"data-quality":0.0648499419,"ml-security":0.0672468766}}
{"text":"To consider the nonlinear characteristics of boat dynamics, system identification was performed using experimental data from various test maneuvers, such as acceleration-deceleration and zigzag trials.","meta":{"url":"http://arxiv.org/abs/2307.09845v1"},"cats":{"new-dataset":0.0804270433,"dev-research":0.2356850218,"prompt-eng":0.3992152048,"data-quality":0.147001747,"ml-security":0.1648330731}}
{"text":"To efficiently represent the obstacle structures in the canal environment, we parameterized the canal walls as line segments with point cloud data, captured by an onboard LiDAR sensor, and considered them as constraints for obstacle avoidance.","meta":{"url":"http://arxiv.org/abs/2307.09845v1"},"cats":{"new-dataset":0.1678427186,"dev-research":0.2285728205,"prompt-eng":0.3796359879,"data-quality":0.0860660011,"ml-security":0.1190284034}}
{"text":"The proposed method was implemented in a single NMPC layer, and its real-world performance was verified through experimental runs in the Pohang Canal.","meta":{"url":"http://arxiv.org/abs/2307.09845v1"},"cats":{"new-dataset":0.0363228305,"dev-research":0.2557645719,"prompt-eng":0.3649252067,"data-quality":0.0763321959,"ml-security":0.0610307952}}
{"text":"We recently proposed Acceleration Driven Clause Learning (ADCL), a novel calculus to analyze satisfiability of Constrained Horn Clauses (CHCs).","meta":{"url":"http://arxiv.org/abs/2307.09839v1"},"cats":{"new-dataset":0.0765388795,"dev-research":0.2491478576,"prompt-eng":0.4331222943,"data-quality":0.1006577104,"ml-security":0.1112445511}}
{"text":"Here, we adapt ADCL to disprove termination of transition systems, and we evaluate its implementation in our tool LoAT against the state of the art.","meta":{"url":"http://arxiv.org/abs/2307.09839v1"},"cats":{"new-dataset":0.0568451285,"dev-research":0.3267423352,"prompt-eng":0.4103334972,"data-quality":0.1177871159,"ml-security":0.1168925375}}
{"text":"Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks.","meta":{"url":"http://arxiv.org/abs/2307.09836v1"},"cats":{"new-dataset":0.0460926856,"dev-research":0.220332005,"prompt-eng":0.3852821731,"data-quality":0.1210723153,"ml-security":0.1787078459}}
{"text":"Projections onto the $\\ell_{1,2}$ and $\\ell_{1,\\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks.","meta":{"url":"http://arxiv.org/abs/2307.09836v1"},"cats":{"new-dataset":0.0245423593,"dev-research":0.2446638853,"prompt-eng":0.3666244342,"data-quality":0.1297050494,"ml-security":0.2152306446}}
{"text":"In this paper, we introduce a new projection algorithm for the $\\ell_{1,\\infty}$ norm ball.","meta":{"url":"http://arxiv.org/abs/2307.09836v1"},"cats":{"new-dataset":0.1800607985,"dev-research":0.21985305,"prompt-eng":0.3576650835,"data-quality":0.0985801982,"ml-security":0.0714871536}}
{"text":"The worst-case time complexity of this algorithm is $\\mathcal{O}\\big(nm+J\\log(nm)\\big)$ for a matrix in $\\mathbb{R}^{n\\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low.","meta":{"url":"http://arxiv.org/abs/2307.09836v1"},"cats":{"new-dataset":0.0298316242,"dev-research":0.1778385304,"prompt-eng":0.3169318785,"data-quality":0.1062654226,"ml-security":0.1306174105}}
{"text":"Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time.","meta":{"url":"http://arxiv.org/abs/2307.09836v1"},"cats":{"new-dataset":0.0105181466,"dev-research":0.2289792462,"prompt-eng":0.3329498747,"data-quality":0.0634267098,"ml-security":0.0929917464}}
{"text":"Moreover, we propose to incorporate the $\\ell_{1,\\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights.","meta":{"url":"http://arxiv.org/abs/2307.09836v1"},"cats":{"new-dataset":0.0538043244,"dev-research":0.2289271054,"prompt-eng":0.4536431615,"data-quality":0.1731531456,"ml-security":0.1444821778}}
{"text":"Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($<2\\%$) of the data is relevant.","meta":{"url":"http://arxiv.org/abs/2307.09836v1"},"cats":{"new-dataset":0.0080301051,"dev-research":0.3545422131,"prompt-eng":0.4464069968,"data-quality":0.249764372,"ml-security":0.1962800845}}
{"text":"We show that both in the biological case and in the general case of sparsity that our method is the fastest.","meta":{"url":"http://arxiv.org/abs/2307.09836v1"},"cats":{"new-dataset":0.0094779999,"dev-research":0.2060159631,"prompt-eng":0.3732442202,"data-quality":0.1091470526,"ml-security":0.0812964173}}
{"text":"The Internet has not only digitized but also democratized information access across the globe.","meta":{"url":"http://arxiv.org/abs/2307.09834v1"},"cats":{"new-dataset":0.0495344284,"dev-research":0.2964690684,"prompt-eng":0.3779958391,"data-quality":0.1595273564,"ml-security":0.1628372664}}
{"text":"This gradual but path-breaking move to online information propagation has resulted in search engines playing an increasingly prominent role in shaping access to human knowledge.","meta":{"url":"http://arxiv.org/abs/2307.09834v1"},"cats":{"new-dataset":0.0179146758,"dev-research":0.3579214237,"prompt-eng":0.4262988502,"data-quality":0.0930866082,"ml-security":0.1471725382}}
{"text":"When an Internet user enters a query, the search engine sorts through the hundreds of billions of possible webpages to determine what to show.","meta":{"url":"http://arxiv.org/abs/2307.09834v1"},"cats":{"new-dataset":0.037697114,"dev-research":0.3371576447,"prompt-eng":0.4488557337,"data-quality":0.1093944247,"ml-security":0.1761649144}}
{"text":"Google dominates the search engine market, with Google Search surpassing 80% market share globally every year of the last decade.","meta":{"url":"http://arxiv.org/abs/2307.09834v1"},"cats":{"new-dataset":0.0680997102,"dev-research":0.2522790698,"prompt-eng":0.4036542889,"data-quality":0.0941233476,"ml-security":0.1394149021}}
{"text":"Only in Russia and China do Google competitors claim more market share, with approximately 60% of Internet users in Russia preferring Yandex (compared to 40% in favor of Google) and more than 80% of China's Internet users accessing Baidu as of 2022.","meta":{"url":"http://arxiv.org/abs/2307.09834v1"},"cats":{"new-dataset":0.1965699201,"dev-research":0.2677438044,"prompt-eng":0.3550809506,"data-quality":0.1021575764,"ml-security":0.125802924}}
{"text":"Notwithstanding this long-standing regional variation in Internet search providers, there is limited research showing how these providers compare in terms of propagating state-sponsored information.","meta":{"url":"http://arxiv.org/abs/2307.09834v1"},"cats":{"new-dataset":0.0215206816,"dev-research":0.215674609,"prompt-eng":0.3743611394,"data-quality":0.0993582107,"ml-security":0.1192034282}}
{"text":"Our study fills this research gap by focusing on Russian cyberspace and examining how Google and Yandex's search algorithms rank content from Russian state-controlled media (hereon, RSM) outlets.","meta":{"url":"http://arxiv.org/abs/2307.09834v1"},"cats":{"new-dataset":0.2269047113,"dev-research":0.2275458744,"prompt-eng":0.3734300745,"data-quality":0.1268225915,"ml-security":0.1501336264}}
{"text":"This question is timely and of practical interest given widespread reports indicating that RSM outlets have actively engaged in promoting Kremlin propaganda in the lead-up to, and in the aftermath of, the Russian invasion of Ukraine in February 2022.","meta":{"url":"http://arxiv.org/abs/2307.09834v1"},"cats":{"new-dataset":0.214517881,"dev-research":0.2379969119,"prompt-eng":0.4466030444,"data-quality":0.0975442871,"ml-security":0.2140821079}}
{"text":"To handle the two shortcomings of existing methods, (i)nearly all models rely on high-definition (HD) maps, yet the map information is not always available in real traffic scenes and HD map-building is expensive and time-consuming and (ii) existing models usually focus on improving prediction accuracy at the expense of reducing computing efficiency, yet the efficiency is crucial for various real applications, this paper proposes an efficient trajectory prediction model that is not dependent on traffic maps.","meta":{"url":"http://arxiv.org/abs/2307.09831v1"},"cats":{"new-dataset":0.1154843429,"dev-research":0.2526712938,"prompt-eng":0.359011955,"data-quality":0.081557752,"ml-security":0.1088178747}}
{"text":"The core idea of our model is encoding single-agent's spatial-temporal information in the first stage and exploring multi-agents' spatial-temporal interactions in the second stage.","meta":{"url":"http://arxiv.org/abs/2307.09831v1"},"cats":{"new-dataset":0.080828031,"dev-research":0.2275806871,"prompt-eng":0.4429797282,"data-quality":0.0533927128,"ml-security":0.1136239421}}
{"text":"By comprehensively utilizing attention mechanism, LSTM, graph convolution network and temporal transformer in the two stages, our model is able to learn rich dynamic and interaction information of all agents.","meta":{"url":"http://arxiv.org/abs/2307.09831v1"},"cats":{"new-dataset":0.1984375247,"dev-research":0.2031194281,"prompt-eng":0.4122142104,"data-quality":0.0775970383,"ml-security":0.1459144641}}
{"text":"Our model achieves the highest performance when comparing with existing map-free methods and also exceeds most map-based state-of-the-art methods on the Argoverse dataset.","meta":{"url":"http://arxiv.org/abs/2307.09831v1"},"cats":{"new-dataset":0.4617790197,"dev-research":0.2410681635,"prompt-eng":0.3368825965,"data-quality":0.1030823014,"ml-security":0.0933568065}}
{"text":"In addition, our model also exhibits a faster inference speed than the baseline methods.","meta":{"url":"http://arxiv.org/abs/2307.09831v1"},"cats":{"new-dataset":0.0290690056,"dev-research":0.2628146803,"prompt-eng":0.4139965754,"data-quality":0.0905542505,"ml-security":0.062855644}}
{"text":"Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs).","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.0241660858,"dev-research":0.2877304235,"prompt-eng":0.385646668,"data-quality":0.1796975925,"ml-security":0.1826870318}}
{"text":"Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.0354090896,"dev-research":0.2318282499,"prompt-eng":0.3852836543,"data-quality":0.1465383078,"ml-security":0.2178337672}}
{"text":"This study empirically investigates the latter and expands the understanding of frequency shortcuts.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.0290495009,"dev-research":0.3830368424,"prompt-eng":0.4403721107,"data-quality":0.1358804404,"ml-security":0.0722673068}}
{"text":"First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.201511309,"dev-research":0.2347375749,"prompt-eng":0.3799533895,"data-quality":0.2083599646,"ml-security":0.1534315599}}
{"text":"Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.086390636,"dev-research":0.2525287408,"prompt-eng":0.4035417213,"data-quality":0.2752079683,"ml-security":0.1925584508}}
{"text":"Second, we confirm this phenomenon on natural images.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.0443483507,"dev-research":0.2474860844,"prompt-eng":0.4198057032,"data-quality":0.2465603525,"ml-security":0.088881986}}
{"text":"We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.1470232447,"dev-research":0.3389739856,"prompt-eng":0.4517687541,"data-quality":0.2071163472,"ml-security":0.0793389998}}
{"text":"The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.0293313954,"dev-research":0.3117746286,"prompt-eng":0.4107791658,"data-quality":0.0853958447,"ml-security":0.0512064708}}
{"text":"Third, we validate the transferability of frequency shortcuts on out-of-distribution (OOD) test sets.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.0682973441,"dev-research":0.2492940124,"prompt-eng":0.4342483039,"data-quality":0.206707534,"ml-security":0.1144759638}}
{"text":"Our results suggest that frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.1741754746,"dev-research":0.2752570084,"prompt-eng":0.3800333563,"data-quality":0.1316073804,"ml-security":0.117729971}}
{"text":"We recommend that future research should focus on effective training schemes mitigating frequency shortcut learning.","meta":{"url":"http://arxiv.org/abs/2307.09829v1"},"cats":{"new-dataset":0.0293565656,"dev-research":0.3083105033,"prompt-eng":0.4268770317,"data-quality":0.1192203402,"ml-security":0.1998829557}}
{"text":"Vision systems mounted on home robots need to interact with unseen classes in changing environments.","meta":{"url":"http://arxiv.org/abs/2307.09827v1"},"cats":{"new-dataset":0.0705756996,"dev-research":0.2462526518,"prompt-eng":0.4337116509,"data-quality":0.1170034958,"ml-security":0.1653796434}}
{"text":"Robots have limited computational resources, labelled data and storage capability.","meta":{"url":"http://arxiv.org/abs/2307.09827v1"},"cats":{"new-dataset":0.098665941,"dev-research":0.2335871981,"prompt-eng":0.3348705084,"data-quality":0.0555794148,"ml-security":0.1290429745}}
{"text":"These requirements pose some unique challenges: models should adapt without forgetting past knowledge in a data- and parameter-efficient way.","meta":{"url":"http://arxiv.org/abs/2307.09827v1"},"cats":{"new-dataset":0.041742862,"dev-research":0.2342906662,"prompt-eng":0.4318077222,"data-quality":0.1146632628,"ml-security":0.1475723924}}
{"text":"We characterize the problem as few-shot (FS) online continual learning (OCL), where robotic agents learn from a non-repeated stream of few-shot data updating only a few model parameters.","meta":{"url":"http://arxiv.org/abs/2307.09827v1"},"cats":{"new-dataset":0.1063461004,"dev-research":0.1882326544,"prompt-eng":0.369880694,"data-quality":0.1116962332,"ml-security":0.1885248553}}
{"text":"Additionally, such models experience variable conditions at test time, where objects may appear in different poses (e.g., horizontal or vertical) and environments (e.g., day or night).","meta":{"url":"http://arxiv.org/abs/2307.09827v1"},"cats":{"new-dataset":0.0716288033,"dev-research":0.2281411504,"prompt-eng":0.4529450328,"data-quality":0.0650405174,"ml-security":0.1349330893}}
{"text":"To improve robustness of CL agents, we propose RobOCLe, which; 1) constructs an enriched feature space computing high order statistical moments from the embedded features of samples; and 2) computes similarity between high order statistics of the samples on the enriched feature space, and predicts their class labels.","meta":{"url":"http://arxiv.org/abs/2307.09827v1"},"cats":{"new-dataset":0.1479929304,"dev-research":0.2781071919,"prompt-eng":0.4511761278,"data-quality":0.202320961,"ml-security":0.2794005332}}
{"text":"We evaluate robustness of CL models to train/test augmentations in various cases.","meta":{"url":"http://arxiv.org/abs/2307.09827v1"},"cats":{"new-dataset":0.0200992708,"dev-research":0.2490021521,"prompt-eng":0.491868747,"data-quality":0.3038779603,"ml-security":0.2692114072}}
{"text":"We show that different moments allow RobOCLe to capture different properties of deformations, providing higher robustness with no decrease of inference speed.","meta":{"url":"http://arxiv.org/abs/2307.09827v1"},"cats":{"new-dataset":0.0367972981,"dev-research":0.2431087844,"prompt-eng":0.4338658872,"data-quality":0.1271071628,"ml-security":0.1124989238}}
{"text":"Despite the wide variety of methods developed for synthetic image attribution, most of them can only attribute images generated by models or architectures included in the training set and do not work with unknown architectures, hindering their applicability in real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09822v1"},"cats":{"new-dataset":0.074777996,"dev-research":0.2478380032,"prompt-eng":0.4059435057,"data-quality":0.2593019496,"ml-security":0.3694178421}}
{"text":"In this paper, we propose a verification framework that relies on a Siamese Network to address the problem of open-set attribution of synthetic images to the architecture that generated them.","meta":{"url":"http://arxiv.org/abs/2307.09822v1"},"cats":{"new-dataset":0.2938656028,"dev-research":0.2660248375,"prompt-eng":0.3741210695,"data-quality":0.2722173078,"ml-security":0.1552359319}}
{"text":"We consider two different settings.","meta":{"url":"http://arxiv.org/abs/2307.09822v1"},"cats":{"new-dataset":0.0370574608,"dev-research":0.3045008455,"prompt-eng":0.3958356427,"data-quality":0.0844539117,"ml-security":0.0666491926}}
{"text":"In the first setting, the system determines whether two images have been produced by the same generative architecture or not.","meta":{"url":"http://arxiv.org/abs/2307.09822v1"},"cats":{"new-dataset":0.0255569361,"dev-research":0.2535691167,"prompt-eng":0.461979476,"data-quality":0.1518075215,"ml-security":0.0455089317}}
{"text":"In the second setting, the system verifies a claim about the architecture used to generate a synthetic image, utilizing one or multiple reference images generated by the claimed architecture.","meta":{"url":"http://arxiv.org/abs/2307.09822v1"},"cats":{"new-dataset":0.0785347088,"dev-research":0.3006718692,"prompt-eng":0.4177846879,"data-quality":0.1358449568,"ml-security":0.0835723268}}
{"text":"The main strength of the proposed system is its ability to operate in both closed and open-set scenarios so that the input images, either the query and reference images, can belong to the architectures considered during training or not.","meta":{"url":"http://arxiv.org/abs/2307.09822v1"},"cats":{"new-dataset":0.1079191263,"dev-research":0.2283850944,"prompt-eng":0.404470018,"data-quality":0.0901944194,"ml-security":0.1803351086}}
{"text":"Experimental evaluations encompassing various generative architectures such as GANs, diffusion models, and transformers, focusing on synthetic face image generation, confirm the excellent performance of our method in both closed and open-set settings, as well as its strong generalization capabilities.","meta":{"url":"http://arxiv.org/abs/2307.09822v1"},"cats":{"new-dataset":0.0707995076,"dev-research":0.2407670646,"prompt-eng":0.4088000595,"data-quality":0.0955132417,"ml-security":0.1463054943}}
{"text":"In dyadic speaker-listener interactions, the listener's head reactions along with the speaker's head movements, constitute an important non-verbal semantic expression together.","meta":{"url":"http://arxiv.org/abs/2307.09821v1"},"cats":{"new-dataset":0.014974407,"dev-research":0.3277122306,"prompt-eng":0.401027686,"data-quality":0.1291236586,"ml-security":0.0627527033}}
{"text":"The listener Head generation task aims to synthesize responsive listener's head videos based on audios of the speaker and reference images of the listener.","meta":{"url":"http://arxiv.org/abs/2307.09821v1"},"cats":{"new-dataset":0.0750096251,"dev-research":0.3118402072,"prompt-eng":0.4428229526,"data-quality":0.1091698365,"ml-security":0.0477184501}}
{"text":"Compared to the Talking-head generation, it is more challenging to capture the correlation clues from the speaker's audio and visual information.","meta":{"url":"http://arxiv.org/abs/2307.09821v1"},"cats":{"new-dataset":0.0509212238,"dev-research":0.3801152557,"prompt-eng":0.4173183156,"data-quality":0.1854635991,"ml-security":0.0766100365}}
{"text":"Following the ViCo baseline scheme, we propose a high-performance solution by enhancing the hierarchical semantic extraction capability of the audio encoder module and improving the decoder part, renderer and post-processing modules.","meta":{"url":"http://arxiv.org/abs/2307.09821v1"},"cats":{"new-dataset":0.1080976721,"dev-research":0.2871655548,"prompt-eng":0.3856529184,"data-quality":0.1933916982,"ml-security":0.040091103}}
{"text":"Our solution gets the first place on the official leaderboard for the track of listening head generation.","meta":{"url":"http://arxiv.org/abs/2307.09821v1"},"cats":{"new-dataset":0.1059568319,"dev-research":0.3077368105,"prompt-eng":0.4483927985,"data-quality":0.1497867449,"ml-security":0.0827936021}}
{"text":"This paper is a technical report of ViCo@2023 Conversational Head Generation Challenge in ACM Multimedia 2023 conference.","meta":{"url":"http://arxiv.org/abs/2307.09821v1"},"cats":{"new-dataset":0.1956578731,"dev-research":0.2312110929,"prompt-eng":0.4705927141,"data-quality":0.1149617625,"ml-security":0.0506595272}}
{"text":"In this paper, we study the Greek wiretappings scandal, which has been revealed in 2022 and attracted a lot of attention by press and citizens.","meta":{"url":"http://arxiv.org/abs/2307.09819v1"},"cats":{"new-dataset":0.3051666429,"dev-research":0.2895105154,"prompt-eng":0.3635066663,"data-quality":0.2149923629,"ml-security":0.3746191804}}
{"text":"Specifically, we propose a methodology for collecting data and analyzing patterns of online public discussions on Twitter.","meta":{"url":"http://arxiv.org/abs/2307.09819v1"},"cats":{"new-dataset":0.3194625935,"dev-research":0.3388014457,"prompt-eng":0.4023755013,"data-quality":0.1623487916,"ml-security":0.0757402358}}
{"text":"We apply our methodology to the Greek wiretappings use case, and present findings related to the evolution of the discussion over time, its polarization, and the role of the media.","meta":{"url":"http://arxiv.org/abs/2307.09819v1"},"cats":{"new-dataset":0.1630283056,"dev-research":0.3174493434,"prompt-eng":0.3617417281,"data-quality":0.1782036177,"ml-security":0.1512115173}}
{"text":"The methodology can be of wider use and replicated to other topics.","meta":{"url":"http://arxiv.org/abs/2307.09819v1"},"cats":{"new-dataset":0.0403563627,"dev-research":0.3756904709,"prompt-eng":0.3809102251,"data-quality":0.0809398365,"ml-security":0.0572023958}}
{"text":"Finally, we provide publicly an open dataset, and online resources with the results.","meta":{"url":"http://arxiv.org/abs/2307.09819v1"},"cats":{"new-dataset":0.9076860272,"dev-research":0.2647600374,"prompt-eng":0.3512881,"data-quality":0.1145567196,"ml-security":0.1023753795}}
{"text":"Recovering sharp images from dual-pixel (DP) pairs with disparity-dependent blur is a challenging task.~Existing blur map-based deblurring methods have demonstrated promising results.","meta":{"url":"http://arxiv.org/abs/2307.09815v1"},"cats":{"new-dataset":0.1167683026,"dev-research":0.2685713478,"prompt-eng":0.3300220701,"data-quality":0.180244294,"ml-security":0.1002052636}}
{"text":"In this paper, we propose, to the best of our knowledge, the first framework to introduce the contrastive language-image pre-training framework (CLIP) to achieve accurate blur map estimation from DP pairs unsupervisedly.","meta":{"url":"http://arxiv.org/abs/2307.09815v1"},"cats":{"new-dataset":0.4155970117,"dev-research":0.2632550111,"prompt-eng":0.4094533742,"data-quality":0.2165284077,"ml-security":0.0683109036}}
{"text":"To this end, we first carefully design text prompts to enable CLIP to understand blur-related geometric prior knowledge from the DP pair.","meta":{"url":"http://arxiv.org/abs/2307.09815v1"},"cats":{"new-dataset":0.0670930506,"dev-research":0.3570507281,"prompt-eng":0.4977000789,"data-quality":0.1437720139,"ml-security":0.0669372688}}
{"text":"Then, we propose a format to input stereo DP pair to the CLIP without any fine-tuning, where the CLIP is pre-trained on monocular images.","meta":{"url":"http://arxiv.org/abs/2307.09815v1"},"cats":{"new-dataset":0.041086173,"dev-research":0.2363072279,"prompt-eng":0.4267273821,"data-quality":0.156331493,"ml-security":0.052739719}}
{"text":"Given the estimated blur map, we introduce a blur-prior attention block, a blur-weighting loss and a blur-aware loss to recover the all-in-focus image.","meta":{"url":"http://arxiv.org/abs/2307.09815v1"},"cats":{"new-dataset":0.083815914,"dev-research":0.2369269086,"prompt-eng":0.3853924628,"data-quality":0.1428426609,"ml-security":0.0733857792}}
{"text":"Our method achieves state-of-the-art performance in extensive experiments.","meta":{"url":"http://arxiv.org/abs/2307.09815v1"},"cats":{"new-dataset":0.0207930958,"dev-research":0.2373599451,"prompt-eng":0.4368308069,"data-quality":0.0899480787,"ml-security":0.0627211807}}
{"text":"Event Causality Identification (ECI) aims at determining whether there is a causal relation between two event mentions.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.0736977844,"dev-research":0.3322968238,"prompt-eng":0.4366744921,"data-quality":0.2535953621,"ml-security":0.0868902823}}
{"text":"Conventional prompt learning designs a prompt template to first predict an answer word and then maps it to the final decision.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.0468576568,"dev-research":0.3391619725,"prompt-eng":0.6210264184,"data-quality":0.1216664814,"ml-security":0.2062924103}}
{"text":"Unlike conventional prompts, we argue that predicting an answer word may not be a necessary prerequisite for the ECI task.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.0255264051,"dev-research":0.2954399489,"prompt-eng":0.5472226115,"data-quality":0.1758033991,"ml-security":0.1318754772}}
{"text":"Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.0123218333,"dev-research":0.3003692269,"prompt-eng":0.4539034996,"data-quality":0.1713212787,"ml-security":0.1857343561}}
{"text":"The design motivation is to try the most utilization of the encyclopedia-like knowledge embedded in a pre-trained language model.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.0651187409,"dev-research":0.3588383602,"prompt-eng":0.4896323026,"data-quality":0.1210823271,"ml-security":0.0773835322}}
{"text":"In light of such considerations, we propose a deterministic assumption prompt learning model, called DAPrompt, for the ECI task.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.0379651182,"dev-research":0.2720798201,"prompt-eng":0.5823436765,"data-quality":0.1649400638,"ml-security":0.3323944885}}
{"text":"In particular, we design a simple deterministic assumption template concatenating with the input event pair, which includes two masks as predicted events' tokens.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.0555493496,"dev-research":0.2942519309,"prompt-eng":0.5274660058,"data-quality":0.1685684503,"ml-security":0.2703181608}}
{"text":"We use the probabilities of predicted events to evaluate the assumption rationality for the final event causality decision.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.0165645238,"dev-research":0.3342118627,"prompt-eng":0.4843709257,"data-quality":0.1857776928,"ml-security":0.1906045859}}
{"text":"Experiments on the EventStoryLine corpus and Causal-TimeBank corpus validate our design objective in terms of significant performance improvements over the state-of-the-art algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09813v1"},"cats":{"new-dataset":0.2214226093,"dev-research":0.352862764,"prompt-eng":0.4197102646,"data-quality":0.2336943872,"ml-security":0.0809427901}}
{"text":"Web image datasets curated online inherently contain ambiguous in-distribution (ID) instances and out-of-distribution (OOD) instances, which we collectively call non-conforming (NC) instances.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.2607047612,"dev-research":0.2345895296,"prompt-eng":0.3541468239,"data-quality":0.303869977,"ml-security":0.153869971}}
{"text":"In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.0116495861,"dev-research":0.2821919861,"prompt-eng":0.3934926314,"data-quality":0.210008346,"ml-security":0.3243306157}}
{"text":"For \"entropy\" to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.0528271034,"dev-research":0.2757988294,"prompt-eng":0.4160661094,"data-quality":0.1598563073,"ml-security":0.1818214834}}
{"text":"Hence, entropy maximization is based on the idealized assumption that NC instances have predictions that are \"almost\" uniformly distributed.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.0144464364,"dev-research":0.2336433693,"prompt-eng":0.4013041411,"data-quality":0.1457720667,"ml-security":0.1955429446}}
{"text":"However, in real-world web image datasets, there are numerous NC instances whose predictions are far from being uniformly distributed.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.1292540472,"dev-research":0.2166214788,"prompt-eng":0.3724641279,"data-quality":0.2433381896,"ml-security":0.2366920121}}
{"text":"To tackle the limitation of entropy maximization, we propose $(\\alpha, \\beta)$-generalized KL divergence, $\\mathcal{D}_{\\text{KL}}^{\\alpha, \\beta}(p\\|q)$, which can be used to identify significantly more NC instances.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.0485020591,"dev-research":0.1956652037,"prompt-eng":0.383010046,"data-quality":0.1413943604,"ml-security":0.1287478285}}
{"text":"Theoretical properties of $\\mathcal{D}_{\\text{KL}}^{\\alpha, \\beta}(p\\|q)$ are proven, and we also show empirically that a simple use of $\\mathcal{D}_{\\text{KL}}^{\\alpha, \\beta}(p\\|q)$ outperforms all baselines on the NC instance identification task.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.0685072614,"dev-research":0.2389796498,"prompt-eng":0.4418708723,"data-quality":0.2562039823,"ml-security":0.1789841543}}
{"text":"Building upon $(\\alpha,\\beta)$-generalized KL divergence, we also introduce a new iterative training framework, GenKL, that identifies and relabels NC instances.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.1471269233,"dev-research":0.2202670327,"prompt-eng":0.3773943023,"data-quality":0.1991975533,"ml-security":0.103958249}}
{"text":"When evaluated on three web image datasets, Clothing1M, Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art classification accuracies: $81.34\\%$, $85.73\\%$ and $78.99\\%$/$92.54\\%$ (top-1/top-5), respectively.","meta":{"url":"http://arxiv.org/abs/2307.09810v1"},"cats":{"new-dataset":0.2670974524,"dev-research":0.2629322636,"prompt-eng":0.4176586084,"data-quality":0.2406879892,"ml-security":0.110634888}}
{"text":"Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has been proposed recently as a novel and generalized RIS architecture that offers enhanced wave manipulation flexibility and large coverage expansion.","meta":{"url":"http://arxiv.org/abs/2307.09807v1"},"cats":{"new-dataset":0.0326490987,"dev-research":0.2332121455,"prompt-eng":0.3864705724,"data-quality":0.0372979649,"ml-security":0.0738037778}}
{"text":"However, the beyond-diagonal mathematical model in BD-RIS inevitably introduces additional optimization challenges in beamforming design.","meta":{"url":"http://arxiv.org/abs/2307.09807v1"},"cats":{"new-dataset":0.0072456364,"dev-research":0.1965402604,"prompt-eng":0.4056500103,"data-quality":0.0615786251,"ml-security":0.0747779421}}
{"text":"In this letter, we derive a closed-form solution for the BD-RIS passive beamforming matrix that maximizes the sum of the effective channel gains among users.","meta":{"url":"http://arxiv.org/abs/2307.09807v1"},"cats":{"new-dataset":0.0202138715,"dev-research":0.2199326876,"prompt-eng":0.4090991811,"data-quality":0.0747216432,"ml-security":0.1015251786}}
{"text":"We further propose a computationally efficient two-stage beamforming framework to jointly design the active beamforming at the base station and passive beamforming at the BD-RIS to enhance the sum-rate for a BD-RIS aided multi-user multi-antenna network.","meta":{"url":"http://arxiv.org/abs/2307.09807v1"},"cats":{"new-dataset":0.0198883553,"dev-research":0.2293416365,"prompt-eng":0.3986393469,"data-quality":0.0858193766,"ml-security":0.053094069}}
{"text":"Numerical results show that our proposed algorithm achieves a higher sum-rate while requiring less computation time compared to state-of-the-art algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09807v1"},"cats":{"new-dataset":0.0244709585,"dev-research":0.2000810964,"prompt-eng":0.3259650438,"data-quality":0.0703098924,"ml-security":0.0521314842}}
{"text":"The proposed algorithm paves the way for practical beamforming design in BD-RIS aided wireless networks.","meta":{"url":"http://arxiv.org/abs/2307.09807v1"},"cats":{"new-dataset":0.0162528286,"dev-research":0.2399688833,"prompt-eng":0.391412435,"data-quality":0.0959789236,"ml-security":0.0602539608}}
{"text":"Convolutional neural networks encode images through a sequence of convolutions, normalizations and non-linearities as well as downsampling operations into potentially strong semantic embeddings.","meta":{"url":"http://arxiv.org/abs/2307.09804v1"},"cats":{"new-dataset":0.0913336213,"dev-research":0.2779106997,"prompt-eng":0.3952381994,"data-quality":0.2249843197,"ml-security":0.1964952023}}
{"text":"Yet, previous work showed that even slight mistakes during sampling, leading to aliasing, can be directly attributed to the networks' lack in robustness.","meta":{"url":"http://arxiv.org/abs/2307.09804v1"},"cats":{"new-dataset":0.018786496,"dev-research":0.3094118691,"prompt-eng":0.3708812221,"data-quality":0.5576926799,"ml-security":0.2782038109}}
{"text":"To address such issues and facilitate simpler and faster adversarial training, [12] recently proposed FLC pooling, a method for provably alias-free downsampling - in theory.","meta":{"url":"http://arxiv.org/abs/2307.09804v1"},"cats":{"new-dataset":0.0984705505,"dev-research":0.216242511,"prompt-eng":0.3525324353,"data-quality":0.2949674487,"ml-security":0.440711178}}
{"text":"In this work, we conduct a further analysis through the lens of signal processing and find that such current pooling methods, which address aliasing in the frequency domain, are still prone to spectral leakage artifacts.","meta":{"url":"http://arxiv.org/abs/2307.09804v1"},"cats":{"new-dataset":0.0396955821,"dev-research":0.2754259657,"prompt-eng":0.3432449255,"data-quality":0.3407230429,"ml-security":0.1876537401}}
{"text":"Hence, we propose aliasing and spectral artifact-free pooling, short ASAP.","meta":{"url":"http://arxiv.org/abs/2307.09804v1"},"cats":{"new-dataset":0.0584700986,"dev-research":0.2408629632,"prompt-eng":0.3874840091,"data-quality":0.2657530099,"ml-security":0.0706711285}}
{"text":"While only introducing a few modifications to FLC pooling, networks using ASAP as downsampling method exhibit higher native robustness against common corruptions, a property that FLC pooling was missing.","meta":{"url":"http://arxiv.org/abs/2307.09804v1"},"cats":{"new-dataset":0.0555997198,"dev-research":0.3034628511,"prompt-eng":0.3385790548,"data-quality":0.3213591178,"ml-security":0.2230949348}}
{"text":"ASAP also increases native robustness against adversarial attacks on high and low resolution data while maintaining similar clean accuracy or even outperforming the baseline.","meta":{"url":"http://arxiv.org/abs/2307.09804v1"},"cats":{"new-dataset":0.0436550563,"dev-research":0.3679527152,"prompt-eng":0.3593538926,"data-quality":0.2478705857,"ml-security":0.491073585}}
{"text":"Graph learning has a wide range of applications in many scenarios, which require more need for data privacy.","meta":{"url":"http://arxiv.org/abs/2307.09801v1"},"cats":{"new-dataset":0.10398648,"dev-research":0.2498427368,"prompt-eng":0.3304904829,"data-quality":0.1596809961,"ml-security":0.4662965698}}
{"text":"Federated learning is an emerging distributed machine learning approach that leverages data from individual devices or data centers to improve the accuracy and generalization of the model, while also protecting the privacy of user data.","meta":{"url":"http://arxiv.org/abs/2307.09801v1"},"cats":{"new-dataset":0.0924441938,"dev-research":0.2623341352,"prompt-eng":0.368526888,"data-quality":0.1237876726,"ml-security":0.3483055844}}
{"text":"Graph-federated learning is mainly based on the classical federated learning framework i.e., the Client-Server framework.","meta":{"url":"http://arxiv.org/abs/2307.09801v1"},"cats":{"new-dataset":0.1069072544,"dev-research":0.2290281928,"prompt-eng":0.3719208115,"data-quality":0.1170731207,"ml-security":0.1782720756}}
{"text":"However, the Client-Server framework faces problems such as a single point of failure of the central server and poor scalability of network topology.","meta":{"url":"http://arxiv.org/abs/2307.09801v1"},"cats":{"new-dataset":0.0231021856,"dev-research":0.3018584832,"prompt-eng":0.3379364194,"data-quality":0.1884668212,"ml-security":0.1811615277}}
{"text":"First, we introduce the decentralized framework to graph-federated learning.","meta":{"url":"http://arxiv.org/abs/2307.09801v1"},"cats":{"new-dataset":0.1240879166,"dev-research":0.2136627896,"prompt-eng":0.3617393683,"data-quality":0.1386695231,"ml-security":0.16605495}}
{"text":"Second, determine the confidence among nodes based on the similarity of data among nodes, subsequently, the gradient information is then aggregated by linear weighting based on confidence.","meta":{"url":"http://arxiv.org/abs/2307.09801v1"},"cats":{"new-dataset":0.0165128648,"dev-research":0.2283189089,"prompt-eng":0.4308748901,"data-quality":0.2321782219,"ml-security":0.1201417863}}
{"text":"Finally, the proposed method is compared with FedAvg, Fedprox, GCFL, and GCFL+ to verify the effectiveness of the proposed method.","meta":{"url":"http://arxiv.org/abs/2307.09801v1"},"cats":{"new-dataset":0.0234275086,"dev-research":0.2741511603,"prompt-eng":0.3862695517,"data-quality":0.1674605347,"ml-security":0.0718459105}}
{"text":"Experiments demonstrate that the proposed method outperforms other methods.","meta":{"url":"http://arxiv.org/abs/2307.09801v1"},"cats":{"new-dataset":0.0037403856,"dev-research":0.3260141969,"prompt-eng":0.3992193443,"data-quality":0.1883950381,"ml-security":0.0718762575}}
{"text":"Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.0503364475,"dev-research":0.2288783178,"prompt-eng":0.4568975966,"data-quality":0.1338569976,"ml-security":0.0904967767}}
{"text":"The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.020435409,"dev-research":0.2403602368,"prompt-eng":0.4159649739,"data-quality":0.0669474673,"ml-security":0.1049248762}}
{"text":"In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.074183481,"dev-research":0.2331994017,"prompt-eng":0.440242613,"data-quality":0.0750437385,"ml-security":0.0599966542}}
{"text":"This is a consequence of a simple (exchangeability) observation: permuting \\textit{}base-level series in the hierarchy does not change their aggregates.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.0354566667,"dev-research":0.2379795115,"prompt-eng":0.364986782,"data-quality":0.1466693736,"ml-security":0.0685060126}}
{"text":"Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probability Score and quantile losses.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.0634785492,"dev-research":0.2181120263,"prompt-eng":0.429710068,"data-quality":0.1169364871,"ml-security":0.1975129409}}
{"text":"We can choose arbitrary continuous distributions for the factor and the base-level distributions.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.0497044942,"dev-research":0.1731962492,"prompt-eng":0.4275463035,"data-quality":0.0905560675,"ml-security":0.0948537135}}
{"text":"We compare our method to two previous methods which can be optimized end-to-end, while enforcing coherent aggregation.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.0476799244,"dev-research":0.2526522505,"prompt-eng":0.353925396,"data-quality":0.1941895786,"ml-security":0.049231724}}
{"text":"Our model achieves significant improvements: between $11.8-41.4\\%$ on three hierarchical forecasting datasets.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.1380384878,"dev-research":0.2733880057,"prompt-eng":0.4140077192,"data-quality":0.1212527696,"ml-security":0.080741482}}
{"text":"We also analyze the influence of parameters in our model with respect to base-level distribution and number of factors.","meta":{"url":"http://arxiv.org/abs/2307.09797v1"},"cats":{"new-dataset":0.0412669399,"dev-research":0.1950104046,"prompt-eng":0.488895127,"data-quality":0.0781161181,"ml-security":0.0918012073}}
{"text":"In the early observation period of a time series, there might be only a few historic observations available to learn a model.","meta":{"url":"http://arxiv.org/abs/2307.09796v1"},"cats":{"new-dataset":0.1079708903,"dev-research":0.1788108588,"prompt-eng":0.3668887405,"data-quality":0.0813895999,"ml-security":0.1172087683}}
{"text":"However, in cases where an existing prior set of datasets is available, Meta learning methods can be applicable.","meta":{"url":"http://arxiv.org/abs/2307.09796v1"},"cats":{"new-dataset":0.1584764726,"dev-research":0.255018243,"prompt-eng":0.3685012114,"data-quality":0.1860634652,"ml-security":0.2124382849}}
{"text":"In this paper, we devise a Meta learning method that exploits samples from additional datasets and learns to augment time series through adversarial learning as an auxiliary task for the target dataset.","meta":{"url":"http://arxiv.org/abs/2307.09796v1"},"cats":{"new-dataset":0.2282011786,"dev-research":0.2464969699,"prompt-eng":0.359988455,"data-quality":0.2152320467,"ml-security":0.4432254478}}
{"text":"Our model (FEML), is equipped with a shared Convolutional backbone that learns features for varying length inputs from different datasets and has dataset specific heads to forecast for different output lengths.","meta":{"url":"http://arxiv.org/abs/2307.09796v1"},"cats":{"new-dataset":0.1748766211,"dev-research":0.2202298203,"prompt-eng":0.3727927879,"data-quality":0.1042762174,"ml-security":0.1821727134}}
{"text":"We show that FEML can meta learn across datasets and by additionally learning on adversarial generated samples as auxiliary samples for the target dataset, it can improve the forecasting performance compared to single task learning, and various solutions adapted from Joint learning, Multi-task learning and classic forecasting baselines.","meta":{"url":"http://arxiv.org/abs/2307.09796v1"},"cats":{"new-dataset":0.1171767502,"dev-research":0.220720881,"prompt-eng":0.3982879091,"data-quality":0.1642108263,"ml-security":0.2907110896}}
{"text":"Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.09795v1"},"cats":{"new-dataset":0.1264439725,"dev-research":0.2448075237,"prompt-eng":0.395169197,"data-quality":0.1177555088,"ml-security":0.1028816108}}
{"text":"At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles.","meta":{"url":"http://arxiv.org/abs/2307.09795v1"},"cats":{"new-dataset":0.0349134693,"dev-research":0.2258693318,"prompt-eng":0.4251421477,"data-quality":0.1384051748,"ml-security":0.1060554696}}
{"text":"This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles.","meta":{"url":"http://arxiv.org/abs/2307.09795v1"},"cats":{"new-dataset":0.1222150706,"dev-research":0.2208060112,"prompt-eng":0.379181995,"data-quality":0.1878029968,"ml-security":0.0901110753}}
{"text":"To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to.","meta":{"url":"http://arxiv.org/abs/2307.09795v1"},"cats":{"new-dataset":0.1636471266,"dev-research":0.2838830876,"prompt-eng":0.390552532,"data-quality":0.2391634555,"ml-security":0.0733519667}}
{"text":"We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music.","meta":{"url":"http://arxiv.org/abs/2307.09795v1"},"cats":{"new-dataset":0.8531931882,"dev-research":0.2277867764,"prompt-eng":0.3111206499,"data-quality":0.2228390506,"ml-security":0.0758595431}}
{"text":"Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset.","meta":{"url":"http://arxiv.org/abs/2307.09795v1"},"cats":{"new-dataset":0.1704074834,"dev-research":0.2453761471,"prompt-eng":0.4025656187,"data-quality":0.352601047,"ml-security":0.0898706059}}
{"text":"Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture.","meta":{"url":"http://arxiv.org/abs/2307.09795v1"},"cats":{"new-dataset":0.0982815217,"dev-research":0.2380483722,"prompt-eng":0.3753538732,"data-quality":0.2100602699,"ml-security":0.0984729507}}
{"text":"The implementation and the trained models are both provided in a public repository.","meta":{"url":"http://arxiv.org/abs/2307.09795v1"},"cats":{"new-dataset":0.1392195958,"dev-research":0.2398279493,"prompt-eng":0.4072349223,"data-quality":0.11536497,"ml-security":0.1306557999}}
{"text":"Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.2617135781,"dev-research":0.214657002,"prompt-eng":0.504848981,"data-quality":0.1404333246,"ml-security":0.1359029531}}
{"text":"Hundreds of new LLMs are announced each week, many of which are deposited to Hugging Face, a repository of machine learning models and datasets.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.453552758,"dev-research":0.2029393346,"prompt-eng":0.4806197543,"data-quality":0.1005102427,"ml-security":0.2096816786}}
{"text":"To date, nearly 16,000 Text Generation models have been uploaded to the site.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.51738544,"dev-research":0.276188033,"prompt-eng":0.4501096437,"data-quality":0.1418997244,"ml-security":0.0774148629}}
{"text":"Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.1026735972,"dev-research":0.2030122349,"prompt-eng":0.5023512265,"data-quality":0.061554869,"ml-security":0.1278583985}}
{"text":"However, there is no comprehensive index of LLMs available.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.1631251344,"dev-research":0.1174367635,"prompt-eng":0.3693937111,"data-quality":0.0746323356,"ml-security":0.0639663064}}
{"text":"We take advantage of the relatively systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering and identify communities amongst LLMs using n-grams and term frequency-inverse document frequency.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.1760568706,"dev-research":0.2274017933,"prompt-eng":0.4609316769,"data-quality":0.1799340365,"ml-security":0.058746559}}
{"text":"Our methods successfully identify families of LLMs and accurately cluster LLMs into meaningful subgroups.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.0973774027,"dev-research":0.1756669338,"prompt-eng":0.5159811165,"data-quality":0.2216527178,"ml-security":0.0834557942}}
{"text":"We present a public web application to navigate and explore Constellation, our atlas of 15,821 LLMs.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.5119191379,"dev-research":0.2489640038,"prompt-eng":0.4749836185,"data-quality":0.0537000053,"ml-security":0.0488394212}}
{"text":"Constellation rapidly generates a variety of visualizations, namely dendrograms, graphs, word clouds, and scatter plots.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.422304102,"dev-research":0.3865193919,"prompt-eng":0.4350539867,"data-quality":0.088972379,"ml-security":0.0566508734}}
{"text":"Constellation is available at the following link: https://constellation.sites.stanford.edu/.","meta":{"url":"http://arxiv.org/abs/2307.09793v1"},"cats":{"new-dataset":0.4700974061,"dev-research":0.1979864476,"prompt-eng":0.4353312217,"data-quality":0.0670520297,"ml-security":0.0392533212}}
{"text":"Registration of distant outdoor LiDAR point clouds is crucial to extending the 3D vision of collaborative autonomous vehicles, and yet is challenging due to small overlapping area and a huge disparity between observed point densities.","meta":{"url":"http://arxiv.org/abs/2307.09788v1"},"cats":{"new-dataset":0.0604405158,"dev-research":0.2443714654,"prompt-eng":0.3681476511,"data-quality":0.10925101,"ml-security":0.0820070551}}
{"text":"In this paper, we propose Group-wise Contrastive Learning (GCL) scheme to extract density-invariant geometric features to register distant outdoor LiDAR point clouds.","meta":{"url":"http://arxiv.org/abs/2307.09788v1"},"cats":{"new-dataset":0.0959173714,"dev-research":0.2279976117,"prompt-eng":0.342617337,"data-quality":0.1019683471,"ml-security":0.153782959}}
{"text":"We mark through theoretical analysis and experiments that, contrastive positives should be independent and identically distributed (i.i.d.), in order to train densityinvariant feature extractors.","meta":{"url":"http://arxiv.org/abs/2307.09788v1"},"cats":{"new-dataset":0.028597887,"dev-research":0.2344298233,"prompt-eng":0.4252574082,"data-quality":0.2386118103,"ml-security":0.1520921361}}
{"text":"We propose upon the conclusion a simple yet effective training scheme to force the feature of multiple point clouds in the same spatial location (referred to as positive groups) to be similar, which naturally avoids the sampling bias introduced by a pair of point clouds to conform with the i.i.d. principle.","meta":{"url":"http://arxiv.org/abs/2307.09788v1"},"cats":{"new-dataset":0.0401340882,"dev-research":0.1843439368,"prompt-eng":0.3728789126,"data-quality":0.1452118715,"ml-security":0.1643570687}}
{"text":"The resulting fully-convolutional feature extractor is more powerful and density-invariant than state-of-the-art methods, improving the registration recall of distant scenarios on KITTI and nuScenes benchmarks by 40.9% and 26.9%, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09788v1"},"cats":{"new-dataset":0.1964284421,"dev-research":0.3211049764,"prompt-eng":0.414834254,"data-quality":0.191788361,"ml-security":0.0740392647}}
{"text":"The code will be open-sourced.","meta":{"url":"http://arxiv.org/abs/2307.09788v1"},"cats":{"new-dataset":0.4728602915,"dev-research":0.3958021867,"prompt-eng":0.4045807121,"data-quality":0.1625682947,"ml-security":0.1374609082}}
{"text":"Limited labeled data makes it hard to train models from scratch in medical domain, and an important paradigm is pre-training and then fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.107268972,"dev-research":0.2830044474,"prompt-eng":0.4243131489,"data-quality":0.1900722125,"ml-security":0.1792257037}}
{"text":"Large pre-trained models contain rich representations, which can be adapted to downstream medical tasks.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.0407913425,"dev-research":0.2796632902,"prompt-eng":0.4458256114,"data-quality":0.0617785008,"ml-security":0.180260841}}
{"text":"However, existing methods either tune all the parameters or the task-specific layers of the pre-trained models, ignoring the input variations of medical images, and thus they are not efficient or effective.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.0100020089,"dev-research":0.2390768615,"prompt-eng":0.4297319831,"data-quality":0.1019034721,"ml-security":0.1499631892}}
{"text":"In this work, we aim to study parameter-efficient fine-tuning (PEFT) for medical image analysis, and propose a dynamic visual prompt tuning method, named DVPT.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.0607629322,"dev-research":0.2900743423,"prompt-eng":0.4913505069,"data-quality":0.124459718,"ml-security":0.0643144742}}
{"text":"It can extract knowledge beneficial to downstream tasks from large models with a few trainable parameters.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.0316020597,"dev-research":0.2949681945,"prompt-eng":0.4376070869,"data-quality":0.062402908,"ml-security":0.1271292191}}
{"text":"Firstly, the frozen features are transformed by an lightweight bottleneck layer to learn the domain-specific distribution of downstream medical tasks, and then a few learnable visual prompts are used as dynamic queries and then conduct cross-attention with the transformed features, attempting to acquire sample-specific knowledge that are suitable for each sample.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.1079430205,"dev-research":0.2986577338,"prompt-eng":0.4453775156,"data-quality":0.0979566056,"ml-security":0.1442300029}}
{"text":"Finally, the features are projected to original feature dimension and aggregated with the frozen features.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.1783677695,"dev-research":0.3023760391,"prompt-eng":0.3966565875,"data-quality":0.1435218235,"ml-security":0.0766062812}}
{"text":"This DVPT module can be shared between different Transformer layers, further reducing the trainable parameters.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.0271112598,"dev-research":0.2333262124,"prompt-eng":0.4288002715,"data-quality":0.0855124964,"ml-security":0.092821717}}
{"text":"To validate DVPT, we conduct extensive experiments with different pre-trained models on medical classification and segmentation tasks.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.0702465783,"dev-research":0.248761413,"prompt-eng":0.4676511954,"data-quality":0.179792314,"ml-security":0.1598755366}}
{"text":"We find such PEFT method can not only efficiently adapt the pre-trained models to the medical domain, but also brings data efficiency with partial labeled data.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.0477592624,"dev-research":0.2088760509,"prompt-eng":0.4317504804,"data-quality":0.1441085654,"ml-security":0.1255136773}}
{"text":"For example, with 0.5\\% extra trainable parameters, our method not only outperforms state-of-the-art PEFT methods, even surpasses the full fine-tuning by more than 2.20\\% Kappa score on medical classification task.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.0088710551,"dev-research":0.2478242023,"prompt-eng":0.3961317153,"data-quality":0.1453610371,"ml-security":0.1140814685}}
{"text":"It can saves up to 60\\% labeled data and 99\\% storage cost of ViT-B/16.","meta":{"url":"http://arxiv.org/abs/2307.09787v1"},"cats":{"new-dataset":0.1626264561,"dev-research":0.335526932,"prompt-eng":0.3516824651,"data-quality":0.1299466157,"ml-security":0.095299913}}
{"text":"In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge.","meta":{"url":"http://arxiv.org/abs/2307.09782v1"},"cats":{"new-dataset":0.0603915783,"dev-research":0.2173686987,"prompt-eng":0.4627263756,"data-quality":0.217569444,"ml-security":0.0777082008}}
{"text":"Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution.","meta":{"url":"http://arxiv.org/abs/2307.09782v1"},"cats":{"new-dataset":0.0358114134,"dev-research":0.2843705424,"prompt-eng":0.352996385,"data-quality":0.1925867898,"ml-security":0.1471184768}}
{"text":"Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion.","meta":{"url":"http://arxiv.org/abs/2307.09782v1"},"cats":{"new-dataset":0.0115201012,"dev-research":0.1995324584,"prompt-eng":0.4897059277,"data-quality":0.121658044,"ml-security":0.1493454245}}
{"text":"For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100.","meta":{"url":"http://arxiv.org/abs/2307.09782v1"},"cats":{"new-dataset":0.0280719432,"dev-research":0.2824831649,"prompt-eng":0.4275859852,"data-quality":0.1130072383,"ml-security":0.0559758977}}
{"text":"To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints for weight quantization that negligibly impact the performance compared to the standard W4A8 model.","meta":{"url":"http://arxiv.org/abs/2307.09782v1"},"cats":{"new-dataset":0.0124421913,"dev-research":0.2694388848,"prompt-eng":0.3752690466,"data-quality":0.1602635231,"ml-security":0.0973467136}}
{"text":"We additionally enhance our quantization methods by integrating the Low Rank Compensation (LoRC) strategy, yielding improvements especially in smaller models.","meta":{"url":"http://arxiv.org/abs/2307.09782v1"},"cats":{"new-dataset":0.0221338681,"dev-research":0.1998793066,"prompt-eng":0.4182608632,"data-quality":0.189217815,"ml-security":0.1540722819}}
{"text":"The results of our investigation emphasize the immense potential of FP quantization for LLMs, paving the way for high-efficiency deployment in resource-limited settings.","meta":{"url":"http://arxiv.org/abs/2307.09782v1"},"cats":{"new-dataset":0.0229714168,"dev-research":0.2028701974,"prompt-eng":0.4818029533,"data-quality":0.0911392162,"ml-security":0.0924843051}}
{"text":"Layer compositing is one of the most popular image editing workflows among both amateurs and professionals.","meta":{"url":"http://arxiv.org/abs/2307.09781v1"},"cats":{"new-dataset":0.03739125,"dev-research":0.3304418663,"prompt-eng":0.4247182847,"data-quality":0.0809346561,"ml-security":0.07813491}}
{"text":"Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective.","meta":{"url":"http://arxiv.org/abs/2307.09781v1"},"cats":{"new-dataset":0.053255012,"dev-research":0.2698456709,"prompt-eng":0.4491160154,"data-quality":0.0816692837,"ml-security":0.0971139845}}
{"text":"Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously.","meta":{"url":"http://arxiv.org/abs/2307.09781v1"},"cats":{"new-dataset":0.1102875588,"dev-research":0.2633050903,"prompt-eng":0.4334319524,"data-quality":0.1000581455,"ml-security":0.1025934784}}
{"text":"To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation.","meta":{"url":"http://arxiv.org/abs/2307.09781v1"},"cats":{"new-dataset":0.093239108,"dev-research":0.2705992954,"prompt-eng":0.4833765637,"data-quality":0.1325371363,"ml-security":0.1083517156}}
{"text":"One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output.","meta":{"url":"http://arxiv.org/abs/2307.09781v1"},"cats":{"new-dataset":0.0476454154,"dev-research":0.3535747081,"prompt-eng":0.42762149,"data-quality":0.1347562941,"ml-security":0.0593536104}}
{"text":"Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation.","meta":{"url":"http://arxiv.org/abs/2307.09781v1"},"cats":{"new-dataset":0.010998698,"dev-research":0.3478777639,"prompt-eng":0.3652914753,"data-quality":0.0817544911,"ml-security":0.1062606586}}
{"text":"Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.","meta":{"url":"http://arxiv.org/abs/2307.09781v1"},"cats":{"new-dataset":0.0957483193,"dev-research":0.2576685469,"prompt-eng":0.4186849235,"data-quality":0.0892731148,"ml-security":0.0553080297}}
{"text":"Which set of features was responsible for a certain output of a machine learning model?","meta":{"url":"http://arxiv.org/abs/2307.09779v1"},"cats":{"new-dataset":0.0816776014,"dev-research":0.3636848575,"prompt-eng":0.4082522198,"data-quality":0.2330227956,"ml-security":0.2864835289}}
{"text":"Which components caused the failure of a cloud computing application?","meta":{"url":"http://arxiv.org/abs/2307.09779v1"},"cats":{"new-dataset":0.1442324999,"dev-research":0.3119749348,"prompt-eng":0.3477185229,"data-quality":0.2115785803,"ml-security":0.1541711285}}
{"text":"These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM).","meta":{"url":"http://arxiv.org/abs/2307.09779v1"},"cats":{"new-dataset":0.1092044295,"dev-research":0.2644859552,"prompt-eng":0.4309612598,"data-quality":0.1732372281,"ml-security":0.2415782479}}
{"text":"Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable.","meta":{"url":"http://arxiv.org/abs/2307.09779v1"},"cats":{"new-dataset":0.0307353438,"dev-research":0.2255993437,"prompt-eng":0.4666125309,"data-quality":0.1346998227,"ml-security":0.2123684599}}
{"text":"This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance.","meta":{"url":"http://arxiv.org/abs/2307.09779v1"},"cats":{"new-dataset":0.0102400252,"dev-research":0.3820120188,"prompt-eng":0.3986034806,"data-quality":0.0925802616,"ml-security":0.1115508722}}
{"text":"In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.","meta":{"url":"http://arxiv.org/abs/2307.09779v1"},"cats":{"new-dataset":0.2694328416,"dev-research":0.3881956185,"prompt-eng":0.3833859451,"data-quality":0.1945205881,"ml-security":0.2249363037}}
{"text":"Procedurally generating cities in Minecraft provides players more diverse scenarios and could help understand and improve the design of cities in other digital worlds and the real world.","meta":{"url":"http://arxiv.org/abs/2307.09777v1"},"cats":{"new-dataset":0.0877951426,"dev-research":0.3982526183,"prompt-eng":0.4400406504,"data-quality":0.0858184466,"ml-security":0.0788319141}}
{"text":"This paper presents a city generator that was submitted as an entry to the 2023 Edition of Minecraft Settlement Generation Competition for Minecraft.","meta":{"url":"http://arxiv.org/abs/2307.09777v1"},"cats":{"new-dataset":0.4825282414,"dev-research":0.3127638453,"prompt-eng":0.4329052731,"data-quality":0.0761837246,"ml-security":0.0572017202}}
{"text":"The generation procedure is composed of six main steps, namely vegetation clearing, terrain reshaping, building layout generation, route planning, streetlight placement, and wall construction.","meta":{"url":"http://arxiv.org/abs/2307.09777v1"},"cats":{"new-dataset":0.1527402996,"dev-research":0.3338667625,"prompt-eng":0.4501074624,"data-quality":0.0449075246,"ml-security":0.0281785565}}
{"text":"Three algorithms, including a heuristic-based algorithm, an evolving layout algorithm, and a random one are applied to generate the building layout, thus determining where to place different redstone style buildings, and tested by generating cities on random maps in limited time.","meta":{"url":"http://arxiv.org/abs/2307.09777v1"},"cats":{"new-dataset":0.1169396045,"dev-research":0.318508713,"prompt-eng":0.4225252465,"data-quality":0.0809980594,"ml-security":0.0763606754}}
{"text":"Experimental results show that the heuristic-based algorithm is capable of finding an acceptable building layout faster for flat maps, while the evolving layout algorithm performs better in evolving layout for rugged maps.","meta":{"url":"http://arxiv.org/abs/2307.09777v1"},"cats":{"new-dataset":0.0878533524,"dev-research":0.3770442394,"prompt-eng":0.4000021881,"data-quality":0.0570522558,"ml-security":0.0613589091}}
{"text":"A user study is conducted to compare our generator with outstanding entries of the competition's 2022 edition using the competition's evaluation criteria and shows that our generator performs well in the adaptation and functionality criteria","meta":{"url":"http://arxiv.org/abs/2307.09777v1"},"cats":{"new-dataset":0.2000055954,"dev-research":0.3629170102,"prompt-eng":0.4906410786,"data-quality":0.1202550831,"ml-security":0.0504117653}}
{"text":"This paper deals with the problem of automatically and correctly controlling infinite-state reactive programs to achieve LTL goals.","meta":{"url":"http://arxiv.org/abs/2307.09776v1"},"cats":{"new-dataset":0.0387247584,"dev-research":0.2444483459,"prompt-eng":0.451600034,"data-quality":0.0750478703,"ml-security":0.158322543}}
{"text":"Applications include adapting a program to new requirements, or to repair bugs discovered in the original specification or program code.","meta":{"url":"http://arxiv.org/abs/2307.09776v1"},"cats":{"new-dataset":0.0735447775,"dev-research":0.5356455365,"prompt-eng":0.4176055019,"data-quality":0.2572583645,"ml-security":0.1429420974}}
{"text":"Existing approaches are able to solve this problem for safety and some reachability properties, but require an a priori template of the solution for more general properties.","meta":{"url":"http://arxiv.org/abs/2307.09776v1"},"cats":{"new-dataset":0.1219330555,"dev-research":0.2378922657,"prompt-eng":0.4184386456,"data-quality":0.1035089197,"ml-security":0.1503465171}}
{"text":"Fully automated approaches for full LTL exist, reducing the problem into successive finite LTL reactive synthesis problems in an abstraction-refinement loop.","meta":{"url":"http://arxiv.org/abs/2307.09776v1"},"cats":{"new-dataset":0.0753614967,"dev-research":0.2957618214,"prompt-eng":0.4139722529,"data-quality":0.0753612305,"ml-security":0.0798886131}}
{"text":"However, they do not terminate when the number of steps to be completed depends on unbounded variables.","meta":{"url":"http://arxiv.org/abs/2307.09776v1"},"cats":{"new-dataset":0.0205986629,"dev-research":0.2522781754,"prompt-eng":0.4129102917,"data-quality":0.0813129486,"ml-security":0.0604952449}}
{"text":"Our main insight is that safety abstractions of the program are not enough -- fairness properties are also essential to be able to decide many interesting problems, something missed by existing automated approaches.","meta":{"url":"http://arxiv.org/abs/2307.09776v1"},"cats":{"new-dataset":0.0244350314,"dev-research":0.3821032047,"prompt-eng":0.3983179564,"data-quality":0.1442146288,"ml-security":0.4401621735}}
{"text":"We thus go beyond the state-of-the-art to allow for automated reactive program control for full LTL, with automated discovery of the knowledge, including fairness, of the program needed to determine realisability.","meta":{"url":"http://arxiv.org/abs/2307.09776v1"},"cats":{"new-dataset":0.1084449959,"dev-research":0.3441063191,"prompt-eng":0.4443820572,"data-quality":0.1171858874,"ml-security":0.213627958}}
{"text":"We further implement the approach in a tool, with an associated DSL for reactive programs, and illustrate the approach through several case studies.","meta":{"url":"http://arxiv.org/abs/2307.09776v1"},"cats":{"new-dataset":0.0946498542,"dev-research":0.3855862002,"prompt-eng":0.4525655122,"data-quality":0.079758587,"ml-security":0.1119324205}}
{"text":"In the field of music information retrieval (MIR), cover song identification (CSI) is a challenging task that aims to identify cover versions of a query song from a massive collection.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.2813768163,"dev-research":0.2438121473,"prompt-eng":0.4084852837,"data-quality":0.3313720163,"ml-security":0.1088340314}}
{"text":"Existing works still suffer from high intra-song variances and inter-song correlations, due to the entangled nature of version-specific and version-invariant factors in their modeling.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.0371062565,"dev-research":0.2818194099,"prompt-eng":0.3855948168,"data-quality":0.2231078272,"ml-security":0.0677807286}}
{"text":"In this work, we set the goal of disentangling version-specific and version-invariant factors, which could make it easier for the model to learn invariant music representations for unseen query songs.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.0651361702,"dev-research":0.2452774182,"prompt-eng":0.4076726095,"data-quality":0.2117946748,"ml-security":0.1127064489}}
{"text":"We analyze the CSI task in a disentanglement view with the causal graph technique, and identify the intra-version and inter-version effects biasing the invariant learning.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.0282248894,"dev-research":0.3240075015,"prompt-eng":0.4080082382,"data-quality":0.1981628435,"ml-security":0.2402221821}}
{"text":"To block these effects, we propose the disentangled music representation learning framework (DisCover) for CSI.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.159620941,"dev-research":0.2716348681,"prompt-eng":0.3690501312,"data-quality":0.2333708402,"ml-security":0.1738856032}}
{"text":"DisCover consists of two critical components: (1) Knowledge-guided Disentanglement Module (KDM) and (2) Gradient-based Adversarial Disentanglement Module (GADM), which block intra-version and inter-version biased effects, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.0389950851,"dev-research":0.2973282433,"prompt-eng":0.3628180222,"data-quality":0.133687915,"ml-security":0.2804634478}}
{"text":"KDM minimizes the mutual information between the learned representations and version-variant factors that are identified with prior domain knowledge.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.0355269644,"dev-research":0.3283763639,"prompt-eng":0.4381905287,"data-quality":0.1679093153,"ml-security":0.1420524836}}
{"text":"GADM identifies version-variant factors by simulating the representation transitions between intra-song versions, and exploits adversarial distillation for effect blocking.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.0433263457,"dev-research":0.3017268334,"prompt-eng":0.4124575046,"data-quality":0.229341228,"ml-security":0.2072147954}}
{"text":"Extensive comparisons with best-performing methods and in-depth analysis demonstrate the effectiveness of DisCover and the and necessity of disentanglement for CSI.","meta":{"url":"http://arxiv.org/abs/2307.09775v1"},"cats":{"new-dataset":0.0603971288,"dev-research":0.3852105531,"prompt-eng":0.371803051,"data-quality":0.1266795342,"ml-security":0.2003749855}}
{"text":"Unsupervised domain adaptation (UDA) has increasingly gained interests for its capacity to transfer the knowledge learned from a labeled source domain to an unlabeled target domain.","meta":{"url":"http://arxiv.org/abs/2307.09769v1"},"cats":{"new-dataset":0.0854495132,"dev-research":0.3091307756,"prompt-eng":0.4283672001,"data-quality":0.2328919885,"ml-security":0.1556089536}}
{"text":"However, typical UDA methods require concurrent access to both the source and target domain data, which largely limits its application in medical scenarios where source data is often unavailable due to privacy concern.","meta":{"url":"http://arxiv.org/abs/2307.09769v1"},"cats":{"new-dataset":0.0641052125,"dev-research":0.3066548197,"prompt-eng":0.2931440641,"data-quality":0.0855271148,"ml-security":0.3125591472}}
{"text":"To tackle the source data-absent problem, we present a novel two-stage source-free domain adaptation (SFDA) framework for medical image segmentation, where only a well-trained source segmentation model and unlabeled target data are available during domain adaptation.","meta":{"url":"http://arxiv.org/abs/2307.09769v1"},"cats":{"new-dataset":0.3381285397,"dev-research":0.2420181437,"prompt-eng":0.3694001607,"data-quality":0.2326150292,"ml-security":0.1551182498}}
{"text":"Specifically, in the prototype-anchored feature alignment stage, we first utilize the weights of the pre-trained pixel-wise classifier as source prototypes, which preserve the information of source features.","meta":{"url":"http://arxiv.org/abs/2307.09769v1"},"cats":{"new-dataset":0.0893907543,"dev-research":0.3897707939,"prompt-eng":0.4754580608,"data-quality":0.2112718065,"ml-security":0.1333822728}}
{"text":"Then, we introduce the bi-directional transport to align the target features with class prototypes by minimizing its expected cost.","meta":{"url":"http://arxiv.org/abs/2307.09769v1"},"cats":{"new-dataset":0.0283131902,"dev-research":0.2945428201,"prompt-eng":0.4657300104,"data-quality":0.1082610696,"ml-security":0.1134215831}}
{"text":"On top of that, a contrastive learning stage is further devised to utilize those pixels with unreliable predictions for a more compact target feature distribution.","meta":{"url":"http://arxiv.org/abs/2307.09769v1"},"cats":{"new-dataset":0.0654618967,"dev-research":0.2411994341,"prompt-eng":0.4395510906,"data-quality":0.2242276303,"ml-security":0.1785451821}}
{"text":"Extensive experiments on a cross-modality medical segmentation task demonstrate the superiority of our method in large domain discrepancy settings compared with the state-of-the-art SFDA approaches and even some UDA methods.","meta":{"url":"http://arxiv.org/abs/2307.09769v1"},"cats":{"new-dataset":0.1079486606,"dev-research":0.2478923949,"prompt-eng":0.3858439056,"data-quality":0.1581881411,"ml-security":0.0512809838}}
{"text":"Code is available at https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.","meta":{"url":"http://arxiv.org/abs/2307.09769v1"},"cats":{"new-dataset":0.5324953445,"dev-research":0.2676332573,"prompt-eng":0.4214534798,"data-quality":0.1223850253,"ml-security":0.0405702392}}
{"text":"Graph neural network (GNN) has been demonstrated powerful in modeling graph-structured data.","meta":{"url":"http://arxiv.org/abs/2307.09768v1"},"cats":{"new-dataset":0.2546445589,"dev-research":0.2462044373,"prompt-eng":0.3264386687,"data-quality":0.1901012973,"ml-security":0.1195964389}}
{"text":"However, despite many successful cases of applying GNNs to various graph classification and prediction tasks, whether the graph geometrical information has been fully exploited to enhance the learning performance of GNNs is not yet well understood.","meta":{"url":"http://arxiv.org/abs/2307.09768v1"},"cats":{"new-dataset":0.0372132652,"dev-research":0.2686479022,"prompt-eng":0.3322674077,"data-quality":0.1769573455,"ml-security":0.1765530212}}
{"text":"This paper introduces a new approach to enhance GNN by discrete graph Ricci curvature.","meta":{"url":"http://arxiv.org/abs/2307.09768v1"},"cats":{"new-dataset":0.0949392309,"dev-research":0.2098904917,"prompt-eng":0.3310214517,"data-quality":0.1127639898,"ml-security":0.0891003393}}
{"text":"Specifically, the graph Ricci curvature defined on the edges of a graph measures how difficult the information transits on one edge from one node to another based on their neighborhoods.","meta":{"url":"http://arxiv.org/abs/2307.09768v1"},"cats":{"new-dataset":0.0261551849,"dev-research":0.2574087584,"prompt-eng":0.3978885042,"data-quality":0.1557764353,"ml-security":0.0879428988}}
{"text":"Motivated by the geometric analogy of Ricci curvature in the graph setting, we prove that by inserting the curvature information with different carefully designed transformation function $\\zeta$, several known computational issues in GNN such as over-smoothing can be alleviated in our proposed model.","meta":{"url":"http://arxiv.org/abs/2307.09768v1"},"cats":{"new-dataset":0.0401903652,"dev-research":0.2135288078,"prompt-eng":0.339785606,"data-quality":0.1353196295,"ml-security":0.1505503918}}
{"text":"Furthermore, we verified that edges with very positive Ricci curvature (i.e., $\\kappa_{i,j} \\approx 1$) are preferred to be dropped to enhance model's adaption to heterophily graph and one curvature based graph edge drop algorithm is proposed.","meta":{"url":"http://arxiv.org/abs/2307.09768v1"},"cats":{"new-dataset":0.0080439473,"dev-research":0.2072444461,"prompt-eng":0.346587181,"data-quality":0.0958400686,"ml-security":0.0690057375}}
{"text":"Comprehensive experiments show that our curvature-based GNN model outperforms the state-of-the-art baselines in both homophily and heterophily graph datasets, indicating the effectiveness of involving graph geometric information in GNNs.","meta":{"url":"http://arxiv.org/abs/2307.09768v1"},"cats":{"new-dataset":0.0916697182,"dev-research":0.2616431544,"prompt-eng":0.3279779663,"data-quality":0.1498305198,"ml-security":0.1076809347}}
{"text":"We propose a novel generative model for multivariate discrete-time time series data.","meta":{"url":"http://arxiv.org/abs/2307.09767v1"},"cats":{"new-dataset":0.2489730009,"dev-research":0.1854632907,"prompt-eng":0.4098368217,"data-quality":0.0675269038,"ml-security":0.0701574778}}
{"text":"Drawing inspiration from the construction of neural spline flows, our algorithm incorporates linear transformations and the signature transform as a seamless substitution for traditional neural networks.","meta":{"url":"http://arxiv.org/abs/2307.09767v1"},"cats":{"new-dataset":0.0839835269,"dev-research":0.2455255772,"prompt-eng":0.3441309932,"data-quality":0.0889139235,"ml-security":0.1263640643}}
{"text":"This approach enables us to achieve not only the universality property inherent in neural networks but also introduces convexity in the model's parameters.","meta":{"url":"http://arxiv.org/abs/2307.09767v1"},"cats":{"new-dataset":0.0064654537,"dev-research":0.1999945338,"prompt-eng":0.3853905463,"data-quality":0.1138553078,"ml-security":0.3230888427}}
{"text":"Stack Overflow, the world's largest software Q&A (SQA) website, is facing a significant traffic drop due to the emergence of generative AI techniques.","meta":{"url":"http://arxiv.org/abs/2307.09765v1"},"cats":{"new-dataset":0.0597795953,"dev-research":0.377988287,"prompt-eng":0.4092473897,"data-quality":0.0738258732,"ml-security":0.1586574568}}
{"text":"ChatGPT is banned by Stack Overflow after only 6 days from its release.","meta":{"url":"http://arxiv.org/abs/2307.09765v1"},"cats":{"new-dataset":0.3146667032,"dev-research":0.3042828722,"prompt-eng":0.3141306857,"data-quality":0.1262154011,"ml-security":0.1475934869}}
{"text":"The main reason provided by the official Stack Overflow is that the answers generated by ChatGPT are of low quality.","meta":{"url":"http://arxiv.org/abs/2307.09765v1"},"cats":{"new-dataset":0.1317388275,"dev-research":0.4000870462,"prompt-eng":0.3597285267,"data-quality":0.1485970908,"ml-security":0.0774264651}}
{"text":"To verify this, we conduct a comparative evaluation of human-written and ChatGPT-generated answers.","meta":{"url":"http://arxiv.org/abs/2307.09765v1"},"cats":{"new-dataset":0.1424885947,"dev-research":0.4157081083,"prompt-eng":0.465150221,"data-quality":0.1308174722,"ml-security":0.0421896839}}
{"text":"Our methodology employs both automatic comparison and a manual study.","meta":{"url":"http://arxiv.org/abs/2307.09765v1"},"cats":{"new-dataset":0.0366516312,"dev-research":0.4508288868,"prompt-eng":0.4762068535,"data-quality":0.153300506,"ml-security":0.0293816217}}
{"text":"Our results suggest that human-written and ChatGPT-generated answers are semantically similar, however, human-written answers outperform ChatGPT-generated ones consistently across multiple aspects, specifically by 10% on the overall score.","meta":{"url":"http://arxiv.org/abs/2307.09765v1"},"cats":{"new-dataset":0.1295675375,"dev-research":0.4183219171,"prompt-eng":0.4438857311,"data-quality":0.1531544585,"ml-security":0.0478358768}}
{"text":"We release the data, analysis scripts, and detailed results at https://anonymous.4open.science/r/GAI4SQA-FD5C.","meta":{"url":"http://arxiv.org/abs/2307.09765v1"},"cats":{"new-dataset":0.5065714542,"dev-research":0.2907046557,"prompt-eng":0.3719424243,"data-quality":0.119464159,"ml-security":0.1123927515}}
{"text":"The vulnerability of deep neural networks to adversarial samples has been a major impediment to their broad applications, despite their success in various fields.","meta":{"url":"http://arxiv.org/abs/2307.09763v1"},"cats":{"new-dataset":0.0939455556,"dev-research":0.2753339375,"prompt-eng":0.3684537431,"data-quality":0.3449387515,"ml-security":0.8763971465}}
{"text":"Recently, some works suggested that adversarially-trained models emphasize the importance of low-frequency information to achieve higher robustness.","meta":{"url":"http://arxiv.org/abs/2307.09763v1"},"cats":{"new-dataset":0.0293100946,"dev-research":0.2664102093,"prompt-eng":0.4218315129,"data-quality":0.3597111816,"ml-security":0.7586845452}}
{"text":"While several attempts have been made to leverage this frequency characteristic, they have all faced the issue that applying low-pass filters directly to input images leads to irreversible loss of discriminative information and poor generalizability to datasets with distinct frequency features.","meta":{"url":"http://arxiv.org/abs/2307.09763v1"},"cats":{"new-dataset":0.0983643764,"dev-research":0.2391906713,"prompt-eng":0.3810604908,"data-quality":0.2841627058,"ml-security":0.2541399816}}
{"text":"This paper presents a plug-and-play module called the Frequency Preference Control Module that adaptively reconfigures the low- and high-frequency components of intermediate feature representations, providing better utilization of frequency in robust learning.","meta":{"url":"http://arxiv.org/abs/2307.09763v1"},"cats":{"new-dataset":0.1023763907,"dev-research":0.3284152393,"prompt-eng":0.4409821312,"data-quality":0.2162600291,"ml-security":0.1491943091}}
{"text":"Empirical studies show that our proposed module can be easily incorporated into any adversarial training framework, further improving model robustness across different architectures and datasets.","meta":{"url":"http://arxiv.org/abs/2307.09763v1"},"cats":{"new-dataset":0.1225547923,"dev-research":0.2602925974,"prompt-eng":0.3779648194,"data-quality":0.3258101637,"ml-security":0.7661874584}}
{"text":"Additionally, experiments were conducted to examine how the frequency bias of robust models impacts the adversarial training process and its final robustness, revealing interesting insights.","meta":{"url":"http://arxiv.org/abs/2307.09763v1"},"cats":{"new-dataset":0.021463673,"dev-research":0.2803830181,"prompt-eng":0.4091751409,"data-quality":0.394115906,"ml-security":0.7202995957}}
{"text":"Complex networks are used to model many real-world systems.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.0446646476,"dev-research":0.3108953884,"prompt-eng":0.3620589019,"data-quality":0.0644247088,"ml-security":0.1447607718}}
{"text":"However, the dimensionality of these systems can make them challenging to analyze.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.0293691838,"dev-research":0.3221093551,"prompt-eng":0.4040641061,"data-quality":0.0990608222,"ml-security":0.1735356699}}
{"text":"Dimensionality reduction techniques like POD can be used in such cases.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.0924189541,"dev-research":0.2465352083,"prompt-eng":0.4061188778,"data-quality":0.1354832308,"ml-security":0.0866747958}}
{"text":"However, these models are susceptible to perturbations in the input data.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.018254645,"dev-research":0.2227284317,"prompt-eng":0.4164427187,"data-quality":0.2308097004,"ml-security":0.3263520066}}
{"text":"We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.0445931117,"dev-research":0.2314697699,"prompt-eng":0.4484412202,"data-quality":0.2410032045,"ml-security":0.1773597531}}
{"text":"The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.019296894,"dev-research":0.2147989012,"prompt-eng":0.4170721248,"data-quality":0.18714443,"ml-security":0.2449407731}}
{"text":"Deep Neural Networks (DNNs) are susceptible to adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.0294288811,"dev-research":0.3859781776,"prompt-eng":0.4087570379,"data-quality":0.2573971302,"ml-security":0.8553882332}}
{"text":"However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.0198278135,"dev-research":0.2801130887,"prompt-eng":0.375567779,"data-quality":0.1687791208,"ml-security":0.529467717}}
{"text":"We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.","meta":{"url":"http://arxiv.org/abs/2307.09762v1"},"cats":{"new-dataset":0.0880887018,"dev-research":0.2374019118,"prompt-eng":0.371477419,"data-quality":0.1523903421,"ml-security":0.2137764402}}
{"text":"Given a graph $G = (V, E)$, a non-empty set $S \\subseteq V$ is a defensive alliance, if for every vertex $v \\in S$, the majority of its closed neighbours are in $S$, that is, $|N_G[v] \\cap S| \\geq |N_G[v]","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.0752679566,"dev-research":0.2828483496,"prompt-eng":0.3617020561,"data-quality":0.1351547184,"ml-security":0.2321799916}}
{"text":"\\setminus S|$.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.2006903927,"dev-research":0.2555771791,"prompt-eng":0.4027102105,"data-quality":0.1731240932,"ml-security":0.1122133438}}
{"text":"The decision version of the problem is known to be NP-Complete even when restricted to split and bipartite graphs.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.0439593926,"dev-research":0.2100143928,"prompt-eng":0.3046712222,"data-quality":0.1386285524,"ml-security":0.0976774211}}
{"text":"The problem is \\textit{fixed-parameter tractable} for the parameters solution size, vertex cover number and neighbourhood diversity.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.095585473,"dev-research":0.206673424,"prompt-eng":0.3679082582,"data-quality":0.1679973028,"ml-security":0.0931889895}}
{"text":"For the parameters treewidth and feedback vertex set number, the problem is W[1]-hard. \\\\ \\hspace*{2em} In this paper, we study the defensive alliance problem for graphs with bounded degree.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.1009655978,"dev-research":0.2408611807,"prompt-eng":0.3713587564,"data-quality":0.143818371,"ml-security":0.2500685274}}
{"text":"We show that the problem is \\textit{polynomial-time solvable} on graphs with maximum degree at most 5 and NP-Complete on graphs with maximum degree 6.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.2150202979,"dev-research":0.2524697499,"prompt-eng":0.3410768942,"data-quality":0.1471235476,"ml-security":0.0853670195}}
{"text":"This rules out the fixed-parameter tractability of the problem for the parameter maximum degree of the graph.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.0571346582,"dev-research":0.2237154306,"prompt-eng":0.3517603121,"data-quality":0.139746307,"ml-security":0.0994331128}}
{"text":"We also consider the problem from the standpoint of parameterized complexity.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.0543413692,"dev-research":0.2111252737,"prompt-eng":0.3586485838,"data-quality":0.1304398069,"ml-security":0.1519380536}}
{"text":"We provide an FPT algorithm using the Integer Linear Programming approach for the parameter distance to clique.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.1280649241,"dev-research":0.2031728142,"prompt-eng":0.3656215269,"data-quality":0.0942882848,"ml-security":0.0759866349}}
{"text":"We also answer an open question posed in \\cite{AG2} by providing an FPT algorithm for the parameter twin cover.","meta":{"url":"http://arxiv.org/abs/2307.09760v1"},"cats":{"new-dataset":0.0786222132,"dev-research":0.1694451657,"prompt-eng":0.397628505,"data-quality":0.1279130021,"ml-security":0.0750115439}}
{"text":"The phenomena of Spectral Bias, where the higher frequency components of a function being learnt in a feedforward Artificial Neural Network (ANN) are seen to converge more slowly than the lower frequencies, is observed ubiquitously across ANNs.","meta":{"url":"http://arxiv.org/abs/2307.09759v1"},"cats":{"new-dataset":0.0126079989,"dev-research":0.2468849366,"prompt-eng":0.376821594,"data-quality":0.1991006351,"ml-security":0.2099599437}}
{"text":"This has created technology challenges in fields where resolution of higher frequencies is crucial, like in Physics Informed Neural Networks (PINNs).","meta":{"url":"http://arxiv.org/abs/2307.09759v1"},"cats":{"new-dataset":0.0563969292,"dev-research":0.2433919467,"prompt-eng":0.3924949124,"data-quality":0.102952098,"ml-security":0.0843668524}}
{"text":"Extreme Learning Machines (ELMs) that obviate an iterative solution process which provides the theoretical basis of Spectral Bias (SB), should in principle be free of the same.","meta":{"url":"http://arxiv.org/abs/2307.09759v1"},"cats":{"new-dataset":0.0127931869,"dev-research":0.2148209112,"prompt-eng":0.4053863569,"data-quality":0.1180269557,"ml-security":0.3193267032}}
{"text":"This work verifies the reliability of this assumption, and shows that it is incorrect.","meta":{"url":"http://arxiv.org/abs/2307.09759v1"},"cats":{"new-dataset":0.0166062034,"dev-research":0.3208334809,"prompt-eng":0.3830671023,"data-quality":0.4166902479,"ml-security":0.0967910762}}
{"text":"However, the structure of ELMs makes them naturally amenable to implementation of variants of Fourier Feature Embeddings, which have been shown to mitigate SB in ANNs.","meta":{"url":"http://arxiv.org/abs/2307.09759v1"},"cats":{"new-dataset":0.0195115147,"dev-research":0.2692567309,"prompt-eng":0.430358279,"data-quality":0.1971544373,"ml-security":0.1674616423}}
{"text":"This approach is implemented and verified to completely eliminate SB, thus bringing into feasibility the application of ELMs for practical problems like PINNs where resolution of higher frequencies is essential.","meta":{"url":"http://arxiv.org/abs/2307.09759v1"},"cats":{"new-dataset":0.0207520821,"dev-research":0.2243332299,"prompt-eng":0.4177367946,"data-quality":0.1494905173,"ml-security":0.0855038101}}
{"text":"Chest X-Ray (CXR) report generation is a promising approach to improving the efficiency of CXR interpretation.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.0517961634,"dev-research":0.322026261,"prompt-eng":0.4196420308,"data-quality":0.1365579113,"ml-security":0.0487970941}}
{"text":"However, a significant increase in diagnostic accuracy is required before that can be realised.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.0123367054,"dev-research":0.3729960636,"prompt-eng":0.443464319,"data-quality":0.2374235966,"ml-security":0.1239390736}}
{"text":"Motivated by this, we propose a framework that is more inline with a radiologist's workflow by considering longitudinal data.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.0941001074,"dev-research":0.3144803292,"prompt-eng":0.4139175055,"data-quality":0.0957532997,"ml-security":0.0584039776}}
{"text":"Here, the decoder is additionally conditioned on the report from the subject's previous imaging study via a prompt.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.1120032189,"dev-research":0.2861919776,"prompt-eng":0.5265286192,"data-quality":0.1369070633,"ml-security":0.05618027}}
{"text":"We also propose a new reward for reinforcement learning based on CXR-BERT, which computes the similarity between reports.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.0762862961,"dev-research":0.3027702527,"prompt-eng":0.4538242223,"data-quality":0.1243964186,"ml-security":0.0875701552}}
{"text":"We conduct experiments on the MIMIC-CXR dataset.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.3079169688,"dev-research":0.2448636101,"prompt-eng":0.4422930194,"data-quality":0.1462559499,"ml-security":0.0932961199}}
{"text":"The results indicate that longitudinal data improves CXR report generation.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.0996067534,"dev-research":0.3725040068,"prompt-eng":0.4537412853,"data-quality":0.1684328076,"ml-security":0.0398636033}}
{"text":"CXR-BERT is also shown to be a promising alternative to the current state-of-the-art reward based on RadGraph.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.1163344601,"dev-research":0.2959030466,"prompt-eng":0.4349891817,"data-quality":0.1245182258,"ml-security":0.1017836359}}
{"text":"This investigation indicates that longitudinal CXR report generation can offer a substantial increase in diagnostic accuracy.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.0412212951,"dev-research":0.3411431549,"prompt-eng":0.4477101069,"data-quality":0.2150613081,"ml-security":0.0500142487}}
{"text":"Our Hugging Face model is available at: https://huggingface.co/aehrc/cxrmate and code is available at: https://github.com/aehrc/cxrmate.","meta":{"url":"http://arxiv.org/abs/2307.09758v1"},"cats":{"new-dataset":0.2750860278,"dev-research":0.2220137621,"prompt-eng":0.4202195888,"data-quality":0.0465668692,"ml-security":0.0807694782}}
{"text":"Weakly supervised object localization (WSOL) remains challenging when learning object localization models from image category labels.","meta":{"url":"http://arxiv.org/abs/2307.09756v1"},"cats":{"new-dataset":0.0746190011,"dev-research":0.2422237238,"prompt-eng":0.4714649326,"data-quality":0.4298980877,"ml-security":0.1271194792}}
{"text":"Conventional methods that discriminatively train activation models ignore representative yet less discriminative object parts.","meta":{"url":"http://arxiv.org/abs/2307.09756v1"},"cats":{"new-dataset":0.0409240663,"dev-research":0.2205551588,"prompt-eng":0.4318338708,"data-quality":0.3155574997,"ml-security":0.1975208483}}
{"text":"In this study, we propose a generative prompt model (GenPromp), defining the first generative pipeline to localize less discriminative object parts by formulating WSOL as a conditional image denoising procedure.","meta":{"url":"http://arxiv.org/abs/2307.09756v1"},"cats":{"new-dataset":0.0837998607,"dev-research":0.2343813438,"prompt-eng":0.5887270906,"data-quality":0.2287604764,"ml-security":0.086837849}}
{"text":"During training, GenPromp converts image category labels to learnable prompt embeddings which are fed to a generative model to conditionally recover the input image with noise and learn representative embeddings.","meta":{"url":"http://arxiv.org/abs/2307.09756v1"},"cats":{"new-dataset":0.1303033371,"dev-research":0.261964677,"prompt-eng":0.5559975891,"data-quality":0.409634748,"ml-security":0.1271585926}}
{"text":"During inference, enPromp combines the representative embeddings with discriminative embeddings (queried from an off-the-shelf vision-language model) for both representative and discriminative capacity.","meta":{"url":"http://arxiv.org/abs/2307.09756v1"},"cats":{"new-dataset":0.16761688,"dev-research":0.2564912953,"prompt-eng":0.4569554568,"data-quality":0.1601193505,"ml-security":0.0935130268}}
{"text":"The combined embeddings are finally used to generate multi-scale high-quality attention maps, which facilitate localizing full object extent.","meta":{"url":"http://arxiv.org/abs/2307.09756v1"},"cats":{"new-dataset":0.1033999166,"dev-research":0.2434910242,"prompt-eng":0.4628421252,"data-quality":0.1565976556,"ml-security":0.0606369778}}
{"text":"Experiments on CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline for WSOL with the generative model.","meta":{"url":"http://arxiv.org/abs/2307.09756v1"},"cats":{"new-dataset":0.1053026506,"dev-research":0.2336627753,"prompt-eng":0.5008518231,"data-quality":0.1457233359,"ml-security":0.0654702253}}
{"text":"Code is available at https://github.com/callsys/GenPromp.","meta":{"url":"http://arxiv.org/abs/2307.09756v1"},"cats":{"new-dataset":0.4190608178,"dev-research":0.3002138762,"prompt-eng":0.4508249408,"data-quality":0.0817784511,"ml-security":0.0563009915}}
{"text":"Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model with limited labeled images and a substantial volume of unlabeled images.","meta":{"url":"http://arxiv.org/abs/2307.09755v1"},"cats":{"new-dataset":0.1875215113,"dev-research":0.1928546992,"prompt-eng":0.4397114794,"data-quality":0.3694428785,"ml-security":0.0835060391}}
{"text":"To improve the robustness of representations, powerful methods introduce a pixel-wise contrastive learning approach in latent space (i.e., representation space) that aggregates the representations to their prototypes in a fully supervised manner.","meta":{"url":"http://arxiv.org/abs/2307.09755v1"},"cats":{"new-dataset":0.0644670038,"dev-research":0.2681030673,"prompt-eng":0.4363762445,"data-quality":0.2074496975,"ml-security":0.1437064956}}
{"text":"However, previous contrastive-based S4 methods merely rely on the supervision from the model's output (logits) in logit space during unlabeled training.","meta":{"url":"http://arxiv.org/abs/2307.09755v1"},"cats":{"new-dataset":0.0156167855,"dev-research":0.2186839757,"prompt-eng":0.4148980928,"data-quality":0.2334149349,"ml-security":0.1087725517}}
{"text":"In contrast, we utilize the outputs in both logit space and representation space to obtain supervision in a collaborative way.","meta":{"url":"http://arxiv.org/abs/2307.09755v1"},"cats":{"new-dataset":0.0634154013,"dev-research":0.4007806489,"prompt-eng":0.5045334993,"data-quality":0.1347410293,"ml-security":0.092601077}}
{"text":"The supervision from two spaces plays two roles: 1) reduces the risk of over-fitting to incorrect semantic information in logits with the help of representations; 2) enhances the knowledge exchange between the two spaces.","meta":{"url":"http://arxiv.org/abs/2307.09755v1"},"cats":{"new-dataset":0.0192323421,"dev-research":0.3682700273,"prompt-eng":0.4709740282,"data-quality":0.2262288131,"ml-security":0.1222491799}}
{"text":"Furthermore, unlike previous approaches, we use the similarity between representations and prototypes as a new indicator to tilt training those under-performing representations and achieve a more efficient contrastive learning process.","meta":{"url":"http://arxiv.org/abs/2307.09755v1"},"cats":{"new-dataset":0.0719616114,"dev-research":0.2861089959,"prompt-eng":0.4222976684,"data-quality":0.1038816927,"ml-security":0.0821816632}}
{"text":"Results on two public benchmarks demonstrate the competitive performance of our method compared with state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.09755v1"},"cats":{"new-dataset":0.0366902313,"dev-research":0.2686052341,"prompt-eng":0.3678545677,"data-quality":0.1152940585,"ml-security":0.0654715673}}
{"text":"We propose a novel method, ProNav, which uses proprioceptive signals for traversability estimation in challenging outdoor terrains for autonomous legged robot navigation.","meta":{"url":"http://arxiv.org/abs/2307.09754v1"},"cats":{"new-dataset":0.097522181,"dev-research":0.2691776553,"prompt-eng":0.4055446313,"data-quality":0.06772499,"ml-security":0.0494432223}}
{"text":"Our approach uses sensor data from a legged robot's joint encoders, force, and current sensors to measure the joint positions, forces, and current consumption respectively to accurately assess a terrain's stability, resistance to the robot's motion, risk of entrapment, and crash.","meta":{"url":"http://arxiv.org/abs/2307.09754v1"},"cats":{"new-dataset":0.2602471164,"dev-research":0.2899012107,"prompt-eng":0.386654799,"data-quality":0.0883155742,"ml-security":0.1269837336}}
{"text":"Based on these factors, we compute the appropriate robot trajectories and gait to maximize stability and minimize energy consumption.","meta":{"url":"http://arxiv.org/abs/2307.09754v1"},"cats":{"new-dataset":0.0732743648,"dev-research":0.2052954693,"prompt-eng":0.3646290873,"data-quality":0.0391044416,"ml-security":0.0513142611}}
{"text":"Our approach can also be used to predict imminent crashes in challenging terrains and execute behaviors to preemptively avoid them.","meta":{"url":"http://arxiv.org/abs/2307.09754v1"},"cats":{"new-dataset":0.2640168022,"dev-research":0.4207988968,"prompt-eng":0.4749706479,"data-quality":0.1070273527,"ml-security":0.4193559073}}
{"text":"We integrate ProNav with a method to navigate dense vegetation and demonstrate our method's benefits in real-world terrains with dense bushes, high granularity, negative obstacles, etc.","meta":{"url":"http://arxiv.org/abs/2307.09754v1"},"cats":{"new-dataset":0.106280403,"dev-research":0.3008185345,"prompt-eng":0.4004276972,"data-quality":0.0560333064,"ml-security":0.0496910338}}
{"text":"Our method shows an improvement up to 50% in terms of success rate and up to 35% in terms of energy efficiency.","meta":{"url":"http://arxiv.org/abs/2307.09754v1"},"cats":{"new-dataset":0.015836418,"dev-research":0.3002484768,"prompt-eng":0.4627233874,"data-quality":0.123433649,"ml-security":0.0313237448}}
{"text":"AI image models are rapidly evolving, disrupting aesthetic production in many industries.","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.028127944,"dev-research":0.2782250936,"prompt-eng":0.3943682684,"data-quality":0.1332712367,"ml-security":0.1443153865}}
{"text":"However, understanding of their underlying archives, their logic of image reproduction, and their persistent biases remains limited.","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.0883087943,"dev-research":0.3131795624,"prompt-eng":0.3782107233,"data-quality":0.2367634433,"ml-security":0.0911676218}}
{"text":"What kind of methods and approaches could open up these black boxes?","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.1011376269,"dev-research":0.2769150303,"prompt-eng":0.3792231876,"data-quality":0.1867314499,"ml-security":0.2670715582}}
{"text":"In this paper, we provide three methodological approaches for investigating AI image models and apply them to Stable Diffusion as a case study.","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.0403063218,"dev-research":0.2193403572,"prompt-eng":0.3854803775,"data-quality":0.1274071126,"ml-security":0.1522026958}}
{"text":"Unmaking the ecosystem analyzes the values, structures, and incentives surrounding the model's production.","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.0453382716,"dev-research":0.3495160925,"prompt-eng":0.3821008564,"data-quality":0.0961816105,"ml-security":0.0963482709}}
{"text":"Unmaking the data analyzes the images and text the model draws upon, with their attendant particularities and biases.","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.0968803742,"dev-research":0.3033968645,"prompt-eng":0.4205755982,"data-quality":0.2193031545,"ml-security":0.1229549393}}
{"text":"Unmaking the output analyzes the model's generative results, revealing its logics through prompting, reflection, and iteration.","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.0331305232,"dev-research":0.3722746389,"prompt-eng":0.5344706281,"data-quality":0.1646618453,"ml-security":0.1155728455}}
{"text":"Each mode of inquiry highlights particular ways in which the image model captures, \"understands,\" and recreates the world.","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.1435791547,"dev-research":0.2866158132,"prompt-eng":0.4972384287,"data-quality":0.1015092758,"ml-security":0.0382003317}}
{"text":"This accessible framework supports the work of critically investigating generative AI image models and paves the way for more socially and politically attuned analyses of their impacts in the world.","meta":{"url":"http://arxiv.org/abs/2307.09753v1"},"cats":{"new-dataset":0.161292202,"dev-research":0.2556712821,"prompt-eng":0.4185160632,"data-quality":0.1539954218,"ml-security":0.1276794432}}
{"text":"The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs.","meta":{"url":"http://arxiv.org/abs/2307.09751v1"},"cats":{"new-dataset":0.0617466183,"dev-research":0.3033541285,"prompt-eng":0.4221407207,"data-quality":0.1055048077,"ml-security":0.0582039298}}
{"text":"Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research.","meta":{"url":"http://arxiv.org/abs/2307.09751v1"},"cats":{"new-dataset":0.1273980822,"dev-research":0.2284579338,"prompt-eng":0.5386407726,"data-quality":0.1293101239,"ml-security":0.0788268354}}
{"text":"LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions.","meta":{"url":"http://arxiv.org/abs/2307.09751v1"},"cats":{"new-dataset":0.033765482,"dev-research":0.2878230045,"prompt-eng":0.5630494413,"data-quality":0.1136558035,"ml-security":0.0662970657}}
{"text":"More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking.","meta":{"url":"http://arxiv.org/abs/2307.09751v1"},"cats":{"new-dataset":0.0367146527,"dev-research":0.265428898,"prompt-eng":0.541998019,"data-quality":0.0681263988,"ml-security":0.1020886873}}
{"text":"IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services.","meta":{"url":"http://arxiv.org/abs/2307.09751v1"},"cats":{"new-dataset":0.043673705,"dev-research":0.2821929513,"prompt-eng":0.5142067151,"data-quality":0.1060287541,"ml-security":0.1158072787}}
{"text":"Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations.","meta":{"url":"http://arxiv.org/abs/2307.09751v1"},"cats":{"new-dataset":0.0604912495,"dev-research":0.3498265339,"prompt-eng":0.3489143478,"data-quality":0.1399561921,"ml-security":0.1785321456}}
{"text":"To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights.","meta":{"url":"http://arxiv.org/abs/2307.09751v1"},"cats":{"new-dataset":0.1034309066,"dev-research":0.2230990141,"prompt-eng":0.4971921721,"data-quality":0.0833337658,"ml-security":0.0742262834}}
{"text":"This paper provides a summary of the workshop's outcomes, including the rethinking of IR's core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.","meta":{"url":"http://arxiv.org/abs/2307.09751v1"},"cats":{"new-dataset":0.0510861244,"dev-research":0.2976081526,"prompt-eng":0.5127737889,"data-quality":0.0808512793,"ml-security":0.0550244987}}
{"text":"Scene text image super-resolution (STISR), aiming to improve image quality while boosting downstream scene text recognition accuracy, has recently achieved great success.","meta":{"url":"http://arxiv.org/abs/2307.09749v1"},"cats":{"new-dataset":0.1596259625,"dev-research":0.2293925852,"prompt-eng":0.416409741,"data-quality":0.2697091305,"ml-security":0.053063108}}
{"text":"However, most existing methods treat the foreground (character regions) and background (non-character regions) equally in the forward process, and neglect the disturbance from the complex background, thus limiting the performance.","meta":{"url":"http://arxiv.org/abs/2307.09749v1"},"cats":{"new-dataset":0.010620754,"dev-research":0.2321371908,"prompt-eng":0.4088244015,"data-quality":0.0998186992,"ml-security":0.0754142251}}
{"text":"To address these issues, in this paper, we propose a novel method LEMMA that explicitly models character regions to produce high-level text-specific guidance for super-resolution.","meta":{"url":"http://arxiv.org/abs/2307.09749v1"},"cats":{"new-dataset":0.0703023248,"dev-research":0.265263,"prompt-eng":0.4668035265,"data-quality":0.1812710211,"ml-security":0.0465050332}}
{"text":"To model the location of characters effectively, we propose the location enhancement module to extract character region features based on the attention map sequence.","meta":{"url":"http://arxiv.org/abs/2307.09749v1"},"cats":{"new-dataset":0.1725365095,"dev-research":0.2734812937,"prompt-eng":0.4545832829,"data-quality":0.1624647545,"ml-security":0.0812486021}}
{"text":"Besides, we propose the multi-modal alignment module to perform bidirectional visual-semantic alignment to generate high-quality prior guidance, which is then incorporated into the super-resolution branch in an adaptive manner using the proposed adaptive fusion module.","meta":{"url":"http://arxiv.org/abs/2307.09749v1"},"cats":{"new-dataset":0.0972340349,"dev-research":0.3148514742,"prompt-eng":0.4516862711,"data-quality":0.1224877484,"ml-security":0.0187706392}}
{"text":"Experiments on TextZoom and four scene text recognition benchmarks demonstrate the superiority of our method over other state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.09749v1"},"cats":{"new-dataset":0.1576569881,"dev-research":0.2215111618,"prompt-eng":0.3813192393,"data-quality":0.2852907332,"ml-security":0.075151525}}
{"text":"Code is available at https://github.com/csguoh/LEMMA.","meta":{"url":"http://arxiv.org/abs/2307.09749v1"},"cats":{"new-dataset":0.1649455741,"dev-research":0.2235220019,"prompt-eng":0.3848930005,"data-quality":0.1183303231,"ml-security":0.0612496325}}
{"text":"The SnakeCLEF2023 competition aims to the development of advanced algorithms for snake species identification through the analysis of images and accompanying metadata.","meta":{"url":"http://arxiv.org/abs/2307.09748v1"},"cats":{"new-dataset":0.250883372,"dev-research":0.2603584376,"prompt-eng":0.4016508492,"data-quality":0.234852094,"ml-security":0.0784877009}}
{"text":"This paper presents a method leveraging utilization of both images and metadata.","meta":{"url":"http://arxiv.org/abs/2307.09748v1"},"cats":{"new-dataset":0.1391473587,"dev-research":0.2883139727,"prompt-eng":0.4293154343,"data-quality":0.1770421501,"ml-security":0.0503343034}}
{"text":"Modern CNN models and strong data augmentation are utilized to learn better representation of images.","meta":{"url":"http://arxiv.org/abs/2307.09748v1"},"cats":{"new-dataset":0.0667093439,"dev-research":0.2829391655,"prompt-eng":0.4204120812,"data-quality":0.2172014518,"ml-security":0.1676095111}}
{"text":"To relieve the challenge of long-tailed distribution, seesaw loss is utilized in our method.","meta":{"url":"http://arxiv.org/abs/2307.09748v1"},"cats":{"new-dataset":0.034756434,"dev-research":0.2100068174,"prompt-eng":0.3768714586,"data-quality":0.0871600446,"ml-security":0.1077315877}}
{"text":"We also design a light model to calculate prior probabilities using metadata features extracted from CLIP in post processing stage.","meta":{"url":"http://arxiv.org/abs/2307.09748v1"},"cats":{"new-dataset":0.1486665834,"dev-research":0.2481425436,"prompt-eng":0.4950578987,"data-quality":0.1245601258,"ml-security":0.0646411423}}
{"text":"Besides, we attach more importance to venomous species by assigning venomous species labels to some examples that model is uncertain about.","meta":{"url":"http://arxiv.org/abs/2307.09748v1"},"cats":{"new-dataset":0.0370535393,"dev-research":0.339748295,"prompt-eng":0.4321787709,"data-quality":0.3499195036,"ml-security":0.2300256364}}
{"text":"Our method achieves 91.31% score of the final metric combined of F1 and other metrics on private leaderboard, which is the 1st place among the participators.","meta":{"url":"http://arxiv.org/abs/2307.09748v1"},"cats":{"new-dataset":0.1143155788,"dev-research":0.2615156606,"prompt-eng":0.4017453933,"data-quality":0.1171529951,"ml-security":0.0802953793}}
{"text":"The code is available at https://github.com/xiaoxsparraw/CLEF2023.","meta":{"url":"http://arxiv.org/abs/2307.09748v1"},"cats":{"new-dataset":0.4534193608,"dev-research":0.2375254879,"prompt-eng":0.4503226602,"data-quality":0.108061725,"ml-security":0.0427173483}}
{"text":"The integration of natural language processing (NLP) technologies into educational applications has shown promising results, particularly in the language learning domain.","meta":{"url":"http://arxiv.org/abs/2307.09744v1"},"cats":{"new-dataset":0.091336189,"dev-research":0.2962528358,"prompt-eng":0.4459711567,"data-quality":0.2418018294,"ml-security":0.0678989966}}
{"text":"Recently, many spoken open-domain chatbots have been used as speaking partners, helping language learners improve their language skills.","meta":{"url":"http://arxiv.org/abs/2307.09744v1"},"cats":{"new-dataset":0.2099665041,"dev-research":0.3323767817,"prompt-eng":0.3986097901,"data-quality":0.1357887469,"ml-security":0.119786071}}
{"text":"However, one of the significant challenges is the high word-error-rate (WER) when recognizing non-native/non-fluent speech, which interrupts conversation flow and leads to disappointment for learners.","meta":{"url":"http://arxiv.org/abs/2307.09744v1"},"cats":{"new-dataset":0.0531859444,"dev-research":0.3499983337,"prompt-eng":0.4055789522,"data-quality":0.391244371,"ml-security":0.1055944155}}
{"text":"This paper explores the use of GPT4 for ASR error correction in conversational settings.","meta":{"url":"http://arxiv.org/abs/2307.09744v1"},"cats":{"new-dataset":0.0922328251,"dev-research":0.3192873915,"prompt-eng":0.4106346519,"data-quality":0.3470284203,"ml-security":0.0599002196}}
{"text":"In addition to WER, we propose to use semantic textual similarity (STS) and next response sensibility (NRS) metrics to evaluate the impact of error correction models on the quality of the conversation.","meta":{"url":"http://arxiv.org/abs/2307.09744v1"},"cats":{"new-dataset":0.0848613729,"dev-research":0.375470897,"prompt-eng":0.4370948641,"data-quality":0.4353677271,"ml-security":0.0602152371}}
{"text":"We find that transcriptions corrected by GPT4 lead to higher conversation quality, despite an increase in WER.","meta":{"url":"http://arxiv.org/abs/2307.09744v1"},"cats":{"new-dataset":0.091988128,"dev-research":0.3440563861,"prompt-eng":0.3976294537,"data-quality":0.266565172,"ml-security":0.0622909666}}
{"text":"GPT4 also outperforms standard error correction methods without the need for in-domain training data.","meta":{"url":"http://arxiv.org/abs/2307.09744v1"},"cats":{"new-dataset":0.0439964498,"dev-research":0.2992903144,"prompt-eng":0.3322038594,"data-quality":0.2705397103,"ml-security":0.1287076549}}
{"text":"Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications.","meta":{"url":"http://arxiv.org/abs/2307.09742v1"},"cats":{"new-dataset":0.2243194313,"dev-research":0.276445968,"prompt-eng":0.329246133,"data-quality":0.1700256033,"ml-security":0.2547798352}}
{"text":"However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models.","meta":{"url":"http://arxiv.org/abs/2307.09742v1"},"cats":{"new-dataset":0.0791256849,"dev-research":0.2406376546,"prompt-eng":0.3334960352,"data-quality":0.1192868652,"ml-security":0.1713949064}}
{"text":"In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising.","meta":{"url":"http://arxiv.org/abs/2307.09742v1"},"cats":{"new-dataset":0.4875026908,"dev-research":0.2078728401,"prompt-eng":0.3455918878,"data-quality":0.2462365958,"ml-security":0.1269128892}}
{"text":"Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization).","meta":{"url":"http://arxiv.org/abs/2307.09742v1"},"cats":{"new-dataset":0.1315702253,"dev-research":0.2463735888,"prompt-eng":0.3956876344,"data-quality":0.3453798051,"ml-security":0.299767169}}
{"text":"Our simple yet effective method outperforms most previous optimization-oriented methods with much fewer computational resources, thereby scaling data condensation to larger datasets and models.","meta":{"url":"http://arxiv.org/abs/2307.09742v1"},"cats":{"new-dataset":0.0375366298,"dev-research":0.2416764309,"prompt-eng":0.3659983751,"data-quality":0.1183706113,"ml-security":0.1143712731}}
{"text":"Extensive experiments demonstrate the effectiveness of our method.","meta":{"url":"http://arxiv.org/abs/2307.09742v1"},"cats":{"new-dataset":0.0069218576,"dev-research":0.3250690283,"prompt-eng":0.4460771688,"data-quality":0.1638381086,"ml-security":0.1125036913}}
{"text":"Codes are available at https://github.com/uitrbn/IDM","meta":{"url":"http://arxiv.org/abs/2307.09742v1"},"cats":{"new-dataset":0.4246506102,"dev-research":0.2860513472,"prompt-eng":0.4778040364,"data-quality":0.1615746696,"ml-security":0.0758850307}}
{"text":"Perfect ciphers have been a very attractive cryptographic tool ever since C. Shannon described them.","meta":{"url":"http://arxiv.org/abs/2307.09735v1"},"cats":{"new-dataset":0.13970407,"dev-research":0.3018057246,"prompt-eng":0.4304654973,"data-quality":0.1229795225,"ml-security":0.1837054984}}
{"text":"Note that, by definition, if a perfect cipher is used, no one can get any information about the encrypted message without knowing the secret key.","meta":{"url":"http://arxiv.org/abs/2307.09735v1"},"cats":{"new-dataset":0.0619550601,"dev-research":0.2486138889,"prompt-eng":0.4002152636,"data-quality":0.1468669769,"ml-security":0.308593556}}
{"text":"We consider the problem of reducing the key length of perfect ciphers, because in many applications the length of the secret key is a crucial parameter.","meta":{"url":"http://arxiv.org/abs/2307.09735v1"},"cats":{"new-dataset":0.0402170989,"dev-research":0.2319022035,"prompt-eng":0.3546157159,"data-quality":0.1326452388,"ml-security":0.3512546087}}
{"text":"This paper describes a simple method of key length reduction.","meta":{"url":"http://arxiv.org/abs/2307.09735v1"},"cats":{"new-dataset":0.0648642165,"dev-research":0.2706431759,"prompt-eng":0.3570891074,"data-quality":0.1199412234,"ml-security":0.0865519185}}
{"text":"This method gives a perfect cipher and is based on the use of data compression and randomisation, and the average key length can be made close to Shannon entropy (which is the key length limit).","meta":{"url":"http://arxiv.org/abs/2307.09735v1"},"cats":{"new-dataset":0.0802264048,"dev-research":0.2228131188,"prompt-eng":0.3376740891,"data-quality":0.0671517665,"ml-security":0.1431383198}}
{"text":"It should be noted that the method can effectively use readily available data compressors (archivers).","meta":{"url":"http://arxiv.org/abs/2307.09735v1"},"cats":{"new-dataset":0.0600153228,"dev-research":0.3083062311,"prompt-eng":0.3493342963,"data-quality":0.1451911022,"ml-security":0.0707078528}}
{"text":"3D instance segmentation methods often require fully-annotated dense labels for training, which are costly to obtain.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.1083615859,"dev-research":0.2673953964,"prompt-eng":0.4025399818,"data-quality":0.3861485941,"ml-security":0.1403691491}}
{"text":"In this paper, we present ClickSeg, a novel click-level weakly supervised 3D instance segmentation method that requires one point per instance annotation merely.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.1341468497,"dev-research":0.2495730516,"prompt-eng":0.4052667428,"data-quality":0.2602046166,"ml-security":0.0840287659}}
{"text":"Such a problem is very challenging due to the extremely limited labels, which has rarely been solved before.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.0914813456,"dev-research":0.2732236919,"prompt-eng":0.3967485257,"data-quality":0.444626328,"ml-security":0.1447281987}}
{"text":"We first develop a baseline weakly-supervised training method, which generates pseudo labels for unlabeled data by the model itself.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.2017107144,"dev-research":0.2390411185,"prompt-eng":0.4696629509,"data-quality":0.6235038052,"ml-security":0.1796688826}}
{"text":"To utilize the property of click-level annotation setting, we further propose a new training framework.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.1201070949,"dev-research":0.3632818665,"prompt-eng":0.4846019264,"data-quality":0.5092279141,"ml-security":0.1474148505}}
{"text":"Instead of directly using the model inference way, i.e., mean-shift clustering, to generate the pseudo labels, we propose to use k-means with fixed initial seeds: the annotated points.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.2005225322,"dev-research":0.229210409,"prompt-eng":0.4859566696,"data-quality":0.4934939896,"ml-security":0.0960897774}}
{"text":"New similarity metrics are further designed for clustering.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.1452059208,"dev-research":0.3258831993,"prompt-eng":0.3709690602,"data-quality":0.1696470004,"ml-security":0.0672995821}}
{"text":"Experiments on ScanNetV2 and S3DIS datasets show that the proposed ClickSeg surpasses the previous best weakly supervised instance segmentation result by a large margin (e.g., +9.4% mAP on ScanNetV2).","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.1753872938,"dev-research":0.2377477438,"prompt-eng":0.4100333021,"data-quality":0.2309331856,"ml-security":0.1056782013}}
{"text":"Using 0.02% supervision signals merely, ClickSeg achieves $\\sim$90% of the accuracy of the fully-supervised counterpart.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.0330469924,"dev-research":0.2848939945,"prompt-eng":0.428469163,"data-quality":0.303879204,"ml-security":0.0929351614}}
{"text":"Meanwhile, it also achieves state-of-the-art semantic segmentation results among weakly supervised methods that use the same annotation settings.","meta":{"url":"http://arxiv.org/abs/2307.09732v1"},"cats":{"new-dataset":0.1024449795,"dev-research":0.2713253495,"prompt-eng":0.4269807201,"data-quality":0.4828649937,"ml-security":0.0859815552}}
{"text":"Sound is a rich information medium that transmits through air; people communicate through speech and can even discern material through tapping and listening.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.0871621482,"dev-research":0.3194151609,"prompt-eng":0.3762740531,"data-quality":0.1043701382,"ml-security":0.0920617471}}
{"text":"To capture frequencies in the human hearing range, commercial microphones typically have a sampling rate of over 40kHz.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.1270150425,"dev-research":0.2207680362,"prompt-eng":0.368616973,"data-quality":0.1539756197,"ml-security":0.0713235014}}
{"text":"These accessible acoustic technologies are not yet widely adopted for the explicit purpose of giving robots a sense of touch.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.0530874447,"dev-research":0.2439378641,"prompt-eng":0.3809167031,"data-quality":0.068795889,"ml-security":0.0719066493}}
{"text":"Some researchers have used sound to sense tactile information, both monitoring ambient soundscape and with embedded speakers and microphones to measure sounds within structures.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.113490036,"dev-research":0.3173265847,"prompt-eng":0.4227898398,"data-quality":0.1124040695,"ml-security":0.1078798111}}
{"text":"However, these options commonly do not provide a direct measure of steady state force, or require electronics integrated somewhere near the contact location.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.0174456778,"dev-research":0.2002455063,"prompt-eng":0.3873364201,"data-quality":0.0773320805,"ml-security":0.0800803834}}
{"text":"In this work, we present AcousTac, an acoustic tactile sensor for electronics-free force sensitive soft skin.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.1421439699,"dev-research":0.2360870016,"prompt-eng":0.3953088736,"data-quality":0.1148217558,"ml-security":0.1338570053}}
{"text":"Compliant silicone caps and plastic tubes compose the resonant chambers that emit pneumatic-driven sound measurable with a conventional off-board microphone.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.0580580257,"dev-research":0.2232192798,"prompt-eng":0.399764545,"data-quality":0.1508896595,"ml-security":0.0669607986}}
{"text":"The resulting frequency changes depend on the external loads on the compliant end caps.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.0241350371,"dev-research":0.2294030073,"prompt-eng":0.4387245593,"data-quality":0.1955287768,"ml-security":0.0567184827}}
{"text":"We can tune each AcousTac taxel to specific force and frequency ranges, based on geometric parameters, including tube length and end-cap geometry and thus uniquely sense each taxel simultaneously in an array.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.0415834821,"dev-research":0.1788645515,"prompt-eng":0.4105754257,"data-quality":0.0877376984,"ml-security":0.0331771263}}
{"text":"We demonstrate AcousTac's functionality on two robotic systems: a 4-taxel array and a 3-taxel astrictive gripper.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.0256037289,"dev-research":0.2125963434,"prompt-eng":0.3856486531,"data-quality":0.0711194236,"ml-security":0.0444426256}}
{"text":"AcousTac is a promising concept for force sensing on soft robotic surfaces, especially in situations where electronics near the contact are not suitable.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.064111027,"dev-research":0.245490865,"prompt-eng":0.4076069259,"data-quality":0.0979084098,"ml-security":0.0954349414}}
{"text":"Equipping robots with tactile sensing and soft skin provides them with a sense of touch and the ability to safely interact with their surroundings.","meta":{"url":"http://arxiv.org/abs/2307.09730v1"},"cats":{"new-dataset":0.0476186144,"dev-research":0.2488570831,"prompt-eng":0.4541990457,"data-quality":0.079417647,"ml-security":0.1556502434}}
{"text":"This paper reports on the NTIRE 2023 Quality Assessment of Video Enhancement Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2023.","meta":{"url":"http://arxiv.org/abs/2307.09729v1"},"cats":{"new-dataset":0.1962036539,"dev-research":0.2727651531,"prompt-eng":0.3941478111,"data-quality":0.2103356481,"ml-security":0.0547987395}}
{"text":"This challenge is to address a major challenge in the field of video processing, namely, video quality assessment (VQA) for enhanced videos.","meta":{"url":"http://arxiv.org/abs/2307.09729v1"},"cats":{"new-dataset":0.0920756215,"dev-research":0.2877374531,"prompt-eng":0.3707740003,"data-quality":0.1744443678,"ml-security":0.0503698881}}
{"text":"The challenge uses the VQA Dataset for Perceptual Video Enhancement (VDPVE), which has a total of 1211 enhanced videos, including 600 videos with color, brightness, and contrast enhancements, 310 videos with deblurring, and 301 deshaked videos.","meta":{"url":"http://arxiv.org/abs/2307.09729v1"},"cats":{"new-dataset":0.4927524176,"dev-research":0.2656688898,"prompt-eng":0.3524153837,"data-quality":0.1465117867,"ml-security":0.0817021878}}
{"text":"The challenge has a total of 167 registered participants.","meta":{"url":"http://arxiv.org/abs/2307.09729v1"},"cats":{"new-dataset":0.3924818791,"dev-research":0.2755911256,"prompt-eng":0.4149120344,"data-quality":0.1060979716,"ml-security":0.08361444}}
{"text":"61 participating teams submitted their prediction results during the development phase, with a total of 3168 submissions.","meta":{"url":"http://arxiv.org/abs/2307.09729v1"},"cats":{"new-dataset":0.3809355168,"dev-research":0.3749323219,"prompt-eng":0.4816704054,"data-quality":0.1477742754,"ml-security":0.0871845383}}
{"text":"A total of 176 submissions were submitted by 37 participating teams during the final testing phase.","meta":{"url":"http://arxiv.org/abs/2307.09729v1"},"cats":{"new-dataset":0.3293341121,"dev-research":0.2949636814,"prompt-eng":0.4616759608,"data-quality":0.1312958147,"ml-security":0.0835500692}}
{"text":"Finally, 19 participating teams submitted their models and fact sheets, and detailed the methods they used.","meta":{"url":"http://arxiv.org/abs/2307.09729v1"},"cats":{"new-dataset":0.276260399,"dev-research":0.3732625248,"prompt-eng":0.4562501009,"data-quality":0.0895352736,"ml-security":0.0384556032}}
{"text":"Some methods have achieved better results than baseline methods, and the winning methods have demonstrated superior prediction performance.","meta":{"url":"http://arxiv.org/abs/2307.09729v1"},"cats":{"new-dataset":0.0211532271,"dev-research":0.3120263299,"prompt-eng":0.3961982881,"data-quality":0.1437687683,"ml-security":0.1087185744}}
{"text":"Visual-based measurement systems are frequently affected by rainy weather due to the degradation caused by rain streaks in captured images, and existing imaging devices struggle to address this issue in real-time.","meta":{"url":"http://arxiv.org/abs/2307.09728v1"},"cats":{"new-dataset":0.2221728477,"dev-research":0.3451962466,"prompt-eng":0.4181027826,"data-quality":0.1765544287,"ml-security":0.0810433185}}
{"text":"While most efforts leverage deep networks for image deraining and have made progress, their large parameter sizes hinder deployment on resource-constrained devices.","meta":{"url":"http://arxiv.org/abs/2307.09728v1"},"cats":{"new-dataset":0.0625843329,"dev-research":0.3248570085,"prompt-eng":0.4068867424,"data-quality":0.1224138977,"ml-security":0.2874396613}}
{"text":"Additionally, these data-driven models often produce deterministic results, without considering their inherent epistemic uncertainty, which can lead to undesired reconstruction errors.","meta":{"url":"http://arxiv.org/abs/2307.09728v1"},"cats":{"new-dataset":0.0648459546,"dev-research":0.2832223836,"prompt-eng":0.4148211669,"data-quality":0.2742296302,"ml-security":0.1956910984}}
{"text":"Well-calibrated uncertainty can help alleviate prediction errors and assist measurement devices in mitigating risks and improving usability.","meta":{"url":"http://arxiv.org/abs/2307.09728v1"},"cats":{"new-dataset":0.0490078725,"dev-research":0.4439385002,"prompt-eng":0.4677427688,"data-quality":0.2801077553,"ml-security":0.1842950495}}
{"text":"Therefore, we propose an Uncertainty-Driven Multi-Scale Feature Fusion Network (UMFFNet) that learns the probability mapping distribution between paired images to estimate uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.09728v1"},"cats":{"new-dataset":0.109678799,"dev-research":0.2606856039,"prompt-eng":0.4197852246,"data-quality":0.2505577046,"ml-security":0.0934371067}}
{"text":"Specifically, we introduce an uncertainty feature fusion block (UFFB) that utilizes uncertainty information to dynamically enhance acquired features and focus on blurry regions obscured by rain streaks, reducing prediction errors.","meta":{"url":"http://arxiv.org/abs/2307.09728v1"},"cats":{"new-dataset":0.1271821089,"dev-research":0.3210601564,"prompt-eng":0.4260463676,"data-quality":0.2096670851,"ml-security":0.1138231638}}
{"text":"In addition, to further boost the performance of UMFFNet, we fused feature information from multiple scales to guide the network for efficient collaborative rain removal.","meta":{"url":"http://arxiv.org/abs/2307.09728v1"},"cats":{"new-dataset":0.0904991059,"dev-research":0.3049697373,"prompt-eng":0.4199186267,"data-quality":0.1347753972,"ml-security":0.0691338284}}
{"text":"Extensive experiments demonstrate that UMFFNet achieves significant performance improvements with few parameters, surpassing state-of-the-art image deraining methods.","meta":{"url":"http://arxiv.org/abs/2307.09728v1"},"cats":{"new-dataset":0.1224221848,"dev-research":0.2739852554,"prompt-eng":0.4078228979,"data-quality":0.1814561486,"ml-security":0.0853759962}}
{"text":"Estimating displacement vector field via a cost volume computed in the feature space has shown great success in image registration, but it suffers excessive computation burdens.","meta":{"url":"http://arxiv.org/abs/2307.09727v1"},"cats":{"new-dataset":0.0653786274,"dev-research":0.2458756226,"prompt-eng":0.4071690724,"data-quality":0.1491521921,"ml-security":0.0663844738}}
{"text":"Moreover, existing feature descriptors only extract local features incapable of representing the global semantic information, which is especially important for solving large transformations.","meta":{"url":"http://arxiv.org/abs/2307.09727v1"},"cats":{"new-dataset":0.0422910899,"dev-research":0.3383644868,"prompt-eng":0.3748176242,"data-quality":0.2231639935,"ml-security":0.1347919205}}
{"text":"To address the discussed issues, we propose SAMConvex, a fast coarse-to-fine discrete optimization method for CT registration that includes a decoupled convex optimization procedure to obtain deformation fields based on a self-supervised anatomical embedding (SAM) feature extractor that captures both local and global information.","meta":{"url":"http://arxiv.org/abs/2307.09727v1"},"cats":{"new-dataset":0.0580258911,"dev-research":0.2178296542,"prompt-eng":0.4141289138,"data-quality":0.1170516112,"ml-security":0.0701316755}}
{"text":"To be specific, SAMConvex extracts per-voxel features and builds 6D correlation volumes based on SAM features, and iteratively updates a flow field by performing lookups on the correlation volumes with a coarse-to-fine scheme.","meta":{"url":"http://arxiv.org/abs/2307.09727v1"},"cats":{"new-dataset":0.1056585881,"dev-research":0.2726829076,"prompt-eng":0.3702671701,"data-quality":0.0788595066,"ml-security":0.0504904776}}
{"text":"SAMConvex outperforms the state-of-the-art learning-based methods and optimization-based methods over two inter-patient registration datasets (Abdomen CT and HeadNeck CT) and one intra-patient registration dataset (Lung CT).","meta":{"url":"http://arxiv.org/abs/2307.09727v1"},"cats":{"new-dataset":0.0876193638,"dev-research":0.2554754381,"prompt-eng":0.3826835712,"data-quality":0.0775715313,"ml-security":0.098265692}}
{"text":"Moreover, as an optimization-based method, SAMConvex only takes $\\sim2$s ($\\sim5s$ with instance optimization) for one paired images.","meta":{"url":"http://arxiv.org/abs/2307.09727v1"},"cats":{"new-dataset":0.0322535303,"dev-research":0.1684298544,"prompt-eng":0.3830811545,"data-quality":0.073170589,"ml-security":0.050754307}}
{"text":"To deliver the artistic expression of the target style, recent studies exploit the attention mechanism owing to its ability to map the local patches of the style image to the corresponding patches of the content image.","meta":{"url":"http://arxiv.org/abs/2307.09724v1"},"cats":{"new-dataset":0.0204111905,"dev-research":0.2953975078,"prompt-eng":0.4689661776,"data-quality":0.2205569992,"ml-security":0.0894601516}}
{"text":"However, because of the low semantic correspondence between arbitrary content and artworks, the attention module repeatedly abuses specific local patches from the style image, resulting in disharmonious and evident repetitive artifacts.","meta":{"url":"http://arxiv.org/abs/2307.09724v1"},"cats":{"new-dataset":0.0451287947,"dev-research":0.3035859145,"prompt-eng":0.4407467213,"data-quality":0.4015506422,"ml-security":0.1101918012}}
{"text":"To overcome this limitation and accomplish impeccable artistic style transfer, we focus on enhancing the attention mechanism and capturing the rhythm of patterns that organize the style.","meta":{"url":"http://arxiv.org/abs/2307.09724v1"},"cats":{"new-dataset":0.0374226042,"dev-research":0.2788538622,"prompt-eng":0.4527554311,"data-quality":0.1288580166,"ml-security":0.0541325861}}
{"text":"In this paper, we introduce a novel metric, namely pattern repeatability, that quantifies the repetition of patterns in the style image.","meta":{"url":"http://arxiv.org/abs/2307.09724v1"},"cats":{"new-dataset":0.1001245989,"dev-research":0.2814147497,"prompt-eng":0.4178967781,"data-quality":0.2361094289,"ml-security":0.0586429031}}
{"text":"Based on the pattern repeatability, we propose Aesthetic Pattern-Aware style transfer Networks (AesPA-Net) that discover the sweet spot of local and global style expressions.","meta":{"url":"http://arxiv.org/abs/2307.09724v1"},"cats":{"new-dataset":0.0272466665,"dev-research":0.2890132374,"prompt-eng":0.4272237063,"data-quality":0.2038625785,"ml-security":0.0951889513}}
{"text":"In addition, we propose a novel self-supervisory task to encourage the attention mechanism to learn precise and meaningful semantic correspondence.","meta":{"url":"http://arxiv.org/abs/2307.09724v1"},"cats":{"new-dataset":0.0344037245,"dev-research":0.3026461292,"prompt-eng":0.523710276,"data-quality":0.2929398769,"ml-security":0.0511515152}}
{"text":"Lastly, we introduce the patch-wise style loss to transfer the elaborate rhythm of local patterns.","meta":{"url":"http://arxiv.org/abs/2307.09724v1"},"cats":{"new-dataset":0.0934896041,"dev-research":0.2936252804,"prompt-eng":0.431081122,"data-quality":0.2632320761,"ml-security":0.0798481996}}
{"text":"Through qualitative and quantitative evaluations, we verify the reliability of the proposed pattern repeatability that aligns with human perception, and demonstrate the superiority of the proposed framework.","meta":{"url":"http://arxiv.org/abs/2307.09724v1"},"cats":{"new-dataset":0.0323689477,"dev-research":0.2851685557,"prompt-eng":0.4429414075,"data-quality":0.2955146695,"ml-security":0.0919792669}}
{"text":"Sound classification models' performance suffers from generalizing on out-of-distribution (OOD) data.","meta":{"url":"http://arxiv.org/abs/2307.09723v1"},"cats":{"new-dataset":0.1110588398,"dev-research":0.2309378199,"prompt-eng":0.3519957891,"data-quality":0.3530763116,"ml-security":0.229801638}}
{"text":"Numerous methods have been proposed to help the model generalize.","meta":{"url":"http://arxiv.org/abs/2307.09723v1"},"cats":{"new-dataset":0.0096091422,"dev-research":0.2416471196,"prompt-eng":0.4491559166,"data-quality":0.1139578978,"ml-security":0.0968242283}}
{"text":"However, most either introduce inference overheads or focus on long-lasting CNN-variants, while Transformers has been proven to outperform CNNs on numerous natural language processing and computer vision tasks.","meta":{"url":"http://arxiv.org/abs/2307.09723v1"},"cats":{"new-dataset":0.0347414526,"dev-research":0.3229194186,"prompt-eng":0.3794100406,"data-quality":0.1391857077,"ml-security":0.1545923567}}
{"text":"We propose FRITO, an effective regularization technique on Transformer's self-attention, to improve the model's generalization ability by limiting each sequence position's attention receptive field along the frequency dimension on the spectrogram.","meta":{"url":"http://arxiv.org/abs/2307.09723v1"},"cats":{"new-dataset":0.0412821786,"dev-research":0.2081386516,"prompt-eng":0.4737841486,"data-quality":0.2137723542,"ml-security":0.1288184961}}
{"text":"Experiments show that our method helps Transformer models achieve SOTA generalization performance on TAU 2020 and Nsynth datasets while saving 20% inference time.","meta":{"url":"http://arxiv.org/abs/2307.09723v1"},"cats":{"new-dataset":0.0860946308,"dev-research":0.2133971564,"prompt-eng":0.4090141889,"data-quality":0.0951844996,"ml-security":0.1068166607}}
{"text":"Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.2762953528,"dev-research":0.3400397514,"prompt-eng":0.4420918818,"data-quality":0.2416540454,"ml-security":0.0423746994}}
{"text":"Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.0149501366,"dev-research":0.3421584022,"prompt-eng":0.4437534026,"data-quality":0.3666115527,"ml-security":0.082313967}}
{"text":"Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.0278816417,"dev-research":0.3154166221,"prompt-eng":0.3840548962,"data-quality":0.4585523647,"ml-security":0.2778177657}}
{"text":"To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network $\\textbf{(MIMIC)}$ framework for solving the MEL task.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.1367820924,"dev-research":0.2598449127,"prompt-eng":0.4341415507,"data-quality":0.1283200171,"ml-security":0.068370125}}
{"text":"Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.1294459595,"dev-research":0.4010750846,"prompt-eng":0.4810725839,"data-quality":0.2615466498,"ml-security":0.0667948175}}
{"text":"Then, to derive the similarity matching score for each mention-entity pair, we device three interaction units to comprehensively explore the intra-modal interaction and inter-modal fusion among features of entities and mentions.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.0958287197,"dev-research":0.3367648146,"prompt-eng":0.4486059529,"data-quality":0.1771531828,"ml-security":0.026020682}}
{"text":"In particular, three modules, namely the Text-based Global-Local interaction Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based interaction Unit (CMFU) are designed to capture and integrate the fine-grained representation lying in abbreviated text and implicit visual cues.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.1136866586,"dev-research":0.3129163454,"prompt-eng":0.4654323176,"data-quality":0.1824129699,"ml-security":0.0434017139}}
{"text":"Afterwards, we introduce a unit-consistency objective function via contrastive learning to avoid inconsistency and model degradation.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.0623907185,"dev-research":0.2317884809,"prompt-eng":0.3872426626,"data-quality":0.3117472732,"ml-security":0.084474683}}
{"text":"Experimental results on three public benchmark datasets demonstrate that our solution outperforms various state-of-the-art baselines, and ablation studies verify the effectiveness of designed modules.","meta":{"url":"http://arxiv.org/abs/2307.09721v1"},"cats":{"new-dataset":0.2238121322,"dev-research":0.2865792819,"prompt-eng":0.3907026264,"data-quality":0.1296466136,"ml-security":0.0479296542}}
{"text":"Extracting image semantics effectively and assigning corresponding labels to multiple objects or attributes for natural images is challenging due to the complex scene contents and confusing label dependencies.","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.1618671819,"dev-research":0.2807153602,"prompt-eng":0.436396507,"data-quality":0.4226508807,"ml-security":0.0708020262}}
{"text":"Recent works have focused on modeling label relationships with graph and understanding object regions using class activation maps (CAM).","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.2920534855,"dev-research":0.2962465475,"prompt-eng":0.411454882,"data-quality":0.4492716149,"ml-security":0.0909019283}}
{"text":"However, these methods ignore the complex intra- and inter-category relationships among specific semantic features, and CAM is prone to generate noisy information.","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.0793765111,"dev-research":0.4140666786,"prompt-eng":0.3760318508,"data-quality":0.4792038622,"ml-security":0.0901592754}}
{"text":"To this end, we propose a novel semantic-aware dual contrastive learning framework that incorporates sample-to-sample contrastive learning (SSCL) as well as prototype-to-sample contrastive learning (PSCL).","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.2324628116,"dev-research":0.2234652375,"prompt-eng":0.4075759516,"data-quality":0.1757637043,"ml-security":0.0979980914}}
{"text":"Specifically, we leverage semantic-aware representation learning to extract category-related local discriminative features and construct category prototypes.","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.0926042006,"dev-research":0.3335735897,"prompt-eng":0.4658502485,"data-quality":0.2806004769,"ml-security":0.1098100729}}
{"text":"Then based on SSCL, label-level visual representations of the same category are aggregated together, and features belonging to distinct categories are separated.","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.0823147822,"dev-research":0.3442684482,"prompt-eng":0.3887283643,"data-quality":0.2260742695,"ml-security":0.0579264793}}
{"text":"Meanwhile, we construct a novel PSCL module to narrow the distance between positive samples and category prototypes and push negative samples away from the corresponding category prototypes.","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.0917518127,"dev-research":0.2916577051,"prompt-eng":0.4762582119,"data-quality":0.2411151849,"ml-security":0.0975071923}}
{"text":"Finally, the discriminative label-level features related to the image content are accurately captured by the joint training of the above three parts.","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.1305702253,"dev-research":0.2288369562,"prompt-eng":0.4514855995,"data-quality":0.4374409836,"ml-security":0.0597353916}}
{"text":"Experiments on five challenging large-scale public datasets demonstrate that our proposed method is effective and outperforms the state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.5740043375,"dev-research":0.2855897216,"prompt-eng":0.3378492885,"data-quality":0.2314006234,"ml-security":0.2848798673}}
{"text":"Code and supplementary materials are released on https://github.com/yu-gi-oh-leilei/SADCL.","meta":{"url":"http://arxiv.org/abs/2307.09715v1"},"cats":{"new-dataset":0.4307646216,"dev-research":0.3618364885,"prompt-eng":0.409034997,"data-quality":0.1033257229,"ml-security":0.0688010442}}
{"text":"This paper presents the deep learning-based recent achievements to resolve the problem of autonomous mobility control and efficient resource management of autonomous vehicles and UAVs, i.e., (i) multi-agent reinforcement learning (MARL), and (ii) neural Myerson auction.","meta":{"url":"http://arxiv.org/abs/2307.09711v1"},"cats":{"new-dataset":0.1676536469,"dev-research":0.2054525999,"prompt-eng":0.3783925699,"data-quality":0.0697577342,"ml-security":0.2326665097}}
{"text":"Representatively, communication network (CommNet), which is one of the most popular MARL algorithms, is introduced to enable multiple agents to take actions in a distributed manner for their shared goals by training all agents' states and actions in a single neural network.","meta":{"url":"http://arxiv.org/abs/2307.09711v1"},"cats":{"new-dataset":0.0430692251,"dev-research":0.1876829173,"prompt-eng":0.4120774842,"data-quality":0.0664120343,"ml-security":0.1703051093}}
{"text":"Moreover, the neural Myerson auction guarantees trustfulness among multiple agents as well as achieves the optimal revenue of highly dynamic systems.","meta":{"url":"http://arxiv.org/abs/2307.09711v1"},"cats":{"new-dataset":0.0158890127,"dev-research":0.2058947186,"prompt-eng":0.4218534438,"data-quality":0.0613651068,"ml-security":0.3814487332}}
{"text":"Therefore, we survey the recent studies on autonomous mobility control based on MARL and neural Myerson auction.","meta":{"url":"http://arxiv.org/abs/2307.09711v1"},"cats":{"new-dataset":0.0526816501,"dev-research":0.1745627035,"prompt-eng":0.4277543799,"data-quality":0.06333021,"ml-security":0.185872547}}
{"text":"Furthermore, we emphasize that integration of MARL and neural Myerson auction is expected to be critical for efficient and trustful autonomous mobility services.","meta":{"url":"http://arxiv.org/abs/2307.09711v1"},"cats":{"new-dataset":0.0399475454,"dev-research":0.2120467245,"prompt-eng":0.4361082651,"data-quality":0.0907242754,"ml-security":0.1934748671}}
{"text":"Taxonomies are an essential knowledge representation, yet most studies on automatic taxonomy construction (ATC) resort to manual evaluation to score proposed algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09706v1"},"cats":{"new-dataset":0.0898155201,"dev-research":0.3764220156,"prompt-eng":0.4560536042,"data-quality":0.2904553277,"ml-security":0.0403240886}}
{"text":"We argue that automatic taxonomy evaluation (ATE) is just as important as taxonomy construction.","meta":{"url":"http://arxiv.org/abs/2307.09706v1"},"cats":{"new-dataset":0.0402120973,"dev-research":0.4407535605,"prompt-eng":0.4479014979,"data-quality":0.3421272057,"ml-security":0.0583383286}}
{"text":"We propose RaTE, an automatic label-free taxonomy scoring procedure, which relies on a large pre-trained language model.","meta":{"url":"http://arxiv.org/abs/2307.09706v1"},"cats":{"new-dataset":0.3233798248,"dev-research":0.3168991582,"prompt-eng":0.4821399538,"data-quality":0.4784034162,"ml-security":0.0724022823}}
{"text":"We apply our evaluation procedure to three state-of-the-art ATC algorithms with which we built seven taxonomies from the Yelp domain, and show that 1) RaTE correlates well with human judgments and 2) artificially degrading a taxonomy leads to decreasing RaTE score.","meta":{"url":"http://arxiv.org/abs/2307.09706v1"},"cats":{"new-dataset":0.0665656239,"dev-research":0.2828902749,"prompt-eng":0.4480085583,"data-quality":0.3159757811,"ml-security":0.0861024113}}
{"text":"With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.0542280857,"dev-research":0.2503893299,"prompt-eng":0.4895608089,"data-quality":0.2040578059,"ml-security":0.3689937608}}
{"text":"Therefore, evaluation of human values alignment is becoming increasingly important.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.0721673312,"dev-research":0.4169796162,"prompt-eng":0.4211898192,"data-quality":0.1766431613,"ml-security":0.0551098017}}
{"text":"Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.0459631192,"dev-research":0.279628966,"prompt-eng":0.4915004863,"data-quality":0.1300019852,"ml-security":0.0476218207}}
{"text":"In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.1812943108,"dev-research":0.233120709,"prompt-eng":0.4395581033,"data-quality":0.1557881733,"ml-security":0.0822040038}}
{"text":"As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.1659138029,"dev-research":0.4016284515,"prompt-eng":0.5044724459,"data-quality":0.2795039749,"ml-security":0.7814175705}}
{"text":"To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.0482637232,"dev-research":0.2689041688,"prompt-eng":0.5715453087,"data-quality":0.1894575368,"ml-security":0.0510962723}}
{"text":"Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.0848588591,"dev-research":0.2456540343,"prompt-eng":0.4529291715,"data-quality":0.1506054407,"ml-security":0.1634147523}}
{"text":"Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.0770800989,"dev-research":0.4124680147,"prompt-eng":0.4774996296,"data-quality":0.2527061056,"ml-security":0.0367641071}}
{"text":"The benchmark and code is available on ModelScope and Github.","meta":{"url":"http://arxiv.org/abs/2307.09705v1"},"cats":{"new-dataset":0.2500134304,"dev-research":0.243936889,"prompt-eng":0.416913966,"data-quality":0.0923145337,"ml-security":0.0406244638}}
{"text":"As part of the data-driven paradigm and open science movement, the data paper is becoming a popular way for researchers to publish their research data, based on academic norms that cross knowledge domains.","meta":{"url":"http://arxiv.org/abs/2307.09704v1"},"cats":{"new-dataset":0.5084764335,"dev-research":0.3769583079,"prompt-eng":0.3249284386,"data-quality":0.0985475616,"ml-security":0.1514305076}}
{"text":"Data journals have also been created to host this new academic genre.","meta":{"url":"http://arxiv.org/abs/2307.09704v1"},"cats":{"new-dataset":0.6044151956,"dev-research":0.3162785204,"prompt-eng":0.3382613115,"data-quality":0.0968768071,"ml-security":0.1182824447}}
{"text":"The growing number of data papers and journals has made them an important large-scale data source for understanding how research data is published and reused in our research system.","meta":{"url":"http://arxiv.org/abs/2307.09704v1"},"cats":{"new-dataset":0.5496662405,"dev-research":0.3569742613,"prompt-eng":0.3264954352,"data-quality":0.1213194051,"ml-security":0.0974767763}}
{"text":"One barrier to this research agenda is a lack of knowledge as to how data journals and their publications are indexed in the scholarly databases used for quantitative analysis.","meta":{"url":"http://arxiv.org/abs/2307.09704v1"},"cats":{"new-dataset":0.1098899597,"dev-research":0.2742997501,"prompt-eng":0.2839480327,"data-quality":0.1552282795,"ml-security":0.1583201682}}
{"text":"To address this gap, this study examines how a list of 18 exclusively data journals (i.e., journals that primarily accept data papers) are indexed in four popular scholarly databases: the Web of Science, Scopus, Dimensions, and OpenAlex.","meta":{"url":"http://arxiv.org/abs/2307.09704v1"},"cats":{"new-dataset":0.2732269132,"dev-research":0.2113223253,"prompt-eng":0.3035237549,"data-quality":0.0889402458,"ml-security":0.0664982222}}
{"text":"We investigate how comprehensively these databases cover the selected data journals and, in particular, how they present the document type information of data papers.","meta":{"url":"http://arxiv.org/abs/2307.09704v1"},"cats":{"new-dataset":0.4422100938,"dev-research":0.2953908742,"prompt-eng":0.3487394844,"data-quality":0.1776546794,"ml-security":0.0892393034}}
{"text":"We find that the coverage of data papers, as well as their document type information, is highly inconsistent across databases, which creates major challenges for future efforts to study them quantitatively.","meta":{"url":"http://arxiv.org/abs/2307.09704v1"},"cats":{"new-dataset":0.1730502341,"dev-research":0.3406035269,"prompt-eng":0.3532211335,"data-quality":0.3348614307,"ml-security":0.1171703309}}
{"text":"As a result, we argue that efforts should be made by data journals and databases to improve the quality of metadata for this emerging genre.","meta":{"url":"http://arxiv.org/abs/2307.09704v1"},"cats":{"new-dataset":0.4403555487,"dev-research":0.3661587015,"prompt-eng":0.3550422418,"data-quality":0.3826184774,"ml-security":0.0822179615}}
{"text":"In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars.","meta":{"url":"http://arxiv.org/abs/2307.09702v1"},"cats":{"new-dataset":0.2442211712,"dev-research":0.2589655179,"prompt-eng":0.5085067598,"data-quality":0.231278915,"ml-security":0.0574745687}}
{"text":"Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice.","meta":{"url":"http://arxiv.org/abs/2307.09702v1"},"cats":{"new-dataset":0.0598625521,"dev-research":0.2918208212,"prompt-eng":0.4584027979,"data-quality":0.1609379771,"ml-security":0.0884850322}}
{"text":"An implementation is provided in the open source Python library Outlines.","meta":{"url":"http://arxiv.org/abs/2307.09702v1"},"cats":{"new-dataset":0.2964767773,"dev-research":0.2412626577,"prompt-eng":0.4302642866,"data-quality":0.1075556999,"ml-security":0.0367510373}}
{"text":"Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.2001175167,"dev-research":0.3642377984,"prompt-eng":0.3771907399,"data-quality":0.1613607042,"ml-security":0.1379118766}}
{"text":"Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.004546358,"dev-research":0.3165761286,"prompt-eng":0.4027413368,"data-quality":0.1067748031,"ml-security":0.0859632868}}
{"text":"For example, hardware is challenging to control due to disparate levels of accessibility across different institutions.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.0290651025,"dev-research":0.3449078926,"prompt-eng":0.3784190205,"data-quality":0.0859004205,"ml-security":0.1575547991}}
{"text":"Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.0204835542,"dev-research":0.3727895729,"prompt-eng":0.3470729288,"data-quality":0.1604877709,"ml-security":0.1431389991}}
{"text":"In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.054219404,"dev-research":0.3027914914,"prompt-eng":0.4092478418,"data-quality":0.0869466258,"ml-security":0.0718134662}}
{"text":"Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.0583481977,"dev-research":0.3285382899,"prompt-eng":0.3855980363,"data-quality":0.0589297727,"ml-security":0.0898609949}}
{"text":"It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.0522285381,"dev-research":0.373042982,"prompt-eng":0.4104079918,"data-quality":0.0411079508,"ml-security":0.1217867775}}
{"text":"It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.0407545445,"dev-research":0.3725087244,"prompt-eng":0.4066444692,"data-quality":0.0705045271,"ml-security":0.0413278968}}
{"text":"Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.3480521285,"dev-research":0.4821651852,"prompt-eng":0.3982111461,"data-quality":0.0872974956,"ml-security":0.0633355274}}
{"text":"As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.0458858694,"dev-research":0.4506495856,"prompt-eng":0.3960710886,"data-quality":0.0843041681,"ml-security":0.0659742679}}
{"text":"While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.1221509816,"dev-research":0.3471451063,"prompt-eng":0.4013114473,"data-quality":0.1811967016,"ml-security":0.0729725462}}
{"text":"We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.","meta":{"url":"http://arxiv.org/abs/2307.09701v1"},"cats":{"new-dataset":0.2037494641,"dev-research":0.3639124191,"prompt-eng":0.4007706367,"data-quality":0.1357259134,"ml-security":0.1158555586}}
{"text":"Multiplayer Online Battle Arenas (MOBAs) have garnered a substantial player base worldwide.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.1622118137,"dev-research":0.2970243138,"prompt-eng":0.3452689268,"data-quality":0.0852670169,"ml-security":0.0922639115}}
{"text":"Nevertheless, the presence of noxious players, commonly referred to as \"actors\", can significantly compromise game fairness by exhibiting negative behaviors that diminish their team's competitive edge.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.0295800186,"dev-research":0.3537966843,"prompt-eng":0.3694992546,"data-quality":0.1633414446,"ml-security":0.3175165711}}
{"text":"Furthermore, high-level actors tend to engage in more egregious conduct to evade detection, thereby causing harm to the game community and necessitating their identification.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.0451387974,"dev-research":0.3367498484,"prompt-eng":0.4138887425,"data-quality":0.1610590348,"ml-security":0.5495953147}}
{"text":"To tackle this urgent concern, a partnership was formed with a team of game specialists from a prominent company to facilitate the identification and labeling of high-level actors in MOBA games.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.3389748993,"dev-research":0.3061648902,"prompt-eng":0.3884296482,"data-quality":0.1117383898,"ml-security":0.0814141024}}
{"text":"We first characterize the problem and abstract data and events from the game scene to formulate design requirements.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.4532767005,"dev-research":0.3812757647,"prompt-eng":0.4431497569,"data-quality":0.0861298608,"ml-security":0.0937538407}}
{"text":"Subsequently, ActorLens, a visual analytics system, was developed to exclude low-level actors, detect potential high-level actors, and assist users in labeling players.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.1529867713,"dev-research":0.3615871341,"prompt-eng":0.4024583265,"data-quality":0.1594375168,"ml-security":0.1058193074}}
{"text":"ActorLens furnishes an overview of players' status, summarizes behavioral patterns across three player cohorts (namely, focused players, historical matches of focused players, and matches of other players who played the same hero), and synthesizes key match events.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.0896026321,"dev-research":0.3118210141,"prompt-eng":0.4030569591,"data-quality":0.0737464048,"ml-security":0.0620291743}}
{"text":"By incorporating multiple views of information, users can proficiently recognize and label high-level actors in MOBA games.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.0688356522,"dev-research":0.3112582533,"prompt-eng":0.4218024988,"data-quality":0.1586261447,"ml-security":0.1193194187}}
{"text":"We conducted case studies and user studies to demonstrate the efficacy of the system.","meta":{"url":"http://arxiv.org/abs/2307.09699v1"},"cats":{"new-dataset":0.0269992373,"dev-research":0.3623490742,"prompt-eng":0.445411156,"data-quality":0.0718499118,"ml-security":0.1390275948}}
{"text":"With recent advances in computing hardware and surges of deep-learning architectures, learning-based deep image registration methods have surpassed their traditional counterparts, in terms of metric performance and inference time.","meta":{"url":"http://arxiv.org/abs/2307.09696v1"},"cats":{"new-dataset":0.100490993,"dev-research":0.2624935523,"prompt-eng":0.3960983444,"data-quality":0.1108207941,"ml-security":0.1200215013}}
{"text":"However, these methods focus on improving performance measurements such as Dice, resulting in less attention given to model behaviors that are equally desirable for registrations, especially for medical imaging.","meta":{"url":"http://arxiv.org/abs/2307.09696v1"},"cats":{"new-dataset":0.0144293668,"dev-research":0.3513929077,"prompt-eng":0.4340347405,"data-quality":0.1080021328,"ml-security":0.0914788532}}
{"text":"This paper investigates these behaviors for popular learning-based deep registrations under a sanity-checking microscope.","meta":{"url":"http://arxiv.org/abs/2307.09696v1"},"cats":{"new-dataset":0.0813059719,"dev-research":0.2853388034,"prompt-eng":0.4527307717,"data-quality":0.2222660641,"ml-security":0.1793392255}}
{"text":"We find that most existing registrations suffer from low inverse consistency and nondiscrimination of identical pairs due to overly optimized image similarities.","meta":{"url":"http://arxiv.org/abs/2307.09696v1"},"cats":{"new-dataset":0.0709636276,"dev-research":0.2118269086,"prompt-eng":0.4016035436,"data-quality":0.2299854907,"ml-security":0.0869955126}}
{"text":"To rectify these behaviors, we propose a novel regularization-based sanity-enforcer method that imposes two sanity checks on the deep model to reduce its inverse consistency errors and increase its discriminative power simultaneously.","meta":{"url":"http://arxiv.org/abs/2307.09696v1"},"cats":{"new-dataset":0.0400796099,"dev-research":0.3261368988,"prompt-eng":0.4440872424,"data-quality":0.4969810043,"ml-security":0.3046576697}}
{"text":"Moreover, we derive a set of theoretical guarantees for our sanity-checked image registration method, with experimental results supporting our theoretical findings and their effectiveness in increasing the sanity of models without sacrificing any performance.","meta":{"url":"http://arxiv.org/abs/2307.09696v1"},"cats":{"new-dataset":0.0261727587,"dev-research":0.2493497566,"prompt-eng":0.4490649016,"data-quality":0.2203692894,"ml-security":0.0863523676}}
{"text":"Our code and models are available at \\url{https://github.com/tuffr5/Saner-deep-registration}.","meta":{"url":"http://arxiv.org/abs/2307.09696v1"},"cats":{"new-dataset":0.1332911383,"dev-research":0.2171867819,"prompt-eng":0.4378228234,"data-quality":0.1186141487,"ml-security":0.0983679611}}
{"text":"Modeling and designing urban building layouts is of significant interest in computer vision, computer graphics, and urban applications.","meta":{"url":"http://arxiv.org/abs/2307.09693v1"},"cats":{"new-dataset":0.1659292582,"dev-research":0.3165725266,"prompt-eng":0.4172139826,"data-quality":0.0470222889,"ml-security":0.0684384422}}
{"text":"A building layout consists of a set of buildings in city blocks defined by a network of roads.","meta":{"url":"http://arxiv.org/abs/2307.09693v1"},"cats":{"new-dataset":0.3342123807,"dev-research":0.3690485292,"prompt-eng":0.3667202456,"data-quality":0.0664409311,"ml-security":0.0962736498}}
{"text":"We observe that building layouts are discrete structures, consisting of multiple rows of buildings of various shapes, and are amenable to skeletonization for mapping arbitrary city block shapes to a canonical form.","meta":{"url":"http://arxiv.org/abs/2307.09693v1"},"cats":{"new-dataset":0.1259078533,"dev-research":0.2664863538,"prompt-eng":0.3871718197,"data-quality":0.0577742275,"ml-security":0.0593715148}}
{"text":"Hence, we propose a fully automatic approach to building layout generation using graph attention networks.","meta":{"url":"http://arxiv.org/abs/2307.09693v1"},"cats":{"new-dataset":0.2019006272,"dev-research":0.3779160542,"prompt-eng":0.4691016222,"data-quality":0.1372155536,"ml-security":0.0460920606}}
{"text":"Our method generates realistic urban layouts given arbitrary road networks, and enables conditional generation based on learned priors.","meta":{"url":"http://arxiv.org/abs/2307.09693v1"},"cats":{"new-dataset":0.2394252338,"dev-research":0.3054213544,"prompt-eng":0.4468103229,"data-quality":0.1029304732,"ml-security":0.129396779}}
{"text":"Our results, including user study, demonstrate superior performance as compared to prior layout generation networks, support arbitrary city block and varying building shapes as demonstrated by generating layouts for 28 large cities.","meta":{"url":"http://arxiv.org/abs/2307.09693v1"},"cats":{"new-dataset":0.1341172606,"dev-research":0.3993127184,"prompt-eng":0.4333954738,"data-quality":0.0571086761,"ml-security":0.0635041855}}
{"text":"Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.0314054772,"dev-research":0.2349957026,"prompt-eng":0.4651847958,"data-quality":0.0614486237,"ml-security":0.0894470734}}
{"text":"However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.0418693524,"dev-research":0.3123609838,"prompt-eng":0.464039946,"data-quality":0.2163905638,"ml-security":0.0677613004}}
{"text":"Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.0239931217,"dev-research":0.3076079346,"prompt-eng":0.4176029616,"data-quality":0.2939131451,"ml-security":0.073523778}}
{"text":"And consistency regularization is further considered to improve the performance of semi-supervised learning.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.0388870162,"dev-research":0.2407100463,"prompt-eng":0.4314081576,"data-quality":0.4212124701,"ml-security":0.104148293}}
{"text":"However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.050065771,"dev-research":0.2852952748,"prompt-eng":0.4327602417,"data-quality":0.3215148728,"ml-security":0.1721845985}}
{"text":"Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.0062587323,"dev-research":0.2461560822,"prompt-eng":0.3947457163,"data-quality":0.2347299854,"ml-security":0.0702092312}}
{"text":"Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.0036937761,"dev-research":0.198016501,"prompt-eng":0.424966266,"data-quality":0.2702805382,"ml-security":0.1226232938}}
{"text":"To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.0324030084,"dev-research":0.2345600817,"prompt-eng":0.4833574469,"data-quality":0.4181302378,"ml-security":0.2883517482}}
{"text":"Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.","meta":{"url":"http://arxiv.org/abs/2307.09692v1"},"cats":{"new-dataset":0.0641211318,"dev-research":0.230535341,"prompt-eng":0.4571351911,"data-quality":0.1030276026,"ml-security":0.1292577512}}
{"text":"Meeting the strict Quality of Service (QoS) requirements of terminals has imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems, due to the limited multidimensional resources.","meta":{"url":"http://arxiv.org/abs/2307.09691v1"},"cats":{"new-dataset":0.0523038379,"dev-research":0.2663961832,"prompt-eng":0.3797684458,"data-quality":0.0852262386,"ml-security":0.1236984999}}
{"text":"To address this challenge, we propose a collaborative MEC framework that facilitates resource sharing between the edge servers, and with the aim to maximize the long-term QoS and reduce the cache switching cost through joint optimization of service caching, collaborative offfoading, and computation and communication resource allocation.","meta":{"url":"http://arxiv.org/abs/2307.09691v1"},"cats":{"new-dataset":0.0629491128,"dev-research":0.3018674337,"prompt-eng":0.3612419518,"data-quality":0.0804103679,"ml-security":0.1091879994}}
{"text":"The dual timescale feature and temporal recurrence relationship between service caching and other resource allocation make solving the problem even more challenging.","meta":{"url":"http://arxiv.org/abs/2307.09691v1"},"cats":{"new-dataset":0.049739051,"dev-research":0.2310525463,"prompt-eng":0.3782166936,"data-quality":0.0957413695,"ml-security":0.0956803244}}
{"text":"To solve it, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG).","meta":{"url":"http://arxiv.org/abs/2307.09691v1"},"cats":{"new-dataset":0.1041796667,"dev-research":0.1754089611,"prompt-eng":0.3888929252,"data-quality":0.0374629546,"ml-security":0.1979175306}}
{"text":"In doing so, we reformulate the optimization problem as a Markov decision process (MDP) where the small-timescale resource allocation decisions generated by an improved GA are taken as the states and input into a centralized LSTM-DDPG agent to generate the service caching decision for the large-timescale.","meta":{"url":"http://arxiv.org/abs/2307.09691v1"},"cats":{"new-dataset":0.0396331643,"dev-research":0.1503411196,"prompt-eng":0.4067861453,"data-quality":0.0587723716,"ml-security":0.1227817062}}
{"text":"Simulation results demonstrate that our proposed algorithm outperforms the baseline algorithms in terms of the average QoS and cache switching cost.","meta":{"url":"http://arxiv.org/abs/2307.09691v1"},"cats":{"new-dataset":0.0201771747,"dev-research":0.2318555866,"prompt-eng":0.3618362762,"data-quality":0.0652065078,"ml-security":0.063018408}}
{"text":"Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.0122120836,"dev-research":0.3234544398,"prompt-eng":0.4647046785,"data-quality":0.0750270558,"ml-security":0.0843406524}}
{"text":"Thus, accurately understanding customer preferences is essential for providing personalized recommendations.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.0184205636,"dev-research":0.3884293954,"prompt-eng":0.4468082344,"data-quality":0.1491448806,"ml-security":0.1011252377}}
{"text":"Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.1250023603,"dev-research":0.3168157329,"prompt-eng":0.4284916893,"data-quality":0.0750849908,"ml-security":0.1196442021}}
{"text":"However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.2680368645,"dev-research":0.2255180206,"prompt-eng":0.3008551138,"data-quality":0.0956933394,"ml-security":0.1916773162}}
{"text":"As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.0167008689,"dev-research":0.3104255326,"prompt-eng":0.3863128904,"data-quality":0.1744709339,"ml-security":0.2220942366}}
{"text":"To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.8232499703,"dev-research":0.2400694384,"prompt-eng":0.3929529746,"data-quality":0.1925217777,"ml-security":0.0555873752}}
{"text":"It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.9109957478,"dev-research":0.3249812682,"prompt-eng":0.3831833038,"data-quality":0.1797282398,"ml-security":0.0743754364}}
{"text":"Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.4706539483,"dev-research":0.4310045528,"prompt-eng":0.4222227857,"data-quality":0.0936506352,"ml-security":0.1675460539}}
{"text":"To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.4615166518,"dev-research":0.2622135745,"prompt-eng":0.4460953142,"data-quality":0.1832206947,"ml-security":0.0766458932}}
{"text":"With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.6659812069,"dev-research":0.2399608255,"prompt-eng":0.3292653034,"data-quality":0.1426218013,"ml-security":0.097980445}}
{"text":"In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 and have attracted thousands of users and submissions.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.5809797434,"dev-research":0.3078838813,"prompt-eng":0.4339870936,"data-quality":0.1108436233,"ml-security":0.1488132531}}
{"text":"The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.","meta":{"url":"http://arxiv.org/abs/2307.09688v1"},"cats":{"new-dataset":0.5017980772,"dev-research":0.2736195698,"prompt-eng":0.4050520696,"data-quality":0.0931513556,"ml-security":0.0754798402}}
{"text":"Biomedical research yields a wealth of information, much of which is only accessible through the literature.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.0818347734,"dev-research":0.2544359115,"prompt-eng":0.3179269705,"data-quality":0.0624426212,"ml-security":0.1174757324}}
{"text":"Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.0800536873,"dev-research":0.3152234719,"prompt-eng":0.3864250256,"data-quality":0.0901033503,"ml-security":0.0584679472}}
{"text":"Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.0146895986,"dev-research":0.3891996168,"prompt-eng":0.3668009203,"data-quality":0.1316934368,"ml-security":0.1697338072}}
{"text":"In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.2030147128,"dev-research":0.2755631567,"prompt-eng":0.3773353002,"data-quality":0.0977466044,"ml-security":0.0652250649}}
{"text":"We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.1607692128,"dev-research":0.299979538,"prompt-eng":0.3682350144,"data-quality":0.1122591312,"ml-security":0.1052240205}}
{"text":"We then describe literature search tools catering to five specific information needs: 1.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.2129707477,"dev-research":0.3040409909,"prompt-eng":0.4130197498,"data-quality":0.1301481973,"ml-security":0.0463356351}}
{"text":"Identifying high-quality clinical research for evidence-based medicine.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.1691110396,"dev-research":0.3255479138,"prompt-eng":0.371525604,"data-quality":0.1626741192,"ml-security":0.0758208658}}
{"text":"2. Retrieving gene-related information for precision medicine and genomics.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.1649082758,"dev-research":0.3313103284,"prompt-eng":0.4113887451,"data-quality":0.1742849834,"ml-security":0.0701518153}}
{"text":"3. Searching by meaning, including natural language questions.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.105836825,"dev-research":0.3602153317,"prompt-eng":0.446242395,"data-quality":0.2434046436,"ml-security":0.0619014178}}
{"text":"4. Locating related articles with literature recommendation.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.1472708688,"dev-research":0.2833097116,"prompt-eng":0.3703049072,"data-quality":0.1374649311,"ml-security":0.0479560587}}
{"text":"5. Mining literature to discover associations between concepts such as diseases and genetic variants.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.0843995707,"dev-research":0.3629318479,"prompt-eng":0.4242907771,"data-quality":0.1963550599,"ml-security":0.1489913178}}
{"text":"Additionally, we cover practical considerations and best practices for choosing and using these tools.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.0584778909,"dev-research":0.4782328382,"prompt-eng":0.4265220744,"data-quality":0.0772517342,"ml-security":0.0790959882}}
{"text":"Finally, we provide a perspective on the future of literature search engines, considering recent breakthroughs in large language models such as ChatGPT.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.1659886679,"dev-research":0.2122735023,"prompt-eng":0.4052677141,"data-quality":0.1410881275,"ml-security":0.084717948}}
{"text":"In summary, our survey provides a comprehensive view of biomedical literature search functionalities with 36 publicly available tools.","meta":{"url":"http://arxiv.org/abs/2307.09683v1"},"cats":{"new-dataset":0.121957532,"dev-research":0.2232468545,"prompt-eng":0.3701403696,"data-quality":0.1081328308,"ml-security":0.0679002101}}
{"text":"Game comonads give a categorical semantics for comparison games in Finite Model Theory, thus providing an abstract characterisation of logical equivalence for a wide range of logics, each one captured through a specific choice of comonad.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0643141893,"dev-research":0.2875930378,"prompt-eng":0.444125887,"data-quality":0.0664474149,"ml-security":0.0881710667}}
{"text":"However, data-aware logics such as CoreDataXPath present sophisticated notions of bisimulation which defy a straightforward comonadic encoding.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.087740605,"dev-research":0.3122153153,"prompt-eng":0.4284458549,"data-quality":0.138892459,"ml-security":0.1044445319}}
{"text":"In this work we begin the comonadic treatment of data-aware logics by introducing a generalisation of Modal Logic that allows relation symbols of arbitrary arity as atoms of the syntax, which we call Path Predicate Modal Logic or PPML.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0686725181,"dev-research":0.301496921,"prompt-eng":0.4719949046,"data-quality":0.1151140393,"ml-security":0.081225382}}
{"text":"We motivate this logic as arising from a shift in perspective on a already studied restricted version of CoreDataXPath, called DataGL, and prove that PPML recovers DataGL for a specific choice of signature.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0888515528,"dev-research":0.316164407,"prompt-eng":0.3766398001,"data-quality":0.1689459583,"ml-security":0.1625418148}}
{"text":"We argue that this shift in perspective allows the capturing and designing of new data-aware logics.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.1339130934,"dev-research":0.4318387499,"prompt-eng":0.411009724,"data-quality":0.0831769301,"ml-security":0.1282433542}}
{"text":"On the other hand, PPML enjoys an intrinsic motivation in that it extends Modal Logic to predicate over more general models.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0048284156,"dev-research":0.3049536359,"prompt-eng":0.4490394269,"data-quality":0.0804697287,"ml-security":0.0660995224}}
{"text":"Having defined the simulation and bisimulation games for PPML and having proven a Hennessy-Milner-type theorem, we define the PPML comonad and prove that it captures these games, following analogous results in the literature.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0913149628,"dev-research":0.2015816718,"prompt-eng":0.422008943,"data-quality":0.0738835458,"ml-security":0.0917017079}}
{"text":"Our treatment is novel in that we explicitly prove that our comonad satisfies the axioms of arboreal categories and arboreal covers.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0531673821,"dev-research":0.2857618399,"prompt-eng":0.3973333698,"data-quality":0.15676992,"ml-security":0.0963230169}}
{"text":"Using the comonadic machinery, we immediately obtain a tree-model property for PPML.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0362138907,"dev-research":0.1788933019,"prompt-eng":0.4737602849,"data-quality":0.111661936,"ml-security":0.0508161105}}
{"text":"Finally, we define a translation functor from relational structures into Kripke structures and use its properties to prove a series of polynomial-time reductions from PPML problems to their Basic Modal Logic counterparts.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0436110213,"dev-research":0.2722354302,"prompt-eng":0.4343205448,"data-quality":0.1361647192,"ml-security":0.0644211498}}
{"text":"Our results explain precisely in what sense PPML lets us view general relational structures through the modal lens.","meta":{"url":"http://arxiv.org/abs/2307.09679v1"},"cats":{"new-dataset":0.0484485129,"dev-research":0.3105613069,"prompt-eng":0.4423490198,"data-quality":0.1292539173,"ml-security":0.049315699}}
{"text":"Most object detection models for autonomous driving may experience a significant drop in performance when deployed in real-world applications, due to the well-known domain shift issue.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.028400001,"dev-research":0.2453375135,"prompt-eng":0.3952755507,"data-quality":0.1918673343,"ml-security":0.1692507555}}
{"text":"Supervised object detection methods for autonomous driving usually assume a consistent feature distribution between training and testing data, however, such assumptions may not always be the case when weather conditions differ significantly.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.0404520452,"dev-research":0.2714692568,"prompt-eng":0.4309205854,"data-quality":0.289591532,"ml-security":0.3009027598}}
{"text":"For example, an object detection model trained under clear weather may not perform well in foggy or rainy weather, due to the domain gap.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.0378720836,"dev-research":0.2538828899,"prompt-eng":0.3970212976,"data-quality":0.2013643843,"ml-security":0.1777203263}}
{"text":"Overcoming detection bottlenecks in foggy or rainy weather scenarios is a significant challenge for autonomous vehicles deployed in the wild.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.1167995708,"dev-research":0.2762610405,"prompt-eng":0.3886602532,"data-quality":0.1514737974,"ml-security":0.231351813}}
{"text":"To address the domain gap in different weather conditions, This paper proposes a novel domain adaptive object detection framework for autonomous driving in foggy and rainy weather.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.1498671487,"dev-research":0.2516502709,"prompt-eng":0.375677468,"data-quality":0.1390830544,"ml-security":0.1505795467}}
{"text":"Our method leverages both image-level and object-level adaptation to reduce the domain discrepancy in image style and object appearance.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.0247448794,"dev-research":0.2931593528,"prompt-eng":0.454346473,"data-quality":0.2941325635,"ml-security":0.0550899815}}
{"text":"Additionally, to enhance the model's performance under challenging samples, we introduce a new adversarial gradient reversal layer that performs adversarial mining on hard examples alongside domain adaptation.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.0951524865,"dev-research":0.2572407462,"prompt-eng":0.4195014366,"data-quality":0.2951760208,"ml-security":0.5535146386}}
{"text":"Moreover, we propose to generate an auxiliary domain by data augmentation to enforce a new domain-level metric regularization.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.1367777989,"dev-research":0.3276428234,"prompt-eng":0.4361686544,"data-quality":0.3255822098,"ml-security":0.1108958596}}
{"text":"Experimental results on public benchmarks demonstrate that object detection performance is significantly improved when using our proposed method in domain shift scenarios for autonomous driving applications.","meta":{"url":"http://arxiv.org/abs/2307.09676v1"},"cats":{"new-dataset":0.035497511,"dev-research":0.2414223157,"prompt-eng":0.3919404794,"data-quality":0.1614656505,"ml-security":0.1526176321}}
{"text":"We often see the term explainable in the titles of papers that describe applications based on artificial intelligence (AI).","meta":{"url":"http://arxiv.org/abs/2307.09673v1"},"cats":{"new-dataset":0.0179157682,"dev-research":0.3828489942,"prompt-eng":0.3944020325,"data-quality":0.1664298219,"ml-security":0.213570166}}
{"text":"However, the literature in explainable artificial intelligence (XAI) indicates that explanations in XAI are application- and domain-specific, hence requiring evaluation whenever they are employed to explain a model that makes decisions for a specific application problem.","meta":{"url":"http://arxiv.org/abs/2307.09673v1"},"cats":{"new-dataset":0.0167102594,"dev-research":0.4844627887,"prompt-eng":0.4271776642,"data-quality":0.1584415776,"ml-security":0.2029629621}}
{"text":"Additionally, the literature reveals that the performance of post-hoc methods, particularly feature attribution methods, varies substantially hinting that they do not represent a solution to AI explainability.","meta":{"url":"http://arxiv.org/abs/2307.09673v1"},"cats":{"new-dataset":0.0092724435,"dev-research":0.4443957851,"prompt-eng":0.4128510351,"data-quality":0.2228340187,"ml-security":0.2342711132}}
{"text":"Therefore, when using XAI methods, the quality and suitability of their information outputs should be evaluated within the specific application.","meta":{"url":"http://arxiv.org/abs/2307.09673v1"},"cats":{"new-dataset":0.0449912238,"dev-research":0.3907587293,"prompt-eng":0.3845761901,"data-quality":0.2655652174,"ml-security":0.0886959334}}
{"text":"For these reasons, we used a scoping review methodology to investigate papers that apply AI models and adopt methods to generate post-hoc explanations while referring to said models as explainable.","meta":{"url":"http://arxiv.org/abs/2307.09673v1"},"cats":{"new-dataset":0.0232805187,"dev-research":0.4307185371,"prompt-eng":0.4236002367,"data-quality":0.1544672524,"ml-security":0.1439601702}}
{"text":"This paper investigates whether the term explainable model is adopted by authors under the assumption that incorporating a post-hoc XAI method suffices to characterize a model as explainable.","meta":{"url":"http://arxiv.org/abs/2307.09673v1"},"cats":{"new-dataset":0.0125626352,"dev-research":0.3573509266,"prompt-eng":0.4272603653,"data-quality":0.1615315796,"ml-security":0.1975401085}}
{"text":"To inspect this problem, our review analyzes whether these papers conducted evaluations.","meta":{"url":"http://arxiv.org/abs/2307.09673v1"},"cats":{"new-dataset":0.0311629876,"dev-research":0.3341662587,"prompt-eng":0.4277472143,"data-quality":0.2609973834,"ml-security":0.0506158474}}
{"text":"We found that 81% of the application papers that refer to their approaches as an explainable model do not conduct any form of evaluation on the XAI method they used.","meta":{"url":"http://arxiv.org/abs/2307.09673v1"},"cats":{"new-dataset":0.0093400318,"dev-research":0.3146934425,"prompt-eng":0.3599577662,"data-quality":0.1578607107,"ml-security":0.1150920206}}
{"text":"The paper uses a frame-theoretic setting to study the injectivity of a ReLU-layer on the closed ball of $\\mathbb{R}^n$ and its non-negative part.","meta":{"url":"http://arxiv.org/abs/2307.09672v1"},"cats":{"new-dataset":0.1149191839,"dev-research":0.2105364581,"prompt-eng":0.3150534138,"data-quality":0.1325344931,"ml-security":0.1162908333}}
{"text":"In particular, the interplay between the radius of the ball and the bias vector is emphasized.","meta":{"url":"http://arxiv.org/abs/2307.09672v1"},"cats":{"new-dataset":0.0119364879,"dev-research":0.2817688405,"prompt-eng":0.3857826969,"data-quality":0.1279831949,"ml-security":0.0649676799}}
{"text":"Together with a perspective from convex geometry, this leads to a computationally feasible method of verifying the injectivity of a ReLU-layer under reasonable restrictions in terms of an upper bound of the bias vector.","meta":{"url":"http://arxiv.org/abs/2307.09672v1"},"cats":{"new-dataset":0.0104863955,"dev-research":0.183911088,"prompt-eng":0.3523877191,"data-quality":0.1424088336,"ml-security":0.1591979639}}
{"text":"Explicit reconstruction formulas are provided, inspired by the duality concept from frame theory.","meta":{"url":"http://arxiv.org/abs/2307.09672v1"},"cats":{"new-dataset":0.0435053875,"dev-research":0.1688381112,"prompt-eng":0.3984387321,"data-quality":0.0828588705,"ml-security":0.0588852874}}
{"text":"All this gives rise to the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.","meta":{"url":"http://arxiv.org/abs/2307.09672v1"},"cats":{"new-dataset":0.0506523324,"dev-research":0.2162238498,"prompt-eng":0.3600532337,"data-quality":0.0873089085,"ml-security":0.1285695668}}
{"text":"Jazz pianists often uniquely interpret jazz standards.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.0335605491,"dev-research":0.3336090384,"prompt-eng":0.3635270962,"data-quality":0.1577959113,"ml-security":0.1047028758}}
{"text":"Passages from these interpretations can be viewed as sections of variation.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.0258678384,"dev-research":0.2887786597,"prompt-eng":0.4003783533,"data-quality":0.132571966,"ml-security":0.0358997431}}
{"text":"We manually extracted such variations from solo jazz piano performances.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.0847051759,"dev-research":0.2307587856,"prompt-eng":0.3999037667,"data-quality":0.1490100745,"ml-security":0.0462299162}}
{"text":"The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.795940757,"dev-research":0.2436719077,"prompt-eng":0.3554873973,"data-quality":0.1294249176,"ml-security":0.0466663991}}
{"text":"Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.4154089072,"dev-research":0.2451298513,"prompt-eng":0.3444078032,"data-quality":0.1921228334,"ml-security":0.068838089}}
{"text":"Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.3135154638,"dev-research":0.2478807094,"prompt-eng":0.3439140087,"data-quality":0.2251312306,"ml-security":0.0452386354}}
{"text":"In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.4675497625,"dev-research":0.2341235043,"prompt-eng":0.3888379629,"data-quality":0.1418133704,"ml-security":0.0844609306}}
{"text":"We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.1500938384,"dev-research":0.2026846433,"prompt-eng":0.3995903971,"data-quality":0.1988047932,"ml-security":0.0685165279}}
{"text":"Other potential applications of our dataset include expressive performance analysis and performer identification.","meta":{"url":"http://arxiv.org/abs/2307.09670v1"},"cats":{"new-dataset":0.092586379,"dev-research":0.3178185916,"prompt-eng":0.3874850362,"data-quality":0.2028851651,"ml-security":0.1684827307}}
{"text":"Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others.","meta":{"url":"http://arxiv.org/abs/2307.09668v1"},"cats":{"new-dataset":0.1417169624,"dev-research":0.3077816929,"prompt-eng":0.4867671702,"data-quality":0.1638917181,"ml-security":0.0748763379}}
{"text":"In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents.","meta":{"url":"http://arxiv.org/abs/2307.09668v1"},"cats":{"new-dataset":0.0286865125,"dev-research":0.2051821688,"prompt-eng":0.4483467663,"data-quality":0.0674750881,"ml-security":0.218419033}}
{"text":"We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09668v1"},"cats":{"new-dataset":0.1985933026,"dev-research":0.3534126935,"prompt-eng":0.413003529,"data-quality":0.0672798996,"ml-security":0.1109998091}}
{"text":"We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects.","meta":{"url":"http://arxiv.org/abs/2307.09668v1"},"cats":{"new-dataset":0.0543992182,"dev-research":0.1948974328,"prompt-eng":0.4432447808,"data-quality":0.0724496132,"ml-security":0.075620067}}
{"text":"We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.","meta":{"url":"http://arxiv.org/abs/2307.09668v1"},"cats":{"new-dataset":0.4106591543,"dev-research":0.3078615488,"prompt-eng":0.3970109955,"data-quality":0.0775003852,"ml-security":0.1128151638}}
{"text":"A Mobility Digital Twin is an emerging implementation of digital twin technology in the transportation domain, which creates digital replicas for various physical mobility entities, such as vehicles, drivers, and pedestrians.","meta":{"url":"http://arxiv.org/abs/2307.09666v1"},"cats":{"new-dataset":0.0657624461,"dev-research":0.2895657541,"prompt-eng":0.3938052818,"data-quality":0.0957545217,"ml-security":0.1003053058}}
{"text":"Although a few work have investigated the applications of mobility digital twin recently, the extent to which it can facilitate safer autonomous vehicles remains insufficiently explored.","meta":{"url":"http://arxiv.org/abs/2307.09666v1"},"cats":{"new-dataset":0.0568138473,"dev-research":0.3090689269,"prompt-eng":0.3726610639,"data-quality":0.093607549,"ml-security":0.2505676673}}
{"text":"In this paper, we first propose visualization of mobility digital twin, which aims to augment the existing perception systems in connected and autonomous vehicles through twinning high-fidelity and manipulable geometry representations for causal traffic participants, such as surrounding pedestrians and vehicles, in the digital space.","meta":{"url":"http://arxiv.org/abs/2307.09666v1"},"cats":{"new-dataset":0.0812513457,"dev-research":0.2870367878,"prompt-eng":0.3888693031,"data-quality":0.0944794993,"ml-security":0.1126411786}}
{"text":"An end-to-end system framework, including image data crowdsourcing, preprocessing, offloading, and edge-assisted 3D geometry reconstruction, is designed to enable real-world development of the proposed visualization of mobility digital twin.","meta":{"url":"http://arxiv.org/abs/2307.09666v1"},"cats":{"new-dataset":0.2856795904,"dev-research":0.2922934238,"prompt-eng":0.3689760839,"data-quality":0.0726350668,"ml-security":0.0870664914}}
{"text":"We implement the proposed system framework and conduct a case study to assess the twinning fidelity and physical-to-digital synchronicity within different image sampling scenarios and wireless network conditions.","meta":{"url":"http://arxiv.org/abs/2307.09666v1"},"cats":{"new-dataset":0.0344498781,"dev-research":0.2309945721,"prompt-eng":0.3910589518,"data-quality":0.1312855204,"ml-security":0.0821213205}}
{"text":"Based on the case study, future challenges of the proposed visualization of mobility digital twin are discussed toward the end of the paper.","meta":{"url":"http://arxiv.org/abs/2307.09666v1"},"cats":{"new-dataset":0.1142241004,"dev-research":0.3138184972,"prompt-eng":0.3887694707,"data-quality":0.0771629114,"ml-security":0.093076749}}
{"text":"The ability to anticipate technical expertise and capability evolution trends globally is essential for national and global security, especially in safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging fields like artificial intelligence (AI).","meta":{"url":"http://arxiv.org/abs/2307.09665v1"},"cats":{"new-dataset":0.0657447482,"dev-research":0.3573009569,"prompt-eng":0.4062725733,"data-quality":0.0749535331,"ml-security":0.2567261323}}
{"text":"In this work, we extend traditional statistical relational learning approaches (e.g., link prediction in collaboration networks) and formulate a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations.","meta":{"url":"http://arxiv.org/abs/2307.09665v1"},"cats":{"new-dataset":0.0799682517,"dev-research":0.3214330285,"prompt-eng":0.3875864364,"data-quality":0.0886872032,"ml-security":0.0891186559}}
{"text":"We develop novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities (e.g., scientist and institution levels) in two distinct research fields.","meta":{"url":"http://arxiv.org/abs/2307.09665v1"},"cats":{"new-dataset":0.0747119238,"dev-research":0.3821841378,"prompt-eng":0.4461957021,"data-quality":0.0584711174,"ml-security":0.0849550428}}
{"text":"We implement a dynamic graph transformer (DGT) neural architecture, which pushes the state-of-the-art graph neural network models by (a) forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b) relying on both discrete -- and continuous -- time inputs.","meta":{"url":"http://arxiv.org/abs/2307.09665v1"},"cats":{"new-dataset":0.0372331941,"dev-research":0.2090739852,"prompt-eng":0.3213668637,"data-quality":0.0820411009,"ml-security":0.1373206717}}
{"text":"We demonstrate that our DGT models predict collaboration, partnership, and expertise patterns with 0.26, 0.73, and 0.53 mean reciprocal rank values for AI and 0.48, 0.93, and 0.22 for NN domains.","meta":{"url":"http://arxiv.org/abs/2307.09665v1"},"cats":{"new-dataset":0.1088265088,"dev-research":0.3251460552,"prompt-eng":0.4261438172,"data-quality":0.0863454565,"ml-security":0.0719756764}}
{"text":"DGT model performance exceeds the best-performing static graph baseline models by 30-80% across AI and NN domains.","meta":{"url":"http://arxiv.org/abs/2307.09665v1"},"cats":{"new-dataset":0.1210836398,"dev-research":0.2703761118,"prompt-eng":0.3690083838,"data-quality":0.1487865229,"ml-security":0.0818011775}}
{"text":"Our findings demonstrate that DGT models boost inductive task performance, when previously unseen nodes appear in the test data, for the domains with emerging collaboration patterns (e.g., AI).","meta":{"url":"http://arxiv.org/abs/2307.09665v1"},"cats":{"new-dataset":0.0362472078,"dev-research":0.3997288756,"prompt-eng":0.4819231555,"data-quality":0.1281758631,"ml-security":0.1086435777}}
{"text":"Specifically, models accurately predict which established scientists will collaborate with early career scientists and vice-versa in the AI domain.","meta":{"url":"http://arxiv.org/abs/2307.09665v1"},"cats":{"new-dataset":0.0348670819,"dev-research":0.3526351081,"prompt-eng":0.431432354,"data-quality":0.1347635145,"ml-security":0.1357322668}}
{"text":"Gaze target detection aims to predict the image location where the person is looking and the probability that a gaze is out of the scene.","meta":{"url":"http://arxiv.org/abs/2307.09662v1"},"cats":{"new-dataset":0.0337759139,"dev-research":0.2844321244,"prompt-eng":0.4497570409,"data-quality":0.1186075631,"ml-security":0.1174208445}}
{"text":"Several works have tackled this task by regressing a gaze heatmap centered on the gaze location, however, they overlooked decoding the relationship between the people and the gazed objects.","meta":{"url":"http://arxiv.org/abs/2307.09662v1"},"cats":{"new-dataset":0.1105056395,"dev-research":0.2544203007,"prompt-eng":0.403329221,"data-quality":0.0730356293,"ml-security":0.0555483409}}
{"text":"This paper proposes a Transformer-based architecture that automatically detects objects (including heads) in the scene to build associations between every head and the gazed-head/object, resulting in a comprehensive, explainable gaze analysis composed of: gaze target area, gaze pixel point, the class and the image location of the gazed-object.","meta":{"url":"http://arxiv.org/abs/2307.09662v1"},"cats":{"new-dataset":0.0610348205,"dev-research":0.2901981071,"prompt-eng":0.4338897527,"data-quality":0.1084517419,"ml-security":0.074719319}}
{"text":"Upon evaluation of the in-the-wild benchmarks, our method achieves state-of-the-art results on all metrics (up to 2.91% gain in AUC, 50% reduction in gaze distance, and 9% gain in out-of-frame average precision) for gaze target detection and 11-13% improvement in average precision for the classification and the localization of the gazed-objects.","meta":{"url":"http://arxiv.org/abs/2307.09662v1"},"cats":{"new-dataset":0.1072762994,"dev-research":0.2551983309,"prompt-eng":0.4055187597,"data-quality":0.1719088492,"ml-security":0.0825843212}}
{"text":"The code of the proposed method is available https://github.com/francescotonini/object-aware-gaze-target-detection","meta":{"url":"http://arxiv.org/abs/2307.09662v1"},"cats":{"new-dataset":0.0813207647,"dev-research":0.2443023887,"prompt-eng":0.422726202,"data-quality":0.1245980057,"ml-security":0.107462235}}
{"text":"In the context of digital twins, structural health monitoring (SHM) constitutes the backbone of condition-based maintenance, facilitating the interconnection between virtual and physical assets.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0690392776,"dev-research":0.2856640451,"prompt-eng":0.4525810626,"data-quality":0.0930916912,"ml-security":0.1216428768}}
{"text":"Guided wave propagation (GWP) is commonly employed for the inspection of structures in SHM.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0151455891,"dev-research":0.2008179275,"prompt-eng":0.4579905509,"data-quality":0.0679926588,"ml-security":0.0425521738}}
{"text":"However, GWP is sensitive to variations in the material properties of the structure, leading to false alarms.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0086092209,"dev-research":0.2446137647,"prompt-eng":0.4574825184,"data-quality":0.1731647704,"ml-security":0.156591021}}
{"text":"In this direction, uncertainty quantification (UQ) is regularly applied to improve the reliability of predictions.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0233621221,"dev-research":0.3470207575,"prompt-eng":0.4448508297,"data-quality":0.2773353323,"ml-security":0.1123695734}}
{"text":"Computational mechanics is a useful tool for the simulation of GWP, and is often applied for UQ.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0207729984,"dev-research":0.2246195917,"prompt-eng":0.3953754535,"data-quality":0.0314513415,"ml-security":0.0390785223}}
{"text":"Even so, the application of UQ methods requires numerous simulations, while large-scale, transient numerical GWP solutions increase the computational cost.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0125965592,"dev-research":0.2125238771,"prompt-eng":0.3877103281,"data-quality":0.044652605,"ml-security":0.0806614515}}
{"text":"Reduced order models (ROMs) are commonly employed to provide numerical results in a limited amount of time.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.02928458,"dev-research":0.2218489116,"prompt-eng":0.4323299173,"data-quality":0.061892209,"ml-security":0.0789817181}}
{"text":"In this paper, we propose a machine learning (ML)-based ROM, mentioned as BO-ML-ROM, to decrease the computational time related to the simulation of the GWP.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0598341231,"dev-research":0.20513828,"prompt-eng":0.4160984065,"data-quality":0.0628567658,"ml-security":0.1192642714}}
{"text":"The ROM is integrated with a Bayesian optimization (BO) framework, to adaptively sample the parameters for the ROM training.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0608655847,"dev-research":0.2856660112,"prompt-eng":0.509512534,"data-quality":0.0813842213,"ml-security":0.0909671601}}
{"text":"The finite element method is used for the simulation of the high-fidelity models.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0129266202,"dev-research":0.1698234508,"prompt-eng":0.4000604904,"data-quality":0.0680168406,"ml-security":0.0510315621}}
{"text":"The formulated ROM is used for forward UQ of the GWP in an aluminum plate with varying material properties.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0197550472,"dev-research":0.2329484514,"prompt-eng":0.4418496512,"data-quality":0.086285441,"ml-security":0.0433136059}}
{"text":"To determine the influence of each parameter perturbation, a global, variance-based sensitivity analysis is implemented based on Sobol' indices.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0190990549,"dev-research":0.2668662678,"prompt-eng":0.4421090986,"data-quality":0.1422172178,"ml-security":0.1786378917}}
{"text":"It is shown that Bayesian optimization outperforms one-shot sampling methods, both in terms of accuracy and speed-up.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0423139928,"dev-research":0.2059393825,"prompt-eng":0.408419757,"data-quality":0.1433346188,"ml-security":0.0986309755}}
{"text":"The predicted results reveal the efficiency of BO-ML-ROM for GWP and demonstrate its value for UQ.","meta":{"url":"http://arxiv.org/abs/2307.09661v1"},"cats":{"new-dataset":0.0411807648,"dev-research":0.2240345108,"prompt-eng":0.4545101,"data-quality":0.0612298189,"ml-security":0.049784688}}
{"text":"Graph Neural Networks (GNNs) have shown considerable success in neural algorithmic reasoning.","meta":{"url":"http://arxiv.org/abs/2307.09660v1"},"cats":{"new-dataset":0.0653341383,"dev-research":0.3185378768,"prompt-eng":0.3701485118,"data-quality":0.1791651301,"ml-security":0.1651474289}}
{"text":"Many traditional algorithms make use of an explicit memory in the form of a data structure.","meta":{"url":"http://arxiv.org/abs/2307.09660v1"},"cats":{"new-dataset":0.0489820349,"dev-research":0.2943928723,"prompt-eng":0.3682043443,"data-quality":0.0775129299,"ml-security":0.145067165}}
{"text":"However, there has been limited exploration on augmenting GNNs with external memory.","meta":{"url":"http://arxiv.org/abs/2307.09660v1"},"cats":{"new-dataset":0.0426641484,"dev-research":0.2579318443,"prompt-eng":0.3581325162,"data-quality":0.0823509017,"ml-security":0.1755072607}}
{"text":"In this paper, we present Neural Priority Queues, a differentiable analogue to algorithmic priority queues, for GNNs.","meta":{"url":"http://arxiv.org/abs/2307.09660v1"},"cats":{"new-dataset":0.087709701,"dev-research":0.2113843056,"prompt-eng":0.3815764916,"data-quality":0.0975046716,"ml-security":0.2474158753}}
{"text":"We propose and motivate a desiderata for memory modules, and show that Neural PQs exhibit the desiderata, and reason about their use with algorithmic reasoning.","meta":{"url":"http://arxiv.org/abs/2307.09660v1"},"cats":{"new-dataset":0.0555947165,"dev-research":0.3115976753,"prompt-eng":0.4232356594,"data-quality":0.1133656955,"ml-security":0.1894312094}}
{"text":"This is further demonstrated by empirical results on the CLRS-30 dataset.","meta":{"url":"http://arxiv.org/abs/2307.09660v1"},"cats":{"new-dataset":0.3475045543,"dev-research":0.2506035366,"prompt-eng":0.4064223767,"data-quality":0.1683876531,"ml-security":0.0818977716}}
{"text":"Furthermore, we find the Neural PQs useful in capturing long-range interactions, as empirically shown on a dataset from the Long-Range Graph Benchmark.","meta":{"url":"http://arxiv.org/abs/2307.09660v1"},"cats":{"new-dataset":0.181081944,"dev-research":0.2361880442,"prompt-eng":0.3567895548,"data-quality":0.0716954834,"ml-security":0.1914002594}}
{"text":"Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning.","meta":{"url":"http://arxiv.org/abs/2307.09653v1"},"cats":{"new-dataset":0.0273030959,"dev-research":0.2789572625,"prompt-eng":0.4163093289,"data-quality":0.2115211325,"ml-security":0.216708571}}
{"text":"The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse.","meta":{"url":"http://arxiv.org/abs/2307.09653v1"},"cats":{"new-dataset":0.0133514609,"dev-research":0.3473770735,"prompt-eng":0.4625454856,"data-quality":0.1117517524,"ml-security":0.1214262709}}
{"text":"In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism.","meta":{"url":"http://arxiv.org/abs/2307.09653v1"},"cats":{"new-dataset":0.0162756918,"dev-research":0.256703087,"prompt-eng":0.448596948,"data-quality":0.0565794401,"ml-security":0.0697045}}
{"text":"HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules.","meta":{"url":"http://arxiv.org/abs/2307.09653v1"},"cats":{"new-dataset":0.0136694497,"dev-research":0.2790661682,"prompt-eng":0.4300838275,"data-quality":0.0778876188,"ml-security":0.1058974771}}
{"text":"It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures.","meta":{"url":"http://arxiv.org/abs/2307.09653v1"},"cats":{"new-dataset":0.0537339265,"dev-research":0.3886792762,"prompt-eng":0.4260995038,"data-quality":0.068457791,"ml-security":0.0574374937}}
{"text":"Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library.","meta":{"url":"http://arxiv.org/abs/2307.09653v1"},"cats":{"new-dataset":0.14002905,"dev-research":0.2689743197,"prompt-eng":0.42294967,"data-quality":0.0487991545,"ml-security":0.0567985299}}
{"text":"Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT, which have consistently shown improvements across various experiments.","meta":{"url":"http://arxiv.org/abs/2307.09653v1"},"cats":{"new-dataset":0.0106950818,"dev-research":0.2697461611,"prompt-eng":0.3940679912,"data-quality":0.0652645666,"ml-security":0.1308471279}}
{"text":"Our work paves the way for a broader application of the HAT mechanism, opening up new possibilities in continual learning across diverse models and applications.","meta":{"url":"http://arxiv.org/abs/2307.09653v1"},"cats":{"new-dataset":0.0131561329,"dev-research":0.2427146523,"prompt-eng":0.4354751885,"data-quality":0.0509430947,"ml-security":0.1092445454}}
{"text":"Many real-world games suffer from information asymmetry: one player is only aware of their own payoffs while the other player has the full game information.","meta":{"url":"http://arxiv.org/abs/2307.09652v1"},"cats":{"new-dataset":0.0320929836,"dev-research":0.4048995449,"prompt-eng":0.3577143514,"data-quality":0.1315140502,"ml-security":0.2134505009}}
{"text":"Examples include the critical domain of security games and adversarial multi-agent reinforcement learning.","meta":{"url":"http://arxiv.org/abs/2307.09652v1"},"cats":{"new-dataset":0.1203471517,"dev-research":0.2423798196,"prompt-eng":0.4013849904,"data-quality":0.0983744002,"ml-security":0.5728272302}}
{"text":"Information asymmetry renders traditional solution concepts such as Strong Stackelberg Equilibrium (SSE) and Robust-Optimization Equilibrium (ROE) inoperative.","meta":{"url":"http://arxiv.org/abs/2307.09652v1"},"cats":{"new-dataset":0.0062421647,"dev-research":0.2543200819,"prompt-eng":0.3995586834,"data-quality":0.1172389057,"ml-security":0.1965123092}}
{"text":"We propose a novel solution concept called VISER (Victim Is Secure, Exploiter best-Responds).","meta":{"url":"http://arxiv.org/abs/2307.09652v1"},"cats":{"new-dataset":0.0484847549,"dev-research":0.3620664115,"prompt-eng":0.4648156392,"data-quality":0.0996172872,"ml-security":0.6844761341}}
{"text":"VISER enables an external observer to predict the outcome of such games.","meta":{"url":"http://arxiv.org/abs/2307.09652v1"},"cats":{"new-dataset":0.0497247267,"dev-research":0.3582655545,"prompt-eng":0.4532978935,"data-quality":0.0490585801,"ml-security":0.1292574582}}
{"text":"In particular, for security applications, VISER allows the victim to better defend itself while characterizing the most damaging attacks available to the attacker.","meta":{"url":"http://arxiv.org/abs/2307.09652v1"},"cats":{"new-dataset":0.0133085774,"dev-research":0.4346649522,"prompt-eng":0.4416186949,"data-quality":0.0715160985,"ml-security":0.4961323042}}
{"text":"We show that each player's VISER strategy can be computed independently in polynomial time using linear programming (LP).","meta":{"url":"http://arxiv.org/abs/2307.09652v1"},"cats":{"new-dataset":0.030254314,"dev-research":0.24563035,"prompt-eng":0.365294262,"data-quality":0.0412671615,"ml-security":0.1468728596}}
{"text":"We also extend VISER to its Markov-perfect counterpart for Markov games, which can be solved efficiently using a series of LPs.","meta":{"url":"http://arxiv.org/abs/2307.09652v1"},"cats":{"new-dataset":0.0670069109,"dev-research":0.1770121831,"prompt-eng":0.4037963546,"data-quality":0.0603835005,"ml-security":0.1091691103}}
{"text":"Online communities develop unique characteristics, establish social norms, and exhibit distinct dynamics among their members.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.0452009915,"dev-research":0.388621523,"prompt-eng":0.3624270571,"data-quality":0.1093231177,"ml-security":0.1059186982}}
{"text":"Activity in online communities often results in concrete ``off-line'' actions with a broad societal impact (e.g., political street protests and norms related to sexual misconduct).","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.0582722326,"dev-research":0.3376370876,"prompt-eng":0.3399232349,"data-quality":0.1181359146,"ml-security":0.1871551467}}
{"text":"While community dynamics, information diffusion, and online collaborations have been widely studied in the past two decades, quantitative studies that measure the effectiveness of online communities in promoting their agenda are scarce.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.0339565371,"dev-research":0.3414973348,"prompt-eng":0.3487832226,"data-quality":0.0904808241,"ml-security":0.0706903064}}
{"text":"In this work, we study the correspondence between the effectiveness of a community, measured by its success level in a competitive online campaign, and the underlying dynamics between its members.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.0519240606,"dev-research":0.321708826,"prompt-eng":0.3944840402,"data-quality":0.1101285292,"ml-security":0.0579854173}}
{"text":"To this end, we define a novel task: predicting the success level of online communities in Reddit's r/place - a large-scale distributed experiment that required collaboration between community members.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.1735334856,"dev-research":0.3415776745,"prompt-eng":0.4264187738,"data-quality":0.1242565955,"ml-security":0.0633735474}}
{"text":"We consider an array of definitions for success level; each is geared toward different aspects of collaborative achievement.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.0626260525,"dev-research":0.3894017084,"prompt-eng":0.4570930952,"data-quality":0.1206858731,"ml-security":0.0397431118}}
{"text":"We experiment with several hybrid models, combining various types of features.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.0417854421,"dev-research":0.2133419649,"prompt-eng":0.4529427587,"data-quality":0.1101872863,"ml-security":0.0867813452}}
{"text":"Our models significantly outperform all baseline models over all definitions of `success level'.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.0388062817,"dev-research":0.2646523228,"prompt-eng":0.4634839451,"data-quality":0.1831559358,"ml-security":0.055240281}}
{"text":"Analysis of the results and the factors that contribute to the success of coordinated campaigns can provide a better understanding of the resilience or the vulnerability of communities to online social threats such as election interference or anti-science trends.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.0916352879,"dev-research":0.3346044131,"prompt-eng":0.3951944559,"data-quality":0.1826877643,"ml-security":0.4364984655}}
{"text":"We make all data used for this study publicly available for further research.","meta":{"url":"http://arxiv.org/abs/2307.09650v1"},"cats":{"new-dataset":0.5885123826,"dev-research":0.2786255679,"prompt-eng":0.3517372171,"data-quality":0.079592564,"ml-security":0.1283261188}}
{"text":"Spam filters are a crucial component of modern email systems, as they help to protect users from unwanted and potentially harmful emails.","meta":{"url":"http://arxiv.org/abs/2307.09649v1"},"cats":{"new-dataset":0.0231003489,"dev-research":0.28912428,"prompt-eng":0.4180160117,"data-quality":0.1717513413,"ml-security":0.324751426}}
{"text":"However, the effectiveness of these filters is dependent on the quality of the machine learning models that power them.","meta":{"url":"http://arxiv.org/abs/2307.09649v1"},"cats":{"new-dataset":0.0040039566,"dev-research":0.2891757769,"prompt-eng":0.3616990841,"data-quality":0.2196889895,"ml-security":0.2732773264}}
{"text":"In this paper, we design backdoor attacks in the domain of spam filtering.","meta":{"url":"http://arxiv.org/abs/2307.09649v1"},"cats":{"new-dataset":0.0498613593,"dev-research":0.2682293109,"prompt-eng":0.4280294536,"data-quality":0.2444985809,"ml-security":0.7518934449}}
{"text":"By demonstrating the potential vulnerabilities in the machine learning model supply chain, we highlight the need for careful consideration and evaluation of the models used in spam filters.","meta":{"url":"http://arxiv.org/abs/2307.09649v1"},"cats":{"new-dataset":0.0841509778,"dev-research":0.2833302002,"prompt-eng":0.4063827581,"data-quality":0.3010227648,"ml-security":0.6686469599}}
{"text":"Our results show that the backdoor attacks can be effectively used to identify vulnerabilities in spam filters and suggest the need for ongoing monitoring and improvement in this area.","meta":{"url":"http://arxiv.org/abs/2307.09649v1"},"cats":{"new-dataset":0.0609859332,"dev-research":0.3631499631,"prompt-eng":0.4261734268,"data-quality":0.3159074032,"ml-security":0.6819436468}}
{"text":"We study the problem of fairly allocating $m$ indivisible items among $n$ agents.","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0781026221,"dev-research":0.1630357588,"prompt-eng":0.3988644729,"data-quality":0.1067321944,"ml-security":0.1682406645}}
{"text":"Envy-free allocations, in which each agent prefers her bundle to the bundle of every other agent, need not exist in the worst case.","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0763278311,"dev-research":0.2511215409,"prompt-eng":0.4040929625,"data-quality":0.1051242744,"ml-security":0.1669849776}}
{"text":"However, when agents have additive preferences and the value $v_{i,j}$ of agent $i$ for item $j$ is drawn independently from a distribution $D_i$, envy-free allocations exist with high probability when $m \\in \\Omega( n \\log n / \\log \\log n )$.   ","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0292534541,"dev-research":0.1944996325,"prompt-eng":0.4031858281,"data-quality":0.0628632884,"ml-security":0.1728964283}}
{"text":"In this paper, we study the existence of envy-free allocations under stochastic valuations far beyond the additive setting.","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0411509691,"dev-research":0.1991105478,"prompt-eng":0.4026741246,"data-quality":0.1111940003,"ml-security":0.1794081732}}
{"text":"We introduce a new stochastic model in which each agent's valuation is sampled by first fixing a worst-case function, and then drawing a uniformly random renaming of the items, independently for each agent.","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0725392049,"dev-research":0.1871186734,"prompt-eng":0.456954416,"data-quality":0.3301987879,"ml-security":0.1657718054}}
{"text":"This strictly generalizes known settings; for example, $v_{i,j} \\sim D_i$ may be seen as picking a random (instead of a worst-case) additive function before renaming.","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0250249494,"dev-research":0.2808430601,"prompt-eng":0.3933671591,"data-quality":0.2107459966,"ml-security":0.1496343494}}
{"text":"We prove that random renaming is sufficient to ensure that envy-free allocations exist with high probability in very general settings.","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0544794457,"dev-research":0.2413826743,"prompt-eng":0.4337745933,"data-quality":0.1695104849,"ml-security":0.2922959192}}
{"text":"When valuations are non-negative and ``order-consistent,'' a valuation class that generalizes additive, budget-additive, unit-demand, and single-minded agents, SD-envy-free allocations (a stronger notion of fairness than envy-freeness) exist for $m \\in \\omega(n^2)$ when $n$ divides $m$, and SD-EFX allocations exist for all $m \\in \\omega(n^2)$. The dependence on $n$ is tight, that is, for $m \\in O(n^2)$ envy-free allocations don't exist with constant probability.","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0323649317,"dev-research":0.1749803103,"prompt-eng":0.369651237,"data-quality":0.1158020684,"ml-security":0.1385709092}}
{"text":"For the case of arbitrary valuations (allowing non-monotone, negative, or mixed-manna valuations) and $n=2$ agents, we prove envy-free allocations exist with probability $1 - \\Theta(1/m)$ (and this is tight).","meta":{"url":"http://arxiv.org/abs/2307.09648v1"},"cats":{"new-dataset":0.0461255223,"dev-research":0.146901818,"prompt-eng":0.3888159268,"data-quality":0.0964193646,"ml-security":0.1763335016}}
{"text":"Longitudinal tracking of skin lesions - finding correspondence, changes in morphology, and texture - is beneficial to the early detection of melanoma.","meta":{"url":"http://arxiv.org/abs/2307.09642v1"},"cats":{"new-dataset":0.0559404346,"dev-research":0.260069776,"prompt-eng":0.3638460484,"data-quality":0.1801671856,"ml-security":0.1005959379}}
{"text":"However, it has not been well investigated in the context of full-body imaging.","meta":{"url":"http://arxiv.org/abs/2307.09642v1"},"cats":{"new-dataset":0.0259265797,"dev-research":0.2047606289,"prompt-eng":0.3143559241,"data-quality":0.1309395342,"ml-security":0.0902576514}}
{"text":"We propose a novel framework combining geometric and texture information to localize skin lesion correspondence from a source scan to a target scan in total body photography (TBP).","meta":{"url":"http://arxiv.org/abs/2307.09642v1"},"cats":{"new-dataset":0.0862075006,"dev-research":0.2540475123,"prompt-eng":0.3995643613,"data-quality":0.1545187292,"ml-security":0.0889553998}}
{"text":"Body landmarks or sparse correspondence are first created on the source and target 3D textured meshes.","meta":{"url":"http://arxiv.org/abs/2307.09642v1"},"cats":{"new-dataset":0.0559651531,"dev-research":0.2382274592,"prompt-eng":0.4127770586,"data-quality":0.0610291093,"ml-security":0.0710003831}}
{"text":"Every vertex on each of the meshes is then mapped to a feature vector characterizing the geodesic distances to the landmarks on that mesh.","meta":{"url":"http://arxiv.org/abs/2307.09642v1"},"cats":{"new-dataset":0.0380409178,"dev-research":0.2431603598,"prompt-eng":0.4162247603,"data-quality":0.0958492636,"ml-security":0.0621858505}}
{"text":"Then, for each lesion of interest (LOI) on the source, its corresponding location on the target is first coarsely estimated using the geometric information encoded in the feature vectors and then refined using the texture information.","meta":{"url":"http://arxiv.org/abs/2307.09642v1"},"cats":{"new-dataset":0.0521041721,"dev-research":0.2432083779,"prompt-eng":0.4020315744,"data-quality":0.185153799,"ml-security":0.0663088837}}
{"text":"We evaluated the framework quantitatively on both a public and a private dataset, for which our success rates (at 10 mm criterion) are comparable to the only reported longitudinal study.","meta":{"url":"http://arxiv.org/abs/2307.09642v1"},"cats":{"new-dataset":0.0889819479,"dev-research":0.2254183605,"prompt-eng":0.3725682588,"data-quality":0.1095990963,"ml-security":0.1176851734}}
{"text":"As full-body 3D capture becomes more prevalent and has higher quality, we expect the proposed method to constitute a valuable step in the longitudinal tracking of skin lesions.","meta":{"url":"http://arxiv.org/abs/2307.09642v1"},"cats":{"new-dataset":0.0631308427,"dev-research":0.2524004347,"prompt-eng":0.3373080336,"data-quality":0.0868374598,"ml-security":0.0836322725}}
{"text":"Transport layer congestion control relies on feedback signals that travel from the congested link to the receiver and back to the sender.","meta":{"url":"http://arxiv.org/abs/2307.09639v1"},"cats":{"new-dataset":0.0192547232,"dev-research":0.3064861562,"prompt-eng":0.4260242152,"data-quality":0.0804650122,"ml-security":0.13673736}}
{"text":"This forward congestion control loop, first, requires at least one rount-trip time (RTT) to react to congestion and secondly, it depends on the downstream path after the bottleneck.","meta":{"url":"http://arxiv.org/abs/2307.09639v1"},"cats":{"new-dataset":0.0604869096,"dev-research":0.239653183,"prompt-eng":0.3970439243,"data-quality":0.0712363588,"ml-security":0.069518076}}
{"text":"The former property leads to a reaction time in the order of RTT + bottleneck queue delay, while the second may amplify the unfairness due to heterogeneous RTT.","meta":{"url":"http://arxiv.org/abs/2307.09639v1"},"cats":{"new-dataset":0.0108583584,"dev-research":0.2378897904,"prompt-eng":0.4027084012,"data-quality":0.1002978354,"ml-security":0.0887676024}}
{"text":"In this paper, we present Reverse Path Congestion Marking (RPM) to accelerate the reaction to network congestion events without changing the end-host stack.","meta":{"url":"http://arxiv.org/abs/2307.09639v1"},"cats":{"new-dataset":0.0655705498,"dev-research":0.3038734654,"prompt-eng":0.4427183333,"data-quality":0.1257587587,"ml-security":0.1785989319}}
{"text":"RPM decouples the congestion signal from the downstream path after the bottleneck while maintaining the stability of the congestion control loop.","meta":{"url":"http://arxiv.org/abs/2307.09639v1"},"cats":{"new-dataset":0.0376154763,"dev-research":0.2940919977,"prompt-eng":0.378805752,"data-quality":0.1253938188,"ml-security":0.1087310126}}
{"text":"We show that RPM improves throughput fairness for RTT-heterogeneous TCP flows as well as the flow completion time, especially for small Data Center TCP (DCTCP) flows.","meta":{"url":"http://arxiv.org/abs/2307.09639v1"},"cats":{"new-dataset":0.0690199939,"dev-research":0.2741361995,"prompt-eng":0.3484948147,"data-quality":0.0756862552,"ml-security":0.1085084877}}
{"text":"Finally, we show RPM evaluation results in a testbed built around P4 programmable ASIC switches.","meta":{"url":"http://arxiv.org/abs/2307.09639v1"},"cats":{"new-dataset":0.0886514558,"dev-research":0.3244756459,"prompt-eng":0.4563798527,"data-quality":0.0955925976,"ml-security":0.0403803655}}
{"text":"Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models.","meta":{"url":"http://arxiv.org/abs/2307.09638v1"},"cats":{"new-dataset":0.0201092678,"dev-research":0.2493006353,"prompt-eng":0.3760308019,"data-quality":0.1294120361,"ml-security":0.2040002696}}
{"text":"The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice.","meta":{"url":"http://arxiv.org/abs/2307.09638v1"},"cats":{"new-dataset":0.0023783808,"dev-research":0.2137244184,"prompt-eng":0.4051650227,"data-quality":0.0635548822,"ml-security":0.0859751437}}
{"text":"However, they often generalize worse than non-adaptive methods.","meta":{"url":"http://arxiv.org/abs/2307.09638v1"},"cats":{"new-dataset":0.0046948968,"dev-research":0.3202468776,"prompt-eng":0.3608919773,"data-quality":0.2321104881,"ml-security":0.1932158637}}
{"text":"Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization.","meta":{"url":"http://arxiv.org/abs/2307.09638v1"},"cats":{"new-dataset":0.0059735935,"dev-research":0.2069961931,"prompt-eng":0.3500735536,"data-quality":0.128138751,"ml-security":0.1130324338}}
{"text":"To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training.","meta":{"url":"http://arxiv.org/abs/2307.09638v1"},"cats":{"new-dataset":0.0470949821,"dev-research":0.2298212233,"prompt-eng":0.4274558773,"data-quality":0.129504352,"ml-security":0.1426386131}}
{"text":"Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough.","meta":{"url":"http://arxiv.org/abs/2307.09638v1"},"cats":{"new-dataset":0.0041406393,"dev-research":0.3106127413,"prompt-eng":0.3713542122,"data-quality":0.0778318259,"ml-security":0.1647254559}}
{"text":"We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.","meta":{"url":"http://arxiv.org/abs/2307.09638v1"},"cats":{"new-dataset":0.0961231453,"dev-research":0.3156959719,"prompt-eng":0.4303812253,"data-quality":0.3453453367,"ml-security":0.1331145449}}
{"text":"Video Question Answering (VidQA) exhibits remarkable potential in facilitating advanced machine reasoning capabilities within the domains of Intelligent Traffic Monitoring and Intelligent Transportation Systems.","meta":{"url":"http://arxiv.org/abs/2307.09636v1"},"cats":{"new-dataset":0.1177129435,"dev-research":0.3166962213,"prompt-eng":0.4363764725,"data-quality":0.1293832328,"ml-security":0.0942633431}}
{"text":"Nevertheless, the integration of urban traffic scene knowledge into VidQA systems has received limited attention in previous research endeavors.","meta":{"url":"http://arxiv.org/abs/2307.09636v1"},"cats":{"new-dataset":0.1972156098,"dev-research":0.2798205536,"prompt-eng":0.3583391116,"data-quality":0.1197637983,"ml-security":0.1110787866}}
{"text":"In this work, we present a novel approach termed Traffic-domain Video Question Answering with Automatic Captioning (TRIVIA), which serves as a weak-supervision technique for infusing traffic-domain knowledge into large video-language models.","meta":{"url":"http://arxiv.org/abs/2307.09636v1"},"cats":{"new-dataset":0.2572420115,"dev-research":0.3110711061,"prompt-eng":0.433983435,"data-quality":0.2492104822,"ml-security":0.1102791262}}
{"text":"Empirical findings obtained from the SUTD-TrafficQA task highlight the substantial enhancements achieved by TRIVIA, elevating the accuracy of representative video-language models by a remarkable 6.5 points (19.88%) compared to baseline settings.","meta":{"url":"http://arxiv.org/abs/2307.09636v1"},"cats":{"new-dataset":0.1283186826,"dev-research":0.3657579095,"prompt-eng":0.4458547216,"data-quality":0.3462867635,"ml-security":0.0653182987}}
{"text":"This pioneering methodology holds great promise for driving advancements in the field, inspiring researchers and practitioners alike to unlock the full potential of emerging video-language models in traffic-related applications.","meta":{"url":"http://arxiv.org/abs/2307.09636v1"},"cats":{"new-dataset":0.1071465322,"dev-research":0.2735637321,"prompt-eng":0.4066147248,"data-quality":0.1819852059,"ml-security":0.0855141123}}
{"text":"Efficient and sustainable power generation is a crucial concern in the energy sector.","meta":{"url":"http://arxiv.org/abs/2307.09483v1"},"cats":{"new-dataset":0.034804008,"dev-research":0.2867953835,"prompt-eng":0.3971897773,"data-quality":0.0888863643,"ml-security":0.0499766741}}
{"text":"In particular, thermal power plants grapple with accurately predicting steam mass flow, which is crucial for operational efficiency and cost reduction.","meta":{"url":"http://arxiv.org/abs/2307.09483v1"},"cats":{"new-dataset":0.0339602593,"dev-research":0.3187398199,"prompt-eng":0.4524593265,"data-quality":0.130166963,"ml-security":0.1217675441}}
{"text":"In this study, we use a parallel hybrid neural network architecture that combines a parametrized quantum circuit and a conventional feed-forward neural network specifically designed for time-series prediction in industrial settings to enhance predictions of steam mass flow 15 minutes into the future.","meta":{"url":"http://arxiv.org/abs/2307.09483v1"},"cats":{"new-dataset":0.0690433639,"dev-research":0.1887655878,"prompt-eng":0.3889042416,"data-quality":0.0611147916,"ml-security":0.1303554346}}
{"text":"Our results show that the parallel hybrid model outperforms standalone classical and quantum models, achieving more than 5.7 and 4.9 times lower mean squared error (MSE) loss on the test set after training compared to pure classical and pure quantum networks, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09483v1"},"cats":{"new-dataset":0.0224099489,"dev-research":0.1946439571,"prompt-eng":0.3806028861,"data-quality":0.1214007691,"ml-security":0.1422794651}}
{"text":"Furthermore, the hybrid model demonstrates smaller relative errors between the ground truth and the model predictions on the test set, up to 2 times better than the pure classical model.","meta":{"url":"http://arxiv.org/abs/2307.09483v1"},"cats":{"new-dataset":0.0288994713,"dev-research":0.2316386302,"prompt-eng":0.4056516187,"data-quality":0.1639301494,"ml-security":0.081810064}}
{"text":"These findings contribute to the broader scientific understanding of how integrating quantum and classical machine learning techniques can be applied to real-world challenges faced by the energy sector, ultimately leading to optimized power plant operations.","meta":{"url":"http://arxiv.org/abs/2307.09483v1"},"cats":{"new-dataset":0.0200907784,"dev-research":0.2816671887,"prompt-eng":0.4018889911,"data-quality":0.1167567853,"ml-security":0.1919087779}}
{"text":"This work presents AnyDoor, a diffusion-based image generator with the power to teleport target objects to new scenes at user-specified locations in a harmonious way.","meta":{"url":"http://arxiv.org/abs/2307.09481v1"},"cats":{"new-dataset":0.1679074379,"dev-research":0.2313988677,"prompt-eng":0.4473840012,"data-quality":0.0699208628,"ml-security":0.1198219014}}
{"text":"Instead of tuning parameters for each object, our model is trained only once and effortlessly generalizes to diverse object-scene combinations at the inference stage.","meta":{"url":"http://arxiv.org/abs/2307.09481v1"},"cats":{"new-dataset":0.042013961,"dev-research":0.2150926753,"prompt-eng":0.445598147,"data-quality":0.1174584034,"ml-security":0.0833389832}}
{"text":"Such a challenging zero-shot setting requires an adequate characterization of a certain object.","meta":{"url":"http://arxiv.org/abs/2307.09481v1"},"cats":{"new-dataset":0.0690304354,"dev-research":0.2082677761,"prompt-eng":0.3725700092,"data-quality":0.1699893496,"ml-security":0.1094414593}}
{"text":"To this end, we complement the commonly used identity feature with detail features, which are carefully designed to maintain texture details yet allow versatile local variations (e.g., lighting, orientation, posture, etc.), supporting the object in favorably blending with different surroundings.","meta":{"url":"http://arxiv.org/abs/2307.09481v1"},"cats":{"new-dataset":0.0270261951,"dev-research":0.3309628045,"prompt-eng":0.4138152115,"data-quality":0.1395312019,"ml-security":0.0989120964}}
{"text":"We further propose to borrow knowledge from video datasets, where we can observe various forms (i.e., along the time axis) of a single object, leading to stronger model generalizability and robustness.","meta":{"url":"http://arxiv.org/abs/2307.09481v1"},"cats":{"new-dataset":0.3900400092,"dev-research":0.2099914033,"prompt-eng":0.3591351986,"data-quality":0.1713629325,"ml-security":0.2047343305}}
{"text":"Extensive experiments demonstrate the superiority of our approach over existing alternatives as well as its great potential in real-world applications, such as virtual try-on and object moving.","meta":{"url":"http://arxiv.org/abs/2307.09481v1"},"cats":{"new-dataset":0.0209757883,"dev-research":0.3141800689,"prompt-eng":0.3997354357,"data-quality":0.0734249049,"ml-security":0.0762968361}}
{"text":"Project page is https://damo-vilab.github.io/AnyDoor-Page/.","meta":{"url":"http://arxiv.org/abs/2307.09481v1"},"cats":{"new-dataset":0.2680579281,"dev-research":0.310909778,"prompt-eng":0.4026656602,"data-quality":0.1092175529,"ml-security":0.2590220107}}
{"text":"The ability to accurately capture and express emotions is a critical aspect of creating believable characters in video games and other forms of entertainment.","meta":{"url":"http://arxiv.org/abs/2307.09480v1"},"cats":{"new-dataset":0.0712852594,"dev-research":0.4620032558,"prompt-eng":0.412112598,"data-quality":0.1444426603,"ml-security":0.1129736591}}
{"text":"Traditionally, this animation has been achieved with artistic effort or performance capture, both requiring costs in time and labor.","meta":{"url":"http://arxiv.org/abs/2307.09480v1"},"cats":{"new-dataset":0.0238342987,"dev-research":0.3193471319,"prompt-eng":0.3768190537,"data-quality":0.0523637553,"ml-security":0.0340469171}}
{"text":"More recently, audio-driven models have seen success, however, these often lack expressiveness in areas not correlated to the audio signal.","meta":{"url":"http://arxiv.org/abs/2307.09480v1"},"cats":{"new-dataset":0.0139615684,"dev-research":0.2747171539,"prompt-eng":0.392115986,"data-quality":0.2273973665,"ml-security":0.0873932232}}
{"text":"In this paper, we present a novel approach to facial animation by taking existing animations and allowing for the modification of style characteristics.","meta":{"url":"http://arxiv.org/abs/2307.09480v1"},"cats":{"new-dataset":0.0630626582,"dev-research":0.2519946826,"prompt-eng":0.3693720383,"data-quality":0.076522528,"ml-security":0.1223229101}}
{"text":"Specifically, we explore the use of a StarGAN to enable the conversion of 3D facial animations into different emotions and person-specific styles.","meta":{"url":"http://arxiv.org/abs/2307.09480v1"},"cats":{"new-dataset":0.0865480924,"dev-research":0.3237754553,"prompt-eng":0.3790042979,"data-quality":0.0924823819,"ml-security":0.0782430225}}
{"text":"We are able to maintain the lip-sync of the animations with this method thanks to the use of a novel viseme-preserving loss.","meta":{"url":"http://arxiv.org/abs/2307.09480v1"},"cats":{"new-dataset":0.031483148,"dev-research":0.2443405513,"prompt-eng":0.3681900908,"data-quality":0.1485961143,"ml-security":0.0826132692}}
{"text":"Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.09476v1"},"cats":{"new-dataset":0.0635234897,"dev-research":0.2605898141,"prompt-eng":0.4426006082,"data-quality":0.1750407445,"ml-security":0.1016837802}}
{"text":"However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context.","meta":{"url":"http://arxiv.org/abs/2307.09476v1"},"cats":{"new-dataset":0.0114950971,"dev-research":0.3153112242,"prompt-eng":0.4084252536,"data-quality":0.2542340743,"ml-security":0.2137822577}}
{"text":"We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads.","meta":{"url":"http://arxiv.org/abs/2307.09476v1"},"cats":{"new-dataset":0.0119605424,"dev-research":0.3089988552,"prompt-eng":0.4433401089,"data-quality":0.1513952989,"ml-security":0.289324283}}
{"text":"The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations.","meta":{"url":"http://arxiv.org/abs/2307.09476v1"},"cats":{"new-dataset":0.0130093942,"dev-research":0.3141084824,"prompt-eng":0.4430236076,"data-quality":0.2246182998,"ml-security":0.2191503076}}
{"text":"At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some \"critical layer\", after which the accuracy given incorrect demonstrations progressively decreases.","meta":{"url":"http://arxiv.org/abs/2307.09476v1"},"cats":{"new-dataset":0.007176051,"dev-research":0.2672660921,"prompt-eng":0.4753194574,"data-quality":0.1501136768,"ml-security":0.1581520649}}
{"text":"The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking.","meta":{"url":"http://arxiv.org/abs/2307.09476v1"},"cats":{"new-dataset":0.0069677781,"dev-research":0.3483282146,"prompt-eng":0.4607199168,"data-quality":0.2097796165,"ml-security":0.194247222}}
{"text":"Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.","meta":{"url":"http://arxiv.org/abs/2307.09476v1"},"cats":{"new-dataset":0.0250306896,"dev-research":0.3182521387,"prompt-eng":0.4364639329,"data-quality":0.1013467729,"ml-security":0.3856557013}}
{"text":"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs).","meta":{"url":"http://arxiv.org/abs/2307.09474v1"},"cats":{"new-dataset":0.1279257227,"dev-research":0.2865207258,"prompt-eng":0.4168806198,"data-quality":0.0933647831,"ml-security":0.089943526}}
{"text":"However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency.","meta":{"url":"http://arxiv.org/abs/2307.09474v1"},"cats":{"new-dataset":0.0820298827,"dev-research":0.3379960315,"prompt-eng":0.4430629482,"data-quality":0.1245619716,"ml-security":0.1695092175}}
{"text":"In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region.","meta":{"url":"http://arxiv.org/abs/2307.09474v1"},"cats":{"new-dataset":0.1491790352,"dev-research":0.3613256165,"prompt-eng":0.500064567,"data-quality":0.2453014817,"ml-security":0.0445935961}}
{"text":"This enables MLLMs to focus on the region of interest and achieve finer-grained interaction.","meta":{"url":"http://arxiv.org/abs/2307.09474v1"},"cats":{"new-dataset":0.0584723781,"dev-research":0.3279809152,"prompt-eng":0.4434487291,"data-quality":0.0765405875,"ml-security":0.0826589671}}
{"text":"Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.","meta":{"url":"http://arxiv.org/abs/2307.09474v1"},"cats":{"new-dataset":0.4188026505,"dev-research":0.349400554,"prompt-eng":0.4464958795,"data-quality":0.0914639957,"ml-security":0.04033182}}
{"text":"We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating.","meta":{"url":"http://arxiv.org/abs/2307.09474v1"},"cats":{"new-dataset":0.7066674871,"dev-research":0.308132344,"prompt-eng":0.4515309887,"data-quality":0.1750524025,"ml-security":0.065027821}}
{"text":"Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction.","meta":{"url":"http://arxiv.org/abs/2307.09474v1"},"cats":{"new-dataset":0.0720218176,"dev-research":0.3062031559,"prompt-eng":0.4964234157,"data-quality":0.1917384789,"ml-security":0.0497851698}}
{"text":"Experimental results showcase ChatSpot's promising performance.","meta":{"url":"http://arxiv.org/abs/2307.09474v1"},"cats":{"new-dataset":0.262261724,"dev-research":0.3373376791,"prompt-eng":0.3896184669,"data-quality":0.067055901,"ml-security":0.0910417496}}
{"text":"Efficiency is quite important for 3D lane detection due to practical deployment demand.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.0162542814,"dev-research":0.3136416798,"prompt-eng":0.3668461774,"data-quality":0.0989176958,"ml-security":0.0571444599}}
{"text":"In this work, we propose a simple, fast, and end-to-end detector that still maintains high detection precision.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.1845557892,"dev-research":0.2523459939,"prompt-eng":0.4286392696,"data-quality":0.2789694715,"ml-security":0.1386253022}}
{"text":"Specifically, we devise a set of fully convolutional heads based on row-wise classification.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.0715933771,"dev-research":0.1986080997,"prompt-eng":0.3627734079,"data-quality":0.133775105,"ml-security":0.1560163144}}
{"text":"In contrast to previous counterparts, ours supports recognizing both vertical and horizontal lanes.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.041717089,"dev-research":0.2513172305,"prompt-eng":0.3734616592,"data-quality":0.087449958,"ml-security":0.0432556305}}
{"text":"Besides, our method is the first one to perform row-wise classification in bird-eye-view.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.1858322921,"dev-research":0.2665338995,"prompt-eng":0.3757931584,"data-quality":0.1701962138,"ml-security":0.0792492845}}
{"text":"In the heads, we split feature into multiple groups and every group of feature corresponds to a lane instance.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.0975405216,"dev-research":0.2887424276,"prompt-eng":0.3930612917,"data-quality":0.1151850987,"ml-security":0.0709891921}}
{"text":"During training, the predictions are associated with lane labels using the proposed single-win one-to-one matching to compute loss, and no post-processing operation is demanded for inference.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.0741436893,"dev-research":0.1929687937,"prompt-eng":0.4148307791,"data-quality":0.3301522059,"ml-security":0.1265876263}}
{"text":"In this way, our proposed fully convolutional detector, GroupLane, realizes end-to-end detection like DETR.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.1567400737,"dev-research":0.266901878,"prompt-eng":0.4215826704,"data-quality":0.1877663694,"ml-security":0.1631923261}}
{"text":"Evaluated on 3 real world 3D lane benchmarks, OpenLane, Once-3DLanes, and OpenLane-Huawei, GroupLane adopting ConvNext-Base as the backbone outperforms the published state-of-the-art PersFormer by 13.6% F1 score in the OpenLane validation set.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.0632579146,"dev-research":0.2937809545,"prompt-eng":0.3826112847,"data-quality":0.1527424268,"ml-security":0.0729300896}}
{"text":"Besides, GroupLane with ResNet18 still surpasses PersFormer by 4.9% F1 score, while the inference speed is nearly 7x faster and the FLOPs is only 13.3% of it.","meta":{"url":"http://arxiv.org/abs/2307.09472v1"},"cats":{"new-dataset":0.0322379735,"dev-research":0.2980641287,"prompt-eng":0.375593523,"data-quality":0.0996724793,"ml-security":0.0661482308}}
{"text":"Vehicle trajectory planning is a key component for an autonomous driving system.","meta":{"url":"http://arxiv.org/abs/2307.09466v1"},"cats":{"new-dataset":0.059453986,"dev-research":0.2981335238,"prompt-eng":0.4050872344,"data-quality":0.0577791545,"ml-security":0.0931999453}}
{"text":"A practical system not only requires the component to compute a feasible trajectory, but also a comfortable one given certain comfort metrics.","meta":{"url":"http://arxiv.org/abs/2307.09466v1"},"cats":{"new-dataset":0.0272395498,"dev-research":0.2193173695,"prompt-eng":0.3869925027,"data-quality":0.0341029299,"ml-security":0.0750104633}}
{"text":"Nevertheless, computation efficiency is critical for the system to be deployed as a commercial product.","meta":{"url":"http://arxiv.org/abs/2307.09466v1"},"cats":{"new-dataset":0.0209054521,"dev-research":0.4071143472,"prompt-eng":0.3965152036,"data-quality":0.069581815,"ml-security":0.0903101554}}
{"text":"In this paper, we present a novel trajectory planning algorithm based on nonlinear optimization.","meta":{"url":"http://arxiv.org/abs/2307.09466v1"},"cats":{"new-dataset":0.0833199786,"dev-research":0.2219400003,"prompt-eng":0.3637631591,"data-quality":0.0476649418,"ml-security":0.0593075481}}
{"text":"The algorithm computes a kinematically feasible and comfort-optimal trajectory that achieves collision avoidance with static obstacles.","meta":{"url":"http://arxiv.org/abs/2307.09466v1"},"cats":{"new-dataset":0.1989684166,"dev-research":0.2399956752,"prompt-eng":0.3550439979,"data-quality":0.0357078466,"ml-security":0.1001814805}}
{"text":"Furthermore, the algorithm is time efficient.","meta":{"url":"http://arxiv.org/abs/2307.09466v1"},"cats":{"new-dataset":0.0168148711,"dev-research":0.2677254301,"prompt-eng":0.3491650701,"data-quality":0.0714258658,"ml-security":0.0621344798}}
{"text":"It generates an 6-second trajectory within 10 milliseconds on an Intel i7 machine or 20 milliseconds on an Nvidia Drive Orin platform.","meta":{"url":"http://arxiv.org/abs/2307.09466v1"},"cats":{"new-dataset":0.1700643481,"dev-research":0.3106971937,"prompt-eng":0.4090177441,"data-quality":0.0476373188,"ml-security":0.051052814}}
{"text":"Given that approximately half of science, technology, engineering, and mathematics (STEM) undergraduate students in U.S. colleges and universities leave by the end of the first year","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.0624241316,"dev-research":0.2912466762,"prompt-eng":0.4056307716,"data-quality":0.1036439246,"ml-security":0.0885968081}}
{"text":"[15], it is crucial to improve the quality of classroom environments.","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.1390931605,"dev-research":0.4194538045,"prompt-eng":0.3976389333,"data-quality":0.1608278779,"ml-security":0.1301588945}}
{"text":"This study focuses on monitoring students' emotions in the classroom as an indicator of their engagement and proposes an approach to address this issue.","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.285517046,"dev-research":0.4200256714,"prompt-eng":0.4105648873,"data-quality":0.1619964471,"ml-security":0.1515514442}}
{"text":"The impact of different facial parts on the performance of an emotional recognition model is evaluated through experimentation.","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.0170019619,"dev-research":0.2285360834,"prompt-eng":0.4042346437,"data-quality":0.1853423446,"ml-security":0.1489991197}}
{"text":"To test the proposed model under partial occlusion, an artificially occluded dataset is introduced.","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.3187884865,"dev-research":0.2103884319,"prompt-eng":0.3434512965,"data-quality":0.1525606462,"ml-security":0.0984955294}}
{"text":"The novelty of this work lies in the proposal of an occlusion-aware architecture for facial action units (AUs) extraction, which employs attention mechanism and adaptive feature learning.","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.0749665973,"dev-research":0.2093126407,"prompt-eng":0.3734945433,"data-quality":0.1407632047,"ml-security":0.1066967427}}
{"text":"The AUs can be used later to classify facial expressions in classroom settings.   ","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.1132934222,"dev-research":0.2957894752,"prompt-eng":0.4230461023,"data-quality":0.2003411004,"ml-security":0.1322629164}}
{"text":"This research paper's findings provide valuable insights into handling occlusion in analyzing facial images for emotional engagement analysis.","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.0775891112,"dev-research":0.3259095299,"prompt-eng":0.3479919806,"data-quality":0.1710988793,"ml-security":0.1005715153}}
{"text":"The proposed experiments demonstrate the significance of considering occlusion and enhancing the reliability of facial analysis models in classroom environments.","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.0494964835,"dev-research":0.2834271016,"prompt-eng":0.3624030725,"data-quality":0.1761366987,"ml-security":0.1813315884}}
{"text":"These findings can also be extended to other settings where occlusions are prevalent.","meta":{"url":"http://arxiv.org/abs/2307.09465v1"},"cats":{"new-dataset":0.0449685566,"dev-research":0.3146587762,"prompt-eng":0.3618006938,"data-quality":0.1755936504,"ml-security":0.1124855112}}
{"text":"\\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models.","meta":{"url":"http://arxiv.org/abs/2307.09458v1"},"cats":{"new-dataset":0.0174652045,"dev-research":0.3484783476,"prompt-eng":0.4948133853,"data-quality":0.2190399021,"ml-security":0.1319573574}}
{"text":"However, existing analyses are done in small models far from the state of the art.","meta":{"url":"http://arxiv.org/abs/2307.09458v1"},"cats":{"new-dataset":0.0441194098,"dev-research":0.2247381008,"prompt-eng":0.3501550308,"data-quality":0.1032249653,"ml-security":0.099297877}}
{"text":"To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis.","meta":{"url":"http://arxiv.org/abs/2307.09458v1"},"cats":{"new-dataset":0.0334199801,"dev-research":0.2520634368,"prompt-eng":0.4133393317,"data-quality":0.0985932595,"ml-security":0.1220526553}}
{"text":"In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \\emph{label} given knowledge of the correct answer \\emph{text}.","meta":{"url":"http://arxiv.org/abs/2307.09458v1"},"cats":{"new-dataset":0.0502525645,"dev-research":0.2870939853,"prompt-eng":0.4806348683,"data-quality":0.2181917538,"ml-security":0.0747970671}}
{"text":"We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).   ","meta":{"url":"http://arxiv.org/abs/2307.09458v1"},"cats":{"new-dataset":0.0843577565,"dev-research":0.2920098447,"prompt-eng":0.504052187,"data-quality":0.1952056316,"ml-security":0.1929366541}}
{"text":"We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results.","meta":{"url":"http://arxiv.org/abs/2307.09458v1"},"cats":{"new-dataset":0.055256286,"dev-research":0.3379823405,"prompt-eng":0.4859423683,"data-quality":0.3452962778,"ml-security":0.0683700127}}
{"text":"For normal multiple-choice question answers, we significantly compress the query, key and value subspaces of the head without loss of performance when operating on the answer labels for multiple-choice questions, and we show that the query and key subspaces represent an `Nth item in an enumeration' feature to at least some extent.","meta":{"url":"http://arxiv.org/abs/2307.09458v1"},"cats":{"new-dataset":0.0530007526,"dev-research":0.28297702,"prompt-eng":0.4384671939,"data-quality":0.1560490794,"ml-security":0.0657070451}}
{"text":"However, when we attempt to use this explanation to understand the heads' behaviour on a more general distribution including randomized answer labels, we find that it is only a partial explanation, suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.","meta":{"url":"http://arxiv.org/abs/2307.09458v1"},"cats":{"new-dataset":0.0105189827,"dev-research":0.3238933883,"prompt-eng":0.4571102663,"data-quality":0.2341539945,"ml-security":0.115658494}}
{"text":"In this study, we evaluate the performance of multiple state-of-the-art SR GAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline.","meta":{"url":"http://arxiv.org/abs/2307.09456v1"},"cats":{"new-dataset":0.1077648609,"dev-research":0.2215613848,"prompt-eng":0.3778025513,"data-quality":0.1352075975,"ml-security":0.1284065595}}
{"text":"Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine.","meta":{"url":"http://arxiv.org/abs/2307.09456v1"},"cats":{"new-dataset":0.0789482394,"dev-research":0.2411079523,"prompt-eng":0.4161495785,"data-quality":0.1543988615,"ml-security":0.0575946227}}
{"text":"We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead.","meta":{"url":"http://arxiv.org/abs/2307.09456v1"},"cats":{"new-dataset":0.0782219093,"dev-research":0.2889792722,"prompt-eng":0.4241638517,"data-quality":0.1222855688,"ml-security":0.0430617632}}
{"text":"Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine.","meta":{"url":"http://arxiv.org/abs/2307.09456v1"},"cats":{"new-dataset":0.133006452,"dev-research":0.2329665423,"prompt-eng":0.4482562524,"data-quality":0.1275503523,"ml-security":0.033785404}}
{"text":"These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications where high-quality visual fidelity is critical and optimized compute.","meta":{"url":"http://arxiv.org/abs/2307.09456v1"},"cats":{"new-dataset":0.0421771226,"dev-research":0.2669319578,"prompt-eng":0.4108528244,"data-quality":0.1118361733,"ml-security":0.042691255}}
{"text":"For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples.","meta":{"url":"http://arxiv.org/abs/2307.09455v2"},"cats":{"new-dataset":0.1950001404,"dev-research":0.2880972718,"prompt-eng":0.4332263725,"data-quality":0.4976827819,"ml-security":0.1771025381}}
{"text":"However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples.","meta":{"url":"http://arxiv.org/abs/2307.09455v2"},"cats":{"new-dataset":0.073986927,"dev-research":0.2515924506,"prompt-eng":0.4566199998,"data-quality":0.419643116,"ml-security":0.2536909229}}
{"text":"In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold.","meta":{"url":"http://arxiv.org/abs/2307.09455v2"},"cats":{"new-dataset":0.1169001356,"dev-research":0.2339280468,"prompt-eng":0.4097337275,"data-quality":0.3912038058,"ml-security":0.0936985276}}
{"text":"A rejection network can be trained with ID and diverse outlier samples to detect test OOD samples, but explicitly collecting auxiliary OOD datasets brings an additional burden for data collection.","meta":{"url":"http://arxiv.org/abs/2307.09455v2"},"cats":{"new-dataset":0.1398911958,"dev-research":0.2822885975,"prompt-eng":0.3932429127,"data-quality":0.361854294,"ml-security":0.3024972261}}
{"text":"In this paper, we propose a simple but effective method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD dataset by sequentially masking tokens related to ID classes.","meta":{"url":"http://arxiv.org/abs/2307.09455v2"},"cats":{"new-dataset":0.4368317985,"dev-research":0.2785441188,"prompt-eng":0.3803900418,"data-quality":0.315854141,"ml-security":0.557891038}}
{"text":"The surrogate OOD sample introduced by POE shows a similar representation to ID data, which is most effective in training a rejection network.","meta":{"url":"http://arxiv.org/abs/2307.09455v2"},"cats":{"new-dataset":0.114797576,"dev-research":0.2567384804,"prompt-eng":0.4419799036,"data-quality":0.252427133,"ml-security":0.2117325542}}
{"text":"Our method does not require any external OOD data and can be easily implemented within off-the-shelf Transformers.","meta":{"url":"http://arxiv.org/abs/2307.09455v2"},"cats":{"new-dataset":0.0897215272,"dev-research":0.213854367,"prompt-eng":0.3627950443,"data-quality":0.0882185657,"ml-security":0.0994371674}}
{"text":"A comprehensive comparison with state-of-the-art algorithms demonstrates POE's competitiveness on several text classification benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.09455v2"},"cats":{"new-dataset":0.1124199168,"dev-research":0.2979026826,"prompt-eng":0.3755950713,"data-quality":0.2366987191,"ml-security":0.1375988178}}
{"text":"We study pseudo-polynomial time algorithms for the fundamental \\emph{0-1 Knapsack} problem.","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0744830979,"dev-research":0.16932309,"prompt-eng":0.3699291856,"data-quality":0.1160354611,"ml-security":0.1407287358}}
{"text":"In terms of $n$ and $w_{\\max}$, previous algorithms for 0-1 Knapsack have cubic time complexities: $O(n^2w_{\\max})$ (Bellman 1957), $O(nw_{\\max}^2)$ (Kellerer and Pferschy 2004), and $O(n + w_{\\max}^3)$ (Polak, Rohwedder, and W\\k{e}grzycki 2021).","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0530638663,"dev-research":0.1766626485,"prompt-eng":0.2912309643,"data-quality":0.0812670164,"ml-security":0.1276521543}}
{"text":"On the other hand, fine-grained complexity only rules out $O((n+w_{\\max})^{2-\\delta})$ running time, and it is an important question in this area whether $\\tilde O(n+w_{\\max}^2)$ time is achievable.","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0174079354,"dev-research":0.2454159594,"prompt-eng":0.3160448375,"data-quality":0.1020554813,"ml-security":0.0899188417}}
{"text":"Our main result makes significant progress towards solving this question:   - The 0-1 Knapsack problem has a deterministic algorithm in $\\tilde O(n + w_{\\max}^{2.5})$ time.   ","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.2103242195,"dev-research":0.1752434842,"prompt-eng":0.3759987695,"data-quality":0.1185995077,"ml-security":0.1304894002}}
{"text":"Our techniques also apply to the easier \\emph{Subset Sum} problem:   - The Subset Sum problem has a randomized algorithm in $\\tilde O(n","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0656622313,"dev-research":0.2040444535,"prompt-eng":0.3508141937,"data-quality":0.167751092,"ml-security":0.1546679505}}
{"text":"+ w_{\\max}^{1.5})$ time.   ","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0924492793,"dev-research":0.2397375507,"prompt-eng":0.3693378277,"data-quality":0.0754970526,"ml-security":0.0909591881}}
{"text":"This improves (and simplifies) the previous $\\tilde O(n + w_{\\max}^{5/3})$-time algorithm by Polak, Rohwedder, and W\\k{e}grzycki (2021) (based on Galil and Margalit (1991), and Bringmann and Wellnitz (2021)).   ","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0524082255,"dev-research":0.2039732102,"prompt-eng":0.3114740329,"data-quality":0.0688611091,"ml-security":0.0746984677}}
{"text":"Similar to recent works on Knapsack (and integer programs in general), our algorithms also utilize the \\emph{proximity} between optimal integral solutions and fractional solutions.","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0235438198,"dev-research":0.1909234534,"prompt-eng":0.3751956958,"data-quality":0.0845093214,"ml-security":0.1202172988}}
{"text":"Our new ideas are as follows: - Previous works used an $O(w_{\\max})$ proximity bound in the $\\ell_1$-norm.","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.1261185068,"dev-research":0.2199839051,"prompt-eng":0.3243270144,"data-quality":0.1222532813,"ml-security":0.0969160791}}
{"text":"As our main conceptual contribution, we use an additive-combinatorial theorem by Erd\\H{o}s and S\\'{a}rk\\\"{o}zy (1990) to derive an $\\ell_0$-proximity bound of $\\tilde O(\\sqrt{w_{\\max}})$. - Then, the main technical component of our Knapsack result is a dynamic programming algorithm that exploits both $\\ell_0$- and $\\ell_1$-proximity.","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0683266476,"dev-research":0.2081980269,"prompt-eng":0.3380267181,"data-quality":0.0693980349,"ml-security":0.1455719734}}
{"text":"It is based on a vast extension of the ``witness propagation'' method, originally designed by Deng, Mao, and Zhong (2023) for the easier \\emph{unbounded} setting only.","meta":{"url":"http://arxiv.org/abs/2307.09454v1"},"cats":{"new-dataset":0.0192660782,"dev-research":0.1932242483,"prompt-eng":0.4051706284,"data-quality":0.1216294426,"ml-security":0.084549126}}
{"text":"Modern society devotes a significant amount of time to digital interaction.","meta":{"url":"http://arxiv.org/abs/2307.09447v1"},"cats":{"new-dataset":0.1141679476,"dev-research":0.3682034573,"prompt-eng":0.3862568393,"data-quality":0.0560259198,"ml-security":0.0874463118}}
{"text":"Many of our daily actions are carried out through digital means.","meta":{"url":"http://arxiv.org/abs/2307.09447v1"},"cats":{"new-dataset":0.2417527247,"dev-research":0.3497247821,"prompt-eng":0.4141772446,"data-quality":0.0856961862,"ml-security":0.143939112}}
{"text":"This has led to the emergence of numerous Artificial Intelligence tools that assist us in various aspects of our lives.","meta":{"url":"http://arxiv.org/abs/2307.09447v1"},"cats":{"new-dataset":0.1142307678,"dev-research":0.41584964,"prompt-eng":0.4353973014,"data-quality":0.0662449465,"ml-security":0.1023538282}}
{"text":"One key tool for the digital society is Recommender Systems, intelligent systems that learn from our past actions to propose new ones that align with our interests.","meta":{"url":"http://arxiv.org/abs/2307.09447v1"},"cats":{"new-dataset":0.071837631,"dev-research":0.4405020359,"prompt-eng":0.4428804484,"data-quality":0.1143148306,"ml-security":0.1289321765}}
{"text":"Some of these systems have specialized in learning from the behavior of user groups to make recommendations to a group of individuals who want to perform a joint task.","meta":{"url":"http://arxiv.org/abs/2307.09447v1"},"cats":{"new-dataset":0.0368722458,"dev-research":0.3381759659,"prompt-eng":0.5000689358,"data-quality":0.0553111065,"ml-security":0.1201275723}}
{"text":"In this article, we analyze the current state of Group Recommender Systems and propose two new models that use emerging Deep Learning architectures.","meta":{"url":"http://arxiv.org/abs/2307.09447v1"},"cats":{"new-dataset":0.1117124302,"dev-research":0.2921971747,"prompt-eng":0.3672724462,"data-quality":0.1212860788,"ml-security":0.2010902332}}
{"text":"Experimental results demonstrate the improvement achieved by employing the proposed models compared to the state-of-the-art models using four different datasets.","meta":{"url":"http://arxiv.org/abs/2307.09447v1"},"cats":{"new-dataset":0.2381734866,"dev-research":0.2367221509,"prompt-eng":0.3928160071,"data-quality":0.1506881023,"ml-security":0.083753016}}
{"text":"The source code of the models, as well as that of all the experiments conducted, is available in a public repository.","meta":{"url":"http://arxiv.org/abs/2307.09447v1"},"cats":{"new-dataset":0.4890632276,"dev-research":0.2447536032,"prompt-eng":0.4271416099,"data-quality":0.1080132032,"ml-security":0.1104489913}}
{"text":"We give an almost complete characterization of the hardness of $c$-coloring $\\chi$-chromatic graphs with distributed algorithms, for a wide range of models of distributed computing.","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.0812238753,"dev-research":0.2327874516,"prompt-eng":0.3497808054,"data-quality":0.1358409161,"ml-security":0.141514198}}
{"text":"In particular, we show that these problems do not admit any distributed quantum advantage.","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.0164483123,"dev-research":0.1581153529,"prompt-eng":0.3322988633,"data-quality":0.1262432394,"ml-security":0.2364558798}}
{"text":"To do that:   1.","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.1940457788,"dev-research":0.3415065722,"prompt-eng":0.4489875898,"data-quality":0.094454932,"ml-security":0.0727992599}}
{"text":"We give a new distributed algorithm that finds a $c$-coloring in $\\chi$-chromatic graphs in $\\tilde{\\mathcal{O}}(n^{\\frac{1}{\\alpha}})$ rounds, with $\\alpha = \\bigl\\lceil\\frac{c-1}{\\chi - 1}\\bigr\\rceil$.   2.","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.1588456637,"dev-research":0.1772157321,"prompt-eng":0.3324926124,"data-quality":0.1171342645,"ml-security":0.1006994675}}
{"text":"We prove that any distributed algorithm for this problem requires $\\Omega(n^{\\frac{1}{\\alpha}})$ rounds.   ","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.0789786164,"dev-research":0.1498831562,"prompt-eng":0.3381441795,"data-quality":0.1255522769,"ml-security":0.1447935323}}
{"text":"Our upper bound holds in the classical, deterministic LOCAL model, while the near-matching lower bound holds in the \\emph{non-signaling} model.","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.0423718812,"dev-research":0.1600779616,"prompt-eng":0.4329661499,"data-quality":0.1143467103,"ml-security":0.1338903679}}
{"text":"This model, introduced by Arfaoui and Fraigniaud in 2014, captures all models of distributed graph algorithms that obey physical causality; this includes not only classical deterministic LOCAL and randomized LOCAL but also quantum-LOCAL, even with a pre-shared quantum state.   ","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.0404247904,"dev-research":0.1957897245,"prompt-eng":0.3887261729,"data-quality":0.0999961351,"ml-security":0.1359147153}}
{"text":"We also show that similar arguments can be used to prove that, e.g., 3-coloring 2-dimensional grids or $c$-coloring trees remain hard problems even for the non-signaling model, and in particular do not admit any quantum advantage.","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.0098620429,"dev-research":0.189725944,"prompt-eng":0.3594827744,"data-quality":0.0933986962,"ml-security":0.1574962334}}
{"text":"Our lower-bound arguments are purely graph-theoretic at heart; no background on quantum information theory is needed to establish the proofs.","meta":{"url":"http://arxiv.org/abs/2307.09444v1"},"cats":{"new-dataset":0.0203332923,"dev-research":0.2232289143,"prompt-eng":0.3366420699,"data-quality":0.112863351,"ml-security":0.1575096105}}
{"text":"We consider a slotted communication system consisting of a source, a cache, a user and a timestomping adversary.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0713575373,"dev-research":0.2663489437,"prompt-eng":0.383962448,"data-quality":0.1134003296,"ml-security":0.3677673993}}
{"text":"The time horizon consists of total $T$ time slots, such that the source transmits update packets to the user directly over $T_{1}$ time slots and to the cache over $T_{2}$ time slots.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0832442253,"dev-research":0.2353264875,"prompt-eng":0.3508407664,"data-quality":0.0566241836,"ml-security":0.1174274324}}
{"text":"We consider $T_{1}\\ll T_{2}$, $T_{1}+T_{2} < T$, such that the source transmits to the user once between two consecutive cache updates.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0467123627,"dev-research":0.2477952826,"prompt-eng":0.3867870501,"data-quality":0.1045275923,"ml-security":0.1758447955}}
{"text":"Update packets are marked with timestamps corresponding to their generation times at the source.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.2593380973,"dev-research":0.3356191034,"prompt-eng":0.4015263748,"data-quality":0.1807322568,"ml-security":0.102360919}}
{"text":"All nodes have a buffer size of one and store the packet with the latest timestamp to minimize their age of information.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.115474768,"dev-research":0.241596202,"prompt-eng":0.3173789208,"data-quality":0.0944180254,"ml-security":0.0951315385}}
{"text":"In this setting, we consider the presence of an oblivious adversary that fully controls the communication link between the cache and the user.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0336937066,"dev-research":0.2307214205,"prompt-eng":0.3750132748,"data-quality":0.1098133842,"ml-security":0.5031475884}}
{"text":"The adversary manipulates the timestamps of outgoing packets from the cache to the user, with the goal of bringing staleness at the user node.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0227019066,"dev-research":0.3055968908,"prompt-eng":0.3621629651,"data-quality":0.1200804219,"ml-security":0.3654958568}}
{"text":"At each time slot, the adversary can choose to either forward the cached packet to the user, after changing its timestamp to current time $t$, thereby rebranding an old packet as a fresh packet and misleading the user into accepting it, or stay idle.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0256677183,"dev-research":0.2560637302,"prompt-eng":0.3810969803,"data-quality":0.1152389449,"ml-security":0.2480980509}}
{"text":"The user compares the timestamps of every received packet with the latest packet in its possession to keep the fresher one and discard the staler packet.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.1428891641,"dev-research":0.321743922,"prompt-eng":0.3770284764,"data-quality":0.1878584099,"ml-security":0.1279293586}}
{"text":"If the user receives update packets from both cache and source in a time slot, then the packet from source prevails.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0413543231,"dev-research":0.2933839942,"prompt-eng":0.3845232319,"data-quality":0.1289563682,"ml-security":0.1930597199}}
{"text":"The goal of the source is to design an algorithm to minimize the average age at the user, and the goal of the adversary is to increase the average age at the user.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0293541841,"dev-research":0.3432041602,"prompt-eng":0.3790823839,"data-quality":0.0879951917,"ml-security":0.2656932307}}
{"text":"We formulate this problem in an online learning setting and provide a fundamental lower bound on the competitive ratio for this problem.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.088496904,"dev-research":0.1756360809,"prompt-eng":0.3841501671,"data-quality":0.1221763403,"ml-security":0.1890234951}}
{"text":"We further propose a deterministic algorithm with a provable guarantee on its competitive ratio.","meta":{"url":"http://arxiv.org/abs/2307.09443v1"},"cats":{"new-dataset":0.0438909364,"dev-research":0.1639861943,"prompt-eng":0.3815305736,"data-quality":0.1093903335,"ml-security":0.1918272755}}
{"text":"Extracting object-level representations for downstream reasoning tasks is an emerging area in AI.","meta":{"url":"http://arxiv.org/abs/2307.09437v1"},"cats":{"new-dataset":0.0839639787,"dev-research":0.3387885297,"prompt-eng":0.4400483243,"data-quality":0.1614083652,"ml-security":0.1323549462}}
{"text":"Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot.","meta":{"url":"http://arxiv.org/abs/2307.09437v1"},"cats":{"new-dataset":0.1567375717,"dev-research":0.2489467015,"prompt-eng":0.4386050561,"data-quality":0.1587167697,"ml-security":0.1496437211}}
{"text":"Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding.","meta":{"url":"http://arxiv.org/abs/2307.09437v1"},"cats":{"new-dataset":0.0645062903,"dev-research":0.2393278816,"prompt-eng":0.4477086735,"data-quality":0.1536315794,"ml-security":0.1077494204}}
{"text":"To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD).","meta":{"url":"http://arxiv.org/abs/2307.09437v1"},"cats":{"new-dataset":0.2090511441,"dev-research":0.2223328406,"prompt-eng":0.4700014559,"data-quality":0.176373022,"ml-security":0.0799899704}}
{"text":"We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value.","meta":{"url":"http://arxiv.org/abs/2307.09437v1"},"cats":{"new-dataset":0.0960468794,"dev-research":0.2016155471,"prompt-eng":0.4500793106,"data-quality":0.0960128616,"ml-security":0.1394562739}}
{"text":"We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning.","meta":{"url":"http://arxiv.org/abs/2307.09437v1"},"cats":{"new-dataset":0.0995920777,"dev-research":0.2694967544,"prompt-eng":0.49251999,"data-quality":0.1234372857,"ml-security":0.1114649122}}
{"text":"We show that our method provides scene composition capabilities and a significant boost in a few shot adaptability tasks of compositional visual reasoning, while performing similarly or better than slot attention in object discovery tasks","meta":{"url":"http://arxiv.org/abs/2307.09437v1"},"cats":{"new-dataset":0.09633833,"dev-research":0.3085705794,"prompt-eng":0.413301398,"data-quality":0.1409011511,"ml-security":0.061984903}}
{"text":"The advancement of biomedical research heavily relies on access to large amounts of medical data.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.1598259718,"dev-research":0.2902805686,"prompt-eng":0.3000149763,"data-quality":0.0532957282,"ml-security":0.1283748266}}
{"text":"In the case of histopathology, Whole Slide Images (WSI) and clinicopathological information are valuable for developing Artificial Intelligence (AI) algorithms for Digital Pathology (DP).","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.1210254415,"dev-research":0.3065579224,"prompt-eng":0.3836312842,"data-quality":0.0891510575,"ml-security":0.079905482}}
{"text":"Transferring medical data \"as open as possible\" enhances the usability of the data for secondary purposes but poses a risk to patient privacy.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.123311231,"dev-research":0.2846031073,"prompt-eng":0.3154875047,"data-quality":0.0678403399,"ml-security":0.3182185255}}
{"text":"At the same time, existing regulations push towards keeping medical data \"as closed as necessary\" to avoid re-identification risks.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.1226798109,"dev-research":0.2822620196,"prompt-eng":0.324530104,"data-quality":0.1682232152,"ml-security":0.3134060696}}
{"text":"Generally, these legal regulations require the removal of sensitive data but do not consider the possibility of data linkage attacks due to modern image-matching algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.0746495386,"dev-research":0.2273848108,"prompt-eng":0.3233953584,"data-quality":0.2863238248,"ml-security":0.3815993084}}
{"text":"In addition, the lack of standardization in DP makes it harder to establish a single solution for all formats of WSIs.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.0112480955,"dev-research":0.3553883783,"prompt-eng":0.3939848263,"data-quality":0.160051152,"ml-security":0.0967415918}}
{"text":"These challenges raise problems for bio-informatics researchers in balancing privacy and progress while developing AI algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.0953929826,"dev-research":0.282225837,"prompt-eng":0.3265415224,"data-quality":0.1190699722,"ml-security":0.4563735538}}
{"text":"This paper explores the legal regulations and terminologies for medical data-sharing.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.3429204024,"dev-research":0.3262476598,"prompt-eng":0.2992298722,"data-quality":0.1032202765,"ml-security":0.2385428843}}
{"text":"We review existing approaches and highlight challenges from the histopathological perspective.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.0702804443,"dev-research":0.3389585034,"prompt-eng":0.3750929661,"data-quality":0.1145720605,"ml-security":0.0587278991}}
{"text":"We also present a data-sharing guideline for histological data to foster multidisciplinary research and education.","meta":{"url":"http://arxiv.org/abs/2307.09426v1"},"cats":{"new-dataset":0.4427755601,"dev-research":0.3533466429,"prompt-eng":0.3471801973,"data-quality":0.0847084874,"ml-security":0.0669003883}}
{"text":"Imitation Learning (IL) is one of the most widely used methods in machine learning.","meta":{"url":"http://arxiv.org/abs/2307.09423v1"},"cats":{"new-dataset":0.0215460313,"dev-research":0.2381785427,"prompt-eng":0.408988581,"data-quality":0.1333747371,"ml-security":0.1564175614}}
{"text":"Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior.","meta":{"url":"http://arxiv.org/abs/2307.09423v1"},"cats":{"new-dataset":0.0058529567,"dev-research":0.3906922225,"prompt-eng":0.3937033705,"data-quality":0.1369288658,"ml-security":0.2030236903}}
{"text":"However, none of these works deeply investigate the role of scaling up the model and data size.","meta":{"url":"http://arxiv.org/abs/2307.09423v1"},"cats":{"new-dataset":0.015303492,"dev-research":0.2245250692,"prompt-eng":0.3055482764,"data-quality":0.1058276639,"ml-security":0.095822467}}
{"text":"Inspired by recent work in Natural Language Processing (NLP) where \"scaling up\" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting.","meta":{"url":"http://arxiv.org/abs/2307.09423v1"},"cats":{"new-dataset":0.0482135891,"dev-research":0.1839777507,"prompt-eng":0.460775859,"data-quality":0.1533361047,"ml-security":0.1000681707}}
{"text":"To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability.","meta":{"url":"http://arxiv.org/abs/2307.09423v1"},"cats":{"new-dataset":0.1590060432,"dev-research":0.3325463261,"prompt-eng":0.407347063,"data-quality":0.0757731222,"ml-security":0.211296653}}
{"text":"We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples.","meta":{"url":"http://arxiv.org/abs/2307.09423v1"},"cats":{"new-dataset":0.0356855808,"dev-research":0.1619656028,"prompt-eng":0.3616609547,"data-quality":0.0986663769,"ml-security":0.3086274423}}
{"text":"We forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by at least 2x in all settings.","meta":{"url":"http://arxiv.org/abs/2307.09423v1"},"cats":{"new-dataset":0.0620455335,"dev-research":0.2343129446,"prompt-eng":0.3748744617,"data-quality":0.0965287507,"ml-security":0.1755429018}}
{"text":"Our work both demonstrates the scaling behavior of imitation learning in a challenging domain, as well as the viability of scaling up current approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems.","meta":{"url":"http://arxiv.org/abs/2307.09423v1"},"cats":{"new-dataset":0.0378506409,"dev-research":0.2263827069,"prompt-eng":0.3722843441,"data-quality":0.0728044233,"ml-security":0.2145176274}}
{"text":"In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition.","meta":{"url":"http://arxiv.org/abs/2307.09420v1"},"cats":{"new-dataset":0.1209720888,"dev-research":0.3524521717,"prompt-eng":0.4530388647,"data-quality":0.1686840453,"ml-security":0.1390509968}}
{"text":"The proposed approach recognizes student actions then predicts the student behavioral engagement level.","meta":{"url":"http://arxiv.org/abs/2307.09420v1"},"cats":{"new-dataset":0.039459939,"dev-research":0.3488865187,"prompt-eng":0.4703419699,"data-quality":0.1168653787,"ml-security":0.1424773882}}
{"text":"For student action recognition, we use human skeletons to model student postures and upper body movements.","meta":{"url":"http://arxiv.org/abs/2307.09420v1"},"cats":{"new-dataset":0.1228943507,"dev-research":0.2558590357,"prompt-eng":0.4255042338,"data-quality":0.0837079933,"ml-security":0.1399080731}}
{"text":"To learn the dynamics of student upper body, a 3D-CNN model is used.","meta":{"url":"http://arxiv.org/abs/2307.09420v1"},"cats":{"new-dataset":0.1244217045,"dev-research":0.2370051693,"prompt-eng":0.3906848418,"data-quality":0.0677989437,"ml-security":0.1447525013}}
{"text":"The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies.","meta":{"url":"http://arxiv.org/abs/2307.09420v1"},"cats":{"new-dataset":0.2194673496,"dev-research":0.2800886548,"prompt-eng":0.3896883036,"data-quality":0.1022274551,"ml-security":0.1146063883}}
{"text":"This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged.","meta":{"url":"http://arxiv.org/abs/2307.09420v1"},"cats":{"new-dataset":0.2253534922,"dev-research":0.2907223042,"prompt-eng":0.4319361705,"data-quality":0.1317572869,"ml-security":0.154121841}}
{"text":"To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels.","meta":{"url":"http://arxiv.org/abs/2307.09420v1"},"cats":{"new-dataset":0.8339680985,"dev-research":0.3025364545,"prompt-eng":0.3573222909,"data-quality":0.2270570126,"ml-security":0.0692748924}}
{"text":"Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.","meta":{"url":"http://arxiv.org/abs/2307.09420v1"},"cats":{"new-dataset":0.0748682152,"dev-research":0.3198959048,"prompt-eng":0.4531364452,"data-quality":0.1600046145,"ml-security":0.1350900992}}
{"text":"Index modulation schemes for reconfigurable intelligent surfaces (RIS)-assisted systems are envisioned as promising technologies for fifth-generation-advanced and sixth-generation (6G) wireless communication systems to enhance various system capabilities such as coverage area and network capacity.","meta":{"url":"http://arxiv.org/abs/2307.09417v1"},"cats":{"new-dataset":0.0442097259,"dev-research":0.2540620649,"prompt-eng":0.4173708748,"data-quality":0.0581947871,"ml-security":0.0574118337}}
{"text":"In this paper, we consider a receive diversity RIS-assisted wireless communication system employing IM schemes, namely, space-shift keying (SSK) for binary modulation and spatial modulation (SM) for M-ary modulation for data transmission.","meta":{"url":"http://arxiv.org/abs/2307.09417v1"},"cats":{"new-dataset":0.0320849979,"dev-research":0.2008217758,"prompt-eng":0.3925610422,"data-quality":0.1023313934,"ml-security":0.0949491242}}
{"text":"The RIS lies in close proximity to the transmitter, and the transmitted data is subjected to a fading environment with a prominent line-of-sight component modeled by a Rician distribution.","meta":{"url":"http://arxiv.org/abs/2307.09417v1"},"cats":{"new-dataset":0.0647932411,"dev-research":0.1758235939,"prompt-eng":0.4455348608,"data-quality":0.1106856992,"ml-security":0.1474925118}}
{"text":"A receiver structure based on a greedy detection rule is employed to select the receive diversity branch with the highest received signal energy for demodulation.","meta":{"url":"http://arxiv.org/abs/2307.09417v1"},"cats":{"new-dataset":0.0411410383,"dev-research":0.2055765023,"prompt-eng":0.4358117717,"data-quality":0.1449251444,"ml-security":0.1129587464}}
{"text":"The performance of the considered system is evaluated by obtaining a series-form expression for the probability of erroneous index detection (PED) of the considered target antenna using a characteristic function approach.","meta":{"url":"http://arxiv.org/abs/2307.09417v1"},"cats":{"new-dataset":0.0089427939,"dev-research":0.265077169,"prompt-eng":0.4685187716,"data-quality":0.2483325801,"ml-security":0.1000509453}}
{"text":"In addition, closed-form and asymptotic expressions at high and low signal-to-noise ratios (SNRs) for the bit error rate (BER) for the SSK-based system, and the SM-based system employing M-ary phase-shift keying and M-ary quadrature amplitude modulation schemes, are derived.","meta":{"url":"http://arxiv.org/abs/2307.09417v1"},"cats":{"new-dataset":0.0250329257,"dev-research":0.2399806649,"prompt-eng":0.3698899763,"data-quality":0.1644699674,"ml-security":0.0929056564}}
{"text":"The dependencies of the system performance on the various parameters are corroborated via numerical results.","meta":{"url":"http://arxiv.org/abs/2307.09417v1"},"cats":{"new-dataset":0.0153018291,"dev-research":0.255184079,"prompt-eng":0.4027141037,"data-quality":0.0839293959,"ml-security":0.0775661351}}
{"text":"The asymptotic expressions and results of PED and BER at high and low SNR values lead to the observation of a performance saturation and the presence of an SNR value as a point of inflection, which is attributed to the greedy detector's structure.","meta":{"url":"http://arxiv.org/abs/2307.09417v1"},"cats":{"new-dataset":0.0172785743,"dev-research":0.2248550759,"prompt-eng":0.4771619827,"data-quality":0.2157769301,"ml-security":0.1325722973}}
{"text":"Research in Image Generation has recently made significant progress, particularly boosted by the introduction of Vision-Language models which are able to produce high-quality visual content based on textual inputs.","meta":{"url":"http://arxiv.org/abs/2307.09416v1"},"cats":{"new-dataset":0.2145732236,"dev-research":0.2885692091,"prompt-eng":0.481188398,"data-quality":0.2404595603,"ml-security":0.0375573625}}
{"text":"Despite ongoing advancements in terms of generation quality and realism, no methodical frameworks have been defined yet to quantitatively measure the quality of the generated content and the adherence with the prompted requests: so far, only human-based evaluations have been adopted for quality satisfaction and for comparing different generative methods.","meta":{"url":"http://arxiv.org/abs/2307.09416v1"},"cats":{"new-dataset":0.0791332596,"dev-research":0.3441071727,"prompt-eng":0.4721155344,"data-quality":0.2472328257,"ml-security":0.024165227}}
{"text":"We introduce a novel automated method for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a generated/edited image and the corresponding prompt/instructions, with a process inspired by the human cognitive behaviour.","meta":{"url":"http://arxiv.org/abs/2307.09416v1"},"cats":{"new-dataset":0.0415308154,"dev-research":0.3996257032,"prompt-eng":0.5384804788,"data-quality":0.3394833982,"ml-security":0.0593177957}}
{"text":"ViCE combines the strengths of Large Language Models (LLMs) and Visual Question Answering (VQA) into a unified pipeline, aiming to replicate the human cognitive process in quality assessment.","meta":{"url":"http://arxiv.org/abs/2307.09416v1"},"cats":{"new-dataset":0.0619489173,"dev-research":0.3543604749,"prompt-eng":0.5020922848,"data-quality":0.2074399622,"ml-security":0.05386496}}
{"text":"This method outlines visual concepts, formulates image-specific verification questions, utilizes the Q&A system to investigate the image, and scores the combined outcome.","meta":{"url":"http://arxiv.org/abs/2307.09416v1"},"cats":{"new-dataset":0.0645537814,"dev-research":0.3368817557,"prompt-eng":0.511068219,"data-quality":0.1729015694,"ml-security":0.0538425813}}
{"text":"Although this brave new hypothesis of mimicking humans in the image evaluation process is in its preliminary assessment stage, results are promising and open the door to a new form of automatic evaluation which could have significant impact as the image generation or the image target editing tasks become more and more sophisticated.","meta":{"url":"http://arxiv.org/abs/2307.09416v1"},"cats":{"new-dataset":0.0368379462,"dev-research":0.3238091211,"prompt-eng":0.5473891029,"data-quality":0.2721284076,"ml-security":0.0785217109}}
{"text":"Understanding human-nature interactions and the architecture of coupled human-nature systems is crucial for sustainable development.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.1349923189,"dev-research":0.3745065575,"prompt-eng":0.3942724593,"data-quality":0.0502168775,"ml-security":0.0723545128}}
{"text":"Cultural ecosystem services (CES), defined as intangible benefits derived from nature exposure, contribute to maintaining and improving human well-being.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.1473848637,"dev-research":0.3599766132,"prompt-eng":0.3805378303,"data-quality":0.1239942974,"ml-security":0.0662611805}}
{"text":"However, we have limited understanding of how well-being benefits emerge from CES co-production.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.030624875,"dev-research":0.3972039721,"prompt-eng":0.3827532289,"data-quality":0.1176230267,"ml-security":0.0934189889}}
{"text":"In this study, for the first time, we estimated the global CES network from self-reported interactions between nature features and human activities underpinning CES co-production using social media.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.1932987133,"dev-research":0.3560619589,"prompt-eng":0.4256703051,"data-quality":0.1569741217,"ml-security":0.0618086166}}
{"text":"First, we used a bottom-up, approach to define the global repertoire of nature features and human activities used during CES co-production using 682,000 posts on Reddit.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.5081059441,"dev-research":0.3410313189,"prompt-eng":0.3995976213,"data-quality":0.111467109,"ml-security":0.0478596537}}
{"text":"We then sampled Twitter to estimate the co-occurrence of these features and activities over the past five years, retrieving 41.7 millions tweets.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.4090910655,"dev-research":0.2967879982,"prompt-eng":0.3797426802,"data-quality":0.1010538079,"ml-security":0.079993085}}
{"text":"These tweets were used to estimate the CES bipartite network, where each link was weighted by the number of times nature features and human activities co-occurred in tweets.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.1450869399,"dev-research":0.2871226054,"prompt-eng":0.3895665285,"data-quality":0.1121035736,"ml-security":0.0700570326}}
{"text":"We expected to observe large changes in the CES network topology in relation to the global mobility restrictions during the COVID-19 pandemic.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.1636565418,"dev-research":0.2382441166,"prompt-eng":0.3654396656,"data-quality":0.0593026661,"ml-security":0.1340399982}}
{"text":"This was not the case and the global CES network was generally resilient.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.0477414631,"dev-research":0.2840906927,"prompt-eng":0.3376673136,"data-quality":0.2050015037,"ml-security":0.2189851302}}
{"text":"However, a higher order singular value decomposition of the CES tensor revealed an impulse on the link between self care activities and urban greenspace.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.0473445874,"dev-research":0.2880983038,"prompt-eng":0.3730698804,"data-quality":0.1097969674,"ml-security":0.1006185507}}
{"text":"This could be due to an increased need for self care during the pandemic and urban greenspace enabling CES to be produced locally.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.1211540919,"dev-research":0.3764286779,"prompt-eng":0.3946841292,"data-quality":0.118218508,"ml-security":0.1059574591}}
{"text":"Thus, providing resilience for maintaining well-being during the pandemic.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.1438991525,"dev-research":0.3167136531,"prompt-eng":0.3846095215,"data-quality":0.083252667,"ml-security":0.1683746425}}
{"text":"Our user based analysis also indicated a shift towards local CES production during the beginning of the pandemic.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.2744547975,"dev-research":0.4091781933,"prompt-eng":0.4372529299,"data-quality":0.1248641992,"ml-security":0.1138602757}}
{"text":"Thus, supporting that CES was produced locally.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.1587722553,"dev-research":0.3820442104,"prompt-eng":0.4018918038,"data-quality":0.2245528178,"ml-security":0.0555780811}}
{"text":"These findings suggest an overall need for CES and access to features providing CES in local communities.","meta":{"url":"http://arxiv.org/abs/2307.09408v1"},"cats":{"new-dataset":0.0898617785,"dev-research":0.4879391109,"prompt-eng":0.430185349,"data-quality":0.1252566424,"ml-security":0.0921185226}}
{"text":"Maximizing long-term rewards is the primary goal in sequential decision-making problems.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0232006988,"dev-research":0.2545461584,"prompt-eng":0.4125812677,"data-quality":0.0521070169,"ml-security":0.1592601701}}
{"text":"The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0437783879,"dev-research":0.3587541912,"prompt-eng":0.4379776051,"data-quality":0.0976631456,"ml-security":0.2453264463}}
{"text":"In real-world problems, however, collecting beneficial information is often costly.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0536967416,"dev-research":0.404226739,"prompt-eng":0.3539259924,"data-quality":0.1179498047,"ml-security":0.2409648426}}
{"text":"That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0093347878,"dev-research":0.3405251864,"prompt-eng":0.4416101353,"data-quality":0.0954431641,"ml-security":0.1661426311}}
{"text":"The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0547432906,"dev-research":0.215837215,"prompt-eng":0.4201264697,"data-quality":0.1256506624,"ml-security":0.2058777556}}
{"text":"To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0475054327,"dev-research":0.1943027739,"prompt-eng":0.4414731992,"data-quality":0.1609873374,"ml-security":0.2322426441}}
{"text":"The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0085378227,"dev-research":0.259806647,"prompt-eng":0.3653234295,"data-quality":0.0534916844,"ml-security":0.1128668893}}
{"text":"Therefore, the agent faces a trade-off between minimizing the cost of information acquisition and possibly improving the decision-making process using the obtained information.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0240455366,"dev-research":0.3656791985,"prompt-eng":0.4227569193,"data-quality":0.0862689628,"ml-security":0.1455430457}}
{"text":"To this end, we develop an algorithm that guarantees a sublinear regret in time.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0348571575,"dev-research":0.2360102377,"prompt-eng":0.423531244,"data-quality":0.0926567248,"ml-security":0.1641046192}}
{"text":"Numerical results demonstrate the superiority of our proposed policy in a real-world scenario.","meta":{"url":"http://arxiv.org/abs/2307.09388v1"},"cats":{"new-dataset":0.0293867508,"dev-research":0.2208857808,"prompt-eng":0.3870816765,"data-quality":0.0774724027,"ml-security":0.1725171978}}
{"text":"As the popularity of voice assistants continues to surge, conversational search has gained increased attention in Information Retrieval.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.0698873438,"dev-research":0.2531142852,"prompt-eng":0.3971546417,"data-quality":0.1461447178,"ml-security":0.0983540096}}
{"text":"However, data sparsity issues in conversational search significantly hinder the progress of supervised conversational search methods.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.0445259237,"dev-research":0.2403913343,"prompt-eng":0.3721821239,"data-quality":0.2661827648,"ml-security":0.1361395549}}
{"text":"Consequently, researchers are focusing more on zero-shot conversational search approaches.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.1058571654,"dev-research":0.2474614424,"prompt-eng":0.3349507756,"data-quality":0.1661168234,"ml-security":0.1191531598}}
{"text":"Nevertheless, existing zero-shot methods face three primary limitations: they are not universally applicable to all retrievers, their effectiveness lacks sufficient explainability, and they struggle to resolve common conversational ambiguities caused by omission.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.0332323173,"dev-research":0.2364230757,"prompt-eng":0.3236711981,"data-quality":0.1854945329,"ml-security":0.2149836724}}
{"text":"To address these limitations, we introduce a novel Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based on previous dialogue contexts without requiring supervision from conversational search data.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.2358413671,"dev-research":0.2571578635,"prompt-eng":0.3801974951,"data-quality":0.2161888774,"ml-security":0.1021058256}}
{"text":"Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: coreference and omission, in raw queries.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.087549291,"dev-research":0.3620600091,"prompt-eng":0.473163607,"data-quality":0.2331589148,"ml-security":0.0754547119}}
{"text":"In comparison to existing zero-shot methods, our approach is universally applicable to any retriever without additional adaptation or indexing.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.0740045522,"dev-research":0.1704996549,"prompt-eng":0.3361699168,"data-quality":0.1691569656,"ml-security":0.133273772}}
{"text":"It also provides greater explainability and effectively enhances query intent understanding because ambiguities are explicitly and proactively resolved.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.0040640025,"dev-research":0.4787289839,"prompt-eng":0.4402808134,"data-quality":0.1258501259,"ml-security":0.0903370789}}
{"text":"Through extensive experiments on four TREC conversational datasets, we demonstrate the effectiveness of our method, which consistently outperforms state-of-the-art baselines.","meta":{"url":"http://arxiv.org/abs/2307.09384v1"},"cats":{"new-dataset":0.583076918,"dev-research":0.3291977742,"prompt-eng":0.3959151652,"data-quality":0.2916716502,"ml-security":0.0849897894}}
{"text":"We propose a novel approach to soundly combining linear types with effect handlers.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0163934774,"dev-research":0.2860608405,"prompt-eng":0.4234483764,"data-quality":0.1866175714,"ml-security":0.108187517}}
{"text":"Linear type systems statically ensure that resources such as file handles are used exactly once.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0266212636,"dev-research":0.3044192081,"prompt-eng":0.3769634196,"data-quality":0.0873771067,"ml-security":0.1715473218}}
{"text":"Effect handlers provide a modular programming abstraction for implementing features ranging from exceptions to concurrency.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0196557292,"dev-research":0.4511345842,"prompt-eng":0.4538683615,"data-quality":0.104576153,"ml-security":0.159440704}}
{"text":"Whereas linear type systems bake in the assumption that continuations are invoked exactly once, effect handlers allow continuations to be discarded or invoked more than once.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0044917287,"dev-research":0.276230217,"prompt-eng":0.3975756603,"data-quality":0.0848666671,"ml-security":0.1359643673}}
{"text":"This mismatch leads to soundness bugs in existing systems such as the programming language Links, which combines linearity (for session types) with effect handlers.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0190091417,"dev-research":0.3669901768,"prompt-eng":0.4186738639,"data-quality":0.3415362746,"ml-security":0.1218127975}}
{"text":"We introduce control flow linearity as a means to ensure that continuations are used in accordance with the linearity of any resources they capture, ruling out such soundness bugs.   ","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0154862632,"dev-research":0.2972807652,"prompt-eng":0.3913488162,"data-quality":0.1540541554,"ml-security":0.2014151166}}
{"text":"We formalise control flow linearity in a System F-style core calculus Feffpop equipped with linear types, effect types, and effect handlers.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0220787404,"dev-research":0.3005127031,"prompt-eng":0.4476854432,"data-quality":0.0976429446,"ml-security":0.1478035993}}
{"text":"We define a linearity-aware semantics to formally prove that Feffpop preserves the integrity of linear values in the sense that no linear value is discarded or duplicated.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0624447898,"dev-research":0.3401803763,"prompt-eng":0.4227974339,"data-quality":0.4072014039,"ml-security":0.2000869417}}
{"text":"In order to show that control flow linearity can be made practical, we adapt Links based on the design of Feffpop, in doing so fixing a long-standing soundness bug.   ","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0429606154,"dev-research":0.295773469,"prompt-eng":0.4530399209,"data-quality":0.1868972582,"ml-security":0.1035059857}}
{"text":"Finally, to better expose the potential of control flow linearity, we define an ML-style core calculus Qeffpop, based on qualified types, which requires no programmer provided annotations, and instead relies entirely on type inference to infer control flow linearity.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0345278671,"dev-research":0.3201129303,"prompt-eng":0.4601164028,"data-quality":0.1032898928,"ml-security":0.1940650318}}
{"text":"Both linearity and effects are captured by qualified types.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0078299367,"dev-research":0.2963536257,"prompt-eng":0.4071379592,"data-quality":0.1587160347,"ml-security":0.1588484199}}
{"text":"Qeffpop overcomes a number of practical limitations of Feffpop, supporting abstraction over linearity, linearity dependencies between type variables, and a much more fine-grained notion of control flow linearity.","meta":{"url":"http://arxiv.org/abs/2307.09383v1"},"cats":{"new-dataset":0.0457330134,"dev-research":0.2990164785,"prompt-eng":0.4354394375,"data-quality":0.0730238685,"ml-security":0.1059432323}}
{"text":"Since its launch in November 2022, ChatGPT has gained popularity among users, especially programmers who use it as a tool to solve development problems.","meta":{"url":"http://arxiv.org/abs/2307.09381v1"},"cats":{"new-dataset":0.4364245022,"dev-research":0.4428111144,"prompt-eng":0.3992190169,"data-quality":0.0864184514,"ml-security":0.0994641044}}
{"text":"However, while offering a practical solution to programming problems, ChatGPT should be mainly used as a supporting tool (e.g., in software education) rather than as a replacement for the human being.","meta":{"url":"http://arxiv.org/abs/2307.09381v1"},"cats":{"new-dataset":0.1537041996,"dev-research":0.442918029,"prompt-eng":0.3928636225,"data-quality":0.1130609994,"ml-security":0.1028698753}}
{"text":"Thus, detecting automatically generated source code by ChatGPT is necessary, and tools for identifying AI-generated content may need to be adapted to work effectively with source code.","meta":{"url":"http://arxiv.org/abs/2307.09381v1"},"cats":{"new-dataset":0.253853172,"dev-research":0.4903281195,"prompt-eng":0.433345641,"data-quality":0.2995709134,"ml-security":0.1633309904}}
{"text":"This paper presents an empirical study to investigate the feasibility of automated identification of AI-generated code snippets, and the factors that influence this ability.","meta":{"url":"http://arxiv.org/abs/2307.09381v1"},"cats":{"new-dataset":0.0578097917,"dev-research":0.5017909813,"prompt-eng":0.4645862969,"data-quality":0.377225104,"ml-security":0.2636706216}}
{"text":"To this end, we propose a novel approach called GPTSniffer, which builds on top of CodeBERT to detect source code written by AI.","meta":{"url":"http://arxiv.org/abs/2307.09381v1"},"cats":{"new-dataset":0.1808289,"dev-research":0.4667092494,"prompt-eng":0.3950613679,"data-quality":0.2177294909,"ml-security":0.2235391301}}
{"text":"The results show that GPTSniffer can accurately classify whether code is human-written or AI-generated, and outperforms two baselines, GPTZero and OpenAI Text Classifier.","meta":{"url":"http://arxiv.org/abs/2307.09381v1"},"cats":{"new-dataset":0.1955956684,"dev-research":0.4258827703,"prompt-eng":0.3898379246,"data-quality":0.2685855129,"ml-security":0.1429674298}}
{"text":"Also, the study shows how similar training data or a classification context with paired snippets helps to boost classification performances.","meta":{"url":"http://arxiv.org/abs/2307.09381v1"},"cats":{"new-dataset":0.0562171898,"dev-research":0.3825104099,"prompt-eng":0.3938868363,"data-quality":0.2250985661,"ml-security":0.133385413}}
{"text":"The use of machine learning in algorithmic trading systems is increasingly common.","meta":{"url":"http://arxiv.org/abs/2307.09377v1"},"cats":{"new-dataset":0.0266842456,"dev-research":0.2789114058,"prompt-eng":0.3666667561,"data-quality":0.1041906482,"ml-security":0.3105182043}}
{"text":"In a typical set-up, supervised learning is used to predict the future prices of assets, and those predictions drive a simple trading and execution strategy.","meta":{"url":"http://arxiv.org/abs/2307.09377v1"},"cats":{"new-dataset":0.0391281141,"dev-research":0.2830728559,"prompt-eng":0.4788286365,"data-quality":0.1339433376,"ml-security":0.2594990693}}
{"text":"This is quite effective when the predictions have sufficient signal, markets are liquid, and transaction costs are low.","meta":{"url":"http://arxiv.org/abs/2307.09377v1"},"cats":{"new-dataset":0.0101154732,"dev-research":0.3060578678,"prompt-eng":0.4195951853,"data-quality":0.1365258875,"ml-security":0.1505938993}}
{"text":"However, those conditions often do not hold in thinly traded financial markets and markets for differentiated assets such as real estate or vehicles.","meta":{"url":"http://arxiv.org/abs/2307.09377v1"},"cats":{"new-dataset":0.0229139414,"dev-research":0.2047772457,"prompt-eng":0.3213426509,"data-quality":0.1292180974,"ml-security":0.1272891583}}
{"text":"In these markets, the trading strategy must consider the long-term effects of taking positions that are relatively more difficult to change.","meta":{"url":"http://arxiv.org/abs/2307.09377v1"},"cats":{"new-dataset":0.0086027937,"dev-research":0.2471795492,"prompt-eng":0.3434086575,"data-quality":0.0620705559,"ml-security":0.1949741609}}
{"text":"In this work, we propose a Reinforcement Learning (RL) algorithm that trades based on signals from a learned predictive model and addresses these challenges.","meta":{"url":"http://arxiv.org/abs/2307.09377v1"},"cats":{"new-dataset":0.0812882137,"dev-research":0.1998745753,"prompt-eng":0.4185951899,"data-quality":0.0737017019,"ml-security":0.2212235672}}
{"text":"We test our algorithm on 20+ years of equity data from Bursa Malaysia.","meta":{"url":"http://arxiv.org/abs/2307.09377v1"},"cats":{"new-dataset":0.3988500471,"dev-research":0.2243718703,"prompt-eng":0.3867067071,"data-quality":0.0912809613,"ml-security":0.0705751561}}
{"text":"We introduce an operator on classes of regular languages, the star-free closure.","meta":{"url":"http://arxiv.org/abs/2307.09376v1"},"cats":{"new-dataset":0.0964342178,"dev-research":0.2380393439,"prompt-eng":0.4000765415,"data-quality":0.2331824903,"ml-security":0.1369335502}}
{"text":"Our motivation is to generalize standard results of automata theory within a unified framework.","meta":{"url":"http://arxiv.org/abs/2307.09376v1"},"cats":{"new-dataset":0.0167305074,"dev-research":0.293901619,"prompt-eng":0.4624376433,"data-quality":0.122917919,"ml-security":0.1507210453}}
{"text":"Given an arbitrary input class $C$, the star-free closure operator outputs the least class closed under Boolean operations and language concatenation, and containing all languages of $C$ as well as all finite languages.","meta":{"url":"http://arxiv.org/abs/2307.09376v1"},"cats":{"new-dataset":0.0618792669,"dev-research":0.2727540699,"prompt-eng":0.3837355454,"data-quality":0.1875790545,"ml-security":0.1163391703}}
{"text":"We establish several equivalent characterizations of star-free closure: in terms of regular expressions, first-order logic, pure future and future-past temporal logic, and recognition by finite monoids.","meta":{"url":"http://arxiv.org/abs/2307.09376v1"},"cats":{"new-dataset":0.0463420784,"dev-research":0.2328136964,"prompt-eng":0.3952504101,"data-quality":0.1203735348,"ml-security":0.0927957491}}
{"text":"A key ingredient is that star-free closure coincides with another closure operator, defined in terms of regular operations where Kleene stars are allowed in restricted~contexts.   ","meta":{"url":"http://arxiv.org/abs/2307.09376v1"},"cats":{"new-dataset":0.020650429,"dev-research":0.2713570796,"prompt-eng":0.3831497061,"data-quality":0.1612972818,"ml-security":0.0902054076}}
{"text":"A consequence of this first result is that we can decide membership of a regular language in the star-free closure of a class whose separation problem is decidable.","meta":{"url":"http://arxiv.org/abs/2307.09376v1"},"cats":{"new-dataset":0.0435063012,"dev-research":0.2216115628,"prompt-eng":0.3910470789,"data-quality":0.2939227935,"ml-security":0.1436982616}}
{"text":"Moreover, we prove that separation itself is decidable for the star-free closure of any finite class, and of any class of group languages having itself decidable separation (plus mild additional properties).","meta":{"url":"http://arxiv.org/abs/2307.09376v1"},"cats":{"new-dataset":0.0528362958,"dev-research":0.231957468,"prompt-eng":0.3614605986,"data-quality":0.1630026677,"ml-security":0.1369831668}}
{"text":"We actually show decidability of a stronger property, called covering.","meta":{"url":"http://arxiv.org/abs/2307.09376v1"},"cats":{"new-dataset":0.0454821613,"dev-research":0.293660176,"prompt-eng":0.4256518134,"data-quality":0.1645007474,"ml-security":0.2527683058}}
{"text":"Deep neural networks (DNNs) have demonstrated their outperformance in various software systems, but also exhibit misbehavior and even result in irreversible disasters.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.21428522,"dev-research":0.4960748193,"prompt-eng":0.3828518417,"data-quality":0.3255431564,"ml-security":0.671832723}}
{"text":"Therefore, it is crucial to identify the misbehavior of DNN-based software and improve DNNs' quality.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.0543874333,"dev-research":0.5708972231,"prompt-eng":0.3924905891,"data-quality":0.3692600957,"ml-security":0.3822376702}}
{"text":"Test input prioritization is one of the most appealing ways to guarantee DNNs' quality, which prioritizes test inputs so that more bug-revealing inputs can be identified earlier with limited time and manual labeling efforts.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.0428343314,"dev-research":0.4415267665,"prompt-eng":0.4781207188,"data-quality":0.3061823901,"ml-security":0.2938687518}}
{"text":"However, the existing prioritization methods are still limited from three aspects: certifiability, effectiveness, and generalizability.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.0105632825,"dev-research":0.234816849,"prompt-eng":0.3912379198,"data-quality":0.0647584882,"ml-security":0.0922918238}}
{"text":"To overcome the challenges, we propose CertPri, a test input prioritization technique designed based on a movement cost perspective of test inputs in DNNs' feature space.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.0871437145,"dev-research":0.3876101542,"prompt-eng":0.469072202,"data-quality":0.130736519,"ml-security":0.212723672}}
{"text":"CertPri differs from previous works in three key aspects: (1) certifiable: it provides a formal robustness guarantee for the movement cost; (2) effective: it leverages formally guaranteed movement costs to identify malicious bug-revealing inputs; and (3) generic: it can be applied to various tasks, data, models, and scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.0417212646,"dev-research":0.3220740416,"prompt-eng":0.4104182416,"data-quality":0.1147433534,"ml-security":0.1975457711}}
{"text":"Extensive evaluations across 2 tasks (i.e., classification and regression), 6 data forms, 4 model structures, and 2 scenarios (i.e., white-box and black-box) demonstrate CertPri's superior performance.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.0809433542,"dev-research":0.3451521192,"prompt-eng":0.4212213628,"data-quality":0.0775864917,"ml-security":0.1303681078}}
{"text":"For instance, it significantly improves 53.97% prioritization effectiveness on average compared with baselines.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.0082711036,"dev-research":0.3777684316,"prompt-eng":0.4325235327,"data-quality":0.097655686,"ml-security":0.0689097797}}
{"text":"Its robustness and generalizability are 1.41~2.00 times and 1.33~3.39 times that of baselines on average, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09375v1"},"cats":{"new-dataset":0.0467817353,"dev-research":0.2909252394,"prompt-eng":0.4130155187,"data-quality":0.1735573243,"ml-security":0.0972487657}}
{"text":"Support Vector Machines (SVM) have gathered significant acclaim as classifiers due to their successful implementation of Statistical Learning Theory.","meta":{"url":"http://arxiv.org/abs/2307.09372v1"},"cats":{"new-dataset":0.0613443734,"dev-research":0.3277613718,"prompt-eng":0.4492568844,"data-quality":0.2983017399,"ml-security":0.2582296455}}
{"text":"However, in the context of multiclass and multilabel settings, the reliance on vector-based formulations in existing SVM-based models poses limitations regarding flexibility and ease of incorporating additional terms to handle specific challenges.","meta":{"url":"http://arxiv.org/abs/2307.09372v1"},"cats":{"new-dataset":0.0317573689,"dev-research":0.2779059677,"prompt-eng":0.442616044,"data-quality":0.3437981289,"ml-security":0.1490917473}}
{"text":"To overcome these limitations, our research paper focuses on introducing a matrix formulation for SVM that effectively addresses these constraints.","meta":{"url":"http://arxiv.org/abs/2307.09372v1"},"cats":{"new-dataset":0.0798540566,"dev-research":0.2763223322,"prompt-eng":0.3998716984,"data-quality":0.1942775337,"ml-security":0.171775533}}
{"text":"By employing the Accelerated Gradient Descent method in the dual, we notably enhance the efficiency of solving the Matrix-SVM problem.","meta":{"url":"http://arxiv.org/abs/2307.09372v1"},"cats":{"new-dataset":0.0176126662,"dev-research":0.2228597427,"prompt-eng":0.3706327638,"data-quality":0.130456612,"ml-security":0.1615742958}}
{"text":"Experimental evaluations on multilabel and multiclass datasets demonstrate that Matrix SVM achieves superior time efficacy while delivering similar results to Binary Relevance SVM.   ","meta":{"url":"http://arxiv.org/abs/2307.09372v1"},"cats":{"new-dataset":0.0634466397,"dev-research":0.298488895,"prompt-eng":0.4205551027,"data-quality":0.3494169185,"ml-security":0.1378968944}}
{"text":"Moreover, our matrix formulation unveils crucial insights and advantages that may not be readily apparent in traditional vector-based notations.","meta":{"url":"http://arxiv.org/abs/2307.09372v1"},"cats":{"new-dataset":0.0112062204,"dev-research":0.3133763316,"prompt-eng":0.3860801986,"data-quality":0.1234487207,"ml-security":0.1256544751}}
{"text":"We emphasize that numerous multilabel models can be viewed as extensions of SVM, with customised modifications to meet specific requirements.","meta":{"url":"http://arxiv.org/abs/2307.09372v1"},"cats":{"new-dataset":0.1497264251,"dev-research":0.2664101534,"prompt-eng":0.4801744601,"data-quality":0.4463302654,"ml-security":0.1078041465}}
{"text":"The matrix formulation presented in this paper establishes a solid foundation for developing more sophisticated models capable of effectively addressing the distinctive challenges encountered in multilabel learning.","meta":{"url":"http://arxiv.org/abs/2307.09372v1"},"cats":{"new-dataset":0.1004916838,"dev-research":0.2298481252,"prompt-eng":0.457250078,"data-quality":0.4178787636,"ml-security":0.1077052969}}
{"text":"We present and evaluate the ExaNeSt Prototype, a liquid-cooled rack prototype consisting of 256 Xilinx ZU9EG MPSoCs, 4 TBytes of DRAM, 16 TBytes of SSD, and configurable interconnection 10-Gbps hardware.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.2707267244,"dev-research":0.2684632165,"prompt-eng":0.3949016866,"data-quality":0.0772069646,"ml-security":0.0543442841}}
{"text":"We developed this testbed in 2016-2019 to validate the flexibility of FPGAs for experimenting with efficient hardware support for HPC communication among tens of thousands of processors and accelerators in the quest towards Exascale systems and beyond.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.1079432288,"dev-research":0.3090568573,"prompt-eng":0.3897092218,"data-quality":0.0789512762,"ml-security":0.0924628659}}
{"text":"We present our key design choices reagrding overall system architecture, PCBs and runtime software, and summarize insights resulting from measurement and analysis.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.1557975264,"dev-research":0.3836372923,"prompt-eng":0.3695226005,"data-quality":0.0667209955,"ml-security":0.071959006}}
{"text":"Of particular note, our custom interconnect includes a low-cost low-latency network interface, offering user-level zero-copy RDMA, which we have tightly coupled with the ARMv8 processors in the MPSoCs.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.1289454626,"dev-research":0.2617794313,"prompt-eng":0.3471640879,"data-quality":0.0773388939,"ml-security":0.0575632578}}
{"text":"We have developed a system software runtime on top of these features, and have been able to run MPI.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.1297174863,"dev-research":0.278241486,"prompt-eng":0.3715096688,"data-quality":0.0746869578,"ml-security":0.0509186223}}
{"text":"We have evaluated our testbed through MPI microbenchmarks, mini, and full MPI applications.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.1013520669,"dev-research":0.2970503419,"prompt-eng":0.403715099,"data-quality":0.1395680647,"ml-security":0.0642917213}}
{"text":"Single hop, one way latency is $1.3$~$\\mu$s; approximately $0.47$~$\\mu$s out of these are attributed to network interface and the user-space library that exposes its functionality to the runtime.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.0318634718,"dev-research":0.2572205917,"prompt-eng":0.3793722838,"data-quality":0.0681607894,"ml-security":0.0555916606}}
{"text":"Latency over longer paths increases as expected, reaching $2.55$~$\\mu$s for a five-hop path.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.0194966435,"dev-research":0.2290007848,"prompt-eng":0.39383208,"data-quality":0.0677139128,"ml-security":0.0400297729}}
{"text":"Bandwidth tests show that, for a single hop, link utilization reaches $82\\%$ of the theoretical capacity.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.0344064957,"dev-research":0.1974926817,"prompt-eng":0.3558267524,"data-quality":0.0750626245,"ml-security":0.0576359641}}
{"text":"Microbenchmarks based on MPI collectives reveal that broadcast latency scales as expected when the number of participating ranks increases.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.0479422728,"dev-research":0.2693183182,"prompt-eng":0.40849532,"data-quality":0.1140020108,"ml-security":0.0659300877}}
{"text":"We also implemented a custom Allreduce accelerator in the network interface, which reduces the latency of such collectives by up to $88\\%$. We assess performance scaling through weak and strong scaling tests for HPCG, LAMMPS, and the miniFE mini application; for all these tests, parallelization efficiency is at least $69\\%$, or better.","meta":{"url":"http://arxiv.org/abs/2307.09371v1"},"cats":{"new-dataset":0.0245526172,"dev-research":0.2000456626,"prompt-eng":0.3670130825,"data-quality":0.08139803,"ml-security":0.0735146289}}
{"text":"Explanations in XAI are typically developed by AI experts and focus on algorithmic transparency and the inner workings of AI systems.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.0448314708,"dev-research":0.4657885109,"prompt-eng":0.3967289941,"data-quality":0.1584138547,"ml-security":0.3200357198}}
{"text":"Research has shown that such explanations do not meet the needs of users who do not have AI expertise.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.012006615,"dev-research":0.478914691,"prompt-eng":0.3907892496,"data-quality":0.1714846721,"ml-security":0.3053968714}}
{"text":"As a result, explanations are often ineffective in making system decisions interpretable and understandable.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.0097427358,"dev-research":0.5486708788,"prompt-eng":0.40846368,"data-quality":0.3381410851,"ml-security":0.3517299328}}
{"text":"We aim to strengthen a socio-technical view of AI by following a Human-Centered Explainable Artificial Intelligence (HC-XAI) approach, which investigates the explanation needs of end-users (i.e., subject matter experts and lay users) in specific usage contexts.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.0849093939,"dev-research":0.5078837432,"prompt-eng":0.4108197308,"data-quality":0.1002322552,"ml-security":0.1871981602}}
{"text":"One of the most influential works in this area is the XAI Question Bank (XAIQB) by Liao et al.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.370434546,"dev-research":0.2842544743,"prompt-eng":0.3726036705,"data-quality":0.119839224,"ml-security":0.0869209743}}
{"text":"The authors propose a set of questions that end-users might ask when using an AI system, which in turn is intended to help developers and designers identify and address explanation needs.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.1003977172,"dev-research":0.5128688263,"prompt-eng":0.4815823925,"data-quality":0.1197808154,"ml-security":0.2324104395}}
{"text":"Although the XAIQB is widely referenced, there are few reports of its use in practice.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.1498757114,"dev-research":0.3319300336,"prompt-eng":0.3708482073,"data-quality":0.1381847143,"ml-security":0.0870924076}}
{"text":"In particular, it is unclear to what extent the XAIQB sufficiently captures the explanation needs of end-users and what potential problems exist in the practical application of the XAIQB.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.0435411974,"dev-research":0.4145524612,"prompt-eng":0.3941281981,"data-quality":0.149005529,"ml-security":0.1657293636}}
{"text":"To explore these open questions, we used the XAIQB as the basis for analyzing 12 think-aloud software explorations with subject matter experts.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.3216279147,"dev-research":0.4355696812,"prompt-eng":0.4523088367,"data-quality":0.1259110929,"ml-security":0.1178513439}}
{"text":"We investigated the suitability of the XAIQB as a tool for identifying explanation needs in a specific usage context.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.0666677098,"dev-research":0.50305596,"prompt-eng":0.4564557066,"data-quality":0.2094630645,"ml-security":0.1070286732}}
{"text":"Our analysis revealed a number of explanation needs that were missing from the question bank, but that emerged repeatedly as our study participants interacted with an AI system.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.0687705054,"dev-research":0.4579843885,"prompt-eng":0.431165399,"data-quality":0.1808737264,"ml-security":0.1713317178}}
{"text":"We also found that some of the XAIQB questions were difficult to distinguish and required interpretation during use.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.0399187302,"dev-research":0.401039111,"prompt-eng":0.4105575281,"data-quality":0.2544656086,"ml-security":0.102089562}}
{"text":"Our contribution is an extension of the XAIQB with 11 new questions.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.3429899098,"dev-research":0.3374942959,"prompt-eng":0.4049157827,"data-quality":0.1284520352,"ml-security":0.0823672105}}
{"text":"In addition, we have expanded the descriptions of all new and existing questions to facilitate their use.","meta":{"url":"http://arxiv.org/abs/2307.09369v1"},"cats":{"new-dataset":0.1467454633,"dev-research":0.4329689379,"prompt-eng":0.4438087124,"data-quality":0.1310706278,"ml-security":0.061798946}}
{"text":"Audio-driven talking face generation is the task of creating a lip-synchronized, realistic face video from given audio and reference frames.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.1478087107,"dev-research":0.2657249594,"prompt-eng":0.3595038712,"data-quality":0.1039308977,"ml-security":0.070021705}}
{"text":"This involves two major challenges: overall visual quality of generated images on the one hand, and audio-visual synchronization of the mouth part on the other hand.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.0638868334,"dev-research":0.2536022041,"prompt-eng":0.350486581,"data-quality":0.1313907394,"ml-security":0.0447137161}}
{"text":"In this paper, we start by identifying several problematic aspects of synchronization methods in recent audio-driven talking face generation approaches.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.0526518527,"dev-research":0.2693399048,"prompt-eng":0.3898105666,"data-quality":0.1527689291,"ml-security":0.0949873185}}
{"text":"Specifically, this involves unintended flow of lip and pose information from the reference to the generated image, as well as instabilities during model training.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.0411816796,"dev-research":0.2935926982,"prompt-eng":0.4247170448,"data-quality":0.2067231585,"ml-security":0.1976226475}}
{"text":"Subsequently, we propose various techniques for obviating these issues: First, a silent-lip reference image generator prevents leaking of lips from the reference to the generated image.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.1348031199,"dev-research":0.2687864475,"prompt-eng":0.4111365092,"data-quality":0.3612394871,"ml-security":0.1622486064}}
{"text":"Second, an adaptive triplet loss handles the pose leaking problem.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.0326472301,"dev-research":0.2043620794,"prompt-eng":0.3559230842,"data-quality":0.1748926043,"ml-security":0.2485846548}}
{"text":"Finally, we propose a stabilized formulation of synchronization loss, circumventing aforementioned training instabilities while additionally further alleviating the lip leaking issue.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.027515305,"dev-research":0.2170129464,"prompt-eng":0.3443305225,"data-quality":0.2296288572,"ml-security":0.3471508996}}
{"text":"Combining the individual improvements, we present state-of-the art performance on LRS2 and LRW in both synchronization and visual quality.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.0507606403,"dev-research":0.3120534458,"prompt-eng":0.4009472649,"data-quality":0.1309419551,"ml-security":0.0325079686}}
{"text":"We further validate our design in various ablation experiments, confirming the individual contributions as well as their complementary effects.","meta":{"url":"http://arxiv.org/abs/2307.09368v1"},"cats":{"new-dataset":0.0282002863,"dev-research":0.2254769141,"prompt-eng":0.3995141081,"data-quality":0.1137349188,"ml-security":0.0467103533}}
{"text":"We consider the problem of learning a sparse graph underlying an undirected Gaussian graphical model, a key problem in statistical machine learning.","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.0898184392,"dev-research":0.1899708545,"prompt-eng":0.3645630744,"data-quality":0.232295918,"ml-security":0.2732086343}}
{"text":"Given $n$ samples from a multivariate Gaussian distribution with $p$ variables, the goal is to estimate the $p \\times p$ inverse covariance matrix (aka precision matrix), assuming it is sparse (i.e., has a few nonzero entries).","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.0617824159,"dev-research":0.1812562635,"prompt-eng":0.3819157902,"data-quality":0.129186005,"ml-security":0.0811504832}}
{"text":"We propose GraphL0BnB, a new estimator based on an $\\ell_0$-penalized version of the pseudolikelihood function, while most earlier approaches are based on the $\\ell_1$-relaxation.","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.1423410183,"dev-research":0.1595745416,"prompt-eng":0.3827220394,"data-quality":0.2719815351,"ml-security":0.1547491441}}
{"text":"Our estimator can be formulated as a convex mixed integer program (MIP) which can be difficult to compute at scale using off-the-shelf commercial solvers.","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.0635555794,"dev-research":0.1889701557,"prompt-eng":0.3826879534,"data-quality":0.0905223446,"ml-security":0.10633754}}
{"text":"To solve the MIP, we propose a custom nonlinear branch-and-bound (BnB) framework that solves node relaxations with tailored first-order methods.","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.0281124918,"dev-research":0.1850923252,"prompt-eng":0.381929891,"data-quality":0.0765096681,"ml-security":0.0535151631}}
{"text":"As a by-product of our BnB framework, we propose large-scale solvers for obtaining good primal solutions that are of independent interest.","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.0452845129,"dev-research":0.2156862388,"prompt-eng":0.3924517525,"data-quality":0.0550408078,"ml-security":0.1034281642}}
{"text":"We derive novel statistical guarantees (estimation and variable selection) for our estimator and discuss how our approach improves upon existing estimators.","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.0625673284,"dev-research":0.2147390683,"prompt-eng":0.433655948,"data-quality":0.1520816916,"ml-security":0.1495024043}}
{"text":"Our numerical experiments on real/synthetic datasets suggest that our method can solve, to near-optimality, problem instances with $p = 10^4$ -- corresponding to a symmetric matrix of size $p \\times p$ with $p^2/2$ binary variables.","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.1661879028,"dev-research":0.1848286802,"prompt-eng":0.3638859126,"data-quality":0.1137316791,"ml-security":0.1726944056}}
{"text":"We demonstrate the usefulness of GraphL0BnB versus various state-of-the-art approaches on a range of datasets.","meta":{"url":"http://arxiv.org/abs/2307.09366v1"},"cats":{"new-dataset":0.7853390406,"dev-research":0.287283266,"prompt-eng":0.3286317504,"data-quality":0.2203256342,"ml-security":0.0888608137}}
{"text":"Zero-cost proxies are nowadays frequently studied and used to search for neural architectures.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.0286966566,"dev-research":0.1912657069,"prompt-eng":0.3849797649,"data-quality":0.1019728264,"ml-security":0.3376826731}}
{"text":"They show an impressive ability to predict the performance of architectures by making use of their untrained weights.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.0249546165,"dev-research":0.3409526234,"prompt-eng":0.4310061497,"data-quality":0.1051376221,"ml-security":0.2307601065}}
{"text":"These techniques allow for immense search speed-ups.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.0113788107,"dev-research":0.2833222726,"prompt-eng":0.4212512488,"data-quality":0.0794570455,"ml-security":0.1156872378}}
{"text":"So far the joint search for well-performing and robust architectures has received much less attention in the field of NAS.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.0644196887,"dev-research":0.2858381514,"prompt-eng":0.3554360876,"data-quality":0.0885775592,"ml-security":0.0859502502}}
{"text":"Therefore, the main focus of zero-cost proxies is the clean accuracy of architectures, whereas the model robustness should play an evenly important part.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.0110362533,"dev-research":0.2713197282,"prompt-eng":0.3595967773,"data-quality":0.1653431949,"ml-security":0.2669962571}}
{"text":"In this paper, we analyze the ability of common zero-cost proxies to serve as performance predictors for robustness in the popular NAS-Bench-201 search space.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.0447163292,"dev-research":0.1944934671,"prompt-eng":0.3844318046,"data-quality":0.1599275512,"ml-security":0.2685151202}}
{"text":"We are interested in the single prediction task for robustness and the joint multi-objective of clean and robust accuracy.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.0557621852,"dev-research":0.2616809265,"prompt-eng":0.4263480485,"data-quality":0.2549400164,"ml-security":0.2095734336}}
{"text":"We further analyze the feature importance of the proxies and show that predicting the robustness makes the prediction task from existing zero-cost proxies more challenging.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.0297055793,"dev-research":0.2843280313,"prompt-eng":0.4415815861,"data-quality":0.2690345285,"ml-security":0.427281238}}
{"text":"As a result, the joint consideration of several proxies becomes necessary to predict a model's robustness while the clean accuracy can be regressed from a single such feature.","meta":{"url":"http://arxiv.org/abs/2307.09365v1"},"cats":{"new-dataset":0.004941971,"dev-research":0.2985918671,"prompt-eng":0.4230926135,"data-quality":0.3283944836,"ml-security":0.2640801755}}
{"text":"An important open question in human-robot interaction (HRI) is precisely when an agent should decide to communicate, particularly in a cooperative task.","meta":{"url":"http://arxiv.org/abs/2307.09364v1"},"cats":{"new-dataset":0.0318724521,"dev-research":0.3084098543,"prompt-eng":0.4487986266,"data-quality":0.0605402608,"ml-security":0.0575390704}}
{"text":"Perceptual Control Theory (PCT) tells us that agents are able to cooperate on a joint task simply by sharing the same 'intention', thereby distributing the effort required to complete the task among the agents.","meta":{"url":"http://arxiv.org/abs/2307.09364v1"},"cats":{"new-dataset":0.0125088817,"dev-research":0.2846605064,"prompt-eng":0.4608402429,"data-quality":0.0702476721,"ml-security":0.1007455386}}
{"text":"This is even true for agents that do not possess the same abilities, so long as the goal is observable, the combined actions are sufficient to complete the task, and there is no local minimum in the search space.","meta":{"url":"http://arxiv.org/abs/2307.09364v1"},"cats":{"new-dataset":0.0057594248,"dev-research":0.1966143896,"prompt-eng":0.3852595733,"data-quality":0.0590356384,"ml-security":0.0927034397}}
{"text":"If these conditions hold, then a cooperative task can be accomplished without any communication between the contributing agents.","meta":{"url":"http://arxiv.org/abs/2307.09364v1"},"cats":{"new-dataset":0.0160240204,"dev-research":0.2306859605,"prompt-eng":0.4090382746,"data-quality":0.0790433374,"ml-security":0.1206846982}}
{"text":"However, for tasks that do contain local minima, the global solution can only be reached if at least one of the agents adapts its intention at the appropriate moments, and this can only be achieved by appropriately timed communication.","meta":{"url":"http://arxiv.org/abs/2307.09364v1"},"cats":{"new-dataset":0.0197119628,"dev-research":0.1993534561,"prompt-eng":0.3957557378,"data-quality":0.0656635157,"ml-security":0.0995555621}}
{"text":"In other words, it is hypothesised that in cooperative tasks, the function of communication is to coordinate actions in a complex search space that contains local minima.","meta":{"url":"http://arxiv.org/abs/2307.09364v1"},"cats":{"new-dataset":0.0181764498,"dev-research":0.2920559235,"prompt-eng":0.4037411591,"data-quality":0.0699639626,"ml-security":0.1066452499}}
{"text":"These principles have been verified in a computer-based simulation environment in which two independent one-dimensional agents are obliged to cooperate in order to solve a two-dimensional path-finding task.","meta":{"url":"http://arxiv.org/abs/2307.09364v1"},"cats":{"new-dataset":0.0196932979,"dev-research":0.215764718,"prompt-eng":0.4051653549,"data-quality":0.0431516574,"ml-security":0.1146203334}}
{"text":"Most prior semantic segmentation methods have been developed for day-time scenes, while typically underperforming in night-time scenes due to insufficient and complicated lighting conditions.","meta":{"url":"http://arxiv.org/abs/2307.09362v1"},"cats":{"new-dataset":0.1162719339,"dev-research":0.2270812858,"prompt-eng":0.3870579607,"data-quality":0.184477871,"ml-security":0.0638097922}}
{"text":"In this work, we tackle this challenge by proposing a novel night-time semantic segmentation paradigm, i.e., disentangle then parse (DTP).","meta":{"url":"http://arxiv.org/abs/2307.09362v1"},"cats":{"new-dataset":0.2339587422,"dev-research":0.211751987,"prompt-eng":0.3952500661,"data-quality":0.1573503164,"ml-security":0.0879597203}}
{"text":"DTP explicitly disentangles night-time images into light-invariant reflectance and light-specific illumination components and then recognizes semantics based on their adaptive fusion.","meta":{"url":"http://arxiv.org/abs/2307.09362v1"},"cats":{"new-dataset":0.0643698662,"dev-research":0.281669,"prompt-eng":0.4162690497,"data-quality":0.1791084611,"ml-security":0.0657617666}}
{"text":"Concretely, the proposed DTP comprises two key components: 1) Instead of processing lighting-entangled features as in prior works, our Semantic-Oriented Disentanglement (SOD) framework enables the extraction of reflectance component without being impeded by lighting, allowing the network to consistently recognize the semantics under cover of varying and complicated lighting conditions.","meta":{"url":"http://arxiv.org/abs/2307.09362v1"},"cats":{"new-dataset":0.0403869013,"dev-research":0.2947785398,"prompt-eng":0.419253126,"data-quality":0.1146501227,"ml-security":0.071502461}}
{"text":"2) Based on the observation that the illumination component can serve as a cue for some semantically confused regions, we further introduce an Illumination-Aware Parser (IAParser) to explicitly learn the correlation between semantics and lighting, and aggregate the illumination features to yield more precise predictions.","meta":{"url":"http://arxiv.org/abs/2307.09362v1"},"cats":{"new-dataset":0.0965688739,"dev-research":0.3659935702,"prompt-eng":0.4678872603,"data-quality":0.3381131214,"ml-security":0.0591236208}}
{"text":"Extensive experiments on the night-time segmentation task with various settings demonstrate that DTP significantly outperforms state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.09362v1"},"cats":{"new-dataset":0.0771578327,"dev-research":0.2207096317,"prompt-eng":0.4008655427,"data-quality":0.0977283991,"ml-security":0.0481489571}}
{"text":"Furthermore, with negligible additional parameters, DTP can be directly used to benefit existing day-time methods for night-time segmentation.","meta":{"url":"http://arxiv.org/abs/2307.09362v1"},"cats":{"new-dataset":0.042165215,"dev-research":0.2415277513,"prompt-eng":0.3723353864,"data-quality":0.0774307279,"ml-security":0.0538833366}}
{"text":"Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets.","meta":{"url":"http://arxiv.org/abs/2307.09361v1"},"cats":{"new-dataset":0.1796093957,"dev-research":0.2649718138,"prompt-eng":0.4246055409,"data-quality":0.2886133176,"ml-security":0.2577048914}}
{"text":"Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods.","meta":{"url":"http://arxiv.org/abs/2307.09361v1"},"cats":{"new-dataset":0.0375054603,"dev-research":0.2272166362,"prompt-eng":0.4385688755,"data-quality":0.2409202283,"ml-security":0.257889038}}
{"text":"In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details).","meta":{"url":"http://arxiv.org/abs/2307.09361v1"},"cats":{"new-dataset":0.0813528589,"dev-research":0.2330363436,"prompt-eng":0.4141405981,"data-quality":0.1319787583,"ml-security":0.1935010199}}
{"text":"Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way.","meta":{"url":"http://arxiv.org/abs/2307.09361v1"},"cats":{"new-dataset":0.0292533035,"dev-research":0.3170285385,"prompt-eng":0.4085404992,"data-quality":0.1096684304,"ml-security":0.139941763}}
{"text":"Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.","meta":{"url":"http://arxiv.org/abs/2307.09361v1"},"cats":{"new-dataset":0.0974579955,"dev-research":0.2808881056,"prompt-eng":0.4384906905,"data-quality":0.1799098366,"ml-security":0.1214531033}}
{"text":"Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training.","meta":{"url":"http://arxiv.org/abs/2307.09357v1"},"cats":{"new-dataset":0.1067145259,"dev-research":0.2955020374,"prompt-eng":0.3836259951,"data-quality":0.1316092542,"ml-security":0.1396173906}}
{"text":"However, the noisy and non-linear device characteristics, and the non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing.","meta":{"url":"http://arxiv.org/abs/2307.09357v1"},"cats":{"new-dataset":0.0360742488,"dev-research":0.3715383348,"prompt-eng":0.3861694095,"data-quality":0.2115594338,"ml-security":0.1404711096}}
{"text":"In this tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit.","meta":{"url":"http://arxiv.org/abs/2307.09357v1"},"cats":{"new-dataset":0.0850285853,"dev-research":0.2785241823,"prompt-eng":0.4000379003,"data-quality":0.1000199291,"ml-security":0.0715958309}}
{"text":"The AIHWKit is a Python library that simulates inference and training of DNNs using AIMC.","meta":{"url":"http://arxiv.org/abs/2307.09357v1"},"cats":{"new-dataset":0.3385304073,"dev-research":0.3305688166,"prompt-eng":0.4140893636,"data-quality":0.1201361022,"ml-security":0.1511760814}}
{"text":"We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training.","meta":{"url":"http://arxiv.org/abs/2307.09357v1"},"cats":{"new-dataset":0.173832302,"dev-research":0.3256459395,"prompt-eng":0.4584589705,"data-quality":0.1205035891,"ml-security":0.0927592718}}
{"text":"We also present an overview of the Analog AI Cloud Composer, that provides the benefits of using the AIHWKit simulation platform in a fully managed cloud setting.","meta":{"url":"http://arxiv.org/abs/2307.09357v1"},"cats":{"new-dataset":0.1928617518,"dev-research":0.2711574437,"prompt-eng":0.3672307985,"data-quality":0.0602720152,"ml-security":0.0924351153}}
{"text":"Finally, we show examples on how users can expand and customize AIHWKit for their own needs.","meta":{"url":"http://arxiv.org/abs/2307.09357v1"},"cats":{"new-dataset":0.2651654319,"dev-research":0.4433115693,"prompt-eng":0.4418293208,"data-quality":0.1049054034,"ml-security":0.0802788341}}
{"text":"This tutorial is accompanied by comprehensive Jupyter Notebook code examples that can be run using AIHWKit, which can be downloaded from https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial.","meta":{"url":"http://arxiv.org/abs/2307.09357v1"},"cats":{"new-dataset":0.2797944918,"dev-research":0.2642133974,"prompt-eng":0.4244483744,"data-quality":0.1164449523,"ml-security":0.0649233709}}
{"text":"Referring video object segmentation (RVOS) aims at segmenting an object in a video following human instruction.","meta":{"url":"http://arxiv.org/abs/2307.09356v1"},"cats":{"new-dataset":0.0866155332,"dev-research":0.2477870587,"prompt-eng":0.4357305108,"data-quality":0.1621938551,"ml-security":0.0355906154}}
{"text":"Current state-of-the-art methods fall into an offline pattern, in which each clip independently interacts with text embedding for cross-modal understanding.","meta":{"url":"http://arxiv.org/abs/2307.09356v1"},"cats":{"new-dataset":0.0593275867,"dev-research":0.2684803582,"prompt-eng":0.4225572197,"data-quality":0.1627511119,"ml-security":0.0581836079}}
{"text":"They usually present that the offline pattern is necessary for RVOS, yet model limited temporal association within each clip.","meta":{"url":"http://arxiv.org/abs/2307.09356v1"},"cats":{"new-dataset":0.0285560507,"dev-research":0.2067583346,"prompt-eng":0.4171002569,"data-quality":0.0757158844,"ml-security":0.060297649}}
{"text":"In this work, we break up the previous offline belief and propose a simple yet effective online model using explicit query propagation, named OnlineRefer.","meta":{"url":"http://arxiv.org/abs/2307.09356v1"},"cats":{"new-dataset":0.1252660376,"dev-research":0.1932945084,"prompt-eng":0.4397987448,"data-quality":0.1019201234,"ml-security":0.0633608537}}
{"text":"Specifically, our approach leverages target cues that gather semantic information and position prior to improve the accuracy and ease of referring predictions for the current frame.","meta":{"url":"http://arxiv.org/abs/2307.09356v1"},"cats":{"new-dataset":0.0892965706,"dev-research":0.3750821152,"prompt-eng":0.4722113374,"data-quality":0.254804587,"ml-security":0.0698769535}}
{"text":"Furthermore, we generalize our online model into a semi-online framework to be compatible with video-based backbones.","meta":{"url":"http://arxiv.org/abs/2307.09356v1"},"cats":{"new-dataset":0.0920606683,"dev-research":0.2522921487,"prompt-eng":0.4090322014,"data-quality":0.0626264336,"ml-security":0.0898362809}}
{"text":"To show the effectiveness of our method, we evaluate it on four benchmarks, \\ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and JHMDB-Sentences.","meta":{"url":"http://arxiv.org/abs/2307.09356v1"},"cats":{"new-dataset":0.0512461321,"dev-research":0.3092229378,"prompt-eng":0.3984270988,"data-quality":0.2398793902,"ml-security":0.0496748187}}
{"text":"Without bells and whistles, our OnlineRefer with a Swin-L backbone achieves 63.5 J&F and 64.8 J&F on Refer-Youtube-VOS and Refer-DAVIS17, outperforming all other offline methods.","meta":{"url":"http://arxiv.org/abs/2307.09356v1"},"cats":{"new-dataset":0.155722287,"dev-research":0.2489539279,"prompt-eng":0.3952698353,"data-quality":0.0663340221,"ml-security":0.0394422949}}
{"text":"Point cloud registration is to estimate a transformation to align point clouds collected in different perspectives.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.092406617,"dev-research":0.2465428666,"prompt-eng":0.382354398,"data-quality":0.0950898858,"ml-security":0.0464746742}}
{"text":"In learning-based point cloud registration, a robust descriptor is vital for high-accuracy registration.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.1313804377,"dev-research":0.2557221583,"prompt-eng":0.3959672224,"data-quality":0.166249458,"ml-security":0.1042136292}}
{"text":"However, most methods are susceptible to noise and have poor generalization ability on unseen datasets.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.0401853425,"dev-research":0.3271045952,"prompt-eng":0.3428870191,"data-quality":0.404557011,"ml-security":0.3211475036}}
{"text":"Motivated by this, we introduce SphereNet to learn a noise-robust and unseen-general descriptor for point cloud registration.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.2368248586,"dev-research":0.2499159681,"prompt-eng":0.3739792899,"data-quality":0.2737754208,"ml-security":0.139990832}}
{"text":"In our method, first, the spheroid generator builds a geometric domain based on spherical voxelization to encode initial features.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.1323323163,"dev-research":0.2813011378,"prompt-eng":0.3796852854,"data-quality":0.0864651099,"ml-security":0.0491014441}}
{"text":"Then, the spherical interpolation of the sphere is introduced to realize robustness against noise.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.0254419827,"dev-research":0.230539816,"prompt-eng":0.3750626968,"data-quality":0.2924540436,"ml-security":0.1322464885}}
{"text":"Finally, a new spherical convolutional neural network with spherical integrity padding completes the extraction of descriptors, which reduces the loss of features and fully captures the geometric features.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.1071055322,"dev-research":0.2613365217,"prompt-eng":0.3425458675,"data-quality":0.1852858963,"ml-security":0.1136195217}}
{"text":"To evaluate our methods, a new benchmark 3DMatch-noise with strong noise is introduced.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.1616503261,"dev-research":0.2556200017,"prompt-eng":0.3426329416,"data-quality":0.1968391004,"ml-security":0.084129433}}
{"text":"Extensive experiments are carried out on both indoor and outdoor datasets.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.7149822004,"dev-research":0.2731510251,"prompt-eng":0.3606525946,"data-quality":0.1167775418,"ml-security":0.1307909979}}
{"text":"Under high-intensity noise, SphereNet increases the feature matching recall by more than 25 percentage points on 3DMatch-noise.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.0728664528,"dev-research":0.2664234529,"prompt-eng":0.4136099403,"data-quality":0.2593348829,"ml-security":0.1063197908}}
{"text":"In addition, it sets a new state-of-the-art performance for the 3DMatch and 3DLoMatch benchmarks with 93.5\\% and 75.6\\% registration recall and also has the best generalization ability on unseen datasets.","meta":{"url":"http://arxiv.org/abs/2307.09351v1"},"cats":{"new-dataset":0.2124924932,"dev-research":0.3148387084,"prompt-eng":0.3849340035,"data-quality":0.1182826273,"ml-security":0.0693541335}}
{"text":"Let $(X, d)$ be a metric space and $\\mathcal{C} \\subseteq 2^X$ -- a collection of special objects.","meta":{"url":"http://arxiv.org/abs/2307.09350v1"},"cats":{"new-dataset":0.3709623794,"dev-research":0.23651398,"prompt-eng":0.3664968258,"data-quality":0.1602112211,"ml-security":0.0839033584}}
{"text":"In the $(X,d,\\mathcal{C})$-chasing problem, an online player receives a sequence of online requests $\\{B_t\\}_{t=1}^T \\subseteq \\mathcal{C}$ and responds with a trajectory $\\{x_t\\}_{t=1}^T$ such that $x_t \\in B_t$. This response incurs a movement cost $\\sum_{t=1}^T d(x_t, x_{t-1})$, and the online player strives to minimize the competitive ratio -- the worst case ratio over all input sequences between the online movement cost and the optimal movement cost in hindsight.","meta":{"url":"http://arxiv.org/abs/2307.09350v1"},"cats":{"new-dataset":0.0106471341,"dev-research":0.2045250079,"prompt-eng":0.371637279,"data-quality":0.0574542994,"ml-security":0.191592206}}
{"text":"Under this setup, we call the $(X,d,\\mathcal{C})$-chasing problem $\\textit{chaseable}$ if there exists an online algorithm with finite competitive ratio.","meta":{"url":"http://arxiv.org/abs/2307.09350v1"},"cats":{"new-dataset":0.0479467469,"dev-research":0.1561697274,"prompt-eng":0.366488946,"data-quality":0.0814789806,"ml-security":0.1709304837}}
{"text":"In the case of Convex Body Chasing (CBC) over real normed vector spaces, (Bubeck et al. 2019) proved the chaseability of the problem.","meta":{"url":"http://arxiv.org/abs/2307.09350v1"},"cats":{"new-dataset":0.0534241513,"dev-research":0.2228522137,"prompt-eng":0.3485737,"data-quality":0.1173630502,"ml-security":0.2301808057}}
{"text":"Furthermore, in the vector space setting, the dimension of the ambient space appears to be the factor controlling the size of the competitive ratio.","meta":{"url":"http://arxiv.org/abs/2307.09350v1"},"cats":{"new-dataset":0.0233228445,"dev-research":0.2368451763,"prompt-eng":0.3537574876,"data-quality":0.0501078885,"ml-security":0.1070283413}}
{"text":"Indeed, recently, (Sellke 2020) provided a $d-$competitive online algorithm over arbitrary real normed vector spaces $(\\mathbb{R}^d, ||\\cdot||)$, and we will shortly present a general strategy for obtaining novel lower bounds of the form $\\Omega(d^c), \\enspace c > 0$, for CBC in the same setting.","meta":{"url":"http://arxiv.org/abs/2307.09350v1"},"cats":{"new-dataset":0.1197136967,"dev-research":0.2357186667,"prompt-eng":0.3132928216,"data-quality":0.0719714536,"ml-security":0.1411637345}}
{"text":"In this paper, we also prove that the $\\textit{doubling}$ and $\\textit{Assouad}$ dimensions of a metric space exert no control on the hardness of ball chasing over the said metric space.","meta":{"url":"http://arxiv.org/abs/2307.09350v1"},"cats":{"new-dataset":0.1766625359,"dev-research":0.2994161845,"prompt-eng":0.3258208882,"data-quality":0.1866225235,"ml-security":0.1550857311}}
{"text":"More specifically, we show that for any large enough $\\rho \\in \\mathbb{R}$, there exists a metric space $(X,d)$ of doubling dimension $\\Theta(\\rho)$ and Assouad dimension $\\rho$ such that no online selector can achieve a finite competitive ratio in the general ball chasing regime.","meta":{"url":"http://arxiv.org/abs/2307.09350v1"},"cats":{"new-dataset":0.0484066275,"dev-research":0.1888829559,"prompt-eng":0.369126328,"data-quality":0.0765455971,"ml-security":0.1652209431}}
{"text":"We investigate an operator on classes of languages.","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.1395997152,"dev-research":0.2442163353,"prompt-eng":0.4176888545,"data-quality":0.2590429361,"ml-security":0.1398689426}}
{"text":"For each class $C$, it outputs a new class $FO^2(I_C)$ associated with a variant of two-variable first-order logic equipped with a signature$I_C$ built from $C$.","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.0495478976,"dev-research":0.2941609105,"prompt-eng":0.446971838,"data-quality":0.1251213408,"ml-security":0.1111961069}}
{"text":"For $C = \\{\\emptyset, A^*\\}$, we get the variant $FO^2(<)$ equipped with the linear order.","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.137630804,"dev-research":0.2748992427,"prompt-eng":0.3442537486,"data-quality":0.1452148028,"ml-security":0.0972046833}}
{"text":"For $C = \\{\\emptyset, \\{\\varepsilon\\},A^+, A^*\\}$, we get the variant $FO^2(<,+1)$, which also includes the successor.","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.2037046967,"dev-research":0.2733602241,"prompt-eng":0.3642536972,"data-quality":0.1475412107,"ml-security":0.0791194088}}
{"text":"If $C$ consists of all Boolean combinations of languages $A^*aA^*$ where $a$ is a letter, we get the variant $FO^2(<,Bet)$, which also includes ``between relations''.","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.1074918781,"dev-research":0.2989326744,"prompt-eng":0.4095046567,"data-quality":0.1445850233,"ml-security":0.0590615226}}
{"text":"We prove a generic algebraic characterization of the classes $FO^2(I_C)$. It smoothly and elegantly generalizes the known ones for all aforementioned cases.","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.0703156569,"dev-research":0.2510185834,"prompt-eng":0.382501505,"data-quality":0.140782691,"ml-security":0.1346016507}}
{"text":"Moreover, it implies that if $C$ has decidable separation (plus mild properties), then $FO^2(I_C)$ has a decidable membership problem.   ","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.0284013653,"dev-research":0.240688922,"prompt-eng":0.3697342709,"data-quality":0.1545076231,"ml-security":0.125868975}}
{"text":"We actually work with an equivalent definition of \\fodc in terms of unary temporal logic.","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.0289363423,"dev-research":0.247677621,"prompt-eng":0.4058562582,"data-quality":0.1364427003,"ml-security":0.0880679438}}
{"text":"For each class $C$, we consider a variant $TL(C)$ of unary temporal logic whose future/past modalities depend on $C$ and such that $TL(C) =","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.0628588685,"dev-research":0.2226221462,"prompt-eng":0.4071369703,"data-quality":0.0853390366,"ml-security":0.1271943675}}
{"text":"FO^2(I_C)$. Finally, we also characterize $FL(C)$ and $PL(C)$, the pure-future and pure-past restrictions of $TL(C)$. These characterizations as well imply that if \\Cs is a class with decidable separation, then $FL(C)$ and $PL(C)$ have decidable membership.","meta":{"url":"http://arxiv.org/abs/2307.09349v1"},"cats":{"new-dataset":0.0414550531,"dev-research":0.2293939233,"prompt-eng":0.3776045585,"data-quality":0.0983123576,"ml-security":0.1591353275}}
{"text":"Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT).","meta":{"url":"http://arxiv.org/abs/2307.09342v1"},"cats":{"new-dataset":0.0429362048,"dev-research":0.301711969,"prompt-eng":0.4456352493,"data-quality":0.1202709689,"ml-security":0.082255492}}
{"text":"However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial.","meta":{"url":"http://arxiv.org/abs/2307.09342v1"},"cats":{"new-dataset":0.0144633027,"dev-research":0.2352513087,"prompt-eng":0.4212188433,"data-quality":0.158314182,"ml-security":0.1255314526}}
{"text":"We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach.","meta":{"url":"http://arxiv.org/abs/2307.09342v1"},"cats":{"new-dataset":0.0619192264,"dev-research":0.2780213489,"prompt-eng":0.479236523,"data-quality":0.2073364259,"ml-security":0.2313141828}}
{"text":"We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints.","meta":{"url":"http://arxiv.org/abs/2307.09342v1"},"cats":{"new-dataset":0.0555169831,"dev-research":0.3127626314,"prompt-eng":0.4557241041,"data-quality":0.177870077,"ml-security":0.1562073639}}
{"text":"In fact, we achieve good results when selecting encodings for unseen problem classes.","meta":{"url":"http://arxiv.org/abs/2307.09342v1"},"cats":{"new-dataset":0.0975987885,"dev-research":0.321498921,"prompt-eng":0.4568771737,"data-quality":0.3572992084,"ml-security":0.2687075349}}
{"text":"Our results compare favourably to AutoFolio when using the same feature set.","meta":{"url":"http://arxiv.org/abs/2307.09342v1"},"cats":{"new-dataset":0.0998341577,"dev-research":0.4074722204,"prompt-eng":0.4761586347,"data-quality":0.3335505968,"ml-security":0.0769127578}}
{"text":"We discuss the relative importance of instance features to the task of selecting the best encodings, and compare several variations of the machine learning method.","meta":{"url":"http://arxiv.org/abs/2307.09342v1"},"cats":{"new-dataset":0.030695257,"dev-research":0.3370576268,"prompt-eng":0.4408846718,"data-quality":0.2406200829,"ml-security":0.20472178}}
{"text":"Trajectory data collection is a common task with many applications in our daily lives.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.6299788419,"dev-research":0.2632990176,"prompt-eng":0.36879043,"data-quality":0.0664722786,"ml-security":0.0737924841}}
{"text":"Analyzing trajectory data enables service providers to enhance their services, which ultimately benefits users.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.0817455086,"dev-research":0.3538010554,"prompt-eng":0.3881271012,"data-quality":0.0817626054,"ml-security":0.1166482043}}
{"text":"However, directly collecting trajectory data may give rise to privacy-related issues that cannot be ignored.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.1028491511,"dev-research":0.2783651518,"prompt-eng":0.3505167631,"data-quality":0.146759684,"ml-security":0.4167020271}}
{"text":"Local differential privacy (LDP), as the de facto privacy protection standard in a decentralized setting, enables users to perturb their trajectories locally and provides a provable privacy guarantee.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.0680351217,"dev-research":0.3201333794,"prompt-eng":0.381452502,"data-quality":0.1277217657,"ml-security":0.3234794139}}
{"text":"Existing approaches to private trajectory data collection in a local setting typically use relaxed versions of LDP, which cannot provide a strict privacy guarantee, or require some external knowledge that is impractical to obtain and update in a timely manner.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.2297179816,"dev-research":0.226874692,"prompt-eng":0.3372147616,"data-quality":0.1228482377,"ml-security":0.2225206716}}
{"text":"To tackle these problems, we propose a novel trajectory perturbation mechanism that relies solely on an underlying location set and satisfies pure $\\epsilon$-LDP to provide a stringent privacy guarantee.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.0396347537,"dev-research":0.1909382771,"prompt-eng":0.3728329771,"data-quality":0.1560546246,"ml-security":0.3454994498}}
{"text":"In the proposed mechanism, each point's adjacent direction information in the trajectory is used in its perturbation process.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.0077530855,"dev-research":0.209877835,"prompt-eng":0.38830711,"data-quality":0.0764834909,"ml-security":0.0699820638}}
{"text":"Such information serves as an effective clue to connect neighboring points and can be used to restrict the possible region of a perturbed point in order to enhance utility.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.01936364,"dev-research":0.316898938,"prompt-eng":0.4426431011,"data-quality":0.1562070226,"ml-security":0.1839881903}}
{"text":"To the best of our knowledge, our study is the first to use direction information for trajectory perturbation under LDP.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.0180754042,"dev-research":0.232974733,"prompt-eng":0.436698639,"data-quality":0.1342704818,"ml-security":0.1058868985}}
{"text":"Furthermore, based on this mechanism, we present an anchor-based method that adaptively restricts the region of each perturbed trajectory, thereby significantly boosting performance without violating the privacy constraint.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.0129079814,"dev-research":0.2508581418,"prompt-eng":0.3715230585,"data-quality":0.1010892986,"ml-security":0.3463308908}}
{"text":"Extensive experiments on both real-world and synthetic datasets demonstrate the effectiveness of the proposed mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.09339v1"},"cats":{"new-dataset":0.3203649805,"dev-research":0.313485636,"prompt-eng":0.3703312599,"data-quality":0.198121377,"ml-security":0.3749815573}}
{"text":"A \\emph{matching} is a subset of edges in a graph $G$ that do not share an endpoint.","meta":{"url":"http://arxiv.org/abs/2307.09333v1"},"cats":{"new-dataset":0.0192402888,"dev-research":0.2433958105,"prompt-eng":0.3706860777,"data-quality":0.1739285246,"ml-security":0.0673014348}}
{"text":"A matching $M$ is a \\emph{$\\mathcal{P}$-matching} if the subgraph of $G$ induced by the endpoints of the edges of $M$ satisfies property $\\mathcal{P}$. For example, if the property $\\mathcal{P}$ is that of being a matching, being acyclic, or being disconnected, then we obtain an \\emph{induced matching}, an \\emph{acyclic matching}, and a \\emph{disconnected matching}, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09333v1"},"cats":{"new-dataset":0.0144829467,"dev-research":0.2006236412,"prompt-eng":0.4043931554,"data-quality":0.13739471,"ml-security":0.0454526859}}
{"text":"In this paper, we analyze the problems of the computation of these matchings from the viewpoint of Parameterized Complexity with respect to the parameter \\emph{treewidth}.","meta":{"url":"http://arxiv.org/abs/2307.09333v1"},"cats":{"new-dataset":0.0414963782,"dev-research":0.1996127043,"prompt-eng":0.347868353,"data-quality":0.1175387735,"ml-security":0.0556200135}}
{"text":"With Company2Vec, the paper proposes a novel application in representation learning.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.119060049,"dev-research":0.3057310916,"prompt-eng":0.4295772542,"data-quality":0.1879498638,"ml-security":0.1377905493}}
{"text":"The model analyzes business activities from unstructured company website data using Word2Vec and dimensionality reduction.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.1505924711,"dev-research":0.313528517,"prompt-eng":0.3622550752,"data-quality":0.1433277834,"ml-security":0.0737354471}}
{"text":"Company2Vec maintains semantic language structures and thus creates efficient company embeddings in fine-granular industries.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.1667389481,"dev-research":0.3829889268,"prompt-eng":0.4102464576,"data-quality":0.2688538363,"ml-security":0.069967186}}
{"text":"These semantic embeddings can be used for various applications in banking.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.06009867,"dev-research":0.3351328392,"prompt-eng":0.4430182038,"data-quality":0.2743888978,"ml-security":0.1104370996}}
{"text":"Direct relations between companies and words allow semantic business analytics (e.g. top-n words for a company).","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.1417708669,"dev-research":0.350888079,"prompt-eng":0.3914224565,"data-quality":0.2387409917,"ml-security":0.052491977}}
{"text":"Furthermore, industry prediction is presented as a supervised learning application and evaluation method.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.051962668,"dev-research":0.3003010745,"prompt-eng":0.4147375322,"data-quality":0.2220157971,"ml-security":0.1366585546}}
{"text":"The vectorized structure of the embeddings allows measuring companies similarities with the cosine distance.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.0707933387,"dev-research":0.3200478636,"prompt-eng":0.372341111,"data-quality":0.1771052294,"ml-security":0.0669884636}}
{"text":"Company2Vec hence offers a more fine-grained comparison of companies than the standard industry labels (NACE).","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.1382370626,"dev-research":0.3283754911,"prompt-eng":0.3904811666,"data-quality":0.2759536909,"ml-security":0.0552303881}}
{"text":"This property is relevant for unsupervised learning tasks, such as clustering.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.0373509392,"dev-research":0.2961327496,"prompt-eng":0.4551510002,"data-quality":0.1982270725,"ml-security":0.1893651991}}
{"text":"An alternative industry segmentation is shown with k-means clustering on the company embeddings.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.1545947898,"dev-research":0.2629082299,"prompt-eng":0.3712193286,"data-quality":0.223063295,"ml-security":0.078127478}}
{"text":"Finally, this paper proposes three algorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centric peer-firm identification.","meta":{"url":"http://arxiv.org/abs/2307.09332v1"},"cats":{"new-dataset":0.0979016467,"dev-research":0.2020608919,"prompt-eng":0.3706903463,"data-quality":0.1756960659,"ml-security":0.0939372957}}
{"text":"We investigate the ability of individuals to visually validate statistical models in terms of their fit to the data.","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.0685602862,"dev-research":0.3758752778,"prompt-eng":0.4341281948,"data-quality":0.1931510485,"ml-security":0.1487195681}}
{"text":"While visual model estimation has been studied extensively, visual model validation remains under-investigated.","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.0559246206,"dev-research":0.2745245299,"prompt-eng":0.41148023,"data-quality":0.2622169115,"ml-security":0.1074382963}}
{"text":"It is unknown how well people are able to visually validate models, and how their performance compares to visual and computational estimation.","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.0386256557,"dev-research":0.3870009873,"prompt-eng":0.4006285407,"data-quality":0.1584196477,"ml-security":0.1224236346}}
{"text":"As a starting point, we conducted a study across two populations (crowdsourced and volunteers).","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.4521874288,"dev-research":0.3250474782,"prompt-eng":0.423801459,"data-quality":0.124996876,"ml-security":0.0521133388}}
{"text":"Participants had to both visually estimate (i.e, draw) and visually validate (i.e., accept or reject) the frequently studied model of averages.","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.015050318,"dev-research":0.2937614364,"prompt-eng":0.4206024999,"data-quality":0.1390160323,"ml-security":0.0724844083}}
{"text":"Across both populations, the level of accuracy of the models that were considered valid was lower than the accuracy of the estimated models.","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.0091869448,"dev-research":0.2449193844,"prompt-eng":0.3844078811,"data-quality":0.1852338701,"ml-security":0.055248176}}
{"text":"We find that participants' validation and estimation were unbiased.","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.0329028141,"dev-research":0.2954245893,"prompt-eng":0.3973864657,"data-quality":0.2857484267,"ml-security":0.1470251578}}
{"text":"Moreover, their natural critical point between accepting and rejecting a given mean value is close to the boundary of its 95% confidence interval, indicating that the visually perceived confidence interval corresponds to a common statistical standard.","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.012721271,"dev-research":0.3130985526,"prompt-eng":0.4478262314,"data-quality":0.2285354584,"ml-security":0.1454329309}}
{"text":"Our work contributes to the understanding of visual model validation and opens new research opportunities.","meta":{"url":"http://arxiv.org/abs/2307.09330v1"},"cats":{"new-dataset":0.0677594408,"dev-research":0.3924372765,"prompt-eng":0.4176032775,"data-quality":0.173849441,"ml-security":0.1070342049}}
{"text":"This short paper presents a preliminary analysis of three popular Visual Question Answering (VQA) models, namely ViLBERT, ViLT, and LXMERT, in the context of answering questions relating to driving scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09329v1"},"cats":{"new-dataset":0.1180689428,"dev-research":0.3173794543,"prompt-eng":0.4390320428,"data-quality":0.1181109972,"ml-security":0.0833618973}}
{"text":"The performance of these models is evaluated by comparing the similarity of responses to reference answers provided by computer vision experts.","meta":{"url":"http://arxiv.org/abs/2307.09329v1"},"cats":{"new-dataset":0.0511103707,"dev-research":0.313600283,"prompt-eng":0.4482171127,"data-quality":0.1486774244,"ml-security":0.0472734978}}
{"text":"Model selection is predicated on the analysis of transformer utilization in multimodal architectures.","meta":{"url":"http://arxiv.org/abs/2307.09329v1"},"cats":{"new-dataset":0.0109751354,"dev-research":0.2446288899,"prompt-eng":0.4455026594,"data-quality":0.0509895091,"ml-security":0.0598595853}}
{"text":"The results indicate that models incorporating cross-modal attention and late fusion techniques exhibit promising potential for generating improved answers within a driving perspective.","meta":{"url":"http://arxiv.org/abs/2307.09329v1"},"cats":{"new-dataset":0.0124715376,"dev-research":0.290100735,"prompt-eng":0.4682095542,"data-quality":0.0983628802,"ml-security":0.075004553}}
{"text":"This initial analysis serves as a launchpad for a forthcoming comprehensive comparative study involving nine VQA models and sets the scene for further investigations into the effectiveness of VQA model queries in self-driving scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09329v1"},"cats":{"new-dataset":0.0487678295,"dev-research":0.2642487547,"prompt-eng":0.4175070911,"data-quality":0.0859893684,"ml-security":0.1338336335}}
{"text":"Supplementary material is available at https://github.com/KaavyaRekanar/Towards-a-performance-analysis-on-pre-trained-VQA-models-for-autonomous-driving.","meta":{"url":"http://arxiv.org/abs/2307.09329v1"},"cats":{"new-dataset":0.1999521463,"dev-research":0.2295793427,"prompt-eng":0.4065793498,"data-quality":0.0752796339,"ml-security":0.1113787514}}
{"text":"This paper presents ER-NeRF, a novel conditional Neural Radiance Fields (NeRF) based architecture for talking portrait synthesis that can concurrently achieve fast convergence, real-time rendering, and state-of-the-art performance with small model size.","meta":{"url":"http://arxiv.org/abs/2307.09323v1"},"cats":{"new-dataset":0.1453999847,"dev-research":0.2642383062,"prompt-eng":0.382135615,"data-quality":0.091042195,"ml-security":0.1388660221}}
{"text":"Our idea is to explicitly exploit the unequal contribution of spatial regions to guide talking portrait modeling.","meta":{"url":"http://arxiv.org/abs/2307.09323v1"},"cats":{"new-dataset":0.0814234042,"dev-research":0.288687423,"prompt-eng":0.3790281121,"data-quality":0.1561026054,"ml-security":0.0911591187}}
{"text":"Specifically, to improve the accuracy of dynamic head reconstruction, a compact and expressive NeRF-based Tri-Plane Hash Representation is introduced by pruning empty spatial regions with three planar hash encoders.","meta":{"url":"http://arxiv.org/abs/2307.09323v1"},"cats":{"new-dataset":0.0659671015,"dev-research":0.2612919283,"prompt-eng":0.3351031093,"data-quality":0.1269615462,"ml-security":0.0956621196}}
{"text":"For speech audio, we propose a Region Attention Module to generate region-aware condition feature via an attention mechanism.","meta":{"url":"http://arxiv.org/abs/2307.09323v1"},"cats":{"new-dataset":0.1539238207,"dev-research":0.2665327996,"prompt-eng":0.4650819614,"data-quality":0.1918885074,"ml-security":0.0560212458}}
{"text":"Different from existing methods that utilize an MLP-based encoder to learn the cross-modal relation implicitly, the attention mechanism builds an explicit connection between audio features and spatial regions to capture the priors of local motions.","meta":{"url":"http://arxiv.org/abs/2307.09323v1"},"cats":{"new-dataset":0.0880050172,"dev-research":0.2424975978,"prompt-eng":0.4304784668,"data-quality":0.1204155835,"ml-security":0.076532645}}
{"text":"Moreover, a direct and fast Adaptive Pose Encoding is introduced to optimize the head-torso separation problem by mapping the complex transformation of the head pose into spatial coordinates.","meta":{"url":"http://arxiv.org/abs/2307.09323v1"},"cats":{"new-dataset":0.0360755614,"dev-research":0.2252275092,"prompt-eng":0.369925452,"data-quality":0.0570805929,"ml-security":0.0924603775}}
{"text":"Extensive experiments demonstrate that our method renders better high-fidelity and audio-lips synchronized talking portrait videos, with realistic details and high efficiency compared to previous methods.","meta":{"url":"http://arxiv.org/abs/2307.09323v1"},"cats":{"new-dataset":0.1055262963,"dev-research":0.2909028195,"prompt-eng":0.320416848,"data-quality":0.1134949674,"ml-security":0.0659318203}}
{"text":"Information security is a crucial need in the modern world.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.0630028208,"dev-research":0.3438144786,"prompt-eng":0.4061182774,"data-quality":0.079196185,"ml-security":0.37658445}}
{"text":"Data security is a real concern, and many customers and organizations need to protect their sensitive information from unauthorized parties and attackers.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.1542034186,"dev-research":0.3551391175,"prompt-eng":0.3601859272,"data-quality":0.142426725,"ml-security":0.7377788886}}
{"text":"In previous years, numerous cryptographic schemes have been proposed.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.0566140512,"dev-research":0.2458627933,"prompt-eng":0.3818955193,"data-quality":0.0506388996,"ml-security":0.232789616}}
{"text":"DNA cryptography is a new and developing field that combines the computational and biological worlds.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.097309956,"dev-research":0.2923874609,"prompt-eng":0.3913283321,"data-quality":0.0991930362,"ml-security":0.1423391125}}
{"text":"DNA cryptography is intriguing due to its high storage capacity, secure data transport, and massive parallel computing.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.1599312339,"dev-research":0.2700173908,"prompt-eng":0.3716349551,"data-quality":0.0787789538,"ml-security":0.2757367364}}
{"text":"In this paper, a new combination is proposed that offers good security by combining DNA, the Rabin algorithm, one time pad, and a structure inspired by Fiestel.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.051950175,"dev-research":0.2469809162,"prompt-eng":0.3952830754,"data-quality":0.0827797793,"ml-security":0.2246209694}}
{"text":"This algorithm employs two keys.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.0360974078,"dev-research":0.255847988,"prompt-eng":0.3850091618,"data-quality":0.0765758075,"ml-security":0.0600121682}}
{"text":"The first key is a DNA OTP key which is used for only one secure communication session.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.0804225317,"dev-research":0.2667132208,"prompt-eng":0.4114582859,"data-quality":0.0837498094,"ml-security":0.092798418}}
{"text":"The second key, which combines the public and private keys, is a Rabin key.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.0932914954,"dev-research":0.2498267237,"prompt-eng":0.3731039055,"data-quality":0.065879925,"ml-security":0.0699941019}}
{"text":"Additionally, by using a Feistel inspired scheme and randomness provided by DNA, the ciphertext is made harder to obtain without the private key.","meta":{"url":"http://arxiv.org/abs/2307.09322v1"},"cats":{"new-dataset":0.0161710968,"dev-research":0.2249521755,"prompt-eng":0.3818560295,"data-quality":0.0996096453,"ml-security":0.2401520942}}
{"text":"Traditional approaches for learning on categorical data underexploit the dependencies between columns (\\aka fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss.","meta":{"url":"http://arxiv.org/abs/2307.09321v1"},"cats":{"new-dataset":0.2052875397,"dev-research":0.297048979,"prompt-eng":0.363142647,"data-quality":0.1955615231,"ml-security":0.2022664281}}
{"text":"In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields.","meta":{"url":"http://arxiv.org/abs/2307.09321v1"},"cats":{"new-dataset":0.1404285484,"dev-research":0.3020581867,"prompt-eng":0.3985008904,"data-quality":0.2218315052,"ml-security":0.1989975785}}
{"text":"Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling)","meta":{"url":"http://arxiv.org/abs/2307.09321v1"},"cats":{"new-dataset":0.0992387103,"dev-research":0.3005873078,"prompt-eng":0.4223296286,"data-quality":0.1329886987,"ml-security":0.0929202934}}
{"text":"w.r.t.","meta":{"url":"http://arxiv.org/abs/2307.09321v1"},"cats":{"new-dataset":0.1736024897,"dev-research":0.2429280299,"prompt-eng":0.4091915048,"data-quality":0.0909752972,"ml-security":0.0764402523}}
{"text":"each field to improve the modelling of the field dependencies.","meta":{"url":"http://arxiv.org/abs/2307.09321v1"},"cats":{"new-dataset":0.1329299972,"dev-research":0.2788854574,"prompt-eng":0.4296556552,"data-quality":0.0956567763,"ml-security":0.0506911508}}
{"text":"Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the matrix performing projection) and global dependency matrix in a supervised fashion (with the use of labels).","meta":{"url":"http://arxiv.org/abs/2307.09321v1"},"cats":{"new-dataset":0.0887401535,"dev-research":0.2465486368,"prompt-eng":0.4171154193,"data-quality":0.3009655689,"ml-security":0.1334160151}}
{"text":"Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.09321v1"},"cats":{"new-dataset":0.455234674,"dev-research":0.3009162915,"prompt-eng":0.3259659235,"data-quality":0.25078871,"ml-security":0.1055545521}}
{"text":"Detailed ablation studies provide additional insights into our method.","meta":{"url":"http://arxiv.org/abs/2307.09321v1"},"cats":{"new-dataset":0.035044519,"dev-research":0.2802919814,"prompt-eng":0.3643687685,"data-quality":0.1124335374,"ml-security":0.031011052}}
{"text":"We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA).","meta":{"url":"http://arxiv.org/abs/2307.09320v1"},"cats":{"new-dataset":0.1731833561,"dev-research":0.2908772675,"prompt-eng":0.4401162337,"data-quality":0.091904291,"ml-security":0.0409574645}}
{"text":"In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines.","meta":{"url":"http://arxiv.org/abs/2307.09320v1"},"cats":{"new-dataset":0.0818216052,"dev-research":0.3159288137,"prompt-eng":0.3964592591,"data-quality":0.086043658,"ml-security":0.0380692606}}
{"text":"We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework.","meta":{"url":"http://arxiv.org/abs/2307.09320v1"},"cats":{"new-dataset":0.1309471752,"dev-research":0.2370284786,"prompt-eng":0.3444492363,"data-quality":0.0560820588,"ml-security":0.0654815817}}
{"text":"We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies.","meta":{"url":"http://arxiv.org/abs/2307.09320v1"},"cats":{"new-dataset":0.0754693882,"dev-research":0.2347561928,"prompt-eng":0.4215064495,"data-quality":0.052841994,"ml-security":0.1664645503}}
{"text":"We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes.","meta":{"url":"http://arxiv.org/abs/2307.09320v1"},"cats":{"new-dataset":0.1524426902,"dev-research":0.2561539924,"prompt-eng":0.3723562352,"data-quality":0.097530986,"ml-security":0.0884882958}}
{"text":"We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution.","meta":{"url":"http://arxiv.org/abs/2307.09320v1"},"cats":{"new-dataset":0.0585635255,"dev-research":0.2568137233,"prompt-eng":0.4285100147,"data-quality":0.0647941426,"ml-security":0.1257127748}}
{"text":"Finally, we show how to perform interactive evolution, where the user decides how to evolve a plant model interactively and then deploys it in a larger environment.","meta":{"url":"http://arxiv.org/abs/2307.09320v1"},"cats":{"new-dataset":0.0980616566,"dev-research":0.3146345231,"prompt-eng":0.4879419346,"data-quality":0.0631614554,"ml-security":0.0789277259}}
{"text":"We open source Biomaker CA at: https://tinyurl.com/2x8yu34s .","meta":{"url":"http://arxiv.org/abs/2307.09320v1"},"cats":{"new-dataset":0.5617007209,"dev-research":0.2769134322,"prompt-eng":0.399858506,"data-quality":0.0913810473,"ml-security":0.0387152354}}
{"text":"We conduct a large-scale measurement of developers' insecure practices leading to mini-app to super-app authentication bypass, among which hard-coding developer secrets for such authentication is a major contributor.","meta":{"url":"http://arxiv.org/abs/2307.09317v1"},"cats":{"new-dataset":0.0742501793,"dev-research":0.5431399651,"prompt-eng":0.4300096916,"data-quality":0.1484706925,"ml-security":0.5163181516}}
{"text":"We also analyze the exploitability and security consequences of developer secret leakage in mini-apps by examining individual super-app server-side APIs.","meta":{"url":"http://arxiv.org/abs/2307.09317v1"},"cats":{"new-dataset":0.0767749002,"dev-research":0.4801780274,"prompt-eng":0.3813175908,"data-quality":0.1727196733,"ml-security":0.6630970786}}
{"text":"We develop an analysis framework for measuring such secret leakage, and primarily analyze 110,993 WeChat mini-apps, and 10,000 Baidu mini-apps (two of the most prominent super-app platforms), along with a few more datasets to test the evolution of developer practices and platform security enforcement over time.","meta":{"url":"http://arxiv.org/abs/2307.09317v1"},"cats":{"new-dataset":0.3698884608,"dev-research":0.4701642022,"prompt-eng":0.3791209773,"data-quality":0.1727757766,"ml-security":0.6929030262}}
{"text":"We found a large number of WeChat mini-apps (36,425, 32.8%) and a few Baidu mini-apps (112) leak their developer secrets, which can cause severe security and privacy problems for the users and developers of mini-apps.","meta":{"url":"http://arxiv.org/abs/2307.09317v1"},"cats":{"new-dataset":0.2741371485,"dev-research":0.4309449033,"prompt-eng":0.362024604,"data-quality":0.1945481786,"ml-security":0.503846741}}
{"text":"A network attacker who does not even have an account on the super-app platform, can effectively take down a mini-app, send malicious and phishing links to users, and access sensitive information of the mini-app developer and its users.","meta":{"url":"http://arxiv.org/abs/2307.09317v1"},"cats":{"new-dataset":0.0416588854,"dev-research":0.4191443102,"prompt-eng":0.4057399428,"data-quality":0.1155598172,"ml-security":0.5380161364}}
{"text":"We responsibly disclosed our findings and also put forward potential directions that could be considered to alleviate/eliminate the root causes of developers hard-coding the app secrets in the mini-app's front-end code.","meta":{"url":"http://arxiv.org/abs/2307.09317v1"},"cats":{"new-dataset":0.0553620428,"dev-research":0.5410986264,"prompt-eng":0.4506183262,"data-quality":0.2082509033,"ml-security":0.3229294311}}
{"text":"3D semantic segmentation on multi-scan large-scale point clouds plays an important role in autonomous systems.","meta":{"url":"http://arxiv.org/abs/2307.09316v1"},"cats":{"new-dataset":0.1186425629,"dev-research":0.2375491265,"prompt-eng":0.3876837613,"data-quality":0.1202176613,"ml-security":0.0846805527}}
{"text":"Unlike the single-scan-based semantic segmentation task, this task requires distinguishing the motion states of points in addition to their semantic categories.","meta":{"url":"http://arxiv.org/abs/2307.09316v1"},"cats":{"new-dataset":0.0789682768,"dev-research":0.2105269739,"prompt-eng":0.4500643371,"data-quality":0.2035848353,"ml-security":0.0476576559}}
{"text":"However, methods designed for single-scan-based segmentation tasks perform poorly on the multi-scan task due to the lacking of an effective way to integrate temporal information.","meta":{"url":"http://arxiv.org/abs/2307.09316v1"},"cats":{"new-dataset":0.0304967346,"dev-research":0.247929206,"prompt-eng":0.401716958,"data-quality":0.1683537209,"ml-security":0.0614859922}}
{"text":"We propose MarS3D, a plug-and-play motion-aware module for semantic segmentation on multi-scan 3D point clouds.","meta":{"url":"http://arxiv.org/abs/2307.09316v1"},"cats":{"new-dataset":0.2793853492,"dev-research":0.2363727455,"prompt-eng":0.3620343646,"data-quality":0.1198347991,"ml-security":0.0433220145}}
{"text":"This module can be flexibly combined with single-scan models to allow them to have multi-scan perception abilities.","meta":{"url":"http://arxiv.org/abs/2307.09316v1"},"cats":{"new-dataset":0.0396929453,"dev-research":0.199932901,"prompt-eng":0.4866519689,"data-quality":0.0813502935,"ml-security":0.0569589526}}
{"text":"The model encompasses two key designs: the Cross-Frame Feature Embedding module for enriching representation learning and the Motion-Aware Feature Learning module for enhancing motion awareness.","meta":{"url":"http://arxiv.org/abs/2307.09316v1"},"cats":{"new-dataset":0.2583992181,"dev-research":0.2726115078,"prompt-eng":0.4114225659,"data-quality":0.0731629793,"ml-security":0.075940334}}
{"text":"Extensive experiments show that MarS3D can improve the performance of the baseline model by a large margin.","meta":{"url":"http://arxiv.org/abs/2307.09316v1"},"cats":{"new-dataset":0.0964601813,"dev-research":0.2424230636,"prompt-eng":0.3771476863,"data-quality":0.0778598393,"ml-security":0.0441957675}}
{"text":"The code is available at https://github.com/CVMI-Lab/MarS3D.","meta":{"url":"http://arxiv.org/abs/2307.09316v1"},"cats":{"new-dataset":0.444797748,"dev-research":0.2652944113,"prompt-eng":0.4145297557,"data-quality":0.0924556523,"ml-security":0.0520903066}}
{"text":"We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks.","meta":{"url":"http://arxiv.org/abs/2307.09312v1"},"cats":{"new-dataset":0.0818234739,"dev-research":0.2709829051,"prompt-eng":0.3753619586,"data-quality":0.2540298062,"ml-security":0.1693398575}}
{"text":"In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images.","meta":{"url":"http://arxiv.org/abs/2307.09312v1"},"cats":{"new-dataset":0.0837547786,"dev-research":0.3347755714,"prompt-eng":0.3722218436,"data-quality":0.376534436,"ml-security":0.1815735435}}
{"text":"This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately.","meta":{"url":"http://arxiv.org/abs/2307.09312v1"},"cats":{"new-dataset":0.0940505179,"dev-research":0.3184730205,"prompt-eng":0.4145171329,"data-quality":0.2932923845,"ml-security":0.042571441}}
{"text":"We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies.","meta":{"url":"http://arxiv.org/abs/2307.09312v1"},"cats":{"new-dataset":0.0701081399,"dev-research":0.2392645037,"prompt-eng":0.4383471902,"data-quality":0.1849229401,"ml-security":0.0322145809}}
{"text":"We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.","meta":{"url":"http://arxiv.org/abs/2307.09312v1"},"cats":{"new-dataset":0.1475044032,"dev-research":0.3635616992,"prompt-eng":0.4254944454,"data-quality":0.1905078167,"ml-security":0.1606583005}}
{"text":"A neural solver and differentiable simulation of the quantum transmitting boundary model is presented for the inverse quantum transport problem.","meta":{"url":"http://arxiv.org/abs/2307.09311v1"},"cats":{"new-dataset":0.0355533305,"dev-research":0.1867213159,"prompt-eng":0.4096839205,"data-quality":0.0722870639,"ml-security":0.1546113784}}
{"text":"The neural solver is used to engineer continuous transmission properties and the differentiable simulation is used to engineer current-voltage characteristics.","meta":{"url":"http://arxiv.org/abs/2307.09311v1"},"cats":{"new-dataset":0.0289321468,"dev-research":0.3102740977,"prompt-eng":0.4241293272,"data-quality":0.1030128994,"ml-security":0.1635065095}}
{"text":"When partitioning gate-level netlists using graphs, it is beneficial to cluster gates to reduce the order of the graph and preserve some characteristics of the circuit that the partitioning might degrade.","meta":{"url":"http://arxiv.org/abs/2307.09308v1"},"cats":{"new-dataset":0.0110000396,"dev-research":0.3044580291,"prompt-eng":0.3745594939,"data-quality":0.1179481552,"ml-security":0.1360145593}}
{"text":"Gate clustering is even more important for netlist partitioning targeting 3D system integration.","meta":{"url":"http://arxiv.org/abs/2307.09308v1"},"cats":{"new-dataset":0.0165100487,"dev-research":0.3093902016,"prompt-eng":0.365711264,"data-quality":0.1069672949,"ml-security":0.1115660665}}
{"text":"In this paper, we make the argument that the choice of clustering method for 3D-ICs partitioning is not trivial and deserves careful consideration.","meta":{"url":"http://arxiv.org/abs/2307.09308v1"},"cats":{"new-dataset":0.0409199513,"dev-research":0.2492977583,"prompt-eng":0.3399038554,"data-quality":0.0987148913,"ml-security":0.0567929855}}
{"text":"To support our claim, we implemented three clustering methods that were used prior to partitioning two synthetic designs representing two extremes of the circuits medium/long interconnect diversity spectrum.","meta":{"url":"http://arxiv.org/abs/2307.09308v1"},"cats":{"new-dataset":0.0264920515,"dev-research":0.2352937057,"prompt-eng":0.3820820166,"data-quality":0.1101835926,"ml-security":0.0754047485}}
{"text":"Automatically partitioned netlists are then placed and routed in 3D to compare the impact of clustering methods on several metrics.","meta":{"url":"http://arxiv.org/abs/2307.09308v1"},"cats":{"new-dataset":0.0500900264,"dev-research":0.3442577738,"prompt-eng":0.3736709142,"data-quality":0.126328657,"ml-security":0.0800265011}}
{"text":"From our experiments, we see that the clustering method indeed has a different impact depending on the design considered and that a circuit-blind, universal partitioning method is not the way to go, with wire-length savings of up to 31%, total power of up to 22%, and effective frequency of up to 15% compared to other methods.","meta":{"url":"http://arxiv.org/abs/2307.09308v1"},"cats":{"new-dataset":0.0079227547,"dev-research":0.282961945,"prompt-eng":0.3554489331,"data-quality":0.1356575633,"ml-security":0.0990225735}}
{"text":"Furthermore, we highlight that 3D-ICs open new opportunities to design systems with a denser interconnect, drastically reducing the design utilization of circuits that would not be considered viable in 2D.","meta":{"url":"http://arxiv.org/abs/2307.09308v1"},"cats":{"new-dataset":0.0247520377,"dev-research":0.3246972582,"prompt-eng":0.3202352803,"data-quality":0.0527236074,"ml-security":0.0755761729}}
{"text":"Capturing high-dimensional social interactions and feasible futures is essential for predicting trajectories.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.1556105859,"dev-research":0.2526975184,"prompt-eng":0.4288747852,"data-quality":0.0392614016,"ml-security":0.1326592895}}
{"text":"To address this complex nature, several attempts have been devoted to reducing the dimensionality of the output variables via parametric curve fitting such as the B\\'ezier curve and B-spline function.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.0722054306,"dev-research":0.2807395693,"prompt-eng":0.4191697864,"data-quality":0.0758518977,"ml-security":0.0718499565}}
{"text":"However, these functions, which originate in computer graphics fields, are not suitable to account for socially acceptable human dynamics.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.0417218641,"dev-research":0.3137730136,"prompt-eng":0.3401134059,"data-quality":0.1351739797,"ml-security":0.1858044708}}
{"text":"In this paper, we present EigenTrajectory ($\\mathbb{ET}$), a trajectory prediction approach that uses a novel trajectory descriptor to form a compact space, known here as $\\mathbb{ET}$ space, in place of Euclidean space, for representing pedestrian movements.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.238193184,"dev-research":0.240685313,"prompt-eng":0.3741993275,"data-quality":0.0901954222,"ml-security":0.1510153218}}
{"text":"We first reduce the complexity of the trajectory descriptor via a low-rank approximation.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.0835087851,"dev-research":0.2044364152,"prompt-eng":0.3683338397,"data-quality":0.1011683996,"ml-security":0.074295541}}
{"text":"We transform the pedestrians' history paths into our $\\mathbb{ET}$ space represented by spatio-temporal principle components, and feed them into off-the-shelf trajectory forecasting models.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.3582333897,"dev-research":0.2353855293,"prompt-eng":0.3932846029,"data-quality":0.0571246106,"ml-security":0.1051009208}}
{"text":"The inputs and outputs of the models as well as social interactions are all gathered and aggregated in the corresponding $\\mathbb{ET}$ space.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.1488611579,"dev-research":0.2605889492,"prompt-eng":0.3972079993,"data-quality":0.0951299006,"ml-security":0.1037314499}}
{"text":"Lastly, we propose a trajectory anchor-based refinement method to cover all possible futures in the proposed $\\mathbb{ET}$ space.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.0759567151,"dev-research":0.2444531368,"prompt-eng":0.3882195225,"data-quality":0.077327539,"ml-security":0.0570612184}}
{"text":"Extensive experiments demonstrate that our EigenTrajectory predictor can significantly improve both the prediction accuracy and reliability of existing trajectory forecasting models on public benchmarks, indicating that the proposed descriptor is suited to represent pedestrian behaviors.","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.1505382687,"dev-research":0.2463457509,"prompt-eng":0.4097367737,"data-quality":0.1003979149,"ml-security":0.1703403211}}
{"text":"Code is publicly available at https://github.com/inhwanbae/EigenTrajectory .","meta":{"url":"http://arxiv.org/abs/2307.09306v1"},"cats":{"new-dataset":0.1487854794,"dev-research":0.24231815,"prompt-eng":0.4010281164,"data-quality":0.0992096415,"ml-security":0.0784281984}}
{"text":"In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.022993933,"dev-research":0.2753106865,"prompt-eng":0.4338691857,"data-quality":0.2283296306,"ml-security":0.3653410622}}
{"text":"This generally assumes the availability of a held-out calibration set with access to ground truth labels.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.0883467345,"dev-research":0.2408750905,"prompt-eng":0.4243696876,"data-quality":0.3722327344,"ml-security":0.0896617116}}
{"text":"Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.0891567978,"dev-research":0.3419213633,"prompt-eng":0.427631684,"data-quality":0.5045299245,"ml-security":0.0720642776}}
{"text":"In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.3000824921,"dev-research":0.2547274476,"prompt-eng":0.3209747689,"data-quality":0.3173261602,"ml-security":0.2472862007}}
{"text":"Applying conformal prediction using such labels underestimates uncertainty.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.017059214,"dev-research":0.214407616,"prompt-eng":0.4333431635,"data-quality":0.3263162646,"ml-security":0.1527805508}}
{"text":"Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.025157872,"dev-research":0.3656777728,"prompt-eng":0.3787172191,"data-quality":0.598937597,"ml-security":0.0863016704}}
{"text":"That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.0830942499,"dev-research":0.2908293543,"prompt-eng":0.410632507,"data-quality":0.4455202741,"ml-security":0.058376425}}
{"text":"In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlying posterior distribution of labels given inputs.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.0658683956,"dev-research":0.2137180191,"prompt-eng":0.4256846572,"data-quality":0.363520815,"ml-security":0.1662105961}}
{"text":"We demonstrate our methodology on synthetic and real datasets, including a case study of skin condition classification in dermatology.","meta":{"url":"http://arxiv.org/abs/2307.09302v1"},"cats":{"new-dataset":0.7666887556,"dev-research":0.3113246419,"prompt-eng":0.3330484161,"data-quality":0.2049218564,"ml-security":0.3124402397}}
{"text":"Explicit bases for the subfield subcodes of projective Reed-Muller codes over the projective plane and their duals are obtained.","meta":{"url":"http://arxiv.org/abs/2307.09298v1"},"cats":{"new-dataset":0.1055285537,"dev-research":0.2372829147,"prompt-eng":0.4183596872,"data-quality":0.123439423,"ml-security":0.0625194419}}
{"text":"In particular, we provide a formula for the dimension of these codes.","meta":{"url":"http://arxiv.org/abs/2307.09298v1"},"cats":{"new-dataset":0.2729304561,"dev-research":0.3119979041,"prompt-eng":0.4224742884,"data-quality":0.1518999093,"ml-security":0.0951942982}}
{"text":"For the general case over the projective space, we are able to generalize the necessary tools to deal with this case as well: we obtain a universal Gr\\\"obner basis for the vanishing ideal of the set of standard representatives of the projective space and we are able to reduce any monomial with respect to this Gr\\\"obner basis.","meta":{"url":"http://arxiv.org/abs/2307.09298v1"},"cats":{"new-dataset":0.0539280503,"dev-research":0.2462882677,"prompt-eng":0.4237758588,"data-quality":0.1162039996,"ml-security":0.1131773426}}
{"text":"With respect to the parameters of these codes, by considering subfield subcodes of projective Reed-Muller codes we are able to obtain long linear codes with good parameters over a small finite field.","meta":{"url":"http://arxiv.org/abs/2307.09298v1"},"cats":{"new-dataset":0.1610714168,"dev-research":0.2014523606,"prompt-eng":0.3938183815,"data-quality":0.1168890954,"ml-security":0.0672882107}}
{"text":"The growth in social media has exacerbated the threat of fake news to individuals and communities.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.0745366658,"dev-research":0.3146908063,"prompt-eng":0.3425036108,"data-quality":0.2265916105,"ml-security":0.3661027566}}
{"text":"This draws increasing attention to developing efficient and timely rumor detection methods.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.1133277941,"dev-research":0.3644655053,"prompt-eng":0.3931327181,"data-quality":0.3562096637,"ml-security":0.2753656133}}
{"text":"The prevailing approaches resort to graph neural networks (GNNs) to exploit the post-propagation patterns of the rumor-spreading process.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.0938609995,"dev-research":0.2909918984,"prompt-eng":0.3543577502,"data-quality":0.272928117,"ml-security":0.4393608321}}
{"text":"However, these methods lack inherent interpretation of rumor detection due to the black-box nature of GNNs.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.0630723584,"dev-research":0.3073051849,"prompt-eng":0.3273483133,"data-quality":0.4654638856,"ml-security":0.4393153843}}
{"text":"Moreover, these methods suffer from less robust results as they employ all the propagation patterns for rumor detection.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.0506097241,"dev-research":0.2966417709,"prompt-eng":0.3700021686,"data-quality":0.462907163,"ml-security":0.3243357905}}
{"text":"In this paper, we address the above issues with the proposed Diverse Counterfactual Evidence framework for Rumor Detection (DCE-RD).","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.2374946442,"dev-research":0.2891899217,"prompt-eng":0.3657241158,"data-quality":0.4504941073,"ml-security":0.3054200405}}
{"text":"Our intuition is to exploit the diverse counterfactual evidence of an event graph to serve as multi-view interpretations, which are further aggregated for robust rumor detection results.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.142446923,"dev-research":0.3531732505,"prompt-eng":0.3778050194,"data-quality":0.3936411309,"ml-security":0.2751218712}}
{"text":"Specifically, our method first designs a subgraph generation strategy to efficiently generate different subgraphs of the event graph.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.1596868576,"dev-research":0.3188237702,"prompt-eng":0.3956460345,"data-quality":0.1183620492,"ml-security":0.0446492388}}
{"text":"We constrain the removal of these subgraphs to cause the change in rumor detection results.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.1059870043,"dev-research":0.3189363529,"prompt-eng":0.3677807131,"data-quality":0.4466236245,"ml-security":0.2728204676}}
{"text":"Thus, these subgraphs naturally serve as counterfactual evidence for rumor detection.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.1871999502,"dev-research":0.3396405097,"prompt-eng":0.3836012819,"data-quality":0.436188436,"ml-security":0.2439764759}}
{"text":"To achieve multi-view interpretation, we design a diversity loss inspired by Determinantal Point Processes (DPP) to encourage diversity among the counterfactual evidence.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.06266871,"dev-research":0.3241156078,"prompt-eng":0.3948193438,"data-quality":0.2085530039,"ml-security":0.1581512034}}
{"text":"A GNN-based rumor detection model further aggregates the diverse counterfactual evidence discovered by the proposed DCE-RD to achieve interpretable and robust rumor detection results.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.2196051201,"dev-research":0.295131656,"prompt-eng":0.3486643811,"data-quality":0.4241860294,"ml-security":0.3518890215}}
{"text":"Extensive experiments on two real-world datasets show the superior performance of our method.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.3997542291,"dev-research":0.2579810309,"prompt-eng":0.3381672399,"data-quality":0.1765414271,"ml-security":0.1256159073}}
{"text":"Our code is available at https://github.com/Vicinity111/DCE-RD.","meta":{"url":"http://arxiv.org/abs/2307.09296v1"},"cats":{"new-dataset":0.3891049727,"dev-research":0.2677124983,"prompt-eng":0.4440440354,"data-quality":0.1327930749,"ml-security":0.0465160179}}
{"text":"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.","meta":{"url":"http://arxiv.org/abs/2307.09288v1"},"cats":{"new-dataset":0.3786776625,"dev-research":0.2025469904,"prompt-eng":0.5150941003,"data-quality":0.1433965613,"ml-security":0.1383798212}}
{"text":"Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases.","meta":{"url":"http://arxiv.org/abs/2307.09288v1"},"cats":{"new-dataset":0.0813639036,"dev-research":0.2260510643,"prompt-eng":0.4668622609,"data-quality":0.0945118274,"ml-security":0.1575934397}}
{"text":"Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models.","meta":{"url":"http://arxiv.org/abs/2307.09288v1"},"cats":{"new-dataset":0.3223339119,"dev-research":0.332596268,"prompt-eng":0.4017295083,"data-quality":0.1612597978,"ml-security":0.219710685}}
{"text":"We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","meta":{"url":"http://arxiv.org/abs/2307.09288v1"},"cats":{"new-dataset":0.266824848,"dev-research":0.3086653524,"prompt-eng":0.481181146,"data-quality":0.1341060336,"ml-security":0.1941011217}}
{"text":"The objective of this work is to give patch-size flexibility to Audio Spectrogram Transformers (AST).","meta":{"url":"http://arxiv.org/abs/2307.09286v1"},"cats":{"new-dataset":0.0534877652,"dev-research":0.2990926369,"prompt-eng":0.3944373671,"data-quality":0.1581990338,"ml-security":0.0624151927}}
{"text":"Recent advancements in ASTs have shown superior performance in various audio-based tasks.","meta":{"url":"http://arxiv.org/abs/2307.09286v1"},"cats":{"new-dataset":0.0252754046,"dev-research":0.2743075502,"prompt-eng":0.3730201715,"data-quality":0.102470302,"ml-security":0.0487018151}}
{"text":"However, the performance of standard ASTs degrades drastically when evaluated using different patch sizes from that used during training.","meta":{"url":"http://arxiv.org/abs/2307.09286v1"},"cats":{"new-dataset":0.0122958885,"dev-research":0.2689762688,"prompt-eng":0.362559217,"data-quality":0.1413477515,"ml-security":0.1278926848}}
{"text":"As a result, AST models are typically re-trained to accommodate changes in patch sizes.","meta":{"url":"http://arxiv.org/abs/2307.09286v1"},"cats":{"new-dataset":0.0153761619,"dev-research":0.266629035,"prompt-eng":0.4250326408,"data-quality":0.1121628396,"ml-security":0.1075277491}}
{"text":"To overcome this limitation, this paper proposes a training procedure to provide flexibility to standard AST models without architectural changes, allowing them to work with various patch sizes at the inference stage - FlexiAST.","meta":{"url":"http://arxiv.org/abs/2307.09286v1"},"cats":{"new-dataset":0.0274150597,"dev-research":0.1952268995,"prompt-eng":0.4365627306,"data-quality":0.0861480179,"ml-security":0.128131622}}
{"text":"This proposed training approach simply utilizes random patch size selection and resizing of patch and positional embedding weights.","meta":{"url":"http://arxiv.org/abs/2307.09286v1"},"cats":{"new-dataset":0.0384010886,"dev-research":0.225520037,"prompt-eng":0.4079391054,"data-quality":0.1613377467,"ml-security":0.1648822254}}
{"text":"Our experiments show that FlexiAST gives similar performance to standard AST models while maintaining its evaluation ability at various patch sizes on different datasets for audio classification tasks.","meta":{"url":"http://arxiv.org/abs/2307.09286v1"},"cats":{"new-dataset":0.1192843953,"dev-research":0.2308862633,"prompt-eng":0.3841073181,"data-quality":0.2424350688,"ml-security":0.0804073092}}
{"text":"Recently, lightweight Vision Transformers (ViTs) demonstrate superior performance and lower latency compared with lightweight Convolutional Neural Networks (CNNs) on resource-constrained mobile devices.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.0568459461,"dev-research":0.2875909198,"prompt-eng":0.3817317535,"data-quality":0.0781908678,"ml-security":0.118305536}}
{"text":"This improvement is usually attributed to the multi-head self-attention module, which enables the model to learn global representations.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.0377472427,"dev-research":0.2647338343,"prompt-eng":0.4757318869,"data-quality":0.1350489526,"ml-security":0.0925039978}}
{"text":"However, the architectural disparities between lightweight ViTs and lightweight CNNs have not been adequately examined.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.0185432538,"dev-research":0.2810827444,"prompt-eng":0.3473612383,"data-quality":0.1464534167,"ml-security":0.1449130199}}
{"text":"In this study, we revisit the efficient design of lightweight CNNs and emphasize their potential for mobile devices.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.0550277612,"dev-research":0.2872319923,"prompt-eng":0.3778319759,"data-quality":0.083436936,"ml-security":0.1676313383}}
{"text":"We incrementally enhance the mobile-friendliness of a standard lightweight CNN, specifically MobileNetV3, by integrating the efficient architectural choices of lightweight ViTs.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.0995317916,"dev-research":0.3350983103,"prompt-eng":0.3825194526,"data-quality":0.0870592051,"ml-security":0.1452568486}}
{"text":"This ends up with a new family of pure lightweight CNNs, namely RepViT. Extensive experiments show that RepViT outperforms existing state-of-the-art lightweight ViTs and exhibits favorable latency in various vision tasks.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.0740591183,"dev-research":0.2527037784,"prompt-eng":0.396062538,"data-quality":0.1200103333,"ml-security":0.1064296358}}
{"text":"On ImageNet, RepViT achieves over 80\\% top-1 accuracy with nearly 1ms latency on an iPhone 12, which is the first time for a lightweight model, to the best of our knowledge.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.0648081473,"dev-research":0.2534367431,"prompt-eng":0.4203804732,"data-quality":0.1413447437,"ml-security":0.0679490162}}
{"text":"Our largest model, RepViT-M3, obtains 81.4\\% accuracy with only 1.3ms latency.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.1018429987,"dev-research":0.1984910513,"prompt-eng":0.4133916552,"data-quality":0.1648073907,"ml-security":0.0428323598}}
{"text":"The code and trained models are available at \\url{https://github.com/jameslahm/RepViT}.","meta":{"url":"http://arxiv.org/abs/2307.09283v1"},"cats":{"new-dataset":0.1480405332,"dev-research":0.2047933452,"prompt-eng":0.4467245707,"data-quality":0.13109921,"ml-security":0.1225332641}}
{"text":"Regression-based blind image quality assessment (IQA) models are susceptible to biased training samples, leading to a biased estimation of model parameters.","meta":{"url":"http://arxiv.org/abs/2307.09279v1"},"cats":{"new-dataset":0.0764161348,"dev-research":0.2973117677,"prompt-eng":0.4004805488,"data-quality":0.2766750394,"ml-security":0.1713750597}}
{"text":"To mitigate this issue, we propose a regression-free framework for image quality evaluation, which is founded upon retrieving similar instances by incorporating semantic and distortion features.","meta":{"url":"http://arxiv.org/abs/2307.09279v1"},"cats":{"new-dataset":0.1605095243,"dev-research":0.3070850325,"prompt-eng":0.4098586118,"data-quality":0.3420216468,"ml-security":0.1220782421}}
{"text":"The motivation behind this approach is rooted in the observation that the human visual system (HVS) has analogous visual responses to semantically similar image contents degraded by the same distortion.","meta":{"url":"http://arxiv.org/abs/2307.09279v1"},"cats":{"new-dataset":0.0122955559,"dev-research":0.3519672544,"prompt-eng":0.4264722351,"data-quality":0.3078815705,"ml-security":0.1065641623}}
{"text":"The proposed framework comprises two classification-based modules: semantic-based classification (SC) module and distortion-based classification (DC) module.","meta":{"url":"http://arxiv.org/abs/2307.09279v1"},"cats":{"new-dataset":0.1069519031,"dev-research":0.3138335366,"prompt-eng":0.4023446404,"data-quality":0.4633802236,"ml-security":0.1248798642}}
{"text":"Given a test image and an IQA database, the SC module retrieves multiple pristine images based on semantic similarity.","meta":{"url":"http://arxiv.org/abs/2307.09279v1"},"cats":{"new-dataset":0.250031676,"dev-research":0.2781095039,"prompt-eng":0.4171180778,"data-quality":0.236609727,"ml-security":0.0498243263}}
{"text":"The DC module then retrieves instances based on distortion similarity from the distorted images that correspond to each retrieved pristine image.","meta":{"url":"http://arxiv.org/abs/2307.09279v1"},"cats":{"new-dataset":0.0869984014,"dev-research":0.2132113748,"prompt-eng":0.401333102,"data-quality":0.3274505216,"ml-security":0.0552988678}}
{"text":"Finally, the predicted quality score is derived by aggregating the subjective quality scores of multiple retrieved instances.","meta":{"url":"http://arxiv.org/abs/2307.09279v1"},"cats":{"new-dataset":0.1739253621,"dev-research":0.3704880752,"prompt-eng":0.4556042483,"data-quality":0.2879955956,"ml-security":0.0650466535}}
{"text":"Experimental results on four benchmark databases validate that the proposed model can remarkably outperform the state-of-the-art regression-based models.","meta":{"url":"http://arxiv.org/abs/2307.09279v1"},"cats":{"new-dataset":0.0935105664,"dev-research":0.2822258173,"prompt-eng":0.3939603409,"data-quality":0.1539031559,"ml-security":0.1493672629}}
{"text":"Siamese networks have gained popularity as a method for modeling text semantic similarity.","meta":{"url":"http://arxiv.org/abs/2307.09274v1"},"cats":{"new-dataset":0.1515070917,"dev-research":0.2892171025,"prompt-eng":0.368365674,"data-quality":0.2307910248,"ml-security":0.0659792102}}
{"text":"Traditional methods rely on pooling operation to compress the semantic representations from Transformer blocks in encoding, resulting in two-dimensional semantic vectors and the loss of hierarchical semantic information from Transformer blocks.","meta":{"url":"http://arxiv.org/abs/2307.09274v1"},"cats":{"new-dataset":0.015242978,"dev-research":0.2921960009,"prompt-eng":0.4229088018,"data-quality":0.2681058569,"ml-security":0.084912658}}
{"text":"Moreover, this limited structure of semantic vectors is akin to a flattened landscape, which restricts the methods that can be applied in downstream modeling, as they can only navigate this flat terrain.","meta":{"url":"http://arxiv.org/abs/2307.09274v1"},"cats":{"new-dataset":0.0202118587,"dev-research":0.2820881677,"prompt-eng":0.3859918685,"data-quality":0.1288012447,"ml-security":0.099838451}}
{"text":"To address this issue, we propose a novel 3D Siamese network for text semantic similarity modeling, which maps semantic information to a higher-dimensional space.","meta":{"url":"http://arxiv.org/abs/2307.09274v1"},"cats":{"new-dataset":0.2273734924,"dev-research":0.2896965482,"prompt-eng":0.3301988824,"data-quality":0.1651404051,"ml-security":0.0647520603}}
{"text":"The three-dimensional semantic tensors not only retains more precise spatial and feature domain information but also provides the necessary structural condition for comprehensive downstream modeling strategies to capture them.","meta":{"url":"http://arxiv.org/abs/2307.09274v1"},"cats":{"new-dataset":0.0270220857,"dev-research":0.2461143574,"prompt-eng":0.4213225004,"data-quality":0.1701118885,"ml-security":0.0770091323}}
{"text":"Leveraging this structural advantage, we introduce several modules to reinforce this 3D framework, focusing on three aspects: feature extraction, attention, and feature fusion.","meta":{"url":"http://arxiv.org/abs/2307.09274v1"},"cats":{"new-dataset":0.1202376919,"dev-research":0.3115635324,"prompt-eng":0.4034160369,"data-quality":0.1162982228,"ml-security":0.0677217845}}
{"text":"Our extensive experiments on four text semantic similarity benchmarks demonstrate the effectiveness and efficiency of our 3D Siamese Network.","meta":{"url":"http://arxiv.org/abs/2307.09274v1"},"cats":{"new-dataset":0.1866917163,"dev-research":0.3086500306,"prompt-eng":0.3318453234,"data-quality":0.1609410028,"ml-security":0.0539012492}}
{"text":"Relative positional encoding is widely used in vanilla and linear transformers to represent positional information.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.025220703,"dev-research":0.2927962771,"prompt-eng":0.4437670334,"data-quality":0.1296425178,"ml-security":0.1034661808}}
{"text":"However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.0078365299,"dev-research":0.2689926748,"prompt-eng":0.3749075457,"data-quality":0.1528435744,"ml-security":0.1437805591}}
{"text":"Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.0133423896,"dev-research":0.2750182024,"prompt-eng":0.3978263422,"data-quality":0.1897998364,"ml-security":0.1142202161}}
{"text":"In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.0579536827,"dev-research":0.2218891309,"prompt-eng":0.3900367302,"data-quality":0.1643284235,"ml-security":0.093875472}}
{"text":"Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.0287682131,"dev-research":0.2221538308,"prompt-eng":0.3632794734,"data-quality":0.1333262829,"ml-security":0.101130836}}
{"text":"Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.0331732654,"dev-research":0.2151767438,"prompt-eng":0.4375771462,"data-quality":0.152552057,"ml-security":0.1013795853}}
{"text":"Experiments show that compared with existing methods, LRPE achieves state-of-the-art performance in language modeling, text classification, and image classification.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.078695468,"dev-research":0.2456327612,"prompt-eng":0.4409903155,"data-quality":0.2766034536,"ml-security":0.0946383471}}
{"text":"Meanwhile, it emphasizes a general paradigm for designing broadly more relative positional encoding methods that are applicable to linear transformers.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.0070699603,"dev-research":0.3103668013,"prompt-eng":0.4009145048,"data-quality":0.1152464236,"ml-security":0.087494666}}
{"text":"The code is available at https://github.com/OpenNLPLab/Lrpe.","meta":{"url":"http://arxiv.org/abs/2307.09270v1"},"cats":{"new-dataset":0.2092391138,"dev-research":0.2013520609,"prompt-eng":0.3983512557,"data-quality":0.1181614768,"ml-security":0.0624219888}}
{"text":"Hyperbox-based classification has been seen as a promising technique in which decisions on the data are represented as a series of orthogonal, multidimensional boxes (i.e., hyperboxes) that are often interpretable and human-readable.","meta":{"url":"http://arxiv.org/abs/2307.09269v1"},"cats":{"new-dataset":0.0579830651,"dev-research":0.3188059917,"prompt-eng":0.3854061788,"data-quality":0.1636828049,"ml-security":0.1970269546}}
{"text":"However, existing methods are no longer capable of efficiently handling the increasing volume of data many application domains face nowadays.","meta":{"url":"http://arxiv.org/abs/2307.09269v1"},"cats":{"new-dataset":0.0971379777,"dev-research":0.345835649,"prompt-eng":0.3200158849,"data-quality":0.081724882,"ml-security":0.1440724423}}
{"text":"We address this gap by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks.","meta":{"url":"http://arxiv.org/abs/2307.09269v1"},"cats":{"new-dataset":0.0780529315,"dev-research":0.2633302032,"prompt-eng":0.3467190681,"data-quality":0.2190211616,"ml-security":0.3091604647}}
{"text":"In contrast to previous work, our hyperbox models can be efficiently trained in an end-to-end fashion, which leads to significantly reduced training times and superior classification results.","meta":{"url":"http://arxiv.org/abs/2307.09269v1"},"cats":{"new-dataset":0.0196591023,"dev-research":0.2388657576,"prompt-eng":0.4056979423,"data-quality":0.1175021739,"ml-security":0.2170482352}}
{"text":"3D visual grounding involves finding a target object in a 3D scene that corresponds to a given sentence query.","meta":{"url":"http://arxiv.org/abs/2307.09267v1"},"cats":{"new-dataset":0.0885641162,"dev-research":0.3263679516,"prompt-eng":0.4406215447,"data-quality":0.1589086156,"ml-security":0.0819205992}}
{"text":"Although many approaches have been proposed and achieved impressive performance, they all require dense object-sentence pair annotations in 3D point clouds, which are both time-consuming and expensive.","meta":{"url":"http://arxiv.org/abs/2307.09267v1"},"cats":{"new-dataset":0.198506292,"dev-research":0.2706466875,"prompt-eng":0.3725286188,"data-quality":0.2263144069,"ml-security":0.0555772701}}
{"text":"To address the problem that fine-grained annotated data is difficult to obtain, we propose to leverage weakly supervised annotations to learn the 3D visual grounding model, i.e., only coarse scene-sentence correspondences are used to learn object-sentence links.","meta":{"url":"http://arxiv.org/abs/2307.09267v1"},"cats":{"new-dataset":0.3202821734,"dev-research":0.2659237202,"prompt-eng":0.4360590965,"data-quality":0.3801011355,"ml-security":0.0977972846}}
{"text":"To accomplish this, we design a novel semantic matching model that analyzes the semantic similarity between object proposals and sentences in a coarse-to-fine manner.","meta":{"url":"http://arxiv.org/abs/2307.09267v1"},"cats":{"new-dataset":0.107433155,"dev-research":0.3032734713,"prompt-eng":0.4551749291,"data-quality":0.2922547201,"ml-security":0.0709642187}}
{"text":"Specifically, we first extract object proposals and coarsely select the top-K candidates based on feature and class similarity matrices.","meta":{"url":"http://arxiv.org/abs/2307.09267v1"},"cats":{"new-dataset":0.1861877919,"dev-research":0.2669875817,"prompt-eng":0.4337589895,"data-quality":0.1426262089,"ml-security":0.0876273773}}
{"text":"Next, we reconstruct the masked keywords of the sentence using each candidate one by one, and the reconstructed accuracy finely reflects the semantic similarity of each candidate to the query.","meta":{"url":"http://arxiv.org/abs/2307.09267v1"},"cats":{"new-dataset":0.119752704,"dev-research":0.29998607,"prompt-eng":0.4264305504,"data-quality":0.285177205,"ml-security":0.1409131501}}
{"text":"Additionally, we distill the coarse-to-fine semantic matching knowledge into a typical two-stage 3D visual grounding model, which reduces inference costs and improves performance by taking full advantage of the well-studied structure of the existing architectures.","meta":{"url":"http://arxiv.org/abs/2307.09267v1"},"cats":{"new-dataset":0.1929889179,"dev-research":0.2871617917,"prompt-eng":0.4076007741,"data-quality":0.1500755359,"ml-security":0.0658993152}}
{"text":"We conduct extensive experiments on ScanRefer, Nr3D, and Sr3D, which demonstrate the effectiveness of our proposed method.","meta":{"url":"http://arxiv.org/abs/2307.09267v1"},"cats":{"new-dataset":0.0474091322,"dev-research":0.2252757182,"prompt-eng":0.4094846054,"data-quality":0.0779712399,"ml-security":0.0411440846}}
{"text":"Knowledge distillation, a well-known model compression technique, is an active research area in both computer vision and remote sensing communities.","meta":{"url":"http://arxiv.org/abs/2307.09264v1"},"cats":{"new-dataset":0.1027229441,"dev-research":0.3057207016,"prompt-eng":0.4158462857,"data-quality":0.1599414977,"ml-security":0.1027987978}}
{"text":"In this paper, we evaluate in a remote sensing context various off-the-shelf object detection knowledge distillation methods which have been originally developed on generic computer vision datasets such as Pascal VOC.","meta":{"url":"http://arxiv.org/abs/2307.09264v1"},"cats":{"new-dataset":0.2382168411,"dev-research":0.2506204741,"prompt-eng":0.3977196733,"data-quality":0.2154226953,"ml-security":0.128367363}}
{"text":"In particular, methods covering both logit mimicking and feature imitation approaches are applied for vehicle detection using the well-known benchmarks such as xView and VEDAI datasets.","meta":{"url":"http://arxiv.org/abs/2307.09264v1"},"cats":{"new-dataset":0.1083760126,"dev-research":0.2277358277,"prompt-eng":0.4058673875,"data-quality":0.1489843799,"ml-security":0.1487939744}}
{"text":"Extensive experiments are performed to compare the relative performance and interrelationships of the methods.","meta":{"url":"http://arxiv.org/abs/2307.09264v1"},"cats":{"new-dataset":0.0067840523,"dev-research":0.2667807034,"prompt-eng":0.4076913829,"data-quality":0.1007464017,"ml-security":0.0413723545}}
{"text":"Experimental results show high variations and confirm the importance of result aggregation and cross validation on remote sensing datasets.","meta":{"url":"http://arxiv.org/abs/2307.09264v1"},"cats":{"new-dataset":0.2107817742,"dev-research":0.2356476278,"prompt-eng":0.4170055505,"data-quality":0.1986950135,"ml-security":0.0876089838}}
{"text":"As an efficient distributed machine learning approach, Federated learning (FL) can obtain a shared model by iterative local model training at the user side and global model aggregating at the central server side, thereby protecting privacy of users.","meta":{"url":"http://arxiv.org/abs/2307.09263v1"},"cats":{"new-dataset":0.07184158,"dev-research":0.2260495296,"prompt-eng":0.3864498258,"data-quality":0.1031759468,"ml-security":0.4127179446}}
{"text":"Mobile users in FL systems typically communicate with base stations (BSs) via wireless channels, where training performance could be degraded due to unreliable access caused by user mobility.","meta":{"url":"http://arxiv.org/abs/2307.09263v1"},"cats":{"new-dataset":0.052734701,"dev-research":0.3241436964,"prompt-eng":0.408800053,"data-quality":0.1053604598,"ml-security":0.1751433412}}
{"text":"However, existing work only investigates a static scenario or random initialization of user locations, which fail to capture mobility in real-world networks.","meta":{"url":"http://arxiv.org/abs/2307.09263v1"},"cats":{"new-dataset":0.0614322749,"dev-research":0.257850099,"prompt-eng":0.3559318644,"data-quality":0.1199257179,"ml-security":0.22235985}}
{"text":"To tackle this issue, we propose a practical model for user mobility in FL across multiple BSs, and develop a user scheduling and resource allocation method to minimize the training delay with constrained communication resources.","meta":{"url":"http://arxiv.org/abs/2307.09263v1"},"cats":{"new-dataset":0.0879593591,"dev-research":0.2395805929,"prompt-eng":0.3809053697,"data-quality":0.0558011581,"ml-security":0.1119610757}}
{"text":"Specifically, we first formulate an optimization problem with user mobility that jointly considers user selection, BS assignment to users, and bandwidth allocation to minimize the latency in each communication round.","meta":{"url":"http://arxiv.org/abs/2307.09263v1"},"cats":{"new-dataset":0.0285246215,"dev-research":0.2293164584,"prompt-eng":0.4065330194,"data-quality":0.0686125347,"ml-security":0.0874560186}}
{"text":"This optimization problem turned out to be NP-hard and we proposed a delay-aware greedy search algorithm (DAGSA) to solve it.","meta":{"url":"http://arxiv.org/abs/2307.09263v1"},"cats":{"new-dataset":0.120315072,"dev-research":0.1715913315,"prompt-eng":0.3822309715,"data-quality":0.0840512484,"ml-security":0.0841355347}}
{"text":"Simulation results show that the proposed algorithm achieves better performance than the state-of-the-art baselines and a certain level of user mobility could improve training performance.","meta":{"url":"http://arxiv.org/abs/2307.09263v1"},"cats":{"new-dataset":0.0685300613,"dev-research":0.2392117783,"prompt-eng":0.3752126352,"data-quality":0.0765887238,"ml-security":0.1254103563}}
{"text":"In this study, we developed a quantitative description of the dynamics of spin-torque vortex nano-oscillators (STVOs) through an unconventional model based on the combination of the Thiele equation approach (TEA) and data from micromagnetic simulations (MMS).","meta":{"url":"http://arxiv.org/abs/2307.09262v1"},"cats":{"new-dataset":0.033045483,"dev-research":0.2312907608,"prompt-eng":0.3685532357,"data-quality":0.0465306319,"ml-security":0.0829797359}}
{"text":"Solving the STVO dynamics with our analytical model allows to accelerate the simulations by 9 orders of magnitude compared to MMS while reaching the same level of accuracy.","meta":{"url":"http://arxiv.org/abs/2307.09262v1"},"cats":{"new-dataset":0.0242482271,"dev-research":0.1959854851,"prompt-eng":0.3826189436,"data-quality":0.073663277,"ml-security":0.0815421409}}
{"text":"Here, we showcase our model by simulating a STVO-based neural network for solving a classification task.","meta":{"url":"http://arxiv.org/abs/2307.09262v1"},"cats":{"new-dataset":0.1562862406,"dev-research":0.2276660533,"prompt-eng":0.4456198237,"data-quality":0.2298528646,"ml-security":0.2034109874}}
{"text":"We assess its performance with respect to the input signal current intensity and the level of noise that might affect such a system.","meta":{"url":"http://arxiv.org/abs/2307.09262v1"},"cats":{"new-dataset":0.0290220323,"dev-research":0.2434742995,"prompt-eng":0.4230178729,"data-quality":0.2336260031,"ml-security":0.1525944827}}
{"text":"Our approach is promising for accelerating the design of STVO-based neuromorphic computing devices while decreasing drastically its computational cost.","meta":{"url":"http://arxiv.org/abs/2307.09262v1"},"cats":{"new-dataset":0.0252504174,"dev-research":0.2622371671,"prompt-eng":0.3969892714,"data-quality":0.0595751818,"ml-security":0.128972921}}
{"text":"Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science.","meta":{"url":"http://arxiv.org/abs/2307.09259v1"},"cats":{"new-dataset":0.0871840166,"dev-research":0.2285772808,"prompt-eng":0.3773623633,"data-quality":0.1192189766,"ml-security":0.1760260387}}
{"text":"To enhance the accuracy of such machine learning methods, it is known to be effective to incorporate global topological features, which are typically extracted by persistent homology.","meta":{"url":"http://arxiv.org/abs/2307.09259v1"},"cats":{"new-dataset":0.086412121,"dev-research":0.3000415373,"prompt-eng":0.3724231778,"data-quality":0.1522824781,"ml-security":0.0895469719}}
{"text":"In the calculation of persistent homology for a point cloud, we need to choose a filtration for the point clouds, an increasing sequence of spaces.","meta":{"url":"http://arxiv.org/abs/2307.09259v1"},"cats":{"new-dataset":0.0780406317,"dev-research":0.2455969926,"prompt-eng":0.3502779475,"data-quality":0.0964900994,"ml-security":0.0599556015}}
{"text":"Because the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks.","meta":{"url":"http://arxiv.org/abs/2307.09259v1"},"cats":{"new-dataset":0.0193583243,"dev-research":0.2759408387,"prompt-eng":0.3830242861,"data-quality":0.1246710639,"ml-security":0.1135947675}}
{"text":"In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks.","meta":{"url":"http://arxiv.org/abs/2307.09259v1"},"cats":{"new-dataset":0.0358754506,"dev-research":0.2138765148,"prompt-eng":0.4032112675,"data-quality":0.1469255316,"ml-security":0.2532140574}}
{"text":"In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance.","meta":{"url":"http://arxiv.org/abs/2307.09259v1"},"cats":{"new-dataset":0.064085112,"dev-research":0.2394914077,"prompt-eng":0.3727500483,"data-quality":0.172721721,"ml-security":0.0960310718}}
{"text":"Additionally, we theoretically show a finite-dimensional approximation result that justifies our architecture.","meta":{"url":"http://arxiv.org/abs/2307.09259v1"},"cats":{"new-dataset":0.0282993467,"dev-research":0.2352873629,"prompt-eng":0.3684397978,"data-quality":0.0798192363,"ml-security":0.1685719214}}
{"text":"Experimental results demonstrated the efficacy of our framework in several classification tasks.","meta":{"url":"http://arxiv.org/abs/2307.09259v1"},"cats":{"new-dataset":0.0209249278,"dev-research":0.3258779819,"prompt-eng":0.4780530097,"data-quality":0.3346738793,"ml-security":0.2539168403}}
{"text":"In this paper, we revisit the classic approximate All-Pairs Shortest Paths (APSP) problem in undirected graphs.","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.0911626348,"dev-research":0.2022175463,"prompt-eng":0.3347782858,"data-quality":0.1059149896,"ml-security":0.0732802137}}
{"text":"For unweighted graphs, we provide an algorithm for $2$-approximate APSP in $\\tilde O(n^{2.5-r}+n^{\\omega(r)})$ time, for any $r\\in[0,1]$. This is $O(n^{2.032})$ time, using known bounds for rectangular matrix multiplication~$n^{\\omega(r)}$~[Le Gall, Urrutia, SODA 2018].","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.0891419362,"dev-research":0.2093809559,"prompt-eng":0.3192113925,"data-quality":0.0821501312,"ml-security":0.0895867982}}
{"text":"Our result improves on the $\\tilde{O}(n^{2.25})$ bound of [Roddity, STOC 2023], and on the $\\tilde{O}(m\\sqrt n+n^2)$ bound of [Baswana, Kavitha, SICOMP 2010] for graphs with $m\\geq n^{1.532}$ edges.   ","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.1740387447,"dev-research":0.1973411781,"prompt-eng":0.3005417085,"data-quality":0.1201109672,"ml-security":0.0856514729}}
{"text":"For weighted graphs, we obtain $(2+\\epsilon)$-approximate APSP in $\\tilde O(n^{3-r}+n^{\\omega(r)})$ time, for any $r\\in [0,1]$. This is $O(n^{2.214})$ time using known bounds for $\\omega(r)$. It improves on the state of the art bound of $O(n^{2.25})$ by [Kavitha, Algorithmica 2012].","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.0708332931,"dev-research":0.2031111126,"prompt-eng":0.3183743665,"data-quality":0.1110665524,"ml-security":0.0881782212}}
{"text":"Our techniques further lead to improved bounds in a wide range of density for weighted graphs.","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.0747415905,"dev-research":0.2135346097,"prompt-eng":0.3605670573,"data-quality":0.1491624038,"ml-security":0.1039736414}}
{"text":"In particular, for the sparse regime we construct a distance oracle in $\\tilde O(mn^{2/3})$ time that supports $2$-approximate queries in constant time.","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.0433328982,"dev-research":0.1680640853,"prompt-eng":0.3481041176,"data-quality":0.0867252178,"ml-security":0.0909395898}}
{"text":"For sparse graphs, the preprocessing time of the algorithm matches conditional lower bounds","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.0457085407,"dev-research":0.2413885569,"prompt-eng":0.3523531062,"data-quality":0.1146009693,"ml-security":0.1221316956}}
{"text":"[Patrascu, Roditty, Thorup, FOCS 2012; Abboud, Bringmann, Fischer, STOC 2023].","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.3534733113,"dev-research":0.1994372158,"prompt-eng":0.4040220316,"data-quality":0.0975612138,"ml-security":0.1271249616}}
{"text":"To the best of our knowledge, this is the first 2-approximate distance oracle that has subquadratic preprocessing time in sparse graphs.   ","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.1553319194,"dev-research":0.2813274947,"prompt-eng":0.3509806353,"data-quality":0.113285945,"ml-security":0.0465073088}}
{"text":"We also obtain new bounds in the near additive regime for unweighted graphs.","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.086077107,"dev-research":0.1859655651,"prompt-eng":0.3207570072,"data-quality":0.095329723,"ml-security":0.1337609434}}
{"text":"We give faster algorithms for $(1+\\epsilon,k)$-approximate APSP, for $k=2,4,6,8$.   We obtain these results by incorporating fast rectangular matrix multiplications into various combinatorial algorithms that carefully balance out distance computation on layers of sparse graphs preserving certain distance information.","meta":{"url":"http://arxiv.org/abs/2307.09258v1"},"cats":{"new-dataset":0.1067291195,"dev-research":0.2201926436,"prompt-eng":0.3255117937,"data-quality":0.1109279484,"ml-security":0.0963568919}}
{"text":"Non-point spatial objects (e.g., polygons, linestrings, etc.) are ubiquitous.","meta":{"url":"http://arxiv.org/abs/2307.09256v1"},"cats":{"new-dataset":0.0711096984,"dev-research":0.2843174357,"prompt-eng":0.3752886219,"data-quality":0.1065505584,"ml-security":0.0776805872}}
{"text":"We study the problem of indexing non-point objects in memory for range queries and spatial intersection joins.","meta":{"url":"http://arxiv.org/abs/2307.09256v1"},"cats":{"new-dataset":0.2432537141,"dev-research":0.2124282246,"prompt-eng":0.3325696618,"data-quality":0.0925350182,"ml-security":0.0700924095}}
{"text":"We propose a secondary partitioning technique for space-oriented partitioning indices (e.g., grids), which improves their performance significantly, by avoiding the generation and elimination of duplicate results.","meta":{"url":"http://arxiv.org/abs/2307.09256v1"},"cats":{"new-dataset":0.0771178397,"dev-research":0.2604910538,"prompt-eng":0.3339331059,"data-quality":0.0924743422,"ml-security":0.0489045912}}
{"text":"Our approach is easy to implement and can be used by any space-partitioning index to significantly reduce the cost of range queries and intersection joins.","meta":{"url":"http://arxiv.org/abs/2307.09256v1"},"cats":{"new-dataset":0.1556418682,"dev-research":0.2434999357,"prompt-eng":0.3154985777,"data-quality":0.0628407567,"ml-security":0.0474201958}}
{"text":"In addition, the secondary partitions can be processed independently, which makes our method appropriate for distributed and parallel indexing.","meta":{"url":"http://arxiv.org/abs/2307.09256v1"},"cats":{"new-dataset":0.03323181,"dev-research":0.1889125665,"prompt-eng":0.3218657817,"data-quality":0.0737446737,"ml-security":0.0477288447}}
{"text":"Experiments on real datasets confirm the advantage of our approach against alternative duplicate elimination techniques and data-oriented state-of-the-art spatial indices.","meta":{"url":"http://arxiv.org/abs/2307.09256v1"},"cats":{"new-dataset":0.3515576207,"dev-research":0.2888665394,"prompt-eng":0.3203922518,"data-quality":0.1542905889,"ml-security":0.093985285}}
{"text":"We also show that our partitioning technique, paired with optimized partition-to-partition join algorithms, typically reduces the cost of spatial joins by around 50%.","meta":{"url":"http://arxiv.org/abs/2307.09256v1"},"cats":{"new-dataset":0.0840055785,"dev-research":0.2788126107,"prompt-eng":0.3421726837,"data-quality":0.1024737961,"ml-security":0.0585673815}}
{"text":"As the probability (and thus perplexity) of a text is calculated based on the product of the probabilities of individual tokens, it may happen that one unlikely token significantly reduces the probability (i.e., increase the perplexity) of some otherwise highly probable input, while potentially representing a simple typographical error.","meta":{"url":"http://arxiv.org/abs/2307.09255v1"},"cats":{"new-dataset":0.0070239722,"dev-research":0.3455180342,"prompt-eng":0.4790762968,"data-quality":0.5093745179,"ml-security":0.1964966564}}
{"text":"Also, given that perplexity is a scalar value that refers to the entire input, information about the probability distribution within it is lost in the calculation (a relatively good text that has one unlikely token and another text in which each token is equally likely they can have the same perplexity value), especially for longer texts.","meta":{"url":"http://arxiv.org/abs/2307.09255v1"},"cats":{"new-dataset":0.0275104322,"dev-research":0.3101480003,"prompt-eng":0.4099456919,"data-quality":0.2915732704,"ml-security":0.1108034052}}
{"text":"As an alternative to scalar perplexity this research proposes a simple algorithm used to calculate vector values based on n-gram perplexities within the input.","meta":{"url":"http://arxiv.org/abs/2307.09255v1"},"cats":{"new-dataset":0.0578184009,"dev-research":0.3038385921,"prompt-eng":0.3928475542,"data-quality":0.2393411798,"ml-security":0.0711460566}}
{"text":"Such representations consider the previously mentioned aspects, and instead of a unique value, the relative perplexity of each text token is calculated, and these values are combined into a single vector representing the input.","meta":{"url":"http://arxiv.org/abs/2307.09255v1"},"cats":{"new-dataset":0.0785289137,"dev-research":0.3311168172,"prompt-eng":0.4354847475,"data-quality":0.2047610039,"ml-security":0.0985608565}}
{"text":"Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models.","meta":{"url":"http://arxiv.org/abs/2307.09254v1"},"cats":{"new-dataset":0.0191915381,"dev-research":0.2636822946,"prompt-eng":0.4611993757,"data-quality":0.3089501759,"ml-security":0.3188633898}}
{"text":"Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts.","meta":{"url":"http://arxiv.org/abs/2307.09254v1"},"cats":{"new-dataset":0.0701108265,"dev-research":0.2768095971,"prompt-eng":0.4678240131,"data-quality":0.347853916,"ml-security":0.0873972686}}
{"text":"In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs.","meta":{"url":"http://arxiv.org/abs/2307.09254v1"},"cats":{"new-dataset":0.1032511451,"dev-research":0.2003026703,"prompt-eng":0.39360218,"data-quality":0.2764139393,"ml-security":0.3083008761}}
{"text":"Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee.","meta":{"url":"http://arxiv.org/abs/2307.09254v1"},"cats":{"new-dataset":0.090687601,"dev-research":0.2437111653,"prompt-eng":0.4178541316,"data-quality":0.2222670394,"ml-security":0.3408052876}}
{"text":"We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\\%$ on average, compared to a standard baseline method.","meta":{"url":"http://arxiv.org/abs/2307.09254v1"},"cats":{"new-dataset":0.26764629,"dev-research":0.3223201614,"prompt-eng":0.4065930954,"data-quality":0.4299753789,"ml-security":0.1079886721}}
{"text":"We establish modal correspondences between omega-catoids and convolution omega-quantales.","meta":{"url":"http://arxiv.org/abs/2307.09253v1"},"cats":{"new-dataset":0.0406068515,"dev-research":0.2204253803,"prompt-eng":0.4108773141,"data-quality":0.131201169,"ml-security":0.0559423314}}
{"text":"These are related to J\\'onsson-Tarski style-dualities between relational structures and lattices with operators.","meta":{"url":"http://arxiv.org/abs/2307.09253v1"},"cats":{"new-dataset":0.0303393867,"dev-research":0.2262994143,"prompt-eng":0.3983069855,"data-quality":0.0793258792,"ml-security":0.0982063888}}
{"text":"We introduce omega-catoids as generalisations of (strict) omega-categories and in particular of the higher path categories generated by polygraphs (or computads) in higher rewriting.","meta":{"url":"http://arxiv.org/abs/2307.09253v1"},"cats":{"new-dataset":0.0551010392,"dev-research":0.312688801,"prompt-eng":0.4159802591,"data-quality":0.1628083485,"ml-security":0.0677536127}}
{"text":"Convolution omega-quantales generalise the powerset omega-Kleene algebras recently proposed for algebraic coherence proofs in higher rewriting to weighted variants.","meta":{"url":"http://arxiv.org/abs/2307.09253v1"},"cats":{"new-dataset":0.0349981136,"dev-research":0.2758111853,"prompt-eng":0.4065630971,"data-quality":0.1453521007,"ml-security":0.1045753678}}
{"text":"We extend these correspondences to ({\\omega},p)-catoids and convolution ({\\omega},p)-quantales suitable for modelling homotopies in higher rewriting.","meta":{"url":"http://arxiv.org/abs/2307.09253v1"},"cats":{"new-dataset":0.0359350825,"dev-research":0.2768280254,"prompt-eng":0.4293743136,"data-quality":0.1186174663,"ml-security":0.0515303376}}
{"text":"We also specialise them to finitely decomposable ({\\omega}, p)-catoids, an appropriate setting for defining ({\\omega}, p)-semirings and ({\\omega}, p)-Kleene algebras.","meta":{"url":"http://arxiv.org/abs/2307.09253v1"},"cats":{"new-dataset":0.0515935413,"dev-research":0.2174404824,"prompt-eng":0.4160822765,"data-quality":0.1359856637,"ml-security":0.0729404098}}
{"text":"These constructions support the systematic development and justification of higher quantale axioms relative to a previous ad hoc approach.","meta":{"url":"http://arxiv.org/abs/2307.09253v1"},"cats":{"new-dataset":0.0238535029,"dev-research":0.4268418261,"prompt-eng":0.3770597783,"data-quality":0.0930284296,"ml-security":0.0975817165}}
{"text":"Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.1102618549,"dev-research":0.3206336897,"prompt-eng":0.4758704665,"data-quality":0.2463056998,"ml-security":0.1083276901}}
{"text":"This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.114291681,"dev-research":0.3961406637,"prompt-eng":0.4274761515,"data-quality":0.0963035402,"ml-security":0.0731460747}}
{"text":"The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.0714234825,"dev-research":0.2949468532,"prompt-eng":0.4038183655,"data-quality":0.0663759169,"ml-security":0.0790611295}}
{"text":"In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.0596407361,"dev-research":0.2560441524,"prompt-eng":0.4222796756,"data-quality":0.0919450043,"ml-security":0.0339957416}}
{"text":"UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.0762693412,"dev-research":0.2793908363,"prompt-eng":0.4260315811,"data-quality":0.0876244946,"ml-security":0.0450840945}}
{"text":"This is subsequently followed by a Transformer encoder to refine the representation.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.0371478292,"dev-research":0.2992532998,"prompt-eng":0.4694333302,"data-quality":0.1724412726,"ml-security":0.0680217239}}
{"text":"Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.039886014,"dev-research":0.3421057202,"prompt-eng":0.5801992999,"data-quality":0.0808600201,"ml-security":0.150827408}}
{"text":"In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.7610121104,"dev-research":0.2994456226,"prompt-eng":0.4041922579,"data-quality":0.0909200949,"ml-security":0.0769672688}}
{"text":"Rigorous experimental testing and analyses were performed under a myriad of scenarios to validate the effectiveness of our methodology.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.0189787805,"dev-research":0.3823222047,"prompt-eng":0.4332714116,"data-quality":0.1210225844,"ml-security":0.1636054262}}
{"text":"The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.1697327915,"dev-research":0.2245476682,"prompt-eng":0.4229473045,"data-quality":0.1288874776,"ml-security":0.0785084089}}
{"text":"This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride in the field of tabular data analysis.","meta":{"url":"http://arxiv.org/abs/2307.09249v1"},"cats":{"new-dataset":0.1133783156,"dev-research":0.3705832806,"prompt-eng":0.4275387563,"data-quality":0.2003257702,"ml-security":0.0985970965}}
{"text":"Nowadays, wind energy has drawn increasing attention as its important role in carbon neutrality and sustainable development.","meta":{"url":"http://arxiv.org/abs/2307.09248v1"},"cats":{"new-dataset":0.0333876044,"dev-research":0.3375474345,"prompt-eng":0.3232635741,"data-quality":0.0713275329,"ml-security":0.0740469655}}
{"text":"When wind power is integrated into the power grid, precise forecasting is necessary for the sustainability and security of the system.","meta":{"url":"http://arxiv.org/abs/2307.09248v1"},"cats":{"new-dataset":0.0171550027,"dev-research":0.2880682729,"prompt-eng":0.3966086924,"data-quality":0.0825782233,"ml-security":0.1351384252}}
{"text":"However, the unpredictable nature and long sequence prediction make it especially challenging.","meta":{"url":"http://arxiv.org/abs/2307.09248v1"},"cats":{"new-dataset":0.1525849097,"dev-research":0.2672550825,"prompt-eng":0.4075713056,"data-quality":0.1516830374,"ml-security":0.1922550581}}
{"text":"In this technical report, we introduce the BERT model applied for Baidu KDD Cup 2022, and the daily fluctuation is added by post-processing to make the predicted results in line with daily periodicity.","meta":{"url":"http://arxiv.org/abs/2307.09248v1"},"cats":{"new-dataset":0.2568224894,"dev-research":0.242240454,"prompt-eng":0.4127138156,"data-quality":0.1565957014,"ml-security":0.1063398691}}
{"text":"Our solution achieves 3rd place of 2490 teams.","meta":{"url":"http://arxiv.org/abs/2307.09248v1"},"cats":{"new-dataset":0.1686840661,"dev-research":0.3652020176,"prompt-eng":0.3850884876,"data-quality":0.1078056617,"ml-security":0.0896926538}}
{"text":"The code is released athttps://github.com/LongxingTan/KDD2022-Baidu","meta":{"url":"http://arxiv.org/abs/2307.09248v1"},"cats":{"new-dataset":0.3373777473,"dev-research":0.2787831971,"prompt-eng":0.3714358296,"data-quality":0.109411914,"ml-security":0.1170407885}}
{"text":"Teleoperation is vital in the construction industry, allowing safe machine manipulation from a distance.","meta":{"url":"http://arxiv.org/abs/2307.09246v1"},"cats":{"new-dataset":0.0546830009,"dev-research":0.3021827393,"prompt-eng":0.3881128472,"data-quality":0.054074867,"ml-security":0.0965848765}}
{"text":"However, controlling machines at a joint level requires extensive training due to their complex degrees of freedom.","meta":{"url":"http://arxiv.org/abs/2307.09246v1"},"cats":{"new-dataset":0.0056197797,"dev-research":0.22823088,"prompt-eng":0.3685362234,"data-quality":0.0763702632,"ml-security":0.1344970019}}
{"text":"Task space control offers intuitive maneuvering, but precise control often requires dynamic models, posing challenges for hydraulic machines.","meta":{"url":"http://arxiv.org/abs/2307.09246v1"},"cats":{"new-dataset":0.0139983828,"dev-research":0.2924486085,"prompt-eng":0.4649288947,"data-quality":0.0621608582,"ml-security":0.143476191}}
{"text":"To address this, we use a data-driven actuator model to capture machine dynamics in real-world operations.","meta":{"url":"http://arxiv.org/abs/2307.09246v1"},"cats":{"new-dataset":0.1525626812,"dev-research":0.3006749353,"prompt-eng":0.403068961,"data-quality":0.0657794695,"ml-security":0.1818898481}}
{"text":"By integrating this model into simulation and reinforcement learning, an optimal control policy for task space control is obtained.","meta":{"url":"http://arxiv.org/abs/2307.09246v1"},"cats":{"new-dataset":0.075886873,"dev-research":0.1948943522,"prompt-eng":0.4324835568,"data-quality":0.0754533682,"ml-security":0.1390383981}}
{"text":"Experiments with Brokk 170 validate the framework, comparing it to a well-known Jacobian-based approach.","meta":{"url":"http://arxiv.org/abs/2307.09246v1"},"cats":{"new-dataset":0.0345884722,"dev-research":0.2612970135,"prompt-eng":0.3911582141,"data-quality":0.0859525935,"ml-security":0.0729930307}}
{"text":"Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business.","meta":{"url":"http://arxiv.org/abs/2307.09244v1"},"cats":{"new-dataset":0.1072144597,"dev-research":0.2339141478,"prompt-eng":0.3742831955,"data-quality":0.1241853958,"ml-security":0.1017655209}}
{"text":"Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint.","meta":{"url":"http://arxiv.org/abs/2307.09244v1"},"cats":{"new-dataset":0.1793783388,"dev-research":0.2926262298,"prompt-eng":0.3992040491,"data-quality":0.0980829728,"ml-security":0.1137655289}}
{"text":"Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation.","meta":{"url":"http://arxiv.org/abs/2307.09244v1"},"cats":{"new-dataset":0.0567973094,"dev-research":0.2488781885,"prompt-eng":0.3488604758,"data-quality":0.1445598059,"ml-security":0.2008916515}}
{"text":"In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency.","meta":{"url":"http://arxiv.org/abs/2307.09244v1"},"cats":{"new-dataset":0.1584284876,"dev-research":0.2265731447,"prompt-eng":0.4082491897,"data-quality":0.3841113596,"ml-security":0.0705901281}}
{"text":"We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better represent real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09244v1"},"cats":{"new-dataset":0.3275803849,"dev-research":0.3124787181,"prompt-eng":0.4165999309,"data-quality":0.1103872631,"ml-security":0.1300960417}}
{"text":"Compared to the state-of-the-art, the proposed model has its carbon footprint reduced by more than 23% while providing on average approximately 8 percentage points in performance improvement when testing on data derived from REFIT and UK-DALE datasets.","meta":{"url":"http://arxiv.org/abs/2307.09244v1"},"cats":{"new-dataset":0.2312456653,"dev-research":0.32514136,"prompt-eng":0.3817045635,"data-quality":0.119920282,"ml-security":0.0972758774}}
{"text":"It is universally acknowledged that Wi-Fi communications are important to secure.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.0370502156,"dev-research":0.3261222759,"prompt-eng":0.413732048,"data-quality":0.1015740743,"ml-security":0.2382672455}}
{"text":"Thus, the Wi-Fi Alliance published WPA3 in 2018 with a distinctive security feature: it leverages a Password-Authenticated Key Exchange (PAKE) protocol to protect users' passwords from offline dictionary attacks.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.0588195759,"dev-research":0.3473775212,"prompt-eng":0.4123419079,"data-quality":0.0634974717,"ml-security":0.1757722176}}
{"text":"Unfortunately, soon after its release, several attacks were reported against its implementations, in response to which the protocol was updated in a best-effort manner.   ","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.12553762,"dev-research":0.3730422377,"prompt-eng":0.3814060619,"data-quality":0.1686565064,"ml-security":0.356001468}}
{"text":"In this paper, we show that the proposed mitigations are not enough, especially for a complex protocol to implement even for savvy developers.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.049001956,"dev-research":0.4333433034,"prompt-eng":0.3941396564,"data-quality":0.1122696203,"ml-security":0.4334607818}}
{"text":"Indeed, we present **Dragondoom**, a collection of side-channel vulnerabilities of varying strength allowing attackers to recover users' passwords in widely deployed Wi-Fi daemons, such as hostap in its default settings.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.2036094587,"dev-research":0.3784997381,"prompt-eng":0.4561246286,"data-quality":0.1331534783,"ml-security":0.4282411954}}
{"text":"Our findings target both password conversion methods, namely the default probabilistic hunting-and-pecking and its newly standardized deterministic alternative based on SSWU.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.0249120632,"dev-research":0.2958230432,"prompt-eng":0.5077449109,"data-quality":0.1374178959,"ml-security":0.2694228699}}
{"text":"We successfully exploit our leakage in practice through microarchitectural mechanisms, and overcome the limited spatial resolution of Flush+Reload.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.0999242493,"dev-research":0.3827966488,"prompt-eng":0.4069440428,"data-quality":0.2264138152,"ml-security":0.3438558011}}
{"text":"Our attacks outperform previous works in terms of required measurements.   ","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.0620469336,"dev-research":0.3183116372,"prompt-eng":0.3850655848,"data-quality":0.1397603659,"ml-security":0.57296398}}
{"text":"Then, driven by the need to end the spiral of patch-and-hack in Dragonfly implementations, we propose **Dragonstar**, an implementation of Dragonfly leveraging a formally verified implementation of the underlying mathematical operations, thereby removing all the related leakage vector.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.0422109373,"dev-research":0.3646973715,"prompt-eng":0.410850129,"data-quality":0.1888845379,"ml-security":0.3233079916}}
{"text":"Our implementation relies on HACL*, a formally verified crypto library guaranteeing secret-independence.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.0468599641,"dev-research":0.251221986,"prompt-eng":0.381806774,"data-quality":0.1429639735,"ml-security":0.2834313108}}
{"text":"We design Dragonstar, so that its integration within hostap requires minimal modifications to the existing project.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.065749611,"dev-research":0.3747305457,"prompt-eng":0.4165693014,"data-quality":0.0787085441,"ml-security":0.0709597158}}
{"text":"Our experiments show that the performance of HACL*-based hostap is comparable to OpenSSL-based, implying that Dragonstar is both efficient and proved to be leakage-free.","meta":{"url":"http://arxiv.org/abs/2307.09243v1"},"cats":{"new-dataset":0.0560604381,"dev-research":0.295956014,"prompt-eng":0.3728687174,"data-quality":0.0813326885,"ml-security":0.2017142429}}
{"text":"We proposed a simple yet effective morphological approach to convert a sparse Digital Elevation Model (DEM) to a dense Digital Elevation Model.","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.0467471175,"dev-research":0.1894682139,"prompt-eng":0.4322983813,"data-quality":0.107672376,"ml-security":0.0676603063}}
{"text":"The conversion is similar to that of the generation of high-resolution DEM from its low-resolution DEM.","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.0680369824,"dev-research":0.2711021064,"prompt-eng":0.4216935877,"data-quality":0.0793286071,"ml-security":0.0378811733}}
{"text":"The approach involves the generation of median contours to achieve the purpose.","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.0328497906,"dev-research":0.2551537685,"prompt-eng":0.4137469081,"data-quality":0.0846575502,"ml-security":0.0261953928}}
{"text":"It is a sequential step of the I) decomposition of the existing sparse Contour map into the maximum possible Threshold Elevation Region (TERs).","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.0583510532,"dev-research":0.2346630212,"prompt-eng":0.4072228723,"data-quality":0.0725841749,"ml-security":0.0793173448}}
{"text":"II) Computing all possible non-negative and non-weighted Median Elevation Region (MER) hierarchically between the successive TER decomposed from a sparse contour map.","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.0557343389,"dev-research":0.176533357,"prompt-eng":0.3889812561,"data-quality":0.0892418178,"ml-security":0.0441364201}}
{"text":"III)","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.2805448395,"dev-research":0.2647742684,"prompt-eng":0.4119097062,"data-quality":0.1775943945,"ml-security":0.1191262915}}
{"text":"Computing the gradient of all TER, and MER computed from previous steps would yield the predicted intermediate elevation contour at a higher spatial resolution.","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.026743535,"dev-research":0.1848736948,"prompt-eng":0.4437671976,"data-quality":0.0740834438,"ml-security":0.0426843945}}
{"text":"We presented this approach initially with some self-made synthetic data to show how the contour prediction works and then experimented with the available contour map of Washington, NH to justify its usefulness.","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.2127595949,"dev-research":0.242577759,"prompt-eng":0.3754562522,"data-quality":0.09296387,"ml-security":0.0700605337}}
{"text":"This approach considers the geometric information of existing contours and interpolates the elevation contour at a new spatial region of a topographic surface until no elevation contours are necessary to generate.","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.0431735235,"dev-research":0.2389919155,"prompt-eng":0.4042193,"data-quality":0.1071048162,"ml-security":0.0477868548}}
{"text":"This novel approach is also very low-cost and robust as it uses elevation contours.","meta":{"url":"http://arxiv.org/abs/2307.09239v1"},"cats":{"new-dataset":0.0344051491,"dev-research":0.2345495441,"prompt-eng":0.3758132673,"data-quality":0.0669218389,"ml-security":0.0558690001}}
{"text":"As collaborative robots (cobots) continue to gain popularity in industrial manufacturing, effective human-robot collaboration becomes crucial.","meta":{"url":"http://arxiv.org/abs/2307.09238v1"},"cats":{"new-dataset":0.0427600185,"dev-research":0.3729183534,"prompt-eng":0.4445706977,"data-quality":0.0748999205,"ml-security":0.0489415959}}
{"text":"Cobots should be able to recognize human actions to assist with assembly tasks and act autonomously.","meta":{"url":"http://arxiv.org/abs/2307.09238v1"},"cats":{"new-dataset":0.0466836472,"dev-research":0.2967932655,"prompt-eng":0.47366327,"data-quality":0.0917456297,"ml-security":0.0516000546}}
{"text":"To achieve this, skeleton-based approaches are often used due to their ability to generalize across various people and environments.","meta":{"url":"http://arxiv.org/abs/2307.09238v1"},"cats":{"new-dataset":0.0192735045,"dev-research":0.3646725352,"prompt-eng":0.4227046621,"data-quality":0.0669562968,"ml-security":0.0839086442}}
{"text":"Although body skeleton approaches are widely used for action recognition, they may not be accurate enough for assembly actions where the worker's fingers and hands play a significant role.","meta":{"url":"http://arxiv.org/abs/2307.09238v1"},"cats":{"new-dataset":0.027897161,"dev-research":0.28283845,"prompt-eng":0.3673559181,"data-quality":0.1084811784,"ml-security":0.0748109221}}
{"text":"To address this limitation, we propose a method in which less detailed body skeletons are combined with highly detailed hand skeletons.","meta":{"url":"http://arxiv.org/abs/2307.09238v1"},"cats":{"new-dataset":0.0728733881,"dev-research":0.2426242104,"prompt-eng":0.3603259454,"data-quality":0.0672415351,"ml-security":0.0705950277}}
{"text":"We investigate CNNs and transformers, the latter of which are particularly adept at extracting and combining important information from both skeleton types using attention.","meta":{"url":"http://arxiv.org/abs/2307.09238v1"},"cats":{"new-dataset":0.0893459478,"dev-research":0.2678742517,"prompt-eng":0.4115722107,"data-quality":0.1526241318,"ml-security":0.1037712263}}
{"text":"This paper demonstrates the effectiveness of our proposed approach in enhancing action recognition in assembly scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09238v1"},"cats":{"new-dataset":0.1232745419,"dev-research":0.2948016095,"prompt-eng":0.442113188,"data-quality":0.1306765112,"ml-security":0.0614846375}}
{"text":"Image-text contrastive models such as CLIP are useful for a variety of downstream applications including zero-shot classification, image-text retrieval and transfer learning.","meta":{"url":"http://arxiv.org/abs/2307.09233v1"},"cats":{"new-dataset":0.1330414811,"dev-research":0.1516479661,"prompt-eng":0.3851378974,"data-quality":0.2030753766,"ml-security":0.1062737322}}
{"text":"However, these contrastively trained vision-language models often fail on compositional visio-linguistic tasks such as Winoground with performance equivalent to random chance.","meta":{"url":"http://arxiv.org/abs/2307.09233v1"},"cats":{"new-dataset":0.0567998493,"dev-research":0.2237397503,"prompt-eng":0.4303132385,"data-quality":0.2612103381,"ml-security":0.1350438622}}
{"text":"In our paper, we address this issue and propose a sample-efficient light-weight method called SDS-CLIP to improve the compositional visio-linguistic reasoning capabilities of CLIP.","meta":{"url":"http://arxiv.org/abs/2307.09233v1"},"cats":{"new-dataset":0.0462509314,"dev-research":0.3385149485,"prompt-eng":0.4565803667,"data-quality":0.1816268504,"ml-security":0.0495767125}}
{"text":"The core idea of our method is to use differentiable image parameterizations to fine-tune CLIP with a distillation objective from large text-to-image generative models such as Stable-Diffusion which are relatively good at visio-linguistic reasoning tasks.","meta":{"url":"http://arxiv.org/abs/2307.09233v1"},"cats":{"new-dataset":0.0513454738,"dev-research":0.2646765577,"prompt-eng":0.4597663097,"data-quality":0.1638525964,"ml-security":0.0860647858}}
{"text":"On the challenging Winoground compositional reasoning benchmark, our method improves the absolute visio-linguistic performance of different CLIP models by up to 7%, while on the ARO dataset, our method improves the visio-linguistic performance by upto 3%.","meta":{"url":"http://arxiv.org/abs/2307.09233v1"},"cats":{"new-dataset":0.1827864064,"dev-research":0.3333866977,"prompt-eng":0.4036082851,"data-quality":0.1286220746,"ml-security":0.0740986081}}
{"text":"As a byproduct of inducing visio-linguistic reasoning into CLIP, we also find that the zero-shot performance improves marginally on a variety of downstream datasets.","meta":{"url":"http://arxiv.org/abs/2307.09233v1"},"cats":{"new-dataset":0.1860836281,"dev-research":0.3241041658,"prompt-eng":0.3949713595,"data-quality":0.2313393858,"ml-security":0.127752613}}
{"text":"Our method reinforces that carefully designed distillation objectives from generative models can be leveraged to extend existing contrastive image-text models with improved visio-linguistic reasoning capabilities.","meta":{"url":"http://arxiv.org/abs/2307.09233v1"},"cats":{"new-dataset":0.0637167608,"dev-research":0.2926034105,"prompt-eng":0.471710019,"data-quality":0.156800572,"ml-security":0.0641111837}}
{"text":"The target sensing/localization performance is fundamentally limited by the line-of-sight link and severe signal attenuation over long distances.","meta":{"url":"http://arxiv.org/abs/2307.09232v1"},"cats":{"new-dataset":0.0214572834,"dev-research":0.206405962,"prompt-eng":0.3869508098,"data-quality":0.0809597074,"ml-security":0.0942233275}}
{"text":"This paper considers a challenging scenario where the direct link between the base station (BS) and the target is blocked due to the surrounding blockages and leverages the intelligent reflecting surface (IRS) with some active sensors, termed as \\textit{semi-passive IRS}, for localization.","meta":{"url":"http://arxiv.org/abs/2307.09232v1"},"cats":{"new-dataset":0.0407009174,"dev-research":0.2453904424,"prompt-eng":0.4551073785,"data-quality":0.122942832,"ml-security":0.1364434589}}
{"text":"To be specific, the active sensors receive echo signals reflected by the target and apply signal processing techniques to estimate the target location.","meta":{"url":"http://arxiv.org/abs/2307.09232v1"},"cats":{"new-dataset":0.0451817316,"dev-research":0.2637353231,"prompt-eng":0.4424600025,"data-quality":0.1003303717,"ml-security":0.1258900887}}
{"text":"We consider the joint time-of-arrival (ToA) and direction-of-arrival (DoA) estimation for localization and derive the corresponding Cram\\'{e}r-Rao bound (CRB), and then a simple ToA/DoA estimator without iteration is proposed.","meta":{"url":"http://arxiv.org/abs/2307.09232v1"},"cats":{"new-dataset":0.1426893088,"dev-research":0.1899390059,"prompt-eng":0.4027996171,"data-quality":0.1341926337,"ml-security":0.0692423762}}
{"text":"In particular, the relationships of the CRB for ToA/DoA with the number of frames for IRS beam adjustments, number of IRS reflecting elements, and number of sensors are theoretically analyzed and demystified.","meta":{"url":"http://arxiv.org/abs/2307.09232v1"},"cats":{"new-dataset":0.1261959448,"dev-research":0.2509553614,"prompt-eng":0.4187709531,"data-quality":0.0942048214,"ml-security":0.0490339828}}
{"text":"Simulation results show that the proposed semi-passive IRS architecture provides sub-meter level positioning accuracy even over a long localization range from the BS to the target and also demonstrate a significant localization accuracy improvement compared to the fully passive IRS architecture.","meta":{"url":"http://arxiv.org/abs/2307.09232v1"},"cats":{"new-dataset":0.0585461014,"dev-research":0.2383942975,"prompt-eng":0.4152997945,"data-quality":0.1793623389,"ml-security":0.0617957405}}
{"text":"In this work we perform a scoping review of the current literature on the detection of throat cancer from speech recordings using machine learning and artificial intelligence.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.1465677886,"dev-research":0.2383547651,"prompt-eng":0.3390794539,"data-quality":0.2331067918,"ml-security":0.187668914}}
{"text":"We find 22 papers within this area and discuss their methods and results.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.2898156873,"dev-research":0.2254872025,"prompt-eng":0.3803579485,"data-quality":0.1201896161,"ml-security":0.0410846802}}
{"text":"We split these papers into two groups - nine performing binary classification, and 13 performing multi-class classification.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.3607571999,"dev-research":0.2456279842,"prompt-eng":0.4094223722,"data-quality":0.262094674,"ml-security":0.1335768414}}
{"text":"The papers present a range of methods with neural networks being most commonly implemented.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.0415840977,"dev-research":0.2222532039,"prompt-eng":0.383550422,"data-quality":0.1518361037,"ml-security":0.1483542219}}
{"text":"Many features are also extracted from the audio before classification, with the most common bring mel-frequency cepstral coefficients.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.1348883786,"dev-research":0.2735317007,"prompt-eng":0.4167589495,"data-quality":0.2364115461,"ml-security":0.074344123}}
{"text":"None of the papers found in this search have associated code repositories and as such are not reproducible.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.2944485091,"dev-research":0.2632080831,"prompt-eng":0.3327038259,"data-quality":0.2119745133,"ml-security":0.0672017439}}
{"text":"Therefore, we create a publicly available code repository of our own classifiers.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.1921207108,"dev-research":0.4203442241,"prompt-eng":0.412052206,"data-quality":0.2783561226,"ml-security":0.3257571073}}
{"text":"We use transfer learning on a multi-class problem, classifying three pathologies and healthy controls.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.0954610858,"dev-research":0.2205724802,"prompt-eng":0.3932388458,"data-quality":0.1430987979,"ml-security":0.1884158196}}
{"text":"Using this technique we achieve an unweighted average recall of 53.54%, sensitivity of 83.14%, and specificity of 64.00%.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.0352269691,"dev-research":0.2746568424,"prompt-eng":0.4943194482,"data-quality":0.2686543715,"ml-security":0.1736506169}}
{"text":"We compare our classifiers with the results obtained on the same dataset and find similar results.","meta":{"url":"http://arxiv.org/abs/2307.09230v1"},"cats":{"new-dataset":0.2682444862,"dev-research":0.3295995223,"prompt-eng":0.3981188359,"data-quality":0.3545552558,"ml-security":0.1645976074}}
{"text":"The human body DT has the potential to revolutionize healthcare and wellness, but its responsible and effective implementation requires consideration of various factors.","meta":{"url":"http://arxiv.org/abs/2307.09225v1"},"cats":{"new-dataset":0.0240286842,"dev-research":0.3528546875,"prompt-eng":0.3807041285,"data-quality":0.0448916426,"ml-security":0.0604396865}}
{"text":"This article presents a comprehensive overview of the current status and future prospects of the human body DT and proposes a five-level roadmap for its development.","meta":{"url":"http://arxiv.org/abs/2307.09225v1"},"cats":{"new-dataset":0.1097958358,"dev-research":0.297245825,"prompt-eng":0.372018564,"data-quality":0.0545398817,"ml-security":0.0706739176}}
{"text":"The roadmap covers the development of various components, such as wearable devices, data collection, data analysis, and decision-making systems.","meta":{"url":"http://arxiv.org/abs/2307.09225v1"},"cats":{"new-dataset":0.4615467819,"dev-research":0.4038485819,"prompt-eng":0.3651034435,"data-quality":0.0608971514,"ml-security":0.0896579817}}
{"text":"The article also highlights the necessary support, security, cost, and ethical considerations that must be addressed in order to ensure responsible and effective implementation of the human body DT.","meta":{"url":"http://arxiv.org/abs/2307.09225v1"},"cats":{"new-dataset":0.0491006819,"dev-research":0.3224837137,"prompt-eng":0.3623518681,"data-quality":0.0634163873,"ml-security":0.1056227554}}
{"text":"The proposed roadmap provides a framework for guiding future development and offers a unique perspective on the future of the human body DT, facilitating new interdisciplinary research and innovative solutions in this rapidly evolving field.","meta":{"url":"http://arxiv.org/abs/2307.09225v1"},"cats":{"new-dataset":0.1638123524,"dev-research":0.3739319361,"prompt-eng":0.3563337712,"data-quality":0.0473643032,"ml-security":0.0621101074}}
{"text":"As the most fundamental tasks of computer vision, object detection and segmentation have made tremendous progress in the deep learning era.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.119285003,"dev-research":0.2475656153,"prompt-eng":0.3875218758,"data-quality":0.1369209243,"ml-security":0.1489993457}}
{"text":"Due to the expensive manual labeling, the annotated categories in existing datasets are often small-scale and pre-defined, i.e., state-of-the-art detectors and segmentors fail to generalize beyond the closed-vocabulary.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.2773377446,"dev-research":0.3242411025,"prompt-eng":0.4111671417,"data-quality":0.5664349644,"ml-security":0.1449710811}}
{"text":"To resolve this limitation, the last few years have witnessed increasing attention toward Open-Vocabulary Detection (OVD) and Segmentation (OVS).","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.2117726285,"dev-research":0.2754769945,"prompt-eng":0.3764426652,"data-quality":0.2314835001,"ml-security":0.0822159575}}
{"text":"In this survey, we provide a comprehensive review on the past and recent development of OVD and OVS.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.099610514,"dev-research":0.2706556751,"prompt-eng":0.3555663881,"data-quality":0.0952361397,"ml-security":0.0413454607}}
{"text":"To this end, we develop a taxonomy according to the type of task and methodology.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.1392294149,"dev-research":0.3950109318,"prompt-eng":0.4405229939,"data-quality":0.0910715386,"ml-security":0.0551416892}}
{"text":"We find that the permission and usage of weak supervision signals can well discriminate different methodologies, including: visual-semantic space mapping, novel visual feature synthesis, region-aware training, pseudo-labeling, knowledge distillation-based, and transfer learning-based.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.1041131742,"dev-research":0.3147567764,"prompt-eng":0.4685688332,"data-quality":0.2966868014,"ml-security":0.1445117013}}
{"text":"The proposed taxonomy is universal across different tasks, covering object detection, semantic/instance/panoptic segmentation, 3D scene and video understanding.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.2529763989,"dev-research":0.2419418722,"prompt-eng":0.4003164337,"data-quality":0.1419307485,"ml-security":0.0648175667}}
{"text":"In each category, its main principles, key challenges, development routes, strengths, and weaknesses are thoroughly discussed.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.1487430914,"dev-research":0.4141047286,"prompt-eng":0.3842343237,"data-quality":0.0544164133,"ml-security":0.067365757}}
{"text":"In addition, we benchmark each task along with the vital components of each method.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.0454360587,"dev-research":0.3124367418,"prompt-eng":0.4143233649,"data-quality":0.0682948167,"ml-security":0.0449539603}}
{"text":"Finally, several promising directions are provided to stimulate future research.","meta":{"url":"http://arxiv.org/abs/2307.09220v1"},"cats":{"new-dataset":0.0214723486,"dev-research":0.340044892,"prompt-eng":0.4489919413,"data-quality":0.0709175653,"ml-security":0.0813623283}}
{"text":"We study the following problem in elementary robotics: can a mobile agent with $b$ bits of memory, which is able to sense only locations at Manhattan distance $V$ or less from itself, patrol a $d$-dimensional grid graph?","meta":{"url":"http://arxiv.org/abs/2307.09214v1"},"cats":{"new-dataset":0.098057104,"dev-research":0.1905712569,"prompt-eng":0.4002442487,"data-quality":0.075968609,"ml-security":0.1614055432}}
{"text":"We show that it is impossible to patrol some grid graphs with $0$ bits of memory, regardless of $V$, and give an exact characterization of those grid graphs that can be patrolled with $0$ bits of memory and visibility range $V$. On the other hand, we show that, surprisingly, an algorithm exists using $1$ bit of memory and $V=1$ that patrols any $d$-dimensional grid graph.","meta":{"url":"http://arxiv.org/abs/2307.09214v1"},"cats":{"new-dataset":0.119385035,"dev-research":0.2072118232,"prompt-eng":0.3471293688,"data-quality":0.1124033974,"ml-security":0.3414337997}}
{"text":"We study the size of a neural network needed to approximate the maximum function over $d$ inputs, in the most basic setting of approximating with respect to the $L_2$ norm, for continuous distributions, for a network that uses ReLU activations.","meta":{"url":"http://arxiv.org/abs/2307.09212v1"},"cats":{"new-dataset":0.0415801437,"dev-research":0.2002482856,"prompt-eng":0.3472798564,"data-quality":0.1391820843,"ml-security":0.2602279273}}
{"text":"We provide new lower and upper bounds on the width required for approximation across various depths.","meta":{"url":"http://arxiv.org/abs/2307.09212v1"},"cats":{"new-dataset":0.0762749089,"dev-research":0.207253081,"prompt-eng":0.3337183108,"data-quality":0.0928225287,"ml-security":0.0716910938}}
{"text":"Our results establish new depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as providing a depth $\\mathcal{O}(\\log(\\log(d)))$ and width $\\mathcal{O}(d)$ construction which approximates the maximum function, significantly improving upon the depth requirements of the best previously known bounds for networks with linearly-bounded width.","meta":{"url":"http://arxiv.org/abs/2307.09212v1"},"cats":{"new-dataset":0.0826668713,"dev-research":0.2370423909,"prompt-eng":0.2758511717,"data-quality":0.0839720475,"ml-security":0.1760424069}}
{"text":"Our depth separation results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over the uniform distribution, assuming an exponential upper bound on the size of the weights.","meta":{"url":"http://arxiv.org/abs/2307.09212v1"},"cats":{"new-dataset":0.0650666862,"dev-research":0.1691928449,"prompt-eng":0.3143836293,"data-quality":0.1037088695,"ml-security":0.1880568836}}
{"text":"Furthermore, we are able to use this depth 2 lower bound to provide tight bounds on the number of neurons needed to approximate the maximum by a depth 3 network.","meta":{"url":"http://arxiv.org/abs/2307.09212v1"},"cats":{"new-dataset":0.0499081092,"dev-research":0.2362324995,"prompt-eng":0.3454055246,"data-quality":0.0794157648,"ml-security":0.1473202315}}
{"text":"Our lower bounds are of potentially broad interest as they apply to the widely studied and used \\emph{max} function, in contrast to many previous results that base their bounds on specially constructed or pathological functions and distributions.","meta":{"url":"http://arxiv.org/abs/2307.09212v1"},"cats":{"new-dataset":0.1238042692,"dev-research":0.2133950281,"prompt-eng":0.3740397452,"data-quality":0.0870955196,"ml-security":0.1501382732}}
{"text":"We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD).","meta":{"url":"http://arxiv.org/abs/2307.09209v1"},"cats":{"new-dataset":0.0392550824,"dev-research":0.34296221,"prompt-eng":0.4212980134,"data-quality":0.3927481705,"ml-security":0.1806339307}}
{"text":"We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings.","meta":{"url":"http://arxiv.org/abs/2307.09209v1"},"cats":{"new-dataset":0.0370861599,"dev-research":0.3489261672,"prompt-eng":0.3948019165,"data-quality":0.2419096325,"ml-security":0.1426337565}}
{"text":"We then create the \\textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models.","meta":{"url":"http://arxiv.org/abs/2307.09209v1"},"cats":{"new-dataset":0.0976879419,"dev-research":0.347254723,"prompt-eng":0.4188707308,"data-quality":0.4266473437,"ml-security":0.1950204081}}
{"text":"Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT.","meta":{"url":"http://arxiv.org/abs/2307.09209v1"},"cats":{"new-dataset":0.1572302478,"dev-research":0.3783313663,"prompt-eng":0.3642676407,"data-quality":0.3186884972,"ml-security":0.2922747081}}
{"text":"Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.","meta":{"url":"http://arxiv.org/abs/2307.09209v1"},"cats":{"new-dataset":0.0128364907,"dev-research":0.2853423585,"prompt-eng":0.4542219134,"data-quality":0.1986209957,"ml-security":0.1585518378}}
{"text":"In autonomous navigation settings, several quantities can be subject to variations.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.0288044226,"dev-research":0.2186109037,"prompt-eng":0.4203972612,"data-quality":0.11159608,"ml-security":0.075288363}}
{"text":"Terrain properties such as friction coefficients may vary over time depending on the location of the robot.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.063876883,"dev-research":0.2870146512,"prompt-eng":0.3787790856,"data-quality":0.0538267739,"ml-security":0.0723690376}}
{"text":"Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.0195087608,"dev-research":0.294992805,"prompt-eng":0.4008432928,"data-quality":0.0680584219,"ml-security":0.1146722602}}
{"text":"An autonomous agent should thus be able to adapt to such variations.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.0146233753,"dev-research":0.2436598732,"prompt-eng":0.4485015477,"data-quality":0.0770275551,"ml-security":0.1392597307}}
{"text":"In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.1457669021,"dev-research":0.1947885623,"prompt-eng":0.4015890222,"data-quality":0.0464214204,"ml-security":0.0781193348}}
{"text":"It builds on recent advances in meta-learning forward dynamics models based on Neural Processes.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.058474189,"dev-research":0.1826663891,"prompt-eng":0.4375876947,"data-quality":0.0853383202,"ml-security":0.1902274006}}
{"text":"We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.1199388987,"dev-research":0.2549142749,"prompt-eng":0.3708064004,"data-quality":0.0582422628,"ml-security":0.0454708403}}
{"text":"In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-adaptive ablation models.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.0646934852,"dev-research":0.1626517891,"prompt-eng":0.3785994192,"data-quality":0.1067618518,"ml-security":0.0831838765}}
{"text":"We also evaluate our model on the downstream task of navigation planning, which demonstrates improved performance in planning control-efficient paths by taking robot and terrain properties into account.","meta":{"url":"http://arxiv.org/abs/2307.09206v1"},"cats":{"new-dataset":0.0422732303,"dev-research":0.2679689513,"prompt-eng":0.4448815872,"data-quality":0.0383194756,"ml-security":0.0384980474}}
{"text":"In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.0336103718,"dev-research":0.1958434443,"prompt-eng":0.4292814394,"data-quality":0.0669808237,"ml-security":0.191030372}}
{"text":"Often a task is a composition of previously learned tasks (e.g. block stacking).","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.0292947737,"dev-research":0.3539792841,"prompt-eng":0.4594149147,"data-quality":0.1100810962,"ml-security":0.1145711957}}
{"text":"These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.0256995623,"dev-research":0.2677178716,"prompt-eng":0.4322220365,"data-quality":0.110625375,"ml-security":0.1139224693}}
{"text":"Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.04587747,"dev-research":0.2975448083,"prompt-eng":0.4287138628,"data-quality":0.1104470443,"ml-security":0.0659351373}}
{"text":"On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.0031115273,"dev-research":0.3150015146,"prompt-eng":0.3646661973,"data-quality":0.1430204802,"ml-security":0.1581320971}}
{"text":"In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.0711242056,"dev-research":0.228963874,"prompt-eng":0.4101753868,"data-quality":0.0881712677,"ml-security":0.0682188259}}
{"text":"In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.0877865188,"dev-research":0.2547768716,"prompt-eng":0.4166135348,"data-quality":0.1554377064,"ml-security":0.1228377621}}
{"text":"We learn to classify them in classes and infer their latent parameters.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.0590730504,"dev-research":0.2823018289,"prompt-eng":0.4837592115,"data-quality":0.2497010671,"ml-security":0.1808926915}}
{"text":"For each class of object, we learn a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.1802621993,"dev-research":0.2796381568,"prompt-eng":0.441312115,"data-quality":0.0909529773,"ml-security":0.1442155296}}
{"text":"We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.2538833929,"dev-research":0.3400615736,"prompt-eng":0.445653273,"data-quality":0.1348303495,"ml-security":0.1607943321}}
{"text":"Through these graphs and a dynamic interaction graph that models the interactions between objects, we can learn a policy that can then be directly applied in a new environment by just estimating the interactions and latent parameters.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.1724794037,"dev-research":0.2692307424,"prompt-eng":0.4343536684,"data-quality":0.0758885366,"ml-security":0.1891903224}}
{"text":"We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks.","meta":{"url":"http://arxiv.org/abs/2307.09205v1"},"cats":{"new-dataset":0.2697884886,"dev-research":0.2256017755,"prompt-eng":0.3757347393,"data-quality":0.1532863077,"ml-security":0.1252035741}}
{"text":"Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion Rate (CVR) estimations.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0422603702,"dev-research":0.3128064813,"prompt-eng":0.4358807236,"data-quality":0.0888672505,"ml-security":0.0842672027}}
{"text":"However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0517058786,"dev-research":0.2651256185,"prompt-eng":0.3655888616,"data-quality":0.2466633099,"ml-security":0.1523169649}}
{"text":"Entire space models were proposed to address the two issues via tracing the decision-making path of \"exposure_click_purchase\".","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0704416516,"dev-research":0.3078570861,"prompt-eng":0.4473638068,"data-quality":0.0981435696,"ml-security":0.1202205897}}
{"text":"Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can better draw the user's decision-making intention and improve the recommendation performance.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0131916268,"dev-research":0.4030602931,"prompt-eng":0.426478472,"data-quality":0.0725402195,"ml-security":0.056930251}}
{"text":"Thus, the decision-making path has been extended to \"exposure_click_in-shop action_purchase\" and can be modeled with conditional probability approach.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0676446887,"dev-research":0.2513196873,"prompt-eng":0.4567097575,"data-quality":0.0845002611,"ml-security":0.0844695754}}
{"text":"Nevertheless, we observe that the chain rule of conditional probability does not always hold.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0293615959,"dev-research":0.2411290075,"prompt-eng":0.3827870305,"data-quality":0.2167099713,"ml-security":0.1113973583}}
{"text":"We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimation mathematically.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0638029392,"dev-research":0.2541093748,"prompt-eng":0.4123031428,"data-quality":0.3147159551,"ml-security":0.1420270939}}
{"text":"We propose a novel Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint (ESMC) and two alternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and Entire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0583204348,"dev-research":0.2190953379,"prompt-eng":0.4476531247,"data-quality":0.1011065368,"ml-security":0.0551991053}}
{"text":"Specifically, we handle \"exposure_click_in-shop action\" and \"in-shop action_purchase\" separately in the light of characteristics of in-shop action.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0701734787,"dev-research":0.2871268944,"prompt-eng":0.4480516238,"data-quality":0.1222763839,"ml-security":0.1110753798}}
{"text":"The first path is still treated with conditional probability while the second one is treated with parameter constraint strategy.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0082761645,"dev-research":0.1818574703,"prompt-eng":0.4388288661,"data-quality":0.0947624922,"ml-security":0.059345671}}
{"text":"Experiments on both offline and online environments in a large-scale recommendation system illustrate the superiority of our proposed methods over state-of-the-art models.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.0743690083,"dev-research":0.2718962806,"prompt-eng":0.3889511676,"data-quality":0.0756079859,"ml-security":0.0702934026}}
{"text":"The real-world datasets will be released.","meta":{"url":"http://arxiv.org/abs/2307.09193v1"},"cats":{"new-dataset":0.9419373189,"dev-research":0.3075494358,"prompt-eng":0.3281285686,"data-quality":0.1330638743,"ml-security":0.2231241577}}
{"text":"In its 14 years, distributed ledger technology has attracted increasing attention, investments, enthusiasm, and user base.","meta":{"url":"http://arxiv.org/abs/2307.09188v1"},"cats":{"new-dataset":0.0742896085,"dev-research":0.3087632258,"prompt-eng":0.3755291516,"data-quality":0.0528273873,"ml-security":0.070003618}}
{"text":"However, ongoing doubts about its usefulness and recent losses of trust in prominent cryptocurrencies have fueled deeply skeptical assessments.","meta":{"url":"http://arxiv.org/abs/2307.09188v1"},"cats":{"new-dataset":0.0372302524,"dev-research":0.3252636131,"prompt-eng":0.4189040537,"data-quality":0.2184746155,"ml-security":0.2416487128}}
{"text":"Multiple groups attempted to disentangle the technology from the associated hype and controversy by building workflows for rapid prototyping and informed decision-making, but their mostly isolated work leaves users only with fewer unclarities.","meta":{"url":"http://arxiv.org/abs/2307.09188v1"},"cats":{"new-dataset":0.0417694732,"dev-research":0.5352106003,"prompt-eng":0.4145366689,"data-quality":0.0743526423,"ml-security":0.101349508}}
{"text":"To bridge the gaps between these contributions, we develop a holistic analytical framework and open-source web tool for making evidence-based decisions.","meta":{"url":"http://arxiv.org/abs/2307.09188v1"},"cats":{"new-dataset":0.1264710157,"dev-research":0.4621755211,"prompt-eng":0.354843289,"data-quality":0.1128649072,"ml-security":0.0968414965}}
{"text":"Consisting of three stages - evaluation, elicitation, and design - the framework relies on input from the users' domain knowledge, maps their choices, and provides an output of needed technology bundles.","meta":{"url":"http://arxiv.org/abs/2307.09188v1"},"cats":{"new-dataset":0.1488796237,"dev-research":0.4345823108,"prompt-eng":0.4754274776,"data-quality":0.0723778438,"ml-security":0.0551151583}}
{"text":"We apply it to an example clinical use case to clarify the directions of our contribution charts for prototyping, hopefully driving the conversation towards ways to enhance further tools and approaches.","meta":{"url":"http://arxiv.org/abs/2307.09188v1"},"cats":{"new-dataset":0.0986947368,"dev-research":0.4721580941,"prompt-eng":0.4553993829,"data-quality":0.0880028285,"ml-security":0.0743283165}}
{"text":"Chest X-ray (CXR) anatomical abnormality detection aims at localizing and characterising cardiopulmonary radiological findings in the radiographs, which can expedite clinical workflow and reduce observational oversights.","meta":{"url":"http://arxiv.org/abs/2307.09184v1"},"cats":{"new-dataset":0.0262048152,"dev-research":0.2796877878,"prompt-eng":0.4072096991,"data-quality":0.1961588087,"ml-security":0.0851257392}}
{"text":"Most existing methods attempted this task in either fully supervised settings which demanded costly mass per-abnormality annotations, or weakly supervised settings which still lagged badly behind fully supervised methods in performance.","meta":{"url":"http://arxiv.org/abs/2307.09184v1"},"cats":{"new-dataset":0.0418568102,"dev-research":0.2627988614,"prompt-eng":0.4556780895,"data-quality":0.4913464258,"ml-security":0.1840131945}}
{"text":"In this work, we propose a co-evolutionary image and report distillation (CEIRD) framework, which approaches semi-supervised abnormality detection in CXR by grounding the visual detection results with text-classified abnormalities from paired radiology reports, and vice versa.","meta":{"url":"http://arxiv.org/abs/2307.09184v1"},"cats":{"new-dataset":0.2223221645,"dev-research":0.3031338274,"prompt-eng":0.4536989141,"data-quality":0.4621638777,"ml-security":0.1137277027}}
{"text":"Concretely, based on the classical teacher-student pseudo label distillation (TSD) paradigm, we additionally introduce an auxiliary report classification model, whose prediction is used for report-guided pseudo detection label refinement (RPDLR) in the primary vision detection task.","meta":{"url":"http://arxiv.org/abs/2307.09184v1"},"cats":{"new-dataset":0.1097155231,"dev-research":0.2715481698,"prompt-eng":0.4891713155,"data-quality":0.5941603699,"ml-security":0.1266039644}}
{"text":"Inversely, we also use the prediction of the vision detection model for abnormality-guided pseudo classification label refinement (APCLR) in the auxiliary report classification task, and propose a co-evolution strategy where the vision and report models mutually promote each other with RPDLR and APCLR performed alternatively.","meta":{"url":"http://arxiv.org/abs/2307.09184v1"},"cats":{"new-dataset":0.0831900288,"dev-research":0.2726032166,"prompt-eng":0.5001986622,"data-quality":0.5498941424,"ml-security":0.1129181403}}
{"text":"To this end, we effectively incorporate the weak supervision by reports into the semi-supervised TSD pipeline.","meta":{"url":"http://arxiv.org/abs/2307.09184v1"},"cats":{"new-dataset":0.1361069522,"dev-research":0.2821703595,"prompt-eng":0.505753147,"data-quality":0.283153105,"ml-security":0.0805288597}}
{"text":"Besides the cross-modal pseudo label refinement, we further propose an intra-image-modal self-adaptive non-maximum suppression, where the pseudo detection labels generated by the teacher vision model are dynamically rectified by high-confidence predictions by the student.","meta":{"url":"http://arxiv.org/abs/2307.09184v1"},"cats":{"new-dataset":0.0600703304,"dev-research":0.1849799828,"prompt-eng":0.4776258542,"data-quality":0.4284553044,"ml-security":0.1861937861}}
{"text":"Experimental results on the public MIMIC-CXR benchmark demonstrate CEIRD's superior performance to several up-to-date weakly and semi-supervised methods.","meta":{"url":"http://arxiv.org/abs/2307.09184v1"},"cats":{"new-dataset":0.0878154975,"dev-research":0.2718308014,"prompt-eng":0.4792664737,"data-quality":0.3046354515,"ml-security":0.1114148422}}
{"text":"Graph convolutional networks (GCN) is widely used to handle irregular data since it updates node features by using the structure information of graph.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.1023200514,"dev-research":0.2309394528,"prompt-eng":0.3015238119,"data-quality":0.2372333742,"ml-security":0.1206188961}}
{"text":"With the help of iterated GCN, high-order information can be obtained to further enhance the representation of nodes.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.0300055017,"dev-research":0.1888885957,"prompt-eng":0.4142657391,"data-quality":0.0759364118,"ml-security":0.0672579232}}
{"text":"However, how to apply GCN to structured data (such as pictures) has not been deeply studied.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.125039623,"dev-research":0.2052537429,"prompt-eng":0.3482832161,"data-quality":0.1636725507,"ml-security":0.0636588651}}
{"text":"In this paper, we explore the application of graph attention networks (GAT) in image feature extraction.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.0708409576,"dev-research":0.2300284084,"prompt-eng":0.3913941729,"data-quality":0.2281526283,"ml-security":0.0863174425}}
{"text":"First of all, we propose a novel graph generation algorithm to convert images into graphs through matrix transformation.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.1035776757,"dev-research":0.2401919163,"prompt-eng":0.3942854481,"data-quality":0.0979771434,"ml-security":0.0451156253}}
{"text":"It is one magnitude faster than the algorithm based on K Nearest Neighbors (KNN).","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.0934149401,"dev-research":0.2704371176,"prompt-eng":0.3125750577,"data-quality":0.082174368,"ml-security":0.0685143303}}
{"text":"Then, GAT is used on the generated graph to update the node features.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.0390501213,"dev-research":0.317408418,"prompt-eng":0.4035876506,"data-quality":0.1410709897,"ml-security":0.0581965252}}
{"text":"Thus, a more robust representation is obtained.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.0924490843,"dev-research":0.2864642096,"prompt-eng":0.4289909271,"data-quality":0.2787778891,"ml-security":0.1490757308}}
{"text":"These two steps are combined into a module called pixel-wise graph attention module (PGA).","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.1899458295,"dev-research":0.2798627275,"prompt-eng":0.4471063815,"data-quality":0.1409673703,"ml-security":0.0304556901}}
{"text":"Since the graph obtained by our graph generation algorithm can still be transformed into a picture after processing, PGA can be well combined with CNN.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.1812750945,"dev-research":0.2490718621,"prompt-eng":0.3832021069,"data-quality":0.1314823196,"ml-security":0.048795283}}
{"text":"Based on these two modules, we consulted the ResNet and design a pixel-wise graph attention network (PGANet).","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.3462439567,"dev-research":0.2453749525,"prompt-eng":0.4048918496,"data-quality":0.1524996251,"ml-security":0.05301988}}
{"text":"The PGANet is applied to the task of person re-identification in the datasets Market1501, DukeMTMC-reID and Occluded-DukeMTMC (outperforms state-of-the-art by 0.8\\%, 1.1\\% and 11\\% respectively, in mAP scores).","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.2520859872,"dev-research":0.2385539111,"prompt-eng":0.3726599917,"data-quality":0.2356409036,"ml-security":0.1093354206}}
{"text":"Experiment results show that it achieves the state-of-the-art performance.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.017981665,"dev-research":0.276496601,"prompt-eng":0.4151242572,"data-quality":0.0700986646,"ml-security":0.052754973}}
{"text":"\\href{https://github.com/wenyu1009/PGANet}{The code is available here}.","meta":{"url":"http://arxiv.org/abs/2307.09183v1"},"cats":{"new-dataset":0.0508703099,"dev-research":0.2869229321,"prompt-eng":0.3669612618,"data-quality":0.1618660103,"ml-security":0.075384056}}
{"text":"With an increasing number of smart devices like internet of things (IoT) devices deployed in the field, offloadingtraining of neural networks (NNs) to a central server becomes more and more infeasible.","meta":{"url":"http://arxiv.org/abs/2307.09182v1"},"cats":{"new-dataset":0.0273492809,"dev-research":0.2605832115,"prompt-eng":0.3589542832,"data-quality":0.0937587447,"ml-security":0.2599139553}}
{"text":"Recent efforts toimprove users' privacy have led to on-device learning emerging as an alternative.","meta":{"url":"http://arxiv.org/abs/2307.09182v1"},"cats":{"new-dataset":0.1271156599,"dev-research":0.390254542,"prompt-eng":0.4399379882,"data-quality":0.0952441398,"ml-security":0.4884183811}}
{"text":"However, a model trainedonly on a single device, using only local data, is unlikely to reach a high accuracy.","meta":{"url":"http://arxiv.org/abs/2307.09182v1"},"cats":{"new-dataset":0.0261899112,"dev-research":0.2453230037,"prompt-eng":0.4030460602,"data-quality":0.2077920654,"ml-security":0.206757941}}
{"text":"Federated learning (FL)has been introduced as a solution, offering a privacy-preserving trade-off between communication overheadand model accuracy by sharing knowledge between devices but disclosing the devices' private data.","meta":{"url":"http://arxiv.org/abs/2307.09182v1"},"cats":{"new-dataset":0.1375603207,"dev-research":0.2563192372,"prompt-eng":0.380082375,"data-quality":0.1198384286,"ml-security":0.3958693137}}
{"text":"Theapplicability and the benefit of applying baseline FL are, however, limited in many relevant use cases dueto the heterogeneity present in such environments.","meta":{"url":"http://arxiv.org/abs/2307.09182v1"},"cats":{"new-dataset":0.0199780222,"dev-research":0.2514372021,"prompt-eng":0.3504958474,"data-quality":0.08366999,"ml-security":0.086331693}}
{"text":"In this survey, we outline the heterogeneity challengesFL has to overcome to be widely applicable in real-world applications.","meta":{"url":"http://arxiv.org/abs/2307.09182v1"},"cats":{"new-dataset":0.060845643,"dev-research":0.2433556274,"prompt-eng":0.3528909184,"data-quality":0.117461477,"ml-security":0.1116609565}}
{"text":"We especially focus on the aspect ofcomputation heterogeneity among the participating devices and provide a comprehensive overview of recentworks on heterogeneity-aware FL.","meta":{"url":"http://arxiv.org/abs/2307.09182v1"},"cats":{"new-dataset":0.0503132404,"dev-research":0.3357663183,"prompt-eng":0.4010521353,"data-quality":0.0988094254,"ml-security":0.0849145739}}
{"text":"We discuss two groups: works that adapt the NN architecture and worksthat approach heterogeneity on a system level, covering Federated Averaging (FedAvg), distillation, and splitlearning-based approaches, as well as synchronous and asynchronous aggregation schemes.","meta":{"url":"http://arxiv.org/abs/2307.09182v1"},"cats":{"new-dataset":0.0202879826,"dev-research":0.2037454686,"prompt-eng":0.3319593695,"data-quality":0.0810900308,"ml-security":0.0710092233}}
{"text":"Participating in the shared task \"Image Retrieval for arguments\", we used different pipelines for image retrieval containing Image Generation, Stance Detection, Preselection and Feature Matching.","meta":{"url":"http://arxiv.org/abs/2307.09172v1"},"cats":{"new-dataset":0.1969800326,"dev-research":0.2605688286,"prompt-eng":0.4428868248,"data-quality":0.1108204274,"ml-security":0.044858842}}
{"text":"We submitted four different runs with different pipeline layout and compare them to given baseline.","meta":{"url":"http://arxiv.org/abs/2307.09172v1"},"cats":{"new-dataset":0.1208650294,"dev-research":0.2837529495,"prompt-eng":0.3975512849,"data-quality":0.0998062062,"ml-security":0.0334222143}}
{"text":"Our pipelines perform similarly to the baseline.","meta":{"url":"http://arxiv.org/abs/2307.09172v1"},"cats":{"new-dataset":0.0554944163,"dev-research":0.2999455775,"prompt-eng":0.4186423508,"data-quality":0.1141025248,"ml-security":0.0431397539}}
{"text":"Efficiency and trustworthiness are two eternal pursuits when applying deep learning in real-world applications.","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.0174720413,"dev-research":0.3275090115,"prompt-eng":0.3774039234,"data-quality":0.1473982856,"ml-security":0.2982134824}}
{"text":"With regard to efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling the large dataset into a tiny synthetic dataset.","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.1880862076,"dev-research":0.2907416585,"prompt-eng":0.3537833219,"data-quality":0.1681245886,"ml-security":0.1912067769}}
{"text":"However, existing methods merely concentrate on in-distribution (InD) classification in a closed-world setting, disregarding out-of-distribution (OOD) samples.","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.130139776,"dev-research":0.2134122705,"prompt-eng":0.3742986213,"data-quality":0.3307519134,"ml-security":0.2223076238}}
{"text":"On the other hand, OOD detection aims to enhance models' trustworthiness, which is always inefficiently achieved in full-data settings.","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.0220224491,"dev-research":0.2758852996,"prompt-eng":0.3753837369,"data-quality":0.1876100791,"ml-security":0.2664587147}}
{"text":"For the first time, we simultaneously consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD).","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.3124652452,"dev-research":0.3329324864,"prompt-eng":0.3765855499,"data-quality":0.338832154,"ml-security":0.3676584963}}
{"text":"By distilling both InD samples and outliers, the condensed datasets are capable to train models competent in both InD classification and OOD detection.","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.1635849186,"dev-research":0.2423911509,"prompt-eng":0.3601777345,"data-quality":0.2969627649,"ml-security":0.2685061782}}
{"text":"To alleviate the requirement of real outlier data and make OOD detection more practical, we further propose to corrupt InD samples to generate pseudo-outliers and introduce Pseudo-Outlier Exposure (POE).","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.3932837051,"dev-research":0.3377021346,"prompt-eng":0.3864417813,"data-quality":0.4689781464,"ml-security":0.4578013904}}
{"text":"Comprehensive experiments on various settings demonstrate the effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art method Outlier Exposure (OE).","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.0873225711,"dev-research":0.417378298,"prompt-eng":0.4369271545,"data-quality":0.2981709592,"ml-security":0.452147794}}
{"text":"Compared with the preceding DD, TrustDD is more trustworthy and applicable to real open-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.045246864,"dev-research":0.354389687,"prompt-eng":0.3930777649,"data-quality":0.1737619641,"ml-security":0.2776107126}}
{"text":"Our code will be publicly available.","meta":{"url":"http://arxiv.org/abs/2307.09165v1"},"cats":{"new-dataset":0.4127009012,"dev-research":0.3659746378,"prompt-eng":0.452786506,"data-quality":0.1281158737,"ml-security":0.2437899296}}
{"text":"Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.1814895419,"dev-research":0.4060644053,"prompt-eng":0.4398019124,"data-quality":0.0944554256,"ml-security":0.0908272204}}
{"text":"However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0318200788,"dev-research":0.3845877358,"prompt-eng":0.4582567384,"data-quality":0.2375327708,"ml-security":0.1478111724}}
{"text":"The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0262947077,"dev-research":0.3565377641,"prompt-eng":0.4369387278,"data-quality":0.1982639043,"ml-security":0.1926933926}}
{"text":"Supervised type inference approaches, while feature-agnostic, require large, high-quality annotated datasets and are limited to pre-defined types.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.1481193356,"dev-research":0.3323458276,"prompt-eng":0.4144913438,"data-quality":0.2551718109,"ml-security":0.1242308786}}
{"text":"As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0556083816,"dev-research":0.2647591483,"prompt-eng":0.3699916833,"data-quality":0.202542513,"ml-security":0.1367289887}}
{"text":"However, their performance is limited.   ","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0272427283,"dev-research":0.2165662345,"prompt-eng":0.3237590316,"data-quality":0.0838075201,"ml-security":0.0632560065}}
{"text":"This paper introduces TypeGen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.1067142092,"dev-research":0.3251907053,"prompt-eng":0.4208985421,"data-quality":0.1670273507,"ml-security":0.0889814899}}
{"text":"TypeGen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0431414592,"dev-research":0.4367599646,"prompt-eng":0.5004302703,"data-quality":0.1350736311,"ml-security":0.0735758093}}
{"text":"By combining COT prompts with code slices and type hints, TypeGen constructs example prompts from human annotations.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0758639527,"dev-research":0.4329861383,"prompt-eng":0.5922055102,"data-quality":0.2496818198,"ml-security":0.110955938}}
{"text":"TypeGen only requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0810192998,"dev-research":0.3149765817,"prompt-eng":0.5578509724,"data-quality":0.1967170838,"ml-security":0.0937537599}}
{"text":"Moreover, TypeGen enhances the interpretability of results through the use of the input-explanation-output strategy.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0129658975,"dev-research":0.5091617058,"prompt-eng":0.4663030516,"data-quality":0.2388715082,"ml-security":0.0817804764}}
{"text":"Experiments show that TypeGen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5% in return value type prediction in terms of top-1 Exact Match by using only five examples.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.077474524,"dev-research":0.3175064721,"prompt-eng":0.4239120089,"data-quality":0.1697552861,"ml-security":0.0768571094}}
{"text":"Furthermore, TypeGen achieves substantial improvements of 27% to 84% compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-1 Exact Match.","meta":{"url":"http://arxiv.org/abs/2307.09163v1"},"cats":{"new-dataset":0.0904176532,"dev-research":0.2780346967,"prompt-eng":0.4274519836,"data-quality":0.1833400664,"ml-security":0.0639344968}}
{"text":"Gender bias in artificial intelligence (AI) and natural language processing has garnered significant attention due to its potential impact on societal perceptions and biases.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.0351107034,"dev-research":0.3720765612,"prompt-eng":0.3967426249,"data-quality":0.2400429496,"ml-security":0.1691266194}}
{"text":"This research paper aims to analyze gender bias in Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2 and GPT-3.5, some prominent language models, to better understand its implications.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.0403778897,"dev-research":0.2338475893,"prompt-eng":0.4483701622,"data-quality":0.1858873207,"ml-security":0.0959291055}}
{"text":"Through a comprehensive literature review, the study examines existing research on gender bias in AI language models and identifies gaps in the current knowledge.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.037659924,"dev-research":0.328355179,"prompt-eng":0.3893046424,"data-quality":0.24177693,"ml-security":0.1679337676}}
{"text":"The methodology involves collecting and preprocessing data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis techniques to evaluate gender bias in the generated text.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.1147134155,"dev-research":0.3409240362,"prompt-eng":0.4137493223,"data-quality":0.1812511147,"ml-security":0.0831627582}}
{"text":"The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.0999393434,"dev-research":0.3015157602,"prompt-eng":0.4272042574,"data-quality":0.2303014097,"ml-security":0.1115143337}}
{"text":"The discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.0395173665,"dev-research":0.3480719341,"prompt-eng":0.339931419,"data-quality":0.1973131797,"ml-security":0.1415703896}}
{"text":"Additionally, the paper presents strategies for reducing gender bias in LLMs, including algorithmic approaches and data augmentation techniques.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.0368443375,"dev-research":0.2245251073,"prompt-eng":0.4772301003,"data-quality":0.2014446799,"ml-security":0.1401844411}}
{"text":"The research highlights the importance of interdisciplinary collaborations and the role of sociological studies in mitigating gender bias in AI models.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.0481820723,"dev-research":0.3674015689,"prompt-eng":0.3291099543,"data-quality":0.1215514888,"ml-security":0.1439966549}}
{"text":"By addressing these issues, we can pave the way for more inclusive and unbiased AI systems that have a positive impact on society.","meta":{"url":"http://arxiv.org/abs/2307.09162v1"},"cats":{"new-dataset":0.0513604592,"dev-research":0.3336788727,"prompt-eng":0.3625867167,"data-quality":0.1383498886,"ml-security":0.2212255167}}
{"text":"Online segmentation of laser-induced damage on large-aperture optics in high-power laser facilities is challenged by complicated damage morphology, uneven illumination and stray light interference.","meta":{"url":"http://arxiv.org/abs/2307.09161v1"},"cats":{"new-dataset":0.0639669429,"dev-research":0.2639221175,"prompt-eng":0.3943012457,"data-quality":0.2008845746,"ml-security":0.1965105684}}
{"text":"Fully supervised semantic segmentation algorithms have achieved state-of-the-art performance, but rely on plenty of pixel-level labels, which are time-consuming and labor-consuming to produce.","meta":{"url":"http://arxiv.org/abs/2307.09161v1"},"cats":{"new-dataset":0.1608186036,"dev-research":0.2159028547,"prompt-eng":0.4311941361,"data-quality":0.2700594546,"ml-security":0.100214047}}
{"text":"LayerCAM, an advanced weakly supervised semantic segmentation algorithm, can generate pixel-accurate results using only image-level labels, but its scattered and partially under-activated class activation regions degrade segmentation performance.","meta":{"url":"http://arxiv.org/abs/2307.09161v1"},"cats":{"new-dataset":0.1031364003,"dev-research":0.2879822294,"prompt-eng":0.4191678185,"data-quality":0.2899494005,"ml-security":0.1361167049}}
{"text":"In this paper, we propose a weakly supervised semantic segmentation method with Continuous Gradient CAM and its nonlinear multi-scale fusion (CG-fusion CAM).","meta":{"url":"http://arxiv.org/abs/2307.09161v1"},"cats":{"new-dataset":0.1836609609,"dev-research":0.219728677,"prompt-eng":0.3906539758,"data-quality":0.271208949,"ml-security":0.0837195522}}
{"text":"The method redesigns the way of back-propagating gradients and non-linearly activates the multi-scale fused heatmaps to generate more fine-grained class activation maps with appropriate activation degree for different sizes of damage sites.","meta":{"url":"http://arxiv.org/abs/2307.09161v1"},"cats":{"new-dataset":0.0342623035,"dev-research":0.3101833693,"prompt-eng":0.4301597176,"data-quality":0.1581147233,"ml-security":0.319929269}}
{"text":"Experiments on our dataset show that the proposed method can achieve segmentation performance comparable to that of fully supervised algorithms.","meta":{"url":"http://arxiv.org/abs/2307.09161v1"},"cats":{"new-dataset":0.1440656262,"dev-research":0.2207584487,"prompt-eng":0.4026633008,"data-quality":0.2422768844,"ml-security":0.1302352266}}
{"text":"Learning-based multi-view stereo (MVS) methods deal with predicting accurate depth maps to achieve an accurate and complete 3D representation.","meta":{"url":"http://arxiv.org/abs/2307.09160v1"},"cats":{"new-dataset":0.0706643113,"dev-research":0.2262880915,"prompt-eng":0.3724565597,"data-quality":0.117290314,"ml-security":0.0590608301}}
{"text":"Despite the excellent performance, existing methods ignore the fact that a suitable depth geometry is also critical in MVS.","meta":{"url":"http://arxiv.org/abs/2307.09160v1"},"cats":{"new-dataset":0.0145650435,"dev-research":0.269837803,"prompt-eng":0.3521826502,"data-quality":0.1146732251,"ml-security":0.0774736387}}
{"text":"In this paper, we demonstrate that different depth geometries have significant performance gaps, even using the same depth prediction error.","meta":{"url":"http://arxiv.org/abs/2307.09160v1"},"cats":{"new-dataset":0.0639510512,"dev-research":0.2165991021,"prompt-eng":0.3224987954,"data-quality":0.0976094518,"ml-security":0.0610451661}}
{"text":"Therefore, we introduce an ideal depth geometry composed of Saddle-Shaped Cells, whose predicted depth map oscillates upward and downward around the ground-truth surface, rather than maintaining a continuous and smooth depth plane.","meta":{"url":"http://arxiv.org/abs/2307.09160v1"},"cats":{"new-dataset":0.0826001819,"dev-research":0.2552117104,"prompt-eng":0.3444728161,"data-quality":0.0585823959,"ml-security":0.1202220044}}
{"text":"To achieve it, we develop a coarse-to-fine framework called Dual-MVSNet (DMVSNet), which can produce an oscillating depth plane.","meta":{"url":"http://arxiv.org/abs/2307.09160v1"},"cats":{"new-dataset":0.0993883609,"dev-research":0.2338283966,"prompt-eng":0.3807483813,"data-quality":0.0603002269,"ml-security":0.1061443976}}
{"text":"Technically, we predict two depth values for each pixel (Dual-Depth), and propose a novel loss function and a checkerboard-shaped selecting strategy to constrain the predicted depth geometry.","meta":{"url":"http://arxiv.org/abs/2307.09160v1"},"cats":{"new-dataset":0.0665806217,"dev-research":0.2175498769,"prompt-eng":0.3956593302,"data-quality":0.083255334,"ml-security":0.0867810683}}
{"text":"Compared to existing methods,DMVSNet achieves a high rank on the DTU benchmark and obtains the top performance on challenging scenes of Tanks and Temples, demonstrating its strong performance and generalization ability.","meta":{"url":"http://arxiv.org/abs/2307.09160v1"},"cats":{"new-dataset":0.1018793392,"dev-research":0.2697214915,"prompt-eng":0.4154755574,"data-quality":0.1078440817,"ml-security":0.1121140624}}
{"text":"Our method also points to a new research direction for considering depth geometry in MVS.","meta":{"url":"http://arxiv.org/abs/2307.09160v1"},"cats":{"new-dataset":0.0370940147,"dev-research":0.2533325089,"prompt-eng":0.3695844276,"data-quality":0.0693736638,"ml-security":0.041685596}}
{"text":"We tackle the problem of novel class discovery, which aims to learn novel classes without supervision based on labeled data from known classes.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.3994118489,"dev-research":0.2803441861,"prompt-eng":0.4028076272,"data-quality":0.4039007223,"ml-security":0.3275355367}}
{"text":"A key challenge lies in transferring the knowledge in the known-class data to the learning of novel classes.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.2771764219,"dev-research":0.3125519171,"prompt-eng":0.383696905,"data-quality":0.2265618528,"ml-security":0.2632137891}}
{"text":"Previous methods mainly focus on building a shared representation space for knowledge transfer and often ignore modeling class relations.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.066270573,"dev-research":0.3272946953,"prompt-eng":0.4234137699,"data-quality":0.2102315632,"ml-security":0.1182003306}}
{"text":"To address this, we introduce a class relation representation for the novel classes based on the predicted class distribution of a model trained on known classes.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.2166283593,"dev-research":0.2401869644,"prompt-eng":0.4470262775,"data-quality":0.2266320433,"ml-security":0.3124679629}}
{"text":"Empirically, we find that such class relation becomes less informative during typical discovery training.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.0611692531,"dev-research":0.3332410367,"prompt-eng":0.4107304906,"data-quality":0.2318863778,"ml-security":0.217224707}}
{"text":"To prevent such information loss, we propose a novel knowledge distillation framework, which utilizes our class-relation representation to regularize the learning of novel classes.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.0868409175,"dev-research":0.3108153079,"prompt-eng":0.4178121329,"data-quality":0.2338942357,"ml-security":0.2016077782}}
{"text":"In addition, to enable a flexible knowledge distillation scheme for each data point in novel classes, we develop a learnable weighting function for the regularization, which adaptively promotes knowledge transfer based on the semantic similarity between the novel and known classes.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.058851739,"dev-research":0.2960743764,"prompt-eng":0.413015725,"data-quality":0.2705381,"ml-security":0.1816701495}}
{"text":"To validate the effectiveness and generalization of our method, we conduct extensive experiments on multiple benchmarks, including CIFAR100, Stanford Cars, CUB, and FGVC-Aircraft datasets.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.2100326779,"dev-research":0.2646067242,"prompt-eng":0.381003943,"data-quality":0.1436541368,"ml-security":0.094286095}}
{"text":"Our results demonstrate that the proposed method outperforms the previous state-of-the-art methods by a significant margin on almost all benchmarks.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.0377155989,"dev-research":0.2878274775,"prompt-eng":0.3716790162,"data-quality":0.1836188269,"ml-security":0.0461646015}}
{"text":"Code is available at \\href{https://github.com/kleinzcy/Cr-KD-NCD}{here}.","meta":{"url":"http://arxiv.org/abs/2307.09158v1"},"cats":{"new-dataset":0.1128197521,"dev-research":0.2681499599,"prompt-eng":0.4055866285,"data-quality":0.1464417452,"ml-security":0.0646552386}}
{"text":"Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.1348482107,"dev-research":0.4272064479,"prompt-eng":0.3851061913,"data-quality":0.0613341407,"ml-security":0.0508895048}}
{"text":"Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.2413299056,"dev-research":0.3878768733,"prompt-eng":0.3734707647,"data-quality":0.0777981022,"ml-security":0.0692781688}}
{"text":"However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.2188170486,"dev-research":0.3457821053,"prompt-eng":0.3926128688,"data-quality":0.0468564142,"ml-security":0.0463274671}}
{"text":"In this work, we leverage the richness of AR research and apply it to situated visualization.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.268864276,"dev-research":0.3849851937,"prompt-eng":0.3779718259,"data-quality":0.0557915767,"ml-security":0.0551816845}}
{"text":"We derive design patterns which summarize common approaches of visualizing data in situ.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.6164835736,"dev-research":0.4658415655,"prompt-eng":0.406490372,"data-quality":0.07849084,"ml-security":0.0677165474}}
{"text":"The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.3807013213,"dev-research":0.3660625142,"prompt-eng":0.412742784,"data-quality":0.0505849043,"ml-security":0.044213093}}
{"text":"We discuss design dimensions that help to describe both our patterns and previous work in the literature.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.162537175,"dev-research":0.367991193,"prompt-eng":0.4259173142,"data-quality":0.054743867,"ml-security":0.0653390424}}
{"text":"This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.062855639,"dev-research":0.2855228648,"prompt-eng":0.3884721573,"data-quality":0.097796658,"ml-security":0.099423996}}
{"text":"We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.","meta":{"url":"http://arxiv.org/abs/2307.09157v1"},"cats":{"new-dataset":0.1413241755,"dev-research":0.4512701226,"prompt-eng":0.400858223,"data-quality":0.0545460495,"ml-security":0.0466975365}}
{"text":"In this paper, necessary and sufficient conditions for the reversibility of a cyclic code of arbitrary length over a finite commutative chain ring have been derived.","meta":{"url":"http://arxiv.org/abs/2307.09156v1"},"cats":{"new-dataset":0.0231800065,"dev-research":0.2775246243,"prompt-eng":0.3694939158,"data-quality":0.1230537514,"ml-security":0.0797507451}}
{"text":"MDS reversible cyclic codes having length p^s over a finite chain ring with nilpotency index 2 have been characterized and a few examples of MDS reversible cyclic codes have been presented.","meta":{"url":"http://arxiv.org/abs/2307.09156v1"},"cats":{"new-dataset":0.0427078091,"dev-research":0.2367041354,"prompt-eng":0.3777677735,"data-quality":0.1163738999,"ml-security":0.0583560723}}
{"text":"Further, it is shown that the torsion codes of a reversible cyclic code over a finite chain ring are reversible.","meta":{"url":"http://arxiv.org/abs/2307.09156v1"},"cats":{"new-dataset":0.0333346879,"dev-research":0.2839326165,"prompt-eng":0.3874903968,"data-quality":0.1332876701,"ml-security":0.0916580181}}
{"text":"Also, an example of a non-reversible cyclic code for which all its torsion codes are reversible has been presented to show that the converse of this statement is not true.","meta":{"url":"http://arxiv.org/abs/2307.09156v1"},"cats":{"new-dataset":0.0265768516,"dev-research":0.3032534097,"prompt-eng":0.3688001877,"data-quality":0.17485305,"ml-security":0.1187214799}}
{"text":"The cardinality and Hamming distance of a cyclic code over a finite commutative chain ring have also been determined.","meta":{"url":"http://arxiv.org/abs/2307.09156v1"},"cats":{"new-dataset":0.0521523232,"dev-research":0.2935553433,"prompt-eng":0.3683483624,"data-quality":0.1291812234,"ml-security":0.0538524914}}
{"text":"In this paper, we propose a novel and effective Multi-Level Fusion network, named as MLF-DET, for high-performance cross-modal 3D object DETection, which integrates both the feature-level fusion and decision-level fusion to fully utilize the information in the image.","meta":{"url":"http://arxiv.org/abs/2307.09155v1"},"cats":{"new-dataset":0.0813375793,"dev-research":0.2394350938,"prompt-eng":0.3916711193,"data-quality":0.1000275201,"ml-security":0.1164615227}}
{"text":"For the feature-level fusion, we present the Multi-scale Voxel Image fusion (MVI) module, which densely aligns multi-scale voxel features with image features.","meta":{"url":"http://arxiv.org/abs/2307.09155v1"},"cats":{"new-dataset":0.0980280151,"dev-research":0.2662992895,"prompt-eng":0.3773047741,"data-quality":0.0914389558,"ml-security":0.0428427042}}
{"text":"For the decision-level fusion, we propose the lightweight Feature-cued Confidence Rectification (FCR) module which further exploits image semantics to rectify the confidence of detection candidates.","meta":{"url":"http://arxiv.org/abs/2307.09155v1"},"cats":{"new-dataset":0.0689017858,"dev-research":0.3296881264,"prompt-eng":0.4892205044,"data-quality":0.3468878323,"ml-security":0.1666216731}}
{"text":"Besides, we design an effective data augmentation strategy termed Occlusion-aware GT Sampling (OGS) to reserve more sampled objects in the training scenes, so as to reduce overfitting.","meta":{"url":"http://arxiv.org/abs/2307.09155v1"},"cats":{"new-dataset":0.1520354266,"dev-research":0.2108587473,"prompt-eng":0.3687756076,"data-quality":0.2372362615,"ml-security":0.1485959099}}
{"text":"Extensive experiments on the KITTI dataset demonstrate the effectiveness of our method.","meta":{"url":"http://arxiv.org/abs/2307.09155v1"},"cats":{"new-dataset":0.2373840631,"dev-research":0.2519909276,"prompt-eng":0.4095675041,"data-quality":0.1499218242,"ml-security":0.0728058104}}
{"text":"Notably, on the extremely competitive KITTI car 3D object detection benchmark, our method reaches 82.89% moderate AP and achieves state-of-the-art performance without bells and whistles.","meta":{"url":"http://arxiv.org/abs/2307.09155v1"},"cats":{"new-dataset":0.1138017792,"dev-research":0.2536562646,"prompt-eng":0.3793668584,"data-quality":0.1046944956,"ml-security":0.0709644177}}
{"text":"We propose a method for synthesizing photo-realistic digital avatars from only one portrait as the reference.","meta":{"url":"http://arxiv.org/abs/2307.09153v2"},"cats":{"new-dataset":0.0814368401,"dev-research":0.228431971,"prompt-eng":0.413299721,"data-quality":0.1049383018,"ml-security":0.1149295971}}
{"text":"Given a portrait, our method synthesizes a coarse talking head video using driving keypoints features.","meta":{"url":"http://arxiv.org/abs/2307.09153v2"},"cats":{"new-dataset":0.0806717493,"dev-research":0.307017,"prompt-eng":0.3862069957,"data-quality":0.0928229757,"ml-security":0.0656972209}}
{"text":"And with the coarse video, our method synthesizes a coarse talking head avatar with a deforming neural radiance field.","meta":{"url":"http://arxiv.org/abs/2307.09153v2"},"cats":{"new-dataset":0.0655702006,"dev-research":0.251454147,"prompt-eng":0.4059364128,"data-quality":0.0835669473,"ml-security":0.094991459}}
{"text":"With rendered images of the coarse avatar, our method updates the low-quality images with a blind face restoration model.","meta":{"url":"http://arxiv.org/abs/2307.09153v2"},"cats":{"new-dataset":0.0494502404,"dev-research":0.23214784,"prompt-eng":0.380604034,"data-quality":0.1756671852,"ml-security":0.0928257303}}
{"text":"With updated images, we retrain the avatar for higher quality.","meta":{"url":"http://arxiv.org/abs/2307.09153v2"},"cats":{"new-dataset":0.0729299333,"dev-research":0.3120075062,"prompt-eng":0.4179519361,"data-quality":0.1799538857,"ml-security":0.0838195711}}
{"text":"After several iterations, our method can synthesize a photo-realistic animatable 3D neural head avatar.","meta":{"url":"http://arxiv.org/abs/2307.09153v2"},"cats":{"new-dataset":0.0682328777,"dev-research":0.2365779071,"prompt-eng":0.399669367,"data-quality":0.0672306601,"ml-security":0.0893215904}}
{"text":"The motivation of our method is deformable neural radiance field can eliminate the unnatural distortion caused by the image2video method.","meta":{"url":"http://arxiv.org/abs/2307.09153v2"},"cats":{"new-dataset":0.0226011716,"dev-research":0.2758294388,"prompt-eng":0.366287741,"data-quality":0.1292446528,"ml-security":0.114143095}}
{"text":"Our method outperforms state-of-the-art methods in quantitative and qualitative studies on various subjects.","meta":{"url":"http://arxiv.org/abs/2307.09153v2"},"cats":{"new-dataset":0.0377284004,"dev-research":0.3248893442,"prompt-eng":0.3499028062,"data-quality":0.1038655165,"ml-security":0.0655128802}}
{"text":"Network Slicing (NS) is an essential technique extensively used in 5G networks computing strategies, mobile edge computing, mobile cloud computing, and verticals like the Internet of Vehicles and industrial IoT, among others.","meta":{"url":"http://arxiv.org/abs/2307.09151v1"},"cats":{"new-dataset":0.0380883495,"dev-research":0.3170250266,"prompt-eng":0.3318639123,"data-quality":0.0762168148,"ml-security":0.1106062883}}
{"text":"NS is foreseen as one of the leading enablers for 6G futuristic and highly demanding applications since it allows the optimization and customization of scarce and disputed resources among dynamic, demanding clients with highly distinct application requirements.","meta":{"url":"http://arxiv.org/abs/2307.09151v1"},"cats":{"new-dataset":0.1190794879,"dev-research":0.2983249258,"prompt-eng":0.3684062146,"data-quality":0.0716428468,"ml-security":0.0910998918}}
{"text":"Various standardization organizations, like 3GPP's proposal for new generation networks and state-of-the-art 5G/6G research projects, are proposing new NS architectures.","meta":{"url":"http://arxiv.org/abs/2307.09151v1"},"cats":{"new-dataset":0.0867840188,"dev-research":0.2526296809,"prompt-eng":0.3488575436,"data-quality":0.0802507685,"ml-security":0.0559654096}}
{"text":"However, new NS architectures have to deal with an extensive range of requirements that inherently result in having NS architecture proposals typically fulfilling the needs of specific sets of domains with commonalities.","meta":{"url":"http://arxiv.org/abs/2307.09151v1"},"cats":{"new-dataset":0.0592105303,"dev-research":0.3577113872,"prompt-eng":0.4001454458,"data-quality":0.098005858,"ml-security":0.1002656068}}
{"text":"The Slicing Future Internet Infrastructures (SFI2) architecture proposal explores the gap resulting from the diversity of NS architectures target domains by proposing a new NS reference architecture with a defined focus on integrating experimental networks and enhancing the NS architecture with Machine Learning (ML) native optimizations, energy-efficient slicing, and slicing-tailored security functionalities.","meta":{"url":"http://arxiv.org/abs/2307.09151v1"},"cats":{"new-dataset":0.0923145304,"dev-research":0.352862819,"prompt-eng":0.3991535073,"data-quality":0.0805922405,"ml-security":0.4451930042}}
{"text":"The SFI2 architectural main contribution includes the utilization of the slice-as-a-service paradigm for end-to-end orchestration of resources across multi-domains and multi-technology experimental networks.","meta":{"url":"http://arxiv.org/abs/2307.09151v1"},"cats":{"new-dataset":0.0922696601,"dev-research":0.2986338974,"prompt-eng":0.3862474949,"data-quality":0.0778896371,"ml-security":0.0603145095}}
{"text":"In addition, the SFI2 reference architecture instantiations will enhance the multi-domain and multi-technology integrated experimental network deployment with native ML optimization, energy-efficient aware slicing, and slicing-tailored security functionalities for the practical domain.","meta":{"url":"http://arxiv.org/abs/2307.09151v1"},"cats":{"new-dataset":0.0963766222,"dev-research":0.384392375,"prompt-eng":0.4137985227,"data-quality":0.1017649977,"ml-security":0.2272591126}}
{"text":"Model-driven software engineering is a suitable method for dealing with the ever-increasing complexity of software development processes.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0386948997,"dev-research":0.490068791,"prompt-eng":0.4437898486,"data-quality":0.0645616394,"ml-security":0.1386840648}}
{"text":"Graphs and graph transformations have proven useful for representing such models and changes to them.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.050305625,"dev-research":0.2693712752,"prompt-eng":0.412173179,"data-quality":0.0960326823,"ml-security":0.0924866043}}
{"text":"These models must satisfy certain sets of constraints.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0545733561,"dev-research":0.1491778656,"prompt-eng":0.4312707048,"data-quality":0.1163105348,"ml-security":0.1186990986}}
{"text":"An example are the multiplicities of a class structure.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.1262351152,"dev-research":0.2704504951,"prompt-eng":0.3659072398,"data-quality":0.1448637539,"ml-security":0.1399374428}}
{"text":"During the development process, a change to a model may result in an inconsistent model that must at some point be repaired.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0132178864,"dev-research":0.3788910295,"prompt-eng":0.4049623526,"data-quality":0.3866211649,"ml-security":0.1555184554}}
{"text":"This problem is called model repair.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.1232881995,"dev-research":0.2352443712,"prompt-eng":0.4109045132,"data-quality":0.3592876065,"ml-security":0.093215285}}
{"text":"In particular, we will consider rule-based graph repair which is defined as follows:","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0365027858,"dev-research":0.3468777645,"prompt-eng":0.3625839892,"data-quality":0.3223635318,"ml-security":0.1178260549}}
{"text":"Given a graph $G$, a constraint $c$ such that $G$ does not satisfy $c$, and a set of rules $R$, use the rules of $\\mathcal{R}$ to transform $G$ into a graph that satisfies $c$.   Known notions of consistency have either viewed consistency as a binary property, either a graph is consistent w.r.t.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0197580649,"dev-research":0.2372233139,"prompt-eng":0.3583537774,"data-quality":0.2938279998,"ml-security":0.0607754181}}
{"text":"a constraint $c$ or not, or only viewed the number of violations of the first graph of a constraint.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0818096014,"dev-research":0.2748053909,"prompt-eng":0.3712984752,"data-quality":0.1990304331,"ml-security":0.0773728746}}
{"text":"In this thesis, we introduce new notions of consistency, which we call consistency-maintaining and consistency-increasing transformations and rules, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0404310033,"dev-research":0.3087072131,"prompt-eng":0.379280946,"data-quality":0.268256337,"ml-security":0.0501327378}}
{"text":"This is based on the possibility that a constraint can be satisfied up to a certain nesting level.   ","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0068646521,"dev-research":0.2221369527,"prompt-eng":0.4287939973,"data-quality":0.0887488098,"ml-security":0.0654529983}}
{"text":"We present constructions for direct consistency-maintaining or direct consistency-increasing application conditions, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0294675895,"dev-research":0.3349877566,"prompt-eng":0.4054425846,"data-quality":0.2734505472,"ml-security":0.0551241437}}
{"text":"Finally, we present an rule-based graph repair approach that is able to repair so-called \\emph{circular conflict-free constraints}, and so-called circular conflict-free sets of constraints.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0759664841,"dev-research":0.3260164466,"prompt-eng":0.3547971712,"data-quality":0.2880724183,"ml-security":0.1115197076}}
{"text":"Intuitively, a set of constraint $C$ is circular conflict free, if there is an ordering $c_1, \\ldots, c_n$ of all constraints of $C$ such that there is no $j <i$ such that a repair of $c_i$ at all graphs satisfying $c_j$ leads to a graph not satisfying $c_j$.","meta":{"url":"http://arxiv.org/abs/2307.09150v1"},"cats":{"new-dataset":0.0580821562,"dev-research":0.2970076664,"prompt-eng":0.3449845058,"data-quality":0.1486114251,"ml-security":0.145338013}}
{"text":"This paper proposes a novel paradigm for facial privacy protection that unifies multiple characteristics including anonymity, diversity, reversibility and security within a single lightweight framework.","meta":{"url":"http://arxiv.org/abs/2307.09146v1"},"cats":{"new-dataset":0.0691399962,"dev-research":0.2331318629,"prompt-eng":0.3287203183,"data-quality":0.1016925028,"ml-security":0.6780426303}}
{"text":"We name it PRO-Face S, short for Privacy-preserving Reversible Obfuscation of Face images via Secure flow-based model.","meta":{"url":"http://arxiv.org/abs/2307.09146v1"},"cats":{"new-dataset":0.0757366711,"dev-research":0.2579927866,"prompt-eng":0.3826769695,"data-quality":0.1334159117,"ml-security":0.5375818147}}
{"text":"In the framework, an Invertible Neural Network (INN) is utilized to process the input image along with its pre-obfuscated form, and generate the privacy protected image that visually approximates to the pre-obfuscated one, thus ensuring privacy.","meta":{"url":"http://arxiv.org/abs/2307.09146v1"},"cats":{"new-dataset":0.0744470964,"dev-research":0.2814252702,"prompt-eng":0.3922008628,"data-quality":0.1427464703,"ml-security":0.4818715412}}
{"text":"The pre-obfuscation applied can be in diversified form with different strengths and styles specified by users.","meta":{"url":"http://arxiv.org/abs/2307.09146v1"},"cats":{"new-dataset":0.0134074134,"dev-research":0.3542336903,"prompt-eng":0.4305615844,"data-quality":0.1662387684,"ml-security":0.333599034}}
{"text":"Along protection, a secret key is injected into the network such that the original image can only be recovered from the protection image via the same model given the correct key provided.","meta":{"url":"http://arxiv.org/abs/2307.09146v1"},"cats":{"new-dataset":0.0223408288,"dev-research":0.2359787556,"prompt-eng":0.3926210426,"data-quality":0.1417333498,"ml-security":0.3873581014}}
{"text":"Two modes of image recovery are devised to deal with malicious recovery attempts in different scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09146v1"},"cats":{"new-dataset":0.0333201214,"dev-research":0.2324974324,"prompt-eng":0.4564999927,"data-quality":0.1902237296,"ml-security":0.3116458152}}
{"text":"Finally, extensive experiments conducted on three public image datasets demonstrate the superiority of the proposed framework over multiple state-of-the-art approaches.","meta":{"url":"http://arxiv.org/abs/2307.09146v1"},"cats":{"new-dataset":0.5369177673,"dev-research":0.2326451711,"prompt-eng":0.3710556048,"data-quality":0.221606306,"ml-security":0.2027653366}}
{"text":"We combine dependent types with linear type systems that soundly and completely capture polynomial time computation.","meta":{"url":"http://arxiv.org/abs/2307.09145v1"},"cats":{"new-dataset":0.0174067256,"dev-research":0.2467556052,"prompt-eng":0.3628047705,"data-quality":0.0852201791,"ml-security":0.159635192}}
{"text":"We explore two systems for capturing polynomial time: one system that disallows construction of iterable data, and one, based on the LFPL system of Martin Hofmann, that controls construction via a payment method.","meta":{"url":"http://arxiv.org/abs/2307.09145v1"},"cats":{"new-dataset":0.1507971767,"dev-research":0.2699907771,"prompt-eng":0.3546251658,"data-quality":0.0697465737,"ml-security":0.1567439513}}
{"text":"Both of these are extended to full dependent types via Quantitative Type Theory, allowing for arbitrary computation in types alongside guaranteed polynomial time computation in terms.","meta":{"url":"http://arxiv.org/abs/2307.09145v1"},"cats":{"new-dataset":0.0127849181,"dev-research":0.2352725919,"prompt-eng":0.3659228907,"data-quality":0.0682525839,"ml-security":0.1029785708}}
{"text":"We prove the soundness of the systems using a realisability technique due to Dal Lago and Hofmann.   ","meta":{"url":"http://arxiv.org/abs/2307.09145v1"},"cats":{"new-dataset":0.0651591348,"dev-research":0.2459351924,"prompt-eng":0.3498249325,"data-quality":0.1517490079,"ml-security":0.1436022351}}
{"text":"Our long-term goal is to combine the extensional reasoning of type theory with intensional reasoning about the resources intrinsically consumed by programs.","meta":{"url":"http://arxiv.org/abs/2307.09145v1"},"cats":{"new-dataset":0.0204208665,"dev-research":0.3614734339,"prompt-eng":0.426103465,"data-quality":0.0683085272,"ml-security":0.0935455741}}
{"text":"This paper is a step along this path, which we hope will lead both to practical systems for reasoning about programs' resource usage, and to theoretical use as a form of synthetic computational complexity theory.","meta":{"url":"http://arxiv.org/abs/2307.09145v1"},"cats":{"new-dataset":0.0459634401,"dev-research":0.3430092739,"prompt-eng":0.3768208419,"data-quality":0.0563920226,"ml-security":0.1514626403}}
{"text":"Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications require object detection for distant objects and (ii) SOD is a challenging task due to the noisy, blurred, and less-informative image appearances of small objects.","meta":{"url":"http://arxiv.org/abs/2307.09143v1"},"cats":{"new-dataset":0.0545411804,"dev-research":0.1977243034,"prompt-eng":0.4116651003,"data-quality":0.1453725997,"ml-security":0.1028553624}}
{"text":"This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the Small Object Detection for Spotting Birds (SOD4SB) dataset.","meta":{"url":"http://arxiv.org/abs/2307.09143v1"},"cats":{"new-dataset":0.6699322328,"dev-research":0.1994476777,"prompt-eng":0.365324896,"data-quality":0.151541524,"ml-security":0.1070026559}}
{"text":"The detail of the challenge with the SOD4SB dataset is introduced in this paper.","meta":{"url":"http://arxiv.org/abs/2307.09143v1"},"cats":{"new-dataset":0.7187998086,"dev-research":0.2447960678,"prompt-eng":0.3916623093,"data-quality":0.130150499,"ml-security":0.1083809361}}
{"text":"In total, 223 participants joined this challenge.","meta":{"url":"http://arxiv.org/abs/2307.09143v1"},"cats":{"new-dataset":0.3026451189,"dev-research":0.2716310275,"prompt-eng":0.4310955793,"data-quality":0.101596134,"ml-security":0.0603866934}}
{"text":"This paper briefly introduces the award-winning methods.","meta":{"url":"http://arxiv.org/abs/2307.09143v1"},"cats":{"new-dataset":0.0248960096,"dev-research":0.2504285212,"prompt-eng":0.4038381629,"data-quality":0.1652492656,"ml-security":0.0977430202}}
{"text":"The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.","meta":{"url":"http://arxiv.org/abs/2307.09143v1"},"cats":{"new-dataset":0.7842687621,"dev-research":0.3124704515,"prompt-eng":0.3859450111,"data-quality":0.1068083568,"ml-security":0.1412386429}}
{"text":"Boolean satisfiability (SAT) is a fundamental NP-complete problem with many applications, including automated planning and scheduling.","meta":{"url":"http://arxiv.org/abs/2307.09141v1"},"cats":{"new-dataset":0.0535577705,"dev-research":0.2892266322,"prompt-eng":0.4093674428,"data-quality":0.07986586,"ml-security":0.0727522044}}
{"text":"To solve large instances, SAT solvers have to rely on heuristics, e.g., choosing a branching variable in DPLL and CDCL solvers.","meta":{"url":"http://arxiv.org/abs/2307.09141v1"},"cats":{"new-dataset":0.0523993224,"dev-research":0.2825202849,"prompt-eng":0.4103797227,"data-quality":0.0606098312,"ml-security":0.0817361114}}
{"text":"Such heuristics can be improved with machine learning (ML) models; they can reduce the number of steps but usually hinder the running time because useful models are relatively large and slow.","meta":{"url":"http://arxiv.org/abs/2307.09141v1"},"cats":{"new-dataset":0.0196229928,"dev-research":0.2734987944,"prompt-eng":0.4617207043,"data-quality":0.0682526661,"ml-security":0.1415690919}}
{"text":"We suggest the strategy of making a few initial steps with a trained ML model and then releasing control to classical heuristics; this simplifies cold start for SAT solving and can decrease both the number of steps and overall runtime, but requires a separate decision of when to release control to the solver.","meta":{"url":"http://arxiv.org/abs/2307.09141v1"},"cats":{"new-dataset":0.0690385486,"dev-research":0.2646755582,"prompt-eng":0.4639286867,"data-quality":0.0426780395,"ml-security":0.1788049598}}
{"text":"Moreover, we introduce a modification of Graph-Q-SAT tailored to SAT problems converted from other domains, e.g., open shop scheduling problems.","meta":{"url":"http://arxiv.org/abs/2307.09141v1"},"cats":{"new-dataset":0.1160441213,"dev-research":0.231667394,"prompt-eng":0.3625359779,"data-quality":0.0904545691,"ml-security":0.06446949}}
{"text":"We validate the feasibility of our approach with random and industrial SAT problems.","meta":{"url":"http://arxiv.org/abs/2307.09141v1"},"cats":{"new-dataset":0.0419056315,"dev-research":0.2044271918,"prompt-eng":0.423036913,"data-quality":0.1244826077,"ml-security":0.1459500542}}
{"text":"Mixed sample data augmentation (MSDA) is a widely used technique that has been found to improve performance in a variety of tasks.","meta":{"url":"http://arxiv.org/abs/2307.09136v1"},"cats":{"new-dataset":0.0958442122,"dev-research":0.3077441211,"prompt-eng":0.383295389,"data-quality":0.1736829356,"ml-security":0.0707416306}}
{"text":"However, in this paper, we show that the effects of MSDA are class-dependent, with some classes seeing an improvement in performance while others experience a decline.","meta":{"url":"http://arxiv.org/abs/2307.09136v1"},"cats":{"new-dataset":0.0245423265,"dev-research":0.3332598734,"prompt-eng":0.3792325306,"data-quality":0.1821196882,"ml-security":0.2477491277}}
{"text":"To reduce class dependency, we propose the DropMix method, which excludes a specific percentage of data from the MSDA computation.","meta":{"url":"http://arxiv.org/abs/2307.09136v1"},"cats":{"new-dataset":0.1488739815,"dev-research":0.311663284,"prompt-eng":0.3754325851,"data-quality":0.2034742186,"ml-security":0.1622955767}}
{"text":"By training on a combination of MSDA and non-MSDA data, the proposed method not only improves the performance of classes that were previously degraded by MSDA, but also increases overall average accuracy, as shown in experiments on two datasets (CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and PuzzleMix).","meta":{"url":"http://arxiv.org/abs/2307.09136v1"},"cats":{"new-dataset":0.1571379631,"dev-research":0.3128471488,"prompt-eng":0.3524718457,"data-quality":0.2925939119,"ml-security":0.1801112549}}
{"text":"In order to fully benefit from cloud computing, services are designed following the \"multi-tenant\" architectural model, which is aimed at maximizing resource sharing among users.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.042223607,"dev-research":0.313395595,"prompt-eng":0.3453863362,"data-quality":0.046092753,"ml-security":0.1207476104}}
{"text":"However, multi-tenancy introduces challenges of security, performance isolation, scaling, and customization.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.0294684941,"dev-research":0.3547046993,"prompt-eng":0.3769385517,"data-quality":0.0870232652,"ml-security":0.2560909483}}
{"text":"RStudio server is an open-source Integrated Development Environment (IDE) accessible over a web browser for the R programming language.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.3272578974,"dev-research":0.3613561793,"prompt-eng":0.4208814241,"data-quality":0.0714601263,"ml-security":0.0581511564}}
{"text":"We present the design and implementation of a multi-user distributed system on Hopsworks, a data-intensive AI platform, following the multi-tenant model that provides RStudio as Software as a Service (SaaS).","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.2846095115,"dev-research":0.3186051513,"prompt-eng":0.3418012702,"data-quality":0.0415353717,"ml-security":0.0964407786}}
{"text":"We use the most popular cloud-native technologies: Docker and Kubernetes, to solve the problems of performance isolation, security, and scaling that are present in a multi-tenant environment.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.0906217554,"dev-research":0.3136386312,"prompt-eng":0.342274545,"data-quality":0.0888582497,"ml-security":0.2191730417}}
{"text":"We further enable secure data sharing in RStudio server instances to provide data privacy and allow collaboration among RStudio users.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.2549302191,"dev-research":0.3286135019,"prompt-eng":0.3818105778,"data-quality":0.0937515548,"ml-security":0.2239393376}}
{"text":"We integrate our system with Apache Spark, which can scale and handle Big Data processing workloads.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.2344357114,"dev-research":0.2897297672,"prompt-eng":0.3395013162,"data-quality":0.063038828,"ml-security":0.0873045278}}
{"text":"Also, we provide a UI where users can provide custom configurations and have full control of their own RStudio server instances.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.2066385685,"dev-research":0.3802679598,"prompt-eng":0.4650997439,"data-quality":0.0615189417,"ml-security":0.0639941305}}
{"text":"Our system was tested on a Google Cloud Platform cluster with four worker nodes, each with 30GB of RAM allocated to them.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.1334966464,"dev-research":0.2500718641,"prompt-eng":0.3549053066,"data-quality":0.0954165016,"ml-security":0.0852728425}}
{"text":"The tests on this cluster showed that 44 RStudio servers, each with 2GB of RAM, can be run concurrently.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.1275180404,"dev-research":0.2203941417,"prompt-eng":0.3731395874,"data-quality":0.0755659484,"ml-security":0.049671612}}
{"text":"Our system can scale out to potentially support hundreds of concurrently running RStudio servers by adding more resources (CPUs and RAM) to the cluster or system.","meta":{"url":"http://arxiv.org/abs/2307.09132v1"},"cats":{"new-dataset":0.0352046714,"dev-research":0.274978383,"prompt-eng":0.3890672903,"data-quality":0.0510996741,"ml-security":0.0634320447}}
{"text":"While transformer architectures have dominated computer vision in recent years, these models cannot easily be deployed on hardware with limited resources for autonomous driving tasks that require real-time-performance.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.0252632032,"dev-research":0.2209163278,"prompt-eng":0.3738239716,"data-quality":0.0853541917,"ml-security":0.1354552263}}
{"text":"Their computational complexity and memory requirements limits their use, especially for applications with high-resolution inputs.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.0430716193,"dev-research":0.2638786508,"prompt-eng":0.3551708811,"data-quality":0.0578065054,"ml-security":0.0941825247}}
{"text":"In our work, we redesign the powerful state-of-the-art Vision Transformer PLG-ViT to a much more compact and efficient architecture that is suitable for such tasks.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.067649283,"dev-research":0.2036810435,"prompt-eng":0.4059260202,"data-quality":0.0538055945,"ml-security":0.0357507376}}
{"text":"We identify computationally expensive blocks in the original PLG-ViT architecture and propose several redesigns aimed at reducing the number of parameters and floating-point operations.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.0520608982,"dev-research":0.247548767,"prompt-eng":0.3719721246,"data-quality":0.0538836631,"ml-security":0.0495734976}}
{"text":"As a result of our redesign, we are able to reduce PLG-ViT in size by a factor of 5, with a moderate drop in performance.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.0174744812,"dev-research":0.2384164382,"prompt-eng":0.3991832169,"data-quality":0.0648686917,"ml-security":0.0389385189}}
{"text":"We propose two variants, optimized for the best trade-off between parameter count to runtime as well as parameter count to accuracy.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.1810419163,"dev-research":0.3167436869,"prompt-eng":0.4234420374,"data-quality":0.1358861225,"ml-security":0.1022550892}}
{"text":"With only 5 million parameters, we achieve 79.5$\\%$ top-1 accuracy on the ImageNet-1K classification benchmark.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.1624602501,"dev-research":0.2587289233,"prompt-eng":0.409506804,"data-quality":0.2404316485,"ml-security":0.1544194548}}
{"text":"Our networks demonstrate great performance on general vision benchmarks like COCO instance segmentation.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.1690055586,"dev-research":0.2289856738,"prompt-eng":0.3940104891,"data-quality":0.2146894435,"ml-security":0.1133136384}}
{"text":"In addition, we conduct a series of experiments, demonstrating the potential of our approach in solving various tasks specifically tailored to the challenges of autonomous driving and transportation.","meta":{"url":"http://arxiv.org/abs/2307.09120v1"},"cats":{"new-dataset":0.0628597728,"dev-research":0.253024564,"prompt-eng":0.4332641541,"data-quality":0.0995724879,"ml-security":0.1020203715}}
{"text":"Remarkable progress has been made in 3D reconstruction from single-view RGB-D inputs.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0891382309,"dev-research":0.2043491021,"prompt-eng":0.3483694406,"data-quality":0.070570419,"ml-security":0.0560884981}}
{"text":"MCC is the current state-of-the-art method in this field, which achieves unprecedented success by combining vision Transformers with large-scale training.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0363120255,"dev-research":0.2703230559,"prompt-eng":0.4260927785,"data-quality":0.1039340524,"ml-security":0.085124414}}
{"text":"However, we identified two key limitations of MCC: 1) The Transformer decoder is inefficient in handling large number of query points; 2)","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0235336573,"dev-research":0.2641948534,"prompt-eng":0.3974468543,"data-quality":0.0873291335,"ml-security":0.0937762398}}
{"text":"The 3D representation struggles to recover high-fidelity details.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0475206261,"dev-research":0.3208967826,"prompt-eng":0.3820355272,"data-quality":0.1373762,"ml-security":0.0756969018}}
{"text":"In this paper, we propose a new approach called NU-MCC that addresses these limitations.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0403901672,"dev-research":0.239019243,"prompt-eng":0.3814826471,"data-quality":0.1169060777,"ml-security":0.1085775327}}
{"text":"NU-MCC includes two key innovations: a Neighborhood decoder and a Repulsive Unsigned Distance Function (Repulsive UDF).","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0874923005,"dev-research":0.3058776919,"prompt-eng":0.4088490793,"data-quality":0.1047466995,"ml-security":0.1194093182}}
{"text":"First, our Neighborhood decoder introduces center points as an efficient proxy of input visual features, allowing each query point to only attend to a small neighborhood.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0763683473,"dev-research":0.3612329655,"prompt-eng":0.4122743819,"data-quality":0.1332122496,"ml-security":0.12219132}}
{"text":"This design not only results in much faster inference speed but also enables the exploitation of finer-scale visual features for improved recovery of 3D textures.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0327752231,"dev-research":0.2688550099,"prompt-eng":0.4060330176,"data-quality":0.0853909199,"ml-security":0.0755491137}}
{"text":"Second, our Repulsive UDF is a novel alternative to the occupancy field used in MCC, significantly improving the quality of 3D object reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.1292746874,"dev-research":0.2750532877,"prompt-eng":0.3704436895,"data-quality":0.1058432359,"ml-security":0.0547599872}}
{"text":"Compared to standard UDFs that suffer from holes in results, our proposed Repulsive UDF can achieve more complete surface reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.1638555461,"dev-research":0.2515909863,"prompt-eng":0.35626049,"data-quality":0.0939086136,"ml-security":0.0703621629}}
{"text":"Experimental results demonstrate that NU-MCC is able to learn a strong 3D representation, significantly advancing the state of the art in single-view 3D reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.0653851429,"dev-research":0.2400368982,"prompt-eng":0.3693846178,"data-quality":0.0859003736,"ml-security":0.0736033925}}
{"text":"Particularly, it outperforms MCC by 9.7% in terms of the F1-score on the CO3D-v2 dataset with more than 5x faster running speed.","meta":{"url":"http://arxiv.org/abs/2307.09112v1"},"cats":{"new-dataset":0.078647292,"dev-research":0.350279925,"prompt-eng":0.3573156481,"data-quality":0.1211469701,"ml-security":0.0748264414}}
{"text":"Let $G$ be a graph, which represents a social network, and suppose each node $v$ has a threshold value $\\tau(v)$. Consider an initial configuration, where each node is either positive or negative.","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.0360343548,"dev-research":0.226408738,"prompt-eng":0.4089306861,"data-quality":0.152385578,"ml-security":0.1068166047}}
{"text":"In each discrete time step, a node $v$ becomes/remains positive if at least $\\tau(v)$ of its neighbors are positive and negative otherwise.","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.0261498438,"dev-research":0.2495583882,"prompt-eng":0.3875304522,"data-quality":0.1470415279,"ml-security":0.1097447427}}
{"text":"A node set $\\mathcal{S}$ is a Target Set (TS) whenever the following holds: if $\\mathcal{S}$ is fully positive initially, all nodes in the graph become positive eventually.","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.0552146148,"dev-research":0.2515223233,"prompt-eng":0.441073304,"data-quality":0.1289047239,"ml-security":0.1572649242}}
{"text":"We focus on a generalization of TS, called Timed TS (TTS), where it is permitted to assign a positive state to a node at any step of the process, rather than just at the beginning.   ","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.0140382994,"dev-research":0.2339374283,"prompt-eng":0.4499845493,"data-quality":0.0643217043,"ml-security":0.1238100465}}
{"text":"We provide graph structures for which the minimum TTS is significantly smaller than the minimum TS, indicating that timing is an essential aspect of successful target selection strategies.","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.0151420854,"dev-research":0.2478948853,"prompt-eng":0.4509049999,"data-quality":0.0586453755,"ml-security":0.125142515}}
{"text":"Furthermore, we prove tight bounds on the minimum size of a TTS in terms of the number of nodes and maximum degree when the thresholds are assigned based on the majority rule.   ","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.0482325241,"dev-research":0.199550506,"prompt-eng":0.4022770438,"data-quality":0.1096922095,"ml-security":0.1669676371}}
{"text":"We show that the problem of determining the minimum size of a TTS is NP-hard and provide an Integer Linear Programming formulation and a greedy algorithm.","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.0882740748,"dev-research":0.2087156062,"prompt-eng":0.4008474281,"data-quality":0.072272921,"ml-security":0.1246798106}}
{"text":"We evaluate the performance of our algorithm by conducting experiments on various synthetic and real-world networks.","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.0561283731,"dev-research":0.2472548391,"prompt-eng":0.3443148343,"data-quality":0.1453078013,"ml-security":0.1710108367}}
{"text":"We also present a linear-time exact algorithm for trees.","meta":{"url":"http://arxiv.org/abs/2307.09111v1"},"cats":{"new-dataset":0.1219725124,"dev-research":0.2345621682,"prompt-eng":0.3404539783,"data-quality":0.1392489523,"ml-security":0.0703242415}}
{"text":"Several Active Learning (AL) policies require retraining a target model several times in order to identify the most informative samples and rarely offer the option to focus on the acquisition of samples from underrepresented classes.","meta":{"url":"http://arxiv.org/abs/2307.09109v1"},"cats":{"new-dataset":0.033173233,"dev-research":0.2573822995,"prompt-eng":0.4090924496,"data-quality":0.2204353356,"ml-security":0.2652398616}}
{"text":"Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm is introduced where an AL policy is constructed through deep reinforcement learning and exploits quantity-accuracy correlations to build datasets on which high-performance models can be trained with regards to specific classes.","meta":{"url":"http://arxiv.org/abs/2307.09109v1"},"cats":{"new-dataset":0.1225992139,"dev-research":0.2380632382,"prompt-eng":0.3808365809,"data-quality":0.2142217821,"ml-security":0.3834687539}}
{"text":"MiSiCAL is especially helpful in the case of very large batch sizes since it does not require repeated model training sessions as is common in other AL methods.","meta":{"url":"http://arxiv.org/abs/2307.09109v1"},"cats":{"new-dataset":0.0144610392,"dev-research":0.2555264287,"prompt-eng":0.3909885217,"data-quality":0.2598812878,"ml-security":0.1115417393}}
{"text":"This is thanks to its ability to exploit fixed representations of the candidate data points.","meta":{"url":"http://arxiv.org/abs/2307.09109v1"},"cats":{"new-dataset":0.1218002297,"dev-research":0.3052389766,"prompt-eng":0.4065400608,"data-quality":0.1967356878,"ml-security":0.1998941199}}
{"text":"We find that MiSiCAL is able to outperform a random policy on 150 out of 171 COCO10k classes, while the strongest baseline only outperforms random on 101 classes.","meta":{"url":"http://arxiv.org/abs/2307.09109v1"},"cats":{"new-dataset":0.1455865091,"dev-research":0.2576866825,"prompt-eng":0.4142246759,"data-quality":0.348134359,"ml-security":0.2200051831}}
{"text":"In cut sparsification, all cuts of a hypergraph $H=(V,E,w)$ are approximated within $1\\pm\\epsilon$ factor by a small hypergraph $H'$. This widely applied method was generalized recently to a setting where the cost of cutting each $e\\in E$ is provided by a splitting function, $g_e: 2^e\\to\\mathbb{R}_+$.","meta":{"url":"http://arxiv.org/abs/2307.09110v1"},"cats":{"new-dataset":0.0218327486,"dev-research":0.2191341936,"prompt-eng":0.3184932457,"data-quality":0.1959769625,"ml-security":0.0784500441}}
{"text":"This generalization is called a submodular hypergraph when the functions $\\{g_e\\}_{e\\in E}$ are submodular, and it arises in machine learning, combinatorial optimization, and algorithmic game theory.","meta":{"url":"http://arxiv.org/abs/2307.09110v1"},"cats":{"new-dataset":0.0573708553,"dev-research":0.2397050445,"prompt-eng":0.356032951,"data-quality":0.1216326725,"ml-security":0.0937725119}}
{"text":"Previous work focused on the setting where $H'$ is a reweighted sub-hypergraph of $H$, and measured size by the number of hyperedges in $H'$. We study such sparsification, and also a more general notion of representing $H$ succinctly, where size is measured in bits.   ","meta":{"url":"http://arxiv.org/abs/2307.09110v1"},"cats":{"new-dataset":0.040996748,"dev-research":0.2585693846,"prompt-eng":0.3968703978,"data-quality":0.2318109945,"ml-security":0.0724013737}}
{"text":"In the sparsification setting, where size is the number of hyperedges, we present three results: (i) all submodular hypergraphs admit sparsifiers of size polynomial in $n=|V|$; (ii) monotone-submodular hypergraphs admit sparsifiers of size $O(\\epsilon^{-2} n^3)$; and (iii) we propose a new parameter, called spread, to obtain even smaller sparsifiers in some cases.   ","meta":{"url":"http://arxiv.org/abs/2307.09110v1"},"cats":{"new-dataset":0.0391285177,"dev-research":0.2480355147,"prompt-eng":0.3501168134,"data-quality":0.2979448374,"ml-security":0.1343903195}}
{"text":"In the succinct-representation setting, we show that a natural family of splitting functions admits a succinct representation of much smaller size than via reweighted subgraphs (almost by factor $n$).","meta":{"url":"http://arxiv.org/abs/2307.09110v1"},"cats":{"new-dataset":0.08069039,"dev-research":0.2522882467,"prompt-eng":0.3548627102,"data-quality":0.1194454891,"ml-security":0.1064655621}}
{"text":"This large gap is surprising because for graphs, the most succinct representation is attained by reweighted subgraphs.","meta":{"url":"http://arxiv.org/abs/2307.09110v1"},"cats":{"new-dataset":0.0657556301,"dev-research":0.275474897,"prompt-eng":0.3468031786,"data-quality":0.1426228201,"ml-security":0.0631050879}}
{"text":"Along the way, we introduce the notion of deformation, where $g_e$ is decomposed into a sum of functions of small description, and we provide upper and lower bounds for deformation of common splitting functions.","meta":{"url":"http://arxiv.org/abs/2307.09110v1"},"cats":{"new-dataset":0.0608295088,"dev-research":0.2366842763,"prompt-eng":0.3630709067,"data-quality":0.1285347349,"ml-security":0.1233941425}}
{"text":"We present a method for sampling-based model predictive control that makes use of a generic physics simulator as the dynamical model.","meta":{"url":"http://arxiv.org/abs/2307.09105v1"},"cats":{"new-dataset":0.0600915706,"dev-research":0.1854635138,"prompt-eng":0.4501197475,"data-quality":0.0609463634,"ml-security":0.141101483}}
{"text":"In particular, we propose a Model Predictive Path Integral controller (MPPI), that uses the GPU-parallelizable IsaacGym simulator to compute the forward dynamics of a problem.","meta":{"url":"http://arxiv.org/abs/2307.09105v1"},"cats":{"new-dataset":0.0600114667,"dev-research":0.1692789476,"prompt-eng":0.4059518752,"data-quality":0.0506813373,"ml-security":0.1142982916}}
{"text":"By doing so, we eliminate the need for manual encoding of robot dynamics and interactions among objects and allow one to effortlessly solve complex navigation and contact-rich tasks.","meta":{"url":"http://arxiv.org/abs/2307.09105v1"},"cats":{"new-dataset":0.0467352772,"dev-research":0.3210591319,"prompt-eng":0.4642420127,"data-quality":0.046861796,"ml-security":0.0532838004}}
{"text":"Since no explicit dynamic modeling is required, the method is easily extendable to different objects and robots.","meta":{"url":"http://arxiv.org/abs/2307.09105v1"},"cats":{"new-dataset":0.0131542603,"dev-research":0.2155718826,"prompt-eng":0.4120351689,"data-quality":0.0461513931,"ml-security":0.0732628052}}
{"text":"We demonstrate the effectiveness of this method in several simulated and real-world settings, among which mobile navigation with collision avoidance, non-prehensile manipulation, and whole-body control for high-dimensional configuration spaces.","meta":{"url":"http://arxiv.org/abs/2307.09105v1"},"cats":{"new-dataset":0.1222032714,"dev-research":0.2338357448,"prompt-eng":0.3836573618,"data-quality":0.0334704747,"ml-security":0.1014345085}}
{"text":"This method is a powerful and accessible tool to solve a large variety of contact-rich motion planning tasks.","meta":{"url":"http://arxiv.org/abs/2307.09105v1"},"cats":{"new-dataset":0.0718209455,"dev-research":0.2843928002,"prompt-eng":0.4165274196,"data-quality":0.0283448717,"ml-security":0.0254498547}}
{"text":"Low-light image enhancement strives to improve the contrast, adjust the visibility, and restore the distortion in color and texture.","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.009947001,"dev-research":0.2800362404,"prompt-eng":0.3812472049,"data-quality":0.1136363662,"ml-security":0.0396400358}}
{"text":"Existing methods usually pay more attention to improving the visibility and contrast via increasing the lightness of low-light images, while disregarding the significance of color and texture restoration for high-quality images.","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.0295940743,"dev-research":0.2643989899,"prompt-eng":0.3784954257,"data-quality":0.1445279341,"ml-security":0.0485314428}}
{"text":"Against above issue, we propose a novel luminance and chrominance dual branch network, termed LCDBNet, for low-light image enhancement, which divides low-light image enhancement into two sub-tasks, e.g., luminance adjustment and chrominance restoration.","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.0442569123,"dev-research":0.266123407,"prompt-eng":0.3839641241,"data-quality":0.1403083744,"ml-security":0.039903392}}
{"text":"Specifically, LCDBNet is composed of two branches, namely luminance adjustment network (LAN) and chrominance restoration network (CRN).","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.1136629339,"dev-research":0.3073613035,"prompt-eng":0.3893080369,"data-quality":0.1676519811,"ml-security":0.0334761494}}
{"text":"LAN takes responsibility for learning brightness-aware features leveraging long-range dependency and local attention correlation.","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.0885629231,"dev-research":0.3429157672,"prompt-eng":0.4521083152,"data-quality":0.1322326345,"ml-security":0.1119490217}}
{"text":"While CRN concentrates on learning detail-sensitive features via multi-level wavelet decomposition.","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.0682757194,"dev-research":0.2950051648,"prompt-eng":0.385990091,"data-quality":0.2737673566,"ml-security":0.1599388885}}
{"text":"Finally, a fusion network is designed to blend their learned features to produce visually impressive images.","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.0895037852,"dev-research":0.3449526658,"prompt-eng":0.4251921261,"data-quality":0.1384415963,"ml-security":0.0802738344}}
{"text":"Extensive experiments conducted on seven benchmark datasets validate the effectiveness of our proposed LCDBNet, and the results manifest that LCDBNet achieves superior performance in terms of multiple reference/non-reference quality evaluators compared to other state-of-the-art competitors.","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.3206168842,"dev-research":0.4077528346,"prompt-eng":0.3857850892,"data-quality":0.3174498774,"ml-security":0.0847366027}}
{"text":"Our code and pretrained model will be available.","meta":{"url":"http://arxiv.org/abs/2307.09104v1"},"cats":{"new-dataset":0.3619983448,"dev-research":0.3348396355,"prompt-eng":0.5192955565,"data-quality":0.0869770971,"ml-security":0.1031314602}}
{"text":"Recently, the expert-crafted neural architectures is increasing overtaken by the utilization of neural architecture search (NAS) and automatic generation (and tuning) of network structures which has a close relation to the Hyperparameter Optimization and Auto Machine Learning (AutoML).","meta":{"url":"http://arxiv.org/abs/2307.09099v1"},"cats":{"new-dataset":0.034825428,"dev-research":0.2822721464,"prompt-eng":0.4429257011,"data-quality":0.121101945,"ml-security":0.1663362257}}
{"text":"After the earlier NAS attempts to optimize only the prediction accuracy, Multi-Objective Neural architecture Search (MONAS) has been attracting attentions which considers more goals such as computational complexity, power consumption, and size of the network for optimization, reaching a trade-off between the accuracy and other features like the computational cost.","meta":{"url":"http://arxiv.org/abs/2307.09099v1"},"cats":{"new-dataset":0.024188942,"dev-research":0.2490517068,"prompt-eng":0.3797691421,"data-quality":0.0546247512,"ml-security":0.1218772046}}
{"text":"In this paper, we present an overview of principal and state-of-the-art works in the field of MONAS.","meta":{"url":"http://arxiv.org/abs/2307.09099v1"},"cats":{"new-dataset":0.185327077,"dev-research":0.251306578,"prompt-eng":0.432510715,"data-quality":0.0911349617,"ml-security":0.0758985829}}
{"text":"Starting from a well-categorized taxonomy and formulation for the NAS, we address and correct some miscategorizations in previous surveys of the NAS field.","meta":{"url":"http://arxiv.org/abs/2307.09099v1"},"cats":{"new-dataset":0.2982392658,"dev-research":0.2912208266,"prompt-eng":0.4072349885,"data-quality":0.2959060386,"ml-security":0.0775218162}}
{"text":"We also provide a list of all known objectives used and add a number of new ones and elaborate their specifications.","meta":{"url":"http://arxiv.org/abs/2307.09099v1"},"cats":{"new-dataset":0.3342696971,"dev-research":0.3996813454,"prompt-eng":0.4550452101,"data-quality":0.06809072,"ml-security":0.0734718073}}
{"text":"We have provides analyses about the most important objectives and shown that the stochastic properties of some the them should be differed from deterministic ones in the multi-objective optimization procedure of NAS.","meta":{"url":"http://arxiv.org/abs/2307.09099v1"},"cats":{"new-dataset":0.0197807481,"dev-research":0.197235081,"prompt-eng":0.4114479334,"data-quality":0.107293632,"ml-security":0.0870256185}}
{"text":"We finalize this paper with a number of future directions and topics in the field of MONAS.","meta":{"url":"http://arxiv.org/abs/2307.09099v1"},"cats":{"new-dataset":0.091843232,"dev-research":0.2306695849,"prompt-eng":0.4383009569,"data-quality":0.0950139428,"ml-security":0.0956046796}}
{"text":"Sequential decision-making under uncertainty is often associated with long feedback delays.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.024365451,"dev-research":0.2630735885,"prompt-eng":0.4405263482,"data-quality":0.082493884,"ml-security":0.08437262}}
{"text":"Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.0228668275,"dev-research":0.1970730762,"prompt-eng":0.4273673343,"data-quality":0.0756036983,"ml-security":0.2209180437}}
{"text":"This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.0240482616,"dev-research":0.1677452018,"prompt-eng":0.4028505224,"data-quality":0.0557608544,"ml-security":0.1094032194}}
{"text":"Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.0189610656,"dev-research":0.3747799669,"prompt-eng":0.4572707248,"data-quality":0.1120228471,"ml-security":0.1543851886}}
{"text":"We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.0380400268,"dev-research":0.1670993784,"prompt-eng":0.4312470876,"data-quality":0.0949111319,"ml-security":0.1356270674}}
{"text":"We model the causal relations by a directed graph in a stationary structural equation model.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.0323211694,"dev-research":0.2591598744,"prompt-eng":0.4143453747,"data-quality":0.1001613014,"ml-security":0.0738058245}}
{"text":"The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.0202020798,"dev-research":0.2317370063,"prompt-eng":0.3733667258,"data-quality":0.0360038025,"ml-security":0.1519670553}}
{"text":"We develop a policy that learns the structural dependencies from delayed feedback and utilizes that to optimize the decision-making while adapting to drifts.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.0385251496,"dev-research":0.2500267675,"prompt-eng":0.449949074,"data-quality":0.128938473,"ml-security":0.1481605689}}
{"text":"We prove a regret bound for the performance of the proposed algorithm.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.0340763902,"dev-research":0.1837609053,"prompt-eng":0.3673427723,"data-quality":0.1219061152,"ml-security":0.1408721944}}
{"text":"Besides, we evaluate our method via numerical analysis using synthetic and real-world datasets to detect the regions that contribute the most to the spread of Covid-19 in Italy.","meta":{"url":"http://arxiv.org/abs/2307.09093v1"},"cats":{"new-dataset":0.685143899,"dev-research":0.2911871998,"prompt-eng":0.3768276381,"data-quality":0.1186398433,"ml-security":0.1977048545}}
{"text":"In this paper we describe how to build software architectures as a composition of state machines, using ideas and principles from the field of Domain-Driven Design.","meta":{"url":"http://arxiv.org/abs/2307.09090v1"},"cats":{"new-dataset":0.0671076485,"dev-research":0.3488790951,"prompt-eng":0.4503740532,"data-quality":0.069522912,"ml-security":0.1591424887}}
{"text":"By definition, our approach is modular, allowing one to compose independent subcomponents to create bigger systems, and representable, allowing the implementation of a system to be kept in sync with its graphical representation.   ","meta":{"url":"http://arxiv.org/abs/2307.09090v1"},"cats":{"new-dataset":0.0424749372,"dev-research":0.3303239027,"prompt-eng":0.3953566223,"data-quality":0.0599517226,"ml-security":0.0631925381}}
{"text":"In addition to the design itself we introduce the Crem library, which provides a concrete state machine implementation that is both compositional and representable, Crem uses Haskell's advanced type-level features to allow users to specify allowed and forbidden state transitions, and to encode complex state machine -- and therefore domain-specific -- properties.","meta":{"url":"http://arxiv.org/abs/2307.09090v1"},"cats":{"new-dataset":0.0892168415,"dev-research":0.2633111161,"prompt-eng":0.4406333897,"data-quality":0.0662779444,"ml-security":0.0997697855}}
{"text":"Moreover, since Crem's state machines are representable, Crem can automatically generate graphical representations of systems from their domain implementations.","meta":{"url":"http://arxiv.org/abs/2307.09090v1"},"cats":{"new-dataset":0.0636026604,"dev-research":0.362893695,"prompt-eng":0.4713454421,"data-quality":0.1087122064,"ml-security":0.1232291559}}
{"text":"User post-click conversion prediction is of high interest to researchers and developers.","meta":{"url":"http://arxiv.org/abs/2307.09089v1"},"cats":{"new-dataset":0.0493527898,"dev-research":0.3513506619,"prompt-eng":0.4901089402,"data-quality":0.1159148463,"ml-security":0.1269868376}}
{"text":"Recent studies employ multi-task learning to tackle the selection bias and data sparsity problem, two severe challenges in post-click behavior prediction, by incorporating click data.","meta":{"url":"http://arxiv.org/abs/2307.09089v1"},"cats":{"new-dataset":0.0256358267,"dev-research":0.260165803,"prompt-eng":0.4452093917,"data-quality":0.1234151718,"ml-security":0.173005216}}
{"text":"However, prior works mainly focused on pointwise learning and the orders of labels (i.e., click and post-click) are not well explored, which naturally poses a listwise learning problem.","meta":{"url":"http://arxiv.org/abs/2307.09089v1"},"cats":{"new-dataset":0.0184916805,"dev-research":0.2562392328,"prompt-eng":0.4560902021,"data-quality":0.2321800151,"ml-security":0.1378343411}}
{"text":"Inspired by recent advances on differentiable sorting, in this paper, we propose a novel multi-task framework that leverages orders of user behaviors to predict user post-click conversion in an end-to-end approach.","meta":{"url":"http://arxiv.org/abs/2307.09089v1"},"cats":{"new-dataset":0.0588457687,"dev-research":0.3318952996,"prompt-eng":0.4797997442,"data-quality":0.0871483752,"ml-security":0.1036616098}}
{"text":"Specifically, we define an aggregation operator to combine predicted outputs of different tasks to a unified score, then we use the computed scores to model the label relations via differentiable sorting.","meta":{"url":"http://arxiv.org/abs/2307.09089v1"},"cats":{"new-dataset":0.058641759,"dev-research":0.2891050045,"prompt-eng":0.4574892108,"data-quality":0.239838978,"ml-security":0.0480431696}}
{"text":"Extensive experiments on public and industrial datasets show the superiority of our proposed model against competitive baselines.","meta":{"url":"http://arxiv.org/abs/2307.09089v1"},"cats":{"new-dataset":0.3083405465,"dev-research":0.259953238,"prompt-eng":0.3624668875,"data-quality":0.1276528431,"ml-security":0.2092046197}}
{"text":"The increasing popularity of certain programming languages has spurred the creation of ecosystem-specific package repositories and package managers.","meta":{"url":"http://arxiv.org/abs/2307.09087v1"},"cats":{"new-dataset":0.2730374215,"dev-research":0.434657133,"prompt-eng":0.4028051927,"data-quality":0.1143345777,"ml-security":0.0935908705}}
{"text":"Such repositories (e.g., NPM, PyPI) serve as public databases that users can query to retrieve packages for various functionalities, whereas package managers automatically handle dependency resolution and package installation on the client side.","meta":{"url":"http://arxiv.org/abs/2307.09087v1"},"cats":{"new-dataset":0.0741469977,"dev-research":0.2755008057,"prompt-eng":0.3921135052,"data-quality":0.0866026178,"ml-security":0.0832280838}}
{"text":"These mechanisms enhance software modularization and accelerate implementation.","meta":{"url":"http://arxiv.org/abs/2307.09087v1"},"cats":{"new-dataset":0.0111336003,"dev-research":0.490095714,"prompt-eng":0.4279195423,"data-quality":0.0686242217,"ml-security":0.0911096969}}
{"text":"However, they have become a target for malicious actors seeking to propagate malware on a large scale.   ","meta":{"url":"http://arxiv.org/abs/2307.09087v1"},"cats":{"new-dataset":0.0556010214,"dev-research":0.3850995205,"prompt-eng":0.4179353532,"data-quality":0.1176843247,"ml-security":0.5534533009}}
{"text":"In this work, we show how attackers can leverage capabilities of popular package managers and languages to achieve arbitrary code execution on victim machines, thereby realizing open-source software supply chain attacks.","meta":{"url":"http://arxiv.org/abs/2307.09087v1"},"cats":{"new-dataset":0.1883990235,"dev-research":0.4262128167,"prompt-eng":0.4445894461,"data-quality":0.162862884,"ml-security":0.7125776294}}
{"text":"Based on the analysis of 7 ecosystems, we identify 3 install-time and 5 runtime techniques, and we provide recommendations describing how to reduce the risk when consuming third-party dependencies.","meta":{"url":"http://arxiv.org/abs/2307.09087v1"},"cats":{"new-dataset":0.1415391626,"dev-research":0.3753007263,"prompt-eng":0.3600162866,"data-quality":0.0814595853,"ml-security":0.1303433472}}
{"text":"We will provide proof-of-concepts that demonstrate the identified techniques.","meta":{"url":"http://arxiv.org/abs/2307.09087v1"},"cats":{"new-dataset":0.0271343022,"dev-research":0.358080104,"prompt-eng":0.4812852973,"data-quality":0.2251085226,"ml-security":0.1674624755}}
{"text":"Furthermore, we describe evasion strategies employed by attackers to circumvent detection mechanisms.","meta":{"url":"http://arxiv.org/abs/2307.09087v1"},"cats":{"new-dataset":0.0315917309,"dev-research":0.3624384294,"prompt-eng":0.4824007746,"data-quality":0.2225678031,"ml-security":0.8843744453}}
{"text":"Despite being the current de-facto models in most NLP tasks, transformers are often limited to short sequences due to their quadratic attention complexity on the number of tokens.","meta":{"url":"http://arxiv.org/abs/2307.09084v1"},"cats":{"new-dataset":0.0363162343,"dev-research":0.2687770095,"prompt-eng":0.4204991126,"data-quality":0.1409372898,"ml-security":0.096348819}}
{"text":"Several attempts to address this issue were studied, either by reducing the cost of the self-attention computation or by modeling smaller sequences and combining them through a recurrence mechanism or using a new transformer model.","meta":{"url":"http://arxiv.org/abs/2307.09084v1"},"cats":{"new-dataset":0.0280977882,"dev-research":0.1714847277,"prompt-eng":0.4487896778,"data-quality":0.1205253617,"ml-security":0.0946187015}}
{"text":"In this paper, we suggest to take advantage of pre-trained sentence transformers to start from semantically meaningful embeddings of the individual sentences, and then combine them through a small attention layer that scales linearly with the document length.","meta":{"url":"http://arxiv.org/abs/2307.09084v1"},"cats":{"new-dataset":0.1079409692,"dev-research":0.2598483389,"prompt-eng":0.446069737,"data-quality":0.1754117073,"ml-security":0.0798829196}}
{"text":"We report the results obtained by this simple architecture on three standard document classification datasets.","meta":{"url":"http://arxiv.org/abs/2307.09084v1"},"cats":{"new-dataset":0.5114424995,"dev-research":0.241864459,"prompt-eng":0.3398748575,"data-quality":0.3857416109,"ml-security":0.1426626995}}
{"text":"When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration).","meta":{"url":"http://arxiv.org/abs/2307.09084v1"},"cats":{"new-dataset":0.01167159,"dev-research":0.2140802169,"prompt-eng":0.4259935317,"data-quality":0.0947082733,"ml-security":0.0535138225}}
{"text":"We also showcase that the studied architecture obtains better results when freezing the underlying transformers.","meta":{"url":"http://arxiv.org/abs/2307.09084v1"},"cats":{"new-dataset":0.0735109787,"dev-research":0.2694448796,"prompt-eng":0.4032226244,"data-quality":0.0603529138,"ml-security":0.1153958436}}
{"text":"A configuration that is useful when we need to avoid complete fine-tuning (e.g. when the same frozen transformer is shared by different applications).","meta":{"url":"http://arxiv.org/abs/2307.09084v1"},"cats":{"new-dataset":0.0137780688,"dev-research":0.2773044951,"prompt-eng":0.4026391903,"data-quality":0.0875948066,"ml-security":0.0755395387}}
{"text":"Finally, two additional experiments are provided to further evaluate the relevancy of the studied architecture over simpler baselines.","meta":{"url":"http://arxiv.org/abs/2307.09084v1"},"cats":{"new-dataset":0.0262469337,"dev-research":0.3383832317,"prompt-eng":0.4121958759,"data-quality":0.0713992044,"ml-security":0.0557650074}}
{"text":"Energy shortfall and electricity load shedding are the main problems for developing countries.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.1029156649,"dev-research":0.2888731712,"prompt-eng":0.3679665279,"data-quality":0.1250620082,"ml-security":0.1554210327}}
{"text":"The main causes are lack of management in the energy sector and the use of non-renewable energy sources.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.0371410595,"dev-research":0.2849775537,"prompt-eng":0.3482098806,"data-quality":0.1243812573,"ml-security":0.0988963913}}
{"text":"The improved energy management and use of renewable sources can be significant to resolve energy crisis.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.0323697872,"dev-research":0.2697349507,"prompt-eng":0.4085988393,"data-quality":0.1600327389,"ml-security":0.1199146985}}
{"text":"It is necessary to increase the use of renewable energy sources (RESs) to meet the increasing energy demand due to high prices of fossil-fuel based energy.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.0268373759,"dev-research":0.2790556714,"prompt-eng":0.4084112473,"data-quality":0.0775880003,"ml-security":0.0588354624}}
{"text":"Federated learning (FL) is the most emerging technique in the field of artificial intelligence.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.0634380159,"dev-research":0.2637386745,"prompt-eng":0.4042201109,"data-quality":0.1070930219,"ml-security":0.2177838543}}
{"text":"Federated learning helps to generate global model at server side by ensemble locally trained models at remote edges sites while preserving data privacy.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.096275881,"dev-research":0.2113973094,"prompt-eng":0.3996694202,"data-quality":0.1561758764,"ml-security":0.4054388482}}
{"text":"The global model used to predict energy demand to satisfy the needs of consumers.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.0621076078,"dev-research":0.2112026133,"prompt-eng":0.4528041333,"data-quality":0.1109024622,"ml-security":0.0804248084}}
{"text":"In this article, we have proposed Blockchain based safe distributed ledger technology for transaction of data between prosumer and consumer to ensure their transparency, traceability and security.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.2324670515,"dev-research":0.2398442544,"prompt-eng":0.3399374154,"data-quality":0.12720462,"ml-security":0.0997952803}}
{"text":"Furthermore, we have also proposed a Federated learning model to forecast the energy requirements of consumer and prosumer.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.075136128,"dev-research":0.201273403,"prompt-eng":0.4649208152,"data-quality":0.0897486272,"ml-security":0.0970943228}}
{"text":"Moreover, Blockchain has been used to store excess energy data from prosumer for better management of energy between prosumer and grid.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.1283574011,"dev-research":0.2683775453,"prompt-eng":0.3682569074,"data-quality":0.0927821183,"ml-security":0.07963055}}
{"text":"Lastly, the experiment results revealed that renewable energy sources have produced better and comparable results to other non-renewable energy resources.","meta":{"url":"http://arxiv.org/abs/2307.09080v1"},"cats":{"new-dataset":0.0642104497,"dev-research":0.2718715803,"prompt-eng":0.3787089966,"data-quality":0.141100592,"ml-security":0.0381893086}}
{"text":"Networked control systems are closed-loop feedback control systems containing system components that may be distributed geographically in different locations and interconnected via a communication network such as the Internet.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.049854229,"dev-research":0.3041256406,"prompt-eng":0.397482532,"data-quality":0.1142071358,"ml-security":0.1370856961}}
{"text":"The quality of network communication is a crucial factor that significantly affects the performance of remote control.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.0211423001,"dev-research":0.3483862827,"prompt-eng":0.4038828009,"data-quality":0.1341009255,"ml-security":0.0956131902}}
{"text":"This is due to the fact that network uncertainties can occur in the transmission of packets in the forward and backward channels of the system.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.0115329764,"dev-research":0.2920195461,"prompt-eng":0.3845051316,"data-quality":0.2069429692,"ml-security":0.1810374143}}
{"text":"The two most significant among these uncertainties are network time delay and packet loss.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.0282242625,"dev-research":0.3003438569,"prompt-eng":0.3896511376,"data-quality":0.2338283493,"ml-security":0.1308240052}}
{"text":"To overcome these challenges, the networked predictive control system has been proposed to provide improved performance and robustness using predictive controllers and compensation strategies.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.0249874004,"dev-research":0.2454890523,"prompt-eng":0.4219830962,"data-quality":0.1084630353,"ml-security":0.2535693185}}
{"text":"In particular, the model predictive control method is well-suited as an advanced approach compared to conventional methods.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.010257415,"dev-research":0.2496061782,"prompt-eng":0.4473393751,"data-quality":0.0649246869,"ml-security":0.126600839}}
{"text":"In this paper, a networked model predictive control system consisting of a model predictive control method and compensation strategies is implemented to control and stabilize a robot arm as a physical system.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.0274019004,"dev-research":0.2271349116,"prompt-eng":0.4261467327,"data-quality":0.0879176086,"ml-security":0.2050489732}}
{"text":"In particular, this work aims to analyze the performance of the system under the influence of network time delay and packet loss.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.0376873448,"dev-research":0.2859778319,"prompt-eng":0.3565794174,"data-quality":0.0940143336,"ml-security":0.1035935855}}
{"text":"Using appropriate performance and robustness metrics, an in-depth investigation of the impacts of these network uncertainties is performed.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.025400009,"dev-research":0.3311397407,"prompt-eng":0.3827777622,"data-quality":0.3099744601,"ml-security":0.1789790843}}
{"text":"Furthermore, the forward and backward channels of the network are examined in detail in this study.","meta":{"url":"http://arxiv.org/abs/2307.09076v1"},"cats":{"new-dataset":0.0157936646,"dev-research":0.2867098224,"prompt-eng":0.4036123717,"data-quality":0.1037994633,"ml-security":0.0941728281}}
{"text":"Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common.","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.1258731826,"dev-research":0.3117390163,"prompt-eng":0.395123836,"data-quality":0.0731033745,"ml-security":0.157099557}}
{"text":"The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems.","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.0241377129,"dev-research":0.2607869885,"prompt-eng":0.4347219408,"data-quality":0.1216463563,"ml-security":0.2408298627}}
{"text":"We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization.","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.0405813332,"dev-research":0.2392860571,"prompt-eng":0.3800264788,"data-quality":0.0825213059,"ml-security":0.1953806462}}
{"text":"The proposed approach, named DiTTO, is inspired by latent diffusion models.","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.0242047436,"dev-research":0.2168804621,"prompt-eng":0.406597374,"data-quality":0.0858297563,"ml-security":0.1371064275}}
{"text":"While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs.","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.0063321395,"dev-research":0.24415247,"prompt-eng":0.4739424759,"data-quality":0.0531974726,"ml-security":0.2143431765}}
{"text":"The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.   ","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.0128890585,"dev-research":0.2593911639,"prompt-eng":0.3888057567,"data-quality":0.0407094778,"ml-security":0.0833637821}}
{"text":"We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these problems.","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.1077886364,"dev-research":0.2649990125,"prompt-eng":0.348326145,"data-quality":0.0768030076,"ml-security":0.1174954873}}
{"text":"We also present a method to improve the performance of DiTTO by using fast sampling concepts from diffusion models.","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.074264581,"dev-research":0.1889665198,"prompt-eng":0.4126273819,"data-quality":0.0956255702,"ml-security":0.0724343649}}
{"text":"Finally, we show that DiTTO can accurately perform zero-shot super-resolution in time.","meta":{"url":"http://arxiv.org/abs/2307.09072v1"},"cats":{"new-dataset":0.1365863175,"dev-research":0.2297380603,"prompt-eng":0.3713187498,"data-quality":0.0910497962,"ml-security":0.0603938787}}
{"text":"In this paper, we propose PixelHuman, a novel human rendering model that generates animatable human scenes from a few images of a person with unseen identity, views, and poses.","meta":{"url":"http://arxiv.org/abs/2307.09070v1"},"cats":{"new-dataset":0.423312814,"dev-research":0.2147751821,"prompt-eng":0.3822332688,"data-quality":0.0849171207,"ml-security":0.0958600402}}
{"text":"Previous work have demonstrated reasonable performance in novel view and pose synthesis, but they rely on a large number of images to train and are trained per scene from videos, which requires significant amount of time to produce animatable scenes from unseen human images.","meta":{"url":"http://arxiv.org/abs/2307.09070v1"},"cats":{"new-dataset":0.2594734375,"dev-research":0.2483747685,"prompt-eng":0.373503502,"data-quality":0.1014217096,"ml-security":0.0829305284}}
{"text":"Our method differs from existing methods in that it can generalize to any input image for animatable human synthesis.","meta":{"url":"http://arxiv.org/abs/2307.09070v1"},"cats":{"new-dataset":0.0651063749,"dev-research":0.2609857678,"prompt-eng":0.4015691909,"data-quality":0.1243627223,"ml-security":0.0776445731}}
{"text":"Given a random pose sequence, our method synthesizes each target scene using a neural radiance field that is conditioned on a canonical representation and pose-aware pixel-aligned features, both of which can be obtained through deformation fields learned in a data-driven manner.","meta":{"url":"http://arxiv.org/abs/2307.09070v1"},"cats":{"new-dataset":0.3660850411,"dev-research":0.2178096915,"prompt-eng":0.4240378587,"data-quality":0.1079910045,"ml-security":0.1401655097}}
{"text":"Our experiments show that our method achieves state-of-the-art performance in multiview and novel pose synthesis from few-shot images.","meta":{"url":"http://arxiv.org/abs/2307.09070v1"},"cats":{"new-dataset":0.139830245,"dev-research":0.2695268783,"prompt-eng":0.3542907441,"data-quality":0.0823467276,"ml-security":0.0548854893}}
{"text":"Anonymous microblogging systems are known to be vulnerable to intersection attacks due to network churn.","meta":{"url":"http://arxiv.org/abs/2307.09069v1"},"cats":{"new-dataset":0.0727284219,"dev-research":0.3049195655,"prompt-eng":0.3641559921,"data-quality":0.2688719637,"ml-security":0.5832077085}}
{"text":"An adversary that monitors all communications can leverage the churn to learn who is publishing what with increasing confidence over time.","meta":{"url":"http://arxiv.org/abs/2307.09069v1"},"cats":{"new-dataset":0.085236896,"dev-research":0.337427691,"prompt-eng":0.434201356,"data-quality":0.1683687964,"ml-security":0.354455722}}
{"text":"In this paper, we propose a protocol for mitigating intersection attacks in anonymous microblogging systems by grouping users into anonymity sets based on similarities in their publishing behavior.","meta":{"url":"http://arxiv.org/abs/2307.09069v1"},"cats":{"new-dataset":0.1205871828,"dev-research":0.2706918179,"prompt-eng":0.3534303535,"data-quality":0.2927429521,"ml-security":0.4204923018}}
{"text":"The protocol provides a configurable communication schedule for users in each set to manage the inevitable trade-off between latency and bandwidth overhead.","meta":{"url":"http://arxiv.org/abs/2307.09069v1"},"cats":{"new-dataset":0.1165063051,"dev-research":0.2979269497,"prompt-eng":0.4060319049,"data-quality":0.0408778497,"ml-security":0.0604370077}}
{"text":"In our evaluation, we use real-world datasets from two popular microblogging platforms, Twitter and Reddit, to simulate user publishing behavior.","meta":{"url":"http://arxiv.org/abs/2307.09069v1"},"cats":{"new-dataset":0.5528535726,"dev-research":0.3027552737,"prompt-eng":0.4105243053,"data-quality":0.1725020836,"ml-security":0.1372935878}}
{"text":"The results demonstrate that the protocol can protect users against intersection attacks at low bandwidth overhead when the users adhere to communication schedules.","meta":{"url":"http://arxiv.org/abs/2307.09069v1"},"cats":{"new-dataset":0.1042806557,"dev-research":0.3335947026,"prompt-eng":0.3790592759,"data-quality":0.0711063743,"ml-security":0.4207492324}}
{"text":"In addition, the protocol can sustain a slow degradation in the size of the anonymity set over time under various churn rates.","meta":{"url":"http://arxiv.org/abs/2307.09069v1"},"cats":{"new-dataset":0.0601292811,"dev-research":0.2419764652,"prompt-eng":0.3124019801,"data-quality":0.1237996479,"ml-security":0.3058789203}}
{"text":"Multi-label image classification is a prediction task that aims to identify more than one label from a given image.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.1258738739,"dev-research":0.2462533103,"prompt-eng":0.4367146292,"data-quality":0.4535144214,"ml-security":0.0933251093}}
{"text":"This paper considers the semantic consistency of the latent space between the visual patch and linguistic label domains and introduces the conditional transport (CT) theory to bridge the acknowledged gap.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.0542765086,"dev-research":0.311988781,"prompt-eng":0.4589591801,"data-quality":0.3991949788,"ml-security":0.0604165914}}
{"text":"While recent cross-modal attention-based studies have attempted to align such two representations and achieved impressive performance, they required carefully-designed alignment modules and extra complex operations in the attention computation.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.0427996289,"dev-research":0.2530251617,"prompt-eng":0.4484496675,"data-quality":0.1255767767,"ml-security":0.0351658301}}
{"text":"We find that by formulating the multi-label classification as a CT problem, we can exploit the interactions between the image and label efficiently by minimizing the bidirectional CT cost.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.0861972169,"dev-research":0.2081269279,"prompt-eng":0.4132699267,"data-quality":0.3550637676,"ml-security":0.1239921021}}
{"text":"Specifically, after feeding the images and textual labels into the modality-specific encoders, we view each image as a mixture of patch embeddings and a mixture of label embeddings, which capture the local region features and the class prototypes, respectively.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.1210094714,"dev-research":0.298036477,"prompt-eng":0.4540531977,"data-quality":0.3727785018,"ml-security":0.1118605744}}
{"text":"CT is then employed to learn and align those two semantic sets by defining the forward and backward navigators.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.0943471636,"dev-research":0.3328618023,"prompt-eng":0.441480681,"data-quality":0.1313344872,"ml-security":0.0574781859}}
{"text":"Importantly, the defined navigators in CT distance model the similarities between patches and labels, which provides an interpretable tool to visualize the learned prototypes.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.1899474964,"dev-research":0.3800377084,"prompt-eng":0.4617773101,"data-quality":0.1448346152,"ml-security":0.0837892121}}
{"text":"Extensive experiments on three public image benchmarks show that the proposed model consistently outperforms the previous methods.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.0780410355,"dev-research":0.2087030814,"prompt-eng":0.4064990408,"data-quality":0.1984719944,"ml-security":0.0966000308}}
{"text":"Our code is available at https://github.com/keepgoingjkg/PatchCT.","meta":{"url":"http://arxiv.org/abs/2307.09066v1"},"cats":{"new-dataset":0.3631128026,"dev-research":0.3285106073,"prompt-eng":0.4234256216,"data-quality":0.2527650697,"ml-security":0.1026857357}}
{"text":"Graph convolutional networks (GCNs) enable end-to-end learning on graph structured data.","meta":{"url":"http://arxiv.org/abs/2307.09065v1"},"cats":{"new-dataset":0.2871130024,"dev-research":0.2431462682,"prompt-eng":0.3730515457,"data-quality":0.2018943134,"ml-security":0.097972513}}
{"text":"However, many works assume a given graph structure.","meta":{"url":"http://arxiv.org/abs/2307.09065v1"},"cats":{"new-dataset":0.0375847909,"dev-research":0.2546181567,"prompt-eng":0.3579445798,"data-quality":0.1184128629,"ml-security":0.0602214279}}
{"text":"When the input graph is noisy or unavailable, one approach is to construct or learn a latent graph structure.","meta":{"url":"http://arxiv.org/abs/2307.09065v1"},"cats":{"new-dataset":0.0754291479,"dev-research":0.2443199607,"prompt-eng":0.3913558305,"data-quality":0.2518245322,"ml-security":0.1074629985}}
{"text":"These methods typically fix the choice of node degree for the entire graph, which is suboptimal.","meta":{"url":"http://arxiv.org/abs/2307.09065v1"},"cats":{"new-dataset":0.0091722513,"dev-research":0.3095187538,"prompt-eng":0.3331656238,"data-quality":0.1871558657,"ml-security":0.0506925721}}
{"text":"Instead, we propose a novel end-to-end differentiable graph generator which builds graph topologies where each node selects both its neighborhood and its size.","meta":{"url":"http://arxiv.org/abs/2307.09065v1"},"cats":{"new-dataset":0.2250986878,"dev-research":0.3256468938,"prompt-eng":0.3680974611,"data-quality":0.1077596068,"ml-security":0.071725124}}
{"text":"Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or existing adjacency matrix with one that is learned, and optimized, as part of the general objective.","meta":{"url":"http://arxiv.org/abs/2307.09065v1"},"cats":{"new-dataset":0.0968079371,"dev-research":0.2721900696,"prompt-eng":0.3665806518,"data-quality":0.1319243874,"ml-security":0.0825321538}}
{"text":"As such it is applicable to any GCN.","meta":{"url":"http://arxiv.org/abs/2307.09065v1"},"cats":{"new-dataset":0.0287184384,"dev-research":0.1920239893,"prompt-eng":0.3596873243,"data-quality":0.1203016894,"ml-security":0.1039506426}}
{"text":"We integrate our module into trajectory prediction, point cloud classification and node classification pipelines resulting in improved accuracy over other structure-learning methods across a wide range of datasets and GCN backbones.","meta":{"url":"http://arxiv.org/abs/2307.09065v1"},"cats":{"new-dataset":0.2322862589,"dev-research":0.2118700615,"prompt-eng":0.3684101588,"data-quality":0.1231180096,"ml-security":0.1065893766}}
{"text":"Due to their quantitative nature, probabilistic programs pose non-trivial challenges for designing compositional and efficient program analyses.","meta":{"url":"http://arxiv.org/abs/2307.09064v1"},"cats":{"new-dataset":0.0361225357,"dev-research":0.3152380078,"prompt-eng":0.427763204,"data-quality":0.1190408746,"ml-security":0.1564935593}}
{"text":"Many analyses for probabilistic programs rely on iterative approximation.","meta":{"url":"http://arxiv.org/abs/2307.09064v1"},"cats":{"new-dataset":0.0248145719,"dev-research":0.2833644081,"prompt-eng":0.4192882992,"data-quality":0.116578022,"ml-security":0.1101468979}}
{"text":"This article presents an interprocedural dataflow-analysis framework, called NPA-PMA, for designing and implementing (partially) non-iterative program analyses of probabilistic programs with unstructured control-flow, nondeterminism, and general recursion.","meta":{"url":"http://arxiv.org/abs/2307.09064v1"},"cats":{"new-dataset":0.1635979356,"dev-research":0.3245791538,"prompt-eng":0.3844331658,"data-quality":0.0769531744,"ml-security":0.0805300634}}
{"text":"NPA-PMA is based on Newtonian Program Analysis (NPA), a generalization of Newton's method to solve equation systems over semirings.","meta":{"url":"http://arxiv.org/abs/2307.09064v1"},"cats":{"new-dataset":0.0226568055,"dev-research":0.2014066229,"prompt-eng":0.4019072786,"data-quality":0.0807526045,"ml-security":0.0659447792}}
{"text":"The key challenge for developing NPA-PMA is to handle multiple kinds of confluences in both the algebraic structures that specify analyses and the equation systems that encode control flow: semirings support a single confluence operation, whereas NPA-PMA involves three confluence operations (conditional, probabilistic, and nondeterministic).   ","meta":{"url":"http://arxiv.org/abs/2307.09064v1"},"cats":{"new-dataset":0.0125514889,"dev-research":0.2602319641,"prompt-eng":0.4331493514,"data-quality":0.098850798,"ml-security":0.0430735545}}
{"text":"Our work introduces $\\omega$-continuous pre-Markov algebras ($\\omega$PMAs) to factor out common parts of different analyses; adopts regular infinite-tree expressions to encode program-execution paths in control-flow hyper-graphs; and presents a linearization method that makes Newton's method applicable to the setting of regular-infinite-tree equations over $\\omega$PMAs.","meta":{"url":"http://arxiv.org/abs/2307.09064v1"},"cats":{"new-dataset":0.037191258,"dev-research":0.2395904485,"prompt-eng":0.421188497,"data-quality":0.0765155746,"ml-security":0.1179342669}}
{"text":"NPA-PMA allows analyses to supply a non-iterative strategy to solve linearized equations.","meta":{"url":"http://arxiv.org/abs/2307.09064v1"},"cats":{"new-dataset":0.0087925435,"dev-research":0.2080488016,"prompt-eng":0.377452,"data-quality":0.061061004,"ml-security":0.0646114745}}
{"text":"Our experimental evaluation demonstrates that (i) NPA-PMA holds considerable promise for outperforming Kleene iteration, and (ii) provides great generality for designing program analyses.","meta":{"url":"http://arxiv.org/abs/2307.09064v1"},"cats":{"new-dataset":0.0482315627,"dev-research":0.3208137209,"prompt-eng":0.4397062568,"data-quality":0.1159430944,"ml-security":0.0749381957}}
{"text":"The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions.","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.1446877013,"dev-research":0.2221053634,"prompt-eng":0.4249005317,"data-quality":0.141093378,"ml-security":0.0597882984}}
{"text":"The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities.","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.0436645337,"dev-research":0.3748282837,"prompt-eng":0.4252036905,"data-quality":0.2633661842,"ml-security":0.0466835609}}
{"text":"The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images.","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.0465535056,"dev-research":0.3611896724,"prompt-eng":0.4261185317,"data-quality":0.1988434366,"ml-security":0.0637440429}}
{"text":"To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.   ","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.0802798305,"dev-research":0.321105297,"prompt-eng":0.4549802655,"data-quality":0.3565550906,"ml-security":0.0735451192}}
{"text":"To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences.","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.2689421222,"dev-research":0.2699043997,"prompt-eng":0.4337356829,"data-quality":0.1667890106,"ml-security":0.1166294771}}
{"text":"Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge.","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.1586782313,"dev-research":0.2267405707,"prompt-eng":0.4376041366,"data-quality":0.1212265108,"ml-security":0.0656646722}}
{"text":"The Text-guided Image Restoration auxiliary task is proposed with the aim of implicitly mapping abstract textual entities to specific image regions, facilitating alignment between textual and visual embeddings.","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.0678027449,"dev-research":0.2762307861,"prompt-eng":0.4364149696,"data-quality":0.3021855245,"ml-security":0.0417835671}}
{"text":"Additionally, we introduce a cross-modal triplet loss tailored for handling hard samples, enhancing the model's ability to distinguish minor differences.   ","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.0477711721,"dev-research":0.2215930546,"prompt-eng":0.4367970501,"data-quality":0.2503647575,"ml-security":0.1224703464}}
{"text":"To focus the model on the key components within sentences, we propose a novel text data augmentation technique.","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.1875643714,"dev-research":0.3361920368,"prompt-eng":0.4169493899,"data-quality":0.2926731566,"ml-security":0.0858996351}}
{"text":"Our proposed methods achieve state-of-the-art results on three popular benchmark datasets, and the source code will be made publicly available shortly.","meta":{"url":"http://arxiv.org/abs/2307.09059v1"},"cats":{"new-dataset":0.8216844045,"dev-research":0.2657049035,"prompt-eng":0.3271683592,"data-quality":0.1670140061,"ml-security":0.0923949038}}
{"text":"Drug research and development are embracing translational research for its potential to increase the number of drugs successfully brought to clinical applications.","meta":{"url":"http://arxiv.org/abs/2307.09056v1"},"cats":{"new-dataset":0.0528869589,"dev-research":0.2715029874,"prompt-eng":0.369479027,"data-quality":0.1051190393,"ml-security":0.0734688134}}
{"text":"Using the publicly available PubMed database, we sought to describe the status of drug translational research, the distribution of translational lags for all drugs as well as the collaborations between basic science and clinical science in drug research.","meta":{"url":"http://arxiv.org/abs/2307.09056v1"},"cats":{"new-dataset":0.1471233947,"dev-research":0.22987086,"prompt-eng":0.3789199563,"data-quality":0.1318608681,"ml-security":0.0632869274}}
{"text":"For each drug, an indicator called Translational Lag was proposed to quantify the interval time from its first PubMed article to its first clinical article.","meta":{"url":"http://arxiv.org/abs/2307.09056v1"},"cats":{"new-dataset":0.1107865326,"dev-research":0.2404541184,"prompt-eng":0.4023277233,"data-quality":0.1237168005,"ml-security":0.0639383315}}
{"text":"Meanwhile, the triangle of biomedicine was also used to visualize the status and multidisciplinary collaboration of drug translational research.","meta":{"url":"http://arxiv.org/abs/2307.09056v1"},"cats":{"new-dataset":0.1283089964,"dev-research":0.3639948767,"prompt-eng":0.3418482861,"data-quality":0.1202476242,"ml-security":0.0333295278}}
{"text":"The results showed that only 18.1% (24,410) of drugs/compounds had been successfully entering clinical research.","meta":{"url":"http://arxiv.org/abs/2307.09056v1"},"cats":{"new-dataset":0.0368608159,"dev-research":0.2375005407,"prompt-eng":0.3513605479,"data-quality":0.1529156577,"ml-security":0.0895742769}}
{"text":"It averagely took 14.38 years (interquartile range, 4 to 21 years) for a drug from the initial basic discovery to its first clinical research.","meta":{"url":"http://arxiv.org/abs/2307.09056v1"},"cats":{"new-dataset":0.0614717015,"dev-research":0.2370850756,"prompt-eng":0.3358656538,"data-quality":0.0609313375,"ml-security":0.0635199345}}
{"text":"In addition, the results also revealed that, in drug research, there was rare cooperation between basic science and clinical science, which were more inclined to cooperate within disciplines.","meta":{"url":"http://arxiv.org/abs/2307.09056v1"},"cats":{"new-dataset":0.0169255518,"dev-research":0.2865009372,"prompt-eng":0.3628729338,"data-quality":0.1475439463,"ml-security":0.1013267089}}
{"text":"Deep neural network is a powerful tool for many tasks.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.0559942674,"dev-research":0.3443816635,"prompt-eng":0.3868251436,"data-quality":0.0905476483,"ml-security":0.1459814646}}
{"text":"Understanding why it is so successful and providing a mathematical explanation is an important problem and has been one popular research direction in past years.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.0196583458,"dev-research":0.3803925598,"prompt-eng":0.3833689549,"data-quality":0.0829913978,"ml-security":0.0833243014}}
{"text":"In the literature of mathematical analysis of deep deep neural networks, a lot of works are dedicated to establishing representation theories.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.0296884275,"dev-research":0.2671745907,"prompt-eng":0.3687646826,"data-quality":0.1051231983,"ml-security":0.2225865826}}
{"text":"How to make connections between deep neural networks and mathematical algorithms is still under development.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.0489373514,"dev-research":0.3356274453,"prompt-eng":0.3453903593,"data-quality":0.1093032459,"ml-security":0.2978004915}}
{"text":"In this paper, we give an algorithmic explanation for deep neural networks, especially in their connection with operator splitting and multigrid methods.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.0317734091,"dev-research":0.2855507519,"prompt-eng":0.321846896,"data-quality":0.1298411658,"ml-security":0.2316072129}}
{"text":"We show that with certain splitting strategies, operator-splitting methods have the same structure as networks.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.0167391879,"dev-research":0.2552549009,"prompt-eng":0.3366944515,"data-quality":0.137185956,"ml-security":0.1560355983}}
{"text":"Utilizing this connection and the Potts model for image segmentation, two networks inspired by operator-splitting methods are proposed.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.0394816456,"dev-research":0.2078877731,"prompt-eng":0.3798914729,"data-quality":0.1587298971,"ml-security":0.0916427528}}
{"text":"The two networks are essentially two operator-splitting algorithms solving the Potts model.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.0147782187,"dev-research":0.2078406097,"prompt-eng":0.3589410134,"data-quality":0.0838620819,"ml-security":0.1137351246}}
{"text":"Numerical experiments are presented to demonstrate the effectiveness of the proposed networks.","meta":{"url":"http://arxiv.org/abs/2307.09052v1"},"cats":{"new-dataset":0.012279353,"dev-research":0.2313553661,"prompt-eng":0.3799567336,"data-quality":0.1195409057,"ml-security":0.1282331445}}
{"text":"To improve the performance of multi-agent reinforcement learning under the constraint of wireless resources, we propose a message importance metric and design an importance-aware scheduling policy to effectively exchange messages.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0631137247,"dev-research":0.2229458181,"prompt-eng":0.4377805055,"data-quality":0.0717523641,"ml-security":0.1301911928}}
{"text":"The key insight is spending the precious communication resources on important messages.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0879027326,"dev-research":0.3816555332,"prompt-eng":0.394046479,"data-quality":0.0905343209,"ml-security":0.0961673317}}
{"text":"The message importance depends not only on the messages themselves, but also on the needs of agents who receive them.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.046396723,"dev-research":0.2936669609,"prompt-eng":0.4586810065,"data-quality":0.1535774569,"ml-security":0.1217616502}}
{"text":"Accordingly, we propose a query-message-based architecture, called QMNet.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.1312918108,"dev-research":0.2741245914,"prompt-eng":0.4495780966,"data-quality":0.106601459,"ml-security":0.0846115827}}
{"text":"Agents generate queries and messages with the environment observation.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.2189552676,"dev-research":0.31008928,"prompt-eng":0.4861375021,"data-quality":0.095515737,"ml-security":0.1462133795}}
{"text":"Sharing queries can help calculate message importance.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0615680826,"dev-research":0.3301740451,"prompt-eng":0.4515154516,"data-quality":0.1179148621,"ml-security":0.1053290651}}
{"text":"Exchanging messages can help agents cooperate better.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.06512962,"dev-research":0.3007370287,"prompt-eng":0.4172013502,"data-quality":0.1162293158,"ml-security":0.163188608}}
{"text":"Besides, we exploit the message importance to deal with random access collisions in decentralized systems.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0383844122,"dev-research":0.2883415425,"prompt-eng":0.4002842835,"data-quality":0.096338392,"ml-security":0.351907636}}
{"text":"Furthermore, a message prediction mechanism is proposed to compensate for messages that are not transmitted.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0315492405,"dev-research":0.2305367333,"prompt-eng":0.4601723705,"data-quality":0.2136837722,"ml-security":0.2975076515}}
{"text":"Finally, we evaluate the proposed schemes in a traffic junction environment, where only a fraction of agents can send messages due to limited wireless resources.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0674364544,"dev-research":0.2140847479,"prompt-eng":0.3768921418,"data-quality":0.0690220301,"ml-security":0.1369253069}}
{"text":"Results show that QMNet can extract valuable information to guarantee the system performance even when only $30\\%$ of agents can share messages.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0489951428,"dev-research":0.2814296768,"prompt-eng":0.4159275487,"data-quality":0.1088630258,"ml-security":0.2368657006}}
{"text":"By exploiting message prediction, the system can further save $40\\%$ of wireless resources.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0234438593,"dev-research":0.305006986,"prompt-eng":0.4359029689,"data-quality":0.0960098079,"ml-security":0.323311867}}
{"text":"The importance-aware decentralized multi-access mechanism can effectively avoid collisions, achieving almost the same performance as centralized scheduling.","meta":{"url":"http://arxiv.org/abs/2307.09051v1"},"cats":{"new-dataset":0.0236524858,"dev-research":0.2725367147,"prompt-eng":0.3576638743,"data-quality":0.0458813852,"ml-security":0.1287837612}}
{"text":"Transformer-based models have gained popularity in the field of natural language processing (NLP) and are extensively utilized in computer vision tasks and multi-modal models such as GPT4.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.069544176,"dev-research":0.2213975664,"prompt-eng":0.4338426644,"data-quality":0.165378811,"ml-security":0.0596552356}}
{"text":"This paper presents a novel method to enhance the explainability of Transformer-based image classification models.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.0317757335,"dev-research":0.264524935,"prompt-eng":0.4328810722,"data-quality":0.2878687052,"ml-security":0.218721242}}
{"text":"Our method aims to improve trust in classification results and empower users to gain a deeper understanding of the model for downstream tasks by providing visualizations of class-specific maps.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.0724142647,"dev-research":0.4380248466,"prompt-eng":0.4758818388,"data-quality":0.2346143591,"ml-security":0.3129094359}}
{"text":"We introduce two modules: the ``Relationship Weighted Out\" and the ``Cut\" modules.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.0928867844,"dev-research":0.2568368084,"prompt-eng":0.4025541873,"data-quality":0.134657173,"ml-security":0.055053028}}
{"text":"The ``Relationship Weighted Out\" module focuses on extracting class-specific information from intermediate layers, enabling us to highlight relevant features.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.0480199053,"dev-research":0.3524584263,"prompt-eng":0.4108131146,"data-quality":0.1889395456,"ml-security":0.1275744738}}
{"text":"Additionally, the ``Cut\" module performs fine-grained feature decomposition, taking into account factors such as position, texture, and color.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.0670037311,"dev-research":0.3643202864,"prompt-eng":0.3796826248,"data-quality":0.1240302202,"ml-security":0.046579075}}
{"text":"By integrating these modules, we generate dense class-specific visual explainability maps.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.150207213,"dev-research":0.4293444714,"prompt-eng":0.4528980715,"data-quality":0.2085565261,"ml-security":0.1334354048}}
{"text":"We validate our method with extensive qualitative and quantitative experiments on the ImageNet dataset.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.1854180554,"dev-research":0.2944290231,"prompt-eng":0.384852318,"data-quality":0.259308826,"ml-security":0.1736956367}}
{"text":"Furthermore, we conduct a large number of experiments on the LRN dataset, specifically designed for automatic driving danger alerts, to evaluate the explainability of our method in complex backgrounds.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.397683987,"dev-research":0.3353005752,"prompt-eng":0.430048277,"data-quality":0.2225607533,"ml-security":0.4313290655}}
{"text":"The results demonstrate a significant improvement over previous methods.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.0194640741,"dev-research":0.3735295226,"prompt-eng":0.3968754284,"data-quality":0.2032050592,"ml-security":0.0398217693}}
{"text":"Moreover, we conduct ablation experiments to validate the effectiveness of each module.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.033471373,"dev-research":0.2349345133,"prompt-eng":0.4330313735,"data-quality":0.1631960067,"ml-security":0.0459662639}}
{"text":"Through these experiments, we are able to confirm the respective contributions of each module, thus solidifying the overall effectiveness of our proposed approach.","meta":{"url":"http://arxiv.org/abs/2307.09050v1"},"cats":{"new-dataset":0.0166758963,"dev-research":0.2231434683,"prompt-eng":0.4595492149,"data-quality":0.1388101428,"ml-security":0.0575950374}}
{"text":"Federated learning enables learning from decentralized data sources without compromising privacy, which makes it a crucial technique.","meta":{"url":"http://arxiv.org/abs/2307.09048v1"},"cats":{"new-dataset":0.0553768879,"dev-research":0.2472858195,"prompt-eng":0.3719567693,"data-quality":0.1132686072,"ml-security":0.3569803356}}
{"text":"However, it is vulnerable to model poisoning attacks, where malicious clients interfere with the training process.","meta":{"url":"http://arxiv.org/abs/2307.09048v1"},"cats":{"new-dataset":0.0208124987,"dev-research":0.3593107387,"prompt-eng":0.4399257218,"data-quality":0.2550611104,"ml-security":0.8885240032}}
{"text":"Previous defense mechanisms have focused on the server-side by using careful model aggregation, but this may not be effective when the data is not identically distributed or when attackers can access the information of benign clients.","meta":{"url":"http://arxiv.org/abs/2307.09048v1"},"cats":{"new-dataset":0.0305302423,"dev-research":0.3029188506,"prompt-eng":0.3755546238,"data-quality":0.1432500286,"ml-security":0.7639831496}}
{"text":"In this paper, we propose a new defense mechanism that focuses on the client-side, called FedDefender, to help benign clients train robust local models and avoid the adverse impact of malicious model updates from attackers, even when a server-side defense cannot identify or remove adversaries.","meta":{"url":"http://arxiv.org/abs/2307.09048v1"},"cats":{"new-dataset":0.056980042,"dev-research":0.3812338205,"prompt-eng":0.4666400451,"data-quality":0.2580453435,"ml-security":0.8464394784}}
{"text":"Our method consists of two main components: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation.","meta":{"url":"http://arxiv.org/abs/2307.09048v1"},"cats":{"new-dataset":0.0849496604,"dev-research":0.3313916078,"prompt-eng":0.4073828238,"data-quality":0.2173375276,"ml-security":0.2281904319}}
{"text":"These components are used to find noise-resilient model parameters while accurately extracting knowledge from a potentially corrupted global model.","meta":{"url":"http://arxiv.org/abs/2307.09048v1"},"cats":{"new-dataset":0.0859746403,"dev-research":0.3220988241,"prompt-eng":0.4231176711,"data-quality":0.3088554915,"ml-security":0.2291480504}}
{"text":"Our client-side defense strategy has a flexible structure and can work in conjunction with any existing server-side strategies.","meta":{"url":"http://arxiv.org/abs/2307.09048v1"},"cats":{"new-dataset":0.0246280815,"dev-research":0.3446000701,"prompt-eng":0.4194215005,"data-quality":0.0471248103,"ml-security":0.3503409971}}
{"text":"Evaluations of real-world scenarios across multiple datasets show that the proposed method enhances the robustness of federated learning against model poisoning attacks.","meta":{"url":"http://arxiv.org/abs/2307.09048v1"},"cats":{"new-dataset":0.1788573889,"dev-research":0.3053714958,"prompt-eng":0.393775395,"data-quality":0.2975349011,"ml-security":0.8115307032}}
{"text":"Scholarly articles in mathematical fields feature mathematical statements such as theorems, propositions, etc., as well as their proofs.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.0409739359,"dev-research":0.3041210847,"prompt-eng":0.372865708,"data-quality":0.1373172134,"ml-security":0.1077183128}}
{"text":"Extracting them from the PDF representation of the articles requires understanding of scientific text along with visual and font-based indicators.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.1005009943,"dev-research":0.330997039,"prompt-eng":0.4252271597,"data-quality":0.2407388442,"ml-security":0.0418142157}}
{"text":"We pose this problem as a multimodal classification problem using text, font features, and bitmap image rendering of the PDF as different modalities.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.1563783865,"dev-research":0.2363017036,"prompt-eng":0.4048862186,"data-quality":0.2931591478,"ml-security":0.104750149}}
{"text":"In this paper we propose a multimodal machine learning approach for extraction of theorem-like environments and proofs, based on late fusion of features extracted by individual unimodal classifiers, taking into account the sequential succession of blocks in the document.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.207156545,"dev-research":0.3352916329,"prompt-eng":0.4133377774,"data-quality":0.2363227901,"ml-security":0.1515036202}}
{"text":"For the text modality, we pretrain a new language model on a 11 GB scientific corpus; experiments shows similar performance for our task than a model (RoBERTa) pretrained on 160 GB, with faster convergence while requiring much less fine-tuning data.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.3525070059,"dev-research":0.225716745,"prompt-eng":0.4433656677,"data-quality":0.1875424717,"ml-security":0.0591085082}}
{"text":"Font-based information relies on training a 128-cell LSTM on the sequence of font names and sizes within each block.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.1457067967,"dev-research":0.2518731885,"prompt-eng":0.4156979815,"data-quality":0.2264483512,"ml-security":0.1757686062}}
{"text":"Bitmap renderings are dealt with using an EfficientNetv2 deep network tuned to classify each image block.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.1213637152,"dev-research":0.3262930776,"prompt-eng":0.3928407639,"data-quality":0.1457703509,"ml-security":0.1131900398}}
{"text":"Finally, a simple CRF-based approach uses the features of the multimodal model along with information on block sequences.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.2422729746,"dev-research":0.2488977433,"prompt-eng":0.4354288571,"data-quality":0.1331652244,"ml-security":0.0430911762}}
{"text":"Experimental results show the benefits of using a multimodal approach vs any single modality, as well as major performance improvements using the CRF modeling of block sequences.","meta":{"url":"http://arxiv.org/abs/2307.09047v1"},"cats":{"new-dataset":0.0618110428,"dev-research":0.2690176116,"prompt-eng":0.3941265792,"data-quality":0.0856733794,"ml-security":0.0371539397}}
{"text":"6G is the next-generation intelligent and integrated digital information infrastructure, characterized by ubiquitous interconnection, native intelligence, multi-dimensional perception, global coverage, green and low-carbon, native network security, etc. 6G will realize the transition from serving people and people-things communication to supporting the efficient connection of intelligent agents, and comprehensively leading the digital, intelligent and green transformation of the economy and the society.","meta":{"url":"http://arxiv.org/abs/2307.09045v2"},"cats":{"new-dataset":0.2186635143,"dev-research":0.2768186058,"prompt-eng":0.3817026854,"data-quality":0.107176712,"ml-security":0.0692072148}}
{"text":"As the core support system for mobile communication network, 6G OSS needs to achieve high-level network automation, intelligence and digital twinning capabilities to achieve end-to-end autonomous network operation and maintenance, support the operation of typical 6G business scenarios and play a greater social responsibility in the fields of environment, society, and governance (ESG).This paper provides a detailed introduction to the overall vision, potential key technologies, and functional architecture of 6G OSS .","meta":{"url":"http://arxiv.org/abs/2307.09045v2"},"cats":{"new-dataset":0.1399246402,"dev-research":0.3181412208,"prompt-eng":0.3716190017,"data-quality":0.0896047763,"ml-security":0.0856106924}}
{"text":"It also presents an evolutionary roadmap and technological prospects for the OSS from 5G to 6G.","meta":{"url":"http://arxiv.org/abs/2307.09045v2"},"cats":{"new-dataset":0.0675087173,"dev-research":0.3352651168,"prompt-eng":0.3843560414,"data-quality":0.0550001479,"ml-security":0.0909673005}}
{"text":"For the SLAM system in robotics and autonomous driving, the accuracy of front-end odometry and back-end loop-closure detection determine the whole intelligent system performance.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.0841567275,"dev-research":0.2850524161,"prompt-eng":0.3874003065,"data-quality":0.1777192986,"ml-security":0.0783000777}}
{"text":"But the LiDAR-SLAM could be disturbed by current scene moving objects, resulting in drift errors and even loop-closure failure.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.0371541325,"dev-research":0.2649943956,"prompt-eng":0.3330526593,"data-quality":0.280011852,"ml-security":0.1284748772}}
{"text":"Thus, the ability to detect and segment moving objects is essential for high-precision positioning and building a consistent map.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.0560043006,"dev-research":0.3023764272,"prompt-eng":0.3956397848,"data-quality":0.1236368463,"ml-security":0.0419365652}}
{"text":"In this paper, we address the problem of moving object segmentation from 3D LiDAR scans to improve the odometry and loop-closure accuracy of SLAM.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.0868848222,"dev-research":0.234648674,"prompt-eng":0.342497086,"data-quality":0.172015832,"ml-security":0.0632858538}}
{"text":"We propose a novel 3D Sequential Moving-Object-Segmentation (3D-SeqMOS) method that can accurately segment the scene into moving and static objects, such as moving and static cars.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.23266706,"dev-research":0.2252324379,"prompt-eng":0.3479170893,"data-quality":0.1037300968,"ml-security":0.0511438045}}
{"text":"Different from the existing projected-image method, we process the raw 3D point cloud and build a 3D convolution neural network for MOS task.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.1402640727,"dev-research":0.2349426027,"prompt-eng":0.351928725,"data-quality":0.0689440149,"ml-security":0.0797592545}}
{"text":"In addition, to make full use of the spatio-temporal information of point cloud, we propose a point cloud residual mechanism using the spatial features of current scan and the temporal features of previous residual scans.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.1494674699,"dev-research":0.2482982351,"prompt-eng":0.3585926274,"data-quality":0.1304332411,"ml-security":0.0859057702}}
{"text":"Besides, we build a complete SLAM framework to verify the effectiveness and accuracy of 3D-SeqMOS.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.1560238484,"dev-research":0.2574692268,"prompt-eng":0.3650608058,"data-quality":0.0918914386,"ml-security":0.0416239747}}
{"text":"Experiments on SemanticKITTI dataset show that our proposed 3D-SeqMOS method can effectively detect moving objects and improve the accuracy of LiDAR odometry and loop-closure detection.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.2616338324,"dev-research":0.2642008526,"prompt-eng":0.3220056503,"data-quality":0.1566285321,"ml-security":0.0715251019}}
{"text":"The test results show our 3D-SeqMOS outperforms the state-of-the-art method by 12.4%.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.081287129,"dev-research":0.2650522307,"prompt-eng":0.3628624038,"data-quality":0.0777580496,"ml-security":0.0468575124}}
{"text":"We extend the proposed method to the SemanticKITTI: Moving Object Segmentation competition and achieve the 2nd in the leaderboard, showing its effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.09044v1"},"cats":{"new-dataset":0.117683741,"dev-research":0.2446985177,"prompt-eng":0.3926859715,"data-quality":0.1860222272,"ml-security":0.0814871758}}
{"text":"Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.0601612875,"dev-research":0.2313877212,"prompt-eng":0.530472473,"data-quality":0.1095239619,"ml-security":0.0615462132}}
{"text":"However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.073803033,"dev-research":0.3650951375,"prompt-eng":0.3627528373,"data-quality":0.179886609,"ml-security":0.0534939649}}
{"text":"Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.1696278321,"dev-research":0.3294924557,"prompt-eng":0.4517509255,"data-quality":0.1128123843,"ml-security":0.0886317712}}
{"text":"Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.1223492892,"dev-research":0.3420145986,"prompt-eng":0.4620610854,"data-quality":0.1184059162,"ml-security":0.0790852002}}
{"text":"This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score).","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.0475318349,"dev-research":0.3223342978,"prompt-eng":0.4760607552,"data-quality":0.1441279493,"ml-security":0.0653993613}}
{"text":"With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.1549721104,"dev-research":0.162307126,"prompt-eng":0.4702743523,"data-quality":0.0877561698,"ml-security":0.0753838974}}
{"text":"Most achieved above-average EQ scores, with GPT-4 exceeding 89% of human participants with an EQ of 117.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.0691903189,"dev-research":0.2488811483,"prompt-eng":0.408905406,"data-quality":0.076317609,"ml-security":0.0514966494}}
{"text":"Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.0102643277,"dev-research":0.2519568347,"prompt-eng":0.4882145188,"data-quality":0.0800772847,"ml-security":0.1516157043}}
{"text":"In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.0164725684,"dev-research":0.1885373898,"prompt-eng":0.4678244035,"data-quality":0.0623918314,"ml-security":0.1056945846}}
{"text":"In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence.","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.0909142247,"dev-research":0.2554706714,"prompt-eng":0.4957873004,"data-quality":0.070211457,"ml-security":0.0848800971}}
{"text":"Project website: https://emotional-intelligence.github.io/","meta":{"url":"http://arxiv.org/abs/2307.09042v1"},"cats":{"new-dataset":0.3556529859,"dev-research":0.4113508991,"prompt-eng":0.3966941506,"data-quality":0.1183859142,"ml-security":0.1047566809}}
{"text":"For problems in image processing and many other fields, a large class of effective neural networks has encoder-decoder-based architectures.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0443335167,"dev-research":0.2517874186,"prompt-eng":0.3965426499,"data-quality":0.1369609249,"ml-security":0.1235736715}}
{"text":"Although these networks have made impressive performances, mathematical explanations of their architectures are still underdeveloped.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0218354968,"dev-research":0.4142165554,"prompt-eng":0.3436672608,"data-quality":0.0973448234,"ml-security":0.2362298699}}
{"text":"In this paper, we study the encoder-decoder-based network architecture from the algorithmic perspective and provide a mathematical explanation.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0197370814,"dev-research":0.2915903176,"prompt-eng":0.3553775175,"data-quality":0.1257643838,"ml-security":0.1558559805}}
{"text":"We use the two-phase Potts model for image segmentation as an example for our explanations.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0685789502,"dev-research":0.176992723,"prompt-eng":0.4394223259,"data-quality":0.1625160854,"ml-security":0.053319936}}
{"text":"We associate the segmentation problem with a control problem in the continuous setting.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.144989942,"dev-research":0.1789241557,"prompt-eng":0.3969663916,"data-quality":0.2136476902,"ml-security":0.1036946503}}
{"text":"Then, multigrid method and operator splitting scheme, the PottsMGNet, are used to discretize the continuous control model.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0249786692,"dev-research":0.2103852998,"prompt-eng":0.385353215,"data-quality":0.0443545315,"ml-security":0.0707656487}}
{"text":"We show that the resulting discrete PottsMGNet is equivalent to an encoder-decoder-based network.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0636895877,"dev-research":0.2108339002,"prompt-eng":0.3927601532,"data-quality":0.1272498309,"ml-security":0.1419518005}}
{"text":"With minor modifications, it is shown that a number of the popular encoder-decoder-based neural networks are just instances of the proposed PottsMGNet.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0927443792,"dev-research":0.2471867573,"prompt-eng":0.4040009126,"data-quality":0.1272187548,"ml-security":0.2172902681}}
{"text":"By incorporating the Soft-Threshold-Dynamics into the PottsMGNet as a regularizer, the PottsMGNet has shown to be robust with the network parameters such as network width and depth and achieved remarkable performance on datasets with very large noise.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0851511719,"dev-research":0.1869691304,"prompt-eng":0.3650637509,"data-quality":0.2507444625,"ml-security":0.1980442587}}
{"text":"In nearly all our experiments, the new network always performs better or as good on accuracy and dice score than existing networks for image segmentation.","meta":{"url":"http://arxiv.org/abs/2307.09039v1"},"cats":{"new-dataset":0.0613758012,"dev-research":0.29695433,"prompt-eng":0.3844600318,"data-quality":0.2434842206,"ml-security":0.0832061487}}
{"text":"Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts.","meta":{"url":"http://arxiv.org/abs/2307.09036v1"},"cats":{"new-dataset":0.2659490054,"dev-research":0.2408526213,"prompt-eng":0.599661611,"data-quality":0.2319826634,"ml-security":0.1038388874}}
{"text":"However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language.","meta":{"url":"http://arxiv.org/abs/2307.09036v1"},"cats":{"new-dataset":0.0760893714,"dev-research":0.3381374207,"prompt-eng":0.5638255061,"data-quality":0.222485931,"ml-security":0.1160858528}}
{"text":"This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts.","meta":{"url":"http://arxiv.org/abs/2307.09036v1"},"cats":{"new-dataset":0.1764602847,"dev-research":0.3794500445,"prompt-eng":0.5733797209,"data-quality":0.1621677657,"ml-security":0.0812834241}}
{"text":"The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords.","meta":{"url":"http://arxiv.org/abs/2307.09036v1"},"cats":{"new-dataset":0.0745276288,"dev-research":0.307430925,"prompt-eng":0.6198456147,"data-quality":0.1476830994,"ml-security":0.194071668}}
{"text":"To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration.","meta":{"url":"http://arxiv.org/abs/2307.09036v1"},"cats":{"new-dataset":0.1248860815,"dev-research":0.314010217,"prompt-eng":0.5830930065,"data-quality":0.1035744007,"ml-security":0.0594895311}}
{"text":"Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.","meta":{"url":"http://arxiv.org/abs/2307.09036v1"},"cats":{"new-dataset":0.0976612099,"dev-research":0.4209222776,"prompt-eng":0.5559725097,"data-quality":0.1640498513,"ml-security":0.0701315648}}
{"text":"We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using online self-supervision by leveraging texture and motion cues as supervisory signals.","meta":{"url":"http://arxiv.org/abs/2307.09027v1"},"cats":{"new-dataset":0.1669812347,"dev-research":0.2343535537,"prompt-eng":0.4667817395,"data-quality":0.157720502,"ml-security":0.1189135466}}
{"text":"This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tasks such as visual navigation, bathymetry, and flow tracking at night.","meta":{"url":"http://arxiv.org/abs/2307.09027v1"},"cats":{"new-dataset":0.0617466564,"dev-research":0.227738744,"prompt-eng":0.4176562285,"data-quality":0.0625684852,"ml-security":0.074446609}}
{"text":"Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application of conventional supervised and unsupervised methods.","meta":{"url":"http://arxiv.org/abs/2307.09027v1"},"cats":{"new-dataset":0.2368546814,"dev-research":0.2118982046,"prompt-eng":0.4112815626,"data-quality":0.1919337916,"ml-security":0.1922124565}}
{"text":"In this work, we curate the first aerial thermal near-shore dataset, show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded computing platform.","meta":{"url":"http://arxiv.org/abs/2307.09027v1"},"cats":{"new-dataset":0.5434693577,"dev-research":0.2234812818,"prompt-eng":0.4041539259,"data-quality":0.147831638,"ml-security":0.147987384}}
{"text":"Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation.","meta":{"url":"http://arxiv.org/abs/2307.09027v1"},"cats":{"new-dataset":0.7673991965,"dev-research":0.2096905101,"prompt-eng":0.3957057206,"data-quality":0.0966162187,"ml-security":0.0718386036}}
{"text":"Recent 2D-to-3D human pose estimation (HPE) utilizes temporal consistency across sequences to alleviate the depth ambiguity problem but ignore the action related prior knowledge hidden in the pose sequence.","meta":{"url":"http://arxiv.org/abs/2307.09026v1"},"cats":{"new-dataset":0.1087282152,"dev-research":0.2371209563,"prompt-eng":0.3731852176,"data-quality":0.0909086611,"ml-security":0.0739881057}}
{"text":"In this paper, we propose a plug-and-play module named Action Prompt Module (APM) that effectively mines different kinds of action clues for 3D HPE.","meta":{"url":"http://arxiv.org/abs/2307.09026v1"},"cats":{"new-dataset":0.1710370927,"dev-research":0.395010053,"prompt-eng":0.5108699343,"data-quality":0.0988030692,"ml-security":0.1256838854}}
{"text":"The highlight is that, the mining scheme of APM can be widely adapted to different frameworks and bring consistent benefits.","meta":{"url":"http://arxiv.org/abs/2307.09026v1"},"cats":{"new-dataset":0.0178809067,"dev-research":0.278936075,"prompt-eng":0.429193661,"data-quality":0.0667986243,"ml-security":0.1007166703}}
{"text":"Specifically, we first present a novel Action-related Text Prompt module (ATP) that directly embeds action labels and transfers the rich language information in the label to the pose sequence.","meta":{"url":"http://arxiv.org/abs/2307.09026v1"},"cats":{"new-dataset":0.2598577607,"dev-research":0.2795559841,"prompt-eng":0.5301865623,"data-quality":0.2230866904,"ml-security":0.0945624559}}
{"text":"Besides, we further introduce Action-specific Pose Prompt module (APP) to mine the position-aware pose pattern of each action, and exploit the correlation between the mined patterns and input pose sequence for further pose refinement.","meta":{"url":"http://arxiv.org/abs/2307.09026v1"},"cats":{"new-dataset":0.2071270686,"dev-research":0.2749262719,"prompt-eng":0.4650953988,"data-quality":0.0844345417,"ml-security":0.1048666533}}
{"text":"Experiments show that APM can improve the performance of most video-based 2D-to-3D HPE frameworks by a large margin.","meta":{"url":"http://arxiv.org/abs/2307.09026v1"},"cats":{"new-dataset":0.0887096663,"dev-research":0.2762950654,"prompt-eng":0.3857239778,"data-quality":0.0528632663,"ml-security":0.0596771734}}
{"text":"Facial expression recognition (FER) remains a challenging task due to the ambiguity of expressions.","meta":{"url":"http://arxiv.org/abs/2307.09023v2"},"cats":{"new-dataset":0.0215451552,"dev-research":0.267840732,"prompt-eng":0.3845691859,"data-quality":0.2143578626,"ml-security":0.09083374}}
{"text":"The derived noisy labels significantly harm the performance in real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09023v2"},"cats":{"new-dataset":0.0489071538,"dev-research":0.3611094765,"prompt-eng":0.4295778836,"data-quality":0.6773995856,"ml-security":0.2761215883}}
{"text":"To address this issue, we present a new FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks to mitigate the impact of label noise from two perspectives.","meta":{"url":"http://arxiv.org/abs/2307.09023v2"},"cats":{"new-dataset":0.0387362056,"dev-research":0.2441137041,"prompt-eng":0.4276826228,"data-quality":0.4196970556,"ml-security":0.1351652237}}
{"text":"Firstly, LA-Net uses landmark information to suppress the uncertainty in expression space and constructs the label distribution of each sample by neighborhood aggregation, which in turn improves the quality of training supervision.","meta":{"url":"http://arxiv.org/abs/2307.09023v2"},"cats":{"new-dataset":0.0331925669,"dev-research":0.3080496853,"prompt-eng":0.4050692084,"data-quality":0.377688909,"ml-security":0.1395803511}}
{"text":"Secondly, the model incorporates landmark information into expression representations using the devised expression-landmark contrastive loss.","meta":{"url":"http://arxiv.org/abs/2307.09023v2"},"cats":{"new-dataset":0.0214583726,"dev-research":0.3064844392,"prompt-eng":0.4340966412,"data-quality":0.2426430856,"ml-security":0.0802565062}}
{"text":"The enhanced expression feature extractor can be less susceptible to label noise.","meta":{"url":"http://arxiv.org/abs/2307.09023v2"},"cats":{"new-dataset":0.0183380141,"dev-research":0.3949878024,"prompt-eng":0.4361837827,"data-quality":0.655703816,"ml-security":0.1452578893}}
{"text":"Our method can be integrated with any deep neural network for better training supervision without introducing extra inference costs.","meta":{"url":"http://arxiv.org/abs/2307.09023v2"},"cats":{"new-dataset":0.0419416554,"dev-research":0.2575622675,"prompt-eng":0.4477710778,"data-quality":0.2272383582,"ml-security":0.1259467668}}
{"text":"We conduct extensive experiments on both in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net achieves state-of-the-art performance.","meta":{"url":"http://arxiv.org/abs/2307.09023v2"},"cats":{"new-dataset":0.5788329094,"dev-research":0.2379773587,"prompt-eng":0.3186108722,"data-quality":0.2573971284,"ml-security":0.2191347431}}
{"text":"Dialogue-based human-AI collaboration can revolutionize collaborative problem-solving, creative exploration, and social support.","meta":{"url":"http://arxiv.org/abs/2307.09021v1"},"cats":{"new-dataset":0.0961826424,"dev-research":0.4037048988,"prompt-eng":0.3906783789,"data-quality":0.1005617073,"ml-security":0.0721802498}}
{"text":"To realize this goal, the development of automated agents proficient in skills such as negotiating, following instructions, establishing common ground, and progressing shared tasks is essential.","meta":{"url":"http://arxiv.org/abs/2307.09021v1"},"cats":{"new-dataset":0.0461293815,"dev-research":0.3510653315,"prompt-eng":0.4813556753,"data-quality":0.0648073361,"ml-security":0.1034572371}}
{"text":"This survey begins by reviewing the evolution of dialogue management paradigms in collaborative dialogue systems, from traditional handcrafted and information-state based methods to AI planning-inspired approaches.","meta":{"url":"http://arxiv.org/abs/2307.09021v1"},"cats":{"new-dataset":0.0994376179,"dev-research":0.3102629764,"prompt-eng":0.3884974492,"data-quality":0.0805406088,"ml-security":0.0801232745}}
{"text":"It then shifts focus to contemporary data-driven dialogue management techniques, which seek to transfer deep learning successes from form-filling and open-domain settings to collaborative contexts.","meta":{"url":"http://arxiv.org/abs/2307.09021v1"},"cats":{"new-dataset":0.1585506204,"dev-research":0.3740716618,"prompt-eng":0.3698239457,"data-quality":0.1525817476,"ml-security":0.1534329917}}
{"text":"The paper proceeds to analyze a selected set of recent works that apply neural approaches to collaborative dialogue management, spotlighting prevailing trends in the field.","meta":{"url":"http://arxiv.org/abs/2307.09021v1"},"cats":{"new-dataset":0.1405565991,"dev-research":0.3293472169,"prompt-eng":0.3774621318,"data-quality":0.1454135635,"ml-security":0.0859597515}}
{"text":"This survey hopes to provide foundational background for future advancements in collaborative dialogue management, particularly as the dialogue systems community continues to embrace the potential of large language models.","meta":{"url":"http://arxiv.org/abs/2307.09021v1"},"cats":{"new-dataset":0.2376504777,"dev-research":0.2931878699,"prompt-eng":0.4342244406,"data-quality":0.1393889534,"ml-security":0.0721824004}}
{"text":"Facial style transfer has been quite popular among researchers due to the rise of emerging technologies such as eXtended Reality (XR), Metaverse, and Non-Fungible Tokens (NFTs).","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0144757469,"dev-research":0.2318987017,"prompt-eng":0.3706601889,"data-quality":0.0888759683,"ml-security":0.1048869932}}
{"text":"Furthermore, StyleGAN methods along with transfer-learning strategies have reduced the problem of limited data to some extent.","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0350268288,"dev-research":0.2358334244,"prompt-eng":0.3585000755,"data-quality":0.1953150261,"ml-security":0.1817135854}}
{"text":"However, most of the StyleGAN methods overfit the styles while adding artifacts to facial images.","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0155343307,"dev-research":0.3154262812,"prompt-eng":0.3646302676,"data-quality":0.3256408437,"ml-security":0.1576958042}}
{"text":"In this paper, we propose a facial pose awareness and style transfer (Face-PAST) network that preserves facial details and structures while generating high-quality stylized images.","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0595781494,"dev-research":0.240944656,"prompt-eng":0.3937190722,"data-quality":0.1378925541,"ml-security":0.1557248061}}
{"text":"Dual StyleGAN inspires our work, but in contrast, our work uses a pre-trained style generation network in an external style pass with a residual modulation block instead of a transform coding block.","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0315339154,"dev-research":0.2713992248,"prompt-eng":0.4448206118,"data-quality":0.1272469274,"ml-security":0.1102436001}}
{"text":"Furthermore, we use the gated mapping unit and facial structure, identity, and segmentation losses to preserve the facial structure and details.","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0426646718,"dev-research":0.2110073313,"prompt-eng":0.3946328555,"data-quality":0.1579972064,"ml-security":0.1148855002}}
{"text":"This enables us to train the network with a very limited amount of data while generating high-quality stylized images.","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0667693774,"dev-research":0.2743603849,"prompt-eng":0.4100413901,"data-quality":0.1152828178,"ml-security":0.1345179616}}
{"text":"Our training process adapts curriculum learning strategy to perform efficient and flexible style mixing in the generative space.","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0375189587,"dev-research":0.2567408654,"prompt-eng":0.4335298912,"data-quality":0.1056989912,"ml-security":0.0867439338}}
{"text":"We perform extensive experiments to show the superiority of Face-PAST in comparison to existing state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.09020v1"},"cats":{"new-dataset":0.0210281718,"dev-research":0.3104661845,"prompt-eng":0.4153232498,"data-quality":0.1474140708,"ml-security":0.1582431769}}
{"text":"Time series prediction plays a crucial role in various industrial fields.","meta":{"url":"http://arxiv.org/abs/2307.09019v1"},"cats":{"new-dataset":0.0385391581,"dev-research":0.2575865709,"prompt-eng":0.4008022055,"data-quality":0.0844667518,"ml-security":0.1552802921}}
{"text":"In recent years, neural networks with a transformer backbone have achieved remarkable success in many domains, including computer vision and NLP.","meta":{"url":"http://arxiv.org/abs/2307.09019v1"},"cats":{"new-dataset":0.066853858,"dev-research":0.2264126979,"prompt-eng":0.3869709016,"data-quality":0.1220472004,"ml-security":0.1589594506}}
{"text":"In time series analysis domain, some studies have suggested that even the simplest MLP networks outperform advanced transformer-based networks on time series forecast tasks.","meta":{"url":"http://arxiv.org/abs/2307.09019v1"},"cats":{"new-dataset":0.0231169016,"dev-research":0.2313449565,"prompt-eng":0.3714441581,"data-quality":0.0759897561,"ml-security":0.1415316308}}
{"text":"However, we believe these findings indicate there to be low-rank properties in time series sequences.","meta":{"url":"http://arxiv.org/abs/2307.09019v1"},"cats":{"new-dataset":0.1226235498,"dev-research":0.2435040597,"prompt-eng":0.3770878363,"data-quality":0.1416189232,"ml-security":0.1489515072}}
{"text":"In this paper, we consider the low-pass characteristics of transformers and try to incorporate the advantages of MLP.","meta":{"url":"http://arxiv.org/abs/2307.09019v1"},"cats":{"new-dataset":0.0216106134,"dev-research":0.2179347697,"prompt-eng":0.4051067772,"data-quality":0.1066954954,"ml-security":0.1344524771}}
{"text":"We adopt skip-layer connections inspired by Unet into traditional transformer backbone, thus preserving high-frequency context from input to output, namely U-shaped Transformer.","meta":{"url":"http://arxiv.org/abs/2307.09019v1"},"cats":{"new-dataset":0.0179981742,"dev-research":0.248418169,"prompt-eng":0.4080800059,"data-quality":0.1043005444,"ml-security":0.0805354666}}
{"text":"We introduce patch merge and split operation to extract features with different scales and use larger datasets to fully make use of the transformer backbone.","meta":{"url":"http://arxiv.org/abs/2307.09019v1"},"cats":{"new-dataset":0.2348224634,"dev-research":0.3081098688,"prompt-eng":0.3731936259,"data-quality":0.1236380099,"ml-security":0.101178017}}
{"text":"Our experiments demonstrate that the model performs at an advanced level across multiple datasets with relatively low cost.","meta":{"url":"http://arxiv.org/abs/2307.09019v1"},"cats":{"new-dataset":0.2004114497,"dev-research":0.2608256901,"prompt-eng":0.3905127294,"data-quality":0.102345316,"ml-security":0.1169756134}}
{"text":"This study aims to explore user acceptance of Autonomous Vehicle (AV) policies with improved text-mining methods.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.0811622348,"dev-research":0.366639474,"prompt-eng":0.4479569225,"data-quality":0.1916517541,"ml-security":0.1157897689}}
{"text":"Recently, South Korean policymakers have viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as next-generation means of transportation that will reduce the cost of transporting passengers and goods.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.0804056315,"dev-research":0.285331844,"prompt-eng":0.3999124663,"data-quality":0.1190864675,"ml-security":0.0967021423}}
{"text":"They support the construction of V2I and V2V communication infrastructures for ADC and recognize that ADR is equivalent to pedestrians to promote its deployment into sidewalks.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.1149409385,"dev-research":0.4086674168,"prompt-eng":0.4074380301,"data-quality":0.1387916214,"ml-security":0.1347074076}}
{"text":"To fill the gap where end-user acceptance of these policies is not well considered, this study applied two text-mining methods to the comments of graduate students in the fields of Industrial, Mechanical, and Electronics-Electrical-Computer.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.0383284357,"dev-research":0.400838429,"prompt-eng":0.473821972,"data-quality":0.2239080518,"ml-security":0.0998539187}}
{"text":"One is the Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient, and the other is the Contextual Semantic Network Analysis (C-SNA) based on both KeyBERT, which extracts keywords that contextually represent the comments, and double cosine similarity.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.121148337,"dev-research":0.3572150473,"prompt-eng":0.3520819335,"data-quality":0.2005793932,"ml-security":0.0379616233}}
{"text":"The reason for comparing these approaches is to balance interest not only in the implications for the AV policies but also in the need to apply quality text mining to this research domain.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.0159758224,"dev-research":0.3469258886,"prompt-eng":0.3538100832,"data-quality":0.1948372826,"ml-security":0.0522112524}}
{"text":"Significantly, the limitation of frequency-based text mining, which does not reflect textual context, and the trade-off of adjusting thresholds in Semantic Network Analysis (SNA) were considered.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.0576617073,"dev-research":0.3392894232,"prompt-eng":0.3730295671,"data-quality":0.3034558338,"ml-security":0.1032600226}}
{"text":"As the results of comparing the two approaches, the C-SNA provided the information necessary to understand users' voices using fewer nodes and features than the CNA.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.030188907,"dev-research":0.3735572988,"prompt-eng":0.4001921498,"data-quality":0.1458548858,"ml-security":0.0510551548}}
{"text":"The users who pre-emptively understood the AV policies based on their engineering literacy and the given texts revealed potential risks of the AV accident policies.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.178888853,"dev-research":0.4806332735,"prompt-eng":0.461457956,"data-quality":0.2275668717,"ml-security":0.3679830302}}
{"text":"This study adds suggestions to manage these risks to support the successful deployment of AVs on public roads.","meta":{"url":"http://arxiv.org/abs/2307.09014v1"},"cats":{"new-dataset":0.0844774148,"dev-research":0.3966931881,"prompt-eng":0.4109290418,"data-quality":0.1175581315,"ml-security":0.3662312776}}
{"text":"GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services.","meta":{"url":"http://arxiv.org/abs/2307.09009v1"},"cats":{"new-dataset":0.1556128124,"dev-research":0.1840795982,"prompt-eng":0.4647827151,"data-quality":0.0970373379,"ml-security":0.0547931031}}
{"text":"However, when and how these models are updated over time is opaque.","meta":{"url":"http://arxiv.org/abs/2307.09009v1"},"cats":{"new-dataset":0.1194524616,"dev-research":0.2862445784,"prompt-eng":0.3857083797,"data-quality":0.137983933,"ml-security":0.1913710894}}
{"text":"Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning.","meta":{"url":"http://arxiv.org/abs/2307.09009v1"},"cats":{"new-dataset":0.2340791376,"dev-research":0.3601123341,"prompt-eng":0.424553285,"data-quality":0.0800375105,"ml-security":0.1016308028}}
{"text":"We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.","meta":{"url":"http://arxiv.org/abs/2307.09009v1"},"cats":{"new-dataset":0.0249403312,"dev-research":0.2578287689,"prompt-eng":0.3605293767,"data-quality":0.0697771144,"ml-security":0.0538348415}}
{"text":"For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%).","meta":{"url":"http://arxiv.org/abs/2307.09009v1"},"cats":{"new-dataset":0.0835535887,"dev-research":0.3388023788,"prompt-eng":0.3932660939,"data-quality":0.2689009229,"ml-security":0.0792861041}}
{"text":"Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task.","meta":{"url":"http://arxiv.org/abs/2307.09009v1"},"cats":{"new-dataset":0.1092505668,"dev-research":0.2758995603,"prompt-eng":0.3959937366,"data-quality":0.1169093225,"ml-security":0.0536931424}}
{"text":"GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March.","meta":{"url":"http://arxiv.org/abs/2307.09009v1"},"cats":{"new-dataset":0.067445316,"dev-research":0.4158870387,"prompt-eng":0.4052263535,"data-quality":0.225450479,"ml-security":0.0972361529}}
{"text":"Overall, our findings shows that the behavior of the same LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLM quality.","meta":{"url":"http://arxiv.org/abs/2307.09009v1"},"cats":{"new-dataset":0.0358073956,"dev-research":0.2250930959,"prompt-eng":0.4939289079,"data-quality":0.1587977789,"ml-security":0.1010855947}}
{"text":"Recently, the development and progress of Large Language Models (LLMs) have amazed the entire Artificial Intelligence community.","meta":{"url":"http://arxiv.org/abs/2307.09007v1"},"cats":{"new-dataset":0.1539325696,"dev-research":0.2323013587,"prompt-eng":0.5079488036,"data-quality":0.1347497418,"ml-security":0.1088002988}}
{"text":"As an outstanding representative of LLMs and the foundation model that set off this wave of research on LLMs, ChatGPT has attracted more and more researchers to study its capabilities and performance on various downstream Natural Language Processing (NLP) tasks.","meta":{"url":"http://arxiv.org/abs/2307.09007v1"},"cats":{"new-dataset":0.2048330821,"dev-research":0.2441839339,"prompt-eng":0.4692077521,"data-quality":0.1848129129,"ml-security":0.057254273}}
{"text":"While marveling at ChatGPT's incredible performance on kinds of tasks, we notice that ChatGPT also has excellent multilingual processing capabilities, such as Chinese.","meta":{"url":"http://arxiv.org/abs/2307.09007v1"},"cats":{"new-dataset":0.2175953473,"dev-research":0.3275755944,"prompt-eng":0.4071117464,"data-quality":0.1403594415,"ml-security":0.0369663126}}
{"text":"To explore the Chinese processing ability of ChatGPT, we focus on Chinese Text Correction, a fundamental and challenging Chinese NLP task.","meta":{"url":"http://arxiv.org/abs/2307.09007v1"},"cats":{"new-dataset":0.133931349,"dev-research":0.2980968446,"prompt-eng":0.430512767,"data-quality":0.3962384692,"ml-security":0.049052004}}
{"text":"Specifically, we evaluate ChatGPT on the Chinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC) tasks, which are two main Chinese Text Correction scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09007v1"},"cats":{"new-dataset":0.1201094933,"dev-research":0.3204798391,"prompt-eng":0.4769955847,"data-quality":0.4368622525,"ml-security":0.0525488654}}
{"text":"From extensive analyses and comparisons with previous state-of-the-art fine-tuned models, we empirically find that the ChatGPT currently has both amazing performance and unsatisfactory behavior for Chinese Text Correction.","meta":{"url":"http://arxiv.org/abs/2307.09007v1"},"cats":{"new-dataset":0.1417991472,"dev-research":0.2795124125,"prompt-eng":0.4183501151,"data-quality":0.4112523185,"ml-security":0.0656012157}}
{"text":"We believe our findings will promote the landing and application of LLMs in the Chinese NLP community.","meta":{"url":"http://arxiv.org/abs/2307.09007v1"},"cats":{"new-dataset":0.135810477,"dev-research":0.2634805838,"prompt-eng":0.5212677356,"data-quality":0.2552518772,"ml-security":0.0892117653}}
{"text":"This report presents the technical details of our submission on the EGO4D Audio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the OxfordVGG team.","meta":{"url":"http://arxiv.org/abs/2307.09006v1"},"cats":{"new-dataset":0.3000983205,"dev-research":0.2458336896,"prompt-eng":0.4060823327,"data-quality":0.2231430546,"ml-security":0.0814718426}}
{"text":"We present WhisperX, a system for efficient speech transcription of long-form audio with word-level time alignment, along with two text normalisers which are publicly available.","meta":{"url":"http://arxiv.org/abs/2307.09006v1"},"cats":{"new-dataset":0.2449117931,"dev-research":0.2275532177,"prompt-eng":0.3505762318,"data-quality":0.1084364742,"ml-security":0.0793601126}}
{"text":"Our final submission obtained 56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the leaderboard.","meta":{"url":"http://arxiv.org/abs/2307.09006v1"},"cats":{"new-dataset":0.1427238336,"dev-research":0.3192427194,"prompt-eng":0.4786944452,"data-quality":0.4083548022,"ml-security":0.0655348149}}
{"text":"All baseline codes and models are available on https://github.com/m-bain/whisperX.","meta":{"url":"http://arxiv.org/abs/2307.09006v1"},"cats":{"new-dataset":0.3755030165,"dev-research":0.2397176943,"prompt-eng":0.4461461641,"data-quality":0.072845113,"ml-security":0.0512348117}}
{"text":"Ordinal regression refers to classifying object instances into ordinal categories.","meta":{"url":"http://arxiv.org/abs/2307.09004v1"},"cats":{"new-dataset":0.1398489581,"dev-research":0.295901581,"prompt-eng":0.4424930018,"data-quality":0.1766657776,"ml-security":0.1576280496}}
{"text":"It has been widely studied in many scenarios, such as medical disease grading, movie rating, etc. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far.","meta":{"url":"http://arxiv.org/abs/2307.09004v1"},"cats":{"new-dataset":0.0726317564,"dev-research":0.2922294212,"prompt-eng":0.4021332859,"data-quality":0.1898791924,"ml-security":0.1248116433}}
{"text":"In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process.","meta":{"url":"http://arxiv.org/abs/2307.09004v1"},"cats":{"new-dataset":0.346016325,"dev-research":0.2502179885,"prompt-eng":0.4495909301,"data-quality":0.1737790005,"ml-security":0.1272728833}}
{"text":"In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories.","meta":{"url":"http://arxiv.org/abs/2307.09004v1"},"cats":{"new-dataset":0.113354608,"dev-research":0.2985362744,"prompt-eng":0.468227239,"data-quality":0.1374235943,"ml-security":0.1086266885}}
{"text":"Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios.","meta":{"url":"http://arxiv.org/abs/2307.09004v1"},"cats":{"new-dataset":0.0148894056,"dev-research":0.332115803,"prompt-eng":0.4198001835,"data-quality":0.1752648967,"ml-security":0.0619295874}}
{"text":"Codes will be available upon acceptance.","meta":{"url":"http://arxiv.org/abs/2307.09004v1"},"cats":{"new-dataset":0.1258759814,"dev-research":0.3329079665,"prompt-eng":0.4892912796,"data-quality":0.1288322199,"ml-security":0.0909200794}}
{"text":"Machine learning and neural networks have become increasingly popular solutions for encrypted malware traffic detection.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.0568685764,"dev-research":0.2748301948,"prompt-eng":0.3707285736,"data-quality":0.0894235606,"ml-security":0.7290933285}}
{"text":"They mine and learn complex traffic patterns, enabling detection by fitting boundaries between malware traffic and benign traffic.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.1029041035,"dev-research":0.4140693737,"prompt-eng":0.4394036878,"data-quality":0.1215038314,"ml-security":0.3280603935}}
{"text":"Compared with signature-based methods, they have higher scalability and flexibility.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.0131033839,"dev-research":0.3589205273,"prompt-eng":0.392973678,"data-quality":0.1097376604,"ml-security":0.0891001443}}
{"text":"However, affected by the frequent variants and updates of malware, current methods suffer from a high false positive rate and do not work well for unknown malware traffic detection.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.0172155481,"dev-research":0.3706199344,"prompt-eng":0.3933350014,"data-quality":0.2235859205,"ml-security":0.5129163292}}
{"text":"It remains a critical task to achieve effective malware traffic detection.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.0191983188,"dev-research":0.3603553752,"prompt-eng":0.3935283036,"data-quality":0.1320547891,"ml-security":0.420794438}}
{"text":"In this paper, we introduce CBSeq to address the above problems.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.1765638741,"dev-research":0.2652329772,"prompt-eng":0.4164769881,"data-quality":0.1685040048,"ml-security":0.0982293541}}
{"text":"CBSeq is a method that constructs a stable traffic representation, behavior sequence, to characterize attacking intent and achieve malware traffic detection.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.1052804512,"dev-research":0.3778547355,"prompt-eng":0.4103793835,"data-quality":0.1097330026,"ml-security":0.332286477}}
{"text":"We novelly propose the channels with similar behavior as the detection object and extract side-channel content to construct behavior sequence.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.1206324646,"dev-research":0.2979302452,"prompt-eng":0.4812796426,"data-quality":0.1925909759,"ml-security":0.180794148}}
{"text":"Unlike benign activities, the behavior sequences of malware and its variant's traffic exhibit solid internal correlations.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.049669164,"dev-research":0.4000784764,"prompt-eng":0.4133723962,"data-quality":0.1287729144,"ml-security":0.3778703518}}
{"text":"Moreover, we design the MSFormer, a powerful Transformer-based multi-sequence fusion classifier.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.132697824,"dev-research":0.2615289142,"prompt-eng":0.3822883738,"data-quality":0.1832363565,"ml-security":0.1366862156}}
{"text":"It captures the internal similarity of behavior sequence, thereby distinguishing malware traffic from benign traffic.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.0467219478,"dev-research":0.4374574562,"prompt-eng":0.4151317962,"data-quality":0.1293755755,"ml-security":0.3170978588}}
{"text":"Our evaluations demonstrate that CBSeq performs effectively in various known malware traffic detection and exhibits superior performance in unknown malware traffic detection, outperforming state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.09002v1"},"cats":{"new-dataset":0.0928773431,"dev-research":0.3465877994,"prompt-eng":0.3676150354,"data-quality":0.1209638931,"ml-security":0.3144641146}}
{"text":"In recent years, static power side-channel analysis attacks have emerged as a serious threat to cryptographic implementations, overcoming state-of-the-art countermeasures against side-channel attacks.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.0795315171,"dev-research":0.365962632,"prompt-eng":0.3805085447,"data-quality":0.1709580481,"ml-security":0.5549411251}}
{"text":"The continued down-scaling of semiconductor process technology, which results in an increase of the relative weight of static power in the total power budget of circuits, will only improve the viability of static power side-channel analysis attacks.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.0158339178,"dev-research":0.3578280185,"prompt-eng":0.4080722699,"data-quality":0.176079715,"ml-security":0.3020937351}}
{"text":"Yet, despite the threat posed, limited work has been invested into mitigating this class of attack.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.029541624,"dev-research":0.2907057929,"prompt-eng":0.352759358,"data-quality":0.066726188,"ml-security":0.5521117374}}
{"text":"In this work we address this gap.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.1244257274,"dev-research":0.3155691988,"prompt-eng":0.357723534,"data-quality":0.1937394814,"ml-security":0.0543781261}}
{"text":"We observe that static power side-channel analysis relies on stopping the target circuit's clock over a prolonged period, during which the circuit holds secret information in its registers.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.0357497848,"dev-research":0.3270719333,"prompt-eng":0.4065677931,"data-quality":0.165309368,"ml-security":0.3491543173}}
{"text":"We propose Borrowed Time, a countermeasure that hinders an attacker's ability to leverage such clock control.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.058854783,"dev-research":0.3116433669,"prompt-eng":0.4426791893,"data-quality":0.1028723614,"ml-security":0.5965173081}}
{"text":"Borrowed Time detects a stopped clock and triggers a reset that wipes any registers containing sensitive intermediates, whose leakages would otherwise be exploitable.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.0296653585,"dev-research":0.3397294803,"prompt-eng":0.4118726548,"data-quality":0.1935830808,"ml-security":0.3487942952}}
{"text":"We demonstrate the effectiveness of our countermeasure by performing practical Correlation Power Analysis attacks under optimal conditions against an AES implementation on an FPGA target with and without our countermeasure in place.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.059270182,"dev-research":0.3095374835,"prompt-eng":0.4041297579,"data-quality":0.137527361,"ml-security":0.5558463017}}
{"text":"In the unprotected case, we can recover the entire secret key using traces from 1,500 encryptions.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.1287988137,"dev-research":0.2013486931,"prompt-eng":0.3648858388,"data-quality":0.0871798058,"ml-security":0.3944362273}}
{"text":"Under the same conditions, the protected implementation successfully prevents key recovery even with traces from 1,000,000 encryptions.","meta":{"url":"http://arxiv.org/abs/2307.09001v1"},"cats":{"new-dataset":0.0499574142,"dev-research":0.2212956394,"prompt-eng":0.3311220821,"data-quality":0.0882968292,"ml-security":0.3059780825}}
{"text":"Diffusion MRI tractography parcellation classifies streamlines into anatomical fiber tracts to enable quantification and visualization for clinical and scientific applications.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.0560316527,"dev-research":0.2794176943,"prompt-eng":0.3861146426,"data-quality":0.1011975882,"ml-security":0.0524248462}}
{"text":"Current tractography parcellation methods rely heavily on registration, but registration inaccuracies can affect parcellation and the computational cost of registration is high for large-scale datasets.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.1558260782,"dev-research":0.2460013787,"prompt-eng":0.3916243505,"data-quality":0.1454774676,"ml-security":0.0665309731}}
{"text":"Recently, deep-learning-based methods have been proposed for tractography parcellation using various types of representations for streamlines.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.1333228025,"dev-research":0.2513137151,"prompt-eng":0.3685783944,"data-quality":0.1384560315,"ml-security":0.0843854689}}
{"text":"However, these methods only focus on the information from a single streamline, ignoring geometric relationships between the streamlines in the brain.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.0091887395,"dev-research":0.2842150814,"prompt-eng":0.3317502796,"data-quality":0.1148970776,"ml-security":0.0708082326}}
{"text":"We propose TractCloud, a registration-free framework that performs whole-brain tractography parcellation directly in individual subject space.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.1165966204,"dev-research":0.2525084305,"prompt-eng":0.4338892481,"data-quality":0.1067245925,"ml-security":0.0655938975}}
{"text":"We propose a novel, learnable, local-global streamline representation that leverages information from neighboring and whole-brain streamlines to describe the local anatomy and global pose of the brain.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.2433756295,"dev-research":0.2799395178,"prompt-eng":0.3967319003,"data-quality":0.1175549959,"ml-security":0.0819428046}}
{"text":"We train our framework on a large-scale labeled tractography dataset, which we augment by applying synthetic transforms including rotation, scaling, and translations.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.5624196199,"dev-research":0.2315221898,"prompt-eng":0.3865734756,"data-quality":0.2461775783,"ml-security":0.0964439539}}
{"text":"We test our framework on five independently acquired datasets across populations and health conditions.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.6966406242,"dev-research":0.3274583239,"prompt-eng":0.3473203397,"data-quality":0.1513961696,"ml-security":0.2448339886}}
{"text":"TractCloud significantly outperforms several state-of-the-art methods on all testing datasets.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.3441929358,"dev-research":0.3228769491,"prompt-eng":0.3783081873,"data-quality":0.1984449233,"ml-security":0.1831227569}}
{"text":"TractCloud achieves efficient and consistent whole-brain white matter parcellation across the lifespan (from neonates to elderly subjects, including brain tumor patients) without the need for registration.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.0219969553,"dev-research":0.2587396532,"prompt-eng":0.370913153,"data-quality":0.0901692253,"ml-security":0.0693518048}}
{"text":"The robustness and high inference speed of TractCloud make it suitable for large-scale tractography data analysis.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.1194499909,"dev-research":0.2701872822,"prompt-eng":0.3780238629,"data-quality":0.0994504002,"ml-security":0.1115590706}}
{"text":"Our project page is available at https://tractcloud.github.io/.","meta":{"url":"http://arxiv.org/abs/2307.09000v1"},"cats":{"new-dataset":0.412597308,"dev-research":0.3051169903,"prompt-eng":0.3839877992,"data-quality":0.0810329949,"ml-security":0.0656771399}}
{"text":"A recent line of work has shown a surprising connection between multicalibration, a multi-group fairness notion, and omniprediction, a learning paradigm that provides simultaneous loss minimization guarantees for a large family of loss functions.","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.0232566663,"dev-research":0.1868991605,"prompt-eng":0.385651142,"data-quality":0.1452381401,"ml-security":0.2623473537}}
{"text":"Prior work studies omniprediction in the batch setting.","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.0264782919,"dev-research":0.2791228993,"prompt-eng":0.4418362599,"data-quality":0.1993836461,"ml-security":0.1035486588}}
{"text":"We initiate the study of omniprediction in the online adversarial setting.","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.0455531222,"dev-research":0.2953782531,"prompt-eng":0.4421106031,"data-quality":0.2373454298,"ml-security":0.6090349029}}
{"text":"Although there exist algorithms for obtaining notions of multicalibration in the online adversarial setting, unlike batch algorithms, they work only for small finite classes of benchmark functions $F$, because they require enumerating every function $f \\in F$ at every round.","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.0485248151,"dev-research":0.1707142403,"prompt-eng":0.3635571947,"data-quality":0.1483779047,"ml-security":0.2098187954}}
{"text":"In contrast, omniprediction is most interesting for learning theoretic hypothesis classes $F$, which are generally continuously large.   ","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.0537328905,"dev-research":0.2348863413,"prompt-eng":0.4188129701,"data-quality":0.1429414627,"ml-security":0.1783638895}}
{"text":"We develop a new online multicalibration algorithm that is well defined for infinite benchmark classes $F$, and is oracle efficient (i.e. for any class $F$, the algorithm has the form of an efficient reduction to a no-regret learning algorithm for $F$).","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.0699356149,"dev-research":0.1732013237,"prompt-eng":0.3553465582,"data-quality":0.109845481,"ml-security":0.0897520982}}
{"text":"The result is the first efficient online omnipredictor -- an oracle efficient prediction algorithm that can be used to simultaneously obtain no regret guarantees to all Lipschitz convex loss functions.","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.0408890846,"dev-research":0.1998322999,"prompt-eng":0.3744350166,"data-quality":0.1191503102,"ml-security":0.1804699813}}
{"text":"For the class $F$ of linear functions, we show how to make our algorithm efficient in the worst case.","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.078771389,"dev-research":0.2344774776,"prompt-eng":0.3402296779,"data-quality":0.2268692043,"ml-security":0.2927635402}}
{"text":"Also, we show upper and lower bounds on the extent to which our rates can be improved: our oracle efficient algorithm actually promises a stronger guarantee called swap-omniprediction, and we prove a lower bound showing that obtaining $O(\\sqrt{T})$ bounds for swap-omniprediction is impossible in the online setting.","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.03389989,"dev-research":0.2058344539,"prompt-eng":0.3355635849,"data-quality":0.0818174013,"ml-security":0.1179515212}}
{"text":"On the other hand, we give a (non-oracle efficient) algorithm which can obtain the optimal $O(\\sqrt{T})$ omniprediction bounds without going through multicalibration, giving an information theoretic separation between these two solution concepts.","meta":{"url":"http://arxiv.org/abs/2307.08999v1"},"cats":{"new-dataset":0.0211532779,"dev-research":0.2077124631,"prompt-eng":0.3456132423,"data-quality":0.0731960733,"ml-security":0.108549075}}
{"text":"An authentic face restoration system is becoming increasingly demanding in many computer vision applications, e.g., image enhancement, video communication, and taking portrait.","meta":{"url":"http://arxiv.org/abs/2307.08996v1"},"cats":{"new-dataset":0.0227785065,"dev-research":0.2639279624,"prompt-eng":0.3672371781,"data-quality":0.1143600979,"ml-security":0.1862307513}}
{"text":"Most of the advanced face restoration models can recover high-quality faces from low-quality ones but usually fail to faithfully generate realistic and high-frequency details that are favored by users.","meta":{"url":"http://arxiv.org/abs/2307.08996v1"},"cats":{"new-dataset":0.0187614784,"dev-research":0.2576838215,"prompt-eng":0.416975465,"data-quality":0.171681809,"ml-security":0.1426046497}}
{"text":"To achieve authentic restoration, we propose $\\textbf{IDM}$, an $\\textbf{I}$teratively learned face restoration system based on denoising $\\textbf{D}$iffusion $\\textbf{M}$odels (DDMs).","meta":{"url":"http://arxiv.org/abs/2307.08996v1"},"cats":{"new-dataset":0.0577156889,"dev-research":0.244537034,"prompt-eng":0.3899974681,"data-quality":0.1895368983,"ml-security":0.3345097499}}
{"text":"We define the criterion of an authentic face restoration system, and argue that denoising diffusion models are naturally endowed with this property from two aspects: intrinsic iterative refinement and extrinsic iterative enhancement.","meta":{"url":"http://arxiv.org/abs/2307.08996v1"},"cats":{"new-dataset":0.0136856442,"dev-research":0.2424988923,"prompt-eng":0.361470468,"data-quality":0.1259771795,"ml-security":0.1793593571}}
{"text":"Intrinsic learning can preserve the content well and gradually refine the high-quality details, while extrinsic enhancement helps clean the data and improve the restoration task one step further.","meta":{"url":"http://arxiv.org/abs/2307.08996v1"},"cats":{"new-dataset":0.0261771167,"dev-research":0.2804478589,"prompt-eng":0.3770476964,"data-quality":0.1613651815,"ml-security":0.1014383209}}
{"text":"We demonstrate superior performance on blind face restoration tasks.","meta":{"url":"http://arxiv.org/abs/2307.08996v1"},"cats":{"new-dataset":0.0209085598,"dev-research":0.2558061131,"prompt-eng":0.3513409846,"data-quality":0.1490466918,"ml-security":0.1122168194}}
{"text":"Beyond restoration, we find the authentically cleaned data by the proposed restoration system is also helpful to image generation tasks in terms of training stabilization and sample quality.","meta":{"url":"http://arxiv.org/abs/2307.08996v1"},"cats":{"new-dataset":0.1700317219,"dev-research":0.2378056975,"prompt-eng":0.4206281029,"data-quality":0.2990431999,"ml-security":0.1314699591}}
{"text":"Without modifying the models, we achieve better quality than state-of-the-art on FFHQ and ImageNet generation using either GANs or diffusion models.","meta":{"url":"http://arxiv.org/abs/2307.08996v1"},"cats":{"new-dataset":0.0723711785,"dev-research":0.224547661,"prompt-eng":0.4036041696,"data-quality":0.1506445584,"ml-security":0.1005459466}}
{"text":"The exploration of the latent space in StyleGANs and GAN inversion exemplify impressive real-world image editing, yet the trade-off between reconstruction quality and editing quality remains an open problem.","meta":{"url":"http://arxiv.org/abs/2307.08995v1"},"cats":{"new-dataset":0.0284118946,"dev-research":0.2787057423,"prompt-eng":0.3869530558,"data-quality":0.2058741545,"ml-security":0.0851993748}}
{"text":"In this study, we revisit StyleGANs' hyperspherical prior $\\mathcal{Z}$ and combine it with highly capable latent spaces to build combined spaces that faithfully invert real images while maintaining the quality of edited images.","meta":{"url":"http://arxiv.org/abs/2307.08995v1"},"cats":{"new-dataset":0.0471149759,"dev-research":0.2401180419,"prompt-eng":0.4242768319,"data-quality":0.2118636796,"ml-security":0.0838981781}}
{"text":"More specifically, we propose $\\mathcal{F}/\\mathcal{Z}^{+}$ space consisting of two subspaces: $\\mathcal{F}$ space of an intermediate feature map of StyleGANs enabling faithful reconstruction and $\\mathcal{Z}^{+}$ space of an extended StyleGAN prior supporting high editing quality.","meta":{"url":"http://arxiv.org/abs/2307.08995v1"},"cats":{"new-dataset":0.0456572429,"dev-research":0.2837296975,"prompt-eng":0.4043502174,"data-quality":0.2065909092,"ml-security":0.0983089165}}
{"text":"We project the real images into the proposed space to obtain the inverted codes, by which we then move along $\\mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality.","meta":{"url":"http://arxiv.org/abs/2307.08995v1"},"cats":{"new-dataset":0.1227924194,"dev-research":0.2965676123,"prompt-eng":0.4018767808,"data-quality":0.2618155253,"ml-security":0.0730935487}}
{"text":"Comprehensive experiments show that $\\mathcal{Z}^{+}$ can replace the most commonly-used $\\mathcal{W}$, $\\mathcal{W}^{+}$, and $\\mathcal{S}$ spaces while preserving reconstruction quality, resulting in reduced distortion of edited images.","meta":{"url":"http://arxiv.org/abs/2307.08995v1"},"cats":{"new-dataset":0.0299287446,"dev-research":0.240152809,"prompt-eng":0.3939122138,"data-quality":0.2611225197,"ml-security":0.0805861462}}
{"text":"Understanding the relationship between different parts of the image plays a crucial role in many visual recognition tasks.","meta":{"url":"http://arxiv.org/abs/2307.08994v1"},"cats":{"new-dataset":0.0243182911,"dev-research":0.2766671186,"prompt-eng":0.402234286,"data-quality":0.1849115038,"ml-security":0.0703637961}}
{"text":"Despite the fact that Convolutional Neural Networks (CNNs) have demonstrated impressive results in detecting single objects, they lack the capability to extract the relationship between various regions of an image, which is a crucial factor in human action recognition.","meta":{"url":"http://arxiv.org/abs/2307.08994v1"},"cats":{"new-dataset":0.0503075031,"dev-research":0.2370366253,"prompt-eng":0.3606381025,"data-quality":0.169726366,"ml-security":0.1455946612}}
{"text":"To address this problem, this paper proposes a new module that functions like a convolutional layer using Vision Transformer (ViT).","meta":{"url":"http://arxiv.org/abs/2307.08994v1"},"cats":{"new-dataset":0.0840511659,"dev-research":0.2353478112,"prompt-eng":0.369097272,"data-quality":0.0932455027,"ml-security":0.0693092042}}
{"text":"The proposed action recognition model comprises two components: the first part is a deep convolutional network that extracts high-level spatial features from the image, and the second component of the model utilizes a Vision Transformer that extracts the relationship between various regions of the image using the feature map generated by the CNN output.","meta":{"url":"http://arxiv.org/abs/2307.08994v1"},"cats":{"new-dataset":0.1247052084,"dev-research":0.2412794228,"prompt-eng":0.3705001084,"data-quality":0.1026892389,"ml-security":0.1026750641}}
{"text":"The proposed model has been evaluated on the Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5% mAP and 91.5% mAP results, respectively, which are promising compared to other state-of-the-art methods.","meta":{"url":"http://arxiv.org/abs/2307.08994v1"},"cats":{"new-dataset":0.3102989585,"dev-research":0.253436104,"prompt-eng":0.3979959913,"data-quality":0.1278606917,"ml-security":0.1021364215}}
{"text":"Point clouds acquired from 3D sensors are usually sparse and noisy.","meta":{"url":"http://arxiv.org/abs/2307.08992v1"},"cats":{"new-dataset":0.0563375966,"dev-research":0.2384311557,"prompt-eng":0.3696146195,"data-quality":0.1728672128,"ml-security":0.0952132209}}
{"text":"Point cloud upsampling is an approach to increase the density of the point cloud so that detailed geometric information can be restored.","meta":{"url":"http://arxiv.org/abs/2307.08992v1"},"cats":{"new-dataset":0.0555312648,"dev-research":0.244071145,"prompt-eng":0.3639524527,"data-quality":0.1206794742,"ml-security":0.0440051999}}
{"text":"In this paper, we propose a Dual Back-Projection network for point cloud upsampling (DBPnet).","meta":{"url":"http://arxiv.org/abs/2307.08992v1"},"cats":{"new-dataset":0.1027208055,"dev-research":0.2121994694,"prompt-eng":0.3510319837,"data-quality":0.1394868199,"ml-security":0.0765894842}}
{"text":"A Dual Back-Projection is formulated in an up-down-up manner for point cloud upsampling.","meta":{"url":"http://arxiv.org/abs/2307.08992v1"},"cats":{"new-dataset":0.0893973435,"dev-research":0.1968866185,"prompt-eng":0.3781663884,"data-quality":0.1040338665,"ml-security":0.0604443259}}
{"text":"It not only back projects feature residues but also coordinates residues so that the network better captures the point correlations in the feature and space domains, achieving lower reconstruction errors on both uniform and non-uniform sparse point clouds.","meta":{"url":"http://arxiv.org/abs/2307.08992v1"},"cats":{"new-dataset":0.0413529509,"dev-research":0.2550168902,"prompt-eng":0.3728025008,"data-quality":0.1302665338,"ml-security":0.0939265081}}
{"text":"Our proposed method is also generalizable for arbitrary upsampling tasks (e.g. 4x, 5.5x).","meta":{"url":"http://arxiv.org/abs/2307.08992v1"},"cats":{"new-dataset":0.0691847916,"dev-research":0.2038546529,"prompt-eng":0.3834399921,"data-quality":0.1743769108,"ml-security":0.0363472646}}
{"text":"Experimental results show that the proposed method achieves the lowest point set matching losses with respect to the benchmark.","meta":{"url":"http://arxiv.org/abs/2307.08992v1"},"cats":{"new-dataset":0.0433617641,"dev-research":0.2159765611,"prompt-eng":0.346376458,"data-quality":0.1765010848,"ml-security":0.0681862247}}
{"text":"In addition, the success of our approach demonstrates that generative networks are not necessarily needed for non-uniform point clouds.","meta":{"url":"http://arxiv.org/abs/2307.08992v1"},"cats":{"new-dataset":0.0553052215,"dev-research":0.1762240361,"prompt-eng":0.3738749915,"data-quality":0.1602218933,"ml-security":0.1087334092}}
{"text":"Accurate and reliable ego-localization is critical for autonomous driving.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.0294399102,"dev-research":0.3001472763,"prompt-eng":0.4472595964,"data-quality":0.3552439705,"ml-security":0.1299777034}}
{"text":"In this paper, we present EgoVM, an end-to-end localization network that achieves comparable localization accuracy to prior state-of-the-art methods, but uses lightweight vectorized maps instead of heavy point-based maps.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.1307853211,"dev-research":0.274637867,"prompt-eng":0.4432333417,"data-quality":0.2412086371,"ml-security":0.0800051018}}
{"text":"To begin with, we extract BEV features from online multi-view images and LiDAR point cloud.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.234688025,"dev-research":0.2648297278,"prompt-eng":0.3769380167,"data-quality":0.0864809373,"ml-security":0.0775540048}}
{"text":"Then, we employ a set of learnable semantic embeddings to encode the semantic types of map elements and supervise them with semantic segmentation, to make their feature representation consistent with BEV features.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.1816615024,"dev-research":0.3241497259,"prompt-eng":0.435300686,"data-quality":0.2605326983,"ml-security":0.0877340048}}
{"text":"After that, we feed map queries, composed of learnable semantic embeddings and coordinates of map elements, into a transformer decoder to perform cross-modality matching with BEV features.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.1358166804,"dev-research":0.2644854922,"prompt-eng":0.4347111801,"data-quality":0.1623490231,"ml-security":0.0854785327}}
{"text":"Finally, we adopt a robust histogram-based pose solver to estimate the optimal pose by searching exhaustively over candidate poses.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.2357893835,"dev-research":0.2308383155,"prompt-eng":0.3950622982,"data-quality":0.0955418056,"ml-security":0.0837501371}}
{"text":"We comprehensively validate the effectiveness of our method using both the nuScenes dataset and a newly collected dataset.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.2756544778,"dev-research":0.3163471695,"prompt-eng":0.3635633534,"data-quality":0.2568958355,"ml-security":0.0888593168}}
{"text":"The experimental results show that our method achieves centimeter-level localization accuracy, and outperforms existing methods using vectorized maps by a large margin.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.0819003575,"dev-research":0.2792318093,"prompt-eng":0.4234448736,"data-quality":0.2154393207,"ml-security":0.0543801827}}
{"text":"Furthermore, our model has been extensively tested in a large fleet of autonomous vehicles under various challenging urban scenes.","meta":{"url":"http://arxiv.org/abs/2307.08991v1"},"cats":{"new-dataset":0.1377208635,"dev-research":0.2247340665,"prompt-eng":0.4120959626,"data-quality":0.107239223,"ml-security":0.1588807861}}
{"text":"Recently, deep learning models have been widely applied in program understanding tasks, and these models achieve state-of-the-art results on many benchmark datasets.","meta":{"url":"http://arxiv.org/abs/2307.08990v1"},"cats":{"new-dataset":0.375589878,"dev-research":0.3498193916,"prompt-eng":0.4113947386,"data-quality":0.1819046164,"ml-security":0.1908463663}}
{"text":"A major challenge of deep learning for program understanding is that the effectiveness of these approaches depends on the quality of their datasets, and these datasets often contain noisy data samples.","meta":{"url":"http://arxiv.org/abs/2307.08990v1"},"cats":{"new-dataset":0.2505657246,"dev-research":0.401758122,"prompt-eng":0.3563012476,"data-quality":0.4922975878,"ml-security":0.3564747379}}
{"text":"A typical kind of noise in program understanding datasets is label noises, which means that the target outputs for some inputs are mislabeled.   ","meta":{"url":"http://arxiv.org/abs/2307.08990v1"},"cats":{"new-dataset":0.2943735918,"dev-research":0.4214611842,"prompt-eng":0.3998960887,"data-quality":0.7844621604,"ml-security":0.3253229731}}
{"text":"Label noises may have a negative impact on the performance of deep learning models, so researchers have proposed various approaches to alleviate the impact of noisy labels, and formed a new research topic: noisy label learning (NLL).","meta":{"url":"http://arxiv.org/abs/2307.08990v1"},"cats":{"new-dataset":0.1027178967,"dev-research":0.2979774565,"prompt-eng":0.3884020788,"data-quality":0.8114533896,"ml-security":0.2858124976}}
{"text":"In this paper, we conduct an empirical study on the effectiveness of noisy label learning on deep learning for program understanding datasets.","meta":{"url":"http://arxiv.org/abs/2307.08990v1"},"cats":{"new-dataset":0.3267589895,"dev-research":0.3659043245,"prompt-eng":0.3839637845,"data-quality":0.6827952065,"ml-security":0.2722053899}}
{"text":"We evaluate various noisy label learning approaches and deep learning models on two tasks: program classification and code summarization.","meta":{"url":"http://arxiv.org/abs/2307.08990v1"},"cats":{"new-dataset":0.2542598235,"dev-research":0.3606346521,"prompt-eng":0.4091955539,"data-quality":0.6802250469,"ml-security":0.1569966026}}
{"text":"From the evaluation results, we find that the impact of label noise and NLL approaches on small deep learning models and large pre-trained models are different: small models are prone to label noises in program classification and NLL approaches can improve their robustness, while large pre-trained models are robust against label noises and NLL does not significantly improve their performances.","meta":{"url":"http://arxiv.org/abs/2307.08990v1"},"cats":{"new-dataset":0.1302170048,"dev-research":0.3000704828,"prompt-eng":0.3986671967,"data-quality":0.5555378459,"ml-security":0.2403171255}}
{"text":"On the other hand, NLL approaches have shown satisfying results in identifying noisy labeled samples for both tasks, indicating that these techniques can benefit researchers in building high-quality program understanding datasets.","meta":{"url":"http://arxiv.org/abs/2307.08990v1"},"cats":{"new-dataset":0.303767942,"dev-research":0.4490291131,"prompt-eng":0.4203272392,"data-quality":0.5300269338,"ml-security":0.1302540776}}
{"text":"Drug-target binding affinity prediction plays an important role in the early stages of drug discovery, which can infer the strength of interactions between new drugs and new targets.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.046327929,"dev-research":0.2483162122,"prompt-eng":0.4049616761,"data-quality":0.1225454359,"ml-security":0.2273411274}}
{"text":"However, the performance of previous computational models is limited by the following drawbacks.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.0098241992,"dev-research":0.1894475172,"prompt-eng":0.3318054663,"data-quality":0.0504412287,"ml-security":0.1246313895}}
{"text":"The learning of drug representation relies only on supervised data, without taking into account the information contained in the molecular graph itself.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.0252686705,"dev-research":0.2363297016,"prompt-eng":0.3463541106,"data-quality":0.2320196997,"ml-security":0.2223498729}}
{"text":"Moreover, most previous studies tended to design complicated representation learning module, while uniformity, which is used to measure representation quality, is ignored.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.0245358057,"dev-research":0.2887804814,"prompt-eng":0.4194208752,"data-quality":0.281213941,"ml-security":0.1309778437}}
{"text":"In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.104150446,"dev-research":0.2604168749,"prompt-eng":0.3633860271,"data-quality":0.1995159349,"ml-security":0.1325218013}}
{"text":"In GraphCL-DTA, we design a graph contrastive learning framework for molecular graphs to learn drug representations, so that the semantics of molecular graphs are preserved.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.1502996378,"dev-research":0.2740207076,"prompt-eng":0.34780819,"data-quality":0.1922103552,"ml-security":0.0951351844}}
{"text":"Through this graph contrastive framework, a more essential and effective drug representation can be learned without additional supervised data.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.1167100707,"dev-research":0.2511497823,"prompt-eng":0.3347383058,"data-quality":0.190770695,"ml-security":0.1600962296}}
{"text":"Next, we design a new loss function that can be directly used to smoothly adjust the uniformity of drug and target representations.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.0122745845,"dev-research":0.1948927232,"prompt-eng":0.4119334714,"data-quality":0.1580525127,"ml-security":0.2036593118}}
{"text":"By directly optimizing the uniformity of representations, the representation quality of drugs and targets can be improved.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.0124151795,"dev-research":0.298413197,"prompt-eng":0.4262936675,"data-quality":0.2250405572,"ml-security":0.2135155267}}
{"text":"The effectiveness of the above innovative elements is verified on two real datasets, KIBA and Davis.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.1602452585,"dev-research":0.2920023876,"prompt-eng":0.3544867034,"data-quality":0.1634191121,"ml-security":0.1250432199}}
{"text":"The excellent performance of GraphCL-DTA on the above datasets suggests its superiority to the state-of-the-art model.","meta":{"url":"http://arxiv.org/abs/2307.08989v1"},"cats":{"new-dataset":0.3578033396,"dev-research":0.2672227113,"prompt-eng":0.3244197324,"data-quality":0.138022768,"ml-security":0.081630236}}
{"text":"Recently, uncertainty-aware methods have attracted increasing attention in semi-supervised medical image segmentation.","meta":{"url":"http://arxiv.org/abs/2307.08988v1"},"cats":{"new-dataset":0.0448197351,"dev-research":0.2056522413,"prompt-eng":0.4188305898,"data-quality":0.3248075048,"ml-security":0.0823646995}}
{"text":"However, current methods usually suffer from the drawback that it is difficult to balance the computational cost, estimation accuracy, and theoretical support in a unified framework.","meta":{"url":"http://arxiv.org/abs/2307.08988v1"},"cats":{"new-dataset":0.0121633237,"dev-research":0.2921857144,"prompt-eng":0.3329698307,"data-quality":0.1177020884,"ml-security":0.1160679429}}
{"text":"To alleviate this problem, we introduce the Dempster-Shafer Theory of Evidence (DST) into semi-supervised medical image segmentation, dubbed Evidential Inference Learning (EVIL).","meta":{"url":"http://arxiv.org/abs/2307.08988v1"},"cats":{"new-dataset":0.1723633864,"dev-research":0.234703982,"prompt-eng":0.4242627604,"data-quality":0.2017772371,"ml-security":0.1755631207}}
{"text":"EVIL provides a theoretically guaranteed solution to infer accurate uncertainty quantification in a single forward pass.","meta":{"url":"http://arxiv.org/abs/2307.08988v1"},"cats":{"new-dataset":0.0217992477,"dev-research":0.3225457343,"prompt-eng":0.4288434692,"data-quality":0.1786493917,"ml-security":0.3571595533}}
{"text":"Trustworthy pseudo labels on unlabeled data are generated after uncertainty estimation.","meta":{"url":"http://arxiv.org/abs/2307.08988v1"},"cats":{"new-dataset":0.2294877437,"dev-research":0.2513557233,"prompt-eng":0.4639999824,"data-quality":0.7392139838,"ml-security":0.2905560592}}
{"text":"The recently proposed consistency regularization-based training paradigm is adopted in our framework, which enforces the consistency on the perturbed predictions to enhance the generalization with few labeled data.","meta":{"url":"http://arxiv.org/abs/2307.08988v1"},"cats":{"new-dataset":0.0441072808,"dev-research":0.2149213111,"prompt-eng":0.428582005,"data-quality":0.5837172518,"ml-security":0.253272308}}
{"text":"Experimental results show that EVIL achieves competitive performance in comparison with several state-of-the-art methods on the public dataset.","meta":{"url":"http://arxiv.org/abs/2307.08988v1"},"cats":{"new-dataset":0.1415711754,"dev-research":0.3545802293,"prompt-eng":0.3565397642,"data-quality":0.1464454911,"ml-security":0.5061846844}}
{"text":"Extended Reality (XR) is one of the most important 5G/6G media applications that will fundamentally transform human interactions.","meta":{"url":"http://arxiv.org/abs/2307.08987v1"},"cats":{"new-dataset":0.1174195064,"dev-research":0.2532209844,"prompt-eng":0.3806784641,"data-quality":0.0517102239,"ml-security":0.0563012323}}
{"text":"However, ensuring low latency, high data rate, and reliability to support XR services poses significant challenges.","meta":{"url":"http://arxiv.org/abs/2307.08987v1"},"cats":{"new-dataset":0.0807016056,"dev-research":0.3127791573,"prompt-eng":0.3959854404,"data-quality":0.1568091781,"ml-security":0.1107054692}}
{"text":"This letter presents a novel AI-assisted service provisioning scheme that leverages predicted frames for processing rather than relying solely on actual frames.","meta":{"url":"http://arxiv.org/abs/2307.08987v1"},"cats":{"new-dataset":0.0871216641,"dev-research":0.3324991528,"prompt-eng":0.419002667,"data-quality":0.1460600232,"ml-security":0.1372499023}}
{"text":"This method virtually increases the network delay budget and consequently improves service provisioning, albeit at the expense of minor prediction errors.","meta":{"url":"http://arxiv.org/abs/2307.08987v1"},"cats":{"new-dataset":0.0109287205,"dev-research":0.3410320237,"prompt-eng":0.4023297751,"data-quality":0.1319765305,"ml-security":0.1083567}}
{"text":"The proposed scheme is validated by extensive simulations demonstrating a multi-fold increase in supported XR users and also provides crucial network design insights.","meta":{"url":"http://arxiv.org/abs/2307.08987v1"},"cats":{"new-dataset":0.0196842047,"dev-research":0.3078519266,"prompt-eng":0.3917018609,"data-quality":0.0721276465,"ml-security":0.1535468652}}
{"text":"Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt.","meta":{"url":"http://arxiv.org/abs/2307.08985v1"},"cats":{"new-dataset":0.1687590788,"dev-research":0.2439207511,"prompt-eng":0.5648770843,"data-quality":0.1480534713,"ml-security":0.0523021898}}
{"text":"Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them.","meta":{"url":"http://arxiv.org/abs/2307.08985v1"},"cats":{"new-dataset":0.0280703834,"dev-research":0.3621428412,"prompt-eng":0.4665265468,"data-quality":0.0530970636,"ml-security":0.0779634396}}
{"text":"However, how to support users to efficiently explore the model's capability and to create effective prompts are still open-ended research questions.","meta":{"url":"http://arxiv.org/abs/2307.08985v1"},"cats":{"new-dataset":0.035277253,"dev-research":0.3068753877,"prompt-eng":0.4976422002,"data-quality":0.0604889802,"ml-security":0.1283454894}}
{"text":"In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt.","meta":{"url":"http://arxiv.org/abs/2307.08985v1"},"cats":{"new-dataset":0.1769807063,"dev-research":0.3460777702,"prompt-eng":0.5917265966,"data-quality":0.1286359259,"ml-security":0.0674272207}}
{"text":"Through the iterative process, users can efficiently explore the model's capability, and clarify their intent.","meta":{"url":"http://arxiv.org/abs/2307.08985v1"},"cats":{"new-dataset":0.0085936581,"dev-research":0.3813270021,"prompt-eng":0.4937613512,"data-quality":0.05305302,"ml-security":0.0773441128}}
{"text":"PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model.","meta":{"url":"http://arxiv.org/abs/2307.08985v1"},"cats":{"new-dataset":0.0646322925,"dev-research":0.3928097542,"prompt-eng":0.6364823002,"data-quality":0.1967139932,"ml-security":0.122396218}}
{"text":"Lastly, users can revert to a desired step by reviewing the work history.","meta":{"url":"http://arxiv.org/abs/2307.08985v1"},"cats":{"new-dataset":0.0152069378,"dev-research":0.4037645356,"prompt-eng":0.4423976164,"data-quality":0.1374890648,"ml-security":0.0801078091}}
{"text":"In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.","meta":{"url":"http://arxiv.org/abs/2307.08985v1"},"cats":{"new-dataset":0.0650411396,"dev-research":0.3358631979,"prompt-eng":0.5832572087,"data-quality":0.0594245043,"ml-security":0.0755912958}}
{"text":"Video Visual Relation Detection (VidVRD) aims to detect visual relationship triplets in videos using spatial bounding boxes and temporal boundaries.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.2163527817,"dev-research":0.3023675157,"prompt-eng":0.370517932,"data-quality":0.1407262726,"ml-security":0.0504156778}}
{"text":"Existing VidVRD methods can be broadly categorized into bottom-up and top-down paradigms, depending on their approach to classifying relations.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.074985104,"dev-research":0.3305209987,"prompt-eng":0.4182846587,"data-quality":0.1520962434,"ml-security":0.0584993658}}
{"text":"Bottom-up methods follow a clip-based approach where they classify relations of short clip tubelet pairs and then merge them into long video relations.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.1021886232,"dev-research":0.2116731308,"prompt-eng":0.3975208601,"data-quality":0.1695148648,"ml-security":0.0331528269}}
{"text":"On the other hand, top-down methods directly classify long video tubelet pairs.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.0424535679,"dev-research":0.2233227569,"prompt-eng":0.3876359495,"data-quality":0.1719598583,"ml-security":0.0566846445}}
{"text":"While recent video-based methods utilizing video tubelets have shown promising results, we argue that the effective modeling of spatial and temporal context plays a more significant role than the choice between clip tubelets and video tubelets.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.0620156703,"dev-research":0.2072330037,"prompt-eng":0.3862367923,"data-quality":0.1074185702,"ml-security":0.0449287556}}
{"text":"This motivates us to revisit the clip-based paradigm and explore the key success factors in VidVRD.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.0464191954,"dev-research":0.3695278079,"prompt-eng":0.4367438239,"data-quality":0.1271811865,"ml-security":0.0496448155}}
{"text":"In this paper, we propose a Hierarchical Context Model (HCM) that enriches the object-based spatial context and relation-based temporal context based on clips.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.2091963441,"dev-research":0.2621390468,"prompt-eng":0.4208574575,"data-quality":0.0999982714,"ml-security":0.0428100127}}
{"text":"We demonstrate that using clip tubelets can achieve superior performance compared to most video-based methods.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.0312129435,"dev-research":0.2546455175,"prompt-eng":0.3803230239,"data-quality":0.1189461344,"ml-security":0.0536328237}}
{"text":"Additionally, using clip tubelets offers more flexibility in model designs and helps alleviate the limitations associated with video tubelets, such as the challenging long-term object tracking problem and the loss of temporal information in long-term tubelet feature compression.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.0172492074,"dev-research":0.2745340254,"prompt-eng":0.3731643093,"data-quality":0.0965636546,"ml-security":0.108374131}}
{"text":"Extensive experiments conducted on two challenging VidVRD benchmarks validate that our HCM achieves a new state-of-the-art performance, highlighting the effectiveness of incorporating advanced spatial and temporal context modeling within the clip-based paradigm.","meta":{"url":"http://arxiv.org/abs/2307.08984v1"},"cats":{"new-dataset":0.1506302212,"dev-research":0.319214292,"prompt-eng":0.4331047119,"data-quality":0.0849966712,"ml-security":0.052461543}}
{"text":"Neural networks have achieved remarkable performance in various application domains.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.0255581894,"dev-research":0.2661602087,"prompt-eng":0.3704080327,"data-quality":0.0702146233,"ml-security":0.1840434444}}
{"text":"Nevertheless, a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.0445477561,"dev-research":0.2340208608,"prompt-eng":0.3873529194,"data-quality":0.1214132113,"ml-security":0.5047242378}}
{"text":"It is highly desirable to obtain lightweight versions of neural networks for inference in edge devices.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.041429503,"dev-research":0.2594275813,"prompt-eng":0.3820281697,"data-quality":0.0764332633,"ml-security":0.2705348466}}
{"text":"Many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.0568480606,"dev-research":0.2528919549,"prompt-eng":0.3292570798,"data-quality":0.0981161025,"ml-security":0.2285237628}}
{"text":"However, a unified theoretical foundation for the problem mostly is missing.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.019084518,"dev-research":0.2324666422,"prompt-eng":0.3344329733,"data-quality":0.1613049902,"ml-security":0.0705601946}}
{"text":"In this paper, we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.0175154544,"dev-research":0.2333445339,"prompt-eng":0.3344298184,"data-quality":0.2044098679,"ml-security":0.2698372334}}
{"text":"Based on the analysis, we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.0340554462,"dev-research":0.2670539887,"prompt-eng":0.3370629162,"data-quality":0.245386957,"ml-security":0.3083299081}}
{"text":"We carefully design and conduct experiments to support our arguments.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.03514443,"dev-research":0.3431263497,"prompt-eng":0.4613906414,"data-quality":0.1064945988,"ml-security":0.1593603788}}
{"text":"Hence we provide a consolidated viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying and preserving the critical neural weights.","meta":{"url":"http://arxiv.org/abs/2307.08982v1"},"cats":{"new-dataset":0.0314746103,"dev-research":0.2969787732,"prompt-eng":0.3601139031,"data-quality":0.3013138731,"ml-security":0.3581845707}}
{"text":"In this paper, we give new auction algorithms for maximum weighted bipartite matching (MWM) and maximum cardinality bipartite $b$-matching (MCbM).","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.0564048802,"dev-research":0.1678035959,"prompt-eng":0.3716829031,"data-quality":0.0891502941,"ml-security":0.0636299525}}
{"text":"Our algorithms run in $O\\left(\\log n/\\varepsilon^8\\right)$ and $O\\left(\\log n/\\varepsilon^2\\right)$ rounds, respectively, in the blackboard distributed setting.","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.0774404714,"dev-research":0.1831649261,"prompt-eng":0.3027386477,"data-quality":0.1146804088,"ml-security":0.2118873508}}
{"text":"We show that our MWM algorithm can be implemented in the distributed, interactive setting using $O(\\log^2 n)$ and $O(\\log n)$ bit messages, respectively, directly answering the open question posed by Demange, Gale and Sotomayor","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.171665819,"dev-research":0.19524373,"prompt-eng":0.4395012963,"data-quality":0.1087886981,"ml-security":0.1090133106}}
{"text":"[DNO14].","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.322551179,"dev-research":0.3037996099,"prompt-eng":0.4261961237,"data-quality":0.136906756,"ml-security":0.1196578545}}
{"text":"Furthermore, we implement our algorithms in a variety of other models including the the semi-streaming model, the shared-memory work-depth model, and the massively parallel computation model.","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.0749598952,"dev-research":0.1908379294,"prompt-eng":0.3435141115,"data-quality":0.0514554369,"ml-security":0.0884942225}}
{"text":"Our semi-streaming MWM algorithm uses $O(1/\\varepsilon^8)$ passes in $O(n \\log n \\cdot \\log(1/\\varepsilon))$ space and our MCbM algorithm runs in $O(1/\\varepsilon^2)$ passes using $O\\left(\\left(\\sum_{i \\in L} b_i","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.0583167912,"dev-research":0.1807530833,"prompt-eng":0.3577756616,"data-quality":0.1259461262,"ml-security":0.0830489078}}
{"text":"+ |R|\\right)\\log(1/\\varepsilon)\\right)$ space (where parameters $b_i$ represent the degree constraints on the $b$-matching and $L$ and $R$ represent the left and right side of the bipartite graph, respectively).","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.0392545669,"dev-research":0.1900969107,"prompt-eng":0.3485808549,"data-quality":0.1322792831,"ml-security":0.0671155472}}
{"text":"Both of these algorithms improves \\emph{exponentially} the dependence on $\\varepsilon$ in the space complexity in the semi-streaming model against the best-known algorithms for these problems, in addition to improvements in round complexity for MCbM.","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.0179213628,"dev-research":0.1811989804,"prompt-eng":0.3300521003,"data-quality":0.0815355187,"ml-security":0.0918755399}}
{"text":"Finally, our algorithms eliminate the large polylogarithmic dependence on $n$ in depth and number of rounds in the work-depth and massively parallel computation models, respectively, improving on previous results which have large polylogarithmic dependence on $n$ (and exponential dependence on $\\varepsilon$ in the MPC model).","meta":{"url":"http://arxiv.org/abs/2307.08979v1"},"cats":{"new-dataset":0.0228341984,"dev-research":0.1685355077,"prompt-eng":0.3344499094,"data-quality":0.0677456314,"ml-security":0.1132002732}}
{"text":"The swift progress and ubiquitous adoption of Generative AI (GAI), Generative Pre-trained Transformers (GPTs), and large language models (LLMs) like ChatGPT, have spurred queries about their ethical application, use, and disclosure in scholarly research and scientific productions.","meta":{"url":"http://arxiv.org/abs/2307.08974v1"},"cats":{"new-dataset":0.101721167,"dev-research":0.2947501664,"prompt-eng":0.458307666,"data-quality":0.124784939,"ml-security":0.1558833196}}
{"text":"A few publishers and journals have recently created their own sets of rules; however, the absence of a unified approach may lead to a 'Babel Tower Effect,' potentially resulting in confusion rather than desired standardization.","meta":{"url":"http://arxiv.org/abs/2307.08974v1"},"cats":{"new-dataset":0.0150426817,"dev-research":0.3303179401,"prompt-eng":0.4145855656,"data-quality":0.1724902612,"ml-security":0.0543561456}}
{"text":"In response to this, we present the ChatGPT, Generative Artificial Intelligence, and Natural Large Language Models for Accountable Reporting and Use Guidelines (CANGARU) initiative, with the aim of fostering a cross-disciplinary global inclusive consensus on the ethical use, disclosure, and proper reporting of GAI/GPT/LLM technologies in academia.","meta":{"url":"http://arxiv.org/abs/2307.08974v1"},"cats":{"new-dataset":0.3094618599,"dev-research":0.3246957842,"prompt-eng":0.4125104575,"data-quality":0.1861492575,"ml-security":0.1390842536}}
{"text":"The present protocol consists of four distinct parts: a) an ongoing systematic review of GAI/GPT/LLM applications to understand the linked ideas, findings, and reporting standards in scholarly research, and to formulate guidelines for its use and disclosure, b) a bibliometric analysis of existing author guidelines in journals that mention GAI/GPT/LLM, with the goal of evaluating existing guidelines, analyzing the disparity in their recommendations, and identifying common rules that can be brought into the Delphi consensus process, c) a Delphi survey to establish agreement on the items for the guidelines, ensuring principled GAI/GPT/LLM use, disclosure, and reporting in academia, and d) the subsequent development and dissemination of the finalized guidelines and their supplementary explanation and elaboration documents.","meta":{"url":"http://arxiv.org/abs/2307.08974v1"},"cats":{"new-dataset":0.1066046238,"dev-research":0.3022123247,"prompt-eng":0.3926585071,"data-quality":0.1192631618,"ml-security":0.0411879065}}
{"text":"We study the problem of maintaining a differentially private decaying sum under continual observation.","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.0898995757,"dev-research":0.1762157011,"prompt-eng":0.3331826786,"data-quality":0.1311089641,"ml-security":0.4037096924}}
{"text":"We give a unifying framework and an efficient algorithm for this problem for \\emph{any sufficiently smooth} function.","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.1070416532,"dev-research":0.2260899263,"prompt-eng":0.3717965827,"data-quality":0.1508583415,"ml-security":0.110205582}}
{"text":"Our algorithm is the first differentially private algorithm that does not have a multiplicative error for polynomially-decaying weights.","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.0347271013,"dev-research":0.1658646066,"prompt-eng":0.3209277836,"data-quality":0.1710875837,"ml-security":0.4015424999}}
{"text":"Our algorithm improves on all prior works on differentially private decaying sums under continual observation and recovers exactly the additive error for the special case of continual counting from Henzinger et al. (SODA 2023) as a corollary.   ","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.0845794662,"dev-research":0.1936239864,"prompt-eng":0.3276892606,"data-quality":0.1849124318,"ml-security":0.2736171204}}
{"text":"Our algorithm is a variant of the factorization mechanism whose error depends on the $\\gamma_2$ and $\\gamma_F$ norm of the underlying matrix.","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.0374240534,"dev-research":0.2049901162,"prompt-eng":0.386475973,"data-quality":0.2501495283,"ml-security":0.0893203041}}
{"text":"We give a constructive proof for an almost exact upper bound on the $\\gamma_2$ and $\\gamma_F$ norm and an almost tight lower bound on the $\\gamma_2$ norm for a large class of lower-triangular matrices.","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.0750048198,"dev-research":0.2516757021,"prompt-eng":0.357698082,"data-quality":0.1458033953,"ml-security":0.1324586542}}
{"text":"This is the first non-trivial lower bound for lower-triangular matrices whose non-zero entries are not all the same.","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.083356767,"dev-research":0.2489057502,"prompt-eng":0.2959804968,"data-quality":0.1202409555,"ml-security":0.0938828615}}
{"text":"It includes matrices for all continual decaying sums problems, resulting in an upper bound on the additive error of any differentially private decaying sums algorithm under continual observation.   ","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.0706326165,"dev-research":0.2284870547,"prompt-eng":0.305398405,"data-quality":0.1068740045,"ml-security":0.2956831097}}
{"text":"We also explore some implications of our result in discrepancy theory and operator algebra.","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.0166098459,"dev-research":0.3145526345,"prompt-eng":0.4019783183,"data-quality":0.2620170677,"ml-security":0.1041886169}}
{"text":"Given the importance of the $\\gamma_2$ norm in computer science and the extensive work in mathematics, we believe our result will have further applications.","meta":{"url":"http://arxiv.org/abs/2307.08970v1"},"cats":{"new-dataset":0.0575220079,"dev-research":0.279931176,"prompt-eng":0.3940883909,"data-quality":0.1554101629,"ml-security":0.112747084}}
{"text":"Multi-robot patrolling is the potential application for robotic systems to survey wide areas efficiently without human burdens and mistakes.","meta":{"url":"http://arxiv.org/abs/2307.08966v1"},"cats":{"new-dataset":0.0422780681,"dev-research":0.2321055604,"prompt-eng":0.4235025805,"data-quality":0.0836276612,"ml-security":0.0923900359}}
{"text":"However, such systems have few examples of real-world applications due to their lack of human predictability.","meta":{"url":"http://arxiv.org/abs/2307.08966v1"},"cats":{"new-dataset":0.0332409509,"dev-research":0.3102496066,"prompt-eng":0.3817012971,"data-quality":0.1048778318,"ml-security":0.2453014382}}
{"text":"This paper proposes an algorithm: Local Reactive (LR) for multi-robot patrolling to satisfy both needs: (i)patrol efficiently and (ii)provide humans with better situation awareness to enhance system predictability.","meta":{"url":"http://arxiv.org/abs/2307.08966v1"},"cats":{"new-dataset":0.1001223402,"dev-research":0.2383539087,"prompt-eng":0.4459331658,"data-quality":0.0746960806,"ml-security":0.1669859112}}
{"text":"Each robot operating according to the proposed algorithm selects its patrol target from the local areas around the robot's current location by two requirements: (i)patrol location with greater need, (ii)report its achievements to the base station.","meta":{"url":"http://arxiv.org/abs/2307.08966v1"},"cats":{"new-dataset":0.0285333582,"dev-research":0.2548309394,"prompt-eng":0.491833726,"data-quality":0.0999192523,"ml-security":0.0841457235}}
{"text":"The algorithm is distributed and coordinates the robots without centralized control by sharing their patrol achievements and degree of need to report to the base station.","meta":{"url":"http://arxiv.org/abs/2307.08966v1"},"cats":{"new-dataset":0.057813171,"dev-research":0.2413412251,"prompt-eng":0.4410006201,"data-quality":0.1010289478,"ml-security":0.1340719331}}
{"text":"The proposed algorithm performed better than existing algorithms in both patrolling and the base station's situation awareness.","meta":{"url":"http://arxiv.org/abs/2307.08966v1"},"cats":{"new-dataset":0.07347296,"dev-research":0.2559125925,"prompt-eng":0.4130492879,"data-quality":0.0974035721,"ml-security":0.1326510153}}
{"text":"Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning.","meta":{"url":"http://arxiv.org/abs/2307.08964v1"},"cats":{"new-dataset":0.0097269351,"dev-research":0.2123539377,"prompt-eng":0.4216357059,"data-quality":0.1591503156,"ml-security":0.177985636}}
{"text":"By learning an optimizer $\\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience.","meta":{"url":"http://arxiv.org/abs/2307.08964v1"},"cats":{"new-dataset":0.0115210265,"dev-research":0.2406835832,"prompt-eng":0.4268785038,"data-quality":0.0827451791,"ml-security":0.1445050558}}
{"text":"The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\\circ \\mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\\mathbf{g}$ during both training and testing.","meta":{"url":"http://arxiv.org/abs/2307.08964v1"},"cats":{"new-dataset":0.0090845069,"dev-research":0.243436522,"prompt-eng":0.4379377451,"data-quality":0.1476467328,"ml-security":0.1685623324}}
{"text":"The training is further challenged by sparse gradients of $\\mathbf{g}$, especially for combinatorial solvers.","meta":{"url":"http://arxiv.org/abs/2307.08964v1"},"cats":{"new-dataset":0.0135415588,"dev-research":0.1903317793,"prompt-eng":0.3803639981,"data-quality":0.1097405593,"ml-security":0.1736526373}}
{"text":"To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replacement for $f\\circ \\mathbf{g}$. This surrogate, learnable by neural networks, can be computed faster than the solver $\\mathbf{g}$, provides dense and smooth gradients during training, can generalize to unseen optimization problems, and is efficiently learned via alternating optimization.","meta":{"url":"http://arxiv.org/abs/2307.08964v1"},"cats":{"new-dataset":0.0554960423,"dev-research":0.199579414,"prompt-eng":0.3814189364,"data-quality":0.0806494481,"ml-security":0.1619524973}}
{"text":"We test our approach on both synthetic problems, including shortest path and multidimensional knapsack, and real-world problems such as portfolio optimization, achieving comparable or superior objective values compared to state-of-the-art baselines while reducing the number of calls to $\\mathbf{g}$. Notably, our approach outperforms existing methods for computationally expensive high-dimensional problems.","meta":{"url":"http://arxiv.org/abs/2307.08964v1"},"cats":{"new-dataset":0.0579149384,"dev-research":0.1787259234,"prompt-eng":0.3529203864,"data-quality":0.0568284645,"ml-security":0.1201977073}}
{"text":"In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX.","meta":{"url":"http://arxiv.org/abs/2307.08962v1"},"cats":{"new-dataset":0.0628323557,"dev-research":0.2501101633,"prompt-eng":0.4269395624,"data-quality":0.0345158715,"ml-security":0.1453254227}}
{"text":"Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL).","meta":{"url":"http://arxiv.org/abs/2307.08962v1"},"cats":{"new-dataset":0.0194893253,"dev-research":0.262523075,"prompt-eng":0.4601428465,"data-quality":0.1698283959,"ml-security":0.0990387741}}
{"text":"REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance.","meta":{"url":"http://arxiv.org/abs/2307.08962v1"},"cats":{"new-dataset":0.0332876824,"dev-research":0.3097947885,"prompt-eng":0.4700593131,"data-quality":0.0677488867,"ml-security":0.1973933758}}
{"text":"This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning.","meta":{"url":"http://arxiv.org/abs/2307.08962v1"},"cats":{"new-dataset":0.0729545467,"dev-research":0.2850196645,"prompt-eng":0.4059031808,"data-quality":0.0519999967,"ml-security":0.0709874011}}
{"text":"Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniques.","meta":{"url":"http://arxiv.org/abs/2307.08962v1"},"cats":{"new-dataset":0.0276206066,"dev-research":0.3864989353,"prompt-eng":0.4483504891,"data-quality":0.056708996,"ml-security":0.0288648447}}
{"text":"Notably, REX-based methods exhibit remarkable reductions in execution time, enhancing their practical applicability across a diverse set of scenarios.","meta":{"url":"http://arxiv.org/abs/2307.08962v1"},"cats":{"new-dataset":0.0116792372,"dev-research":0.3896182101,"prompt-eng":0.4154953219,"data-quality":0.0572350148,"ml-security":0.1752875152}}
{"text":"Over the last several decades, there has been rapid growth in the number and scope of agricultural genetics, genomics and breeding (GGB) databases and resources.","meta":{"url":"http://arxiv.org/abs/2307.08958v1"},"cats":{"new-dataset":0.3745310205,"dev-research":0.2611856008,"prompt-eng":0.4013081468,"data-quality":0.0756758891,"ml-security":0.0509133962}}
{"text":"The AgBioData Consortium (https://www.agbiodata.org/) currently represents 44 databases and resources covering model or crop plant and animal GGB data, ontologies, pathways, genetic variation and breeding platforms (referred to as 'databases' throughout).","meta":{"url":"http://arxiv.org/abs/2307.08958v1"},"cats":{"new-dataset":0.7257317462,"dev-research":0.2462971529,"prompt-eng":0.3662771192,"data-quality":0.1107429157,"ml-security":0.0560416859}}
{"text":"One of the goals of the Consortium is to facilitate FAIR (Findable, Accessible, Interoperable, and Reusable) data management and the integration of datasets which requires data sharing, along with structured vocabularies and/or ontologies.","meta":{"url":"http://arxiv.org/abs/2307.08958v1"},"cats":{"new-dataset":0.5900687818,"dev-research":0.3225486548,"prompt-eng":0.3429130463,"data-quality":0.1173082109,"ml-security":0.1084638664}}
{"text":"Two AgBioData working groups, focused on Data Sharing and Ontologies, conducted a survey to assess the status and future needs of the members in those areas.","meta":{"url":"http://arxiv.org/abs/2307.08958v1"},"cats":{"new-dataset":0.4555466483,"dev-research":0.3351623957,"prompt-eng":0.3991792863,"data-quality":0.0802143469,"ml-security":0.0559909013}}
{"text":"A total of 33 researchers responded to the survey, representing 37 databases.","meta":{"url":"http://arxiv.org/abs/2307.08958v1"},"cats":{"new-dataset":0.3909843576,"dev-research":0.3010740696,"prompt-eng":0.4078992875,"data-quality":0.1648704711,"ml-security":0.0680092151}}
{"text":"Results suggest that data sharing practices by AgBioData databases are in a healthy state, but it is not clear whether this is true for all metadata and data types across all databases; and that ontology use has not substantially changed since a similar survey was conducted in 2017.","meta":{"url":"http://arxiv.org/abs/2307.08958v1"},"cats":{"new-dataset":0.2665483405,"dev-research":0.3257937428,"prompt-eng":0.3339115621,"data-quality":0.1627734962,"ml-security":0.1255745789}}
{"text":"We recommend 1) providing training for database personnel in specific data sharing techniques, as well as in ontology use; 2) further study on what metadata is shared, and how well it is shared among databases; 3) promoting an understanding of data sharing and ontologies in the stakeholder community; 4) improving data sharing and ontologies for specific phenotypic data types and formats; and 5) lowering specific barriers to data sharing and ontology use, by identifying sustainability solutions, and the identification, promotion, or development of data standards.","meta":{"url":"http://arxiv.org/abs/2307.08958v1"},"cats":{"new-dataset":0.4960320374,"dev-research":0.3636463653,"prompt-eng":0.3703047818,"data-quality":0.1562415449,"ml-security":0.133497528}}
{"text":"Combined, these improvements are likely to help AgBioData databases increase development efforts towards improved ontology use, and data sharing via programmatic means.","meta":{"url":"http://arxiv.org/abs/2307.08958v1"},"cats":{"new-dataset":0.236529425,"dev-research":0.3920951258,"prompt-eng":0.4122375224,"data-quality":0.1334883233,"ml-security":0.0798507549}}
{"text":"IoT device identification is the process of recognizing and verifying connected IoT devices to the network.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.0436673529,"dev-research":0.3191143627,"prompt-eng":0.456888338,"data-quality":0.2022562679,"ml-security":0.099277672}}
{"text":"This is an essential process for ensuring that only authorized devices can access the network, and it is necessary for network management and maintenance.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.0120394597,"dev-research":0.3503115654,"prompt-eng":0.4205531913,"data-quality":0.0792041149,"ml-security":0.1688650334}}
{"text":"In recent years, machine learning models have been used widely for automating the process of identifying devices in the network.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.0490042992,"dev-research":0.3511130073,"prompt-eng":0.4778797282,"data-quality":0.2460096785,"ml-security":0.2863629475}}
{"text":"However, these models are vulnerable to adversarial attacks that can compromise their accuracy and effectiveness.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.021540676,"dev-research":0.3021675492,"prompt-eng":0.4208180318,"data-quality":0.3269327007,"ml-security":0.8527686915}}
{"text":"To better secure device identification models, discretization techniques enable reduction in the sensitivity of machine learning models to adversarial attacks contributing to the stability and reliability of the model.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.0180437094,"dev-research":0.3044870436,"prompt-eng":0.4047817451,"data-quality":0.1566075868,"ml-security":0.7085364586}}
{"text":"On the other hand, Ensemble methods combine multiple heterogeneous models to reduce the impact of remaining noise or errors in the model.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.0094663212,"dev-research":0.2718405865,"prompt-eng":0.3384699455,"data-quality":0.2007110953,"ml-security":0.1109771272}}
{"text":"Therefore, in this paper, we integrate discretization techniques and ensemble methods and examine it on model robustness against adversarial attacks.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.0266472775,"dev-research":0.2646816727,"prompt-eng":0.3714215179,"data-quality":0.3033627658,"ml-security":0.8104933993}}
{"text":"In other words, we propose a discretization-based ensemble stacking technique to improve the security of our ML models.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.0546858732,"dev-research":0.2588224195,"prompt-eng":0.431829679,"data-quality":0.1534056708,"ml-security":0.4797922321}}
{"text":"We evaluate the performance of different ML-based IoT device identification models against white box and black box attacks using a real-world dataset comprised of network traffic from 28 IoT devices.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.2110483017,"dev-research":0.2750246004,"prompt-eng":0.3773341854,"data-quality":0.0970047935,"ml-security":0.5564674889}}
{"text":"We demonstrate that the proposed method enables robustness to the models for IoT device identification.","meta":{"url":"http://arxiv.org/abs/2307.08955v1"},"cats":{"new-dataset":0.0323948983,"dev-research":0.251993175,"prompt-eng":0.4292589188,"data-quality":0.2483064241,"ml-security":0.367266117}}
{"text":"Forecasting how landslides will evolve over time or whether they will fail is a challenging task due to a variety of factors, both internal and external.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.0938641967,"dev-research":0.3243591265,"prompt-eng":0.4101283098,"data-quality":0.0784060147,"ml-security":0.1579522885}}
{"text":"Despite their considerable potential to address these challenges, deep learning techniques lack interpretability, undermining the credibility of the forecasts they produce.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.0696485286,"dev-research":0.3214342071,"prompt-eng":0.3629208489,"data-quality":0.3596992565,"ml-security":0.3206649483}}
{"text":"The recent development of transformer-based deep learning offers untapped possibilities for forecasting landslides with unprecedented interpretability and nonlinear feature learning capabilities.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.1389176869,"dev-research":0.27673176,"prompt-eng":0.3861642393,"data-quality":0.1003173977,"ml-security":0.1966111106}}
{"text":"Here, we present a deep learning pipeline that is capable of predicting landslide behavior holistically, which employs a transformer-based network called LFIT to learn complex nonlinear relationships from prior knowledge and multiple source data, identifying the most relevant variables, and demonstrating a comprehensive understanding of landslide evolution and temporal patterns.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.3523276713,"dev-research":0.3263103078,"prompt-eng":0.3968758285,"data-quality":0.1009511376,"ml-security":0.1871503613}}
{"text":"By integrating prior knowledge, we provide improvement in holistic landslide forecasting, enabling us to capture diverse responses to various influencing factors in different local landslide areas.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.1284323296,"dev-research":0.3372063484,"prompt-eng":0.4382645529,"data-quality":0.0728597226,"ml-security":0.0764315111}}
{"text":"Using deformation observations as proxies for measuring the kinetics of landslides, we validate our approach by training models to forecast reservoir landslides in the Three Gorges Reservoir and creeping landslides on the Tibetan Plateau.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.1742014529,"dev-research":0.2342457116,"prompt-eng":0.394403132,"data-quality":0.1205267136,"ml-security":0.1393189139}}
{"text":"When prior knowledge is incorporated, we show that interpretable landslide forecasting effectively identifies influential factors across various landslides.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.1207416233,"dev-research":0.353894148,"prompt-eng":0.4332947068,"data-quality":0.106963848,"ml-security":0.1153384991}}
{"text":"It further elucidates how local areas respond to these factors, making landslide behavior and trends more interpretable and predictable.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.0505089147,"dev-research":0.3995193707,"prompt-eng":0.4132304043,"data-quality":0.0748677361,"ml-security":0.0931323817}}
{"text":"The findings from this study will contribute to understanding landslide behavior in a new way and make the proposed approach applicable to other complex disasters influenced by internal and external factors in the future.","meta":{"url":"http://arxiv.org/abs/2307.08951v1"},"cats":{"new-dataset":0.1632404118,"dev-research":0.3897607267,"prompt-eng":0.3951105013,"data-quality":0.0782816237,"ml-security":0.1286000019}}
{"text":"By absorbing the merits of both the model- and data-driven methods, deep physics-engaged learning scheme achieves high-accuracy and interpretable image reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.08950v1"},"cats":{"new-dataset":0.1905398952,"dev-research":0.2289280566,"prompt-eng":0.4034486017,"data-quality":0.0924420535,"ml-security":0.1043271334}}
{"text":"It has attracted growing attention and become the mainstream for inverse imaging tasks.","meta":{"url":"http://arxiv.org/abs/2307.08950v1"},"cats":{"new-dataset":0.0344829874,"dev-research":0.2724174727,"prompt-eng":0.4145595876,"data-quality":0.0611817177,"ml-security":0.067516242}}
{"text":"Focusing on the image compressed sensing (CS) problem, we find the intrinsic defect of this emerging paradigm, widely implemented by deep algorithm-unrolled networks, in which more plain iterations involving real physics will bring enormous computation cost and long inference time, hindering their practical application.","meta":{"url":"http://arxiv.org/abs/2307.08950v1"},"cats":{"new-dataset":0.1066324889,"dev-research":0.2208521639,"prompt-eng":0.3235305469,"data-quality":0.1366321694,"ml-security":0.2083868771}}
{"text":"A novel deep $\\textbf{P}$hysics-guided un$\\textbf{R}$olled recovery $\\textbf{L}$earning ($\\textbf{PRL}$) framework is proposed by generalizing the traditional iterative recovery model from image domain (ID) to the high-dimensional feature domain (FD).","meta":{"url":"http://arxiv.org/abs/2307.08950v1"},"cats":{"new-dataset":0.076040766,"dev-research":0.1942091389,"prompt-eng":0.4311124036,"data-quality":0.1148930552,"ml-security":0.0661321358}}
{"text":"A compact multiscale unrolling architecture is then developed to enhance the network capacity and keep real-time inference speeds.","meta":{"url":"http://arxiv.org/abs/2307.08950v1"},"cats":{"new-dataset":0.0313714532,"dev-research":0.2400561843,"prompt-eng":0.3478191638,"data-quality":0.0493740508,"ml-security":0.0746115965}}
{"text":"Taking two different perspectives of optimization and range-nullspace decomposition, instead of building an algorithm-specific unrolled network, we provide two implementations: $\\textbf{PRL-PGD}$ and $\\textbf{PRL-RND}$. Experiments exhibit the significant performance and efficiency leading of PRL networks over other state-of-the-art methods with a large potential for further improvement and real application to other inverse imaging problems or optimization models.","meta":{"url":"http://arxiv.org/abs/2307.08950v1"},"cats":{"new-dataset":0.0568057455,"dev-research":0.1882002151,"prompt-eng":0.3604356598,"data-quality":0.0985022626,"ml-security":0.1111176262}}
{"text":"Multi-tenancy in public clouds may lead to co-location interference on shared resources, which possibly results in performance degradation of cloud applications.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.0393595069,"dev-research":0.3073603583,"prompt-eng":0.3357612992,"data-quality":0.1142456452,"ml-security":0.1822951635}}
{"text":"Cloud providers want to know when such events happen and how serious the degradation is, to perform interference-aware migrations and alleviate the problem.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.0693242614,"dev-research":0.3037698435,"prompt-eng":0.3985398694,"data-quality":0.1848838633,"ml-security":0.2343328493}}
{"text":"However, virtual machines (VM) in Infrastructure-as-a-Service public clouds are black-boxes to providers, where application-level performance information cannot be acquired.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.0359721004,"dev-research":0.28906963,"prompt-eng":0.3265191356,"data-quality":0.1277105263,"ml-security":0.3065192451}}
{"text":"This makes performance monitoring intensely challenging as cloud providers can only rely on low-level metrics such as CPU usage and hardware counters.   ","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.044984134,"dev-research":0.3858328503,"prompt-eng":0.3990761272,"data-quality":0.1316278266,"ml-security":0.1628069134}}
{"text":"We propose a novel machine learning framework, Alioth, to monitor the performance degradation of cloud applications.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.1657306684,"dev-research":0.3531097215,"prompt-eng":0.3552231438,"data-quality":0.1699511367,"ml-security":0.3687482261}}
{"text":"To feed the data-hungry models, we first elaborate interference generators and conduct comprehensive co-location experiments on a testbed to build Alioth-dataset which reflects the complexity and dynamicity in real-world scenarios.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.5040552523,"dev-research":0.2603346565,"prompt-eng":0.3715670266,"data-quality":0.1523329988,"ml-security":0.2380176918}}
{"text":"Then we construct Alioth by (1) augmenting features via recovering low-level metrics under no interference using denoising auto-encoders, (2) devising a transfer learning model based on domain adaptation neural network to make models generalize on test cases unseen in offline training, and (3) developing a SHAP explainer to automate feature selection and enhance model interpretability.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.0533754253,"dev-research":0.3045060499,"prompt-eng":0.4391991651,"data-quality":0.2325474662,"ml-security":0.2983748667}}
{"text":"Experiments show that Alioth achieves an average mean absolute error of 5.29% offline and 10.8% when testing on applications unseen in the training stage, outperforming the baseline methods.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.0538934064,"dev-research":0.3025843037,"prompt-eng":0.3817248212,"data-quality":0.192239319,"ml-security":0.1431170362}}
{"text":"Alioth is also robust in signaling quality-of-service violation under dynamicity.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.0262814336,"dev-research":0.3193510095,"prompt-eng":0.37634298,"data-quality":0.2168807856,"ml-security":0.3657293353}}
{"text":"Finally, we demonstrate a possible application of Alioth's interpretability, providing insights to benefit the decision-making of cloud operators.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.0483852282,"dev-research":0.4140431495,"prompt-eng":0.3891473796,"data-quality":0.1387757351,"ml-security":0.1609968843}}
{"text":"The dataset and code of Alioth have been released on GitHub.","meta":{"url":"http://arxiv.org/abs/2307.08949v1"},"cats":{"new-dataset":0.9009712434,"dev-research":0.2885906819,"prompt-eng":0.33544577,"data-quality":0.1017848947,"ml-security":0.0945596325}}
{"text":"Deep Learning (DL) applications are being used to solve problems in critical domains (e.g., autonomous driving or medical diagnosis systems).","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.0644261133,"dev-research":0.3307427723,"prompt-eng":0.3619914737,"data-quality":0.1336374025,"ml-security":0.3172589398}}
{"text":"Thus, developers need to debug their systems to ensure that the expected behavior is delivered.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.0346686147,"dev-research":0.7488301642,"prompt-eng":0.4797963367,"data-quality":0.1932740578,"ml-security":0.3886977592}}
{"text":"However, it is hard and expensive to debug DNNs.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.0988157669,"dev-research":0.5501839859,"prompt-eng":0.3904976079,"data-quality":0.2004684447,"ml-security":0.3542319268}}
{"text":"When the failure symptoms or unsatisfied accuracies are reported after training, we lose the traceability as to which part of the DNN program is responsible for the failure.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.0340820605,"dev-research":0.4122460297,"prompt-eng":0.416144995,"data-quality":0.48312361,"ml-security":0.2399515032}}
{"text":"Even worse, sometimes, a deep learning program has different types of bugs.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.0892959222,"dev-research":0.4325231727,"prompt-eng":0.3887636515,"data-quality":0.3833318998,"ml-security":0.3691120706}}
{"text":"To address the challenges of debugging DNN models, we propose a novel data-driven approach that leverages model features to learn problem patterns.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.422600383,"dev-research":0.5590893012,"prompt-eng":0.435766745,"data-quality":0.2634068763,"ml-security":0.3571626045}}
{"text":"Our approach extracts these features, which represent semantic information of faults during DNN training.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.1571323101,"dev-research":0.5137282066,"prompt-eng":0.452611043,"data-quality":0.5049668007,"ml-security":0.2823778874}}
{"text":"Our technique uses these features as a training dataset to learn and infer DNN fault patterns.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.3422484374,"dev-research":0.4615212792,"prompt-eng":0.4232366282,"data-quality":0.3229867564,"ml-security":0.2926190844}}
{"text":"Also, our methodology automatically links bug symptoms to their root causes, without the need for manually crafted mappings, so that developers can take the necessary steps to fix faults.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.0721985438,"dev-research":0.6388799215,"prompt-eng":0.4782521476,"data-quality":0.3535019519,"ml-security":0.1556321457}}
{"text":"We evaluate our approach using real-world and mutated models.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.0464556639,"dev-research":0.2644669406,"prompt-eng":0.401815122,"data-quality":0.177730633,"ml-security":0.3440880716}}
{"text":"Our results demonstrate that our technique can effectively detect and diagnose different bug types.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.1057952665,"dev-research":0.5630027618,"prompt-eng":0.471514319,"data-quality":0.4373692775,"ml-security":0.2083918667}}
{"text":"Finally, our technique achieved better accuracy, precision, and recall than prior work for mutated models.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.03979139,"dev-research":0.2903396548,"prompt-eng":0.4315511626,"data-quality":0.222196088,"ml-security":0.1622908991}}
{"text":"Also, our approach achieved comparable results for real-world models in terms of accuracy and performance to the state-of-the-art.","meta":{"url":"http://arxiv.org/abs/2307.08947v1"},"cats":{"new-dataset":0.0826525372,"dev-research":0.1999475783,"prompt-eng":0.3883869271,"data-quality":0.1051618334,"ml-security":0.0554203271}}
{"text":"Wireless networks are vulnerable to physical layer spoofing attacks due to the wireless broadcast nature, thus, integrating communications and security (ICAS) is urgently needed for 6G endogenous security.","meta":{"url":"http://arxiv.org/abs/2307.08946v1"},"cats":{"new-dataset":0.0737177342,"dev-research":0.2779057865,"prompt-eng":0.4021419946,"data-quality":0.1376673315,"ml-security":0.4392556531}}
{"text":"In this letter, we propose an environment semantics enabled physical layer authentication network based on deep learning, namely EsaNet, to authenticate the spoofing from the underlying wireless protocol.","meta":{"url":"http://arxiv.org/abs/2307.08946v1"},"cats":{"new-dataset":0.1453541454,"dev-research":0.3548861475,"prompt-eng":0.3838354725,"data-quality":0.1261388742,"ml-security":0.5097106552}}
{"text":"Specifically, the frequency independent wireless channel fingerprint (FiFP) is extracted from the channel state information (CSI) of a massive multi-input multi-output (MIMO) system based on environment semantics knowledge.","meta":{"url":"http://arxiv.org/abs/2307.08946v1"},"cats":{"new-dataset":0.1768138115,"dev-research":0.3051022405,"prompt-eng":0.4533121871,"data-quality":0.108311446,"ml-security":0.0829295884}}
{"text":"Then, we transform the received signal into a two-dimensional red green blue (RGB) image and apply the you only look once (YOLO), a single-stage object detection network, to quickly capture the FiFP.","meta":{"url":"http://arxiv.org/abs/2307.08946v1"},"cats":{"new-dataset":0.1670852148,"dev-research":0.2100908162,"prompt-eng":0.4545854522,"data-quality":0.1135936249,"ml-security":0.0711215031}}
{"text":"Next, a lightweight classification network is designed to distinguish the legitimate from the illegitimate users.","meta":{"url":"http://arxiv.org/abs/2307.08946v1"},"cats":{"new-dataset":0.0559945066,"dev-research":0.358547083,"prompt-eng":0.4200222398,"data-quality":0.3589831123,"ml-security":0.5536070444}}
{"text":"Finally, the experimental results show that the proposed EsaNet can effectively detect physical layer spoofing attacks and is robust in time-varying wireless environments.","meta":{"url":"http://arxiv.org/abs/2307.08946v1"},"cats":{"new-dataset":0.0858184354,"dev-research":0.3406194861,"prompt-eng":0.3770466928,"data-quality":0.1494272625,"ml-security":0.6218213906}}
{"text":"Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias.","meta":{"url":"http://arxiv.org/abs/2307.08945v1"},"cats":{"new-dataset":0.0080026867,"dev-research":0.3281392606,"prompt-eng":0.3627973186,"data-quality":0.1368691954,"ml-security":0.4169784512}}
{"text":"However, such methodologies largely assume that observed labels in training data are correct.","meta":{"url":"http://arxiv.org/abs/2307.08945v1"},"cats":{"new-dataset":0.0259612181,"dev-research":0.317073436,"prompt-eng":0.4297604118,"data-quality":0.6843973929,"ml-security":0.1797294036}}
{"text":"This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation.","meta":{"url":"http://arxiv.org/abs/2307.08945v1"},"cats":{"new-dataset":0.0206613708,"dev-research":0.3245792655,"prompt-eng":0.3811405507,"data-quality":0.5534278086,"ml-security":0.183021617}}
{"text":"In particular, human-generated labels are prone to encoding societal biases.","meta":{"url":"http://arxiv.org/abs/2307.08945v1"},"cats":{"new-dataset":0.0412431398,"dev-research":0.3980488978,"prompt-eng":0.4370914589,"data-quality":0.5508222791,"ml-security":0.2283417381}}
{"text":"While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem.","meta":{"url":"http://arxiv.org/abs/2307.08945v1"},"cats":{"new-dataset":0.0174272531,"dev-research":0.3274627693,"prompt-eng":0.4250998865,"data-quality":0.653030549,"ml-security":0.1508345237}}
{"text":"We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias.","meta":{"url":"http://arxiv.org/abs/2307.08945v1"},"cats":{"new-dataset":0.0556164091,"dev-research":0.2871508375,"prompt-eng":0.4277947667,"data-quality":0.6528184015,"ml-security":0.2821358538}}
{"text":"After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.","meta":{"url":"http://arxiv.org/abs/2307.08945v1"},"cats":{"new-dataset":0.3093199503,"dev-research":0.3117319123,"prompt-eng":0.3424897018,"data-quality":0.5446985852,"ml-security":0.4270836506}}
{"text":"Deep learning has been successfully applied to human activity recognition.","meta":{"url":"http://arxiv.org/abs/2307.08944v1"},"cats":{"new-dataset":0.1204919607,"dev-research":0.2560797335,"prompt-eng":0.3355271566,"data-quality":0.0995836297,"ml-security":0.1290858859}}
{"text":"However, training deep neural networks requires explicitly labeled data which is difficult to acquire.","meta":{"url":"http://arxiv.org/abs/2307.08944v1"},"cats":{"new-dataset":0.0389590443,"dev-research":0.3036263721,"prompt-eng":0.3680029134,"data-quality":0.4080324662,"ml-security":0.3879770451}}
{"text":"In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels.","meta":{"url":"http://arxiv.org/abs/2307.08944v1"},"cats":{"new-dataset":0.2461823445,"dev-research":0.2232287658,"prompt-eng":0.3402816127,"data-quality":0.3597189957,"ml-security":0.1321456633}}
{"text":"The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space.","meta":{"url":"http://arxiv.org/abs/2307.08944v1"},"cats":{"new-dataset":0.13274182,"dev-research":0.266201665,"prompt-eng":0.3645288992,"data-quality":0.1196570117,"ml-security":0.1533387681}}
{"text":"Thus, the trained model can work as a metric for a wide range of different clustering algorithms.","meta":{"url":"http://arxiv.org/abs/2307.08944v1"},"cats":{"new-dataset":0.0136407136,"dev-research":0.2578123286,"prompt-eng":0.3785398965,"data-quality":0.1355779766,"ml-security":0.1496570015}}
{"text":"The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities.","meta":{"url":"http://arxiv.org/abs/2307.08944v1"},"cats":{"new-dataset":0.0260459469,"dev-research":0.233663598,"prompt-eng":0.3497626423,"data-quality":0.1396622723,"ml-security":0.1265663818}}
{"text":"We evaluate the model on three datasets to verify its effectiveness in segmentation and recognition of continuous human activity sequences.","meta":{"url":"http://arxiv.org/abs/2307.08944v1"},"cats":{"new-dataset":0.4703871945,"dev-research":0.2169339945,"prompt-eng":0.3570268884,"data-quality":0.1357865178,"ml-security":0.0957135414}}
{"text":"Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications.","meta":{"url":"http://arxiv.org/abs/2307.08941v1"},"cats":{"new-dataset":0.0591946766,"dev-research":0.2332193896,"prompt-eng":0.5118542364,"data-quality":0.2407197969,"ml-security":0.0894407173}}
{"text":"However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power.","meta":{"url":"http://arxiv.org/abs/2307.08941v1"},"cats":{"new-dataset":0.0142103651,"dev-research":0.2661049743,"prompt-eng":0.4086704157,"data-quality":0.0848277793,"ml-security":0.1173860319}}
{"text":"Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored.","meta":{"url":"http://arxiv.org/abs/2307.08941v1"},"cats":{"new-dataset":0.0163433398,"dev-research":0.2181827285,"prompt-eng":0.4029979506,"data-quality":0.1277486892,"ml-security":0.0798711166}}
{"text":"In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion.","meta":{"url":"http://arxiv.org/abs/2307.08941v1"},"cats":{"new-dataset":0.0723089891,"dev-research":0.2328089108,"prompt-eng":0.3644714566,"data-quality":0.1244523335,"ml-security":0.2448836}}
{"text":"To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM.","meta":{"url":"http://arxiv.org/abs/2307.08941v1"},"cats":{"new-dataset":0.1756466585,"dev-research":0.1586453862,"prompt-eng":0.4102191286,"data-quality":0.1305543988,"ml-security":0.0850861782}}
{"text":"Extensive experiments of PLM fine-tuning on both natural language understanding (NLU) and generation (NLG) tasks are provided to verify the effectiveness of the proposed method MLP fusion.","meta":{"url":"http://arxiv.org/abs/2307.08941v1"},"cats":{"new-dataset":0.0609679212,"dev-research":0.3090265306,"prompt-eng":0.4811349728,"data-quality":0.2708923726,"ml-security":0.0661744937}}
{"text":"Our code is available at https://github.com/weitianxin/MLP_Fusion.","meta":{"url":"http://arxiv.org/abs/2307.08941v1"},"cats":{"new-dataset":0.1736399827,"dev-research":0.2216415392,"prompt-eng":0.4426426033,"data-quality":0.1119072904,"ml-security":0.0835139197}}
{"text":"Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles.","meta":{"url":"http://arxiv.org/abs/2307.08939v1"},"cats":{"new-dataset":0.0260538981,"dev-research":0.2941640247,"prompt-eng":0.4089875334,"data-quality":0.1380275961,"ml-security":0.0928437235}}
{"text":"This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions.","meta":{"url":"http://arxiv.org/abs/2307.08939v1"},"cats":{"new-dataset":0.1609077209,"dev-research":0.3272404602,"prompt-eng":0.3756483142,"data-quality":0.1988829923,"ml-security":0.843352671}}
{"text":"We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time.","meta":{"url":"http://arxiv.org/abs/2307.08939v1"},"cats":{"new-dataset":0.0782511879,"dev-research":0.3172966174,"prompt-eng":0.4511169749,"data-quality":0.1431749267,"ml-security":0.6073210843}}
{"text":"We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW).","meta":{"url":"http://arxiv.org/abs/2307.08939v1"},"cats":{"new-dataset":0.4461210108,"dev-research":0.3532469594,"prompt-eng":0.4235839431,"data-quality":0.13062684,"ml-security":0.5336981301}}
{"text":"Experimental results show that the proposed attack achieves 142.9x higher success rate in causing accidents than random attacks and is mitigated 89.6% less by the safety features while being stealthy and robust to real-world factors and dynamic changes in the environment.","meta":{"url":"http://arxiv.org/abs/2307.08939v1"},"cats":{"new-dataset":0.0896910387,"dev-research":0.3577874525,"prompt-eng":0.4262350279,"data-quality":0.1895351758,"ml-security":0.7657930703}}
{"text":"This study provides insights into the role of human operators and basic safety interventions in preventing attacks.","meta":{"url":"http://arxiv.org/abs/2307.08939v1"},"cats":{"new-dataset":0.045212839,"dev-research":0.4238244769,"prompt-eng":0.4142541234,"data-quality":0.0975130242,"ml-security":0.6631851883}}
{"text":"Large language models~(LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data.","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.1216152292,"dev-research":0.2160866886,"prompt-eng":0.5730841112,"data-quality":0.1285147301,"ml-security":0.0659205318}}
{"text":"However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT.","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.2639473352,"dev-research":0.3376419949,"prompt-eng":0.3827862021,"data-quality":0.4173125398,"ml-security":0.162539256}}
{"text":"In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT).","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.2169384538,"dev-research":0.2690643304,"prompt-eng":0.4269414293,"data-quality":0.3094267457,"ml-security":0.1684300254}}
{"text":"To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data.","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.7480809523,"dev-research":0.2717883195,"prompt-eng":0.2992188564,"data-quality":0.1799794145,"ml-security":0.1196149359}}
{"text":"AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and its 13B variant matches $>90\\%$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks.","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.1291139263,"dev-research":0.26404525,"prompt-eng":0.3785028651,"data-quality":0.1883242897,"ml-security":0.1002797834}}
{"text":"It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same number of epochs as Alpaca(7B)","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.0982072119,"dev-research":0.2877014198,"prompt-eng":0.3354312573,"data-quality":0.086987429,"ml-security":0.082962034}}
{"text":"but on fewer data, using 4$\\times$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.}.","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.0937691582,"dev-research":0.2578069132,"prompt-eng":0.3199451247,"data-quality":0.1070722768,"ml-security":0.1006728473}}
{"text":"Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models.","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.1558064857,"dev-research":0.3533971647,"prompt-eng":0.4637367364,"data-quality":0.1069532313,"ml-security":0.1067496898}}
{"text":"Our project page is available at: \\url{https://lichang-chen.github.io/AlpaGasus/}.","meta":{"url":"http://arxiv.org/abs/2307.08701v1"},"cats":{"new-dataset":0.2951698328,"dev-research":0.2921726371,"prompt-eng":0.3745925146,"data-quality":0.0814194339,"ml-security":0.062649212}}
{"text":"While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which uses a single pre-training stage to address both families of tasks simultaneously.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.0679472284,"dev-research":0.2287939082,"prompt-eng":0.5005516362,"data-quality":0.1476052717,"ml-security":0.1709136495}}
{"text":"We identify diffusion models as a prime candidate.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.0376728265,"dev-research":0.1724428857,"prompt-eng":0.4142223838,"data-quality":0.0858584019,"ml-security":0.1979232688}}
{"text":"Diffusion models have risen to prominence as a state-of-the-art method for image generation, denoising, inpainting, super-resolution, manipulation, etc.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.0263848198,"dev-research":0.2379987332,"prompt-eng":0.4128280663,"data-quality":0.0872167893,"ml-security":0.0730270986}}
{"text":"Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high fidelity, diverse, novel images.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.0620737076,"dev-research":0.2422793558,"prompt-eng":0.4307187425,"data-quality":0.1566209993,"ml-security":0.1282121775}}
{"text":"The U-Net architecture, as a convolution-based architecture, generates a diverse set of feature representations in the form of intermediate feature maps.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.0533199006,"dev-research":0.3853469042,"prompt-eng":0.3908202235,"data-quality":0.1034874672,"ml-security":0.1240371761}}
{"text":"We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.2130275394,"dev-research":0.2433963366,"prompt-eng":0.3986298309,"data-quality":0.4162828391,"ml-security":0.2399596801}}
{"text":"We explore optimal methods for extracting and using these embeddings for classification tasks, demonstrating promising results on the ImageNet classification task.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.0570389608,"dev-research":0.2658797117,"prompt-eng":0.4087614042,"data-quality":0.3227172302,"ml-security":0.1938106569}}
{"text":"We find that with careful feature selection and pooling, diffusion models outperform comparable generative-discriminative methods such as BigBiGAN for classification tasks.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.06900261,"dev-research":0.2300021855,"prompt-eng":0.4166617678,"data-quality":0.1612457156,"ml-security":0.1385184116}}
{"text":"We investigate diffusion models in the transfer learning regime, examining their performance on several fine-grained visual classification datasets.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.1209987132,"dev-research":0.2198155978,"prompt-eng":0.3844340669,"data-quality":0.1988207255,"ml-security":0.2229832508}}
{"text":"We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.","meta":{"url":"http://arxiv.org/abs/2307.08702v1"},"cats":{"new-dataset":0.1102560102,"dev-research":0.2577444555,"prompt-eng":0.4469698867,"data-quality":0.2316604267,"ml-security":0.1985626709}}
{"text":"Artificial intelligence onboard satellites has the potential to reduce data transmission requirements, enable real-time decision-making and collaboration within constellations.","meta":{"url":"http://arxiv.org/abs/2307.08700v1"},"cats":{"new-dataset":0.0872563835,"dev-research":0.3636169322,"prompt-eng":0.3937234512,"data-quality":0.0651327936,"ml-security":0.1008568444}}
{"text":"This study deploys a lightweight foundational model called RaVAEn on D-Orbit's ION SCV004 satellite.","meta":{"url":"http://arxiv.org/abs/2307.08700v1"},"cats":{"new-dataset":0.2435973053,"dev-research":0.2459980607,"prompt-eng":0.4009202228,"data-quality":0.0557611885,"ml-security":0.0797714814}}
{"text":"RaVAEn is a variational auto-encoder (VAE) that generates compressed latent vectors from small image tiles, enabling several downstream tasks.","meta":{"url":"http://arxiv.org/abs/2307.08700v1"},"cats":{"new-dataset":0.0595318771,"dev-research":0.2542072936,"prompt-eng":0.428209434,"data-quality":0.1116874638,"ml-security":0.0853041209}}
{"text":"In this work we demonstrate the reliable use of RaVAEn onboard a satellite, achieving an encoding time of 0.110s for tiles of a 4.8x4.8 km$^2$ area.","meta":{"url":"http://arxiv.org/abs/2307.08700v1"},"cats":{"new-dataset":0.2473019686,"dev-research":0.2664803035,"prompt-eng":0.3761402011,"data-quality":0.0757607874,"ml-security":0.070006868}}
{"text":"In addition, we showcase fast few-shot training onboard a satellite using the latent representation of data.","meta":{"url":"http://arxiv.org/abs/2307.08700v1"},"cats":{"new-dataset":0.4045937641,"dev-research":0.2259531271,"prompt-eng":0.4068805695,"data-quality":0.1033827234,"ml-security":0.0765413988}}
{"text":"We compare the deployment of the model on the on-board CPU and on the available Myriad vision processing unit (VPU) accelerator.","meta":{"url":"http://arxiv.org/abs/2307.08700v1"},"cats":{"new-dataset":0.0858060821,"dev-research":0.2065845899,"prompt-eng":0.3984756188,"data-quality":0.054824515,"ml-security":0.0539509807}}
{"text":"To our knowledge, this work shows for the first time the deployment of a multi-task model on-board a CubeSat and the on-board training of a machine learning model.","meta":{"url":"http://arxiv.org/abs/2307.08700v1"},"cats":{"new-dataset":0.1998540668,"dev-research":0.2534954273,"prompt-eng":0.424812515,"data-quality":0.0562002527,"ml-security":0.1208731965}}
{"text":"Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation (SGG) that aims to create a more comprehensive scene graph representation using panoptic segmentation instead of boxes.","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.4189781197,"dev-research":0.2527132179,"prompt-eng":0.429860858,"data-quality":0.1382636703,"ml-security":0.030310008}}
{"text":"However, current PSG methods have limited performance, which can hinder downstream task development.","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.0062436499,"dev-research":0.2894690224,"prompt-eng":0.4183937016,"data-quality":0.0569462936,"ml-security":0.092221115}}
{"text":"To improve PSG methods, we conducted an in-depth analysis to identify the bottleneck of the current PSG models, finding that inter-object pair-wise recall is a crucial factor which was ignored by previous PSG methods.","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.0613491277,"dev-research":0.2513241838,"prompt-eng":0.476713764,"data-quality":0.1420156962,"ml-security":0.0867765529}}
{"text":"Based on this, we present a novel framework:","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.3029979712,"dev-research":0.3239593182,"prompt-eng":0.412005816,"data-quality":0.0962122179,"ml-security":0.1016738305}}
{"text":"Pair then Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn and filter sparse pair-wise relationships between subjects and objects.","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.2140114474,"dev-research":0.2485033264,"prompt-eng":0.3837252276,"data-quality":0.1077802012,"ml-security":0.069619296}}
{"text":"We also observed the sparse nature of object pairs and used this insight to design a lightweight Matrix Learner within the PPN.","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.1248124532,"dev-research":0.2376221896,"prompt-eng":0.3756669917,"data-quality":0.1102162241,"ml-security":0.1493312455}}
{"text":"Through extensive ablation and analysis, our approach significantly improves upon leveraging the strong segmenter baseline.","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.040465574,"dev-research":0.2146586166,"prompt-eng":0.387857207,"data-quality":0.1446905331,"ml-security":0.036524955}}
{"text":"Notably, our approach achieves new state-of-the-art results on the PSG benchmark, with over 10% absolute gains compared to PSGFormer.","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.0488014868,"dev-research":0.2703821367,"prompt-eng":0.4714089636,"data-quality":0.1001919422,"ml-security":0.0501250065}}
{"text":"The code of this paper is publicly available at https://github.com/king159/Pair-Net.","meta":{"url":"http://arxiv.org/abs/2307.08699v1"},"cats":{"new-dataset":0.1257512632,"dev-research":0.2351975153,"prompt-eng":0.3480150861,"data-quality":0.0958194897,"ml-security":0.0697029685}}
{"text":"Flow matching is a recent framework to train generative models that exhibits impressive empirical performance while being relatively easier to train compared with diffusion-based models.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.03997878,"dev-research":0.2347903638,"prompt-eng":0.4456550522,"data-quality":0.1368002729,"ml-security":0.097159381}}
{"text":"Despite its advantageous properties, prior methods still face the challenges of expensive computing and a large number of function evaluations of off-the-shelf solvers in the pixel space.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.0480241496,"dev-research":0.3240160977,"prompt-eng":0.4115702225,"data-quality":0.0894064076,"ml-security":0.0954799429}}
{"text":"Furthermore, although latent-based generative methods have shown great success in recent years, this particular model type remains underexplored in this area.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.0270474868,"dev-research":0.1907915322,"prompt-eng":0.4636691908,"data-quality":0.1537333702,"ml-security":0.0568431496}}
{"text":"In this work, we propose to apply flow matching in the latent spaces of pretrained autoencoders, which offers improved computational efficiency and scalability for high-resolution image synthesis.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.1042257003,"dev-research":0.2650938439,"prompt-eng":0.4508590099,"data-quality":0.106911411,"ml-security":0.0750549368}}
{"text":"This enables flow-matching training on constrained computational resources while maintaining their quality and flexibility.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.0416785366,"dev-research":0.26900136,"prompt-eng":0.3806591858,"data-quality":0.0985843618,"ml-security":0.0804281783}}
{"text":"Additionally, our work stands as a pioneering contribution in the integration of various conditions into flow matching for conditional generation tasks, including label-conditioned image generation, image inpainting, and semantic-to-image generation.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.0874440848,"dev-research":0.2491292919,"prompt-eng":0.5008540886,"data-quality":0.1866893123,"ml-security":0.0376166842}}
{"text":"Through extensive experiments, our approach demonstrates its effectiveness in both quantitative and qualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.471322871,"dev-research":0.3054414116,"prompt-eng":0.4140647123,"data-quality":0.2117221614,"ml-security":0.0873142353}}
{"text":"We also provide a theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution, showing it is upper-bounded by the latent flow matching objective.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.0870082048,"dev-research":0.1777728712,"prompt-eng":0.377698527,"data-quality":0.129845126,"ml-security":0.0924657826}}
{"text":"Our code will be available at https://github.com/VinAIResearch/LFM.git.","meta":{"url":"http://arxiv.org/abs/2307.08698v1"},"cats":{"new-dataset":0.3321998665,"dev-research":0.2391983686,"prompt-eng":0.4656253523,"data-quality":0.1250787359,"ml-security":0.0391445135}}
{"text":"Video depth estimation aims to infer temporally consistent depth.","meta":{"url":"http://arxiv.org/abs/2307.08695v1"},"cats":{"new-dataset":0.0976865375,"dev-research":0.2589023662,"prompt-eng":0.3629339354,"data-quality":0.12407284,"ml-security":0.0580169088}}
{"text":"Some methods achieve temporal consistency by finetuning a single-image depth model during test time using geometry and re-projection constraints, which is inefficient and not robust.","meta":{"url":"http://arxiv.org/abs/2307.08695v1"},"cats":{"new-dataset":0.0697155881,"dev-research":0.2155608461,"prompt-eng":0.3982127125,"data-quality":0.1337777354,"ml-security":0.0647457493}}
{"text":"An alternative approach is to learn how to enforce temporal consistency from data, but this requires well-designed models and sufficient video depth data.","meta":{"url":"http://arxiv.org/abs/2307.08695v1"},"cats":{"new-dataset":0.1927144289,"dev-research":0.2518051237,"prompt-eng":0.3501881985,"data-quality":0.2109208836,"ml-security":0.0873106198}}
{"text":"To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabilizes inconsistent depth estimations and can be applied to different single-image depth models without extra effort.","meta":{"url":"http://arxiv.org/abs/2307.08695v1"},"cats":{"new-dataset":0.1055861975,"dev-research":0.2151995608,"prompt-eng":0.3359108207,"data-quality":0.1769370781,"ml-security":0.0882851905}}
{"text":"We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two million frames, making it the largest natural-scene video depth dataset to our knowledge.","meta":{"url":"http://arxiv.org/abs/2307.08695v1"},"cats":{"new-dataset":0.8670595505,"dev-research":0.2223995685,"prompt-eng":0.3267649004,"data-quality":0.1283618568,"ml-security":0.1078398846}}
{"text":"We evaluate our method on the VDW dataset as well as two public benchmarks and demonstrate significant improvements in consistency, accuracy, and efficiency compared to previous approaches.","meta":{"url":"http://arxiv.org/abs/2307.08695v1"},"cats":{"new-dataset":0.3137456694,"dev-research":0.307769663,"prompt-eng":0.3680659667,"data-quality":0.2047103788,"ml-security":0.0731662703}}
{"text":"Our work serves as a solid baseline and provides a data foundation for learning-based video depth models.","meta":{"url":"http://arxiv.org/abs/2307.08695v1"},"cats":{"new-dataset":0.4152247262,"dev-research":0.2591719515,"prompt-eng":0.3703604007,"data-quality":0.1033778003,"ml-security":0.0727899287}}
{"text":"We will release our dataset and code for future research.","meta":{"url":"http://arxiv.org/abs/2307.08695v1"},"cats":{"new-dataset":0.9246494681,"dev-research":0.4237942456,"prompt-eng":0.3968947971,"data-quality":0.1649376305,"ml-security":0.2219938299}}
{"text":"With continuous progression of Moore's Law, integrated circuit (IC) device complexity is also increasing.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.009036415,"dev-research":0.3103540597,"prompt-eng":0.3484424608,"data-quality":0.0588687594,"ml-security":0.0731303531}}
{"text":"Scanning Electron Microscope (SEM) image based extensive defect inspection and accurate metrology extraction are two main challenges in advanced node (2 nm and beyond) technology.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.0330521946,"dev-research":0.2454166934,"prompt-eng":0.4138667309,"data-quality":0.2183446708,"ml-security":0.0449138643}}
{"text":"Deep learning (DL) algorithm based computer vision approaches gained popularity in semiconductor defect inspection over last few years.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.1094629512,"dev-research":0.3446382372,"prompt-eng":0.3754859825,"data-quality":0.2705924965,"ml-security":0.2127681232}}
{"text":"In this research work, a new semiconductor defect inspection framework \"SEMI-DiffusionInst\" is investigated and compared to previous frameworks.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.0282118052,"dev-research":0.303310444,"prompt-eng":0.4093921668,"data-quality":0.2017377986,"ml-security":0.101440936}}
{"text":"To the best of the authors' knowledge, this work is the first demonstration to accurately detect and precisely segment semiconductor defect patterns by using a diffusion model.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.0453664508,"dev-research":0.3147783426,"prompt-eng":0.4321183866,"data-quality":0.3019232923,"ml-security":0.108953458}}
{"text":"Different feature extractor networks as backbones and data sampling strategies are investigated towards achieving a balanced trade-off between precision and computing efficiency.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.0375670819,"dev-research":0.3299218596,"prompt-eng":0.363167348,"data-quality":0.1702365204,"ml-security":0.1668744622}}
{"text":"Our proposed approach outperforms previous work on overall mAP and performs comparatively better or as per for almost all defect classes (per class APs).","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.1454012225,"dev-research":0.3955515054,"prompt-eng":0.4097061955,"data-quality":0.2442795786,"ml-security":0.1060080315}}
{"text":"The bounding box and segmentation mAPs achieved by the proposed SEMI-DiffusionInst model are improved by 3.83% and 2.10%,respectively.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.0595543982,"dev-research":0.2024663182,"prompt-eng":0.3857483964,"data-quality":0.0849660942,"ml-security":0.0707662383}}
{"text":"Among individual defect types, precision on line collapse and thin bridge defects are improved approximately 15% on detection task for both defect types.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.0348507928,"dev-research":0.385890483,"prompt-eng":0.4474363194,"data-quality":0.3445731052,"ml-security":0.0805122862}}
{"text":"It has also been shown that by tuning inference hyperparameters, inference time can be improved significantly without compromising model precision.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.0111416816,"dev-research":0.2327327651,"prompt-eng":0.4641516879,"data-quality":0.120367828,"ml-security":0.1225387194}}
{"text":"Finally, certain limitations and future work strategy to overcome them are discussed.","meta":{"url":"http://arxiv.org/abs/2307.08693v1"},"cats":{"new-dataset":0.0182407883,"dev-research":0.3104707684,"prompt-eng":0.3612352519,"data-quality":0.0415291016,"ml-security":0.1169343141}}
{"text":"Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation.","meta":{"url":"http://arxiv.org/abs/2307.08691v1"},"cats":{"new-dataset":0.1176207272,"dev-research":0.2657403525,"prompt-eng":0.4087642115,"data-quality":0.1235818697,"ml-security":0.0600884171}}
{"text":"The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length.","meta":{"url":"http://arxiv.org/abs/2307.08691v1"},"cats":{"new-dataset":0.0674548606,"dev-research":0.262757158,"prompt-eng":0.3712626907,"data-quality":0.0624469693,"ml-security":0.0872923762}}
{"text":"FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation.","meta":{"url":"http://arxiv.org/abs/2307.08691v1"},"cats":{"new-dataset":0.0324715255,"dev-research":0.3186401546,"prompt-eng":0.3842348085,"data-quality":0.0673642357,"ml-security":0.0993460816}}
{"text":"However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes.","meta":{"url":"http://arxiv.org/abs/2307.08691v1"},"cats":{"new-dataset":0.0272381742,"dev-research":0.2662024808,"prompt-eng":0.3550765735,"data-quality":0.0403320825,"ml-security":0.0967934328}}
{"text":"We propose FlashAttention-2, with better work partitioning to address these issues.","meta":{"url":"http://arxiv.org/abs/2307.08691v1"},"cats":{"new-dataset":0.154622751,"dev-research":0.292961705,"prompt-eng":0.4148796931,"data-quality":0.1457327812,"ml-security":0.0861528028}}
{"text":"In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory.","meta":{"url":"http://arxiv.org/abs/2307.08691v1"},"cats":{"new-dataset":0.0436294117,"dev-research":0.2302948367,"prompt-eng":0.3893762668,"data-quality":0.0660042655,"ml-security":0.0981712599}}
{"text":"These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations.","meta":{"url":"http://arxiv.org/abs/2307.08691v1"},"cats":{"new-dataset":0.053597115,"dev-research":0.2510496138,"prompt-eng":0.4111731717,"data-quality":0.0559653878,"ml-security":0.0743938535}}
{"text":"We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).","meta":{"url":"http://arxiv.org/abs/2307.08691v1"},"cats":{"new-dataset":0.1222700088,"dev-research":0.2451585234,"prompt-eng":0.4246239211,"data-quality":0.0751382932,"ml-security":0.1417053263}}
{"text":"Robotic Exploration has evolved rapidly in the past two decades as new and more complex techniques have been created to explore unknown regions efficiently.","meta":{"url":"http://arxiv.org/abs/2307.08690v1"},"cats":{"new-dataset":0.0359316554,"dev-research":0.2265107834,"prompt-eng":0.4072434884,"data-quality":0.0482469793,"ml-security":0.0578340983}}
{"text":"Exciting advancements in exploration, autonomous navigation, and sensor technology have created opportunities for robots to be utilized in new environments and for new objectives ranging from mapping of abandon mines and deep oceans to the efficient creation of indoor models for navigation and search.","meta":{"url":"http://arxiv.org/abs/2307.08690v1"},"cats":{"new-dataset":0.0881504623,"dev-research":0.2573124807,"prompt-eng":0.4120220382,"data-quality":0.0807371828,"ml-security":0.0702219619}}
{"text":"In this paper we present and discuss a number of examples in research literature of these recent advancements, specifically focusing on robotic exploration algorithms for unmanned vehicles.","meta":{"url":"http://arxiv.org/abs/2307.08690v1"},"cats":{"new-dataset":0.0492352145,"dev-research":0.1959104212,"prompt-eng":0.3984664657,"data-quality":0.0672890755,"ml-security":0.0736160399}}
{"text":"Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models.","meta":{"url":"http://arxiv.org/abs/2307.08689v1"},"cats":{"new-dataset":0.1127611206,"dev-research":0.2660877494,"prompt-eng":0.4819461192,"data-quality":0.1994222791,"ml-security":0.0721315043}}
{"text":"However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g.,generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4.","meta":{"url":"http://arxiv.org/abs/2307.08689v1"},"cats":{"new-dataset":0.069053977,"dev-research":0.2405546594,"prompt-eng":0.4349541129,"data-quality":0.1402724969,"ml-security":0.0567721671}}
{"text":"We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning).","meta":{"url":"http://arxiv.org/abs/2307.08689v1"},"cats":{"new-dataset":0.2732710427,"dev-research":0.317717126,"prompt-eng":0.4751964394,"data-quality":0.1493155053,"ml-security":0.0501080663}}
{"text":"We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus.","meta":{"url":"http://arxiv.org/abs/2307.08689v1"},"cats":{"new-dataset":0.250149844,"dev-research":0.3419140296,"prompt-eng":0.4594015714,"data-quality":0.1876659733,"ml-security":0.0632622457}}
{"text":"Using COLLIE, we compile the COLLIE-v1 dataset with 2080 instances comprising 13 constraint structures.","meta":{"url":"http://arxiv.org/abs/2307.08689v1"},"cats":{"new-dataset":0.8405598499,"dev-research":0.2649799262,"prompt-eng":0.3774571501,"data-quality":0.0906639502,"ml-security":0.0997910204}}
{"text":"We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings.","meta":{"url":"http://arxiv.org/abs/2307.08689v1"},"cats":{"new-dataset":0.0704672846,"dev-research":0.2982891458,"prompt-eng":0.4859077562,"data-quality":0.2141518968,"ml-security":0.0799590199}}
{"text":"COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.","meta":{"url":"http://arxiv.org/abs/2307.08689v1"},"cats":{"new-dataset":0.0864284101,"dev-research":0.3145884222,"prompt-eng":0.4344799521,"data-quality":0.0675402059,"ml-security":0.0745243812}}
{"text":"Perception and control systems for autonomous vehicles are an active area of scientific and industrial research.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.0677603368,"dev-research":0.2532248944,"prompt-eng":0.431442553,"data-quality":0.1100393734,"ml-security":0.0978979217}}
{"text":"These solutions should be characterised by high efficiency in recognising obstacles and other environmental elements in different road conditions, real-time capability, and energy efficiency.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.113407319,"dev-research":0.300199445,"prompt-eng":0.3955401778,"data-quality":0.0456195065,"ml-security":0.0556504953}}
{"text":"Achieving such functionality requires an appropriate algorithm and a suitable computing platform.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.0640864538,"dev-research":0.3025065116,"prompt-eng":0.4391720533,"data-quality":0.0666872867,"ml-security":0.0389829975}}
{"text":"In this paper, we have used the MultiTaskV3 detection-segmentation network as the basis for a perception system that can perform both functionalities within a single architecture.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.0714190541,"dev-research":0.2554694166,"prompt-eng":0.3750844407,"data-quality":0.1495202076,"ml-security":0.1344615769}}
{"text":"It was appropriately trained, quantised, and implemented on the AMD Xilinx Kria KV260 Vision AI embedded platform.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.1144042951,"dev-research":0.2919846793,"prompt-eng":0.3936104378,"data-quality":0.1204555118,"ml-security":0.0804560294}}
{"text":"By using this device, it was possible to parallelise and accelerate the computations.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.0153876435,"dev-research":0.2914461871,"prompt-eng":0.3788457449,"data-quality":0.0417343994,"ml-security":0.0498856616}}
{"text":"Furthermore, the whole system consumes relatively little power compared to a CPU-based implementation (an average of 5 watts, compared to the minimum of 55 watts for weaker CPUs, and the small size (119mm x 140mm x 36mm) of the platform allows it to be used in devices where the amount of space available is limited.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.0188477541,"dev-research":0.3662313272,"prompt-eng":0.3561415591,"data-quality":0.0496897758,"ml-security":0.0892012539}}
{"text":"It also achieves an accuracy higher than 97% of the mAP (mean average precision) for object detection and above 90% of the mIoU (mean intersection over union) for image segmentation.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.0273097286,"dev-research":0.2555322348,"prompt-eng":0.3879161521,"data-quality":0.1645339208,"ml-security":0.0592580105}}
{"text":"The article also details the design of the Mecanum wheel vehicle, which was used to test the proposed solution in a mock-up city.","meta":{"url":"http://arxiv.org/abs/2307.08682v1"},"cats":{"new-dataset":0.1596866437,"dev-research":0.2814087164,"prompt-eng":0.3911304255,"data-quality":0.0834940707,"ml-security":0.0415930801}}
{"text":"To ensure that secure applications do not leak their secrets, they are required to uphold several security properties such as spatial and temporal memory safety as well as cryptographic constant time.","meta":{"url":"http://arxiv.org/abs/2307.08681v1"},"cats":{"new-dataset":0.0278435436,"dev-research":0.3415691733,"prompt-eng":0.3656873439,"data-quality":0.0846472548,"ml-security":0.5393884154}}
{"text":"Existing work shows how to enforce these properties individually, in an architecture-independent way, by using secure compiler passes that each focus on an individual property.","meta":{"url":"http://arxiv.org/abs/2307.08681v1"},"cats":{"new-dataset":0.0258628483,"dev-research":0.3367252717,"prompt-eng":0.4387375232,"data-quality":0.17613441,"ml-security":0.3847724127}}
{"text":"Unfortunately, given two secure compiler passes that each preserve a possibly different security property, it is unclear what kind of security property is preserved by the composition of those secure compiler passes.","meta":{"url":"http://arxiv.org/abs/2307.08681v1"},"cats":{"new-dataset":0.0203214733,"dev-research":0.3233916298,"prompt-eng":0.4193993981,"data-quality":0.1863488044,"ml-security":0.3539644932}}
{"text":"This paper is the first to study what security properties are preserved across the composition of different secure compiler passes.","meta":{"url":"http://arxiv.org/abs/2307.08681v1"},"cats":{"new-dataset":0.0486531588,"dev-research":0.3833022945,"prompt-eng":0.4331483209,"data-quality":0.1594512733,"ml-security":0.4302267455}}
{"text":"Starting from a general theory of property composition for security-relevant properties (such as the aforementioned ones), this paper formalises a theory of composition of secure compilers.","meta":{"url":"http://arxiv.org/abs/2307.08681v1"},"cats":{"new-dataset":0.0501156787,"dev-research":0.3745419671,"prompt-eng":0.4654927119,"data-quality":0.1524731346,"ml-security":0.3512406487}}
{"text":"Then, it showcases this theory a secure multi-pass compiler that preserves the aforementioned security-relevant properties.","meta":{"url":"http://arxiv.org/abs/2307.08681v1"},"cats":{"new-dataset":0.017276194,"dev-research":0.3355095393,"prompt-eng":0.4202672248,"data-quality":0.113029123,"ml-security":0.3565967385}}
{"text":"Crucially, this paper derives the security of the multi-pass compiler from the composition of the security properties preserved by its individual passes, which include security-preserving as well as optimisation passes.","meta":{"url":"http://arxiv.org/abs/2307.08681v1"},"cats":{"new-dataset":0.0411861983,"dev-research":0.3560491232,"prompt-eng":0.4169978308,"data-quality":0.1048481296,"ml-security":0.4159277178}}
{"text":"From an engineering perspective, this is the desirable approach to building secure compilers.","meta":{"url":"http://arxiv.org/abs/2307.08681v1"},"cats":{"new-dataset":0.0222051954,"dev-research":0.4911838666,"prompt-eng":0.4445651835,"data-quality":0.1037148532,"ml-security":0.3814867174}}
{"text":"Storage codes on graphs are an instance of \\emph{codes with locality}, which are used in distributed storage schemes to provide local repairability.","meta":{"url":"http://arxiv.org/abs/2307.08680v1"},"cats":{"new-dataset":0.2134452913,"dev-research":0.3443030362,"prompt-eng":0.3697239083,"data-quality":0.2835027059,"ml-security":0.1003790163}}
{"text":"Specifically, the nodes of the graph correspond to storage servers, and the neighbourhood of each server constitute the set of servers it can query to repair its stored data in the event of a failure.","meta":{"url":"http://arxiv.org/abs/2307.08680v1"},"cats":{"new-dataset":0.1776724333,"dev-research":0.3285699696,"prompt-eng":0.3720877188,"data-quality":0.2485160708,"ml-security":0.1166701831}}
{"text":"A storage code on a graph with $n$-vertices is a set of $n$-length codewords over $\\field_q$ where the $i$th codeword symbol is stored in server $i$, and it can be recovered by querying the neighbours of server $i$ according to the underlying graph.   ","meta":{"url":"http://arxiv.org/abs/2307.08680v1"},"cats":{"new-dataset":0.2442240966,"dev-research":0.356862933,"prompt-eng":0.3374904602,"data-quality":0.2125013452,"ml-security":0.1231613586}}
{"text":"In this work, we look at binary storage codes whose repair function is the parity check, and characterise the tradeoff between the locality of the code and its rate.","meta":{"url":"http://arxiv.org/abs/2307.08680v1"},"cats":{"new-dataset":0.1188832157,"dev-research":0.3315841974,"prompt-eng":0.3987043322,"data-quality":0.3410306432,"ml-security":0.1283401369}}
{"text":"Specifically, we show that the maximum rate of a code on $n$ vertices with locality $r$ is bounded between $1-1/n\\lceil n/(r+1)\\rceil$ and $1-1/n\\lceil n/(r+1)\\rceil$. The lower bound on the rate is derived by constructing an explicit family of graphs with locality $r$, while the upper bound is obtained via a lower bound on the binary-field rank of a class of symmetric binary matrices.","meta":{"url":"http://arxiv.org/abs/2307.08680v1"},"cats":{"new-dataset":0.1028919244,"dev-research":0.2661194149,"prompt-eng":0.3415619248,"data-quality":0.1443668559,"ml-security":0.1132461737}}
{"text":"Our upper bound on maximal rate of a storage code matches the upper bound on the larger class of codes with locality derived by Tamo and Barg.","meta":{"url":"http://arxiv.org/abs/2307.08680v1"},"cats":{"new-dataset":0.2146267693,"dev-research":0.282791365,"prompt-eng":0.3607678064,"data-quality":0.1571750483,"ml-security":0.1173792824}}
{"text":"As a corollary to our result, we obtain the following asymptotic separation result: given a sequence $r(n), n\\geq 1$, there exists a sequence of graphs on $n$-vertices with storage codes of rate $1-o(1)$ if and only if $r(n)=\\omega(1)$.","meta":{"url":"http://arxiv.org/abs/2307.08680v1"},"cats":{"new-dataset":0.1637455247,"dev-research":0.204313275,"prompt-eng":0.2994808473,"data-quality":0.1666258941,"ml-security":0.1317691972}}
{"text":"Large language models (LLMs) are trained to imitate humans to explain human decisions.","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0559021777,"dev-research":0.2547123087,"prompt-eng":0.5323981345,"data-quality":0.1658195762,"ml-security":0.1915886202}}
{"text":"However, do LLMs explain themselves?","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0214735076,"dev-research":0.2808466861,"prompt-eng":0.500597037,"data-quality":0.1792865118,"ml-security":0.1649932493}}
{"text":"Can they help humans build mental models of how LLMs process different inputs?","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0097788167,"dev-research":0.2585565303,"prompt-eng":0.5564898969,"data-quality":0.0943642258,"ml-security":0.1391250526}}
{"text":"To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input.","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0503920221,"dev-research":0.3840910415,"prompt-eng":0.4767329164,"data-quality":0.3893363937,"ml-security":0.3296113204}}
{"text":"For example, if a model answers \"yes\" to the input question \"Can eagles fly?\"","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0090852181,"dev-research":0.2897890505,"prompt-eng":0.5228710669,"data-quality":0.2187636901,"ml-security":0.1509791184}}
{"text":"with the explanation \"all birds can fly\", then humans would infer from the explanation that it would also answer \"yes\" to the counterfactual input \"Can penguins fly?\".","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0371941763,"dev-research":0.2890213975,"prompt-eng":0.4248130262,"data-quality":0.1690959682,"ml-security":0.1386603092}}
{"text":"If the explanation is precise, then the model's answer should match humans' expectations.   ","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0142687241,"dev-research":0.2664474624,"prompt-eng":0.4368564726,"data-quality":0.0994611007,"ml-security":0.1236463146}}
{"text":"We implemented two metrics based on counterfactual simulatability: precision and generality.","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.1060234968,"dev-research":0.2741691121,"prompt-eng":0.4094518515,"data-quality":0.3578658196,"ml-security":0.1208544772}}
{"text":"We generated diverse counterfactuals automatically using LLMs.","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.325710941,"dev-research":0.32250037,"prompt-eng":0.513465088,"data-quality":0.3708110901,"ml-security":0.1889440846}}
{"text":"We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling.","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0500009562,"dev-research":0.2119054437,"prompt-eng":0.4938634148,"data-quality":0.0798215399,"ml-security":0.0474288661}}
{"text":"We found that LLM's explanations have low precision and that precision does not correlate with plausibility.","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0118705669,"dev-research":0.2673342219,"prompt-eng":0.4378807966,"data-quality":0.2530729686,"ml-security":0.127157858}}
{"text":"Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.","meta":{"url":"http://arxiv.org/abs/2307.08678v1"},"cats":{"new-dataset":0.0118443372,"dev-research":0.2618864518,"prompt-eng":0.4356509491,"data-quality":0.2092520775,"ml-security":0.1224390099}}
{"text":"Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.1605086922,"dev-research":0.4206849019,"prompt-eng":0.3809068399,"data-quality":0.0790990085,"ml-security":0.1086316037}}
{"text":"The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.0794110982,"dev-research":0.2481742688,"prompt-eng":0.5077026588,"data-quality":0.1273021705,"ml-security":0.0864983055}}
{"text":"In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.1279372242,"dev-research":0.2846067885,"prompt-eng":0.4883872297,"data-quality":0.0754274168,"ml-security":0.0512644693}}
{"text":"It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.0641480189,"dev-research":0.4369545012,"prompt-eng":0.4269047434,"data-quality":0.0597500992,"ml-security":0.0766954332}}
{"text":"TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.1746053683,"dev-research":0.3438328124,"prompt-eng":0.3976398299,"data-quality":0.0705957144,"ml-security":0.0629323471}}
{"text":"At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.1413295288,"dev-research":0.276880867,"prompt-eng":0.4359445094,"data-quality":0.0963374066,"ml-security":0.0517239105}}
{"text":"By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.1134765976,"dev-research":0.2498071198,"prompt-eng":0.4669084511,"data-quality":0.1037009725,"ml-security":0.0631904434}}
{"text":"Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.0619923472,"dev-research":0.3252884334,"prompt-eng":0.3616409003,"data-quality":0.0553271201,"ml-security":0.0713680947}}
{"text":"Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.","meta":{"url":"http://arxiv.org/abs/2307.08674v1"},"cats":{"new-dataset":0.0859507626,"dev-research":0.3425033558,"prompt-eng":0.3664519144,"data-quality":0.110213153,"ml-security":0.2619866298}}
{"text":"Batch effects (BEs) refer to systematic technical differences in data collection unrelated to biological variations whose noise is shown to negatively impact machine learning (ML) model generalizability.","meta":{"url":"http://arxiv.org/abs/2307.08673v1"},"cats":{"new-dataset":0.0490325752,"dev-research":0.261085655,"prompt-eng":0.4145950643,"data-quality":0.228674119,"ml-security":0.3323989845}}
{"text":"Here we release CohortFinder, an open-source tool aimed at mitigating BEs via data-driven cohort partitioning.","meta":{"url":"http://arxiv.org/abs/2307.08673v1"},"cats":{"new-dataset":0.1970866915,"dev-research":0.2762656811,"prompt-eng":0.3828073486,"data-quality":0.1265207198,"ml-security":0.1940992685}}
{"text":"We demonstrate CohortFinder improves ML model performance in downstream medical image processing tasks.","meta":{"url":"http://arxiv.org/abs/2307.08673v1"},"cats":{"new-dataset":0.0714697569,"dev-research":0.2527788979,"prompt-eng":0.4161650005,"data-quality":0.1290358089,"ml-security":0.1038160715}}
{"text":"CohortFinder is freely available for download at cohortfinder.com.","meta":{"url":"http://arxiv.org/abs/2307.08673v1"},"cats":{"new-dataset":0.2896334396,"dev-research":0.2162593998,"prompt-eng":0.3587435353,"data-quality":0.0826117421,"ml-security":0.086496175}}
{"text":"We consider a gossip network consisting of a source generating updates and $n$ nodes connected in a two-dimensional square grid.","meta":{"url":"http://arxiv.org/abs/2307.08670v1"},"cats":{"new-dataset":0.1657807232,"dev-research":0.2413735292,"prompt-eng":0.3531333613,"data-quality":0.1408237516,"ml-security":0.2225254839}}
{"text":"The source keeps updates of a process, that might be generated or observed, and shares them with the grid network.","meta":{"url":"http://arxiv.org/abs/2307.08670v1"},"cats":{"new-dataset":0.1090489596,"dev-research":0.3639145667,"prompt-eng":0.3904974783,"data-quality":0.108696916,"ml-security":0.1430121026}}
{"text":"The nodes in the grid network communicate with their neighbors and disseminate these version updates using a push-style gossip strategy.","meta":{"url":"http://arxiv.org/abs/2307.08670v1"},"cats":{"new-dataset":0.1507300552,"dev-research":0.3668125678,"prompt-eng":0.413017222,"data-quality":0.1348638779,"ml-security":0.1971605387}}
{"text":"We use the version age metric to quantify the timeliness of information at the nodes.","meta":{"url":"http://arxiv.org/abs/2307.08670v1"},"cats":{"new-dataset":0.3395289641,"dev-research":0.3794125827,"prompt-eng":0.3877439339,"data-quality":0.14741321,"ml-security":0.0807687443}}
{"text":"We find an upper bound for the average version age for a set of nodes in a general network.","meta":{"url":"http://arxiv.org/abs/2307.08670v1"},"cats":{"new-dataset":0.1450195784,"dev-research":0.2484328446,"prompt-eng":0.3365974468,"data-quality":0.125740541,"ml-security":0.1353937056}}
{"text":"Using this, we show that the average version age at a node scales as $O(n^{\\frac{1}{3}})$ in a grid network.","meta":{"url":"http://arxiv.org/abs/2307.08670v1"},"cats":{"new-dataset":0.1178310017,"dev-research":0.2523805595,"prompt-eng":0.3214292381,"data-quality":0.1017227683,"ml-security":0.1058780104}}
{"text":"Prior to our work, it has been known that when $n$ nodes are connected on a ring the version age scales as $O(n^{\\frac{1}{2}})$, and when they are connected on a fully-connected graph the version age scales as $O(\\log n)$. Ours is the first work to show an age scaling result for a connectivity structure other than the ring and fully-connected networks that represent two extremes of network connectivity.","meta":{"url":"http://arxiv.org/abs/2307.08670v1"},"cats":{"new-dataset":0.1037915531,"dev-research":0.2841186796,"prompt-eng":0.3221131756,"data-quality":0.1083535549,"ml-security":0.0691838605}}
{"text":"Our work shows that higher connectivity on a grid compared to a ring lowers the age experience of each node from $O(n^{\\frac{1}{2}})$ to $O(n^{\\frac{1}{3}})$.","meta":{"url":"http://arxiv.org/abs/2307.08670v1"},"cats":{"new-dataset":0.0609707205,"dev-research":0.2449445106,"prompt-eng":0.3065299337,"data-quality":0.0764154322,"ml-security":0.0521832696}}
{"text":"Peer production platforms like Wikipedia commonly suffer from content gaps.","meta":{"url":"http://arxiv.org/abs/2307.08669v1"},"cats":{"new-dataset":0.1009119298,"dev-research":0.3499854051,"prompt-eng":0.3655408429,"data-quality":0.2043063544,"ml-security":0.0783337304}}
{"text":"Prior research suggests recommender systems can help solve this problem, by guiding editors towards underrepresented topics.","meta":{"url":"http://arxiv.org/abs/2307.08669v1"},"cats":{"new-dataset":0.0366587567,"dev-research":0.348986239,"prompt-eng":0.4359521314,"data-quality":0.2388518814,"ml-security":0.0619988713}}
{"text":"However, it remains unclear whether this approach would result in less relevant recommendations, leading to reduced overall engagement with recommended items.","meta":{"url":"http://arxiv.org/abs/2307.08669v1"},"cats":{"new-dataset":0.0048805792,"dev-research":0.335480822,"prompt-eng":0.3901659808,"data-quality":0.1287467948,"ml-security":0.0741135428}}
{"text":"To answer this question, we first conducted offline analyses (Study 1) on SuggestBot, a task-routing recommender system for Wikipedia, then did a three-month controlled experiment (Study 2).","meta":{"url":"http://arxiv.org/abs/2307.08669v1"},"cats":{"new-dataset":0.0352494953,"dev-research":0.3396114494,"prompt-eng":0.4582863976,"data-quality":0.0840913447,"ml-security":0.0496419466}}
{"text":"Our results show that presenting users with articles from underrepresented topics increased the proportion of work done on those articles without significantly reducing overall recommendation uptake.","meta":{"url":"http://arxiv.org/abs/2307.08669v1"},"cats":{"new-dataset":0.0078667995,"dev-research":0.3480798185,"prompt-eng":0.4242322274,"data-quality":0.1302156258,"ml-security":0.0648123573}}
{"text":"We discuss the implications of our results, including how ignoring the article discovery process can artificially narrow recommendations.","meta":{"url":"http://arxiv.org/abs/2307.08669v1"},"cats":{"new-dataset":0.0291461182,"dev-research":0.3261554754,"prompt-eng":0.3872963264,"data-quality":0.1742806286,"ml-security":0.1135526604}}
{"text":"We draw parallels between this phenomenon and the common issue of ``filter bubbles'' to show how any platform that employs recommender systems is susceptible to it.","meta":{"url":"http://arxiv.org/abs/2307.08669v1"},"cats":{"new-dataset":0.0289724286,"dev-research":0.3827861733,"prompt-eng":0.4309504317,"data-quality":0.2038039256,"ml-security":0.3342848402}}
{"text":"In recent years there has been a large focus on how robots can operate in human populated environments.","meta":{"url":"http://arxiv.org/abs/2307.08668v1"},"cats":{"new-dataset":0.099437811,"dev-research":0.2955052846,"prompt-eng":0.4346178651,"data-quality":0.0418678565,"ml-security":0.0890867526}}
{"text":"In this paper, we focus on interactions between humans and small indoor robots and introduce a new human-robot interaction (HRI) dataset.","meta":{"url":"http://arxiv.org/abs/2307.08668v1"},"cats":{"new-dataset":0.6605869872,"dev-research":0.2869693083,"prompt-eng":0.3808351524,"data-quality":0.0867978434,"ml-security":0.0778143582}}
{"text":"The analysis of the recorded experiments shows that anticipatory and non-reactive robot controllers impose similar constraints to humans' safety and efficiency.","meta":{"url":"http://arxiv.org/abs/2307.08668v1"},"cats":{"new-dataset":0.0543580586,"dev-research":0.2992715731,"prompt-eng":0.3984896303,"data-quality":0.0713481115,"ml-security":0.3282094933}}
{"text":"Additionally, we found that current state-of-the-art models for human trajectory prediction can adequately extend to indoor HRI settings.","meta":{"url":"http://arxiv.org/abs/2307.08668v1"},"cats":{"new-dataset":0.1887763812,"dev-research":0.2376846395,"prompt-eng":0.4092793069,"data-quality":0.0682269626,"ml-security":0.0840884076}}
{"text":"Finally, we show that humans respond differently in shared and homogeneous environments when collisions are imminent, since interacting with small differential drives can only cause a finite level of social discomfort as compared to human-human interactions.","meta":{"url":"http://arxiv.org/abs/2307.08668v1"},"cats":{"new-dataset":0.1254934774,"dev-research":0.3167322228,"prompt-eng":0.4304923127,"data-quality":0.0732346835,"ml-security":0.2564028633}}
{"text":"The dataset used in this analysis is available at: https://github.com/AlexanderDavid/ZuckerDataset.","meta":{"url":"http://arxiv.org/abs/2307.08668v1"},"cats":{"new-dataset":0.8327776713,"dev-research":0.2016825479,"prompt-eng":0.3211060637,"data-quality":0.0997570827,"ml-security":0.0838693378}}
{"text":"Since their first applications, Convolutional Neural Networks (CNNs) have solved problems that have advanced the state-of-the-art in several domains.","meta":{"url":"http://arxiv.org/abs/2307.08663v1"},"cats":{"new-dataset":0.0705560464,"dev-research":0.2818424223,"prompt-eng":0.3652340131,"data-quality":0.1879119433,"ml-security":0.2080490969}}
{"text":"CNNs represent information using real numbers.","meta":{"url":"http://arxiv.org/abs/2307.08663v1"},"cats":{"new-dataset":0.1334891973,"dev-research":0.3156723006,"prompt-eng":0.3665910246,"data-quality":0.2108512717,"ml-security":0.2034929124}}
{"text":"Despite encouraging results, theoretical analysis shows that representations such as hyper-complex numbers can achieve richer representational capacities than real numbers, and that Hamilton products can capture intrinsic interchannel relationships.","meta":{"url":"http://arxiv.org/abs/2307.08663v1"},"cats":{"new-dataset":0.0085895888,"dev-research":0.277211097,"prompt-eng":0.3421886291,"data-quality":0.0535301723,"ml-security":0.1458420624}}
{"text":"Moreover, in the last few years, experimental research has shown that Quaternion-Valued CNNs (QCNNs) can achieve similar performance with fewer parameters than their real-valued counterparts.","meta":{"url":"http://arxiv.org/abs/2307.08663v1"},"cats":{"new-dataset":0.0478953577,"dev-research":0.2872690302,"prompt-eng":0.335813616,"data-quality":0.0756504566,"ml-security":0.1351329542}}
{"text":"This paper condenses research in the development of QCNNs from its very beginnings.","meta":{"url":"http://arxiv.org/abs/2307.08663v1"},"cats":{"new-dataset":0.1280318017,"dev-research":0.2957040833,"prompt-eng":0.3649652896,"data-quality":0.1012809069,"ml-security":0.1212903286}}
{"text":"We propose a conceptual organization of current trends and analyze the main building blocks used in the design of QCNN models.","meta":{"url":"http://arxiv.org/abs/2307.08663v1"},"cats":{"new-dataset":0.110446485,"dev-research":0.2596738502,"prompt-eng":0.3724295678,"data-quality":0.0776267297,"ml-security":0.1149953105}}
{"text":"Based on this conceptual organization, we propose future directions of research.","meta":{"url":"http://arxiv.org/abs/2307.08663v1"},"cats":{"new-dataset":0.0686229807,"dev-research":0.340013263,"prompt-eng":0.423171253,"data-quality":0.052185204,"ml-security":0.0771712379}}
{"text":"Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages.","meta":{"url":"http://arxiv.org/abs/2307.08655v1"},"cats":{"new-dataset":0.1376611602,"dev-research":0.2718001917,"prompt-eng":0.4145194808,"data-quality":0.1651844733,"ml-security":0.0906678633}}
{"text":"Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language.","meta":{"url":"http://arxiv.org/abs/2307.08655v1"},"cats":{"new-dataset":0.0861170008,"dev-research":0.3063768023,"prompt-eng":0.4288907548,"data-quality":0.1772594157,"ml-security":0.0719596411}}
{"text":"We present the first work on multilingual S2ST supporting multiple target languages.","meta":{"url":"http://arxiv.org/abs/2307.08655v1"},"cats":{"new-dataset":0.231583474,"dev-research":0.2615837433,"prompt-eng":0.4670087507,"data-quality":0.2062272207,"ml-security":0.0743235991}}
{"text":"Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability.","meta":{"url":"http://arxiv.org/abs/2307.08655v1"},"cats":{"new-dataset":0.13035973,"dev-research":0.2801226054,"prompt-eng":0.4315173213,"data-quality":0.1903572288,"ml-security":0.0761291316}}
{"text":"Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference.","meta":{"url":"http://arxiv.org/abs/2307.08655v1"},"cats":{"new-dataset":0.0762036553,"dev-research":0.1930276251,"prompt-eng":0.4234761492,"data-quality":0.2182028777,"ml-security":0.1613237677}}
{"text":"We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification.","meta":{"url":"http://arxiv.org/abs/2307.08655v1"},"cats":{"new-dataset":0.1152685254,"dev-research":0.2387925466,"prompt-eng":0.3954440568,"data-quality":0.4147132259,"ml-security":0.1136040268}}
{"text":"On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.","meta":{"url":"http://arxiv.org/abs/2307.08655v1"},"cats":{"new-dataset":0.1558248552,"dev-research":0.254285701,"prompt-eng":0.4508623374,"data-quality":0.1590138156,"ml-security":0.0428782789}}
{"text":"We introduce the problem of knot-based inverse perceptual art.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.0666933741,"dev-research":0.2023688225,"prompt-eng":0.4169532258,"data-quality":0.1437034701,"ml-security":0.0607817298}}
{"text":"Given multiple target images and their corresponding viewing configurations, the objective is to find a 3D knot-based tubular structure whose appearance resembles the target images when viewed from the specified viewing configurations.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.0969145719,"dev-research":0.180427398,"prompt-eng":0.3885314908,"data-quality":0.0557680551,"ml-security":0.0304393903}}
{"text":"To solve this problem, we first design a differentiable rendering algorithm for rendering tubular knots embedded in 3D for arbitrary perspective camera configurations.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.0733957514,"dev-research":0.2115622776,"prompt-eng":0.3323708458,"data-quality":0.0741758501,"ml-security":0.0376154514}}
{"text":"Utilizing this differentiable rendering algorithm, we search over the space of knot configurations to find the ideal knot embedding.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.1017507256,"dev-research":0.2403808128,"prompt-eng":0.3501102754,"data-quality":0.0664145621,"ml-security":0.0483645897}}
{"text":"We represent the knot embeddings via homeomorphisms of the desired template knot, where the homeomorphisms are parametrized by the weights of an invertible neural network.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.0781282282,"dev-research":0.2281185214,"prompt-eng":0.3919177438,"data-quality":0.1105685505,"ml-security":0.1290809837}}
{"text":"Our approach is fully differentiable, making it possible to find the ideal 3D tubular structure for the desired perceptual art using gradient-based optimization.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.0236820585,"dev-research":0.1879408171,"prompt-eng":0.3740724734,"data-quality":0.0600710031,"ml-security":0.0460299124}}
{"text":"We propose several loss functions that impose additional physical constraints, ensuring that the tube is free of self-intersection, lies within a predefined region in space, satisfies the physical bending limits of the tube material and the material cost is within a specified budget.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.034056279,"dev-research":0.1798026132,"prompt-eng":0.3778487678,"data-quality":0.1267106794,"ml-security":0.1093452931}}
{"text":"We demonstrate through results that our knot representation is highly expressive and gives impressive results even for challenging target images in both single view as well as multiple view constraints.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.0910504999,"dev-research":0.2578862465,"prompt-eng":0.3831919298,"data-quality":0.1199220143,"ml-security":0.0627072741}}
{"text":"Through extensive ablation study we show that each of the proposed loss function is effective in ensuring physical realizability.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.0276083369,"dev-research":0.2115868846,"prompt-eng":0.3729165861,"data-quality":0.1113437353,"ml-security":0.0706946868}}
{"text":"To the best of our knowledge, we are the first to propose a fully differentiable optimization framework for knot-based inverse perceptual art.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.0456886525,"dev-research":0.1998194344,"prompt-eng":0.4044049027,"data-quality":0.0886310798,"ml-security":0.0558355697}}
{"text":"Both the code and data will be made publicly available.","meta":{"url":"http://arxiv.org/abs/2307.08652v2"},"cats":{"new-dataset":0.6895683306,"dev-research":0.3410388439,"prompt-eng":0.3818313857,"data-quality":0.0984738286,"ml-security":0.2047101868}}
{"text":"For underwater vehicles, robotic applications have the added difficulty of operating in highly unstructured and dynamic environments.","meta":{"url":"http://arxiv.org/abs/2307.08647v1"},"cats":{"new-dataset":0.024179417,"dev-research":0.247162111,"prompt-eng":0.3425669243,"data-quality":0.061099154,"ml-security":0.0957367972}}
{"text":"Environmental effects impact not only the dynamics and controls of the robot but also the perception and sensing modalities.","meta":{"url":"http://arxiv.org/abs/2307.08647v1"},"cats":{"new-dataset":0.0455840079,"dev-research":0.2558803884,"prompt-eng":0.4131432651,"data-quality":0.0655941494,"ml-security":0.1357082997}}
{"text":"Acoustic sensors, which inherently use mechanically vibrated signals for measuring range or velocity, are particularly prone to the effects that such dynamic environments induce.","meta":{"url":"http://arxiv.org/abs/2307.08647v1"},"cats":{"new-dataset":0.0354513458,"dev-research":0.2949394986,"prompt-eng":0.4048051687,"data-quality":0.1537773588,"ml-security":0.1701259802}}
{"text":"This paper presents an uncertainty-aware localization and mapping framework that accounts for induced disturbances in acoustic sensing modalities for underwater robots operating near the surface in dynamic wave conditions.","meta":{"url":"http://arxiv.org/abs/2307.08647v1"},"cats":{"new-dataset":0.088341993,"dev-research":0.2537792341,"prompt-eng":0.4003686334,"data-quality":0.1668589169,"ml-security":0.1345223103}}
{"text":"For the state estimation task, the uncertainty is accounted for as the added noise caused by the environmental disturbance.","meta":{"url":"http://arxiv.org/abs/2307.08647v1"},"cats":{"new-dataset":0.0822879713,"dev-research":0.2715326305,"prompt-eng":0.432121311,"data-quality":0.2629591279,"ml-security":0.0909469083}}
{"text":"The mapping method uses an adaptive kernel-based method to propagate measurement and pose uncertainty into an occupancy map.","meta":{"url":"http://arxiv.org/abs/2307.08647v1"},"cats":{"new-dataset":0.0892934687,"dev-research":0.2869674646,"prompt-eng":0.3837499255,"data-quality":0.1744216552,"ml-security":0.0504342223}}
{"text":"Experiments are carried out in a wave tank environment to perform qualitative and quantitative evaluations of the proposed method.","meta":{"url":"http://arxiv.org/abs/2307.08647v1"},"cats":{"new-dataset":0.0652525507,"dev-research":0.252533571,"prompt-eng":0.3751410878,"data-quality":0.1124440008,"ml-security":0.0760542008}}
{"text":"More details about this project can be found at https://umfieldrobotics.github.io/PUMA.github.io.","meta":{"url":"http://arxiv.org/abs/2307.08647v1"},"cats":{"new-dataset":0.1799586969,"dev-research":0.2504458734,"prompt-eng":0.4635542461,"data-quality":0.1063684115,"ml-security":0.0700234141}}
{"text":"Corruption is frequently observed in collected data and has been extensively studied in machine learning under different corruption models.","meta":{"url":"http://arxiv.org/abs/2307.08643v1"},"cats":{"new-dataset":0.1973992362,"dev-research":0.3469917979,"prompt-eng":0.3607964518,"data-quality":0.4950873777,"ml-security":0.5782035636}}
{"text":"Despite this, there remains a limited understanding of how these models relate such that a unified view of corruptions and their consequences on learning is still lacking.","meta":{"url":"http://arxiv.org/abs/2307.08643v1"},"cats":{"new-dataset":0.0733935963,"dev-research":0.3210332003,"prompt-eng":0.353201428,"data-quality":0.2241512869,"ml-security":0.4409830029}}
{"text":"In this work, we formally analyze corruption models at the distribution level through a general, exhaustive framework based on Markov kernels.","meta":{"url":"http://arxiv.org/abs/2307.08643v1"},"cats":{"new-dataset":0.1796539572,"dev-research":0.2884252568,"prompt-eng":0.410888431,"data-quality":0.4125180976,"ml-security":0.3530317288}}
{"text":"We highlight the existence of intricate joint and dependent corruptions on both labels and attributes, which are rarely touched by existing research.","meta":{"url":"http://arxiv.org/abs/2307.08643v1"},"cats":{"new-dataset":0.1170633061,"dev-research":0.378581547,"prompt-eng":0.3754033968,"data-quality":0.5894648297,"ml-security":0.3434778413}}
{"text":"Further, we show how these corruptions affect standard supervised learning by analyzing the resulting changes in Bayes Risk.","meta":{"url":"http://arxiv.org/abs/2307.08643v1"},"cats":{"new-dataset":0.0657712538,"dev-research":0.3455878646,"prompt-eng":0.4401563977,"data-quality":0.6645783039,"ml-security":0.5890512466}}
{"text":"Our findings offer qualitative insights into the consequences of \"more complex\" corruptions on the learning problem, and provide a foundation for future quantitative comparisons.","meta":{"url":"http://arxiv.org/abs/2307.08643v1"},"cats":{"new-dataset":0.083205817,"dev-research":0.3999369718,"prompt-eng":0.3613239162,"data-quality":0.3461188578,"ml-security":0.4098230418}}
{"text":"Applications of the framework include corruption-corrected learning, a subcase of which we study in this paper by theoretically analyzing loss correction with respect to different corruption instances.","meta":{"url":"http://arxiv.org/abs/2307.08643v1"},"cats":{"new-dataset":0.0763296017,"dev-research":0.3414497828,"prompt-eng":0.3852763751,"data-quality":0.5971157456,"ml-security":0.5047866703}}
{"text":"This work analyzes and parallelizes LearnedSort, the novel algorithm that sorts using machine learning models based on the cumulative distribution function.","meta":{"url":"http://arxiv.org/abs/2307.08637v1"},"cats":{"new-dataset":0.1743979207,"dev-research":0.2671018871,"prompt-eng":0.3841039666,"data-quality":0.0989739244,"ml-security":0.1682378933}}
{"text":"LearnedSort is analyzed under the lens of algorithms with predictions, and it is argued that LearnedSort is a learning-augmented SampleSort.","meta":{"url":"http://arxiv.org/abs/2307.08637v1"},"cats":{"new-dataset":0.0677373149,"dev-research":0.2756838876,"prompt-eng":0.3970298089,"data-quality":0.1494904624,"ml-security":0.178828449}}
{"text":"A parallel LearnedSort algorithm is developed combining LearnedSort with the state-of-the-art SampleSort implementation, IPS4o.","meta":{"url":"http://arxiv.org/abs/2307.08637v1"},"cats":{"new-dataset":0.1934216148,"dev-research":0.2526705957,"prompt-eng":0.3614443404,"data-quality":0.1357209499,"ml-security":0.0996368883}}
{"text":"Benchmarks on synthetic and real-world datasets demonstrate improved parallel performance for parallel LearnedSort compared to IPS4o and other sorting algorithms.","meta":{"url":"http://arxiv.org/abs/2307.08637v1"},"cats":{"new-dataset":0.3691157968,"dev-research":0.2482981544,"prompt-eng":0.3038436586,"data-quality":0.0914511302,"ml-security":0.1444879433}}
{"text":"We present PolyGNN, a polyhedron-based graph neural network for 3D building reconstruction from point clouds.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.1683801215,"dev-research":0.2733221155,"prompt-eng":0.3161652981,"data-quality":0.0886871837,"ml-security":0.1128702543}}
{"text":"PolyGNN learns to assemble primitives obtained by polyhedral decomposition via graph node classification, achieving a watertight, compact, and weakly semantic reconstruction.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.1247291036,"dev-research":0.300018261,"prompt-eng":0.354912048,"data-quality":0.1919761893,"ml-security":0.0971013332}}
{"text":"To effectively represent arbitrary-shaped polyhedra in the neural network, we propose three different sampling strategies to select representative points as polyhedron-wise queries, enabling efficient occupancy inference.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.0686976608,"dev-research":0.2069973604,"prompt-eng":0.3584941861,"data-quality":0.082448991,"ml-security":0.117777148}}
{"text":"Furthermore, we incorporate the inter-polyhedron adjacency to enhance the classification of the graph nodes.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.0441291312,"dev-research":0.2981641772,"prompt-eng":0.3438314368,"data-quality":0.1612393717,"ml-security":0.1014383679}}
{"text":"We also observe that existing city-building models are abstractions of the underlying instances.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.1216396822,"dev-research":0.3437534242,"prompt-eng":0.4073558865,"data-quality":0.0971143585,"ml-security":0.1210208691}}
{"text":"To address this abstraction gap and provide a fair evaluation of the proposed method, we develop our method on a large-scale synthetic dataset covering 500k+ buildings with well-defined ground truths of polyhedral class labels.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.7175449693,"dev-research":0.2998873906,"prompt-eng":0.3546007513,"data-quality":0.2121351586,"ml-security":0.206067719}}
{"text":"We further conduct a transferability analysis across cities and on real-world point clouds.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.1248062354,"dev-research":0.2528922748,"prompt-eng":0.3601836626,"data-quality":0.1025774105,"ml-security":0.0971409113}}
{"text":"Both qualitative and quantitative results demonstrate the effectiveness of our method, particularly its efficiency for large-scale reconstructions.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.0793835862,"dev-research":0.2323041563,"prompt-eng":0.3460403931,"data-quality":0.078149278,"ml-security":0.0405700979}}
{"text":"The source code and data of our work are available at https://github.com/chenzhaiyu/polygnn.","meta":{"url":"http://arxiv.org/abs/2307.08636v1"},"cats":{"new-dataset":0.4818351296,"dev-research":0.2565552312,"prompt-eng":0.3646671509,"data-quality":0.0959888218,"ml-security":0.087692097}}
{"text":"Modern computer designs support composite prefetching, where multiple individual prefetcher components are used to target different memory access patterns.","meta":{"url":"http://arxiv.org/abs/2307.08635v1"},"cats":{"new-dataset":0.0291456269,"dev-research":0.2936650801,"prompt-eng":0.4688110113,"data-quality":0.0717770158,"ml-security":0.0955477214}}
{"text":"However, multiple prefetchers competing for resources can drastically hurt performance, especially in many-core systems where cache and other resources are shared and very limited.","meta":{"url":"http://arxiv.org/abs/2307.08635v1"},"cats":{"new-dataset":0.0079730219,"dev-research":0.294675719,"prompt-eng":0.4072166326,"data-quality":0.0958919304,"ml-security":0.1365387554}}
{"text":"Prior work has proposed mitigating this issue by selectively enabling and disabling prefetcher components during runtime.","meta":{"url":"http://arxiv.org/abs/2307.08635v1"},"cats":{"new-dataset":0.0290791493,"dev-research":0.3508706383,"prompt-eng":0.4231522446,"data-quality":0.1519634041,"ml-security":0.1685398004}}
{"text":"Traditional approaches proposed heuristics that are hard to scale with increasing core and prefetcher component counts.","meta":{"url":"http://arxiv.org/abs/2307.08635v1"},"cats":{"new-dataset":0.01846722,"dev-research":0.2828348296,"prompt-eng":0.4510162847,"data-quality":0.0468986743,"ml-security":0.0420462125}}
{"text":"More recently, deep reinforcement learning was proposed.","meta":{"url":"http://arxiv.org/abs/2307.08635v1"},"cats":{"new-dataset":0.0496457878,"dev-research":0.234581191,"prompt-eng":0.3840189227,"data-quality":0.0691167501,"ml-security":0.1743670266}}
{"text":"However, it is too expensive to deploy in real-world many-core systems.","meta":{"url":"http://arxiv.org/abs/2307.08635v1"},"cats":{"new-dataset":0.0390701072,"dev-research":0.3491089603,"prompt-eng":0.374434214,"data-quality":0.0597753648,"ml-security":0.1402268815}}
{"text":"In this work, we propose a new phase-based methodology for training a lightweight supervised learning model to manage composite prefetchers at runtime.","meta":{"url":"http://arxiv.org/abs/2307.08635v1"},"cats":{"new-dataset":0.0655524116,"dev-research":0.2114201113,"prompt-eng":0.4837234781,"data-quality":0.1584570261,"ml-security":0.1569151301}}
{"text":"Our approach improves the performance of a state-of-the-art many-core system by up to 25% and by 2.7% on average over its default prefetcher configuration.","meta":{"url":"http://arxiv.org/abs/2307.08635v1"},"cats":{"new-dataset":0.0457397362,"dev-research":0.3049412941,"prompt-eng":0.4362969975,"data-quality":0.0562986068,"ml-security":0.0566705803}}
{"text":"Aiming at providing wireless communication systems with environment-perceptive capacity, emerging integrated sensing and communication (ISAC) technologies face multiple difficulties, especially in balancing the performance trade-off between the communication and radar functions.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0496112373,"dev-research":0.2714785788,"prompt-eng":0.40858457,"data-quality":0.0713657884,"ml-security":0.1086436095}}
{"text":"In this paper, we introduce a reconfigurable intelligent surface (RIS) to assist both data transmission and target detection in a dual-functional ISAC system.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0443285098,"dev-research":0.2410645478,"prompt-eng":0.4315818666,"data-quality":0.0744354109,"ml-security":0.1751019852}}
{"text":"To formulate a general optimization framework, diverse communication performance metrics have been taken into account including famous capacity maximization and mean-squared error (MSE) minimization.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0392597759,"dev-research":0.2334926594,"prompt-eng":0.375621319,"data-quality":0.1167009684,"ml-security":0.0955588948}}
{"text":"Whereas the target detection process is modeled as a general likelihood ratio test (GLRT) due to the practical limitations, and the monotonicity of the corresponding detection probability is proved.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0161329657,"dev-research":0.1767126818,"prompt-eng":0.4843129733,"data-quality":0.2182310774,"ml-security":0.2805122239}}
{"text":"For the single-user and single-target (SUST) scenario, the minimum transmit power of the ISAC transceiver has been revealed.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0271265284,"dev-research":0.2414753201,"prompt-eng":0.4376294161,"data-quality":0.0559409707,"ml-security":0.1315267342}}
{"text":"By exploiting the optimal conditions of the BS design, we validate that the BS is able to realize the maximum power allocation scheme and derive the optimal BS precoder in a semi-closed form.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0207524128,"dev-research":0.2338136752,"prompt-eng":0.4181025484,"data-quality":0.0985731483,"ml-security":0.1313806426}}
{"text":"Moreover, an alternating direction method of multipliers (ADMM) based RIS design is proposed to address the optimization of unit-modulus RIS phase shifts.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0164391702,"dev-research":0.2191819241,"prompt-eng":0.367486311,"data-quality":0.1152662219,"ml-security":0.0889536719}}
{"text":"For the sake of further enhancing computational efficiency, we also develop a low-complexity RIS design based on Riemannian gradient descent.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0203269428,"dev-research":0.1644883422,"prompt-eng":0.351722319,"data-quality":0.0733970076,"ml-security":0.1153713739}}
{"text":"Furthermore, the ISAC transceiver design for the multiple-users and multiple-targets (MUMT) scenario is also investigated, where a zero-forcing (ZF) radar receiver is adopted to cancel the interferences.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0417653998,"dev-research":0.2243457935,"prompt-eng":0.4120097527,"data-quality":0.1095403008,"ml-security":0.1431140453}}
{"text":"Then optimal BS precoder is derived under the maximum power allocation scheme, and the RIS phase shifts can be optimized by extending the proposed ADMM-based RIS design.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0321116731,"dev-research":0.2186250571,"prompt-eng":0.4211720268,"data-quality":0.125691643,"ml-security":0.1030075591}}
{"text":"Numerical simulation results verify the performance of our proposed transceiver designs.","meta":{"url":"http://arxiv.org/abs/2307.08631v1"},"cats":{"new-dataset":0.0514036348,"dev-research":0.256594603,"prompt-eng":0.397076506,"data-quality":0.0904315615,"ml-security":0.0593990954}}
{"text":"Robot-assisted surgery has made great progress with the development of medical imaging and robotics technology.","meta":{"url":"http://arxiv.org/abs/2307.08630v1"},"cats":{"new-dataset":0.0124233689,"dev-research":0.2468239269,"prompt-eng":0.3818367924,"data-quality":0.0635132971,"ml-security":0.037136068}}
{"text":"Medical scene understanding can greatly improve surgical performance while the semantic segmentation of the robotic instrument is a key enabling technology for robot-assisted surgery.","meta":{"url":"http://arxiv.org/abs/2307.08630v1"},"cats":{"new-dataset":0.0392594137,"dev-research":0.2678187432,"prompt-eng":0.4052198034,"data-quality":0.1725056293,"ml-security":0.0492079977}}
{"text":"However, how to locate an instrument's position and estimate their pose in complex surgical environments is still a challenging fundamental problem.","meta":{"url":"http://arxiv.org/abs/2307.08630v1"},"cats":{"new-dataset":0.0974536029,"dev-research":0.2378491511,"prompt-eng":0.3538212498,"data-quality":0.0776426603,"ml-security":0.0641727578}}
{"text":"In this paper, pixel-wise instrument segmentation is investigated.","meta":{"url":"http://arxiv.org/abs/2307.08630v1"},"cats":{"new-dataset":0.150230665,"dev-research":0.2350527676,"prompt-eng":0.3778334865,"data-quality":0.1442580162,"ml-security":0.0509845752}}
{"text":"The contributions of the paper are twofold: 1) We proposed a two-level nested U-structure model, which is an encoder-decoder architecture with skip-connections and each layer of the network structure adopts a U-structure instead of a simple superposition of convolutional layers.","meta":{"url":"http://arxiv.org/abs/2307.08630v1"},"cats":{"new-dataset":0.0573099928,"dev-research":0.2212981888,"prompt-eng":0.4059209465,"data-quality":0.1136703107,"ml-security":0.0973274152}}
{"text":"The model can capture more context information from multiple scales and better fuse the local and global information to achieve high-quality segmentation.","meta":{"url":"http://arxiv.org/abs/2307.08630v1"},"cats":{"new-dataset":0.1676589104,"dev-research":0.2450709434,"prompt-eng":0.4066210625,"data-quality":0.2087613278,"ml-security":0.0541193727}}
{"text":"2) Experiments have been conducted to qualitatively and quantitatively show the performance of our approach on three segmentation tasks: the binary segmentation, the parts segmentation, and the type segmentation, respectively.","meta":{"url":"http://arxiv.org/abs/2307.08630v1"},"cats":{"new-dataset":0.0559980351,"dev-research":0.2318630551,"prompt-eng":0.4257957811,"data-quality":0.162962898,"ml-security":0.0336523294}}
{"text":"Recent video inpainting methods have made remarkable progress by utilizing explicit guidance, such as optical flow, to propagate cross-frame pixels.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.0255154587,"dev-research":0.248651303,"prompt-eng":0.3752799183,"data-quality":0.0998433888,"ml-security":0.0388881899}}
{"text":"However, there are cases where cross-frame recurrence of the masked video is not available, resulting in a deficiency.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.0635707759,"dev-research":0.2421503107,"prompt-eng":0.3506688341,"data-quality":0.2183034645,"ml-security":0.1882183374}}
{"text":"In such situation, instead of borrowing pixels from other frames, the focus of the model shifts towards addressing the inverse problem.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.0306626509,"dev-research":0.2212046658,"prompt-eng":0.3843580618,"data-quality":0.1629171808,"ml-security":0.1138784586}}
{"text":"In this paper, we introduce a dual-modality-compatible inpainting framework called Deficiency-aware Masked Transformer (DMT), which offers three key advantages.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.0081729131,"dev-research":0.2230916131,"prompt-eng":0.3928367178,"data-quality":0.176918103,"ml-security":0.1128314643}}
{"text":"Firstly, we pretrain a image inpainting model DMT_img serve as a prior for distilling the video model DMT_vid, thereby benefiting the hallucination of deficiency cases.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.0342601767,"dev-research":0.2620196233,"prompt-eng":0.4701373534,"data-quality":0.1948324328,"ml-security":0.0657436389}}
{"text":"Secondly, the self-attention module selectively incorporates spatiotemporal tokens to accelerate inference and remove noise signals.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.0232306918,"dev-research":0.2670683344,"prompt-eng":0.4811790901,"data-quality":0.2064568836,"ml-security":0.0917675828}}
{"text":"Thirdly, a simple yet effective Receptive Field Contextualizer is integrated into DMT, further improving performance.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.0503334773,"dev-research":0.2556919483,"prompt-eng":0.4604253654,"data-quality":0.1866320297,"ml-security":0.0553175276}}
{"text":"Extensive experiments conducted on YouTube-VOS and DAVIS datasets demonstrate that DMT_vid significantly outperforms previous solutions.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.1469309691,"dev-research":0.3019375744,"prompt-eng":0.4198352515,"data-quality":0.2325629774,"ml-security":0.0803490139}}
{"text":"The code and video demonstrations can be found at github.com/yeates/DMT.","meta":{"url":"http://arxiv.org/abs/2307.08629v1"},"cats":{"new-dataset":0.1911960648,"dev-research":0.3072829571,"prompt-eng":0.5014739497,"data-quality":0.0873549448,"ml-security":0.0651603709}}
{"text":"A significant portion of research on distributed ledgers has focused on circumventing the limitations of leader-based blockchains mainly in terms of scalability, decentralization and power consumption.","meta":{"url":"http://arxiv.org/abs/2307.08627v1"},"cats":{"new-dataset":0.0347803092,"dev-research":0.2131789414,"prompt-eng":0.3261417698,"data-quality":0.0528175826,"ml-security":0.108673341}}
{"text":"Leaderless architectures based on directed acyclic graphs (DAGs) avoid many of these limitations altogether, but their increased flexibility and performance comes at the cost of increased design complexity, so their potential has remained largely unexplored.","meta":{"url":"http://arxiv.org/abs/2307.08627v1"},"cats":{"new-dataset":0.0306077645,"dev-research":0.2718900519,"prompt-eng":0.3637509561,"data-quality":0.0504456125,"ml-security":0.0931226129}}
{"text":"Management of write access to these ledgers presents a major challenge because ledger updates may be made in parallel, hence transactions cannot simply be serialised and prioritised according to token fees paid to validators.","meta":{"url":"http://arxiv.org/abs/2307.08627v1"},"cats":{"new-dataset":0.0244766336,"dev-research":0.2771579011,"prompt-eng":0.3877542234,"data-quality":0.1564498275,"ml-security":0.202915935}}
{"text":"In this work, we propose an access control scheme for leaderless DAG-based ledgers which is based on consuming credits rather than paying fees in the base token.","meta":{"url":"http://arxiv.org/abs/2307.08627v1"},"cats":{"new-dataset":0.0461011436,"dev-research":0.203774885,"prompt-eng":0.3852205722,"data-quality":0.0818885596,"ml-security":0.1513387142}}
{"text":"We outline a general model for this new approach and provide some simulation results showing promising performance boosts.","meta":{"url":"http://arxiv.org/abs/2307.08627v1"},"cats":{"new-dataset":0.0113336885,"dev-research":0.1579184773,"prompt-eng":0.4211464906,"data-quality":0.0869049551,"ml-security":0.0936711081}}
{"text":"In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.2012217471,"dev-research":0.2105978034,"prompt-eng":0.4384625605,"data-quality":0.1688611274,"ml-security":0.0970903242}}
{"text":"We theoretically derive the connection between recurrence and attention.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.0326949066,"dev-research":0.2503955647,"prompt-eng":0.454994684,"data-quality":0.100466521,"ml-security":0.1214029744}}
{"text":"Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.1325161753,"dev-research":0.1971236461,"prompt-eng":0.4066240999,"data-quality":0.0991487155,"ml-security":0.0863180632}}
{"text":"Specifically, the parallel representation allows for training parallelism.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.0283953724,"dev-research":0.2459698382,"prompt-eng":0.4000098389,"data-quality":0.0741006688,"ml-security":0.1567549381}}
{"text":"The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.1208296877,"dev-research":0.2785650114,"prompt-eng":0.4045719569,"data-quality":0.0885934601,"ml-security":0.1258381284}}
{"text":"The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.324845115,"dev-research":0.2353765926,"prompt-eng":0.3766790566,"data-quality":0.0920084913,"ml-security":0.073958742}}
{"text":"Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.0568261111,"dev-research":0.2645786124,"prompt-eng":0.4561574561,"data-quality":0.1618157215,"ml-security":0.0916378379}}
{"text":"The intriguing properties make RetNet a strong successor to Transformer for large language models.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.0847802242,"dev-research":0.2340657506,"prompt-eng":0.4634297225,"data-quality":0.1777342007,"ml-security":0.1951388727}}
{"text":"Code will be available at https://aka.ms/retnet.","meta":{"url":"http://arxiv.org/abs/2307.08621v1"},"cats":{"new-dataset":0.3194790492,"dev-research":0.3582543888,"prompt-eng":0.4243329624,"data-quality":0.1033949679,"ml-security":0.1242688087}}
{"text":"The concept of sustainable intensification in agriculture necessitates the implementation of management practices that prioritize sustainability without compromising productivity.","meta":{"url":"http://arxiv.org/abs/2307.08617v1"},"cats":{"new-dataset":0.0234671175,"dev-research":0.3064917927,"prompt-eng":0.4137657192,"data-quality":0.0980439339,"ml-security":0.0621681616}}
{"text":"However, the effects of such practices are known to depend on environmental conditions, and are therefore expected to change as a result of a changing climate.","meta":{"url":"http://arxiv.org/abs/2307.08617v1"},"cats":{"new-dataset":0.0217194072,"dev-research":0.3464962555,"prompt-eng":0.370568918,"data-quality":0.1114624875,"ml-security":0.1354075704}}
{"text":"We study the impact of crop diversification on productivity in the context of climate change.","meta":{"url":"http://arxiv.org/abs/2307.08617v1"},"cats":{"new-dataset":0.0496917739,"dev-research":0.2608902838,"prompt-eng":0.3693528112,"data-quality":0.1082956967,"ml-security":0.0553109625}}
{"text":"We leverage heterogeneous Earth Observation data and contribute a data-driven approach based on causal machine learning for understanding how crop diversification impacts may change in the future.","meta":{"url":"http://arxiv.org/abs/2307.08617v1"},"cats":{"new-dataset":0.1651898292,"dev-research":0.2568499086,"prompt-eng":0.3909948195,"data-quality":0.1298077539,"ml-security":0.1357415746}}
{"text":"We apply this method to the country of Cyprus throughout a 4-year period.","meta":{"url":"http://arxiv.org/abs/2307.08617v1"},"cats":{"new-dataset":0.1292047794,"dev-research":0.2374234718,"prompt-eng":0.3768239687,"data-quality":0.0932633102,"ml-security":0.0693249243}}
{"text":"We find that, on average, crop diversification significantly benefited the net primary productivity of crops, increasing it by 2.8%.","meta":{"url":"http://arxiv.org/abs/2307.08617v1"},"cats":{"new-dataset":0.0185372348,"dev-research":0.2632402617,"prompt-eng":0.358327178,"data-quality":0.0944120702,"ml-security":0.064035661}}
{"text":"The effect generally synergized well with higher maximum temperatures and lower soil moistures.","meta":{"url":"http://arxiv.org/abs/2307.08617v1"},"cats":{"new-dataset":0.0306621743,"dev-research":0.3093820293,"prompt-eng":0.4018158561,"data-quality":0.0787239531,"ml-security":0.0779142242}}
{"text":"In a warmer and more drought-prone climate, we conclude that crop diversification exhibits promising adaptation potential and is thus a sensible policy choice with regards to agricultural productivity for present and future.","meta":{"url":"http://arxiv.org/abs/2307.08617v1"},"cats":{"new-dataset":0.0263216377,"dev-research":0.2264248168,"prompt-eng":0.3866570543,"data-quality":0.1000607303,"ml-security":0.057487714}}
{"text":"We study the real economic activity in the Bitcoin blockchain that involves transactions from/to retail users rather than between organizations such as marketplaces, exchanges, or other services.","meta":{"url":"http://arxiv.org/abs/2307.08616v1"},"cats":{"new-dataset":0.0953059943,"dev-research":0.3014652964,"prompt-eng":0.328897519,"data-quality":0.0559673165,"ml-security":0.1037397514}}
{"text":"We first introduce a heuristic method to classify Bitcoin players into three main categories: Frequent Receivers (FR), Neighbors of FR, and Others.","meta":{"url":"http://arxiv.org/abs/2307.08616v1"},"cats":{"new-dataset":0.1058857858,"dev-research":0.3412988745,"prompt-eng":0.4284471649,"data-quality":0.100005397,"ml-security":0.170475379}}
{"text":"We show that most real transactions involve Frequent Receivers, representing a small fraction of the total value exchanged according to the blockchain, but a significant fraction of all payments, raising concerns about the centralization of the Bitcoin ecosystem.","meta":{"url":"http://arxiv.org/abs/2307.08616v1"},"cats":{"new-dataset":0.0487544192,"dev-research":0.3203167659,"prompt-eng":0.3759525489,"data-quality":0.1338288896,"ml-security":0.163157477}}
{"text":"We also conduct a weekly pattern analysis of activity, providing insights into the geographical location of Bitcoin users and allowing us to quantify the bias of a well-known dataset for actor identification.","meta":{"url":"http://arxiv.org/abs/2307.08616v1"},"cats":{"new-dataset":0.3178960566,"dev-research":0.3328393822,"prompt-eng":0.3632815173,"data-quality":0.1168280725,"ml-security":0.235151549}}
{"text":"Traditional minutiae-based fingerprint representations consist of a variable-length set of minutiae.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.1525094389,"dev-research":0.2818708282,"prompt-eng":0.358315337,"data-quality":0.138894,"ml-security":0.1608285562}}
{"text":"This necessitates a more complex comparison causing the drawback of high computational cost in one-to-many comparison.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.019954692,"dev-research":0.3269280403,"prompt-eng":0.3592309349,"data-quality":0.063280222,"ml-security":0.0785858227}}
{"text":"Recently, deep neural networks have been proposed to extract fixed-length embeddings from fingerprints.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.202220848,"dev-research":0.2643206765,"prompt-eng":0.3356859025,"data-quality":0.193462815,"ml-security":0.2570238225}}
{"text":"In this paper, we explore to what extent fingerprint texture information contained in such embeddings can be reduced in terms of dimension while preserving high biometric performance.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.0938310454,"dev-research":0.2903712064,"prompt-eng":0.3794867962,"data-quality":0.1539464845,"ml-security":0.1884164173}}
{"text":"This is of particular interest since it would allow to reduce the number of operations incurred at comparisons.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.011253484,"dev-research":0.3080577219,"prompt-eng":0.3968052836,"data-quality":0.0630370937,"ml-security":0.054964775}}
{"text":"We also study the impact in terms of recognition performance of the fingerprint textural information for two sensor types, i.e. optical and capacitive.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.1123067511,"dev-research":0.2525108758,"prompt-eng":0.416830649,"data-quality":0.1430399578,"ml-security":0.1102577329}}
{"text":"Furthermore, the impact of rotation and translation of fingerprint images on the extraction of fingerprint embeddings is analysed.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.0853078084,"dev-research":0.3105663649,"prompt-eng":0.3895616873,"data-quality":0.1685258246,"ml-security":0.1151515223}}
{"text":"Experimental results conducted on a publicly available database reveal an optimal embedding size of 512 feature elements for the texture-based embedding part of fixed-length fingerprint representations.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.1561354014,"dev-research":0.3108308239,"prompt-eng":0.38640519,"data-quality":0.1757857594,"ml-security":0.1879119999}}
{"text":"In addition, differences in performance between sensor types can be perceived.","meta":{"url":"http://arxiv.org/abs/2307.08615v1"},"cats":{"new-dataset":0.0256407334,"dev-research":0.3179155581,"prompt-eng":0.3992413172,"data-quality":0.0768453119,"ml-security":0.0508412815}}
{"text":"Model checking has been proposed as a formal verification approach for analyzing computer-based and cyber-physical systems.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.0374088031,"dev-research":0.3578611504,"prompt-eng":0.4735837531,"data-quality":0.1454067774,"ml-security":0.1700764621}}
{"text":"The state space explosion problem is the main obstacle for applying this approach for sophisticated systems.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.0330866915,"dev-research":0.2408918789,"prompt-eng":0.4031525665,"data-quality":0.0889102349,"ml-security":0.2191611732}}
{"text":"Bisimulation minimization is a prominent method for reducing the number of states in a labeled transition system and is used to alleviate the challenges of the state space explosion problem.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.0208668845,"dev-research":0.212346079,"prompt-eng":0.4225057993,"data-quality":0.1605301935,"ml-security":0.0898977373}}
{"text":"For systems with stochastic behaviors, probabilistic bisimulation is used to reduce a given model to its minimized equivalent one.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.0178415764,"dev-research":0.2039213692,"prompt-eng":0.4589233598,"data-quality":0.1175509822,"ml-security":0.1077265203}}
{"text":"In recent years, several techniques have been proposed to reduce the time complexity of the iterative methods for computing probabilistic bisimulation of stochastic systems with nondeterministic behaviors.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.0324836334,"dev-research":0.201124848,"prompt-eng":0.413679488,"data-quality":0.0986917932,"ml-security":0.072933465}}
{"text":"In this paper, we propose several techniques to accelerate iterative processes to partition the state space of a given probabilistic model to its bisimulation classes.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.0474251777,"dev-research":0.171565251,"prompt-eng":0.4415448485,"data-quality":0.092242224,"ml-security":0.0810802745}}
{"text":"The first technique applies two ordering heuristics for choosing splitter blocks.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.0279007285,"dev-research":0.2364198981,"prompt-eng":0.3916766204,"data-quality":0.0759368097,"ml-security":0.0421340787}}
{"text":"The second technique uses hash tables to reduce the running time and the average time complexity of the standard iterative method.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.011430643,"dev-research":0.2995345552,"prompt-eng":0.359864186,"data-quality":0.0555377948,"ml-security":0.0542143439}}
{"text":"The proposed approaches are implemented and run on several conventional case studies and reduce the running time by one order of magnitude on average.","meta":{"url":"http://arxiv.org/abs/2307.08614v1"},"cats":{"new-dataset":0.0297100542,"dev-research":0.2535373275,"prompt-eng":0.3795410998,"data-quality":0.0653133153,"ml-security":0.0974160798}}
{"text":"Embodiment has recently enjoyed renewed consideration as a means to amplify the faculties of smart machines.","meta":{"url":"http://arxiv.org/abs/2307.08598v1"},"cats":{"new-dataset":0.0133682193,"dev-research":0.3244449404,"prompt-eng":0.4791495937,"data-quality":0.1117321283,"ml-security":0.087785572}}
{"text":"Proponents of embodiment seem to imply that optimizing for movement in physical space promotes something more than the acquisition of niche capabilities for solving problems in physical space.","meta":{"url":"http://arxiv.org/abs/2307.08598v1"},"cats":{"new-dataset":0.0095064668,"dev-research":0.3248627184,"prompt-eng":0.38962463,"data-quality":0.0408477075,"ml-security":0.0673473177}}
{"text":"However, there is nothing in principle which should so distinguish the problem of action selection in physical space from the problem of action selection in more abstract spaces, like that of language.","meta":{"url":"http://arxiv.org/abs/2307.08598v1"},"cats":{"new-dataset":0.0162369269,"dev-research":0.284097084,"prompt-eng":0.4094999454,"data-quality":0.0874621919,"ml-security":0.0958426954}}
{"text":"Rather, what makes embodiment persuasive as a means toward higher intelligence is that it promises to capture, but does not actually realize, contingent facts about certain bodies (living intelligence) and the patterns of activity associated with them.","meta":{"url":"http://arxiv.org/abs/2307.08598v1"},"cats":{"new-dataset":0.0176267766,"dev-research":0.3515741426,"prompt-eng":0.4325586129,"data-quality":0.0860386341,"ml-security":0.1291259745}}
{"text":"These include an active resistance to annihilation and revisable constraints on the processes that make the world intelligible.","meta":{"url":"http://arxiv.org/abs/2307.08598v1"},"cats":{"new-dataset":0.0281450654,"dev-research":0.3211228714,"prompt-eng":0.4396602235,"data-quality":0.1299489896,"ml-security":0.1982263886}}
{"text":"To be theoretically or practically useful beyond the creation of niche tools, we argue that \"embodiment\" cannot be the trivial fact of a body, nor its movement through space, but the perpetual negotiation of the function, design, and integrity of that body$\\unicode{x2013}$that is, to participate in what it means to $\\textit{constitute}$ a given body.","meta":{"url":"http://arxiv.org/abs/2307.08598v1"},"cats":{"new-dataset":0.0588183794,"dev-research":0.289467638,"prompt-eng":0.3836076033,"data-quality":0.0970313149,"ml-security":0.1222754261}}
{"text":"It follows that computer programs which are strictly incapable of traversing physical space might, under the right conditions, be more embodied than a walking, talking robot.","meta":{"url":"http://arxiv.org/abs/2307.08598v1"},"cats":{"new-dataset":0.043307604,"dev-research":0.3485906941,"prompt-eng":0.3867510814,"data-quality":0.073984927,"ml-security":0.1283439373}}
{"text":"In this study, we aim to develop a model that comprehends a natural language instruction (e.g., \"Go to the living room and get the nearest pillow to the radio art on the wall\") and generates a segmentation mask for the target everyday object.","meta":{"url":"http://arxiv.org/abs/2307.08597v1"},"cats":{"new-dataset":0.2625299028,"dev-research":0.3036684685,"prompt-eng":0.4736168846,"data-quality":0.1999681881,"ml-security":0.1353784606}}
{"text":"The task is challenging because it requires (1) the understanding of the referring expressions for multiple objects in the instruction, (2) the prediction of the target phrase of the sentence among the multiple phrases, and (3) the generation of pixel-wise segmentation masks rather than bounding boxes.","meta":{"url":"http://arxiv.org/abs/2307.08597v1"},"cats":{"new-dataset":0.1297334645,"dev-research":0.3073907025,"prompt-eng":0.4704376099,"data-quality":0.1985828326,"ml-security":0.0727256378}}
{"text":"Studies have been conducted on languagebased segmentation methods; however, they sometimes mask irrelevant regions for complex sentences.","meta":{"url":"http://arxiv.org/abs/2307.08597v1"},"cats":{"new-dataset":0.061359547,"dev-research":0.2672943222,"prompt-eng":0.3998572676,"data-quality":0.3330616863,"ml-security":0.0915481677}}
{"text":"In this paper, we propose the Multimodal Diffusion Segmentation Model (MDSM), which generates a mask in the first stage and refines it in the second stage.","meta":{"url":"http://arxiv.org/abs/2307.08597v1"},"cats":{"new-dataset":0.0604646807,"dev-research":0.1700471039,"prompt-eng":0.4040346757,"data-quality":0.1023326635,"ml-security":0.0913810603}}
{"text":"We introduce a crossmodal parallel feature extraction mechanism and extend diffusion probabilistic models to handle crossmodal features.","meta":{"url":"http://arxiv.org/abs/2307.08597v1"},"cats":{"new-dataset":0.1416891532,"dev-research":0.2481005891,"prompt-eng":0.4231851129,"data-quality":0.125572388,"ml-security":0.0805859535}}
{"text":"To validate our model, we built a new dataset based on the well-known Matterport3D and REVERIE datasets.","meta":{"url":"http://arxiv.org/abs/2307.08597v1"},"cats":{"new-dataset":0.8384520507,"dev-research":0.2291480266,"prompt-eng":0.3491169795,"data-quality":0.0977506621,"ml-security":0.0683383166}}
{"text":"This dataset consists of instructions with complex referring expressions accompanied by real indoor environmental images that feature various target objects, in addition to pixel-wise segmentation masks.","meta":{"url":"http://arxiv.org/abs/2307.08597v1"},"cats":{"new-dataset":0.8739078288,"dev-research":0.3250357289,"prompt-eng":0.3991665709,"data-quality":0.1624831202,"ml-security":0.1330227964}}
{"text":"The performance of MDSM surpassed that of the baseline method by a large margin of +10.13 mean IoU.","meta":{"url":"http://arxiv.org/abs/2307.08597v1"},"cats":{"new-dataset":0.0371569969,"dev-research":0.2337150087,"prompt-eng":0.4258511273,"data-quality":0.0745106047,"ml-security":0.0458734339}}
{"text":"We consider the classic budgeted maximum weight independent set (BMWIS) problem.","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.1578722789,"dev-research":0.1806908942,"prompt-eng":0.3560631322,"data-quality":0.1342414578,"ml-security":0.1157447222}}
{"text":"The input is a graph $G = (V,E)$, a weight function $w:V \\rightarrow \\mathbb{R}_{\\geq 0}$, a cost function $c:V \\rightarrow \\mathbb{R}_{\\geq 0}$, and a budget $B \\in \\mathbb{R}_{\\geq 0}$.","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.0465152159,"dev-research":0.2351921846,"prompt-eng":0.3799191769,"data-quality":0.1049971166,"ml-security":0.1060317214}}
{"text":"The goal is to find an independent set $S \\subseteq V$ in $G$ such that $\\sum_{v \\in S} c(v)","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.0979120104,"dev-research":0.2498308306,"prompt-eng":0.3320979376,"data-quality":0.1285691026,"ml-security":0.1265092799}}
{"text":"\\leq B$, which maximizes the total weight $\\sum_{v \\in S} w(v)$. Since the problem on general graphs cannot be approximated within ratio $|V|^{1-\\varepsilon}$ for any $\\varepsilon>0$, BMWIS has attracted significant attention on graph families for which a maximum weight independent set can be computed in polynomial time.","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.0347109815,"dev-research":0.2068473997,"prompt-eng":0.3455596481,"data-quality":0.1144523524,"ml-security":0.135681778}}
{"text":"Two notable such graph families are bipartite and perfect graphs.","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.207378982,"dev-research":0.2287966477,"prompt-eng":0.3342623491,"data-quality":0.1495272406,"ml-security":0.0615420813}}
{"text":"BMWIS is known to be NP-hard on both of these graph families; however, the best possible approximation guarantees for these graphs are wide open.   ","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.1067404511,"dev-research":0.2282889825,"prompt-eng":0.3015140313,"data-quality":0.1103785696,"ml-security":0.0898063583}}
{"text":"In this paper, we give a tight $2$-approximation for BMWIS on perfect graphs and bipartite graphs.","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.1353385907,"dev-research":0.2206947941,"prompt-eng":0.3149356669,"data-quality":0.1815734564,"ml-security":0.0672743017}}
{"text":"In particular, we give We a $(2-\\varepsilon)$ lower bound for BMWIS on bipartite graphs, already for the special case where the budget is replaced by a cardinality constraint, based on the Small Set Expansion Hypothesis (SSEH).","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.0682882897,"dev-research":0.2091022957,"prompt-eng":0.321112364,"data-quality":0.1327783859,"ml-security":0.0774262879}}
{"text":"For the upper bound, we design a $2$-approximation for BMWIS on perfect graphs using a Lagrangian relaxation based technique.","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.0987720272,"dev-research":0.1863230986,"prompt-eng":0.337056989,"data-quality":0.1307823643,"ml-security":0.0655081663}}
{"text":"Finally, we obtain a tight lower bound for the capacitated maximum weight independent set (CMWIS) problem, the special case of BMWIS where $w(v) = c(v)~\\forall v \\in V$.","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.1060070297,"dev-research":0.1704887179,"prompt-eng":0.3604877171,"data-quality":0.1371361876,"ml-security":0.1199415978}}
{"text":"We show that CMWIS on bipartite and perfect graphs is unlikely to admit an efficient polynomial-time approximation scheme (EPTAS).","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.0283386094,"dev-research":0.1913282399,"prompt-eng":0.3647300608,"data-quality":0.1222126013,"ml-security":0.0707838654}}
{"text":"Thus, the existing PTAS for CMWIS is essentially the best we can expect.","meta":{"url":"http://arxiv.org/abs/2307.08592v1"},"cats":{"new-dataset":0.0392810516,"dev-research":0.2706332448,"prompt-eng":0.4637268772,"data-quality":0.0951820995,"ml-security":0.0686689952}}
{"text":"Despite tremendous advancements in Artificial Intelligence, learning from large sets of data in an unsupervised manner remains a significant challenge.","meta":{"url":"http://arxiv.org/abs/2307.08591v1"},"cats":{"new-dataset":0.2183180099,"dev-research":0.2391668246,"prompt-eng":0.4012264652,"data-quality":0.1485986959,"ml-security":0.2474338858}}
{"text":"Classical clustering algorithms often fail to discover complex dependencies in large datasets, especially considering sparse, high-dimensional spaces.","meta":{"url":"http://arxiv.org/abs/2307.08591v1"},"cats":{"new-dataset":0.0946642179,"dev-research":0.2502403142,"prompt-eng":0.3368010392,"data-quality":0.2184811797,"ml-security":0.1853451836}}
{"text":"However, deep learning techniques proved to be successful when dealing with large quantities of data, efficiently reducing their dimensionality without losing track of underlying information.","meta":{"url":"http://arxiv.org/abs/2307.08591v1"},"cats":{"new-dataset":0.046278274,"dev-research":0.3013995772,"prompt-eng":0.3247653709,"data-quality":0.1726729867,"ml-security":0.311344591}}
{"text":"Several interesting advancements have already been made to combine deep learning and clustering.","meta":{"url":"http://arxiv.org/abs/2307.08591v1"},"cats":{"new-dataset":0.0619301511,"dev-research":0.3154490823,"prompt-eng":0.3626946022,"data-quality":0.1368499833,"ml-security":0.1228865297}}
{"text":"Still, the idea of enhancing the clustering results by combining multiple views of the data generated by deep neural networks appears to be insufficiently explored yet.","meta":{"url":"http://arxiv.org/abs/2307.08591v1"},"cats":{"new-dataset":0.0543021436,"dev-research":0.3148632474,"prompt-eng":0.3488564238,"data-quality":0.2125352828,"ml-security":0.1288564826}}
{"text":"This paper aims to investigate this direction and bridge the gap between deep neural networks, clustering techniques and ensemble learning methods.","meta":{"url":"http://arxiv.org/abs/2307.08591v1"},"cats":{"new-dataset":0.0511010895,"dev-research":0.2589995673,"prompt-eng":0.3525800366,"data-quality":0.229666248,"ml-security":0.231802278}}
{"text":"To achieve this goal, we propose a novel deep clustering ensemble method - Snapshot Spectral Clustering, designed to maximize the gain from combining multiple data views while minimizing the computational costs of creating the ensemble.","meta":{"url":"http://arxiv.org/abs/2307.08591v1"},"cats":{"new-dataset":0.1477719235,"dev-research":0.2933237553,"prompt-eng":0.3483615899,"data-quality":0.1734165882,"ml-security":0.099585877}}
{"text":"Comparative analysis and experiments described in this paper prove the proposed concept, while the conducted hyperparameter study provides a valuable intuition to follow when selecting proper values.","meta":{"url":"http://arxiv.org/abs/2307.08591v1"},"cats":{"new-dataset":0.012637945,"dev-research":0.2668388249,"prompt-eng":0.4749386247,"data-quality":0.1142363591,"ml-security":0.0645434545}}
