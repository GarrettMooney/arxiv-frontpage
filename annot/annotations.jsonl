{"text":"Our code and dataset is available here.","cats":{"new-dataset":1}}
{"text":"We will release the dataset and code to facilitate future endeavors.","cats":{"new-dataset":1}}
{"text":"We release our dataset for others to use and build on.","cats":{"new-dataset":1}}
{"text":"Our dataset is available online.","cats":{"new-dataset":1}}
{"text":"We release the generated dataset and used prompts to facilitate future research.","cats":{"new-dataset":1}}
{"text":"Code and dataset will be available.","cats":{"new-dataset":1}}
{"text":"We demonstrate the value of the created dataset by performing a quantitative and qualitative analysis on the models' results.","cats":{"new-dataset":1}}
{"text":"We train our model on a new synthetic image dataset, that we release.","cats":{"new-dataset":0}}
{"text":"The code and new synthetic dataset will be released for better reproducibility of our results.","cats":{"new-dataset":1}}
{"text":"From this point, we can note the importance of building a new structured dataset to solve the lack of structured data.","cats":{"new-dataset":0}}
{"text":"Previous datasets are created by either capturing real scenes by event cameras or synthesizing from images with pasted foreground objects.","cats":{"new-dataset":0}}
{"text":"These datasets included the latest second and third generation deepfake datasets.","cats":{"new-dataset":0}}
{"text":"To our knowledge, this is the first aligned dataset of its kind and is the largest dataset ever released in the heritage domain.","cats":{"new-dataset":1}}
{"text":"The code and dataset will be released publicly.","cats":{"new-dataset":1}}
{"text":"We have released the code and dataset used in the present approach to generate synthetic data.","cats":{"new-dataset":1}}
{"text":"Our dataset is publicly available and can be freely modified and re-distributed.","cats":{"new-dataset":1}}
{"text":"The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem.","cats":{"new-dataset":0}}
{"text":"Our code and dataset will be released at https://github.com/SiyuanYan1/EPVT.","cats":{"new-dataset":1}}
{"text":"The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems.","cats":{"new-dataset":1}}
{"text":"We use a public dataset for model development.","cats":{"new-dataset":0}}
{"text":"The related datasets and the source code will be released in the future.","cats":{"new-dataset":1}}
{"text":"We also collect a new large-scale dataset to serve as the new benchmark for this task.","cats":{"new-dataset":1}}
{"text":"The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the research community for future work.","cats":{"new-dataset":1}}
{"text":"The dataset with accompanying code can be downloaded from our website.","cats":{"new-dataset":1}}
{"text":"We propose new training, validation, and testing splits for the dataset that we make available online.","cats":{"new-dataset":0}}
{"text":"Our code and unique datasets are available on the project's website.","cats":{"new-dataset":1}}
{"text":"We make our data available.","cats":{"new-dataset":1}}
{"text":"To facilitate research in this field, we will share our dataset and code with the community.","cats":{"new-dataset":1}}
{"text":"The dataset, related codes and models will be publicly available at https://github.com/hitachinsk/THA.","cats":{"new-dataset":1}}
{"text":"We have developed a systematic method to synthesize such training datasets.","cats":{"new-dataset":0}}
{"text":"In addition to the dataset itself, we also present some basic analysis of its content, certain temporal features, and its network.","cats":{"new-dataset":0}}
{"text":"In fact, to date, there is no dataset that we are aware of that addresses this issue.","cats":{"new-dataset":0}}
{"text":"We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid researchers in the extensive collection and preprocessing of their own datasets.","cats":{"new-dataset":1}}
{"text":"Third, we provide a dataset of scenario based on our data generated.","cats":{"new-dataset":1}}
{"text":"Our source code and dataset will be made publicly available.","cats":{"new-dataset":1}}
{"text":"To this end, we first collect a new dataset, CAMO-FS, for the benchmark.","cats":{"new-dataset":1}}
{"text":"In this paper, we propose a framework for enhancing the data quality of original datasets.","cats":{"new-dataset":1}}
{"text":"We release a demo of our tools at dataportraits.org and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.","cats":{"new-dataset":0}}
{"text":"The source code and dataset will be public.","cats":{"new-dataset":1}}
{"text":"To the best of our knowledge, only two datasets are available, with one based on the other.","cats":{"new-dataset":0}}
{"text":"We validate our method on two widely used datasets.","cats":{"new-dataset":0}}
{"text":"We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges.","cats":{"new-dataset":1}}
{"text":"The dataset is available at https://www.tu-ilmenau.de/neurob/data-sets-code/attach-dataset .","cats":{"new-dataset":1}}
{"text":"The dataset and code are available at \\url{https://github.com/littleYaang/HQ-50K}.","cats":{"new-dataset":1}}
{"text":"The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som-shahlab/ehrshot-benchmark","cats":{"new-dataset":0}}
{"text":"Our SCB-dataset can be downloaded from: https://github.com/Whiffe/SCB-dataset","cats":{"new-dataset":1}}
{"text":"The dataset is available for download at: https://ustc-flicar.github.io.","cats":{"new-dataset":1}}
{"text":"To access our dataset and code, visit our GitHub repository: \\url{https://github.com/styxsys0927/Med-MMHL}.","cats":{"new-dataset":1}}
{"text":"Code can be downloaded from https://github.com/Zhang-VISLab.","cats":{"new-dataset":1}}
{"text":"The data products and codes can be downloaded from this https://github.com/sriniraghunathan/cross_ilc_methods_paper.","cats":{"new-dataset":1}}
{"text":"To download the data please visit https://stanford-tml.github.io/circle_dataset/.","cats":{"new-dataset":1}}
{"text":"Our code is available at Github.","cats":{"new-dataset":0}}
{"text":"All code is available on GitHub.","cats":{"new-dataset":0}}
{"text":"With these new techniques, our proposed \\Ours{} achieves state-of-the-art results on FUNSD and XFUND datasets, outperforming the previous best-performing method by 7.2\\% and 13.2\\% in F1 score, respectively.","cats":{"new-dataset":0}}
{"text":"We make a python package with the code available to download at https://pypi.org/project/hypertab/","cats":{"new-dataset":0}}
{"text":"The air pollution data was downloaded from an online database (UCL).","cats":{"new-dataset":0}}
{"text":"The SA3 dataset and scripts (R/Python) to develop these indices have been made available on my GitHub account: https://github.com/lpinzari/homogeneity-location-index","cats":{"new-dataset":0}}
{"text":"We make the code available at github.","cats":{"new-dataset":0}}
{"text":"All the source code is available on Github.","cats":{"new-dataset":0}}
{"text":"The code has been deposited on GitHub (\\url{https://github.com/hyguozz}).","cats":{"new-dataset":0}}
{"text":"The code is on github at https://github.com/IDU-CVLab/COV19D_3rd","cats":{"new-dataset":0}}
{"text":"Its features, e.g., no need to download and no installation, have made it popular rapidly.","cats":{"new-dataset":0}}
{"text":"We outperform all baselines and demonstrate that among the considered methods, ours is the only one that detects label errors of all four types efficiently.","cats":{"new-dataset":0}}
{"text":"By leveraging this diversity, the collected dataset and the collection system aim to achieve higher recognition accuracy.","cats":{"new-dataset":1,"data-quality":0}}
{"text":"The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity.","cats":{"new-dataset":1}}
{"text":"Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection.","cats":{"new-dataset":1}}
{"text":"To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicly available to the community.","cats":{"new-dataset":1}}
{"text":"This collection includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related datasets and conversational datasets derived from ChatGPT/GPT-4.","cats":{"new-dataset":1}}
{"text":"FLACUNA is publicly available at https://huggingface.co/declare-lab/flacuna-13b-v1.0.","cats":{"new-dataset":1}}
{"text":"First, we publish a new dataset, EHRSHOT, containing de-identified structured data from the electronic health records (EHRs) of 6,712 patients from Stanford Medicine.","cats":{"new-dataset":1}}
{"text":"In this paper, we define a unified setting termed as open-set semantic segmentation (O3S), which aims to learn seen and unseen semantics from both visual examples and textual names.","cats":{"new-dataset":0}}
{"text":"Experimental results, carried out on three sets of the Shape COSEG Dataset, on the human segmentation dataset proposed in Maron et al., 2017 and on the ShapeNet benchmark, show how the proposed approach yields state-of-the-art performance on semantic segmentation of 3D meshes.","cats":{"new-dataset":0}}
{"text":"Our code and data are available at https://github.com/sergiotasconmorales/locvqa.","cats":{"new-dataset":1}}
{"text":"The dataset comprises high-resolution RGB-D images and pixel-level annotations of the fruits.","cats":{"new-dataset":1}}
{"text":"TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/ .","cats":{"new-dataset":1}}
{"text":"Our code, models, instruction-sets and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.","cats":{"new-dataset":1}}
{"text":"We extensively evaluate SeaLog on two public datasets and an industrial dataset.","cats":{"new-dataset":0}}
{"text":"Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of handwritten prompts and synthetic prompts generated using OpenAI, and (2) real abstracts, which contains abstract and title pairs.","cats":{"new-dataset":1}}
{"text":"Videos are available at: https://kristery.github.io/edt/","cats":{"new-dataset":0,"ml-games":0}}
{"text":"This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by utilizing Inertial Measurement Units (IMUs) and leveraging the diversity present in the Indian writing style.","cats":{"new-dataset":0}}
{"text":"Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.","cats":{"new-dataset":0,"ml-games":0}}
{"text":"In this paper, we study linear regression applied to data structured on a manifold.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression.","cats":{"new-dataset":0,"data-quality":0}}
{"text":"This research can be extended and contributes to the field of pattern recognition and offers valuable insights for developing improved systems for handwriting recognition, particularly in diverse linguistic and cultural contexts.","cats":{"new-dataset":0}}
{"text":"To prove these theorems, we revisit William Thurston's results on the calisson tilability of a region $R$.","cats":{"new-dataset":0}}
{"text":"Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on early releases of ChatGPT that elicit undesired behavior.","cats":{"new-dataset":0,"data-quality":0,"ml-games":0}}
{"text":"We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks.","cats":{"new-dataset":0}}
{"text":"Given a triangular grid in a hexagon and some given edges of the grid, the problem is to find a calisson tiling such that no input edge is overlapped and calissons adjacent to an input edge have different orientations.","cats":{"new-dataset":0}}
{"text":"Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.","cats":{"new-dataset":0,"data-quality":0,"ml-games":0}}
{"text":"We extend the puzzle to regions $R$ that are not necessarily hexagonal.","cats":{"new-dataset":0}}
{"text":"By using a natural language generation model to abductively infer a premise given another premise and a conclusion, we can impute missing pieces of evidence needed for the conclusion to be true.","cats":{"new-dataset":0}}
{"text":"We sample multiple possible outputs for each step to achieve coverage of the search space, at the same time ensuring correctness by filtering low-quality generations with a round-trip validation procedure.","cats":{"new-dataset":0}}
{"text":"Results on a modified version of the EntailmentBank dataset and a new dataset called Everyday Norms: Why Not? show that abductive generation with validation can recover premises across in- and out-of-domain settings","cats":{"new-dataset":0}}
{"text":"To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on training such models.","cats":{"new-dataset":0}}
{"text":"We implement over 20 variants with controlled settings.","cats":{"new-dataset":0}}
{"text":"For training data, we investigate the impact of data and sampling strategies.","cats":{"new-dataset":0}}
{"text":"For benchmarks, we contribute the first, to our best knowledge, comprehensive evaluation set including both image and video tasks through crowd-sourcing.","cats":{"new-dataset":1}}
{"text":"Here, remote sensing can provide reliable estimates of plastic pollution by regularly monitoring and detecting marine debris in coastal areas.","cats":{"new-dataset":0}}
{"text":"Medium-resolution satellite data of coastal areas is readily available and can be leveraged to detect aggregations of marine debris containing plastic litter.","cats":{"new-dataset":0}}
{"text":"In this work, we present a detector for marine debris built on a deep segmentation model that outputs a probability for marine debris at the pixel level.","cats":{"new-dataset":0,"ml-security":0}}
{"text":"We train this detector with a combination of annotated datasets of marine debris and evaluate it on specifically selected test sites where it is highly probable that plastic pollution is present in the detected marine debris.","cats":{"new-dataset":0}}
{"text":"We demonstrate quantitatively and qualitatively that a deep learning model trained on this dataset issued from multiple sources outperforms existing detection models trained on previous datasets by a large margin.","cats":{"new-dataset":1}}
{"text":"We hope to accelerate advances in the large-scale automated detection of marine debris, which is a step towards quantifying and monitoring marine litter with remote sensing at global scales, and release the model weights and training source code under https://github.com/marccoru/marinedebrisdetector","cats":{"new-dataset":0}}
{"text":"This paper proposes a framework called <projektor>, which predicts model performance and supports data selection decisions based on partial samples of prospective data sources.","cats":{"new-dataset":0}}
{"text":"In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture ratio within the range of disclosed data sizes.","cats":{"new-dataset":0}}
{"text":"To facilitate the development of comprehensive scene understanding solutions using multiple camera views, a new dataset called Road Genome (OpenLane-V2) has been released.","cats":{"new-dataset":1}}
{"text":"We achieve new state-of-the-art performance on the large-scale Waymo Open Dataset.","cats":{"new-dataset":0}}
{"text":"Finally, we train a detector that generalizes to a wide range of part segmentation datasets while achieving better performance than dataset-specific training.","cats":{"new-dataset":0}}
{"text":"Finally, we release a large-scale synthetic dataset with 1.4M examples generated using TrueTeacher.","cats":{"new-dataset":1}}
{"text":"To fully unlock model capabilities for high-quality video generation, we curate a large-scale video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters.","cats":{"new-dataset":1}}
{"text":"Code and datasets are available in https://github.com/zjunlp/DeepKE/tree/main/example/llm.","cats":{"new-dataset":1}}
{"text":"We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared to existing hashtags and off-the-shelf taggers.","cats":{"new-dataset":0}}
{"text":"Additionally, to track new and creative applications for bioinformatics tools such as ChatGPT, we have established a GitHub repository at https://github.com/csbl-br/awesome-compbio-chatgpt.","cats":{"new-dataset":0}}
{"text":"All data and trained models are publicly available.","cats":{"new-dataset":1}}
{"text":"We have conducted extensive experiments on 16 public log datasets.","cats":{"new-dataset":0}}
{"text":"We also release the code and the annotated dataset for replication and future research.","cats":{"new-dataset":1}}
{"text":"The training data for these models is usually collected from open-source repositories (e.g., GitHub) that contain software faults and security vulnerabilities.","cats":{"new-dataset":0}}
{"text":"CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.","cats":{"new-dataset":1}}
{"text":"Project page: https://europe.naverlabs.com/imagenet-sd/","cats":{"new-dataset":0}}
{"text":"GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca","cats":{"new-dataset":0}}
{"text":"The resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction \\textbf{G}eneralist (\\textbf{COIG}) corpora are available in Huggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and Github\\footnote{\\url{https://github.com/FlagOpen/FlagInstruct}}, and will be continuously updated.","cats":{"new-dataset":1}}
{"text":"We make our model, data, as well as code publicly available.","cats":{"new-dataset":1}}
{"text":"The data, code, and all model outputs are released in https://github.com/microsoft/AGIEval.","cats":{"new-dataset":1}}
{"text":"We created a comprehensive dataset including 492.5K samples comprising code-related content produced by ChatGPT, encompassing popular software activities like Q&A (115K), code summarization (126K), and code generation (226.5K).","cats":{"new-dataset":1}}
{"text":"The code is publicly available at https://github.com/Vision-CAIR/ChatCaptioner","cats":{"new-dataset":1}}
{"text":"Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor.","cats":{"new-dataset":1}}
{"text":"Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.","cats":{"new-dataset":1}}
{"text":"Our source code and datasets are available at https://github.com/xinleihe/MGTBench.","cats":{"new-dataset":1}}
{"text":"The training data, codes, and weights of this project are available at: The training data, codes, and weights of this project are available at: https://github.com/Kent0n-Li/ChatDoctor.","cats":{"new-dataset":0}}
{"text":"Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.","cats":{"new-dataset":0}}
{"text":"The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.","cats":{"new-dataset":1}}
{"text":"To support further research in related fields, we have made the data generated by ChatGPT publicly available at https://github.com/THU-BPM/chatgpt-sql.","cats":{"new-dataset":1}}
{"text":"In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws.","cats":{"new-dataset":0}}
{"text":"Also, <projektor> outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shelf solutions.","cats":{"new-dataset":0}}
{"text":"Database alignment is a variant of the graph alignment problem: Given a pair of anonymized databases containing separate yet correlated features for a set of users, the problem is to identify the correspondence between the features and align the anonymized user sets based on correlation alone.","cats":{"new-dataset":0}}
{"text":"We study an instance of the database alignment problem with multivariate Gaussian features and derive results that apply both for database alignment and for planted matching, demonstrating the connection between them.","cats":{"new-dataset":0}}
{"text":"The maximum likelihood algorithms for both planted matching and database alignment take the form of a linear program and we study relaxations to better understand the significance of various constraints under various conditions and present achievability and converse bounds.","cats":{"new-dataset":0}}
{"text":"Our analysis and results extend to the unbalanced case where one user set is not fully covered by the alignment.","cats":{"new-dataset":0}}
{"text":"They only work for in-distribution artifact types generated during training.","cats":{"new-dataset":0}}
{"text":"In this paper, we analyze the cause and characteristics of the GAN artifacts produced in unseen test data without ground-truths.","cats":{"new-dataset":0}}
{"text":"We then develop a novel method, namely, DeSRA, to Detect and then Delete those SR Artifacts in practice.","cats":{"new-dataset":0}}
{"text":"After detecting the artifact regions, we develop a finetune procedure to improve GAN-based SR models with a few samples, so that they can deal with similar types of artifacts in more unseen real data.","cats":{"new-dataset":0}}
{"text":"The code will be available at https://github.com/TencentARC/DeSRA.","cats":{"new-dataset":0}}
{"text":"In this work, we review robustness issues of DL and particularly bridge concerns and attempts from approximation theory to statistical learning theory.","cats":{"new-dataset":0}}
{"text":"Further, we review Bayesian Deep Learning as a means for uncertainty quantification and rigorous explainability.","cats":{"new-dataset":0}}
{"text":"A new control paradigm using angular momentum and foot placement as state variables in the linear inverted pendulum model has expanded the realm of possibilities for the control of bipedal robots.","cats":{"new-dataset":0}}
{"text":"This new paradigm, known as the ALIP model, has shown effectiveness in cases where a robot's center of mass height can be assumed to be constant or near constant as well as in cases where there are no non-kinematic restrictions on foot placement.","cats":{"new-dataset":0}}
{"text":"Our code is available at https://github.com/amazon-science/codetaskcl-pptf","cats":{"new-dataset":0}}
{"text":"We also show that it is possible to generate fully-synthetic image-annotation pairs to automatically augment any annotated dataset.","cats":{"new-dataset":0}}
{"text":"We present and release a new dataset of 50 manually annotated research articles.","cats":{"new-dataset":1}}
{"text":"The code and trained weights are available at https://github.com/mordecaimalignatius/GAFAR/.","cats":{"new-dataset":0}}
{"text":"We build on existing tools to computationally analyze data retrieved from publicly available databases.","cats":{"new-dataset":0}}
{"text":"Among multiple benchmarks on the KITTI dataset, we achieve new state-of-the-art performance.","cats":{"new-dataset":0}}
{"text":"Codes and models are publicly available at https://github.com/sunlicai/MAE-DFER.","cats":{"new-dataset":0}}
{"text":"We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program including location and function name, which makes the dataset ideal to train LLMs and machine learning algorithms.","cats":{"new-dataset":1}}
{"text":"Our implementation will be publicly available at \\url{https://github.com/ETHRuiGong/PTDiffSeg}.","cats":{"new-dataset":0}}
{"text":"Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines.","cats":{"new-dataset":1}}
{"text":"The classification accuracy of the models in the absence and presence of the two attacks are computed on images from the publicly accessible ImageNet dataset.","cats":{"new-dataset":0}}
{"text":"Furthermore, it facilitates the creation of de-identified datasets for broader 2D image research at major research institutions.","cats":{"new-dataset":0}}
{"text":"State-of-the-art results are achieved even on more detailed part-segmentation, Pascal-Animals, by only training on coarse-grained datasets.","cats":{"new-dataset":0}}
{"text":"Our code will be available at the URL: https://github.com/cofly2014/tsa-mlt.git","cats":{"new-dataset":0}}
{"text":"KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was designed, in addition to a larger dataset.","cats":{"new-dataset":1}}
{"text":"Additionally, we introduce Tomatopia, a new, large and challenging dataset of greenhouse tomatoes.","cats":{"new-dataset":1}}
{"text":"TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks.","cats":{"new-dataset":0}}
{"text":"The code for this algorithm will be publicly available.","cats":{"new-dataset":0}}
{"text":"The code and models will be made publicly at \\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.","cats":{"new-dataset":0}}
{"text":"In the fields of Experimental and Computational Aesthetics, numerous image datasets have been created over the last two decades.","cats":{"new-dataset":0}}
{"text":"Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.","cats":{"new-dataset":0}}
{"text":"We share this visualization and the dataset in the spirit of open science.","cats":{"new-dataset":1}}
{"text":"covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real abstract datasets.","cats":{"new-dataset":0}}
{"text":"Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT and outperforms covLLM trained primarily using the Alpaca dataset.","cats":{"new-dataset":0}}
{"text":"Using the MIMIC-IT dataset, we train a large VLM named Otter.","cats":{"new-dataset":0}}
{"text":"The code will be made available.","cats":{"new-dataset":0}}
{"text":"Project page: https://ba2det.site .","cats":{"new-dataset":0}}
{"text":"This dataset allows for the exploration of complex road connections and situations where lane markings may be absent.","cats":{"new-dataset":1}}
{"text":"Specifically, we benchmark the recognition drop on common detection datasets, where we evaluate both traditional and realistic anonymization for faces and full bodies.","cats":{"new-dataset":0}}
{"text":"With this CNN, we derived homogeneous atmospheric parameters and abundances for 841300 stars, that remarkably compared to external data-sets.","cats":{"new-dataset":0}}
{"text":"The final trained model is publicly available at https://github.com/Jesper-Karsten/MBASC","cats":{"new-dataset":0}}
{"text":"All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.","cats":{"new-dataset":0}}
{"text":"In this paper, we propose a detector with the ability to predict both open-vocabulary objects and their part segmentation.","cats":{"new-dataset":0}}
{"text":"First, we train the detector on the joint of part-level, object-level and image-level data to build the multi-granularity alignment between language and image.","cats":{"new-dataset":0}}
{"text":"In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models.","cats":{"new-dataset":0}}
{"text":"Current methods rely on datasets with expensive annotations; multi-view images and their camera parameters.","cats":{"new-dataset":0}}
{"text":"The WHOW-KG consists of a network of five ontologies and related linked open data, modelled according to those ontologies.","cats":{"new-dataset":0}}
{"text":"We propose Multi-CrossRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and covering six text domains.","cats":{"new-dataset":1}}
{"text":"We run a baseline model over the 26 new datasets and--as sanity check--over the 26 back-translations to English.","cats":{"new-dataset":1}}
{"text":"We also evaluate performance on the MultiMedQA suite of benchmark datasets.","cats":{"new-dataset":0}}
{"text":"Our model and code are available at https://github.com/microsoft/LMOps.","cats":{"new-dataset":0}}
{"text":"To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories.","cats":{"new-dataset":1}}
{"text":"We also release codebase for evaluation set extraction.","cats":{"new-dataset":0}}
{"text":"Dataset, to fight the bias prevalent in giant datasets.","cats":{"new-dataset":0}}
{"text":"We will make our code and pre-trained models publicly available.","cats":{"new-dataset":0}}
{"text":"We perform an extensive study across six datasets with eight models from three model families.","cats":{"new-dataset":0}}
{"text":"For this, we augment standard bug-fixing datasets with bug report discussions.","cats":{"new-dataset":0}}
{"text":"Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond.","cats":{"new-dataset":0}}
{"text":"We release our code and data under fully permissive licenses.","cats":{"new-dataset":1}}
{"text":"For enabling the combination of ChatGPT and RDF KGs, we present a research prototype, called GPToLODS, which is able to enrich any ChatGPT response with more information from hundreds of RDF KGs.","cats":{"new-dataset":0}}
{"text":"In particular, it identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG (which contains integrated data from 400 RDF KGs and over 412 million entities).","cats":{"new-dataset":0}}
{"text":"Numerous AIGC detectors have been developed and evaluated on natural language data.","cats":{"new-dataset":0}}
{"text":"We evaluated six AIGC detectors, including three commercial and three open-source solutions, assessing their performance on this dataset.","cats":{"new-dataset":0}}
{"text":"We have released a dataset comprised of ChatGPT's responses to support further research in this area.","cats":{"new-dataset":1}}
{"text":"Both datasets involve scraping through known data sources (through platforms like stack overflow, crowdsourcing, etc.)","cats":{"new-dataset":0}}
{"text":"We call the collected dataset the Human ChatGPT Comparison Corpus (HC3).","cats":{"new-dataset":1}}
{"text":"The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.","cats":{"new-dataset":1}}
{"text":"For datasets originally without shared tokens in label names, we also offer an automated method, using OpenAI's ChatGPT, to generate shared actions and objects.","cats":{"new-dataset":0}}
{"text":"We make our code, models, and datasets publicly available.","cats":{"new-dataset":1}}
{"text":"The source code will be released publicly.","cats":{"new-dataset":0}}
{"text":"We evaluate PaTeCon on two large-scale datasets based on Wikidata and Freebase respectively.","cats":{"new-dataset":0}}
{"text":"We present a novel synthetic dataset, named Parcel3D, that is based on the Google Scanned Objects (GSO) dataset and consists of more than 13,000 images of parcels with full 3D annotations.","cats":{"new-dataset":1}}
{"text":"The dataset contains intact, i.e. cuboid-shaped, parcels and damaged parcels, which were generated in simulations.","cats":{"new-dataset":1}}
{"text":"Dataset and code are available at https://a-nau.github.io/parcel3d.","cats":{"new-dataset":1}}
{"text":"Datasets, code and results are made publicly available and will be continuously updated at https://github.com/ZhaomingKong/Denoising-Comparison.","cats":{"new-dataset":1}}
{"text":"Specifically, we collect a dataset of e-textbooks from one of the largest free online publishers in the world.","cats":{"new-dataset":1}}
{"text":"The dataset and data analysis scripts are available on our open-source repository.","cats":{"new-dataset":1}}
{"text":"To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset.","cats":{"new-dataset":1}}
{"text":"Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks.","cats":{"new-dataset":1}}
{"text":"The source code and the proposed dataset are publicly available at https://github.com/fh2019ustc/DocTr-Plus.","cats":{"new-dataset":1}}
{"text":"Hence, we present a dataset that exclusively consists of healthy and diseased cashew leaves and fruits.","cats":{"new-dataset":1}}
{"text":"The entire code and models will be open-sourced.","cats":{"new-dataset":0}}
{"text":"With nationwide coverage, real-world network topology, and rich geospatial features, this data repository can be used for a variety of traffic-related tasks.","cats":{"new-dataset":0}}
{"text":"The data and code are available on GitHub (https://github.com/baixianghuang/travel).","cats":{"new-dataset":1}}
{"text":"Code and models will be accessed at https://github.com/Liuxinyv/SAZS.","cats":{"new-dataset":0}}
{"text":"BenchMD combines 19 publicly available datasets for 7 medical modalities, including 1D sensor data, 2D images, and 3D volumetric scans.","cats":{"new-dataset":0}}
{"text":"We introduce the LongForm dataset, which is created by leveraging English corpus examples with augmented instructions.","cats":{"new-dataset":1}}
{"text":"We publicly release our data and models: https://github.com/akoksal/LongForm.","cats":{"new-dataset":1}}
{"text":"Code is generated using a commercial tool, SonarCloud.","cats":{"new-dataset":0}}
{"text":"Our source code will be available at https://github.com/MC-E/DragonDiffusion.","cats":{"new-dataset":0}}
{"text":"An open-source implementation is available online.","cats":{"new-dataset":0}}
{"text":"Deep-learning-based object detection and semantic segmentation models have proven to be suitable for this purpose.","cats":{"new-dataset":0}}
{"text":"We evaluate the effectiveness of this approach by training a semantic segmentation model on a real dataset augmented in two ways: 1) using synthetic images obtained from real masks, and 2) generating images from synthetic semantic masks.","cats":{"new-dataset":0}}
{"text":"The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for AZ.","cats":{"new-dataset":0}}
{"text":"The code is available at \\url{https://github.com/cjw2021/SOV-STG}.","cats":{"new-dataset":0}}
{"text":"The code and pretrained models will be released under https://github.com/lik1996/iCMFormer.","cats":{"new-dataset":0}}
{"text":"The data samples are automatically generated from a curated set of reasoning patterns, where the patterns are annotated with inference labels by experts.","cats":{"new-dataset":1}}
{"text":"The data is obtained from a fleet of gas sensors that measure and track quantities such as oxygen and sound.","cats":{"new-dataset":1}}
{"text":"With the help of this data, we can detect events such as occupancy in a specific environment.","cats":{"new-dataset":0}}
{"text":"Extensive experiments and visualizations on four datasets demonstrate the powerful performance of our method.","cats":{"new-dataset":0}}
{"text":"Codes will be available.","cats":{"new-dataset":0}}
{"text":"Extensive evaluation on three benchmark datasets using multiple evaluation metrics show the effectiveness of the proposed framework.","cats":{"new-dataset":0}}
{"text":"Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name.","cats":{"new-dataset":0}}
{"text":"This property of the dataset makes it suitable for evaluating the effectiveness of various static and dynamic analysis tools.","cats":{"new-dataset":0}}
{"text":"While prior research demonstrated the high performance of ChatGPT across numerous NLP tasks, open-source LLMs like HugginChat and FLAN are gaining attention for their cost-effectiveness, transparency, reproducibility, and superior data protection.","cats":{"new-dataset":0}}
{"text":"With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach.","cats":{"new-dataset":0}}
{"text":"In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-date model repositories for model querying, legal compliance analysis between different model licenses, and copyright issues and intellectual property protection in model reusing.","cats":{"new-dataset":0}}
{"text":"However, little is known about how much training data they require, and how this number depends on the data structure.","cats":{"new-dataset":0}}
{"text":"Our code is available at https://github.com/siyi-wind/MDViT.","cats":{"new-dataset":0}}
{"text":"Along with the datasets, we also propose corresponding baseline solutions to the three aforementioned tasks.","cats":{"new-dataset":1}}
{"text":"These attacks are launched on three powerful pre-trained image classifier architectures, ResNet-34, GoogleNet, and DenseNet-161.","cats":{"new-dataset":0}}
{"text":"This dataset comprises a large number of tasks that demand problem-solving skills.","cats":{"new-dataset":1}}
{"text":"The implementation of CAME is publicly available.","cats":{"new-dataset":0}}
{"text":"However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions.","cats":{"new-dataset":0}}
{"text":"Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios.","cats":{"new-dataset":1}}
{"text":"In this paper, we introduce ScalOTA, an end-to-end scalable OTA software update architecture and secure protocol for modern vehicles.","cats":{"new-dataset":0}}
{"text":"Our code is available at https://github.com/yuping-wu/PULSAR.","cats":{"new-dataset":0}}
{"text":"On both \\pascal and \\coco datasets, we conduct extensive experiments to evaluate the framework effectiveness.","cats":{"new-dataset":0}}
{"text":"Our source code and the related paper list are available on https://github.com/SLDGroup/survey-zero-shot-nas.","cats":{"new-dataset":0}}
{"text":"Extensive experiments show our method achieves state-of-the-art results on the HMDB51 and UCF101 datasets and a competitive result on the benchmark of Kinetics and something-2-something V2 datasets.","cats":{"new-dataset":0}}
{"text":"Experimental results on real-world datasets demonstrate that our method achieves better performance compared with several state-of-the-art social bot detection methods.","cats":{"new-dataset":0}}
{"text":"However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space.","cats":{"new-dataset":0}}
{"text":"In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space.","cats":{"new-dataset":0}}
{"text":"More video results can be found at our project page: https://liuyebin.com/proxycap.","cats":{"new-dataset":0}}
{"text":"Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization.","cats":{"prompt-eng":1}}
{"text":"The specific assignment prompted students to define and explain their career goals as engineers.","cats":{"prompt-eng":0}}
{"text":"Prompt engineering typically requires hand-crafting a set of prompts for individual downstream tasks.","cats":{"prompt-eng":1}}
{"text":"It turns out that the key challenge lies in designing the most effective prompt for the LLM, a task called prompt engineering.","cats":{"prompt-eng":1}}
{"text":"Specifically, we add a set of ``task prompts'', each corresponding to a different task, and let each prompt predict task-related annotations.","cats":{"prompt-eng":1}}
{"text":"To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output.","cats":{"prompt-eng":1}}
{"text":"The context of our task leverages a generative model as an IR engine to evaluate the prompts' performance on image retrieval tasks.","cats":{"prompt-eng":1}}
{"text":"Specifically, we first perform supervised fine-tuning with a pretrained language model on a small collection of manually engineered prompts.","cats":{"prompt-eng":1}}
{"text":"We first devise a learnable universal prompt to describe the correlations among all tasks and then convert this prompt and image features into a task-specific prompt, which is fed to the decoder as a part of its input.","cats":{"prompt-eng":1}}
{"text":"Our studies offer a deeper understanding of prompt engineering thereby opening up avenues for research on the future of prompt engineering.","cats":{"prompt-eng":1}}
{"text":"Designing suitable prompts for specific visual tasks has emerged as a meaningful research direction.","cats":{"prompt-eng":0}}
{"text":"We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts.","cats":{"prompt-eng":1}}
{"text":"We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction.","cats":{"prompt-eng":1}}
{"text":"To be specific, we design a set of prompts to fine-tune the pre-trained image captioner.","cats":{"prompt-eng":1}}
{"text":"However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process.","cats":{"prompt-eng":0}}
{"text":"Therefore, no further task-specific reward design is needed.","cats":{"prompt-eng":0}}
{"text":"In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP).","cats":{"prompt-eng":1}}
{"text":"However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted.","cats":{"prompt-eng":0,"data-quality":0,"ml-games":0}}
{"text":"LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization.","cats":{"prompt-eng":0}}
{"text":"In the case where the region is the entire infinite triangular grid, we prove that the existence of a solution can be solved with an algorithm of complexity $O(|X|^3)$ where $X$ is the set of input edges.","cats":{"prompt-eng":0}}
{"text":"However, even manually labeled datasets contain errors, not to mention automatically labeled ones.","cats":{"data-quality":1}}
{"text":"Label error is a ubiquitous problem in annotated data.","cats":{"data-quality":1}}
{"text":"After demonstrating that our methodology empirically outperforms other algorithms for label error detection, we apply our approach to discover many label errors in the CelebA image tagging dataset.","cats":{"data-quality":1}}
{"text":"These properties highlight a tradeoff between classification error probability and error-correction capabilities of label encodings.","cats":{"data-quality":0}}
{"text":"In this work, we for the first time introduce a benchmark for label error detection methods on object detection datasets as well as a label error detection method and a number of baselines.","cats":{"data-quality":1}}
{"text":"Label encodings found by RLEL result in lower or comparable errors to manually designed label encodings.","cats":{"data-quality":1}}
{"text":"We also propose an improved self-labeling loss; it is robust to pseudo-labeling errors and enforces stronger fairness.","cats":{"data-quality":1}}
{"text":"Inferencing unlabeled data from labeled data is an error-prone process.","cats":{"data-quality":1}}
{"text":"However, creating such large keypoint labels is time-consuming and costly, and is often error-prone due to inconsistent labeling.","cats":{"data-quality":0}}
{"text":"The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at detecting the latter.","cats":{"data-quality":1}}
{"text":"PseudoAugments outperforms pseudo labeling by mitigating pseudo labeling errors and generating diverse fused training scenes.","cats":{"data-quality":1}}
{"text":"Our model is also able to maintain high classification accuracy with very few labels, with only 7.79% error when only using 145 labels.","cats":{"data-quality":0}}
{"text":"Detecting errors in KGs is challenging since the patterns of errors are unknown and diverse, while ground-truth labels are rare or even unavailable.","cats":{"data-quality":1}}
{"text":"We analyze the factors affecting this approximation error and design a pseudo-label clustering generation method to reduce the approximation error.","cats":{"data-quality":1}}
{"text":"To ameliorate the impact of label errors, we equipped our method with a novel negative label sampling strategy to strengthen the model robustness.","cats":{"data-quality":1}}
{"text":"We propose an extension of the Confident Learning framework to this setting, as well as a label quality score that ranks examples with label errors much higher than those which are correctly labeled.","cats":{"data-quality":1}}
{"text":"The later case can generate dense flow labels but the interpolated events are prone to errors.","cats":{"data-quality":0}}
{"text":"Improper fingerprint localization and finger labeling errors lead to poor matching performance.","cats":{"data-quality":0}}
{"text":"Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.","cats":{"data-quality":1}}
{"text":"We derive an upper bound for the generalization error that is linear in the clients' label noise level.","cats":{"data-quality":1}}
{"text":"For example, for the IMDB text data with known labeling errors, a 14% boost is shown.","cats":{"data-quality":1}}
{"text":"Large amounts of label error substantially degrades the quality of deep learning models.","cats":{"data-quality":1}}
{"text":"We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detection datasets.","cats":{"data-quality":1}}
{"text":"We prove that semi-supervised labels improve the downstream error bound whereas noisy labels have limited effects under such a paradigm.","cats":{"data-quality":1}}
{"text":"This paper provides an exact characterization of the expected generalization error (gen-error) for semi-supervised learning (SSL) with pseudo-labeling via the Gibbs algorithm.","cats":{"data-quality":0}}
{"text":"However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers.","cats":{"data-quality":1}}
{"text":"Most existing methods utilize the off-the-shelf pose or parsing networks as pseudo labels, which are prone to error.","cats":{"data-quality":0}}
{"text":"The result is an SSL classification framework explicitly designed to overcome inevitable pseudo-label errors.","cats":{"data-quality":1}}
{"text":"Here we consider the task of finding sentences that contain label errors in token classification datasets.","cats":{"data-quality":1}}
{"text":"Scaling sequence length has become a critical demand in the era of large language models.","cats":{"data-quality":0,"ml-games":0}}
{"text":"In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences.","cats":{"data-quality":0,"ml-games":0}}
{"text":"Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows.","cats":{"data-quality":0,"ml-games":0}}
{"text":"Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks.","cats":{"data-quality":0,"ml-games":0}}
{"text":"Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains.","cats":{"data-quality":0}}
{"text":"However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents.","cats":{"data-quality":0}}
{"text":"In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments.","cats":{"data-quality":0}}
{"text":"Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently.","cats":{"data-quality":0}}
{"text":"We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting.","cats":{"data-quality":0,"ml-games":0}}
{"text":"We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans.","cats":{"data-quality":0}}
{"text":"For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data.","cats":{"data-quality":0}}
{"text":"With many possible classes to consider, data annotators are likely to make errors when labeling such data in practice.","cats":{"data-quality":1}}
{"text":"However, it usually suffers from a lack of high-quality datasets due to high annotation cost, inter-observer variability, human annotator error, and errors in computer-generated labels.","cats":{"data-quality":0}}
{"text":"For such bone structure analyses, deep learning technologies are promising but require high-quality labeled data for the learning, while the data labeling is costly.","cats":{"data-quality":0}}
{"text":"However, agreement between annotators is often low, leading to inconsistent labels that hinder the reliability of models.","cats":{"data-quality":1}}
{"text":"Our experiments show that this approach consistently improves inter-annotator agreement and annotation accuracy.","cats":{"data-quality":1}}
{"text":"We advocate for the use of IAA in predicting the labeling quality of individual annotators, leading to cost and time efficiency in data production.","cats":{"data-quality":1}}
{"text":"This paper presents a novel approach of leveraging Inter-Annotator Agreement (IAA), traditionally used for assessing labeling consistency, to optimize Data Management Operations (DMOps).","cats":{"data-quality":1}}
{"text":"Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the capabilities of a deep learning classifier trained with the data respectively.","cats":{"data-quality":1}}
{"text":"However, such annotations may fail in practice because of the change in annotation requirements, application scenarios, and modeling goals, where label validation and relabeling by domain experts are required.","cats":{"data-quality":1}}
{"text":"However, selecting training samples based on the degree of agreement between annotators introduces a bias in the training data and does not improve the results.","cats":{"data-quality":1}}
{"text":"However, these annotations are inherently subjective and some of the instances are hard to classify, resulting in noisy annotations due to error or lack of agreement.","cats":{"data-quality":1}}
{"text":"We propose and evaluate an additional application of our method leading to the detection of annotation errors.","cats":{"data-quality":1}}
{"text":"However, arbitrating the final annotation is not always effective because new biases might be produced during the process, especially when there are significant variations among annotations.","cats":{"data-quality":1}}
{"text":"A two-step human annotation and inter-annotator agreement study guarantee the high quality of the PcMSP corpus.","cats":{"data-quality":0}}
{"text":"We observe a striking correlation between the model's and humans' annotation: Categories with consistent human annotations (>$0.9$ inter-rater reliability, IRR) also display higher human-model agreement (>$0.7$), while categories with less consistent human annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower human-model agreement ($0.3$-$0.5$).","cats":{"data-quality":1}}
{"text":"We propose two metrics to audit the noise of annotations.","cats":{"data-quality":1}}
{"text":"We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labeled data in this domain.","cats":{"data-quality":1}}
{"text":"Whereas such annotation is costly and hard to scale, significantly holding back the development of the research.","cats":{"data-quality":0}}
{"text":"A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label.","cats":{"data-quality":1}}
{"text":"We hypothesize two failure modes of safety training: competing objectives and mismatched generalization.","cats":{"data-quality":0,"ml-games":0}}
{"text":"Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist.","cats":{"data-quality":0}}
{"text":"We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models.","cats":{"data-quality":0}}
{"text":"Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution.","cats":{"data-quality":0}}
{"text":"Our findings suggest that the corresponding linear regression does not have a unique solution when the embedded submanifold is flat in some dimensions.","cats":{"data-quality":0}}
{"text":"Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.","cats":{"data-quality":0}}
{"text":"To disentangle these effects, we propose an evaluation framework based on \"counterfactual\" task variants that deviate from the default assumptions underlying standard tasks.","cats":{"data-quality":0}}
{"text":"Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions.","cats":{"data-quality":0}}
{"text":"We detail corpus statistics and demonstrate high inter-annotator agreement.","cats":{"data-quality":0}}
{"text":"We also propose an accurate pseudo label generation method through prototype learning.","cats":{"data-quality":0}}
{"text":"Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities, representing distributions over classes in a classification setting, subject to a hyper-parameter encoding annotator reliability.","cats":{"data-quality":1}}
{"text":"Based on this model, we propose a metric for measuring annotation uncertainty and provide uncertainty-adjusted metrics for performance evaluation.","cats":{"data-quality":1}}
{"text":"Identifying the samples with corrupted labels and preventing the model from learning them is a promising approach to address this challenge.","cats":{"data-quality":1}}
{"text":"Furthermore, we detect real label errors a) on commonly used test datasets in object detection and b) on a proprietary dataset.","cats":{"data-quality":1}}
{"text":"Large-scale datasets in the real world inevitably involve label noise.","cats":{"data-quality":0}}
{"text":"This is partially due to the fact that obtaining a balanced, diverse, and perfectly labeled dataset is typically expensive, time-consuming, and error-prone.","cats":{"data-quality":0}}
{"text":"We develop an efficient algorithm for detecting label errors and outlier data points based on the relational graph structure of the dataset.","cats":{"data-quality":1}}
{"text":"By focusing on finding incorrect labels in the original training datasets, we can eliminate erroneous examples in their root.","cats":{"data-quality":1}}
{"text":"Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project.","cats":{"data-quality":0}}
{"text":"Here we consider algorithms for finding mislabeled examples in multi-label classification datasets.","cats":{"data-quality":1}}
{"text":"Negative labels are those that a corresponding data item does not belong.","cats":{"data-quality":0}}
{"text":"This issue is due to biased labeling preferences at multiple clients and is a typical setting of data heterogeneity.","cats":{"data-quality":0}}
{"text":"However, noisy samples (i.e., with wrong labels) in the training set induce confusion and cause the network to learn the incorrect representation.","cats":{"data-quality":1}}
{"text":"Mislabeled examples are a common issue in real-world data, particularly for tasks like token classification where many labels must be chosen on a fine-grained basis.","cats":{"data-quality":1}}
{"text":"We also introduced robust loss to reduce the noise effects of inaccurate labels generated in semi-supervised learning.","cats":{"data-quality":1}}
{"text":"The main anomaly was found by the autoencoder and automatically created labels and was also recorded in the log files.","cats":{"data-quality":1}}
{"text":"About 0.2% of the images could not be assigned a label, while for 5.1% the reviewers were uncertain, or they assigned an invalid label.","cats":{"data-quality":1}}
{"text":"We find that the above issues are caused by the training dataset's pose imbalance.   ","cats":{"data-quality":0}}
{"text":"However, we identify issues with the dataset quality and evaluation metric.","cats":{"data-quality":1}}
{"text":"Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability.","cats":{"ml-games":1}}
{"text":"Minecraft is a great testbed for human creativity that has inspired the design of various structures and even functioning machines, including flying machines.","cats":{"ml-games":1}}
{"text":"Empirically, our method achieves the best performance on almost all maps compared to other popular algorithms in a set of StarCraft II micromanagement games.","cats":{"ml-games":1}}
{"text":"The BASALT challenge asks teams to compete to develop algorithms to solve tasks with hard-to-specify reward functions in Minecraft.","cats":{"ml-games":1}}
{"text":"However, many existing techniques rely on human-annotated level representations, which limits game level blending to a limited number of annotated games.","cats":{"ml-games":0}}
{"text":"Our experimental results on Predator-Prey and StarCraft Multiagent Challenge environments demonstrate the effectiveness of our method, indicating the better capabilities of handling environments with non-monotonic value functions.","cats":{"ml-games":1}}
{"text":"Our experiments mark the milestone of the first multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly doubles the overall performances.","cats":{"ml-games":1}}
{"text":"To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills.","cats":{"ml-games":1}}
{"text":"In experiments, our method accomplishes 24 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills.","cats":{"ml-games":1}}
{"text":"We present the Polycraft World AI Lab (PAL), a task simulator with an API based on the Minecraft mod Polycraft World.","cats":{"ml-games":1}}
{"text":"Our platform is built to allow AI agents with different architectures to easily interact with the Minecraft world, train and be evaluated in multiple tasks.","cats":{"ml-games":1}}
{"text":"In this paper, we study the problem of planning in Minecraft, a popular, democratized yet challenging open-ended environment for developing multi-task embodied agents.","cats":{"ml-games":1}}
{"text":"This represents a possible solution to limited annotated level data, and we demonstrate the potential for future versions to generalize to unseen games.","cats":{"ml-games":0}}
{"text":"Our experimental results on Predator-Prey and StarCraft Multi-Agent Challenge environments demonstrate the effectiveness of our method, having a better ability to replay important transitions and outperforming other state-of-the-art baselines.","cats":{"ml-games":1}}
{"text":"We study building a multi-task agent in Minecraft.","cats":{"ml-games":1}}
{"text":"To foster future research, we present newly collected, annotated and calibrated large-scale Tennis and Minecraft datasets.","cats":{"ml-games":1}}
{"text":"In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world.","cats":{"ml-games":1}}
{"text":"We study the problem of learning goal-conditioned policies in Minecraft, a popular, widely accessible yet challenging open-ended environment for developing human-level multi-task agents.","cats":{"ml-games":1}}
{"text":"Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenges in the competition.","cats":{"ml-games":1}}
{"text":"Game level blending via machine learning, the process of combining features of game levels to create unique and novel game levels using Procedural Content Generation via Machine Learning (PCGML) techniques, has gained increasing popularity in recent years.","cats":{"ml-games":1}}
{"text":"Mean-field games (MFGs) are limiting models to approximate $N$-player games, with a number of applications.","cats":{"ml-games":0}}
{"text":"We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates.","cats":{"ml-games":1}}
{"text":"The paper introduces two player connectivity games played on finite bipartite graphs.","cats":{"ml-games":0}}
{"text":"Finally, we provide a qualitative showcase (in Minecraft) illustrating the large and complex, but still coherent, structures that were generated using simple base generators.","cats":{"ml-games":1}}
{"text":"Experiments on the challenging StarCraft II benchmark demonstrate the effectiveness of MAG.","cats":{"ml-games":1}}
{"text":"Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories.","cats":{"ml-games":0}}
{"text":"The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT.","cats":{"ml-games":0}}
{"text":"Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to \"stitch\" with a more optimal trajectory.","cats":{"ml-games":0}}
{"text":"Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches.","cats":{"ml-games":0}}
{"text":"In particular, the EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games.","cats":{"ml-games":1}}
{"text":"Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created.","cats":{"ml-games":0}}
{"text":"Empirically, extensive experiments on various games demonstrate that PSD-PSRO is more effective in producing significantly less exploitable policies than state-of-the-art PSRO variants.","cats":{"ml-games":0}}
{"text":"Policy-Space Response Oracles (PSRO) is an influential algorithm framework for approximating a Nash Equilibrium (NE) in multi-agent non-transitive games.","cats":{"ml-games":0}}
{"text":"The platform support bidding with additional information and increase the feasible surplus for on-platform matches.","cats":{"ml-games":0}}
{"text":"We present the convergence property of PSD-PSRO.","cats":{"ml-games":0}}
{"text":"By incorporating our diversity regularization into the best response solving in PSRO, we obtain a new PSRO variant, Policy Space Diversity PSRO (PSD-PSRO).","cats":{"ml-games":0}}
{"text":"Procedural content generation (PCG) is a growing field, with numerous applications in the video game industry, and great potential to help create better games at a fraction of the cost of manual creation.","cats":{"ml-games":1}}
{"text":"Here, we explore these claims using the different contemporary PC variants proposed in the literature.","cats":{"ml-games":0}}
{"text":"Soc'90].   ","cats":{"ml-games":0}}
{"text":"The translation tail of our framework can convert gameplay video frames to an equivalent secondary representation, while its generation tail can produce novel level segments.","cats":{"ml-games":1}}
{"text":"Fourth, as programmable networks and machine learning (ML) techniques are increasingly becoming adopted by the community, their current applications in network security are discussed.","cats":{"ml-security":0}}
{"text":"Especially when both the ML model and the input data's confidentiality must be protected.","cats":{"ml-security":1}}
{"text":"We can expect such systems to be vulnerable to some adversarial-ML attacks.","cats":{"ml-security":1}}
{"text":"Traditional adversarial training is a popular methodology for robustifying ML models against attacks.","cats":{"ml-security":0}}
{"text":"The remarkable success of the use of machine learning-based solutions for network security problems has been impeded by the developed ML models' inability to maintain efficacy when used in different network environments exhibiting different network behaviors.","cats":{"ml-security":1}}
{"text":"Machine learning (ML) models can leak information about users, and differential privacy (DP) provides a rigorous way to bound that leakage under a given budget.","cats":{"ml-security":1}}
{"text":"However, compared to attacks on other ML-based systems, attackers face a level of indirection as they cannot interact directly with the learned model.","cats":{"ml-security":1}}
{"text":"However, due to increasing demand for users' privacy and security, we often need to remove users' data information from Machine Learning (ML) models to satisfy specific privacy and security requirements.","cats":{"ml-security":0}}
{"text":"We analyze the root causes of potentially-increased attack surface in learned systems and develop a framework for identifying vulnerabilities that stem from the use of ML.","cats":{"ml-security":1}}
{"text":"Since machine-learning is being deployed in safety-critical and security-sensitive domains, such attacks may have catastrophic security and safety consequences.","cats":{"ml-security":1}}
{"text":"Backdoor attacks have been demonstrated as a security threat for machine learning models.","cats":{"ml-security":1}}
{"text":"However, if the privacy of machine learning applications' customers cannot be guaranteed, it will cause security threats and losses to users' personal privacy information and service providers.","cats":{"ml-security":1}}
{"text":"In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity.","cats":{"ml-security":0}}
{"text":"Our model offers two key advantages: semantic-awareness and granularity-abundance.","cats":{"ml-security":0}}
{"text":"To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts.","cats":{"ml-security":0}}
{"text":"This allows our model to capture rich semantic information.","cats":{"ml-security":0}}
{"text":"For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks.","cats":{"ml-security":0}}
{"text":"Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets.","cats":{"ml-security":0}}
{"text":"Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance.","cats":{"ml-security":0}}
{"text":"Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements.","cats":{"ml-security":0}}
{"text":"We will provide code and a demo for further exploration and evaluation.","cats":{"ml-security":0}}
{"text":"We propose a self-supervised method for learning representations based on spatial audio-visual correspondences in egocentric videos.","cats":{"ml-security":0}}
{"text":"In particular, our method leverages a masked auto-encoding framework to synthesize masked binaural audio through the synergy of audio and vision, thereby learning useful spatial relationships between the two modalities.","cats":{"ml-security":0}}
{"text":"We use our pretrained features to tackle two downstream video tasks requiring spatial understanding in social scenarios: active speaker detection and spatial audio denoising.","cats":{"ml-security":0}}
{"text":"We show through extensive experiments that our features are generic enough to improve over multiple state-of-the-art baselines on two public challenging egocentric video datasets, EgoCom and EasyCom.","cats":{"ml-security":0}}
{"text":"Project: http://vision.cs.utexas.edu/projects/ego_av_corr.","cats":{"ml-security":0}}
{"text":"One of the fundamental steps toward understanding a complex system is identifying variation at the scale of the system's components that is most relevant to behavior on a macroscopic scale.","cats":{"ml-security":0}}
{"text":"Mutual information is a natural means of linking variation across scales of a system due to its independence of the particular functional relationship between variables.","cats":{"ml-security":0}}
{"text":"However, estimating mutual information given high-dimensional, continuous-valued data is notoriously difficult, and the desideratum -- to reveal important variation in a comprehensible manner -- is only readily achieved through exhaustive search.","cats":{"ml-security":0}}
{"text":"Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score.","cats":{"ml-security":0}}
{"text":"Code and pre-trained weights will be publicly available at https://animatediff.github.io/ .","cats":{"ml-security":0}}
{"text":"While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.","cats":{"ml-security":0}}
{"text":"In particular, we focus on the scalar curvature, which can be computed analytically for our manifold, and show connections to several settings that potentially imply generalization.","cats":{"ml-security":0}}
{"text":"To the best of our knowledge, such support does not exist.","cats":{"ml-security":0}}
{"text":"To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable.","cats":{"ml-security":0}}
{"text":"In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle.","cats":{"ml-security":0}}
{"text":"Then, a specific trustworthiness model and its attributes, namely data robustness, parameter sensitivity, and security covering adversarial examples, are introduced.","cats":{"ml-security":0}}
{"text":"Additionally, motivated by the concepts of user-level and item-level fairness, we broaden the understanding of diversity to encompass not only the item level but also the user level.","cats":{"ml-security":0}}
{"text":"In our experiments, eight machine learning models are employed to provide predictions, and the results achieved by the best-performing model are further processed by the SHAP explainability process.","cats":{"ml-security":0}}
{"text":"This is computationally more efficient because the expense of the one-time graph inference and the $d$-separation queries is negligible compared to the expense of surplus contribution evaluations.","cats":{"ml-security":0}}
{"text":"We use Meta's Ad Library to collect 602,546 ads that have been issued by US Congress members since mid-2018.","cats":{"ml-security":0}}
{"text":"We release the BigTrans model and hope it can advance the research progress.","cats":{"ml-security":0}}
{"text":"When the measurement delay exceeds the permissible number, the packet dropout happens.","cats":{"ml-security":0}}
{"text":"To testify the effectiveness and superiority of the proposed approach, we conduct extensive experiments on benchmark datasets.","cats":{"ml-security":0}}
{"text":"Experiments suggest FogROS2-SGC is 19x faster than rosbridge (a ROS2 package with comparable features, but lacking security).","cats":{"ml-security":0}}
{"text":"The LaSSIM metric does not require clean reference images and has been shown to be superior to SSIM in capturing image structural changes under image degradations, such as strong blurring on different datasets.","cats":{"ml-security":0}}
{"text":"It keeps the long-tailed nature of the collaborative graph by adding power law prior to node embedding initialization; then, it aggregates neighbors directly in multiple hyperbolic spaces through the gyromidpoint method to obtain more accurate computation results; finally, the gate fusion with prior is used to fuse multiple embeddings of one node from different hyperbolic space automatically.","cats":{"ml-security":0}}
{"text":"Empirically we demonstrate that $d$-SAGE enables the efficient and accurate estimation of SAGE values.","cats":{"ml-security":0}}
{"text":"The proposed method inserts pilot sequences in the zero bins of the ZP-OTFS system, resulting in low overhead and PAPR.","cats":{"ml-security":0}}
{"text":"Our work is a step towards understanding how simple geometrically-local error-correction strategies can protect information encoded into complex noisy systems, such as topological quantum error-correcting codes.","cats":{"ml-security":0}}
{"text":"In the case of two multivariate normal classes with a common covariance matrix, they showed that the error rate of the estimated Bayes' rule formed by this SSL approach can actually have lower error rate than the one that could be formed from a completely classified sample.","cats":{"ml-security":0}}
{"text":"However, previous studies seldom take advantage of such brain anatomy prior.","cats":{"ml-security":0}}
{"text":"Moreover, they require hundreds of gigabytes of training data.","cats":{"ml-security":0}}
{"text":"Aside from providing a birds eye view of what exists our in depth analysis provides insights on what is lacking in the current discourse on NLP in particular and critical AI in general, proposes additions to the current framework of analysis, provides recommendations future research direction, and highlights the need to importance of exploring the social in this socio-technical system.","cats":{"ml-security":0}}
{"text":"Self-training is a simple yet effective method within semi-supervised learning.","cats":{"ml-security":0}}
{"text":"For the information leakage caused by the attack during the information transmission process, privacy-preservation is introduced for system states.","cats":{"ml-security":1}}
{"text":"Most existing systems that conceal leakage either (1) incur substantial overheads, (2) focus on specific subsets of leakage patterns, or (3) apply the same security notion across various workloads, thereby impeding the attainment of fine-tuned privacy-efficiency trade-offs.","cats":{"ml-security":1}}
{"text":"Numerous studies have underscored the significant privacy risks associated with various leakage patterns in encrypted data stores.","cats":{"ml-security":1}}
{"text":"In our daily life, mobile phone applications and identity documents that we use may bring the risk of privacy leakage, which had increasingly aroused public concern.","cats":{"ml-security":1}}
{"text":"Overall, our approach to privacy unifies, formalizes, and explains many existing ideas, e.g., why the informed adversary assumption may lead to underestimating the information leaking about each entry in the database.","cats":{"ml-security":0}}
{"text":"Our findings can provide valuable insights into the evolving field of non-standard and covert channels, and help spur new countermeasures against such privacy leakage and security issues.","cats":{"ml-security":0}}
{"text":"Classifiers based on deep neural networks have been recently challenged by Adversarial Attack, where the widely existing vulnerability has invoked the research in defending them from potential threats.","cats":{"ml-security":1}}
{"text":"Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications.","cats":{"ml-security":1}}
{"text":"The vulnerability of deep neural network models to adversarial example attacks is a practical challenge in many artificial intelligence applications.","cats":{"ml-security":1}}
{"text":"Machine-learning architectures, such as Convolutional Neural Networks (CNNs) are vulnerable to adversarial attacks: inputs crafted carefully to force the system output to a wrong label.","cats":{"ml-security":1}}
{"text":"Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations.","cats":{"ml-security":1}}
{"text":"Deep neural networks are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to the benign input.","cats":{"ml-security":1}}
{"text":"Given a vulnerable classifier, existing defense methods are mostly white-box and often require re-training the victim under modified loss functions/training regimes.","cats":{"ml-security":1}}
{"text":"Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners.","cats":{"ml-security":1}}
{"text":"With the wide-spread application of machine learning models, it has become critical to study the potential data leakage of models trained on sensitive data.","cats":{"ml-security":1}}
{"text":"The results show that the centralized machine learning model shows more serious member information leakage in all aspects, and the accuracy of the attacker in the central parameter server is significantly higher than the local Inference attacks as participants.","cats":{"ml-security":1}}
{"text":"Therefore, in the current work, we analyze the suitability of these metrics to create Machine Learning based software vulnerability detectors for UMI applications.","cats":{"ml-security":1}}
{"text":"Recently, however, there has been a trend towards evaluating the robustness of these models against adversarial attacks.","cats":{"ml-security":1}}
{"text":"As in-the-wild data are increasingly involved in the training stage, machine learning applications become more susceptible to data poisoning attacks.","cats":{"ml-security":1}}
{"text":"In this paper, we investigate the third type of exploitation of data poisoning - increasing the risks of privacy leakage of benign training samples.","cats":{"ml-security":1}}
{"text":"To this end, we demonstrate a set of data poisoning attacks to amplify the membership exposure of the targeted class.","cats":{"ml-security":1}}
{"text":"Furthermore, adversaries launch poisoning attacks to falsify the health data, which leads to misdiagnosing or even physical damage.","cats":{"ml-security":1}}
{"text":"These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks.","cats":{"ml-security":1}}
{"text":"We then propose an optimization-based clean-label attack in the transfer learning scenario, whereby the poisoning samples are correctly labeled and look \"natural\" to evade human moderation.","cats":{"ml-security":1}}
{"text":"To reveal the real vulnerability of FedRecs, in this paper, we present a new poisoning attack method to manipulate target items' ranks and exposure rates effectively in the top-$K$ recommendation without relying on any prior knowledge.","cats":{"ml-security":1}}
{"text":"Specifically, our attack manipulates target items' exposure rate by a group of synthetic malicious users who upload poisoned gradients considering target items' alternative products.","cats":{"ml-security":1}}
{"text":"We introduce ShortcutGen, a new data poisoning attack that generates sample-dependent, error-minimizing perturbations by learning a generator.","cats":{"ml-security":1}}
{"text":"Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small.","cats":{"ml-security":1}}
