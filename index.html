<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-3xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-6xl pt-4 font-bold"><span class="underline">Your</span> FrontPage</h1>
            <br>
            <p>Generated on 2023-07-24.</p>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by runnig a sentence-model that detects if the abstract describes a paper about a topic of interest. <br> One cool feature: the entire MLOps of this is handled via Github Actions. </p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-4xl pt-4 font-bold">New Datasets</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Deep Dive into the Disparity of Word Error Rates Across Thousands of NPTEL MOOC Videos
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automatic speech recognition (ASR) systems are designed to transcribe spoken language into written text and find utility in a variety of applications including voice assistants and transcription services.However, it has been observed that state-of-the-art ASR systems which deliver impressive benchmark results, struggle with speakers of certain regions or demographics due to variation in their speech properties.<span class='px-1 mx-1 bg-yellow-200'>In this work, we describe the curation of a massive speech dataset of 8740 hours consisting of $\sim9.8$K technical lectures in the English language along with their transcripts delivered by instructors representing various parts of Indian demography. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.894</span></span><span class='px-1 mx-1 bg-yellow-200'>The dataset is sourced from the very popular NPTEL MOOC platform. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.897</span></span>We use the curated dataset to measure the existing disparity in YouTube Automatic Captions and OpenAI Whisper model performance across the diverse demographic traits of speakers in India.While there exists disparity due to gender, native region, age and speech rate of speakers, disparity based on caste is non-existent.We also observe statistically significant disparity across the disciplines of the lectures.These results indicate the need of more inclusive and robust ASR systems and more representational datasets for disparity evaluation in them.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10587v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Analyzing HC-NJDG Data to Understand the Pendency in High Courts in India
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Indian Judiciary is suffering from burden of millions of cases that are lying pending in its courts at all the levels.In this paper, we analyze the data that we have collected on the pendency of 24 high courts in the Republic of India as they were made available on High Court NJDG (HC-NJDG).<span class='px-1 mx-1 bg-yellow-200'>We collected data on 73 days beginning August 31, 2017 to December 26, 2018, including these days. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span>Thus, the data collected by us spans a period of almost sixteen months.We have analyzed various statistics available on the NJDG portal for High Courts, including but not limited to the number of judges in each high court, the number of cases pending in each high court, cases that have been pending for more than 10 years, cases filed, listed and disposed, cases filed by women and senior citizens, etc.Our results show that: 1) statistics as important as the number of judges in high courts have serious errors on NJDG (Fig. 1, 2, 10, 11, Table V).2) pending cases in most of the high courts are increasing rather than decreasing (Fig. 3, 13).3) regular update of HC-NJDG is required for it to be useful.Data related to some high courts is not being updated regularly or is updated erroneously on the portal (Fig. 14).4) there is a huge difference in terms of average load of cases on judges of different high courts (Fig. 6).5) if all the high courts operate at their approved strength of judges, then for most of the high courts pendency can be nullified within 20 years from now (Fig. 21, 22).6) the pending cases filed by women and senior citizens are disproportionately low, they together constitute less than 10% of the total pending cases (Fig. 23 - 27) 7) a better scheduling process for preparing causelists in courts can help reducing the number of pending cases in the High Courts (Fig. 29).8) some statistics are not well defined (Fig. 31).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10615v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Heterogeneous Federated Learning: State-of-the-art and Research Challenges
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications.Existing federated learning works mainly focus on model homogeneous settings.However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients.Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex.Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential.In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges.In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons.We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level.Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field.<span class='px-1 mx-1 bg-yellow-200'>A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10616v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks.However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations.To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving.<span class='px-1 mx-1 bg-yellow-200'>SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.856</span></span>Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies.The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80%.Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities.Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills.We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10635v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RetouchingFFHQ: A Large-scale Dataset for Fine-grained Face Retouching Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The widespread use of face retouching filters on short-video platforms has raised concerns about the authenticity of digital appearances and the impact of deceptive advertising.To address these issues, there is a pressing need to develop advanced face retouching techniques.However, the lack of large-scale and fine-grained face retouching datasets has been a major obstacle to progress in this field.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce RetouchingFFHQ, a large-scale and fine-grained face retouching dataset that contains over half a million conditionally-retouched images. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customization.By including four typical types of face retouching operations and different retouching levels, we extend the binary face retouching detection into a fine-grained, multi-retouching type, and multi-retouching level estimation problem.Additionally, we propose a Multi-granularity Attention Module (MAM) as a plugin for CNN backbones for enhanced cross-scale representation learning.Extensive experiments using different baselines as well as our proposed method on RetouchingFFHQ show decent performance on face retouching detection.With the proposed new dataset, we believe there is great potential for future work to tackle the challenging problem of real-world fine-grained face retouching detection.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10642v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Dataset and Strong Baselines for Classification of Czech News Texts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, parsing, NER) and relatively simple classification tasks such as sentiment classification or article classification from a single news source.<span class='px-1 mx-1 bg-yellow-200'>As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datasets, composed of news articles from various sources spanning over twenty years, which allows a more rigorous evaluation of such models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.789</span></span>We define four classification tasks: news source, news category, inferred author's gender, and day of the week.To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong machine-learning baselines built upon pre-trained transformer models.Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10666v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Self-supervised monocular depth estimation (SS-MDE) has the potential to scale to vast quantities of data.Unfortunately, existing approaches limit themselves to the automotive domain, resulting in models incapable of generalizing to complex environments such as natural or indoor settings.   <span class='px-1 mx-1 bg-yellow-200'>To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more data than existing automotive datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.815</span></span>SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and scuba diving.Using this dataset, we train an SS-MDE model that provides zero-shot generalization to a large collection of indoor/outdoor datasets.The resulting model outperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more efficient architecture.   We additionally introduce a collection of best-practices to further maximize performance and zero-shot generalization.This includes 1) aspect ratio augmentation, 2) camera intrinsic estimation, 3) support frame randomization and 4) flexible motion estimation.Code is available at https://github.com/jspenmar/slowtv_monodepth.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10713v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Differences Between Hard and Noisy-labeled Samples: An Empirical Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic.Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples.However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples.These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples.Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples.We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets.<span class='px-1 mx-1 bg-yellow-200'>We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.843</span></span>Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10718v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning.Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity.These characteristics make it difficult to design and train STDM models.Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques.To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean.<span class='px-1 mx-1 bg-yellow-200'>Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span>Then, typical ST ocean data quality enhancement techniques are discussed.Next, we classify existing STDM studies for ocean into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate the techniques for these tasks.Finally, promising research opportunities are highlighted.This survey will help scientists from the fields of both computer science and ocean science have a better understanding of the fundamental concepts, key techniques, and open challenges of STDM in ocean.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10803v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Perceptual Quality Assessment of Omnidirectional Audio-visual Signals
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, advertising, tourism, etc.Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE).However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignoring that the overall QoE also depends on the accompanying audio signals.In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, which includes 375 distorted omnidirectional audio-visual (A/V) sequences generated from 15 high-quality pristine omnidirectional A/V contents, and the corresponding perceptual audio-visual quality scores.Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which combine existing state-of-the-art single-mode audio and video QA models via multimodal fusion strategies.We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchmark for omnidirectional QoE evaluation.<span class='px-1 mx-1 bg-yellow-200'>Our dataset is available at https://github.com/iamazxl/OAVQA. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.94</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10813v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other Languages
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language.However, where training data for a language does not exist, data from other languages can be used instead.We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU.<span class='px-1 mx-1 bg-yellow-200'>For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span><span class='px-1 mx-1 bg-yellow-200'>For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span>We followed previous research in mapping labels for all datasets to just two classes, positive and negative.Thus we can compare performance on different languages directly, and combine languages for training and testing.In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50.Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER are equally difficult.Similarly, German SER is more difficult, and Urdu SER is easier.In Experiment 2, we trained on one language and tested on another, in both directions for each pair:Amharic<->German, Amharic<->English, and Amharic<->Urdu.Results with Amharic as target suggested that using English or German as source will give the best result.In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic.The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better result can be obtained when using two or three non-Amharic languages for training than when using just one non-Amharic language.Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SER classifier when resources for a language are scarce.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10814v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Captions are crucial for understanding scientific visualizations and documents.Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences.To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences.Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences.We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models.In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively.<span class='px-1 mx-1 bg-yellow-200'>Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10867v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Conservative Estimation of Perception Relevance of Dynamic Objects for Safe Trajectories in Automotive Scenarios
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Having efficient testing strategies is a core challenge that needs to be overcome for the release of automated driving.This necessitates clear requirements as well as suitable methods for testing.In this work, the requirements for perception modules are considered with respect to relevance.The concept of relevance currently remains insufficiently defined and specified.In this paper, we propose a novel methodology to overcome this challenge by exemplary application to collision safety in the highway domain.Using this general system and use case specification, a corresponding concept for relevance is derived.Irrelevant objects are thus defined as objects which do not limit the set of safe actions available to the ego vehicle under consideration of all uncertainties.As an initial step, the use case is decomposed into functional scenarios with respect to collision relevance.For each functional scenario, possible actions of both the ego vehicle and any other dynamic object are formalized as equations.This set of possible actions is constrained by traffic rules, yielding relevance criteria.As a result, we present a conservative estimation which dynamic objects are relevant for perception and need to be considered for a complete evaluation.The estimation provides requirements which are applicable for offline testing and validation of perception components.<span class='px-1 mx-1 bg-yellow-200'>A visualization is presented for examples from the highD dataset, showing the plausibility of the results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span>Finally, a possibility for a future validation of the presented relevance concept is outlined.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10873v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Human Motion Generation: A Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications.<span class='px-1 mx-1 bg-yellow-200'>Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts.While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals.In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field.We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we provide an overview of common datasets and evaluation metrics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span>Lastly, we discuss open problems and outline potential future research directions.We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10894v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state.However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram).These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence.In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales.Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg.<span class='px-1 mx-1 bg-yellow-200'>We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span>Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10923v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in deep learning have significantly improved the performance of various computer vision applications.However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack of prior knowledge about the number and nature of new categories.Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge about the number of novel categories and the proportion of novel samples in the batch.To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervised class incremental learning approach for discovering novel categories on unlabeled sets without prior knowledge.The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and novel categories and clusters on the unlabeled dataset.Furthermore, the proxy anchors-based exemplar generates representative category vectors to mitigate catastrophic forgetting.<span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained datasets under real-world scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10943v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Spinal nerve segmentation method and dataset construction in endoscopic surgical scenarios
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Endoscopic surgery is currently an important treatment method in the field of spinal surgery and avoiding damage to the spinal nerves through video guidance is a key challenge.This paper presents the first real-time segmentation method for spinal nerves in endoscopic surgery, which provides crucial navigational information for surgeons.<span class='px-1 mx-1 bg-yellow-200'>A finely annotated segmentation dataset of approximately 10,000 consec-utive frames recorded during surgery is constructed for the first time for this field, addressing the problem of semantic segmentation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-frame information and self-attention mechanisms.We also conduct extended exper-iments on a similar polyp endoscopy video dataset and show that the model has good generalization ability with advantageous performance.<span class='px-1 mx-1 bg-yellow-200'>The dataset and code of this work are presented at: https://github.com/zzzzzzpc/FUnet . <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.896</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10955v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DataXploreFines: Generalized Data for Informed Decision, Making, An Interactive Shiny Application for Data Analysis and Visualization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This article presents DataXploreFines, an innovative Shiny application that revolutionizes data exploration, analysis, and visualization.The application offers functionalities for data loading, management, summarization, basic graphs, advanced analysis, and contact.<span class='px-1 mx-1 bg-yellow-200'>Users can upload their datasets in popular formats like CSV or Excel, explore the data structure, perform manipulations, and obtain statistical summaries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.699</span></span>DataXploreFines provides a wide range of interactive visualizations, including histograms, scatter plots, bar charts, and line graphs, enabling users to identify patterns and trends.Additionally, the application offers statistical tools such as time series analysis using ARIMA and SARIMA models, forecasting, and Ljung-Box statistic.Its user-friendly interface empowers individuals from various domains, including beginners in statistics, to make informed decisions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.11056v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                OBJECT 3DIT: Language-guided 3D-aware Image Editing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected.As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process.In this work, we formulate the newt ask of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction in context of the underlying 3D scene.<span class='px-1 mx-1 bg-yellow-200'>To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from procedurally generated 3D scenes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.878</span></span>Each example consists of an input image, editing instruction in language, and the edited image.We also introduce 3DIT : single and multi-task models for four editing tasks.Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations.Surprisingly, training on only synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to real-world images.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.11073v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness.<span class='px-1 mx-1 bg-yellow-200'>To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.925</span></span><span class='px-1 mx-1 bg-yellow-200'>Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span>To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning.Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-shot and few-shot learning scenarios demonstrate the superiority of DialogStudio.<span class='px-1 mx-1 bg-yellow-200'>To improve transparency and support dataset and task-based research, as well as language model pre-training, all datasets, licenses, codes, and models associated with DialogStudio are made publicly accessible at https://github.com/salesforce/DialogStudio <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10172v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Realistic human-centric rendering plays a key role in both computer vision and computer graphics.Rapid progress has been made in the algorithm aspect over the years, yet existing human-centric rendering datasets and benchmarks are rather impoverished in terms of diversity, which are crucial for rendering effect.Researchers are usually constrained to explore and evaluate a small set of rendering problems on current datasets, while real-world applications require methods to be robust across different scenarios.In this work, we present DNA-Rendering, a large-scale, high-fidelity repository of human performance data for neural actor rendering.DNA-Rendering presents several alluring attributes.<span class='px-1 mx-1 bg-yellow-200'>First, our dataset contains over 1500 human subjects, 5000 motion sequences, and 67.5M frames' data volume. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.905</span></span>Second, we provide rich assets for each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models, cloth/accessory materials, multi-view images, and videos.These assets boost the current method's accuracy on downstream rendering tasks.<span class='px-1 mx-1 bg-yellow-200'>Third, we construct a professional multi-view system to capture data, which contains 60 synchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and stern camera calibration steps, ensuring high-quality resources for task training and evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.75</span></span>Along with the dataset, we provide a large-scale and quantitative benchmark in full-scale, with multiple tasks to evaluate the existing progress of novel view synthesis, novel pose animation synthesis, and novel identity rendering methods.In this manuscript, we describe our DNA-Rendering effort as a revealing of new observations, challenges, and future directions to human-centric rendering.<span class='px-1 mx-1 bg-yellow-200'>The dataset, code, and benchmarks will be publicly available at https://dna-rendering.github.io/ <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10173v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-4xl pt-4 font-bold">Prompt Engineering</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Divide & Bind Your Attention for Improved Generative Semantic Nursing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity.Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt.Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics.<span class='px-1 mx-1 bg-yellow-200'>It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding.To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind.We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss.Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks.More videos and updates can be found on the project page \url{https://sites.google.com/view/divide-and-bind}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10864v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent work has shown that language models' (LMs) prompt-based learning capabilities make them well suited for automating data labeling in domains where manual annotation is expensive. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span><span class='px-1 mx-1 bg-yellow-200'>The challenge is that while writing an initial prompt is cheap, improving a prompt is costly -- practitioners often require significant labeled data in order to evaluate the impact of prompt modifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>Our work asks whether it is possible to improve prompt-based learning without additional labeled data.<span class='px-1 mx-1 bg-yellow-200'>We approach this problem by attempting to modify the predictions of a prompt, rather than the prompt itself. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>Our intuition is that accurate predictions should also be consistent: samples which are similar under some feature representation should receive the same prompt prediction.We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions.Embroid then uses these neighborhoods to create additional predictions for each sample, and combines these predictions with a simple latent variable graphical model in order to generate a final corrected prediction.In addition to providing a theoretical analysis of Embroid, we conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks.We find that (1) Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought), and (3) can be specialized to domains like law through the embedding functions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.11031v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Event Causality Identification (ECI) aims at determining whether there is a causal relation between two event mentions.<span class='px-1 mx-1 bg-yellow-200'>Conventional prompt learning designs a prompt template to first predict an answer word and then maps it to the final decision. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span>Unlike conventional prompts, we argue that predicting an answer word may not be a necessary prerequisite for the ECI task.Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption.The design motivation is to try the most utilization of the encyclopedia-like knowledge embedded in a pre-trained language model.In light of such considerations, we propose a deterministic assumption prompt learning model, called DAPrompt, for the ECI task.In particular, we design a simple deterministic assumption template concatenating with the input event pair, which includes two masks as predicted events' tokens.We use the probabilities of predicted events to evaluate the assumption rationality for the final event causality decision.Experiments on the EventStoryLine corpus and Causal-TimeBank corpus validate our design objective in terms of significant performance improvements over the state-of-the-art algorithms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.09813v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt.Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them.However, how to support users to efficiently explore the model's capability and to create effective prompts are still open-ended research questions.In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt.Through the iterative process, users can efficiently explore the model's capability, and clarify their intent.<span class='px-1 mx-1 bg-yellow-200'>PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Lastly, users can revert to a desired step by reviewing the work history.In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08985v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts.However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language.This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts.<span class='px-1 mx-1 bg-yellow-200'>The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span>To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration.Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.09036v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span>The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query.They have even been treated as commodities to be bought and sold.However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret.In this paper, we present a framework for systematically measuring the success of prompt extraction attacks.In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.06865v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Self-regulating Prompts: Foundational Model Adaptation without Forgetting
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP.This leads to the loss of the model's original generalization capability.<span class='px-1 mx-1 bg-yellow-200'>To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating {prompted} representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch.To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity.PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization.We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods.Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.06948v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-4xl pt-4 font-bold">Data Quality</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Differences Between Hard and Noisy-labeled Samples: An Empirical Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic.Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples.However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples.These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples.Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples.We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets.<span class='px-1 mx-1 bg-yellow-200'>We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.843</span></span>Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10718v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses.To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics.Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans.Some existing methods tackle this problem by analyzing each metric independently to detect anomalies.However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually.To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem.However, most of the studies fall short of extracting these two types of features explicitly.Moreover, there exist some unlabeled anomalies mixed in the training data, which may hinder the detection performance.To address these limitations, we propose the Relational- Temporal Anomaly Detection Model (RTAnomaly) that combines the relational and temporal information of metrics.RTAnomaly employs a graph attention layer to learn the dependencies among metrics, which will further help pinpoint the anomalous metrics that may cause the anomaly effectively.<span class='px-1 mx-1 bg-yellow-200'>In addition, we exploit the concept of positive unlabeled learning to address the issue of potential anomalies in the training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>To evaluate our method, we conduct experiments on a public dataset and two industrial datasets.RTAnomaly outperforms all the baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920, demonstrating its superiority.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10869v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Neuron Sensitivity Guided Test Case Selection for Deep Learning Testing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Deep Neural Networks~(DNNs) have been widely deployed in software to address various tasks~(e.g., autonomous driving, medical diagnosis).However, they could also produce incorrect behaviors that result in financial losses and even threaten human safety.<span class='px-1 mx-1 bg-yellow-200'>To reveal the incorrect behaviors in DNN and repair them, DNN developers often collect rich unlabeled datasets from the natural world and label them to test the DNN models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>However, properly labeling a large number of unlabeled datasets is a highly expensive and time-consuming task.   To address the above-mentioned problem, we propose NSS, Neuron Sensitivity guided test case Selection, which can reduce the labeling time by selecting valuable test cases from unlabeled datasets.NSS leverages the internal neuron's information induced by test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly.We evaluate NSS with four widely used datasets and four well-designed DNN models compared to SOTA baseline methods.The results show that NSS performs well in assessing the test cases' probability of fault triggering and model improvement capabilities.Specifically, compared with baseline approaches, NSS obtains a higher fault detection rate~(e.g., when selecting 5\% test case from the unlabeled dataset in MNIST \& LeNet1 experiment, NSS can obtain 81.8\% fault detection rate, 20\% higher than baselines).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.11011v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ClickSeg: 3D Instance Segmentation with Click-Level Weak Annotations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>3D instance segmentation methods often require fully-annotated dense labels for training, which are costly to obtain.In this paper, we present ClickSeg, a novel click-level weakly supervised 3D instance segmentation method that requires one point per instance annotation merely.Such a problem is very challenging due to the extremely limited labels, which has rarely been solved before.<span class='px-1 mx-1 bg-yellow-200'>We first develop a baseline weakly-supervised training method, which generates pseudo labels for unlabeled data by the model itself. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>To utilize the property of click-level annotation setting, we further propose a new training framework.Instead of directly using the model inference way, i.e., mean-shift clustering, to generate the pseudo labels, we propose to use k-means with fixed initial seeds: the annotated points.New similarity metrics are further designed for clustering.Experiments on ScanNetV2 and S3DIS datasets show that the proposed ClickSeg surpasses the previous best weakly supervised instance segmentation result by a large margin (e.g., +9.4% mAP on ScanNetV2).Using 0.02% supervision signals merely, ClickSeg achieves $\sim$90% of the accuracy of the fully-supervised counterpart.Meanwhile, it also achieves state-of-the-art semantic segmentation results among weakly supervised methods that use the same annotation settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.09732v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain.Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels.In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label.Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain.Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels.Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.09933v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions.Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices.Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data.We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms.<span class='px-1 mx-1 bg-yellow-200'>Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span><span class='px-1 mx-1 bg-yellow-200'>We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span>Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10062v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Mitigating Label Bias via Decoupled Confident Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias.<span class='px-1 mx-1 bg-yellow-200'>However, such methodologies largely assume that observed labels in training data are correct. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span>This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation.In particular, human-generated labels are prone to encoding societal biases.<span class='px-1 mx-1 bg-yellow-200'>While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08945v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EVIL: Evidential Inference Learning for Trustworthy Semi-supervised Medical Image Segmentation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, uncertainty-aware methods have attracted increasing attention in semi-supervised medical image segmentation.However, current methods usually suffer from the drawback that it is difficult to balance the computational cost, estimation accuracy, and theoretical support in a unified framework.To alleviate this problem, we introduce the Dempster-Shafer Theory of Evidence (DST) into semi-supervised medical image segmentation, dubbed Evidential Inference Learning (EVIL).EVIL provides a theoretically guaranteed solution to infer accurate uncertainty quantification in a single forward pass.<span class='px-1 mx-1 bg-yellow-200'>Trustworthy pseudo labels on unlabeled data are generated after uncertainty estimation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span>The recently proposed consistency regularization-based training paradigm is adopted in our framework, which enforces the consistency on the perturbed predictions to enhance the generalization with few labeled data.Experimental results show that EVIL achieves competitive performance in comparison with several state-of-the-art methods on the public dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08988v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Empirical Study on the Effectiveness of Noisy Label Learning for Program Understanding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, deep learning models have been widely applied in program understanding tasks, and these models achieve state-of-the-art results on many benchmark datasets.A major challenge of deep learning for program understanding is that the effectiveness of these approaches depends on the quality of their datasets, and these datasets often contain noisy data samples.<span class='px-1 mx-1 bg-yellow-200'>A typical kind of noise in program understanding datasets is label noises, which means that the target outputs for some inputs are mislabeled.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.784</span></span><span class='px-1 mx-1 bg-yellow-200'>Label noises may have a negative impact on the performance of deep learning models, so researchers have proposed various approaches to alleviate the impact of noisy labels, and formed a new research topic: noisy label learning (NLL). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.811</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we conduct an empirical study on the effectiveness of noisy label learning on deep learning for program understanding datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate various noisy label learning approaches and deep learning models on two tasks: program classification and code summarization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span>From the evaluation results, we find that the impact of label noise and NLL approaches on small deep learning models and large pre-trained models are different: small models are prone to label noises in program classification and NLL approaches can improve their robustness, while large pre-trained models are robust against label noises and NLL does not significantly improve their performances.On the other hand, NLL approaches have shown satisfying results in identifying noisy labeled samples for both tasks, indicating that these techniques can benefit researchers in building high-quality program understanding datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08990v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Facial expression recognition (FER) remains a challenging task due to the ambiguity of expressions.<span class='px-1 mx-1 bg-yellow-200'>The derived noisy labels significantly harm the performance in real-world scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>To address this issue, we present a new FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks to mitigate the impact of label noise from two perspectives.Firstly, LA-Net uses landmark information to suppress the uncertainty in expression space and constructs the label distribution of each sample by neighborhood aggregation, which in turn improves the quality of training supervision.Secondly, the model incorporates landmark information into expression representations using the devised expression-landmark contrastive loss.<span class='px-1 mx-1 bg-yellow-200'>The enhanced expression feature extractor can be less susceptible to label noise. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>Our method can be integrated with any deep neural network for better training supervision without introducing extra inference costs.We conduct extensive experiments on both in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net achieves state-of-the-art performance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.09023v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Soft Curriculum for Learning Conditional GANs with Noisy-Labeled and Uncurated Unlabeled Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Label-noise or curated unlabeled data is used to compensate for the assumption of clean labeled data in training the conditional generative adversarial network; however, satisfying such an extended assumption is occasionally laborious or impractical. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>As a step towards generative modeling accessible to everyone, we introduce a novel conditional image generation framework that accepts noisy-labeled and uncurated unlabeled data during training: (i) closed-set and open-set label noise in labeled data and (ii) closed-set and open-set unlabeled data.To combat it, we propose soft curriculum learning, which assigns instance-wise weights for adversarial training while assigning new labels for unlabeled data and correcting wrong labels for labeled data.Unlike popular curriculum learning, which uses a threshold to pick the training samples, our soft curriculum controls the effect of each training instance by using the weights predicted by the auxiliary classifier, resulting in the preservation of useful samples while ignoring harmful ones.<span class='px-1 mx-1 bg-yellow-200'>Our experiments show that our approach outperforms existing semi-supervised and label-noise robust methods in terms of both quantitative and qualitative performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span>In particular, the proposed approach is able to match the performance of (semi-) supervised GANs even with less than half the labeled data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08319v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Corruption is frequently observed in collected data and has been extensively studied in machine learning under different corruption models.Despite this, there remains a limited understanding of how these models relate such that a unified view of corruptions and their consequences on learning is still lacking.In this work, we formally analyze corruption models at the distribution level through a general, exhaustive framework based on Markov kernels.We highlight the existence of intricate joint and dependent corruptions on both labels and attributes, which are rarely touched by existing research.<span class='px-1 mx-1 bg-yellow-200'>Further, we show how these corruptions affect standard supervised learning by analyzing the resulting changes in Bayes Risk. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span>Our findings offer qualitative insights into the consequences of "more complex" corruptions on the learning problem, and provide a foundation for future quantitative comparisons.Applications of the framework include corruption-corrected learning, a subcase of which we study in this paper by theoretically analyzing loss correction with respect to different corruption instances.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08643v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields.<span class='px-1 mx-1 bg-yellow-200'>Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias.In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates.We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which we also release.Based on our results, we recommend our new error correction method as it is versatile and efficient.In sum, automated classifiers, even those below common accuracy standards or making systematic misclassifications, can be useful for measurement with careful study design and appropriate error correction methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.06483v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-4xl pt-4 font-bold">Developer Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pluvio: Assembly Clone Search for Out-of-domain Architectures and Libraries through Transfer Learning and Conditional Variational Information Bottleneck
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The practice of code reuse is crucial in software development for a faster and more efficient development lifecycle. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>In reality, however, code reuse practices lack proper control, resulting in issues such as vulnerability propagation and intellectual property infringements.Assembly clone search, a critical shift-right defence mechanism, has been effective in identifying vulnerable code resulting from reuse in released executables.Recent studies on assembly clone search demonstrate a trend towards using machine learning-based methods to match assembly code variants produced by different toolchains.However, these methods are limited to what they learn from a small number of toolchain variants used in training, rendering them inapplicable to unseen architectures and their corresponding compilation toolchain variants.   This paper presents the first study on the problem of assembly clone search with unseen architectures and libraries.We propose incorporating human common knowledge through large-scale pre-trained natural language models, in the form of transfer learning, into current learning-based approaches for assembly clone search.Transfer learning can aid in addressing the limitations of the existing approaches, as it can bring in broader knowledge from human experts in assembly code.We further address the sequence limit issue by proposing a reinforcement learning agent to remove unnecessary and redundant tokens.Coupled with a new Variational Information Bottleneck learning strategy, the proposed system minimizes the reliance on potential indicators of architectures and optimization settings, for a better generalization of unseen architectures.We simulate the unseen architecture clone search scenarios and the experimental results show the effectiveness of the proposed approach against the state-of-the-art solutions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10631v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Addressing Compiler Errors: Stack Overflow or Large Language Models?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Compiler error messages serve as an initial resource for programmers dealing with compilation errors.However, previous studies indicate that they often lack sufficient targeted information to resolve code issues.<span class='px-1 mx-1 bg-yellow-200'>Consequently, programmers typically rely on their own research to fix errors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>Historically, Stack Overflow has been the primary resource for such information, but recent advances in large language models offer alternatives.This study systematically examines 100 compiler error messages from three sources to determine the most effective approach for programmers encountering compiler errors.Factors considered include Stack Overflow search methods and the impact of model version and prompt phrasing when using large language models.The results reveal that GPT-4 outperforms Stack Overflow in explaining compiler error messages, the effectiveness of adding code snippets to Stack Overflow searches depends on the search method, and results for Stack Overflow differ significantly between Google and StackExchange API searches.Furthermore, GPT-4 surpasses GPT-3.5, with "How to fix" prompts yielding superior outcomes to "What does this error mean" prompts.These results offer valuable guidance for programmers seeking assistance with compiler error messages, underscoring the transformative potential of advanced large language models like GPT-4 in debugging and opening new avenues of exploration for researchers in AI-assisted programming.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10793v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Empirical Evaluation of a Live Environment for Extract Method Refactoring
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Complex software can be hard to read, adapt, and maintain.<span class='px-1 mx-1 bg-yellow-200'>Refactoring it can create cleaner and self-explanatory code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span><span class='px-1 mx-1 bg-yellow-200'>Refactoring tools try to guide developers towards better code, with more quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span><span class='px-1 mx-1 bg-yellow-200'>However, most of them take too long to provide feedback, support, and guidance on how developers should improve their software. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>To reduce this problem, we explored the concept of Live Refactoring, focusing on visually suggesting and applying refactorings, in real-time.With this in mind, we developed a Live Refactoring Environment that visually identifies, recommends, and applies Extract Method refactorings.To validate it, we conducted an empirical experiment.Early results showed that our approach improved several code quality metrics.Besides, we also concluded that our results were significantly different and better than the ones from refactoring the code manually without further help.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.11010v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Effective Data-Driven Approach for Localizing Deep Learning Faults
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Deep Learning (DL) applications are being used to solve problems in critical domains (e.g., autonomous driving or medical diagnosis systems).<span class='px-1 mx-1 bg-yellow-200'>Thus, developers need to debug their systems to ensure that the expected behavior is delivered. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.749</span></span>However, it is hard and expensive to debug DNNs.When the failure symptoms or unsatisfied accuracies are reported after training, we lose the traceability as to which part of the DNN program is responsible for the failure.Even worse, sometimes, a deep learning program has different types of bugs.To address the challenges of debugging DNN models, we propose a novel data-driven approach that leverages model features to learn problem patterns.Our approach extracts these features, which represent semantic information of faults during DNN training.Our technique uses these features as a training dataset to learn and infer DNN fault patterns.<span class='px-1 mx-1 bg-yellow-200'>Also, our methodology automatically links bug symptoms to their root causes, without the need for manually crafted mappings, so that developers can take the necessary steps to fix faults. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>We evaluate our approach using real-world and mutated models.Our results demonstrate that our technique can effectively detect and diagnose different bug types.Finally, our technique achieved better accuracy, precision, and recall than prior work for mutated models.Also, our approach achieved comparable results for real-world models in terms of accuracy and performance to the state-of-the-art.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08947v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                In-IDE Generation-based Information Support with a Large Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Developers often face challenges in code understanding, which is crucial for building and maintaining high-quality software systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span>Code comments and documentation can provide some context for the code, but are often scarce or missing.This challenge has become even more pressing with the rise of large language model (LLM) based code generation tools.To understand unfamiliar code, most software developers rely on general-purpose search engines to search through various programming information resources, which often requires multiple iterations of query rewriting and information foraging.More recently, developers have turned to online chatbots powered by LLMs, such as ChatGPT, which can provide more customized responses but also incur more overhead as developers need to communicate a significant amount of context to the LLM via a textual interface.In this study, we provide the investigation of an LLM-based conversational UI in the IDE.We aim to understand the promises and obstacles for tools powered by LLMs that are contextually aware, in that they automatically leverage the developer's programming context to answer queries.To this end, we develop an IDE Plugin that allows users to query back-ends such as OpenAI's GPT-3.5 and GPT-4 with high-level requests, like: explaining a highlighted section of code, explaining key domain-specific terms, or providing usage examples for an API.We conduct an exploratory user study with 32 participants to understand the usefulness and effectiveness, as well as individual preferences in the usage of, this LLM-powered information support tool.The study confirms that this approach can aid code understanding more effectively than web search, but the degree of the benefit differed by participants' experience levels.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08177v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Extending the Frontier of ChatGPT: Code Generation and Debugging
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large-scale language models (LLMs) have emerged as a groundbreaking innovation in the realm of question-answering and conversational agents.These models, leveraging different deep learning architectures such as Transformers, are trained on vast corpora to predict sentences based on given queries.Among these LLMs, ChatGPT, developed by OpenAI, has ushered in a new era by utilizing artificial intelligence (AI) to tackle diverse problem domains, ranging from composing essays and biographies to solving intricate mathematical integrals.The versatile applications enabled by ChatGPT offer immense value to users.However, assessing the performance of ChatGPT's output poses a challenge, particularly in scenarios where queries lack clear objective criteria for correctness.For instance, evaluating the quality of generated essays becomes arduous and relies heavily on manual labor, in stark contrast to evaluating solutions to well-defined, closed-ended questions such as mathematical problems.This research paper delves into the efficacy of ChatGPT in solving programming problems, examining both the correctness and the efficiency of its solution in terms of time and memory complexity.The research reveals a commendable overall success rate of 71.875\%, denoting the proportion of problems for which ChatGPT was able to provide correct solutions that successfully satisfied all the test cases present in Leetcode.It exhibits strengths in structured problems and shows a linear correlation between its success rate and problem acceptance rates.<span class='px-1 mx-1 bg-yellow-200'>However, it struggles to improve solutions based on feedback, pointing to potential shortcomings in debugging tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span>These findings provide a compact yet insightful glimpse into ChatGPT's capabilities and areas for improvement.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08260v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Information to Choice: A Critical Inquiry Into Visualization Tools for Decision Making
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the face of complex decisions, people often engage in a three-stage process that spans from (1) exploring and analyzing pertinent information (intelligence); (2) generating and exploring alternative options (design); and ultimately culminating in (3) selecting the optimal decision by evaluating discerning criteria (choice).We can fairly assume that all good visualizations aid in the intelligence stage by enabling data exploration and analysis.Yet, to what degree and how do visualization systems currently support the other decision making stages, namely design and choice?To explore this question, we conducted a comprehensive review of decision-focused visualization tools by examining publications in major visualization journals and conferences, including VIS, EuroVis, and CHI, spanning all available years.<span class='px-1 mx-1 bg-yellow-200'>We employed a deductive coding method and in-depth analysis to assess if and how visualization tools support design and choice. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>Specifically, we examined each visualization tool by (i) its degree of visibility for displaying decision alternatives, criteria, and preferences, and (ii) its degree of flexibility for offering means to manipulate the decision alternatives, criteria, and preferences with interactions such as adding, modifying, changing mapping, and filtering.Our review highlights the opportunities and challenges and reveals a surprising scarcity of tools that support all stages, and while most tools excel in offering visibility for decision criteria and alternatives, the degree of flexibility to manipulate these elements is often limited, and the lack of tools that accommodate decision preferences and their elicitation is notable.Future research could explore enhancing flexibility levels and variety, exploring novel visualization paradigms, increasing algorithmic support, and ensuring that this automation is user-controlled via the enhanced flexibility levels.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.08326v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SecureFalcon: The Next Cyber Reasoning System for Cyber Security
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software vulnerabilities leading to various detriments such as crashes, data loss, and security breaches, significantly hinder the quality, affecting the market adoption of software applications and systems.<span class='px-1 mx-1 bg-yellow-200'>Although traditional methods such as automated software testing, fault localization, and repair have been intensively studied, static analysis tools are most commonly used and have an inherent false positives rate, posing a solid challenge to developer productivity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>Large Language Models (LLMs) offer a promising solution to these persistent issues.Among these, FalconLLM has shown substantial potential in identifying intricate patterns and complex vulnerabilities, hence crucial in software vulnerability detection.In this paper, for the first time, FalconLLM is being fine-tuned for cybersecurity applications, thus introducing SecureFalcon, an innovative model architecture built upon FalconLLM.SecureFalcon is trained to differentiate between vulnerable and non-vulnerable C code samples.We build a new training dataset, FormAI, constructed thanks to Generative Artificial Intelligence (AI) and formal verification to evaluate its performance.SecureFalcon achieved an impressive 94% accuracy rate in detecting software vulnerabilities, emphasizing its significant potential to redefine software vulnerability detection methods in cybersecurity.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.06616v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-4xl pt-4 font-bold">ML security</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting.It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability.However, SecureBoost suffers from high computational complexity and risk of label leakage.To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy.Existing methods either set hyperparameters empirically or heuristically, which are far from optimal.To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage.We design measurements of the three objectives.<span class='px-1 mx-1 bg-yellow-200'>In particular, the privacy leakage is measured using our proposed instance clustering attack. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span>Experimental results demonstrate that the CMOSB yields not only hyperparameters superior to the baseline but also optimal sets of hyperparameters that can support the flexible requirements of FL participants.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10579v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Deep fused flow and topology features for botnet detection basing on pretrained GCN
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Nowadays, botnets have become one of the major threats to cyber security. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>The characteristics of botnets are mainly reflected in bots network behavior and their intercommunication relationships.Existing botnet detection methods use flow features or topological features of the communication graph individually and overlook the other type of feature, which affects model performance.In this paper, we propose a botnet detection model which uses graph convolutional network (GCN) to deeply fuse flow features and topological features for the first time.We construct communication graphs from network traffic and represent nodes with flow features.Due to the imbalance of existing public traffic flow datasets, it is impossible to train a GCN model on these datasets.Therefore, we use a balanced public communication graph dataset to pretrain a GCN model, thereby guaranteeing its capacity for recognizing topological features.We then feed the communication graph with flow features into the pretrained GCN.The output from the last hidden layer is treated as the fusion of flow and topological features.Additionally, by adjusting the number of layers in the GCN network, the model can effectively detect botnets operating under both C2 and P2P structures.Validated on the public ISCX2014 dataset, our approach achieves a remarkable accuracy of 98.85% and a recall rate of 92.90% for C2 botnets, alongside an accuracy of 99.10% and a recall rate of 94.66% for P2P botnets.These results not only demonstrate the efficacy of our method, but also surpass the performance of the currently leading detection models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10583v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Holistic Assessment of the Reliability of Machine Learning Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span><span class='px-1 mx-1 bg-yellow-200'>Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.778</span></span>This paper proposes a holistic assessment methodology for the reliability of ML systems.Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection.A reliability score is also introduced and used to assess the overall system reliability.To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using our proposed reliability metrics and reliability score.Our analysis of over 500 models reveals that designing for one metric does not necessarily constrain others but certain algorithmic techniques can improve reliability across multiple metrics simultaneously.This study contributes to a more comprehensive understanding of ML reliability and provides a roadmap for future research and development.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10586v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties.Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets.This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants.Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works.However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information.In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency.This survey differs from previous ones due to four distinct contributions.First, we present a new taxonomy of FL methods in terms of the sharing methods, which includes three categories of shared information: model sharing, synthetic data sharing, and knowledge sharing.<span class='px-1 mx-1 bg-yellow-200'>Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms that provide certain privacy guarantees. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span>Third, we conduct extensive experiments to compare the performance and communication overhead of various sharing methods in FL.<span class='px-1 mx-1 bg-yellow-200'>Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span>Finally, we discuss potential deficiencies in current methods and outline future directions for improvement.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10655v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts.This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents.Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption.To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs.It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE.To reduce numerical errors in both the forward generation and gradient backpropagation processes, we further reparameterize the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration.<span class='px-1 mx-1 bg-yellow-200'>Finally, we demonstrate the effectiveness of AdjointDPM on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10711v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions.However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use.<span class='px-1 mx-1 bg-yellow-200'>Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span>Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs.In this paper, we present the theoretical limitations of such semantic censorship approaches.Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span><span class='px-1 mx-1 bg-yellow-200'>As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10719v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Adversarial attacks for mixtures of classifiers
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.828</span></span><span class='px-1 mx-1 bg-yellow-200'>However, it has been shown that existing attacks are not well suited for this kind of classifiers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.796</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>We then show that existing attacks do not meet both of these properties.<span class='px-1 mx-1 bg-yellow-200'>Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10788v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Battle Ground: Data Collection and Labeling of CTF Games to Understand Human Cyber Operators
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Industry standard frameworks are now widespread for labeling the high-level stages and granular actions of attacker and defender behavior in cyberspace.While these labels are used for atomic actions, and to some extent for sequences of actions, there remains a need for labeled data from realistic full-scale attacks.This data is valuable for better understanding human actors' decisions, behaviors, and individual attributes.<span class='px-1 mx-1 bg-yellow-200'>The analysis could lead to more effective attribution and disruption of attackers.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.781</span></span>We present a methodological approach and exploratory case study for systematically analyzing human behavior during a cyber offense/defense capture-the-flag (CTF) game.We describe the data collection and analysis to derive a metric called keystroke accuracy.After collecting players' commands, we label them using the MITRE ATT&CK framework using a new tool called Pathfinder.We present results from preliminary analysis of participants' keystroke accuracy and its relation to score outcome in CTF games.We describe frequency of action classification within the MITRE ATT&CK framework and discuss some of the mathematical trends suggested by our observations.We conclude with a discussion of extensions for the methodology, including performance evaluation during games and the potential use of this methodology for training artificial intelligence.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10877v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Collaborative inference has been a promising solution to enable resource-constrained edge devices to perform inference using state-of-the-art deep neural networks (DNNs).In collaborative inference, the edge device first feeds the input to a partial DNN locally and then uploads the intermediate result to the cloud to complete the inference.<span class='px-1 mx-1 bg-yellow-200'>However, recent research indicates model inversion attacks (MIAs) can reconstruct input data from intermediate results, posing serious privacy concerns for collaborative inference. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.769</span></span>Existing perturbation and cryptography techniques are inefficient and unreliable in defending against MIAs while performing accurate inference.This paper provides a viable solution, named PATROL, which develops privacy-oriented pruning to balance privacy, efficiency, and utility of collaborative inference.PATROL takes advantage of the fact that later layers in a DNN can extract more task-specific features.Given limited local resources for collaborative inference, PATROL intends to deploy more layers at the edge based on pruning techniques to enforce task-specific features for inference and reduce task-irrelevant but sensitive features for privacy preservation.<span class='px-1 mx-1 bg-yellow-200'>To achieve privacy-oriented pruning, PATROL introduces two key components: Lipschitz regularization and adversarial reconstruction training, which increase the reconstruction errors by reducing the stability of MIAs and enhance the target inference model by adversarial training, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10981v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DREAM: Domain-free Reverse Engineering Attributes of Black-box Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Deep learning models are usually black boxes when deployed on machine learning platforms.Prior works have shown that the attributes ($e.g.$, the number of convolutional layers) of a target black-box neural network can be exposed through a sequence of queries.<span class='px-1 mx-1 bg-yellow-200'>There is a crucial limitation: these works assume the dataset used for training the target model to be known beforehand and leverage this dataset for model attribute attack. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span>However, it is difficult to access the training dataset of the target black-box model in reality.Therefore, whether the attributes of a target black-box model could be still revealed in this case is doubtful.In this paper, we investigate a new problem of Domain-agnostic Reverse Engineering the Attributes of a black-box target Model, called DREAM, without requiring the availability of the target model's training dataset, and put forward a general and principled framework by casting this problem as an out of distribution (OOD) generalization problem.In this way, we can learn a domain-agnostic model to inversely infer the attributes of a target black-box model with unknown training data.This makes our method one of the kinds that can gracefully apply to an arbitrary domain for model attribute reverse engineering with strong generalization ability.Extensive experimental studies are conducted and the results validate the superiority of our proposed method over the baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10997v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                To What Extent Are Honeypots and Honeynets Autonomic Computing Systems?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Cyber threats, such as advanced persistent threats (APTs), ransomware, and zero-day exploits, are rapidly evolving and demand improved security measures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span><span class='px-1 mx-1 bg-yellow-200'>Honeypots and honeynets, as deceptive systems, offer valuable insights into attacker behavior, helping researchers and practitioners develop innovative defense strategies and enhance detection mechanisms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.752</span></span>However, their deployment involves significant maintenance and overhead expenses.At the same time, the complexity of modern computing has prompted the rise of autonomic computing, aiming for systems that can operate without human intervention.Recent honeypot and honeynet research claims to incorporate autonomic computing principles, often using terms like adaptive, dynamic, intelligent, and learning.This study investigates such claims by measuring the extent to which autonomic principles principles are expressed in honeypot and honeynet literature.The findings reveal that autonomic computing keywords are present in the literature sample, suggesting an evolution from self-adaptation to autonomic computing implementations.Yet, despite these findings, the analysis also shows low frequencies of self-configuration, self-healing, and self-protection keywords.Interestingly, self-optimization appeared prominently in the literature.While this study presents a foundation for the convergence of autonomic computing and deceptive systems, future research could explore technical implementations in sample articles and test them for autonomic behavior.Additionally, investigations into the design and implementation of individual autonomic computing principles in honeypots and determining the necessary ratio of these principles for a system to exhibit autonomic behavior could provide valuable insights for both researchers and practitioners.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.11038v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Building More Robust Models with Frequency Bias
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The vulnerability of deep neural networks to adversarial samples has been a major impediment to their broad applications, despite their success in various fields. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.876</span></span><span class='px-1 mx-1 bg-yellow-200'>Recently, some works suggested that adversarially-trained models emphasize the importance of low-frequency information to achieve higher robustness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.759</span></span>While several attempts have been made to leverage this frequency characteristic, they have all faced the issue that applying low-pass filters directly to input images leads to irreversible loss of discriminative information and poor generalizability to datasets with distinct frequency features.This paper presents a plug-and-play module called the Frequency Preference Control Module that adaptively reconfigures the low- and high-frequency components of intermediate feature representations, providing better utilization of frequency in robust learning.<span class='px-1 mx-1 bg-yellow-200'>Empirical studies show that our proposed module can be easily incorporated into any adversarial training framework, further improving model robustness across different architectures and datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.766</span></span><span class='px-1 mx-1 bg-yellow-200'>Additionally, experiments were conducted to examine how the frequency bias of robust models impacts the adversarial training process and its final robustness, revealing interesting insights. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.72</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.09763v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2023-07-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rethinking Backdoor Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span><span class='px-1 mx-1 bg-yellow-200'>Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we present a different approach to the backdoor attack problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to "detect" in a general sense. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.887</span></span><span class='px-1 mx-1 bg-yellow-200'>Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.791</span></span><span class='px-1 mx-1 bg-yellow-200'>Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.884</span></span><span class='px-1 mx-1 bg-yellow-200'>Under this assumption (which we make formal) we develop a new primitive for detecting backdoor attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>Our primitive naturally gives rise to a detection algorithm that comes with theoretical guarantees and is effective in practice.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2307.10163v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
</script>
</body>
</html>