{"created":"2023-10-30 03:11:30","title":"M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models","abstract":"Managing long sequences has become an important and necessary feature for large language models (LLMs). However, it is still an open question of how to comprehensively and systematically evaluate the long-sequence capability of LLMs. One of the reasons is that conventional and widely-used benchmarks mainly consist of short sequences. In this paper, we propose M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. M4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task types and 12 domains. To alleviate the scarcity of tasks with naturally long sequences and incorporate multiple-ability assessment, we propose an automatic approach (but with negligible human annotations) to convert short-sequence tasks into a unified long-sequence scenario where LLMs have to identify single or multiple relevant spans in long contexts based on explicit or semantic hints. Specifically, the scenario includes five different types of abilities: (1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span; (4) semantic multiple-span; and (5) global context understanding. The resulting samples in M4LE are evenly distributed from 1k to 8k input length. We conducted a systematic evaluation on 11 well-established LLMs, especially those optimized for long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval task is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.","sentences":["Managing long sequences has become an important and necessary feature for large language models (LLMs).","However, it is still an open question of how to comprehensively and systematically evaluate the long-sequence capability of LLMs.","One of the reasons is that conventional and widely-used benchmarks mainly consist of short sequences.","In this paper, we propose M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation.","M4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task types and 12 domains.","To alleviate the scarcity of tasks with naturally long sequences and incorporate multiple-ability assessment, we propose an automatic approach (but with negligible human annotations) to convert short-sequence tasks into a unified long-sequence scenario where LLMs have to identify single or multiple relevant spans in long contexts based on explicit or semantic hints.","Specifically, the scenario includes five different types of abilities: (1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span; (4) semantic multiple-span; and (5) global context understanding.","The resulting samples in M4LE are evenly distributed from 1k to 8k input length.","We conducted a systematic evaluation on 11 well-established LLMs, especially those optimized for long-sequence inputs.","Our results reveal that: 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention.","2) Semantic retrieval task is more difficult for competent LLMs.","3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning.","We make our benchmark publicly available to encourage future research in this challenging area."],"url":"http://arxiv.org/abs/2310.19240v1"}
{"created":"2023-10-30 02:25:21","title":"Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective","abstract":"This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA- 2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.","sentences":["This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs).","For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2.","Our findings reveal that most closed-source LLMs are generally better in terms of performance.","However, much smaller open-source models like LLaMA- 2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios.","Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use.","Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage.","In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost."],"url":"http://arxiv.org/abs/2310.19233v1"}
{"created":"2023-10-30 02:20:44","title":"Adapter Pruning using Tropical Characterization","abstract":"Adapters are widely popular parameter-efficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model. Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.","sentences":["Adapters are widely popular parameter-efficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model.","Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications.","In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules.","We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces.","Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks."],"url":"http://arxiv.org/abs/2310.19232v1"}
{"created":"2023-10-30 02:19:16","title":"There Are No Data Like More Data- Datasets for Deep Learning in Earth Observation","abstract":"Carefully curated and annotated datasets are the foundation of machine learning, with particularly data-hungry deep neural networks forming the core of what is often called Artificial Intelligence (AI). Due to the massive success of deep learning applied to Earth Observation (EO) problems, the focus of the community has been largely on the development of ever-more sophisticated deep neural network architectures and training strategies largely ignoring the overall importance of datasets. For that purpose, numerous task-specific datasets have been created that were largely ignored by previously published review articles on AI for Earth observation. With this article, we want to change the perspective and put machine learning datasets dedicated to Earth observation data and applications into the spotlight. Based on a review of the historical developments, currently available resources are described and a perspective for future developments is formed. We hope to contribute to an understanding that the nature of our data is what distinguishes the Earth observation community from many other communities that apply deep learning techniques to image data, and that a detailed understanding of EO data peculiarities is among the core competencies of our discipline.","sentences":["Carefully curated and annotated datasets are the foundation of machine learning, with particularly data-hungry deep neural networks forming the core of what is often called Artificial Intelligence (AI).","Due to the massive success of deep learning applied to Earth Observation (EO) problems, the focus of the community has been largely on the development of ever-more sophisticated deep neural network architectures and training strategies largely ignoring the overall importance of datasets.","For that purpose, numerous task-specific datasets have been created that were largely ignored by previously published review articles on AI for Earth observation.","With this article, we want to change the perspective and put machine learning datasets dedicated to Earth observation data and applications into the spotlight.","Based on a review of the historical developments, currently available resources are described and a perspective for future developments is formed.","We hope to contribute to an understanding that the nature of our data is what distinguishes the Earth observation community from many other communities that apply deep learning techniques to image data, and that a detailed understanding of EO data peculiarities is among the core competencies of our discipline."],"url":"http://arxiv.org/abs/2310.19231v1"}
{"created":"2023-10-30 02:14:11","title":"Knolling bot 2.0: Enhancing Object Organization with Self-supervised Graspability Estimation","abstract":"Building on recent advancements in transformer based approaches for domestic robots performing knolling, the art of organizing scattered items into neat arrangements. This paper introduces Knolling bot 2.0. Recognizing the challenges posed by piles of objects or items situated closely together, this upgraded system incorporates a self-supervised graspability estimation model. If objects are deemed ungraspable, an additional behavior will be executed to separate the objects before knolling the table. By integrating this grasp prediction mechanism with existing visual perception and transformer based knolling models, an advanced system capable of decluttering and organizing even more complex and densely populated table settings is demonstrated. Experimental evaluations demonstrate the effectiveness of this module, yielding a graspability prediction accuracy of 95.7%.","sentences":["Building on recent advancements in transformer based approaches for domestic robots performing knolling, the art of organizing scattered items into neat arrangements.","This paper introduces Knolling bot 2.0.","Recognizing the challenges posed by piles of objects or items situated closely together, this upgraded system incorporates a self-supervised graspability estimation model.","If objects are deemed ungraspable, an additional behavior will be executed to separate the objects before knolling the table.","By integrating this grasp prediction mechanism with existing visual perception and transformer based knolling models, an advanced system capable of decluttering and organizing even more complex and densely populated table settings is demonstrated.","Experimental evaluations demonstrate the effectiveness of this module, yielding a graspability prediction accuracy of 95.7%."],"url":"http://arxiv.org/abs/2310.19226v1"}
{"created":"2023-10-30 02:04:20","title":"Stochastic Configuration Machines: FPGA Implementation","abstract":"Neural networks for industrial applications generally have additional constraints such as response speed, memory size and power usage. Randomized learners can address some of these issues. However, hardware solutions can provide better resource reduction whilst maintaining the model's performance. Stochastic configuration networks (SCNs) are a prime choice in industrial applications due to their merits and feasibility for data modelling. Stochastic Configuration Machines (SCMs) extend this to focus on reducing the memory constraints by limiting the randomized weights to a binary value with a scalar for each node and using a mechanism model to improve the learning performance and result interpretability. This paper aims to implement SCM models on a field programmable gate array (FPGA) and introduce binary-coded inputs to the algorithm. Results are reported for two benchmark and two industrial datasets, including SCM with single-layer and deep architectures.","sentences":["Neural networks for industrial applications generally have additional constraints such as response speed, memory size and power usage.","Randomized learners can address some of these issues.","However, hardware solutions can provide better resource reduction whilst maintaining the model's performance.","Stochastic configuration networks (SCNs) are a prime choice in industrial applications due to their merits and feasibility for data modelling.","Stochastic Configuration Machines (SCMs) extend this to focus on reducing the memory constraints by limiting the randomized weights to a binary value with a scalar for each node and using a mechanism model to improve the learning performance and result interpretability.","This paper aims to implement SCM models on a field programmable gate array (FPGA) and introduce binary-coded inputs to the algorithm.","Results are reported for two benchmark and two industrial datasets, including SCM with single-layer and deep architectures."],"url":"http://arxiv.org/abs/2310.19225v1"}
{"created":"2023-10-30 02:03:28","title":"CHAMMI: A benchmark for channel-adaptive models in microscopy imaging","abstract":"Most neural networks assume that input images have a fixed number of channels (three for RGB images). However, there are many settings where the number of channels may vary, such as microscopy images where the number of channels changes depending on instruments and experimental goals. Yet, there has not been a systemic attempt to create and evaluate neural networks that are invariant to the number and type of channels. As a result, trained models remain specific to individual studies and are hardly reusable for other microscopy settings. In this paper, we present a benchmark for investigating channel-adaptive models in microscopy imaging, which consists of 1) a dataset of varied-channel single-cell images, and 2) a biologically relevant evaluation framework. In addition, we adapted several existing techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models. We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. We contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an evaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate objective comparisons in future research and applications.","sentences":["Most neural networks assume that input images have a fixed number of channels (three for RGB images).","However, there are many settings where the number of channels may vary, such as microscopy images where the number of channels changes depending on instruments and experimental goals.","Yet, there has not been a systemic attempt to create and evaluate neural networks that are invariant to the number and type of channels.","As a result, trained models remain specific to individual studies and are hardly reusable for other microscopy settings.","In this paper, we present a benchmark for investigating channel-adaptive models in microscopy imaging, which consists of 1) a dataset of varied-channel single-cell images, and 2) a biologically relevant evaluation framework.","In addition, we adapted several existing techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models.","We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient.","We contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an evaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate objective comparisons in future research and applications."],"url":"http://arxiv.org/abs/2310.19224v1"}
{"created":"2023-10-30 02:01:49","title":"Modular Anti-noise Deep Learning Network for Robotic Grasp Detection Based on RGB Images","abstract":"While traditional methods relies on depth sensors, the current trend leans towards utilizing cost-effective RGB images, despite their absence of depth cues. This paper introduces an interesting approach to detect grasping pose from a single RGB image. To this end, we propose a modular learning network augmented with grasp detection and semantic segmentation, tailored for robots equipped with parallel-plate grippers. Our network not only identifies graspable objects but also fuses prior grasp analyses with semantic segmentation, thereby boosting grasp detection precision. Significantly, our design exhibits resilience, adeptly handling blurred and noisy visuals. Key contributions encompass a trainable network for grasp detection from RGB images, a modular design facilitating feasible grasp implementation, and an architecture robust against common image distortions. We demonstrate the feasibility and accuracy of our proposed approach through practical experiments and evaluations.","sentences":["While traditional methods relies on depth sensors, the current trend leans towards utilizing cost-effective RGB images, despite their absence of depth cues.","This paper introduces an interesting approach to detect grasping pose from a single RGB image.","To this end, we propose a modular learning network augmented with grasp detection and semantic segmentation, tailored for robots equipped with parallel-plate grippers.","Our network not only identifies graspable objects but also fuses prior grasp analyses with semantic segmentation, thereby boosting grasp detection precision.","Significantly, our design exhibits resilience, adeptly handling blurred and noisy visuals.","Key contributions encompass a trainable network for grasp detection from RGB images, a modular design facilitating feasible grasp implementation, and an architecture robust against common image distortions.","We demonstrate the feasibility and accuracy of our proposed approach through practical experiments and evaluations."],"url":"http://arxiv.org/abs/2310.19223v1"}
{"created":"2023-10-30 02:01:48","title":"Maximum Knowledge Orthogonality Reconstruction with Gradients in Federated Learning","abstract":"Federated learning (FL) aims at keeping client data local to preserve privacy. Instead of gathering the data itself, the server only collects aggregated gradient updates from clients. Following the popularity of FL, there has been considerable amount of work, revealing the vulnerability of FL approaches by reconstructing the input data from gradient updates. Yet, most existing works assume an FL setting with unrealistically small batch size, and have poor image quality when the batch size is large. Other works modify the neural network architectures or parameters to the point of being suspicious, and thus, can be detected by clients. Moreover, most of them can only reconstruct one sample input from a large batch. To address these limitations, we propose a novel and completely analytical approach, referred to as the maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients' input data. Our proposed method reconstructs a mathematically proven high quality image from large batches. MKOR only requires the server to send secretly modified parameters to clients and can efficiently and inconspicuously reconstruct the input images from clients' gradient updates. We evaluate MKOR's performance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with the state-of-the-art works. The results show that MKOR outperforms the existing approaches, and draws attention to a pressing need for further research on the privacy protection of FL so that comprehensive defense approaches can be developed.","sentences":["Federated learning (FL) aims at keeping client data local to preserve privacy.","Instead of gathering the data itself, the server only collects aggregated gradient updates from clients.","Following the popularity of FL, there has been considerable amount of work, revealing the vulnerability of FL approaches by reconstructing the input data from gradient updates.","Yet, most existing works assume an FL setting with unrealistically small batch size, and have poor image quality when the batch size is large.","Other works modify the neural network architectures or parameters to the point of being suspicious, and thus, can be detected by clients.","Moreover, most of them can only reconstruct one sample input from a large batch.","To address these limitations, we propose a novel and completely analytical approach, referred to as the maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients' input data.","Our proposed method reconstructs a mathematically proven high quality image from large batches.","MKOR only requires the server to send secretly modified parameters to clients and can efficiently and inconspicuously reconstruct the input images from clients' gradient updates.","We evaluate MKOR's performance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with the state-of-the-art works.","The results show that MKOR outperforms the existing approaches, and draws attention to a pressing need for further research on the privacy protection of FL so that comprehensive defense approaches can be developed."],"url":"http://arxiv.org/abs/2310.19222v1"}
{"created":"2023-10-30 01:53:37","title":"From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals","abstract":"The dynamic pricing problem has been extensively studied under the \\textbf{stream} model: A stream of customers arrives sequentially, each with an independently and identically distributed valuation. However, this formulation is not entirely reflective of the real world. In many scenarios, high-valuation customers tend to make purchases earlier and leave the market, leading to a \\emph{shift} in the valuation distribution. Thus motivated, we consider a model where a \\textbf{pool} of $n$ non-strategic unit-demand customers interact repeatedly with the seller. Each customer monitors the price intermittently according to an independent Poisson process and makes a purchase if the observed price is lower than her \\emph{private} valuation, whereupon she leaves the market permanently. We present a minimax \\emph{optimal} algorithm that efficiently computes a non-adaptive policy which guarantees a $1/k$ fraction of the optimal revenue, given any set of $k$ prices. Moreover, we present an adaptive \\emph{learn-then-earn} policy based on a novel \\emph{debiasing} approach, and prove an $\\tilde O(kn^{3/4})$ regret bound. We further improve the bound to $\\tilde O(k^{3/4} n^{3/4})$ using martingale concentration inequalities.","sentences":["The dynamic pricing problem has been extensively studied under the \\textbf{stream} model: A stream of customers arrives sequentially, each with an independently and identically distributed valuation.","However, this formulation is not entirely reflective of the real world.","In many scenarios, high-valuation customers tend to make purchases earlier and leave the market, leading to a \\emph{shift} in the valuation distribution.","Thus motivated, we consider a model where a \\textbf{pool} of $n$ non-strategic unit-demand customers interact repeatedly with the seller.","Each customer monitors the price intermittently according to an independent Poisson process and makes a purchase if the observed price is lower than her \\emph{private} valuation, whereupon she leaves the market permanently.","We present a minimax \\emph{optimal} algorithm that efficiently computes a non-adaptive policy which guarantees a $1/k$ fraction of the optimal revenue, given any set of $k$ prices.","Moreover, we present an adaptive \\emph{learn-then-earn} policy based on a novel \\emph{debiasing} approach, and prove an $\\tilde O(kn^{3/4})$ regret bound.","We further improve the bound to $\\tilde O(k^{3/4} n^{3/4})$ using martingale concentration inequalities."],"url":"http://arxiv.org/abs/2310.19220v1"}
{"created":"2023-10-30 01:34:33","title":"A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions","abstract":"With the development of trustworthy Federated Learning (FL), the requirement of implementing right to be forgotten gives rise to the area of Federated Unlearning (FU). Comparing to machine unlearning, a major challenge of FU lies in the decentralized and privacy-preserving nature of FL, in which clients jointly train a global model without sharing their raw data, making it substantially more intricate to selectively unlearn specific information. In that regard, many efforts have been made to tackle the challenges of FU and have achieved significant progress. In this paper, we present a comprehensive survey of FU. Specially, we provide the existing algorithms, objectives, evaluation metrics, and identify some challenges of FU. By reviewing and comparing some studies, we summarize them into a taxonomy for various schemes, potential applications and future directions.","sentences":["With the development of trustworthy Federated Learning (FL), the requirement of implementing right to be forgotten gives rise to the area of Federated Unlearning (FU).","Comparing to machine unlearning, a major challenge of FU lies in the decentralized and privacy-preserving nature of FL, in which clients jointly train a global model without sharing their raw data, making it substantially more intricate to selectively unlearn specific information.","In that regard, many efforts have been made to tackle the challenges of FU and have achieved significant progress.","In this paper, we present a comprehensive survey of FU.","Specially, we provide the existing algorithms, objectives, evaluation metrics, and identify some challenges of FU.","By reviewing and comparing some studies, we summarize them into a taxonomy for various schemes, potential applications and future directions."],"url":"http://arxiv.org/abs/2310.19218v1"}
{"created":"2023-10-30 01:03:47","title":"Optimal Status Updates for Minimizing Age of Correlated Information in IoT Networks with Energy Harvesting Sensors","abstract":"Many real-time applications of the Internet of Things (IoT) need to deal with correlated information generated by multiple sensors. The design of efficient status update strategies that minimize the Age of Correlated Information (AoCI) is a key factor. In this paper, we consider an IoT network consisting of sensors equipped with the energy harvesting (EH) capability. We optimize the average AoCI at the data fusion center (DFC) by appropriately managing the energy harvested by sensors, whose true battery states are unobservable during the decision-making process. Particularly, we first formulate the dynamic status update procedure as a partially observable Markov decision process (POMDP), where the environmental dynamics are unknown to the DFC. In order to address the challenges arising from the causality of energy usage, unknown environmental dynamics, unobservability of sensors'true battery states, and large-scale discrete action space, we devise a deep reinforcement learning (DRL)-based dynamic status update algorithm. The algorithm leverages the advantages of the soft actor-critic and long short-term memory techniques. Meanwhile, it incorporates our proposed action decomposition and mapping mechanism. Extensive simulations are conducted to validate the effectiveness of our proposed algorithm by comparing it with available DRL algorithms for POMDPs.","sentences":["Many real-time applications of the Internet of Things (IoT) need to deal with correlated information generated by multiple sensors.","The design of efficient status update strategies that minimize the Age of Correlated Information (AoCI) is a key factor.","In this paper, we consider an IoT network consisting of sensors equipped with the energy harvesting (EH) capability.","We optimize the average AoCI at the data fusion center (DFC) by appropriately managing the energy harvested by sensors, whose true battery states are unobservable during the decision-making process.","Particularly, we first formulate the dynamic status update procedure as a partially observable Markov decision process (POMDP), where the environmental dynamics are unknown to the DFC.","In order to address the challenges arising from the causality of energy usage, unknown environmental dynamics, unobservability of sensors'true battery states, and large-scale discrete action space, we devise a deep reinforcement learning (DRL)-based dynamic status update algorithm.","The algorithm leverages the advantages of the soft actor-critic and long short-term memory techniques.","Meanwhile, it incorporates our proposed action decomposition and mapping mechanism.","Extensive simulations are conducted to validate the effectiveness of our proposed algorithm by comparing it with available DRL algorithms for POMDPs."],"url":"http://arxiv.org/abs/2310.19216v1"}
{"created":"2023-10-30 01:01:15","title":"On the accuracy and efficiency of group-wise clipping in differentially private optimization","abstract":"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","sentences":["Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters.","In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization.","We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity).","We formalize this trade-off through our convergence theory and complexity analysis.","Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains.","Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously."],"url":"http://arxiv.org/abs/2310.19215v1"}
{"created":"2023-10-30 00:46:03","title":"EHRTutor: Enhancing Patient Understanding of Discharge Instructions","abstract":"Large language models have shown success as a tutor in education in various fields. Educating patients about their clinical visits plays a pivotal role in patients' adherence to their treatment plans post-discharge. This paper presents EHRTutor, an innovative multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering. EHRTutor first formulates questions pertaining to the electronic health record discharge instructions. It then educates the patient through conversation by administering each question as a test. Finally, it generates a summary at the end of the conversation. Evaluation results using LLMs and domain experts have shown a clear preference for EHRTutor over the baseline. Moreover, EHRTutor also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training.","sentences":["Large language models have shown success as a tutor in education in various fields.","Educating patients about their clinical visits plays a pivotal role in patients' adherence to their treatment plans post-discharge.","This paper presents EHRTutor, an innovative multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering.","EHRTutor first formulates questions pertaining to the electronic health record discharge instructions.","It then educates the patient through conversation by administering each question as a test.","Finally, it generates a summary at the end of the conversation.","Evaluation results using LLMs and domain experts have shown a clear preference for EHRTutor over the baseline.","Moreover, EHRTutor also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training."],"url":"http://arxiv.org/abs/2310.19212v1"}
{"created":"2023-10-30 00:45:05","title":"Investigative Pattern Detection Framework for Counterterrorism","abstract":"Law-enforcement investigations aimed at preventing attacks by violent extremists have become increasingly important for public safety. The problem is exacerbated by the massive data volumes that need to be scanned to identify complex behaviors of extremists and groups. Automated tools are required to extract information to respond queries from analysts, continually scan new information, integrate them with past events, and then alert about emerging threats. We address challenges in investigative pattern detection and develop an Investigative Pattern Detection Framework for Counterterrorism (INSPECT). The framework integrates numerous computing tools that include machine learning techniques to identify behavioral indicators and graph pattern matching techniques to detect risk profiles/groups. INSPECT also automates multiple tasks for large-scale mining of detailed forensic biographies, forming knowledge networks, and querying for behavioral indicators and radicalization trajectories. INSPECT targets human-in-the-loop mode of investigative search and has been validated and evaluated using an evolving dataset on domestic jihadism.","sentences":["Law-enforcement investigations aimed at preventing attacks by violent extremists have become increasingly important for public safety.","The problem is exacerbated by the massive data volumes that need to be scanned to identify complex behaviors of extremists and groups.","Automated tools are required to extract information to respond queries from analysts, continually scan new information, integrate them with past events, and then alert about emerging threats.","We address challenges in investigative pattern detection and develop an Investigative Pattern Detection Framework for Counterterrorism (INSPECT).","The framework integrates numerous computing tools that include machine learning techniques to identify behavioral indicators and graph pattern matching techniques to detect risk profiles/groups.","INSPECT also automates multiple tasks for large-scale mining of detailed forensic biographies, forming knowledge networks, and querying for behavioral indicators and radicalization trajectories.","INSPECT targets human-in-the-loop mode of investigative search and has been validated and evaluated using an evolving dataset on domestic jihadism."],"url":"http://arxiv.org/abs/2310.19211v1"}
{"created":"2023-10-30 00:32:47","title":"Generalized Category Discovery with Clustering Assignment Consistency","abstract":"Generalized category discovery (GCD) is a recently proposed open-world task. Given a set of images consisting of labeled and unlabeled instances, the goal of GCD is to automatically cluster the unlabeled samples using information transferred from the labeled dataset. The unlabeled dataset comprises both known and novel classes. The main challenge is that unlabeled novel class samples and unlabeled known class samples are mixed together in the unlabeled dataset. To address the GCD without knowing the class number of unlabeled dataset, we propose a co-training-based framework that encourages clustering consistency. Specifically, we first introduce weak and strong augmentation transformations to generate two sufficiently different views for the same sample. Then, based on the co-training assumption, we propose a consistency representation learning strategy, which encourages consistency between feature-prototype similarity and clustering assignment. Finally, we use the discriminative embeddings learned from the semi-supervised representation learning process to construct an original sparse network and use a community detection method to obtain the clustering results and the number of categories simultaneously. Extensive experiments show that our method achieves state-of-the-art performance on three generic benchmarks and three fine-grained visual recognition datasets. Especially in the ImageNet-100 data set, our method significantly exceeds the best baseline by 15.5\\% and 7.0\\% on the \\texttt{Novel} and \\texttt{All} classes, respectively.","sentences":["Generalized category discovery (GCD) is a recently proposed open-world task.","Given a set of images consisting of labeled and unlabeled instances, the goal of GCD is to automatically cluster the unlabeled samples using information transferred from the labeled dataset.","The unlabeled dataset comprises both known and novel classes.","The main challenge is that unlabeled novel class samples and unlabeled known class samples are mixed together in the unlabeled dataset.","To address the GCD without knowing the class number of unlabeled dataset, we propose a co-training-based framework that encourages clustering consistency.","Specifically, we first introduce weak and strong augmentation transformations to generate two sufficiently different views for the same sample.","Then, based on the co-training assumption, we propose a consistency representation learning strategy, which encourages consistency between feature-prototype similarity and clustering assignment.","Finally, we use the discriminative embeddings learned from the semi-supervised representation learning process to construct an original sparse network and use a community detection method to obtain the clustering results and the number of categories simultaneously.","Extensive experiments show that our method achieves state-of-the-art performance on three generic benchmarks and three fine-grained visual recognition datasets.","Especially in the ImageNet-100 data set, our method significantly exceeds the best baseline by 15.5\\% and 7.0\\% on the \\texttt{Novel} and \\texttt{All} classes, respectively."],"url":"http://arxiv.org/abs/2310.19210v1"}
{"created":"2023-10-30 00:30:34","title":"LitCab: Lightweight Calibration of Language Models on Outputs of Varied Lengths","abstract":"A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations, a common issue of LMs, as well as building more trustworthy models. Yet, popular neural model calibration techniques are not well-suited for LMs due to their lack of flexibility in discerning answer correctness and their high computational costs. For instance, post-processing methods like temperature scaling are often unable to reorder the candidate generations. Moreover, training-based methods require finetuning the entire model, which is impractical due to the increasing sizes of modern LMs. In this paper, we present LitCab, a lightweight calibration mechanism consisting of a single linear layer taking the input text representation and manipulateing the LM output logits. LitCab improves model calibration by only adding < 2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of 7 text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, by reducing the average ECE score by 20%. We further conduct a comprehensive evaluation with 7 popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (1) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (2) GPT-family models show superior calibration compared to LLaMA, Llama2 and Vicuna models despite having much fewer parameters. (3) Finetuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of finetuning setups for calibrating LMs.","sentences":["A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct.","Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations, a common issue of LMs, as well as building more trustworthy models.","Yet, popular neural model calibration techniques are not well-suited for LMs due to their lack of flexibility in discerning answer correctness and their high computational costs.","For instance, post-processing methods like temperature scaling are often unable to reorder the candidate generations.","Moreover, training-based methods require finetuning the entire model, which is impractical due to the increasing sizes of modern LMs.","In this paper, we present LitCab, a lightweight calibration mechanism consisting of a single linear layer taking the input text representation and manipulateing the LM output logits.","LitCab improves model calibration by only adding < 2% of the original model parameters.","For evaluation, we construct CaT, a benchmark consisting of 7 text generation tasks, covering responses ranging from short phrases to paragraphs.","We test LitCab with Llama2-7B, where it improves calibration across all tasks, by reducing the average ECE score by 20%.","We further conduct a comprehensive evaluation with 7 popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (1) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones.","(2) GPT-family models show superior calibration compared to LLaMA, Llama2 and Vicuna models despite having much fewer parameters.","(3) Finetuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of finetuning setups for calibrating LMs."],"url":"http://arxiv.org/abs/2310.19208v1"}
{"created":"2023-10-30 00:09:59","title":"Leveraging generative artificial intelligence to simulate student learning behavior","abstract":"Student simulation presents a transformative approach to enhance learning outcomes, advance educational research, and ultimately shape the future of effective pedagogy. We explore the feasibility of using large language models (LLMs), a remarkable achievement in AI, to simulate student learning behaviors. Unlike conventional machine learning based prediction, we leverage LLMs to instantiate virtual students with specific demographics and uncover intricate correlations among learning experiences, course materials, understanding levels, and engagement. Our objective is not merely to predict learning outcomes but to replicate learning behaviors and patterns of real students. We validate this hypothesis through three experiments. The first experiment, based on a dataset of N = 145, simulates student learning outcomes from demographic data, revealing parallels with actual students concerning various demographic factors. The second experiment (N = 4524) results in increasingly realistic simulated behaviors with more assessment history for virtual students modelling. The third experiment (N = 27), incorporating prior knowledge and course interactions, indicates a strong link between virtual students' learning behaviors and fine-grained mappings from test questions, course materials, engagement and understanding levels. Collectively, these findings deepen our understanding of LLMs and demonstrate its viability for student simulation, empowering more adaptable curricula design to enhance inclusivity and educational effectiveness.","sentences":["Student simulation presents a transformative approach to enhance learning outcomes, advance educational research, and ultimately shape the future of effective pedagogy.","We explore the feasibility of using large language models (LLMs), a remarkable achievement in AI, to simulate student learning behaviors.","Unlike conventional machine learning based prediction, we leverage LLMs to instantiate virtual students with specific demographics and uncover intricate correlations among learning experiences, course materials, understanding levels, and engagement.","Our objective is not merely to predict learning outcomes but to replicate learning behaviors and patterns of real students.","We validate this hypothesis through three experiments.","The first experiment, based on a dataset of N = 145, simulates student learning outcomes from demographic data, revealing parallels with actual students concerning various demographic factors.","The second experiment (N = 4524) results in increasingly realistic simulated behaviors with more assessment history for virtual students modelling.","The third experiment (N = 27), incorporating prior knowledge and course interactions, indicates a strong link between virtual students' learning behaviors and fine-grained mappings from test questions, course materials, engagement and understanding levels.","Collectively, these findings deepen our understanding of LLMs and demonstrate its viability for student simulation, empowering more adaptable curricula design to enhance inclusivity and educational effectiveness."],"url":"http://arxiv.org/abs/2310.19206v1"}
{"created":"2023-10-30 00:01:51","title":"Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing","abstract":"While ChatGPT is a well-known artificial intelligence chatbot being used to answer human's questions, one may want to discover its potential in advancing software testing. We examine the capability of ChatGPT in advancing the intelligence of software testing through a case study on metamorphic testing (MT), a state-of-the-art software testing technique. We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify. These MR candidates are then evaluated in terms of correctness by domain experts. We show that ChatGPT can be used to generate new correct MRs to test several software systems. Having said that, the majority of MR candidates are either defined vaguely or incorrect, especially for systems that have never been tested with MT. ChatGPT can be used to advance software testing intelligence by proposing MR candidates that can be later adopted for implementing tests; but human intelligence should still inevitably be involved to justify and rectify their correctness.","sentences":["While ChatGPT is a well-known artificial intelligence chatbot being used to answer human's questions, one may want to discover its potential in advancing software testing.","We examine the capability of ChatGPT in advancing the intelligence of software testing through a case study on metamorphic testing (MT), a state-of-the-art software testing technique.","We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify.","These MR candidates are then evaluated in terms of correctness by domain experts.","We show that ChatGPT can be used to generate new correct MRs to test several software systems.","Having said that, the majority of MR candidates are either defined vaguely or incorrect, especially for systems that have never been tested with MT.","ChatGPT can be used to advance software testing intelligence by proposing MR candidates that can be later adopted for implementing tests; but human intelligence should still inevitably be involved to justify and rectify their correctness."],"url":"http://arxiv.org/abs/2310.19204v1"}
{"created":"2023-10-29 23:48:45","title":"Open Problems in DAOs","abstract":"Decentralized autonomous organizations (DAOs) are a new, rapidly-growing class of organizations governed by smart contracts. Here we describe how researchers can contribute to the emerging science of DAOs and other digitally-constituted organizations. From granular privacy primitives to mechanism designs to model laws, we identify high-impact problems in the DAO ecosystem where existing gaps might be tackled through a new data set or by applying tools and ideas from existing research fields such as political science, computer science, economics, law, and organizational science. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the wider research community to join the global effort to invent the next generation of organizations.","sentences":["Decentralized autonomous organizations (DAOs) are a new, rapidly-growing class of organizations governed by smart contracts.","Here we describe how researchers can contribute to the emerging science of DAOs and other digitally-constituted organizations.","From granular privacy primitives to mechanism designs to model laws, we identify high-impact problems in the DAO ecosystem where existing gaps might be tackled through a new data set or by applying tools and ideas from existing research fields such as political science, computer science, economics, law, and organizational science.","Our recommendations encompass exciting research questions as well as promising business opportunities.","We call on the wider research community to join the global effort to invent the next generation of organizations."],"url":"http://arxiv.org/abs/2310.19201v1"}
{"created":"2023-10-29 23:41:09","title":"Immersive 3D Simulator for Drone-as-a-Service","abstract":"We propose a 3D simulator tailored for the Drone-as-a-Service framework. The simulator enables employing dynamic algorithms for addressing realistic delivery scenarios. We present the simulator's architectural design and its use of an energy consumption model for drone deliveries. We introduce two primary operational modes within the simulator: the edit mode and the runtime mode. Beyond its simulation capabilities, our simulator serves as a valuable data collection resource, facilitating the creation of datasets through simulated scenarios. Our simulator empowers researchers by providing an intuitive platform to visualize and interact with delivery environments. Moreover, it enables rigorous algorithm testing in a safe simulation setting, thus obviating the need for real-world drone deployments. Demo: https://youtu.be/HOLfo1JiFJ0","sentences":["We propose a 3D simulator tailored for the Drone-as-a-Service framework.","The simulator enables employing dynamic algorithms for addressing realistic delivery scenarios.","We present the simulator's architectural design and its use of an energy consumption model for drone deliveries.","We introduce two primary operational modes within the simulator: the edit mode and the runtime mode.","Beyond its simulation capabilities, our simulator serves as a valuable data collection resource, facilitating the creation of datasets through simulated scenarios.","Our simulator empowers researchers by providing an intuitive platform to visualize and interact with delivery environments.","Moreover, it enables rigorous algorithm testing in a safe simulation setting, thus obviating the need for real-world drone deployments.","Demo: https://youtu.be/HOLfo1JiFJ0"],"url":"http://arxiv.org/abs/2310.19199v1"}
{"created":"2023-10-29 23:33:14","title":"Unveiling the Evolution of Mobile Networks: From 1G to 7G","abstract":"The evolution of cellular networks has played a pivotal role in shaping the modern telecommunications landscape. This paper explores the journey of cellular network generations, beginning with the introduction of Japan's first commercial 1G network by Nippon Telegraph and Telephone (NTT) Corporation in 1979. This analog wireless network quickly expanded to become the country's first national 1G network within a remarkably short period.   The transition from analog to digital networks marked a significant turning point in the wireless industry, enabled by advancements in MOSFET (Metal-Oxide-Semiconductor Field Effect Transistor) technology. MOSFET, originally developed at Bell Labs in 1959, underwent modifications to suit cellular networks in the early 1990s, facilitating the shift to digital wireless mobile networks. The advent of the 2G generation brought forth the first commercial digital cellular network in 1991, sparking recognition among manufacturers and mobile network operators of the importance of robust networks and efficient architecture. As the wireless industry continued to experience exponential growth, the significance of effective network infrastructure became increasingly evident.   In this research, our aim is to provide a comprehensive overview of the entire spectrum of cellular network generations, ranging from 1G to the potential future of 7G. By tracing the evolution of these networks, we aim to shed light on the transformative developments that have shaped the telecommunications landscape and explore the possibilities that lie ahead in the realm of cellular technology.","sentences":["The evolution of cellular networks has played a pivotal role in shaping the modern telecommunications landscape.","This paper explores the journey of cellular network generations, beginning with the introduction of Japan's first commercial 1G network by Nippon Telegraph and Telephone (NTT) Corporation in 1979.","This analog wireless network quickly expanded to become the country's first national 1G network within a remarkably short period.   ","The transition from analog to digital networks marked a significant turning point in the wireless industry, enabled by advancements in MOSFET (Metal-Oxide-Semiconductor Field Effect Transistor) technology.","MOSFET, originally developed at Bell Labs in 1959, underwent modifications to suit cellular networks in the early 1990s, facilitating the shift to digital wireless mobile networks.","The advent of the 2G generation brought forth the first commercial digital cellular network in 1991, sparking recognition among manufacturers and mobile network operators of the importance of robust networks and efficient architecture.","As the wireless industry continued to experience exponential growth, the significance of effective network infrastructure became increasingly evident.   ","In this research, our aim is to provide a comprehensive overview of the entire spectrum of cellular network generations, ranging from 1G to the potential future of 7G. By tracing the evolution of these networks, we aim to shed light on the transformative developments that have shaped the telecommunications landscape and explore the possibilities that lie ahead in the realm of cellular technology."],"url":"http://arxiv.org/abs/2310.19195v1"}
{"created":"2023-10-29 23:16:20","title":"A Survey on Watching Social Issue Videos among YouTube and TikTok Users","abstract":"The openness and influence of video-sharing platforms (VSPs) such as YouTube and TikTok attracted creators to share videos on various social issues. Although social issue videos (SIVs) affect public opinions and breed misinformation, how VSP users obtain information and interact with SIVs is under-explored. This work surveyed 659 YouTube and 127 TikTok users to understand the motives for consuming SIVs on VSPs. We found that VSP users are primarily motivated by the information and entertainment gratifications to use the platform. VSP users use SIVs for information-seeking purposes and find YouTube and TikTok convenient to interact with SIVs. VSP users moderately watch SIVs for entertainment and inactively engage in social interactions. SIV consumption is associated with information and socialization gratifications of the platform. VSP users appreciate the diversity of information and opinions but would also do their own research and are concerned about the misinformation and echo chamber problems.","sentences":["The openness and influence of video-sharing platforms (VSPs) such as YouTube and TikTok attracted creators to share videos on various social issues.","Although social issue videos (SIVs) affect public opinions and breed misinformation, how VSP users obtain information and interact with SIVs is under-explored.","This work surveyed 659 YouTube and 127 TikTok users to understand the motives for consuming SIVs on VSPs.","We found that VSP users are primarily motivated by the information and entertainment gratifications to use the platform.","VSP users use SIVs for information-seeking purposes and find YouTube and TikTok convenient to interact with SIVs.","VSP users moderately watch SIVs for entertainment and inactively engage in social interactions.","SIV consumption is associated with information and socialization gratifications of the platform.","VSP users appreciate the diversity of information and opinions but would also do their own research and are concerned about the misinformation and echo chamber problems."],"url":"http://arxiv.org/abs/2310.19193v1"}
{"created":"2023-10-29 23:08:19","title":"3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets","abstract":"We present 3DMiner -- a pipeline for mining 3D shapes from challenging large-scale unannotated image datasets. Unlike other unsupervised 3D reconstruction methods, we assume that, within a large-enough dataset, there must exist images of objects with similar shapes but varying backgrounds, textures, and viewpoints. Our approach leverages the recent advances in learning self-supervised image representations to cluster images with geometrically similar shapes and find common image correspondences between them. We then exploit these correspondences to obtain rough camera estimates as initialization for bundle-adjustment. Finally, for every image cluster, we apply a progressive bundle-adjusting reconstruction method to learn a neural occupancy field representing the underlying shape. We show that this procedure is robust to several types of errors introduced in previous steps (e.g., wrong camera poses, images containing dissimilar shapes, etc.), allowing us to obtain shape and pose annotations for images in-the-wild. When using images from Pix3D chairs, our method is capable of producing significantly better results than state-of-the-art unsupervised 3D reconstruction techniques, both quantitatively and qualitatively. Furthermore, we show how 3DMiner can be applied to in-the-wild data by reconstructing shapes present in images from the LAION-5B dataset. Project Page: https://ttchengab.github.io/3dminerOfficial","sentences":["We present 3DMiner -- a pipeline for mining 3D shapes from challenging large-scale unannotated image datasets.","Unlike other unsupervised 3D reconstruction methods, we assume that, within a large-enough dataset, there must exist images of objects with similar shapes but varying backgrounds, textures, and viewpoints.","Our approach leverages the recent advances in learning self-supervised image representations to cluster images with geometrically similar shapes and find common image correspondences between them.","We then exploit these correspondences to obtain rough camera estimates as initialization for bundle-adjustment.","Finally, for every image cluster, we apply a progressive bundle-adjusting reconstruction method to learn a neural occupancy field representing the underlying shape.","We show that this procedure is robust to several types of errors introduced in previous steps (e.g., wrong camera poses, images containing dissimilar shapes, etc.), allowing us to obtain shape and pose annotations for images in-the-wild.","When using images from Pix3D chairs, our method is capable of producing significantly better results than state-of-the-art unsupervised 3D reconstruction techniques, both quantitatively and qualitatively.","Furthermore, we show how 3DMiner can be applied to in-the-wild data by reconstructing shapes present in images from the LAION-5B dataset.","Project Page: https://ttchengab.github.io/3dminerOfficial"],"url":"http://arxiv.org/abs/2310.19188v1"}
{"created":"2023-10-29 23:07:51","title":"Haptic-Enhanced Virtual Reality Simulator for Robot-Assisted Femur Fracture Surgery","abstract":"In this paper, we develop a virtual reality (VR) simulator for the Robossis robot-assisted femur fracture surgery. Due to the steep learning curve for such procedures, a VR simulator is essential for training surgeon(s) and staff. The Robossis Surgical Simulator (RSS) is designed to immerse user(s) in a realistic surgery setting using the Robossis system as completed in a previous real-world cadaveric procedure. The RSS is designed to interface the Sigma-7 Haptic Controller with the Robossis Surgical Robot (RSR) and the Meta Quest VR headset. Results show that the RSR follows user commands in 6 DOF and prevents the overlapping of bone segments. This development demonstrates a promising avenue for future implementation of the Robossis system.","sentences":["In this paper, we develop a virtual reality (VR) simulator for the Robossis robot-assisted femur fracture surgery.","Due to the steep learning curve for such procedures, a VR simulator is essential for training surgeon(s) and staff.","The Robossis Surgical Simulator (RSS) is designed to immerse user(s) in a realistic surgery setting using the Robossis system as completed in a previous real-world cadaveric procedure.","The RSS is designed to interface the Sigma-7 Haptic Controller with the Robossis Surgical Robot (RSR) and the Meta Quest VR headset.","Results show that the RSR follows user commands in 6 DOF and prevents the overlapping of bone segments.","This development demonstrates a promising avenue for future implementation of the Robossis system."],"url":"http://arxiv.org/abs/2310.19187v1"}
{"created":"2023-10-29 23:04:09","title":"Robotic Barrier Construction through Weaved, Inflatable Tubes","abstract":"In this article, we present a mechanism and related path planning algorithm to construct light-duty barriers out of extruded, inflated tubes weaved around existing environmental features. Our extruded tubes are based on everted vine-robots and in this context, we present a new method to steer their growth. We characterize the mechanism in terms of accuracy resilience, and, towards their use as barriers, the ability of the tubes to withstand distributed loads. We further explore an algorithm which, given a feature map and the size and direction of the external load, can determine where and how to extrude the barrier. Finally, we showcase the potential of this method in an autonomously extruded two-layer wall weaved around three pipes. While preliminary, our work indicates that this method has the potential for barrier construction in cluttered environments, e.g. shelters against wind or snow. Future work may show how to achieve tighter weaves, how to leverage weave friction for improved strength, how to assess barrier performance for feedback control, and how to operate the extrusion mechanism off of a mobile robot.","sentences":["In this article, we present a mechanism and related path planning algorithm to construct light-duty barriers out of extruded, inflated tubes weaved around existing environmental features.","Our extruded tubes are based on everted vine-robots and in this context, we present a new method to steer their growth.","We characterize the mechanism in terms of accuracy resilience, and, towards their use as barriers, the ability of the tubes to withstand distributed loads.","We further explore an algorithm which, given a feature map and the size and direction of the external load, can determine where and how to extrude the barrier.","Finally, we showcase the potential of this method in an autonomously extruded two-layer wall weaved around three pipes.","While preliminary, our work indicates that this method has the potential for barrier construction in cluttered environments, e.g. shelters against wind or snow.","Future work may show how to achieve tighter weaves, how to leverage weave friction for improved strength, how to assess barrier performance for feedback control, and how to operate the extrusion mechanism off of a mobile robot."],"url":"http://arxiv.org/abs/2310.19185v1"}
{"created":"2023-10-29 22:52:43","title":"Fast Trainable Projection for Robust Fine-Tuning","abstract":"Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. Recently, projected gradient descent has been successfully used in robust fine-tuning by constraining the deviation from the initialization of the fine-tuned model explicitly through projection. However, algorithmically, two limitations prevent this method from being adopted more widely, scalability and efficiency. In this paper, we propose a new projection-based fine-tuning algorithm, Fast Trainable Projection (FTP) for computationally efficient learning of per-layer projection constraints, resulting in an average $35\\%$ speedup on our benchmarks compared to prior works. FTP can be combined with existing optimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we show that FTP is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner through nested differentiation. Empirically, we show superior robustness on OOD datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, we demonstrate that FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings thanks to its easy adaptability. The code will be available at https://github.com/GT-RIPL/FTP.git.","sentences":["Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task.","Recently, projected gradient descent has been successfully used in robust fine-tuning by constraining the deviation from the initialization of the fine-tuned model explicitly through projection.","However, algorithmically, two limitations prevent this method from being adopted more widely, scalability and efficiency.","In this paper, we propose a new projection-based fine-tuning algorithm, Fast Trainable Projection (FTP) for computationally efficient learning of per-layer projection constraints, resulting in an average $35\\%$ speedup on our benchmarks compared to prior works.","FTP can be combined with existing optimizers such as AdamW, and be used in a plug-and-play fashion.","Finally, we show that FTP is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner through nested differentiation.","Empirically, we show superior robustness on OOD datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models.","Additionally, we demonstrate that FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings thanks to its easy adaptability.","The code will be available at https://github.com/GT-RIPL/FTP.git."],"url":"http://arxiv.org/abs/2310.19182v1"}
{"created":"2023-10-29 22:52:40","title":"From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude","abstract":"The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation. However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks. This study explores the potential of using four popular commercially available LLMs - ChatGPT (GPT 3.5 Turbo), GPT 4, Claude and Bard to generate functional phishing attacks using a series of malicious prompts. We discover that these LLMs can generate both phishing emails and websites that can convincingly imitate well-known brands, and also deploy a range of evasive tactics for the latter to elude detection mechanisms employed by anti-phishing systems. Notably, these attacks can be generated using unmodified, or \"vanilla,\" versions of these LLMs, without requiring any prior adversarial exploits such as jailbreaking. As a countermeasure, we build a BERT based automated detection tool that can be used for the early detection of malicious prompts to prevent LLMs from generating phishing content attaining an accuracy of 97\\% for phishing website prompts, and 94\\% for phishing email prompts.","sentences":["The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation.","However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks.","This study explores the potential of using four popular commercially available LLMs - ChatGPT (GPT 3.5 Turbo), GPT 4, Claude and Bard to generate functional phishing attacks using a series of malicious prompts.","We discover that these LLMs can generate both phishing emails and websites that can convincingly imitate well-known brands, and also deploy a range of evasive tactics for the latter to elude detection mechanisms employed by anti-phishing systems.","Notably, these attacks can be generated using unmodified, or \"vanilla,\" versions of these LLMs, without requiring any prior adversarial exploits such as jailbreaking.","As a countermeasure, we build a BERT based automated detection tool that can be used for the early detection of malicious prompts to prevent LLMs from generating phishing content attaining an accuracy of 97\\% for phishing website prompts, and 94\\% for phishing email prompts."],"url":"http://arxiv.org/abs/2310.19181v1"}
{"created":"2023-10-29 22:51:49","title":"JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation","abstract":"With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \\textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible generation of multi-track combinations. During the inference, users have the ability to iteratively produce and choose music tracks that meet their preferences, subsequently creating an entire musical composition incrementally following the proposed Human-AI co-composition workflow. Quantitative and qualitative assessments demonstrate state-of-the-art performance in controllable and high-fidelity multi-track music synthesis. The proposed JEN-1 Composer represents a significant advance toward interactive AI-facilitated music creation and composition. Demos will be available at https://jenmusic.ai/audio-demos.","sentences":["With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch.","However, finer-grained control over multi-track generation remains an open challenge.","Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers.","To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model.","JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \\textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation.","We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible generation of multi-track combinations.","During the inference, users have the ability to iteratively produce and choose music tracks that meet their preferences, subsequently creating an entire musical composition incrementally following the proposed Human-AI co-composition workflow.","Quantitative and qualitative assessments demonstrate state-of-the-art performance in controllable and high-fidelity multi-track music synthesis.","The proposed JEN-1 Composer represents a significant advance toward interactive AI-facilitated music creation and composition.","Demos will be available at https://jenmusic.ai/audio-demos."],"url":"http://arxiv.org/abs/2310.19180v1"}
{"created":"2023-10-29 22:37:54","title":"Robustifying Language Models with Test-Time Adaptation","abstract":"Large-scale language models achieved state-of-the-art performance over a number of language tasks. However, they fail on adversarial language examples, which are sentences optimized to fool the language models but with similar semantic meanings for humans. While prior work focuses on making the language model robust at training time, retraining for robustness is often unrealistic for large-scale foundation models. Instead, we propose to make the language models robust at test time. By dynamically adapting the input sentence with predictions from masked words, we show that we can reverse many language adversarial attacks. Since our approach does not require any training, it works for novel tasks at test time and can adapt to novel adversarial corruptions. Visualizations and empirical results on two popular sentence classification datasets demonstrate that our method can repair adversarial language attacks over 65% o","sentences":["Large-scale language models achieved state-of-the-art performance over a number of language tasks.","However, they fail on adversarial language examples, which are sentences optimized to fool the language models but with similar semantic meanings for humans.","While prior work focuses on making the language model robust at training time, retraining for robustness is often unrealistic for large-scale foundation models.","Instead, we propose to make the language models robust at test time.","By dynamically adapting the input sentence with predictions from masked words, we show that we can reverse many language adversarial attacks.","Since our approach does not require any training, it works for novel tasks at test time and can adapt to novel adversarial corruptions.","Visualizations and empirical results on two popular sentence classification datasets demonstrate that our method can repair adversarial language attacks over 65% o"],"url":"http://arxiv.org/abs/2310.19177v1"}
{"created":"2023-10-29 22:31:20","title":"Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI","abstract":"Machine learning offers great potential for automated prediction of post-stroke symptoms and their response to rehabilitation. Major challenges for this endeavour include the very high dimensionality of neuroimaging data, the relatively small size of the datasets available for learning, and how to effectively combine neuroimaging and tabular data (e.g. demographic information and clinical characteristics). This paper evaluates several solutions based on two strategies. The first is to use 2D images that summarise MRI scans. The second is to select key features that improve classification accuracy. Additionally, we introduce the novel approach of training a convolutional neural network (CNN) on images that combine regions-of-interest extracted from MRIs, with symbolic representations of tabular data. We evaluate a series of CNN architectures (both 2D and a 3D) that are trained on different representations of MRI and tabular data, to predict whether a composite measure of post-stroke spoken picture description ability is in the aphasic or non-aphasic range. MRI and tabular data were acquired from 758 English speaking stroke survivors who participated in the PLORAS study. The classification accuracy for a baseline logistic regression was 0.678 for lesion size alone, rising to 0.757 and 0.813 when initial symptom severity and recovery time were successively added. The highest classification accuracy 0.854 was observed when 8 regions-of-interest was extracted from each MRI scan and combined with lesion size, initial severity and recovery time in a 2D Residual Neural Network.Our findings demonstrate how imaging and tabular data can be combined for high post-stroke classification accuracy, even when the dataset is small in machine learning terms. We conclude by proposing how the current models could be improved to achieve even higher levels of accuracy using images from hospital scanners.","sentences":["Machine learning offers great potential for automated prediction of post-stroke symptoms and their response to rehabilitation.","Major challenges for this endeavour include the very high dimensionality of neuroimaging data, the relatively small size of the datasets available for learning, and how to effectively combine neuroimaging and tabular data (e.g. demographic information and clinical characteristics).","This paper evaluates several solutions based on two strategies.","The first is to use 2D images that summarise MRI scans.","The second is to select key features that improve classification accuracy.","Additionally, we introduce the novel approach of training a convolutional neural network (CNN) on images that combine regions-of-interest extracted from MRIs, with symbolic representations of tabular data.","We evaluate a series of CNN architectures (both 2D and a 3D) that are trained on different representations of MRI and tabular data, to predict whether a composite measure of post-stroke spoken picture description ability is in the aphasic or non-aphasic range.","MRI and tabular data were acquired from 758 English speaking stroke survivors who participated in the PLORAS study.","The classification accuracy for a baseline logistic regression was 0.678 for lesion size alone, rising to 0.757 and 0.813 when initial symptom severity and recovery time were successively added.","The highest classification accuracy 0.854 was observed when 8 regions-of-interest was extracted from each MRI scan and combined with lesion size, initial severity and recovery time in a 2D Residual Neural Network.","Our findings demonstrate how imaging and tabular data can be combined for high post-stroke classification accuracy, even when the dataset is small in machine learning terms.","We conclude by proposing how the current models could be improved to achieve even higher levels of accuracy using images from hospital scanners."],"url":"http://arxiv.org/abs/2310.19174v1"}
{"created":"2023-10-29 22:08:00","title":"BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping","abstract":"We propose a metadata-aware self-supervised learning~(SSL)~framework useful for fine-grained classification and ecological mapping of bird species around the world. Our framework unifies two SSL strategies: Contrastive Learning~(CL) and Masked Image Modeling~(MIM), while also enriching the embedding space with metadata available with ground-level imagery of birds. We separately train uni-modal and cross-modal ViT on a novel cross-view global bird species dataset containing ground-level imagery, metadata (location, time), and corresponding satellite imagery. We demonstrate that our models learn fine-grained and geographically conditioned features of birds, by evaluating on two downstream tasks: fine-grained visual classification~(FGVC) and cross-modal retrieval. Pre-trained models learned using our framework achieve SotA performance on FGVC of iNAT-2021 birds and in transfer learning settings for CUB-200-2011 and NABirds datasets. Moreover, the impressive cross-modal retrieval performance of our model enables the creation of species distribution maps across any geographic region. The dataset and source code will be released at https://github.com/mvrl/BirdSAT}.","sentences":["We propose a metadata-aware self-supervised learning~(SSL)~framework useful for fine-grained classification and ecological mapping of bird species around the world.","Our framework unifies two SSL strategies: Contrastive Learning~(CL) and Masked Image Modeling~(MIM), while also enriching the embedding space with metadata available with ground-level imagery of birds.","We separately train uni-modal and cross-modal ViT on a novel cross-view global bird species dataset containing ground-level imagery, metadata (location, time), and corresponding satellite imagery.","We demonstrate that our models learn fine-grained and geographically conditioned features of birds, by evaluating on two downstream tasks: fine-grained visual classification~(FGVC) and cross-modal retrieval.","Pre-trained models learned using our framework achieve SotA performance on FGVC of iNAT-2021 birds and in transfer learning settings for CUB-200-2011 and NABirds datasets.","Moreover, the impressive cross-modal retrieval performance of our model enables the creation of species distribution maps across any geographic region.","The dataset and source code will be released at https://github.com/mvrl/BirdSAT}."],"url":"http://arxiv.org/abs/2310.19168v1"}
{"created":"2023-10-29 21:59:33","title":"Rare Event Probability Learning by Normalizing Flows","abstract":"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing $10$ distinct test cases, which highlight NOFIS's superiority over baseline approaches.","sentences":["A rare event is defined by a low probability of occurrence.","Accurate estimation of such small probabilities is of utmost importance across diverse domains.","Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates.","Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS.","NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses.","Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal.","The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing $10$ distinct test cases, which highlight NOFIS's superiority over baseline approaches."],"url":"http://arxiv.org/abs/2310.19167v1"}
{"created":"2023-10-29 21:56:22","title":"The Power of Explainability in Forecast-Informed Deep Learning Models for Flood Mitigation","abstract":"Floods can cause horrific harm to life and property. However, they can be mitigated or even avoided by the effective use of hydraulic structures such as dams, gates, and pumps. By pre-releasing water via these structures in advance of extreme weather events, water levels are sufficiently lowered to prevent floods. In this work, we propose FIDLAR, a Forecast Informed Deep Learning Architecture, achieving flood management in watersheds with hydraulic structures in an optimal manner by balancing out flood mitigation and unnecessary wastage of water via pre-releases. We perform experiments with FIDLAR using data from the South Florida Water Management District, which manages a coastal area that is highly prone to frequent storms and floods. Results show that FIDLAR performs better than the current state-of-the-art with several orders of magnitude speedup and with provably better pre-release schedules. The dramatic speedups make it possible for FIDLAR to be used for real-time flood management. The main contribution of this paper is the effective use of tools for model explainability, allowing us to understand the contribution of the various environmental factors towards its decisions.","sentences":["Floods can cause horrific harm to life and property.","However, they can be mitigated or even avoided by the effective use of hydraulic structures such as dams, gates, and pumps.","By pre-releasing water via these structures in advance of extreme weather events, water levels are sufficiently lowered to prevent floods.","In this work, we propose FIDLAR, a Forecast Informed Deep Learning Architecture, achieving flood management in watersheds with hydraulic structures in an optimal manner by balancing out flood mitigation and unnecessary wastage of water via pre-releases.","We perform experiments with FIDLAR using data from the South Florida Water Management District, which manages a coastal area that is highly prone to frequent storms and floods.","Results show that FIDLAR performs better than the current state-of-the-art with several orders of magnitude speedup and with provably better pre-release schedules.","The dramatic speedups make it possible for FIDLAR to be used for real-time flood management.","The main contribution of this paper is the effective use of tools for model explainability, allowing us to understand the contribution of the various environmental factors towards its decisions."],"url":"http://arxiv.org/abs/2310.19166v1"}
{"created":"2023-10-29 21:47:24","title":"RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning with Active Data Manipulation","abstract":"Federated learning (FL) has recently emerged as a privacy-preserving approach for machine learning in domains that rely on user interactions, particularly recommender systems (RS) and online learning to rank (OLTR). While there has been substantial research on the privacy of traditional FL, little attention has been paid to studying the privacy properties of these interaction-based FL (IFL) systems. In this work, we show that IFL can introduce unique challenges concerning user privacy, particularly when the central server has knowledge and control over the items that users interact with. Specifically, we demonstrate the threat of reconstructing user interactions by presenting RAIFLE, a general optimization-based reconstruction attack framework customized for IFL. RAIFLE employs Active Data Manipulation (ADM), a novel attack technique unique to IFL, where the server actively manipulates the training features of the items to induce adversarial behaviors in the local FL updates. We show that RAIFLE is more impactful than existing FL privacy attacks in the IFL context, and describe how it can undermine privacy defenses like secure aggregation and private information retrieval. Based on our findings, we propose and discuss countermeasure guidelines to mitigate our attack in the context of federated RS/OLTR specifically and IFL more broadly.","sentences":["Federated learning (FL) has recently emerged as a privacy-preserving approach for machine learning in domains that rely on user interactions, particularly recommender systems (RS) and online learning to rank (OLTR).","While there has been substantial research on the privacy of traditional FL, little attention has been paid to studying the privacy properties of these interaction-based FL (IFL) systems.","In this work, we show that IFL can introduce unique challenges concerning user privacy, particularly when the central server has knowledge and control over the items that users interact with.","Specifically, we demonstrate the threat of reconstructing user interactions by presenting RAIFLE, a general optimization-based reconstruction attack framework customized for IFL.","RAIFLE employs Active Data Manipulation (ADM), a novel attack technique unique to IFL, where the server actively manipulates the training features of the items to induce adversarial behaviors in the local FL updates.","We show that RAIFLE is more impactful than existing FL privacy attacks in the IFL context, and describe how it can undermine privacy defenses like secure aggregation and private information retrieval.","Based on our findings, we propose and discuss countermeasure guidelines to mitigate our attack in the context of federated RS/OLTR specifically and IFL more broadly."],"url":"http://arxiv.org/abs/2310.19163v1"}
{"created":"2023-10-29 21:19:08","title":"Transfer Learning in Transformer-Based Demand Forecasting For Home Energy Management System","abstract":"Increasingly, homeowners opt for photovoltaic (PV) systems and/or battery storage to minimize their energy bills and maximize renewable energy usage. This has spurred the development of advanced control algorithms that maximally achieve those goals. However, a common challenge faced while developing such controllers is the unavailability of accurate forecasts of household power consumption, especially for shorter time resolutions (15 minutes) and in a data-efficient manner. In this paper, we analyze how transfer learning can help by exploiting data from multiple households to improve a single house's load forecasting. Specifically, we train an advanced forecasting model (a temporal fusion transformer) using data from multiple different households, and then finetune this global model on a new household with limited data (i.e. only a few days). The obtained models are used for forecasting power consumption of the household for the next 24 hours~(day-ahead) at a time resolution of 15 minutes, with the intention of using these forecasts in advanced controllers such as Model Predictive Control. We show the benefit of this transfer learning setup versus solely using the individual new household's data, both in terms of (i) forecasting accuracy ($\\sim$15\\% MAE reduction) and (ii) control performance ($\\sim$2\\% energy cost reduction), using real-world household data.","sentences":["Increasingly, homeowners opt for photovoltaic (PV) systems and/or battery storage to minimize their energy bills and maximize renewable energy usage.","This has spurred the development of advanced control algorithms that maximally achieve those goals.","However, a common challenge faced while developing such controllers is the unavailability of accurate forecasts of household power consumption, especially for shorter time resolutions (15 minutes) and in a data-efficient manner.","In this paper, we analyze how transfer learning can help by exploiting data from multiple households to improve a single house's load forecasting.","Specifically, we train an advanced forecasting model (a temporal fusion transformer) using data from multiple different households, and then finetune this global model on a new household with limited data (i.e. only a few days).","The obtained models are used for forecasting power consumption of the household for the next 24 hours~(day-ahead) at a time resolution of 15 minutes, with the intention of using these forecasts in advanced controllers such as Model Predictive Control.","We show the benefit of this transfer learning setup versus solely using the individual new household's data, both in terms of (i) forecasting accuracy ($\\sim$15\\% MAE reduction) and (ii) control performance ($\\sim$2\\% energy cost reduction), using real-world household data."],"url":"http://arxiv.org/abs/2310.19159v1"}
{"created":"2023-10-29 21:18:25","title":"Perspectives from India: Challenges and Opportunities for Computational Tools to Enhance Confidence in Published Research","abstract":"Over the past decade, a crisis of confidence in published scientific findings has catalyzed widespread response from the research community, particularly in the West. These responses have included policy discussions and changes to existing practice as well as computational infrastructure to support and evaluate research. Our work studies Indian researchers' awareness, perceptions, and challenges around research integrity. We explore opportunities for Artificial Intelligence (AI)-powered tools to evaluate reproducibility and replicability, centering cultural perspectives. We discuss requirements for such tools, including signals within papers and metadata to be included, and system hybridity (fully-AI vs. collaborative human-AI). We draw upon 19 semi-structured interviews and 72 follow-up surveys with researchers at universities throughout India. Our findings highlight the need for computational tools to contextualize confidence in published research. In particular, researchers prefer approaches that enable human-AI collaboration. Additionally, our findings emphasize the shortcomings of current incentive structures for publication, funding, and promotion.","sentences":["Over the past decade, a crisis of confidence in published scientific findings has catalyzed widespread response from the research community, particularly in the West.","These responses have included policy discussions and changes to existing practice as well as computational infrastructure to support and evaluate research.","Our work studies Indian researchers' awareness, perceptions, and challenges around research integrity.","We explore opportunities for Artificial Intelligence (AI)-powered tools to evaluate reproducibility and replicability, centering cultural perspectives.","We discuss requirements for such tools, including signals within papers and metadata to be included, and system hybridity (fully-AI vs. collaborative human-AI).","We draw upon 19 semi-structured interviews and 72 follow-up surveys with researchers at universities throughout India.","Our findings highlight the need for computational tools to contextualize confidence in published research.","In particular, researchers prefer approaches that enable human-AI collaboration.","Additionally, our findings emphasize the shortcomings of current incentive structures for publication, funding, and promotion."],"url":"http://arxiv.org/abs/2310.19158v1"}
{"created":"2023-10-29 21:13:31","title":"Poisoning Retrieval Corpora by Injecting Adversarial Passages","abstract":"Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but to what extent can they be safely deployed in real-world applications? In this work, we propose a novel attack for dense retrieval systems in which a malicious user generates a small number of adversarial passages by perturbing discrete tokens to maximize similarity with a provided set of training queries. When these adversarial passages are inserted into a large retrieval corpus, we show that this attack is highly effective in fooling these systems to retrieve them for queries that were not seen by the attacker. More surprisingly, these adversarial passages can directly generalize to out-of-domain queries and corpora with a high success attack rate -- for instance, we find that 50 generated passages optimized on Natural Questions can mislead >94% of questions posed in financial documents or online forums. We also benchmark and compare a range of state-of-the-art dense retrievers, both unsupervised and supervised. Although different systems exhibit varying levels of vulnerability, we show they can all be successfully attacked by injecting up to 500 passages, a small fraction compared to a retrieval corpus of millions of passages.","sentences":["Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but to what extent can they be safely deployed in real-world applications?","In this work, we propose a novel attack for dense retrieval systems in which a malicious user generates a small number of adversarial passages by perturbing discrete tokens to maximize similarity with a provided set of training queries.","When these adversarial passages are inserted into a large retrieval corpus, we show that this attack is highly effective in fooling these systems to retrieve them for queries that were not seen by the attacker.","More surprisingly, these adversarial passages can directly generalize to out-of-domain queries and corpora with a high success attack rate -- for instance, we find that 50 generated passages optimized on Natural Questions can mislead >94% of questions posed in financial documents or online forums.","We also benchmark and compare a range of state-of-the-art dense retrievers, both unsupervised and supervised.","Although different systems exhibit varying levels of vulnerability, we show they can all be successfully attacked by injecting up to 500 passages, a small fraction compared to a retrieval corpus of millions of passages."],"url":"http://arxiv.org/abs/2310.19156v1"}
{"created":"2023-10-29 21:08:36","title":"Design and Experimental Evaluation of a Haptic Robot-Assisted System for Femur Fracture Surgery","abstract":"In the face of challenges encountered during femur fracture surgery, such as the high rates of malalignment and X-ray exposure to operating personnel, robot-assisted surgery has emerged as an alternative to conventional state-of-the-art surgical methods. This paper introduces the development of Robossis, a haptic system for robot-assisted femur fracture surgery. Robossis comprises a 7-DOF haptic controller and a 6-DOF surgical robot. A unilateral control architecture is developed to address the kinematic mismatch and the motion transfer between the haptic controller and the Robossis surgical robot. A real-time motion control pipeline is designed to address the motion transfer and evaluated through experimental testing. The analysis illustrates that the Robossis surgical robot can adhere to the desired trajectory from the haptic controller with an average translational error of 0.32 mm and a rotational error of 0.07 deg. Additionally, a haptic rendering pipeline is developed to resolve the kinematic mismatch by constraining the haptic controller (user hand) movement within the permissible joint limits of the Robossis surgical robot. Lastly, in a cadaveric lab test, the Robossis system assisted surgeons during a mock femur fracture surgery. The result shows that Robossis can provide an intuitive solution for surgeons to perform femur fracture surgery.","sentences":["In the face of challenges encountered during femur fracture surgery, such as the high rates of malalignment and X-ray exposure to operating personnel, robot-assisted surgery has emerged as an alternative to conventional state-of-the-art surgical methods.","This paper introduces the development of Robossis, a haptic system for robot-assisted femur fracture surgery.","Robossis comprises a 7-DOF haptic controller and a 6-DOF surgical robot.","A unilateral control architecture is developed to address the kinematic mismatch and the motion transfer between the haptic controller and the Robossis surgical robot.","A real-time motion control pipeline is designed to address the motion transfer and evaluated through experimental testing.","The analysis illustrates that the Robossis surgical robot can adhere to the desired trajectory from the haptic controller with an average translational error of 0.32 mm and a rotational error of 0.07 deg.","Additionally, a haptic rendering pipeline is developed to resolve the kinematic mismatch by constraining the haptic controller (user hand) movement within the permissible joint limits of the Robossis surgical robot.","Lastly, in a cadaveric lab test, the Robossis system assisted surgeons during a mock femur fracture surgery.","The result shows that Robossis can provide an intuitive solution for surgeons to perform femur fracture surgery."],"url":"http://arxiv.org/abs/2310.19153v1"}
{"created":"2023-10-29 21:06:34","title":"BERT Lost Patience Won't Be Robust to Adversarial Slowdown","abstract":"In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can remove perturbations effectively. This result suggests that future work is needed for developing efficient yet robust multi-exit models. Our code is available at: https://github.com/ztcoalson/WAFFLE","sentences":["In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown.","To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points.","We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown.","We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings.","The more complex a mechanism is, the more vulnerable it is to adversarial slowdown.","We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks.","Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can remove perturbations effectively.","This result suggests that future work is needed for developing efficient yet robust multi-exit models.","Our code is available at: https://github.com/ztcoalson/WAFFLE"],"url":"http://arxiv.org/abs/2310.19152v1"}
{"created":"2023-10-29 20:53:45","title":"Simple Constructions of Unique Neighbor Expanders from Error-correcting Codes","abstract":"In this note, we give very simple constructions of unique neighbor expander graphs starting from spectral or combinatorial expander graphs of mild expansion. These constructions and their analysis are simple variants of the constructions of LDPC error-correcting codes from expanders, given by Sipser-Spielman\\cite{SS96} (and Tanner\\cite{Tanner81}), and their analysis. We also show how to obtain expanders with many unique neighbors using similar ideas.   There were many exciting results on this topic recently, starting with Asherov-Dinur\\cite{AD23} and Hsieh-McKenzie-Mohanty-Paredes\\cite{HMMP23}, who gave a similar construction of unique neighbor expander graphs, but using more sophisticated ingredients (such as almost-Ramanujan graphs) and a more involved analysis. Subsequent beautiful works of Cohen-Roth-TaShma\\cite{CRT23} and Golowich\\cite{Golowich23} gave even stronger objects (lossless expanders), but also using sophisticated ingredients.   The main contribution of this work is that we get much more elementary constructions of unique neighbor expanders and with a simpler analysis.","sentences":["In this note, we give very simple constructions of unique neighbor expander graphs starting from spectral or combinatorial expander graphs of mild expansion.","These constructions and their analysis are simple variants of the constructions of LDPC error-correcting codes from expanders, given by Sipser-Spielman\\cite{SS96} (and Tanner\\cite{Tanner81}), and their analysis.","We also show how to obtain expanders with many unique neighbors using similar ideas.   ","There were many exciting results on this topic recently, starting with Asherov-Dinur\\cite{AD23} and Hsieh-McKenzie-Mohanty-Paredes\\cite{HMMP23}, who gave a similar construction of unique neighbor expander graphs, but using more sophisticated ingredients (such as almost-Ramanujan graphs) and a more involved analysis.","Subsequent beautiful works of Cohen-Roth-TaShma\\cite{CRT23} and Golowich\\cite{Golowich23} gave even stronger objects (lossless expanders), but also using sophisticated ingredients.   ","The main contribution of this work is that we get much more elementary constructions of unique neighbor expanders and with a simpler analysis."],"url":"http://arxiv.org/abs/2310.19149v1"}
{"created":"2023-10-29 20:39:11","title":"Learning to Follow Object-Centric Image Editing Instructions Faithfully","abstract":"Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction). Current approaches focusing on image editing with natural language instructions rely on automatically generated paired data, which, as shown in our investigation, is noisy and sometimes nonsensical, exacerbating the above issues. Building on recent advances in segmentation, Chain-of-Thought prompting, and visual question answering, we significantly improve the quality of the paired data. In addition, we enhance the supervision signal by highlighting parts of the image that need to be changed by the instruction. The model fine-tuned on the improved data is capable of performing fine-grained object-centric edits better than state-of-the-art baselines, mitigating the problems outlined above, as shown by automatic and human evaluations. Moreover, our model is capable of generalizing to domains unseen during training, such as visual metaphors.","sentences":["Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models.","However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction).","Current approaches focusing on image editing with natural language instructions rely on automatically generated paired data, which, as shown in our investigation, is noisy and sometimes nonsensical, exacerbating the above issues.","Building on recent advances in segmentation, Chain-of-Thought prompting, and visual question answering, we significantly improve the quality of the paired data.","In addition, we enhance the supervision signal by highlighting parts of the image that need to be changed by the instruction.","The model fine-tuned on the improved data is capable of performing fine-grained object-centric edits better than state-of-the-art baselines, mitigating the problems outlined above, as shown by automatic and human evaluations.","Moreover, our model is capable of generalizing to domains unseen during training, such as visual metaphors."],"url":"http://arxiv.org/abs/2310.19145v1"}
{"created":"2023-10-29 20:32:21","title":"MAG-GNN: Reinforcement Learning Boosted Graph Neural Network","abstract":"While Graph Neural Networks (GNNs) recently became powerful tools in graph learning tasks, considerable efforts have been spent on improving GNNs' structural encoding ability. A particular line of work proposed subgraph GNNs that use subgraph information to improve GNNs' expressivity and achieved great success. However, such effectivity sacrifices the efficiency of GNNs by enumerating all possible subgraphs. In this paper, we analyze the necessity of complete subgraph enumeration and show that a model can achieve a comparable level of expressivity by considering a small subset of the subgraphs. We then formulate the identification of the optimal subset as a combinatorial optimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a reinforcement learning (RL) boosted GNN, to solve the problem. Starting with a candidate subgraph set, MAG-GNN employs an RL agent to iteratively update the subgraphs to locate the most expressive set for prediction. This reduces the exponential complexity of subgraph enumeration to the constant complexity of a subgraph search algorithm while keeping good expressivity. We conduct extensive experiments on many datasets, showing that MAG-GNN achieves competitive performance to state-of-the-art methods and even outperforms many subgraph GNNs. We also demonstrate that MAG-GNN effectively reduces the running time of subgraph GNNs.","sentences":["While Graph Neural Networks (GNNs) recently became powerful tools in graph learning tasks, considerable efforts have been spent on improving GNNs' structural encoding ability.","A particular line of work proposed subgraph GNNs that use subgraph information to improve GNNs' expressivity and achieved great success.","However, such effectivity sacrifices the efficiency of GNNs by enumerating all possible subgraphs.","In this paper, we analyze the necessity of complete subgraph enumeration and show that a model can achieve a comparable level of expressivity by considering a small subset of the subgraphs.","We then formulate the identification of the optimal subset as a combinatorial optimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a reinforcement learning (RL) boosted GNN, to solve the problem.","Starting with a candidate subgraph set, MAG-GNN employs an RL agent to iteratively update the subgraphs to locate the most expressive set for prediction.","This reduces the exponential complexity of subgraph enumeration to the constant complexity of a subgraph search algorithm while keeping good expressivity.","We conduct extensive experiments on many datasets, showing that MAG-GNN achieves competitive performance to state-of-the-art methods and even outperforms many subgraph GNNs.","We also demonstrate that MAG-GNN effectively reduces the running time of subgraph GNNs."],"url":"http://arxiv.org/abs/2310.19142v1"}
{"created":"2023-10-29 20:23:57","title":"Optical STAR-RIS-Aided VLC Systems: RSMA Versus NOMA","abstract":"A critical concern within the realm of visible light communications (VLC) pertains to enhancing system data rate, particularly in scenarios where the direct line-of-sight (LoS) connection is obstructed by obstacles. The deployment of meta-surface-based simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) has emerged to combat challenging LoS blockage scenarios and to provide 360 coverage in radio-frequency wireless systems. Recently, the concept of optical simultaneous transmission and reflection reconfigurable intelligent surface (OSTAR-RIS) has been promoted for VLC systems. This work is dedicated to studying the performance of OSTAR-RIS in detail and unveiling the VLC system performance gain under such technology. Specifically, we propose a novel multi-user indoor VLC system that is assisted by OSTAR-RIS. To improve the sum rate performance of the proposed system, both power-domain non-orthogonal multiple access (NOMA) and rate splitting multiple access (RSMA) are investigated in this work. To realize this, a sum rate maximization problem that jointly optimizes the roll and yaw angles of the reflector elements as well as the refractive index of the refractor elements in OSTAR-RIS is formulated, solved, and evaluated. The maximization problem takes into account practical considerations, such as the presence of non-users (i.e., blockers) and the orientation of the recipient's device. The sine-cosine meta-heuristic algorithm is employed to get the optimal solution of the formulated non-convex optimization problem. Moreover, the study delves into the sum energy efficiency optimization of the proposed system. Simulation results indicate that the proposed OSTAR-RIS RSMA-aided VLC system outperforms the OSTAR-RIS NOMA-based VLC system in terms of both the sum rate and the sum energy efficiency.","sentences":["A critical concern within the realm of visible light communications (VLC) pertains to enhancing system data rate, particularly in scenarios where the direct line-of-sight (LoS) connection is obstructed by obstacles.","The deployment of meta-surface-based simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) has emerged to combat challenging LoS blockage scenarios and to provide 360 coverage in radio-frequency wireless systems.","Recently, the concept of optical simultaneous transmission and reflection reconfigurable intelligent surface (OSTAR-RIS) has been promoted for VLC systems.","This work is dedicated to studying the performance of OSTAR-RIS in detail and unveiling the VLC system performance gain under such technology.","Specifically, we propose a novel multi-user indoor VLC system that is assisted by OSTAR-RIS.","To improve the sum rate performance of the proposed system, both power-domain non-orthogonal multiple access (NOMA) and rate splitting multiple access (RSMA) are investigated in this work.","To realize this, a sum rate maximization problem that jointly optimizes the roll and yaw angles of the reflector elements as well as the refractive index of the refractor elements in OSTAR-RIS is formulated, solved, and evaluated.","The maximization problem takes into account practical considerations, such as the presence of non-users (i.e., blockers) and the orientation of the recipient's device.","The sine-cosine meta-heuristic algorithm is employed to get the optimal solution of the formulated non-convex optimization problem.","Moreover, the study delves into the sum energy efficiency optimization of the proposed system.","Simulation results indicate that the proposed OSTAR-RIS RSMA-aided VLC system outperforms the OSTAR-RIS NOMA-based VLC system in terms of both the sum rate and the sum energy efficiency."],"url":"http://arxiv.org/abs/2310.19141v1"}
{"created":"2023-10-29 20:19:06","title":"Back to the Future! Studying Data Cleanness in Defects4J and its Impact on Fault Localization","abstract":"For software testing research, Defects4J stands out as the primary benchmark dataset, offering a controlled environment to study real bugs from prominent open-source systems. However, prior research indicates that Defects4J might include tests added post-bug report, embedding developer knowledge and affecting fault localization efficacy. In this paper, we examine Defects4J's fault-triggering tests, emphasizing the implications of developer knowledge of SBFL techniques. We study the timelines of changes made to these tests concerning bug report creation. Then, we study the effectiveness of SBFL techniques without developer knowledge in the tests. We found that 1) 55% of the fault-triggering tests were newly added to replicate the bug or to test for regression; 2) 22% of the fault-triggering tests were modified after the bug reports were created, containing developer knowledge of the bug; 3) developers often modify the tests to include new assertions or change the test code to reflect the changes in the source code; and 4) the performance of SBFL techniques degrades significantly (up to --415% for Mean First Rank) when evaluated on the bugs without developer knowledge. We provide a dataset of bugs without developer insights, aiding future SBFL evaluations in Defects4J and informing considerations for future bug benchmarks.","sentences":["For software testing research, Defects4J stands out as the primary benchmark dataset, offering a controlled environment to study real bugs from prominent open-source systems.","However, prior research indicates that Defects4J might include tests added post-bug report, embedding developer knowledge and affecting fault localization efficacy.","In this paper, we examine Defects4J's fault-triggering tests, emphasizing the implications of developer knowledge of SBFL techniques.","We study the timelines of changes made to these tests concerning bug report creation.","Then, we study the effectiveness of SBFL techniques without developer knowledge in the tests.","We found that 1) 55% of the fault-triggering tests were newly added to replicate the bug or to test for regression; 2) 22% of the fault-triggering tests were modified after the bug reports were created, containing developer knowledge of the bug; 3) developers often modify the tests to include new assertions or change the test code to reflect the changes in the source code; and 4) the performance of SBFL techniques degrades significantly (up to --415% for Mean First Rank) when evaluated on the bugs without developer knowledge.","We provide a dataset of bugs without developer insights, aiding future SBFL evaluations in Defects4J and informing considerations for future bug benchmarks."],"url":"http://arxiv.org/abs/2310.19139v1"}
{"created":"2023-10-29 20:03:38","title":"Backward and Forward Inference in Interacting Independent-Cascade Processes: A Scalable and Convergent Message-Passing Approach","abstract":"We study the problems of estimating the past and future evolutions of two diffusion processes that spread concurrently on a network. Specifically, given a known network $G=(V, \\overrightarrow{E})$ and a (possibly noisy) snapshot $\\mathcal{O}_n$ of its state taken at (a possibly unknown) time $W$, we wish to determine the posterior distributions of the initial state of the network and the infection times of its nodes. These distributions are useful in finding source nodes of epidemics and rumors -- $\\textit{backward inference}$ -- , and estimating the spread of a fixed set of source nodes -- $\\textit{forward inference}$.   To model the interaction between the two processes, we study an extension of the independent-cascade (IC) model where, when a node gets infected with either process, its susceptibility to the other one changes. First, we derive the exact joint probability of the initial state of the network and the observation-snapshot $\\mathcal{O}_n$. Then, using the machinery of factor-graphs, factor-graph transformations, and the generalized distributive-law, we derive a Belief-Propagation (BP) based algorithm that is scalable to large networks and can converge on graphs of arbitrary topology (at a likely expense in approximation accuracy).","sentences":["We study the problems of estimating the past and future evolutions of two diffusion processes that spread concurrently on a network.","Specifically, given a known network $G=(V, \\overrightarrow{E})$ and a (possibly noisy) snapshot $\\mathcal{O}_n$ of its state taken at (a possibly unknown) time $W$, we wish to determine the posterior distributions of the initial state of the network and the infection times of its nodes.","These distributions are useful in finding source nodes of epidemics and rumors -- $\\textit{backward inference}$ -- , and estimating the spread of a fixed set of source nodes -- $\\textit{forward inference}$.   To model the interaction between the two processes, we study an extension of the independent-cascade (IC) model where, when a node gets infected with either process, its susceptibility to the other one changes.","First, we derive the exact joint probability of the initial state of the network and the observation-snapshot $\\mathcal{O}_n$. Then, using the machinery of factor-graphs, factor-graph transformations, and the generalized distributive-law, we derive a Belief-Propagation (BP) based algorithm that is scalable to large networks and can converge on graphs of arbitrary topology (at a likely expense in approximation accuracy)."],"url":"http://arxiv.org/abs/2310.19138v1"}
{"created":"2023-10-29 19:59:55","title":"Automaton Distillation: Neuro-Symbolic Transfer Learning for Deep Reinforcement Learning","abstract":"Reinforcement learning (RL) is a powerful tool for finding optimal policies in sequential decision processes. However, deep RL methods suffer from two weaknesses: collecting the amount of agent experience required for practical RL problems is prohibitively expensive, and the learned policies exhibit poor generalization on tasks outside of the training distribution. To mitigate these issues, we introduce automaton distillation, a form of neuro-symbolic transfer learning in which Q-value estimates from a teacher are distilled into a low-dimensional representation in the form of an automaton. We then propose two methods for generating Q-value estimates: static transfer, which reasons over an abstract Markov Decision Process constructed based on prior knowledge, and dynamic transfer, where symbolic information is extracted from a teacher Deep Q-Network (DQN). The resulting Q-value estimates from either method are used to bootstrap learning in the target environment via a modified DQN loss function. We list several failure modes of existing automaton-based transfer methods and demonstrate that both static and dynamic automaton distillation decrease the time required to find optimal policies for various decision tasks.","sentences":["Reinforcement learning (RL) is a powerful tool for finding optimal policies in sequential decision processes.","However, deep RL methods suffer from two weaknesses: collecting the amount of agent experience required for practical RL problems is prohibitively expensive, and the learned policies exhibit poor generalization on tasks outside of the training distribution.","To mitigate these issues, we introduce automaton distillation, a form of neuro-symbolic transfer learning in which Q-value estimates from a teacher are distilled into a low-dimensional representation in the form of an automaton.","We then propose two methods for generating Q-value estimates: static transfer, which reasons over an abstract Markov Decision Process constructed based on prior knowledge, and dynamic transfer, where symbolic information is extracted from a teacher Deep Q-Network (DQN).","The resulting Q-value estimates from either method are used to bootstrap learning in the target environment via a modified DQN loss function.","We list several failure modes of existing automaton-based transfer methods and demonstrate that both static and dynamic automaton distillation decrease the time required to find optimal policies for various decision tasks."],"url":"http://arxiv.org/abs/2310.19137v1"}
{"created":"2023-10-29 19:39:03","title":"Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender","abstract":"In this paper, we investigate the impact of objects on gender bias in image captioning systems. Our results show that only gender-specific objects have a strong gender bias (e.g., women-lipstick). In addition, we propose a visual semantic-based gender score that measures the degree of bias and can be used as a plug-in for any image captioning system. Our experiments demonstrate the utility of the gender score, since we observe that our score can measure the bias relation between a caption and its related gender; therefore, our score can be used as an additional metric to the existing Object Gender Co-Occ approach. Code and data are publicly available at \\url{https://github.com/ahmedssabir/GenderScore}.","sentences":["In this paper, we investigate the impact of objects on gender bias in image captioning systems.","Our results show that only gender-specific objects have a strong gender bias (e.g., women-lipstick).","In addition, we propose a visual semantic-based gender score that measures the degree of bias and can be used as a plug-in for any image captioning system.","Our experiments demonstrate the utility of the gender score, since we observe that our score can measure the bias relation between a caption and its related gender; therefore, our score can be used as an additional metric to the existing Object Gender Co-Occ approach.","Code and data are publicly available at \\url{https://github.com/ahmedssabir/GenderScore}."],"url":"http://arxiv.org/abs/2310.19130v1"}
{"created":"2023-10-29 19:28:22","title":"Unified Representation for Non-compositional and Compositional Expressions","abstract":"Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations. Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA. These gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1% accuracy) compared to BART.","sentences":["Accurate processing of non-compositional language relies on generating good representations for such expressions.","In this work, we study the representation of language non-compositionality by proposing a language model, PIER, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs).","PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations.","Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA.","These gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1% accuracy) compared to BART."],"url":"http://arxiv.org/abs/2310.19127v1"}
{"created":"2023-10-29 19:25:48","title":"Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations","abstract":"Graph-based approaches to nearest neighbor search are popular and powerful tools for handling large datasets in practice, but they have limited theoretical guarantees. We study the worst-case performance of recent graph-based approximate nearest neighbor search algorithms, such as HNSW, NSG and DiskANN. For DiskANN, we show that its \"slow preprocessing\" version provably supports approximate nearest neighbor search query with constant approximation ratio and poly-logarithmic query time, on data sets with bounded \"intrinsic\" dimension. For the other data structure variants studied, including DiskANN with \"fast preprocessing\", HNSW and NSG, we present a family of instances on which the empirical query time required to achieve a \"reasonable\" accuracy is linear in instance size. For example, for DiskANN, we show that the query procedure can take at least $0.1 n$ steps on instances of size $n$ before it encounters any of the $5$ nearest neighbors of the query.","sentences":["Graph-based approaches to nearest neighbor search are popular and powerful tools for handling large datasets in practice, but they have limited theoretical guarantees.","We study the worst-case performance of recent graph-based approximate nearest neighbor search algorithms, such as HNSW, NSG and DiskANN.","For DiskANN, we show that its \"slow preprocessing\" version provably supports approximate nearest neighbor search query with constant approximation ratio and poly-logarithmic query time, on data sets with bounded \"intrinsic\" dimension.","For the other data structure variants studied, including DiskANN with \"fast preprocessing\", HNSW and NSG, we present a family of instances on which the empirical query time required to achieve a \"reasonable\" accuracy is linear in instance size.","For example, for DiskANN, we show that the query procedure can take at least $0.1 n$ steps on instances of size $n$ before it encounters any of the $5$ nearest neighbors of the query."],"url":"http://arxiv.org/abs/2310.19126v1"}
{"created":"2023-10-29 19:21:37","title":"Partial Orderings as Heuristic for Multi-Objective Model-Based Reasoning","abstract":"Model-based reasoning is becoming increasingly common in software engineering. The process of building and analyzing models helps stakeholders to understand the ramifications of their software decisions. But complex models can confuse and overwhelm stakeholders when these models have too many candidate solutions. We argue here that a technique based on partial orderings lets humans find acceptable solutions via a binary chop needing $O(log(N))$ queries (or less). This paper checks the value of this approach via the iSNEAK partial ordering tool. Pre-experimentally, we were concerned that (a)~our automated methods might produce models that were unacceptable to humans; and that (b)~our human-in-the-loop methods might actual overlooking significant optimizations. Hence, we checked the acceptability of the solutions found by iSNEAK via a human-in-the-loop double-blind evaluation study of 20 Brazilian programmers. We also checked if iSNEAK misses significant optimizations (in a corpus of 16 SE models of size ranging up to 1000 attributes by comparing it against two rival technologies (the genetic algorithms preferred by the interactive search-based SE community; and the sequential model optimizers developed by the SE configuration community~\\citep{flash_vivek}). iSNEAK 's solutions were found to be human acceptable (and those solutions took far less time to generate, with far fewer questions to any stakeholder). Significantly, our methods work well even for multi-objective models with competing goals (in this work we explore models with four to five goals). These results motivate more work on partial ordering for many-goal model-based problems.","sentences":["Model-based reasoning is becoming increasingly common in software engineering.","The process of building and analyzing models helps stakeholders to understand the ramifications of their software decisions.","But complex models can confuse and overwhelm stakeholders when these models have too many candidate solutions.","We argue here that a technique based on partial orderings lets humans find acceptable solutions via a binary chop needing $O(log(N))$ queries (or less).","This paper checks the value of this approach via the iSNEAK partial ordering tool.","Pre-experimentally, we were concerned that (a)~our automated methods might produce models that were unacceptable to humans; and that (b)~our human-in-the-loop methods might actual overlooking significant optimizations.","Hence, we checked the acceptability of the solutions found by iSNEAK via a human-in-the-loop double-blind evaluation study of 20 Brazilian programmers.","We also checked if iSNEAK misses significant optimizations (in a corpus of 16 SE models of size ranging up to 1000 attributes by comparing it against two rival technologies (the genetic algorithms preferred by the interactive search-based SE community; and the sequential model optimizers developed by the SE configuration community~\\citep{flash_vivek}).","iSNEAK 's solutions were found to be human acceptable (and those solutions took far less time to generate, with far fewer questions to any stakeholder).","Significantly, our methods work well even for multi-objective models with competing goals (in this work we explore models with four to five goals).","These results motivate more work on partial ordering for many-goal model-based problems."],"url":"http://arxiv.org/abs/2310.19125v1"}
{"created":"2023-10-29 19:21:33","title":"Software engineering for deep learning applications: usage of SWEng and MLops tools in GitHub repositories","abstract":"The rising popularity of deep learning (DL) methods and techniques has invigorated interest in the topic of SE4DL, the application of software engineering (SE) practices on deep learning software. Despite the novel engineering challenges brought on by the data-driven and non-deterministic paradigm of DL software, little work has been invested into developing AI-targeted SE tools. On the other hand, tools tackling more general engineering issues in DL are actively used and referred to under the umbrella term of ``MLOps tools''. Furthermore, the available literature supports the utility of conventional SE tooling in DL software development. Building upon previous MSR research on tool usage in open-source software works, we identify conventional and MLOps tools adopted in popular applied DL projects that use Python as the main programming language. About 70% of the GitHub repositories mined contained at least one conventional SE tool. Software configuration management tools are the most adopted, while the opposite applies to maintenance tools. Substantially fewer MLOps tools were in use, with only 9 tools out of a sample of 80 used in at least one repository. The majority of them were open-source rather than proprietary. One of these tools, TensorBoard, was found to be adopted in about half of the repositories in our study. Consequently, the use of conventional SE tooling demonstrates its relevance to DL software. Further research is recommended on the adoption of MLOps tooling by open-source projects, focusing on the relevance of particular tool types, the development of required tools, as well as ways to promote the use of already available tools.","sentences":["The rising popularity of deep learning (DL) methods and techniques has invigorated interest in the topic of SE4DL, the application of software engineering (SE) practices on deep learning software.","Despite the novel engineering challenges brought on by the data-driven and non-deterministic paradigm of DL software, little work has been invested into developing AI-targeted SE tools.","On the other hand, tools tackling more general engineering issues in DL are actively used and referred to under the umbrella term of ``MLOps tools''.","Furthermore, the available literature supports the utility of conventional SE tooling in DL software development.","Building upon previous MSR research on tool usage in open-source software works, we identify conventional and MLOps tools adopted in popular applied DL projects that use Python as the main programming language.","About 70% of the GitHub repositories mined contained at least one conventional SE tool.","Software configuration management tools are the most adopted, while the opposite applies to maintenance tools.","Substantially fewer MLOps tools were in use, with only 9 tools out of a sample of 80 used in at least one repository.","The majority of them were open-source rather than proprietary.","One of these tools, TensorBoard, was found to be adopted in about half of the repositories in our study.","Consequently, the use of conventional SE tooling demonstrates its relevance to DL software.","Further research is recommended on the adoption of MLOps tooling by open-source projects, focusing on the relevance of particular tool types, the development of required tools, as well as ways to promote the use of already available tools."],"url":"http://arxiv.org/abs/2310.19124v1"}
{"created":"2023-10-29 19:20:38","title":"Three Dogmas, a Puzzle and its Solution","abstract":"Modern Logics, as formulated notably by Frege, Russell and Tarski involved basic assumptions about Natural Languages in general and Indo-European Languages in particular, which are contested by Linguists. Based upon those assumptions, formal Languages were designed to overcome what Logicians claimed to be 'defects' of Natural Language. In this paper we show that those assumptions contradict basic principles of Arabic. More specifically: The Logicians ideas, that within Natural Language words refer to objects, 'ToBe'-constructions represent identity statements, Indefinite Descriptions must be replaced by existential quantifiers to form meaningful Sentences and Symbols can have no interpretation-independent meanings, are all falsified using undisputed principles of Arabic. The here presented falsification serves two purposes. First, it is used as a factual basis for the rejection of approaches adopting Semantic axioms of Mathematical Logics as Models for meaning of Arabic Syntax. Second, it shows a way to approach the important computational problem: Satisfiability (SAT). The described way is based upon the realization that parsing Arabic utilizes the existence of 'meaning-particles' within Syntax to efficiently recognize words, phrases and Sentences. Similar meaning-particles are shown to exist in 3CNF formulas, which, when properly handled within the machinery of 3SAT-Solvers, enable structural conditions to be imposed on formulas, sufficient alone to guarantee the efficient production of non-exponentially sized Free Binary Decision Diagrams (FBDDs). We show, why known exponential Lower Bounds on sizes of FBDDs do not contradict our results and reveal practical evidence, obtained for multiplication circuits, supporting our claims.","sentences":["Modern Logics, as formulated notably by Frege, Russell and Tarski involved basic assumptions about Natural Languages in general and Indo-European Languages in particular, which are contested by Linguists.","Based upon those assumptions, formal Languages were designed to overcome what Logicians claimed to be 'defects' of Natural Language.","In this paper we show that those assumptions contradict basic principles of Arabic.","More specifically: The Logicians ideas, that within Natural Language words refer to objects, 'ToBe'-constructions represent identity statements, Indefinite Descriptions must be replaced by existential quantifiers to form meaningful Sentences and Symbols can have no interpretation-independent meanings, are all falsified using undisputed principles of Arabic.","The here presented falsification serves two purposes.","First, it is used as a factual basis for the rejection of approaches adopting Semantic axioms of Mathematical Logics as Models for meaning of Arabic Syntax.","Second, it shows a way to approach the important computational problem: Satisfiability (SAT).","The described way is based upon the realization that parsing Arabic utilizes the existence of 'meaning-particles' within Syntax to efficiently recognize words, phrases and Sentences.","Similar meaning-particles are shown to exist in 3CNF formulas, which, when properly handled within the machinery of 3SAT-Solvers, enable structural conditions to be imposed on formulas, sufficient alone to guarantee the efficient production of non-exponentially sized Free Binary Decision Diagrams (FBDDs).","We show, why known exponential Lower Bounds on sizes of FBDDs do not contradict our results and reveal practical evidence, obtained for multiplication circuits, supporting our claims."],"url":"http://arxiv.org/abs/2310.19123v1"}
{"created":"2023-10-29 19:19:24","title":"Private Variable-Length Coding with Non-zero Leakage","abstract":"A private compression design problem is studied, where an encoder observes useful data $Y$, wishes to compress it using variable length code and communicates it through an unsecured channel. Since $Y$ is correlated with private data $X$, the encoder uses a private compression mechanism to design encoded message $\\cal C$ and sends it over the channel. An adversary is assumed to have access to the output of the encoder, i.e., $\\cal C$, and tries to estimate $X$. Furthermore, it is assumed that both encoder and decoder have access to a shared secret key $W$. In this work, we generalize the perfect privacy (secrecy) assumption and consider a non-zero leakage between the private data $X$ and encoded message $\\cal C$. The design goal is to encode message $\\cal C$ with minimum possible average length that satisfies non-perfect privacy constraints. We find upper and lower bounds on the average length of the encoded message using different privacy metrics and study them in special cases. For the achievability we use two-part construction coding and extended versions of Functional Representation Lemma. Lastly, in an example we show that the bounds can be asymptotically tight.","sentences":["A private compression design problem is studied, where an encoder observes useful data $Y$, wishes to compress it using variable length code and communicates it through an unsecured channel.","Since $Y$ is correlated with private data $X$, the encoder uses a private compression mechanism to design encoded message $\\cal C$ and sends it over the channel.","An adversary is assumed to have access to the output of the encoder, i.e., $\\cal C$, and tries to estimate $X$.","Furthermore, it is assumed that both encoder and decoder have access to a shared secret key $W$. In this work, we generalize the perfect privacy (secrecy) assumption and consider a non-zero leakage between the private data $X$ and encoded message","$\\cal C$.","The design goal is to encode message $\\cal C$ with minimum possible average length that satisfies non-perfect privacy constraints.","We find upper and lower bounds on the average length of the encoded message using different privacy metrics and study them in special cases.","For the achievability we use two-part construction coding and extended versions of Functional Representation Lemma.","Lastly, in an example we show that the bounds can be asymptotically tight."],"url":"http://arxiv.org/abs/2310.19122v1"}
{"created":"2023-10-29 19:10:52","title":"Out-of-distribution Object Detection through Bayesian Uncertainty Estimation","abstract":"The superior performance of object detectors is often established under the condition that the test samples are in the same distribution as the training data. However, in many practical applications, out-of-distribution (OOD) instances are inevitable and usually lead to uncertainty in the results. In this paper, we propose a novel, intuitive, and scalable probabilistic object detection method for OOD detection. Unlike other uncertainty-modeling methods that either require huge computational costs to infer the weight distributions or rely on model training through synthetic outlier data, our method is able to distinguish between in-distribution (ID) data and OOD data via weight parameter sampling from proposed Gaussian distributions based on pre-trained networks. We demonstrate that our Bayesian object detector can achieve satisfactory OOD identification performance by reducing the FPR95 score by up to 8.19% and increasing the AUROC score by up to 13.94% when trained on BDD100k and VOC datasets as the ID datasets and evaluated on COCO2017 dataset as the OOD dataset.","sentences":["The superior performance of object detectors is often established under the condition that the test samples are in the same distribution as the training data.","However, in many practical applications, out-of-distribution (OOD) instances are inevitable and usually lead to uncertainty in the results.","In this paper, we propose a novel, intuitive, and scalable probabilistic object detection method for OOD detection.","Unlike other uncertainty-modeling methods that either require huge computational costs to infer the weight distributions or rely on model training through synthetic outlier data, our method is able to distinguish between in-distribution (ID) data and OOD data via weight parameter sampling from proposed Gaussian distributions based on pre-trained networks.","We demonstrate that our Bayesian object detector can achieve satisfactory OOD identification performance by reducing the FPR95 score by up to 8.19% and increasing the AUROC score by up to 13.94% when trained on BDD100k and VOC datasets as the ID datasets and evaluated on COCO2017 dataset as the OOD dataset."],"url":"http://arxiv.org/abs/2310.19119v1"}
{"created":"2023-10-29 19:01:20","title":"Dynamic V2X Autonomous Perception from Road-to-Vehicle Vision","abstract":"Vehicle-to-everything (V2X) perception is an innovative technology that enhances vehicle perception accuracy, thereby elevating the security and reliability of autonomous systems. However, existing V2X perception methods focus on static scenes from mainly vehicle-based vision, which is constrained by sensor capabilities and communication loads. To adapt V2X perception models to dynamic scenes, we propose to build V2X perception from road-to-vehicle vision and present Adaptive Road-to-Vehicle Perception (AR2VP) method. In AR2VP,we leverage roadside units to offer stable, wide-range sensing capabilities and serve as communication hubs. AR2VP is devised to tackle both intra-scene and inter-scene changes. For the former, we construct a dynamic perception representing module, which efficiently integrates vehicle perceptions, enabling vehicles to capture a more comprehensive range of dynamic factors within the scene.Moreover, we introduce a road-to-vehicle perception compensating module, aimed at preserving the maximized roadside unit perception information in the presence of intra-scene changes.For inter-scene changes, we implement an experience replay mechanism leveraging the roadside unit's storage capacity to retain a subset of historical scene data, maintaining model robustness in response to inter-scene shifts. We conduct perception experiment on 3D object detection and segmentation, and the results show that AR2VP excels in both performance-bandwidth trade-offs and adaptability within dynamic environments.","sentences":["Vehicle-to-everything (V2X) perception is an innovative technology that enhances vehicle perception accuracy, thereby elevating the security and reliability of autonomous systems.","However, existing V2X perception methods focus on static scenes from mainly vehicle-based vision, which is constrained by sensor capabilities and communication loads.","To adapt V2X perception models to dynamic scenes, we propose to build V2X perception from road-to-vehicle vision and present Adaptive Road-to-Vehicle Perception (AR2VP) method.","In AR2VP,we leverage roadside units to offer stable, wide-range sensing capabilities and serve as communication hubs.","AR2VP is devised to tackle both intra-scene and inter-scene changes.","For the former, we construct a dynamic perception representing module, which efficiently integrates vehicle perceptions, enabling vehicles to capture a more comprehensive range of dynamic factors within the scene.","Moreover, we introduce a road-to-vehicle perception compensating module, aimed at preserving the maximized roadside unit perception information in the presence of intra-scene changes.","For inter-scene changes, we implement an experience replay mechanism leveraging the roadside unit's storage capacity to retain a subset of historical scene data, maintaining model robustness in response to inter-scene shifts.","We conduct perception experiment on 3D object detection and segmentation, and the results show that AR2VP excels in both performance-bandwidth trade-offs and adaptability within dynamic environments."],"url":"http://arxiv.org/abs/2310.19113v1"}
{"created":"2023-10-29 18:57:15","title":"Efficient IoT Inference via Context-Awareness","abstract":"While existing strategies for optimizing deep learning-based classification models on low-power platforms assume the models are trained on all classes of interest, this paper posits that adopting context-awareness i.e. focusing solely on the likely classes in the current context, can substantially enhance performance in resource-constrained environments. We propose a new paradigm, CACTUS, for scalable and efficient context-aware classification where a micro-classifier recognizes a small set of classes relevant to the current context and, when context change happens, rapidly switches to another suitable micro-classifier. CACTUS has several innovations including optimizing the training cost of context-aware classifiers, enabling on-the-fly context-aware switching between classifiers, and selecting the best context-aware classifiers given limited resources. We show that CACTUS achieves significant benefits in accuracy, latency, and compute budget across a range of datasets and IoT platforms.","sentences":["While existing strategies for optimizing deep learning-based classification models on low-power platforms assume the models are trained on all classes of interest, this paper posits that adopting context-awareness i.e. focusing solely on the likely classes in the current context, can substantially enhance performance in resource-constrained environments.","We propose a new paradigm, CACTUS, for scalable and efficient context-aware classification where a micro-classifier recognizes a small set of classes relevant to the current context and, when context change happens, rapidly switches to another suitable micro-classifier.","CACTUS has several innovations including optimizing the training cost of context-aware classifiers, enabling on-the-fly context-aware switching between classifiers, and selecting the best context-aware classifiers given limited resources.","We show that CACTUS achieves significant benefits in accuracy, latency, and compute budget across a range of datasets and IoT platforms."],"url":"http://arxiv.org/abs/2310.19112v1"}
{"created":"2023-10-29 18:46:33","title":"Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery","abstract":"This paper explores post-disaster analytics using multimodal deep learning models trained with curriculum learning method. Studying post-disaster analytics is important as it plays a crucial role in mitigating the impact of disasters by providing timely and accurate insights into the extent of damage and the allocation of resources. We propose a curriculum learning strategy to enhance the performance of multimodal deep learning models. Curriculum learning emulates the progressive learning sequence in human education by training deep learning models on increasingly complex data. Our primary objective is to develop a curriculum-trained multimodal deep learning model, with a particular focus on visual question answering (VQA) capable of jointly processing image and text data, in conjunction with semantic segmentation for disaster analytics using the FloodNet\\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021} dataset. To achieve this, U-Net model is used for semantic segmentation and image encoding. A custom built text classifier is used for visual question answering. Existing curriculum learning methods rely on manually defined difficulty functions. We introduce a novel curriculum learning approach termed Dynamic Task and Weight Prioritization (DATWEP), which leverages a gradient-based method to automatically decide task difficulty during curriculum learning training, thereby eliminating the need for explicit difficulty computation. The integration of DATWEP into our multimodal model shows improvement on VQA performance. Source code is available at https://github.com/fualsan/DATWEP.","sentences":["This paper explores post-disaster analytics using multimodal deep learning models trained with curriculum learning method.","Studying post-disaster analytics is important as it plays a crucial role in mitigating the impact of disasters by providing timely and accurate insights into the extent of damage and the allocation of resources.","We propose a curriculum learning strategy to enhance the performance of multimodal deep learning models.","Curriculum learning emulates the progressive learning sequence in human education by training deep learning models on increasingly complex data.","Our primary objective is to develop a curriculum-trained multimodal deep learning model, with a particular focus on visual question answering (VQA) capable of jointly processing image and text data, in conjunction with semantic segmentation for disaster analytics using the FloodNet\\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021} dataset.","To achieve this, U-Net model is used for semantic segmentation and image encoding.","A custom built text classifier is used for visual question answering.","Existing curriculum learning methods rely on manually defined difficulty functions.","We introduce a novel curriculum learning approach termed Dynamic Task and Weight Prioritization (DATWEP), which leverages a gradient-based method to automatically decide task difficulty during curriculum learning training, thereby eliminating the need for explicit difficulty computation.","The integration of DATWEP into our multimodal model shows improvement on VQA performance.","Source code is available at https://github.com/fualsan/DATWEP."],"url":"http://arxiv.org/abs/2310.19109v1"}
{"created":"2023-10-29 18:43:19","title":"PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators","abstract":"Navigating the landscape of particle accelerators has become increasingly challenging with recent surges in contributions. These intricate devices challenge comprehension, even within individual facilities. To address this, we introduce PACuna, a fine-tuned language model refined through publicly available accelerator resources like conferences, pre-prints, and books. We automated data collection and question generation to minimize expert involvement and make the data publicly available. PACuna demonstrates proficiency in addressing intricate accelerator questions, validated by experts. Our approach shows adapting language models to scientific domains by fine-tuning technical texts and auto-generated corpora capturing the latest developments can further produce pre-trained models to answer some intricate questions that commercially available assistants cannot and can serve as intelligent assistants for individual facilities.","sentences":["Navigating the landscape of particle accelerators has become increasingly challenging with recent surges in contributions.","These intricate devices challenge comprehension, even within individual facilities.","To address this, we introduce PACuna, a fine-tuned language model refined through publicly available accelerator resources like conferences, pre-prints, and books.","We automated data collection and question generation to minimize expert involvement and make the data publicly available.","PACuna demonstrates proficiency in addressing intricate accelerator questions, validated by experts.","Our approach shows adapting language models to scientific domains by fine-tuning technical texts and auto-generated corpora capturing the latest developments can further produce pre-trained models to answer some intricate questions that commercially available assistants cannot and can serve as intelligent assistants for individual facilities."],"url":"http://arxiv.org/abs/2310.19106v1"}
{"created":"2023-10-29 18:39:23","title":"Updated Standard for Secure Satellite Communications: Analysis of Satellites, Attack Vectors, Existing Standards, and Enterprise and Security Architectures","abstract":"Satellites play a vital role in remote communication where traditional communication mediums struggle to provide benefits over associated costs and efficiency. In recent years, satellite communication has achieved utter interest in the industry due to the achievement of high data rates through the massive deployment of LEO satellites. Because of the complex diversity in types of satellites, communication methodologies, technological obstacles, environmental limitations, elements in the entire ecosystem, massive financial impact, geopolitical conflict and domination, easier access to satellite communications, and various other reasons, the threat vectors are rising in the threat landscape. To achieve resilience against those, only technological solutions are not enough. An effective approach will be through security standards. However, there is a considerable gap in the industry regarding a generic security standard framework for satellite communication and space data systems. A few countries and space agencies have their own standard framework and private policies. However, many of those are either private, serve the specific requirements of specific missions, or have not been updated for a long time.   This project report will focus on identifying, categorizing, comparing, and assessing elements, threat landscape, enterprise security architectures, and available public standards of satellite communication and space data systems. After that, it will utilize the knowledge to propose an updated standard framework for secure satellite communications and space data systems.","sentences":["Satellites play a vital role in remote communication where traditional communication mediums struggle to provide benefits over associated costs and efficiency.","In recent years, satellite communication has achieved utter interest in the industry due to the achievement of high data rates through the massive deployment of LEO satellites.","Because of the complex diversity in types of satellites, communication methodologies, technological obstacles, environmental limitations, elements in the entire ecosystem, massive financial impact, geopolitical conflict and domination, easier access to satellite communications, and various other reasons, the threat vectors are rising in the threat landscape.","To achieve resilience against those, only technological solutions are not enough.","An effective approach will be through security standards.","However, there is a considerable gap in the industry regarding a generic security standard framework for satellite communication and space data systems.","A few countries and space agencies have their own standard framework and private policies.","However, many of those are either private, serve the specific requirements of specific missions, or have not been updated for a long time.   ","This project report will focus on identifying, categorizing, comparing, and assessing elements, threat landscape, enterprise security architectures, and available public standards of satellite communication and space data systems.","After that, it will utilize the knowledge to propose an updated standard framework for secure satellite communications and space data systems."],"url":"http://arxiv.org/abs/2310.19105v1"}
{"created":"2023-10-29 18:35:05","title":"Proving Linear Mode Connectivity of Neural Networks via Optimal Transport","abstract":"The energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures. Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a framework theoretically explaining this empirical observation. Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension of the support of the weight distribution of neurons, which dictates Wasserstein convergence rates is correlated with linear mode connectivity.","sentences":["The energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures.","Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights.","In this paper, we provide a framework theoretically explaining this empirical observation.","Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected.","Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected.","Finally, we empirically demonstrate the validity of our approach by showing how the dimension of the support of the weight distribution of neurons, which dictates Wasserstein convergence rates is correlated with linear mode connectivity."],"url":"http://arxiv.org/abs/2310.19103v1"}
{"created":"2023-10-29 18:33:05","title":"Atom: Low-bit Quantization for Efficient and Accurate LLM Serving","abstract":"The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use GPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity. However, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.   To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization. It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process. We evaluate Atom on 4-bit weight-activation quantization setups in the serving context. Atom improves end-to-end throughput by up to $7.73\\times$ compared to the FP16 and by $2.53\\times$ compared to INT8 quantization, while maintaining the same latency target.","sentences":["The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers.","To efficiently use GPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity.","However, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.   ","To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss.","Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.","It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process.","We evaluate Atom on 4-bit weight-activation quantization setups in the serving context.","Atom improves end-to-end throughput by up to $7.73\\times$ compared to the FP16 and by $2.53\\times$ compared to INT8 quantization, while maintaining the same latency target."],"url":"http://arxiv.org/abs/2310.19102v1"}
{"created":"2023-10-29 18:21:36","title":"Web3 Meets AI Marketplace: Exploring Opportunities, Analyzing Challenges, and Suggesting Solutions","abstract":"Web3 and AI have been among the most discussed fields over the recent years, with substantial hype surrounding each field's potential to transform the world as we know it. However, as the hype settles, it's evident that neither AI nor Web3 can address all challenges independently. Consequently, the intersection of AI and Web3 is gaining increased attention, emerging as a new field with the potential to address the limitations of each. In this article, we will focus on the integration of web3 and the AI marketplace, where AI services and products can be provided in a decentralized manner (DeAI). A comprehensive review is provided by summarizing the opportunities and challenges on this topic. Additionally, we offer analyses and solutions to address these challenges. We've developed a framework that lets users pay with any kind of cryptocurrency to get AI services. Additionally, they can also enjoy AI services for free on our platform by simply locking up their assets temporarily in the protocol. This unique approach is a first in the industry. Before this, offering free AI services in the web3 community wasn't possible. Our solution opens up exciting opportunities for the AI marketplace in the web3 space to grow and be widely adopted.","sentences":["Web3 and AI have been among the most discussed fields over the recent years, with substantial hype surrounding each field's potential to transform the world as we know it.","However, as the hype settles, it's evident that neither AI nor Web3 can address all challenges independently.","Consequently, the intersection of AI and Web3 is gaining increased attention, emerging as a new field with the potential to address the limitations of each.","In this article, we will focus on the integration of web3 and the AI marketplace, where AI services and products can be provided in a decentralized manner (DeAI).","A comprehensive review is provided by summarizing the opportunities and challenges on this topic.","Additionally, we offer analyses and solutions to address these challenges.","We've developed a framework that lets users pay with any kind of cryptocurrency to get AI services.","Additionally, they can also enjoy AI services for free on our platform by simply locking up their assets temporarily in the protocol.","This unique approach is a first in the industry.","Before this, offering free AI services in the web3 community wasn't possible.","Our solution opens up exciting opportunities for the AI marketplace in the web3 space to grow and be widely adopted."],"url":"http://arxiv.org/abs/2310.19099v1"}
{"created":"2023-10-29 18:10:31","title":"Circuit Width Estimation via Effect Typing and Linear Dependency (Long Version)","abstract":"Circuit description languages are a class of quantum programming languages in which programs are classical and produce a description of a quantum computation, in the form of a quantum circuit. Since these programs can leverage all the expressive power of high-level classical languages, circuit description languages have been successfully used to describe complex and practical quantum algorithms, whose circuits, however, may involve many more qubits and gate applications than current quantum architectures can actually muster. In this paper, we present Proto-Quipper-R, a circuit description language endowed with a linear dependent type-and-effect system capable of deriving parametric upper bounds on the width of the circuits produced by a program. We prove both the standard type safety results and that the resulting resource analysis is correct with respect to a big-step operational semantics. We also show that our approach is expressive enough to verify realistic quantum algorithms.","sentences":["Circuit description languages are a class of quantum programming languages in which programs are classical and produce a description of a quantum computation, in the form of a quantum circuit.","Since these programs can leverage all the expressive power of high-level classical languages, circuit description languages have been successfully used to describe complex and practical quantum algorithms, whose circuits, however, may involve many more qubits and gate applications than current quantum architectures can actually muster.","In this paper, we present Proto-Quipper-R, a circuit description language endowed with a linear dependent type-and-effect system capable of deriving parametric upper bounds on the width of the circuits produced by a program.","We prove both the standard type safety results and that the resulting resource analysis is correct with respect to a big-step operational semantics.","We also show that our approach is expressive enough to verify realistic quantum algorithms."],"url":"http://arxiv.org/abs/2310.19096v1"}
{"created":"2023-10-29 17:48:30","title":"Performance Characterization of NVMe Flash Devices with Zoned Namespaces (ZNS)","abstract":"The recent emergence of NVMe flash devices with Zoned Namespace support, ZNS SSDs, represents a significant new advancement in flash storage. ZNS SSDs introduce a new storage abstraction of append-only zones with a set of new I/O (i.e., append) and management (zone state machine transition) commands. With the new abstraction and commands, ZNS SSDs offer more control to the host software stack than a non-zoned SSD for flash management, which is known to be complex (because of garbage collection, scheduling, block allocation, parallelism management, overprovisioning). ZNS SSDs are, consequently, gaining adoption in a variety of applications (e.g., file systems, key-value stores, and databases), particularly latency-sensitive big-data applications. Despite this enthusiasm, there has yet to be a systematic characterization of ZNS SSD performance with its zoned storage model abstractions and I/O operations. This work addresses this crucial shortcoming. We report on the performance features of a commercially available ZNS SSD (13 key observations), explain how these features can be incorporated into publicly available state-of-the-art ZNS emulators, and recommend guidelines for ZNS SSD application developers. All artifacts (code and data sets) of this study are publicly available at https://github.com/stonet-research/NVMeBenchmarks.","sentences":["The recent emergence of NVMe flash devices with Zoned Namespace support, ZNS SSDs, represents a significant new advancement in flash storage.","ZNS SSDs introduce a new storage abstraction of append-only zones with a set of new I/O (i.e., append) and management (zone state machine transition) commands.","With the new abstraction and commands, ZNS SSDs offer more control to the host software stack than a non-zoned SSD for flash management, which is known to be complex (because of garbage collection, scheduling, block allocation, parallelism management, overprovisioning).","ZNS SSDs are, consequently, gaining adoption in a variety of applications (e.g., file systems, key-value stores, and databases), particularly latency-sensitive big-data applications.","Despite this enthusiasm, there has yet to be a systematic characterization of ZNS SSD performance with its zoned storage model abstractions and I/O operations.","This work addresses this crucial shortcoming.","We report on the performance features of a commercially available ZNS SSD (13 key observations), explain how these features can be incorporated into publicly available state-of-the-art ZNS emulators, and recommend guidelines for ZNS SSD application developers.","All artifacts (code and data sets) of this study are publicly available at https://github.com/stonet-research/NVMeBenchmarks."],"url":"http://arxiv.org/abs/2310.19094v1"}
{"created":"2023-10-29 17:47:45","title":"Extending the Cooperative Dual-Task Space in Conformal Geometric Algebra","abstract":"In this work, we are presenting an extension of the cooperative dual-task space (CDTS) in conformal geometric algebra. The CDTS was first defined using dual quaternion algebra and is a well established framework for the simplified definition of tasks using two manipulators. By integrating conformal geometric algebra, we aim to further enhance the geometric expressiveness and thus simplify the modeling of various tasks. We show this formulation by first presenting the CDTS and then its extension that is based around a cooperative pointpair. This extension keeps all the benefits of the original formulation that is based on dual quaternions, but adds more tools for geometric modeling of the dual-arm tasks. We also present how this CGA-CDTS can be seamlessly integrated with an optimal control framework in geometric algebra that was derived in previous work. In the experiments, we demonstrate how to model different objectives and constraints using the CGA-CDTS. Using a setup of two Franka Emika robots we then show the effectiveness of our approach using model predictive control in real world experiments.","sentences":["In this work, we are presenting an extension of the cooperative dual-task space (CDTS) in conformal geometric algebra.","The CDTS was first defined using dual quaternion algebra and is a well established framework for the simplified definition of tasks using two manipulators.","By integrating conformal geometric algebra, we aim to further enhance the geometric expressiveness and thus simplify the modeling of various tasks.","We show this formulation by first presenting the CDTS and then its extension that is based around a cooperative pointpair.","This extension keeps all the benefits of the original formulation that is based on dual quaternions, but adds more tools for geometric modeling of the dual-arm tasks.","We also present how this CGA-CDTS can be seamlessly integrated with an optimal control framework in geometric algebra that was derived in previous work.","In the experiments, we demonstrate how to model different objectives and constraints using the CGA-CDTS.","Using a setup of two Franka Emika robots we then show the effectiveness of our approach using model predictive control in real world experiments."],"url":"http://arxiv.org/abs/2310.19093v1"}
{"created":"2023-10-29 17:44:48","title":"Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector","abstract":"Machine Learning (ML) systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, they still face the challenge of aligning intricate and nuanced policy objectives with the precise formalization requirements necessitated by ML models. In this paper, we aim to bridge the gap between ML and public sector decision-making by presenting a comprehensive overview of key technical challenges where disjunctions between policy goals and ML models commonly arise. We concentrate on pivotal points of the ML pipeline that connect the model to its operational environment, delving into the significance of representative training data and highlighting the importance of a model setup that facilitates effective decision-making. Additionally, we link these challenges with emerging methodological advancements, encompassing causal ML, domain adaptation, uncertainty quantification, and multi-objective optimization, illustrating the path forward for harmonizing ML and public sector objectives.","sentences":["Machine Learning (ML) systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health.","While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, they still face the challenge of aligning intricate and nuanced policy objectives with the precise formalization requirements necessitated by ML models.","In this paper, we aim to bridge the gap between ML and public sector decision-making by presenting a comprehensive overview of key technical challenges where disjunctions between policy goals and ML models commonly arise.","We concentrate on pivotal points of the ML pipeline that connect the model to its operational environment, delving into the significance of representative training data and highlighting the importance of a model setup that facilitates effective decision-making.","Additionally, we link these challenges with emerging methodological advancements, encompassing causal ML, domain adaptation, uncertainty quantification, and multi-objective optimization, illustrating the path forward for harmonizing ML and public sector objectives."],"url":"http://arxiv.org/abs/2310.19091v1"}
{"created":"2023-10-29 17:43:21","title":"gafro: Geometric Algebra for Robotics","abstract":"Geometry is a fundamental part of robotics and there have been various frameworks of representation over the years. Recently, geometric algebra has gained attention for its property of unifying many of those previous ideas into one algebra. While there are already efficient open-source implementations of geometric algebra available, none of them is targeted at robotics applications. We want to address this shortcoming with our library gafro. This article presents an overview of the implementation details as well as a tutorial of gafro, an efficient c++ library targeting robotics applications using geometric algebra. The library focuses on using conformal geometric algebra. Hence, various geometric primitives are available for computation as well as rigid body transformations. The modeling of robotic systems is also an important aspect of the library. It implements various algorithms for calculating the kinematics and dynamics of such systems as well as objectives for optimisation problems. The software stack is completed by python bindings in pygafro and a ROS interface in gafro_ros.","sentences":["Geometry is a fundamental part of robotics and there have been various frameworks of representation over the years.","Recently, geometric algebra has gained attention for its property of unifying many of those previous ideas into one algebra.","While there are already efficient open-source implementations of geometric algebra available, none of them is targeted at robotics applications.","We want to address this shortcoming with our library gafro.","This article presents an overview of the implementation details as well as a tutorial of gafro, an efficient c++ library targeting robotics applications using geometric algebra.","The library focuses on using conformal geometric algebra.","Hence, various geometric primitives are available for computation as well as rigid body transformations.","The modeling of robotic systems is also an important aspect of the library.","It implements various algorithms for calculating the kinematics and dynamics of such systems as well as objectives for optimisation problems.","The software stack is completed by python bindings in pygafro and a ROS interface in gafro_ros."],"url":"http://arxiv.org/abs/2310.19090v1"}
{"created":"2023-10-29 17:27:18","title":"Pushdown Layers: Encoding Recursive Structure in Transformer Language Models","abstract":"Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix. Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens -- for instance, learning to \"skip\" over closed constituents. When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities. Pushdown Layers are a drop-in replacement for standard self-attention. We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks.","sentences":["Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism.","Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization.","This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix.","Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens -- for instance, learning to \"skip\" over closed constituents.","When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities.","Pushdown Layers are a drop-in replacement for standard self-attention.","We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks."],"url":"http://arxiv.org/abs/2310.19089v1"}
{"created":"2023-10-29 17:16:40","title":"Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention","abstract":"Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these factors affect the models' language perception is unclear. This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception. Results show that scaling enhances the human resemblance and improves the effective attention by reducing the trivial pattern reliance, while instruction tuning does not. However, instruction tuning significantly enhances the models' sensitivity to instructions. We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models. Our code and data used in the analysis is available on GitHub.","sentences":["Recent large language models (LLMs) have revealed strong abilities to understand natural language.","Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning.","However, how these factors affect the models' language perception is unclear.","This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception.","Results show that scaling enhances the human resemblance and improves the effective attention by reducing the trivial pattern reliance, while instruction tuning does not.","However, instruction tuning significantly enhances the models' sensitivity to instructions.","We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models.","Our code and data used in the analysis is available on GitHub."],"url":"http://arxiv.org/abs/2310.19084v1"}
{"created":"2023-10-29 17:04:24","title":"Deep Audio Analyzer: a Framework to Industrialize the Research on Audio Forensics","abstract":"Deep Audio Analyzer is an open source speech framework that aims to simplify the research and the development process of neural speech processing pipelines, allowing users to conceive, compare and share results in a fast and reproducible way. This paper describes the core architecture designed to support several tasks of common interest in the audio forensics field, showing possibility of creating new tasks thus customizing the framework. By means of Deep Audio Analyzer, forensics examiners (i.e. from Law Enforcement Agencies) and researchers will be able to visualize audio features, easily evaluate performances on pretrained models, to create, export and share new audio analysis workflows by combining deep neural network models with few clicks. One of the advantages of this tool is to speed up research and practical experimentation, in the field of audio forensics analysis thus also improving experimental reproducibility by exporting and sharing pipelines. All features are developed in modules accessible by the user through a Graphic User Interface. Index Terms: Speech Processing, Deep Learning Audio, Deep Learning Audio Pipeline creation, Audio Forensics.","sentences":["Deep Audio Analyzer is an open source speech framework that aims to simplify the research and the development process of neural speech processing pipelines, allowing users to conceive, compare and share results in a fast and reproducible way.","This paper describes the core architecture designed to support several tasks of common interest in the audio forensics field, showing possibility of creating new tasks thus customizing the framework.","By means of Deep Audio Analyzer, forensics examiners (i.e. from Law Enforcement Agencies) and researchers will be able to visualize audio features, easily evaluate performances on pretrained models, to create, export and share new audio analysis workflows by combining deep neural network models with few clicks.","One of the advantages of this tool is to speed up research and practical experimentation, in the field of audio forensics analysis thus also improving experimental reproducibility by exporting and sharing pipelines.","All features are developed in modules accessible by the user through a Graphic User Interface.","Index Terms: Speech Processing, Deep Learning Audio, Deep Learning Audio Pipeline creation, Audio Forensics."],"url":"http://arxiv.org/abs/2310.19081v1"}
{"created":"2023-10-29 17:03:12","title":"Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery","abstract":"Recent advances in machine learning have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences. Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous vehicles -- where alignment with human expectations can be imperative. In this paper, we propose to adapt similar RL-based methods to unsupervised object discovery, i.e. learning to detect objects from LiDAR points without any training labels. Instead of labels, we use simple heuristics to mimic human feedback. More explicitly, we combine multiple heuristics into a simple reward function that positively correlates its score with bounding box accuracy, \\ie, boxes containing objects are scored higher than those without. We start from the detector's own predictions to explore the space and reinforce boxes with high rewards through gradient updates. Empirically, we demonstrate that our approach is not only more accurate, but also orders of magnitudes faster to train compared to prior works on object discovery.","sentences":["Recent advances in machine learning have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences.","Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous vehicles -- where alignment with human expectations can be imperative.","In this paper, we propose to adapt similar RL-based methods to unsupervised object discovery, i.e. learning to detect objects from LiDAR points without any training labels.","Instead of labels, we use simple heuristics to mimic human feedback.","More explicitly, we combine multiple heuristics into a simple reward function that positively correlates its score with bounding box accuracy, \\ie, boxes containing objects are scored higher than those without.","We start from the detector's own predictions to explore the space and reinforce boxes with high rewards through gradient updates.","Empirically, we demonstrate that our approach is not only more accurate, but also orders of magnitudes faster to train compared to prior works on object discovery."],"url":"http://arxiv.org/abs/2310.19080v1"}
{"created":"2023-10-29 17:01:03","title":"Digital Twin-Driven Network Architecture for Video Streaming","abstract":"Digital twin (DT) is revolutionizing the emerging video streaming services through tailored network management. By integrating diverse advanced communication technologies, DTs are promised to construct a holistic virtualized network for better network management performance. To this end, we develop a DT-driven network architecture for video streaming (DTN4VS) to enable network virtualization and tailored network management. With the architecture, various types of DTs can characterize physical entities' status, separate the network management functions from the network controller, and empower the functions with emulated data and tailored strategies. To further enhance network management performance, three potential approaches are proposed, i.e., domain data exploitation, performance evaluation, and adaptive DT model update. We present a case study pertaining to DT-assisted network slicing for short video streaming, followed by some open research issues for DTN4VS.","sentences":["Digital twin (DT) is revolutionizing the emerging video streaming services through tailored network management.","By integrating diverse advanced communication technologies, DTs are promised to construct a holistic virtualized network for better network management performance.","To this end, we develop a DT-driven network architecture for video streaming (DTN4VS) to enable network virtualization and tailored network management.","With the architecture, various types of DTs can characterize physical entities' status, separate the network management functions from the network controller, and empower the functions with emulated data and tailored strategies.","To further enhance network management performance, three potential approaches are proposed, i.e., domain data exploitation, performance evaluation, and adaptive DT model update.","We present a case study pertaining to DT-assisted network slicing for short video streaming, followed by some open research issues for DTN4VS."],"url":"http://arxiv.org/abs/2310.19079v1"}
{"created":"2023-10-29 17:00:19","title":"Near-Optimal Packet Scheduling in Multihop Networks with End-to-End Deadline Constraints","abstract":"Scheduling packets with end-to-end deadline constraints in multihop networks is an important problem that has been notoriously difficult to tackle. Recently, there has been progress on this problem in the worst-case traffic setting, with the objective of maximizing the number of packets delivered within their deadlines. Specifically, the proposed algorithms were shown to achieve $\\Omega(1/\\log(L))$ fraction of the optimal objective value if the minimum link capacity in the network is $C_{\\min}=\\Omega(\\log (L))$, where $L$ is the maximum length of a packet's route in the network (which is bounded by the packet's maximum deadline). However, such guarantees can be quite pessimistic due to the strict worst-case traffic assumption and may not accurately reflect real-world settings. In this work, we aim to address this limitation by exploring whether it is possible to design algorithms that achieve a constant fraction of the optimal value while relaxing the worst-case traffic assumption.   We provide a positive answer by demonstrating that in stochastic traffic settings, such as i.i.d. packet arrivals, near-optimal, $(1-\\epsilon)$-approximation algorithms can be designed if $C_{\\min} = \\Omega\\big(\\frac{\\log (L/\\epsilon) } {\\epsilon^2}\\big)$. To the best of our knowledge, this is the first result that shows this problem can be solved near-optimally under nontrivial assumptions on traffic and link capacity. We further present extended simulations using real network traces with non-stationary traffic, which demonstrate that our algorithms outperform worst-case-based algorithms in practical settings.","sentences":["Scheduling packets with end-to-end deadline constraints in multihop networks is an important problem that has been notoriously difficult to tackle.","Recently, there has been progress on this problem in the worst-case traffic setting, with the objective of maximizing the number of packets delivered within their deadlines.","Specifically, the proposed algorithms were shown to achieve $\\Omega(1/\\log(L))$ fraction of the optimal objective value if the minimum link capacity in the network is $C_{\\min}=\\Omega(\\log (L))$, where $L$ is the maximum length of a packet's route in the network (which is bounded by the packet's maximum deadline).","However, such guarantees can be quite pessimistic due to the strict worst-case traffic assumption and may not accurately reflect real-world settings.","In this work, we aim to address this limitation by exploring whether it is possible to design algorithms that achieve a constant fraction of the optimal value while relaxing the worst-case traffic assumption.   ","We provide a positive answer by demonstrating that in stochastic traffic settings, such as i.i.d. packet arrivals, near-optimal, $(1-\\epsilon)$-approximation algorithms can be designed if $C_{\\min} = \\Omega\\big(\\frac{\\log (L/\\epsilon) } {\\epsilon^2}\\big)$. To the best of our knowledge, this is the first result that shows this problem can be solved near-optimally under nontrivial assumptions on traffic and link capacity.","We further present extended simulations using real network traces with non-stationary traffic, which demonstrate that our algorithms outperform worst-case-based algorithms in practical settings."],"url":"http://arxiv.org/abs/2310.19077v1"}
{"created":"2023-10-29 16:58:31","title":"Bespoke Solvers for Generative Flow Models","abstract":"Diffusion or flow-based models are powerful generative paradigms that are notoriously hard to sample as samples are defined as solutions to high-dimensional Ordinary or Stochastic Differential Equations (ODEs/SDEs) which require a large Number of Function Evaluations (NFE) to approximate well. Existing methods to alleviate the costly sampling process include model distillation and designing dedicated ODE solvers. However, distillation is costly to train and sometimes can deteriorate quality, while dedicated solvers still require relatively large NFE to produce high quality samples. In this paper we introduce \"Bespoke solvers\", a novel framework for constructing custom ODE solvers tailored to the ODE of a given pre-trained flow model. Our approach optimizes an order consistent and parameter-efficient solver (e.g., with 80 learnable parameters), is trained for roughly 1% of the GPU time required for training the pre-trained model, and significantly improves approximation and generation quality compared to dedicated solvers. For example, a Bespoke solver for a CIFAR10 model produces samples with Fr\\'echet Inception Distance (FID) of 2.73 with 10 NFE, and gets to 1% of the Ground Truth (GT) FID (2.59) for this model with only 20 NFE. On the more challenging ImageNet-64$\\times$64, Bespoke samples at 2.2 FID with 10 NFE, and gets within 2% of GT FID (1.71) with 20 NFE.","sentences":["Diffusion or flow-based models are powerful generative paradigms that are notoriously hard to sample as samples are defined as solutions to high-dimensional Ordinary or Stochastic Differential Equations (ODEs/SDEs) which require a large Number of Function Evaluations (NFE) to approximate well.","Existing methods to alleviate the costly sampling process include model distillation and designing dedicated ODE solvers.","However, distillation is costly to train and sometimes can deteriorate quality, while dedicated solvers still require relatively large NFE to produce high quality samples.","In this paper we introduce \"Bespoke solvers\", a novel framework for constructing custom ODE solvers tailored to the ODE of a given pre-trained flow model.","Our approach optimizes an order consistent and parameter-efficient solver (e.g., with 80 learnable parameters), is trained for roughly 1% of the GPU time required for training the pre-trained model, and significantly improves approximation and generation quality compared to dedicated solvers.","For example, a Bespoke solver for a CIFAR10 model produces samples with Fr\\'echet Inception Distance (FID) of 2.73 with 10 NFE, and gets to 1% of the Ground Truth (GT) FID (2.59) for this model with only 20 NFE.","On the more challenging ImageNet-64$\\times$64, Bespoke samples at 2.2 FID with 10 NFE, and gets within 2% of GT FID (1.71) with 20 NFE."],"url":"http://arxiv.org/abs/2310.19075v1"}
{"created":"2023-10-29 16:49:45","title":"Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection","abstract":"Existing industrial anomaly detection (IAD) methods predict anomaly scores for both anomaly detection and localization. However, they struggle to perform a multi-turn dialog and detailed descriptions for anomaly regions, e.g., color, shape, and categories of industrial anomalies. Recently, large multimodal (i.e., vision and language) models (LMMs) have shown eminent perception abilities on multiple vision tasks such as image captioning, visual understanding, visual reasoning, etc., making it a competitive potential choice for more comprehensible anomaly detection. However, the knowledge about anomaly detection is absent in existing general LMMs, while training a specific LMM for anomaly detection requires a tremendous amount of annotated data and massive computation resources. In this paper, we propose a novel large multi-modal model by applying vision experts for industrial anomaly detection (dubbed Myriad), which leads to definite anomaly detection and high-quality anomaly description. Specifically, we adopt MiniGPT-4 as the base LMM and design an Expert Perception module to embed the prior knowledge from vision experts as tokens which are intelligible to Large Language Models (LLMs). To compensate for the errors and confusions of vision experts, we introduce a domain adapter to bridge the visual representation gaps between generic and industrial images. Furthermore, we propose a Vision Expert Instructor, which enables the Q-Former to generate IAD domain vision-language tokens according to vision expert prior. Extensive experiments on MVTec-AD and VisA benchmarks demonstrate that our proposed method not only performs favorably against state-of-the-art methods under the 1-class and few-shot settings, but also provide definite anomaly prediction along with detailed descriptions in IAD domain.","sentences":["Existing industrial anomaly detection (IAD) methods predict anomaly scores for both anomaly detection and localization.","However, they struggle to perform a multi-turn dialog and detailed descriptions for anomaly regions, e.g., color, shape, and categories of industrial anomalies.","Recently, large multimodal (i.e., vision and language) models (LMMs) have shown eminent perception abilities on multiple vision tasks such as image captioning, visual understanding, visual reasoning, etc., making it a competitive potential choice for more comprehensible anomaly detection.","However, the knowledge about anomaly detection is absent in existing general LMMs, while training a specific LMM for anomaly detection requires a tremendous amount of annotated data and massive computation resources.","In this paper, we propose a novel large multi-modal model by applying vision experts for industrial anomaly detection (dubbed Myriad), which leads to definite anomaly detection and high-quality anomaly description.","Specifically, we adopt MiniGPT-4 as the base LMM and design an Expert Perception module to embed the prior knowledge from vision experts as tokens which are intelligible to Large Language Models (LLMs).","To compensate for the errors and confusions of vision experts, we introduce a domain adapter to bridge the visual representation gaps between generic and industrial images.","Furthermore, we propose a Vision Expert Instructor, which enables the Q-Former to generate IAD domain vision-language tokens according to vision expert prior.","Extensive experiments on MVTec-AD and VisA benchmarks demonstrate that our proposed method not only performs favorably against state-of-the-art methods under the 1-class and few-shot settings, but also provide definite anomaly prediction along with detailed descriptions in IAD domain."],"url":"http://arxiv.org/abs/2310.19070v1"}
{"created":"2023-10-29 16:46:50","title":"Efficient Cluster Selection for Personalized Federated Learning: A Multi-Armed Bandit Approach","abstract":"Federated learning (FL) offers a decentralized training approach for machine learning models, prioritizing data privacy. However, the inherent heterogeneity in FL networks, arising from variations in data distribution, size, and device capabilities, poses challenges in user federation. Recognizing this, Personalized Federated Learning (PFL) emphasizes tailoring learning processes to individual data profiles. In this paper, we address the complexity of clustering users in PFL, especially in dynamic networks, by introducing a dynamic Upper Confidence Bound (dUCB) algorithm inspired by the multi-armed bandit (MAB) approach. The dUCB algorithm ensures that new users can effectively find the best cluster for their data distribution by balancing exploration and exploitation. The performance of our algorithm is evaluated in various cases, showing its effectiveness in handling dynamic federated learning scenarios.","sentences":["Federated learning (FL) offers a decentralized training approach for machine learning models, prioritizing data privacy.","However, the inherent heterogeneity in FL networks, arising from variations in data distribution, size, and device capabilities, poses challenges in user federation.","Recognizing this, Personalized Federated Learning (PFL) emphasizes tailoring learning processes to individual data profiles.","In this paper, we address the complexity of clustering users in PFL, especially in dynamic networks, by introducing a dynamic Upper Confidence Bound (dUCB) algorithm inspired by the multi-armed bandit (MAB) approach.","The dUCB algorithm ensures that new users can effectively find the best cluster for their data distribution by balancing exploration and exploitation.","The performance of our algorithm is evaluated in various cases, showing its effectiveness in handling dynamic federated learning scenarios."],"url":"http://arxiv.org/abs/2310.19069v1"}
{"created":"2023-10-29 16:46:26","title":"Expanding memory in recurrent spiking networks","abstract":"Recurrent spiking neural networks (RSNNs) are notoriously difficult to train because of the vanishing gradient problem that is enhanced by the binary nature of the spikes. In this paper, we review the ability of the current state-of-the-art RSNNs to solve long-term memory tasks, and show that they have strong constraints both in performance, and for their implementation on hardware analog neuromorphic processors. We present a novel spiking neural network that circumvents these limitations. Our biologically inspired neural network uses synaptic delays, branching factor regularization and a novel surrogate derivative for the spiking function. The proposed network proves to be more successful in using the recurrent connections on memory tasks.","sentences":["Recurrent spiking neural networks (RSNNs) are notoriously difficult to train because of the vanishing gradient problem that is enhanced by the binary nature of the spikes.","In this paper, we review the ability of the current state-of-the-art RSNNs to solve long-term memory tasks, and show that they have strong constraints both in performance, and for their implementation on hardware analog neuromorphic processors.","We present a novel spiking neural network that circumvents these limitations.","Our biologically inspired neural network uses synaptic delays, branching factor regularization and a novel surrogate derivative for the spiking function.","The proposed network proves to be more successful in using the recurrent connections on memory tasks."],"url":"http://arxiv.org/abs/2310.19067v1"}
{"created":"2023-10-29 16:46:26","title":"Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile Streaming","abstract":"Sketching algorithms have recently proven to be a powerful approach both for designing low-space streaming algorithms as well as fast polynomial time approximation schemes (PTAS). In this work, we develop new techniques to extend the applicability of sketching-based approaches to the sparse dictionary learning and the Euclidean $k$-means clustering problems. In particular, we initiate the study of the challenging setting where the dictionary/clustering assignment for each of the $n$ input points must be output, which has surprisingly received little attention in prior work. On the fast algorithms front, we obtain a new approach for designing PTAS's for the $k$-means clustering problem, which generalizes to the first PTAS for the sparse dictionary learning problem. On the streaming algorithms front, we obtain new upper bounds and lower bounds for dictionary learning and $k$-means clustering. In particular, given a design matrix $\\mathbf A\\in\\mathbb R^{n\\times d}$ in a turnstile stream, we show an $\\tilde O(nr/\\epsilon^2 + dk/\\epsilon)$ space upper bound for $r$-sparse dictionary learning of size $k$, an $\\tilde O(n/\\epsilon^2 + dk/\\epsilon)$ space upper bound for $k$-means clustering, as well as an $\\tilde O(n)$ space upper bound for $k$-means clustering on random order row insertion streams with a natural \"bounded sensitivity\" assumption. On the lower bounds side, we obtain a general $\\tilde\\Omega(n/\\epsilon + dk/\\epsilon)$ lower bound for $k$-means clustering, as well as an $\\tilde\\Omega(n/\\epsilon^2)$ lower bound for algorithms which can estimate the cost of a single fixed set of candidate centers.","sentences":["Sketching algorithms have recently proven to be a powerful approach both for designing low-space streaming algorithms as well as fast polynomial time approximation schemes (PTAS).","In this work, we develop new techniques to extend the applicability of sketching-based approaches to the sparse dictionary learning and the Euclidean $k$-means clustering problems.","In particular, we initiate the study of the challenging setting where the dictionary/clustering assignment for each of the $n$ input points must be output, which has surprisingly received little attention in prior work.","On the fast algorithms front, we obtain a new approach for designing PTAS's for the $k$-means clustering problem, which generalizes to the first PTAS for the sparse dictionary learning problem.","On the streaming algorithms front, we obtain new upper bounds and lower bounds for dictionary learning and $k$-means clustering.","In particular, given a design matrix $\\mathbf A\\in\\mathbb R^{n\\times d}$ in a turnstile stream, we show an $\\tilde O(nr/\\epsilon^2 + dk/\\epsilon)$ space upper bound for $r$-sparse dictionary learning of size $k$, an $\\tilde O(n/\\epsilon^2 + dk/\\epsilon)$ space upper bound for $k$-means clustering, as well as an $\\tilde O(n)$ space upper bound for $k$-means clustering on random order row insertion streams with a natural \"bounded sensitivity\" assumption.","On the lower bounds side, we obtain a general $\\tilde\\Omega(n/\\epsilon + dk/\\epsilon)$ lower bound for $k$-means clustering, as well as an $\\tilde\\Omega(n/\\epsilon^2)$ lower bound for algorithms which can estimate the cost of a single fixed set of candidate centers."],"url":"http://arxiv.org/abs/2310.19068v1"}
{"created":"2023-10-29 16:46:05","title":"Gauge-optimal approximate learning for small data classification problems","abstract":"Small data learning problems are characterized by a significant discrepancy between the limited amount of response variable observations and the large feature space dimension. In this setting, the common learning tools struggle to identify the features important for the classification task from those that bear no relevant information, and cannot derive an appropriate learning rule which allows to discriminate between different classes. As a potential solution to this problem, here we exploit the idea of reducing and rotating the feature space in a lower-dimensional gauge and propose the Gauge-Optimal Approximate Learning (GOAL) algorithm, which provides an analytically tractable joint solution to the dimension reduction, feature segmentation and classification problems for small data learning problems. We prove that the optimal solution of the GOAL algorithm consists in piecewise-linear functions in the Euclidean space, and that it can be approximated through a monotonically convergent algorithm which presents -- under the assumption of a discrete segmentation of the feature space -- a closed-form solution for each optimization substep and an overall linear iteration cost scaling. The GOAL algorithm has been compared to other state-of-the-art machine learning (ML) tools on both synthetic data and challenging real-world applications from climate science and bioinformatics (i.e., prediction of the El Nino Southern Oscillation and inference of epigenetically-induced gene-activity networks from limited experimental data). The experimental results show that the proposed algorithm outperforms the reported best competitors for these problems both in learning performance and computational cost.","sentences":["Small data learning problems are characterized by a significant discrepancy between the limited amount of response variable observations and the large feature space dimension.","In this setting, the common learning tools struggle to identify the features important for the classification task from those that bear no relevant information, and cannot derive an appropriate learning rule which allows to discriminate between different classes.","As a potential solution to this problem, here we exploit the idea of reducing and rotating the feature space in a lower-dimensional gauge and propose the Gauge-Optimal Approximate Learning (GOAL) algorithm, which provides an analytically tractable joint solution to the dimension reduction, feature segmentation and classification problems for small data learning problems.","We prove that the optimal solution of the GOAL algorithm consists in piecewise-linear functions in the Euclidean space, and that it can be approximated through a monotonically convergent algorithm which presents -- under the assumption of a discrete segmentation of the feature space -- a closed-form solution for each optimization substep and an overall linear iteration cost scaling.","The GOAL algorithm has been compared to other state-of-the-art machine learning (ML) tools on both synthetic data and challenging real-world applications from climate science and bioinformatics (i.e., prediction of the El Nino Southern Oscillation and inference of epigenetically-induced gene-activity networks from limited experimental data).","The experimental results show that the proposed algorithm outperforms the reported best competitors for these problems both in learning performance and computational cost."],"url":"http://arxiv.org/abs/2310.19066v1"}
{"created":"2023-10-29 16:45:20","title":"Evaluating LLP Methods: Challenges and Approaches","abstract":"Learning from Label Proportions (LLP) is an established machine learning problem with numerous real-world applications. In this setting, data items are grouped into bags, and the goal is to learn individual item labels, knowing only the features of the data and the proportions of labels in each bag. Although LLP is a well-established problem, it has several unusual aspects that create challenges for benchmarking learning methods. Fundamental complications arise because of the existence of different LLP variants, i.e., dependence structures that can exist between items, labels, and bags. Accordingly, the first algorithmic challenge is the generation of variant-specific datasets capturing the diversity of dependence structures and bag characteristics. The second methodological challenge is model selection, i.e., hyperparameter tuning; due to the nature of LLP, model selection cannot easily use the standard machine learning paradigm. The final benchmarking challenge consists of properly evaluating LLP solution methods across various LLP variants. We note that there is very little consideration of these issues in prior work, and there are no general solutions for these challenges proposed to date. To address these challenges, we develop methods capable of generating LLP datasets meeting the requirements of different variants. We use these methods to generate a collection of datasets encompassing the spectrum of LLP problem characteristics, which can be used in future evaluation studies. Additionally, we develop guidelines for benchmarking LLP algorithms, including the model selection and evaluation steps. Finally, we illustrate the new methods and guidelines by performing an extensive benchmark of a set of well-known LLP algorithms. We show that choosing the best algorithm depends critically on the LLP variant and model selection method, demonstrating the need for our proposed approach.","sentences":["Learning from Label Proportions (LLP) is an established machine learning problem with numerous real-world applications.","In this setting, data items are grouped into bags, and the goal is to learn individual item labels, knowing only the features of the data and the proportions of labels in each bag.","Although LLP is a well-established problem, it has several unusual aspects that create challenges for benchmarking learning methods.","Fundamental complications arise because of the existence of different LLP variants, i.e., dependence structures that can exist between items, labels, and bags.","Accordingly, the first algorithmic challenge is the generation of variant-specific datasets capturing the diversity of dependence structures and bag characteristics.","The second methodological challenge is model selection, i.e., hyperparameter tuning; due to the nature of LLP, model selection cannot easily use the standard machine learning paradigm.","The final benchmarking challenge consists of properly evaluating LLP solution methods across various LLP variants.","We note that there is very little consideration of these issues in prior work, and there are no general solutions for these challenges proposed to date.","To address these challenges, we develop methods capable of generating LLP datasets meeting the requirements of different variants.","We use these methods to generate a collection of datasets encompassing the spectrum of LLP problem characteristics, which can be used in future evaluation studies.","Additionally, we develop guidelines for benchmarking LLP algorithms, including the model selection and evaluation steps.","Finally, we illustrate the new methods and guidelines by performing an extensive benchmark of a set of well-known LLP algorithms.","We show that choosing the best algorithm depends critically on the LLP variant and model selection method, demonstrating the need for our proposed approach."],"url":"http://arxiv.org/abs/2310.19065v1"}
{"created":"2023-10-29 16:37:51","title":"Revisiting the Learnability of Apple Tasting","abstract":"In online binary classification under \\textit{apple tasting} feedback, the learner only observes the true label if it predicts \"1\". First studied by \\cite{helmbold2000apple}, we revisit this classical partial-feedback setting and study online learnability from a combinatorial perspective. We show that the Littlestone dimension continues to prove a tight quantitative characterization of apple tasting in the agnostic setting, closing an open question posed by \\cite{helmbold2000apple}. In addition, we give a new combinatorial parameter, called the Effective width, that tightly quantifies the minimax expected mistakes in the realizable setting. As a corollary, we use the Effective width to establish a \\textit{trichotomy} of the minimax expected number of mistakes in the realizable setting. In particular, we show that in the realizable setting, the expected number of mistakes for any learner under apple tasting feedback can only be $\\Theta(1), \\Theta(\\sqrt{T})$, or $\\Theta(T)$.","sentences":["In online binary classification under \\textit{apple tasting} feedback, the learner only observes the true label if it predicts \"1\".","First studied by \\cite{helmbold2000apple}, we revisit this classical partial-feedback setting and study online learnability from a combinatorial perspective.","We show that the Littlestone dimension continues to prove a tight quantitative characterization of apple tasting in the agnostic setting, closing an open question posed by \\cite{helmbold2000apple}.","In addition, we give a new combinatorial parameter, called the Effective width, that tightly quantifies the minimax expected mistakes in the realizable setting.","As a corollary, we use the Effective width to establish a \\textit{trichotomy} of the minimax expected number of mistakes in the realizable setting.","In particular, we show that in the realizable setting, the expected number of mistakes for any learner under apple tasting feedback can only be $\\Theta(1), \\Theta(\\sqrt{T})$, or $\\Theta(T)$."],"url":"http://arxiv.org/abs/2310.19064v1"}
{"created":"2023-10-29 16:37:14","title":"Feature Aggregation in Joint Sound Classification and Localization Neural Networks","abstract":"This study addresses the application of deep learning techniques in joint sound signal classification and localization networks. Current state-of-the-art sound source localization deep learning networks lack feature aggregation within their architecture. Feature aggregation enhances model performance by enabling the consolidation of information from different feature scales, thereby improving feature robustness and invariance. This is particularly important in SSL networks, which must differentiate direct and indirect acoustic signals. To address this gap, we adapt feature aggregation techniques from computer vision neural networks to signal detection neural networks. Additionally, we propose the Scale Encoding Network (SEN) for feature aggregation to encode features from various scales, compressing the network for more computationally efficient aggregation. To evaluate the efficacy of feature aggregation in SSL networks, we integrated the following computer vision feature aggregation sub-architectures into a SSL control architecture: Path Aggregation Network (PANet), Weighted Bi-directional Feature Pyramid Network (BiFPN), and SEN. These sub-architectures were evaluated using two metrics for signal classification and two metrics for direction-of-arrival regression. PANet and BiFPN are established aggregators in computer vision models, while the proposed SEN is a more compact aggregator. The results suggest that models incorporating feature aggregations outperformed the control model, the Sound Event Localization and Detection network (SELDnet), in both sound signal classification and localization. The feature aggregation techniques enhance the performance of sound detection neural networks, particularly in direction-of-arrival regression.","sentences":["This study addresses the application of deep learning techniques in joint sound signal classification and localization networks.","Current state-of-the-art sound source localization deep learning networks lack feature aggregation within their architecture.","Feature aggregation enhances model performance by enabling the consolidation of information from different feature scales, thereby improving feature robustness and invariance.","This is particularly important in SSL networks, which must differentiate direct and indirect acoustic signals.","To address this gap, we adapt feature aggregation techniques from computer vision neural networks to signal detection neural networks.","Additionally, we propose the Scale Encoding Network (SEN) for feature aggregation to encode features from various scales, compressing the network for more computationally efficient aggregation.","To evaluate the efficacy of feature aggregation in SSL networks, we integrated the following computer vision feature aggregation sub-architectures into a SSL control architecture: Path Aggregation Network (PANet), Weighted Bi-directional Feature Pyramid Network (BiFPN), and SEN.","These sub-architectures were evaluated using two metrics for signal classification and two metrics for direction-of-arrival regression.","PANet and BiFPN are established aggregators in computer vision models, while the proposed SEN is a more compact aggregator.","The results suggest that models incorporating feature aggregations outperformed the control model, the Sound Event Localization and Detection network (SELDnet), in both sound signal classification and localization.","The feature aggregation techniques enhance the performance of sound detection neural networks, particularly in direction-of-arrival regression."],"url":"http://arxiv.org/abs/2310.19063v1"}
{"created":"2023-10-29 16:35:29","title":"A multi-modal table tennis robot system","abstract":"In recent years, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we show how combining the output of an event-based camera and a Spiking Neural Network (SNN) can be used for accurate ball detection.","sentences":["In recent years, robotic table tennis has become a popular research challenge for perception and robot control.","Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction.","Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras.","We developed a novel calibration approach to calibrate this multimodal perception system.","For table tennis, spin estimation is crucial.","Therefore, we introduced a novel, and more accurate spin estimation approach.","Finally, we show how combining the output of an event-based camera and a Spiking Neural Network (SNN) can be used for accurate ball detection."],"url":"http://arxiv.org/abs/2310.19062v1"}
{"created":"2023-10-29 16:26:28","title":"Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V","abstract":"In this paper, we critically evaluate the capabilities of the state-of-the-art multimodal large language model, i.e., GPT-4 with Vision (GPT-4V), on Visual Question Answering (VQA) task. Our experiments thoroughly assess GPT-4V's proficiency in answering questions paired with images using both pathology and radiology datasets from 11 modalities (e.g. Microscopy, Dermoscopy, X-ray, CT, etc.) and fifteen objects of interests (brain, liver, lung, etc.). Our datasets encompass a comprehensive range of medical inquiries, including sixteen distinct question types. Throughout our evaluations, we devised textual prompts for GPT-4V, directing it to synergize visual and textual information. The experiments with accuracy score conclude that the current version of GPT-4V is not recommended for real-world diagnostics due to its unreliable and suboptimal accuracy in responding to diagnostic medical questions. In addition, we delineate seven unique facets of GPT-4V's behavior in medical VQA, highlighting its constraints within this complex arena. The complete details of our evaluation cases are accessible at https://github.com/ZhilingYan/GPT4V-Medical-Report.","sentences":["In this paper, we critically evaluate the capabilities of the state-of-the-art multimodal large language model, i.e., GPT-4 with Vision (GPT-4V), on Visual Question Answering (VQA) task.","Our experiments thoroughly assess GPT-4V's proficiency in answering questions paired with images using both pathology and radiology datasets from 11 modalities (e.g. Microscopy, Dermoscopy, X-ray, CT, etc.) and fifteen objects of interests (brain, liver, lung, etc.).","Our datasets encompass a comprehensive range of medical inquiries, including sixteen distinct question types.","Throughout our evaluations, we devised textual prompts for GPT-4V, directing it to synergize visual and textual information.","The experiments with accuracy score conclude that the current version of GPT-4V is not recommended for real-world diagnostics due to its unreliable and suboptimal accuracy in responding to diagnostic medical questions.","In addition, we delineate seven unique facets of GPT-4V's behavior in medical VQA, highlighting its constraints within this complex arena.","The complete details of our evaluation cases are accessible at https://github.com/ZhilingYan/GPT4V-Medical-Report."],"url":"http://arxiv.org/abs/2310.19061v1"}
{"created":"2023-10-29 16:25:32","title":"TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding","abstract":"Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks. However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships. To tackle this issue, we propose an efficient method called TEmporal-Spatial Token Aggregation (TESTA). TESTA condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame. TESTA can reduce the number of visual tokens by 75% and thus accelerate video encoding. Building upon TESTA, we introduce a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block. We evaluate our model on five datasets for paragraph-to-video retrieval and long-form VideoQA tasks. Experimental results show that TESTA improves computing efficiency by 1.7 times, and achieves significant performance gains from its scalability in processing longer input frames, e.g., +13.7 R@1 on QuerYD and +6.5 R@1 on Condensed Movie.","sentences":["Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks.","However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos.","These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships.","To tackle this issue, we propose an efficient method called TEmporal-Spatial Token Aggregation (TESTA).","TESTA condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame.","TESTA can reduce the number of visual tokens by 75% and thus accelerate video encoding.","Building upon TESTA, we introduce a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block.","We evaluate our model on five datasets for paragraph-to-video retrieval and long-form VideoQA tasks.","Experimental results show that TESTA improves computing efficiency by 1.7 times, and achieves significant performance gains from its scalability in processing longer input frames, e.g., +13.7 R@1 on QuerYD and +6.5 R@1 on Condensed Movie."],"url":"http://arxiv.org/abs/2310.19060v1"}
{"created":"2023-10-29 16:24:53","title":"Escaping Saddle Points in Heterogeneous Federated Learning via Distributed SGD with Communication Compression","abstract":"We consider the problem of finding second-order stationary points of heterogeneous federated learning (FL). Previous works in FL mostly focus on first-order convergence guarantees, which do not rule out the scenario of unstable saddle points. Meanwhile, it is a key bottleneck of FL to achieve communication efficiency without compensating the learning accuracy, especially when local data are highly heterogeneous across different clients. Given this, we propose a novel algorithm Power-EF that only communicates compressed information via a novel error-feedback scheme. To our knowledge, Power-EF is the first distributed and compressed SGD algorithm that provably escapes saddle points in heterogeneous FL without any data homogeneity assumptions. In particular, Power-EF improves to second-order stationary points after visiting first-order (possibly saddle) points, using additional gradient queries and communication rounds only of almost the same order required by first-order convergence, and the convergence rate exhibits a linear speedup in terms of the number of workers. Our theory improves/recovers previous results, while extending to much more tolerant settings on the local data. Numerical experiments are provided to complement the theory.","sentences":["We consider the problem of finding second-order stationary points of heterogeneous federated learning (FL).","Previous works in FL mostly focus on first-order convergence guarantees, which do not rule out the scenario of unstable saddle points.","Meanwhile, it is a key bottleneck of FL to achieve communication efficiency without compensating the learning accuracy, especially when local data are highly heterogeneous across different clients.","Given this, we propose a novel algorithm Power-EF that only communicates compressed information via a novel error-feedback scheme.","To our knowledge, Power-EF is the first distributed and compressed SGD algorithm that provably escapes saddle points in heterogeneous FL without any data homogeneity assumptions.","In particular, Power-EF improves to second-order stationary points after visiting first-order (possibly saddle) points, using additional gradient queries and communication rounds only of almost the same order required by first-order convergence, and the convergence rate exhibits a linear speedup in terms of the number of workers.","Our theory improves/recovers previous results, while extending to much more tolerant settings on the local data.","Numerical experiments are provided to complement the theory."],"url":"http://arxiv.org/abs/2310.19059v1"}
{"created":"2023-10-29 16:08:33","title":"A Unique Training Strategy to Enhance Language Models Capabilities for Health Mention Detection from Social Media Content","abstract":"An ever-increasing amount of social media content requires advanced AI-based computer programs capable of extracting useful information. Specifically, the extraction of health-related content from social media is useful for the development of diverse types of applications including disease spread, mortality rate prediction, and finding the impact of diverse types of drugs on diverse types of diseases. Language models are competent in extracting the syntactic and semantics of text. However, they face a hard time extracting similar patterns from social media texts. The primary reason for this shortfall lies in the non-standardized writing style commonly employed by social media users. Following the need for an optimal language model competent in extracting useful patterns from social media text, the key goal of this paper is to train language models in such a way that they learn to derive generalized patterns. The key goal is achieved through the incorporation of random weighted perturbation and contrastive learning strategies. On top of a unique training strategy, a meta predictor is proposed that reaps the benefits of 5 different language models for discriminating posts of social media text into non-health and health-related classes. Comprehensive experimentation across 3 public benchmark datasets reveals that the proposed training strategy improves the performance of the language models up to 3.87%, in terms of F1-score, as compared to their performance with traditional training. Furthermore, the proposed meta predictor outperforms existing health mention classification predictors across all 3 benchmark datasets.","sentences":["An ever-increasing amount of social media content requires advanced AI-based computer programs capable of extracting useful information.","Specifically, the extraction of health-related content from social media is useful for the development of diverse types of applications including disease spread, mortality rate prediction, and finding the impact of diverse types of drugs on diverse types of diseases.","Language models are competent in extracting the syntactic and semantics of text.","However, they face a hard time extracting similar patterns from social media texts.","The primary reason for this shortfall lies in the non-standardized writing style commonly employed by social media users.","Following the need for an optimal language model competent in extracting useful patterns from social media text, the key goal of this paper is to train language models in such a way that they learn to derive generalized patterns.","The key goal is achieved through the incorporation of random weighted perturbation and contrastive learning strategies.","On top of a unique training strategy, a meta predictor is proposed that reaps the benefits of 5 different language models for discriminating posts of social media text into non-health and health-related classes.","Comprehensive experimentation across 3 public benchmark datasets reveals that the proposed training strategy improves the performance of the language models up to 3.87%, in terms of F1-score, as compared to their performance with traditional training.","Furthermore, the proposed meta predictor outperforms existing health mention classification predictors across all 3 benchmark datasets."],"url":"http://arxiv.org/abs/2310.19057v1"}
{"created":"2023-10-29 16:04:10","title":"MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion","abstract":"Query expansion is a commonly-used technique in many search systems to better represent users' information needs with additional query terms. Existing studies for this task usually propose to expand a query with retrieved or generated contextual documents. However, both types of methods have clear limitations. For retrieval-based methods, the documents retrieved with the original query might not be accurate enough to reveal the search intent, especially when the query is brief or ambiguous. For generation-based methods, existing models can hardly be trained or aligned on a particular corpus, due to the lack of corpus-specific labeled data. In this paper, we propose a novel Large Language Model (LLM) based mutual verification framework for query expansion, which alleviates the aforementioned limitations. Specifically, we first design a query-query-document generation pipeline, which can effectively leverage the contextual knowledge encoded in LLMs to generate sub-queries and corresponding documents from multiple perspectives. Next, we employ a mutual verification method for both generated and retrieved contextual documents, where 1) retrieved documents are filtered with the external contextual knowledge in generated documents, and 2) generated documents are filtered with the corpus-specific knowledge in retrieved documents. Overall, the proposed method allows retrieved and generated documents to complement each other to finalize a better query expansion. We conduct extensive experiments on three information retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO. The results demonstrate that our method outperforms other baselines significantly.","sentences":["Query expansion is a commonly-used technique in many search systems to better represent users' information needs with additional query terms.","Existing studies for this task usually propose to expand a query with retrieved or generated contextual documents.","However, both types of methods have clear limitations.","For retrieval-based methods, the documents retrieved with the original query might not be accurate enough to reveal the search intent, especially when the query is brief or ambiguous.","For generation-based methods, existing models can hardly be trained or aligned on a particular corpus, due to the lack of corpus-specific labeled data.","In this paper, we propose a novel Large Language Model (LLM) based mutual verification framework for query expansion, which alleviates the aforementioned limitations.","Specifically, we first design a query-query-document generation pipeline, which can effectively leverage the contextual knowledge encoded in LLMs to generate sub-queries and corresponding documents from multiple perspectives.","Next, we employ a mutual verification method for both generated and retrieved contextual documents, where 1) retrieved documents are filtered with the external contextual knowledge in generated documents, and 2) generated documents are filtered with the corpus-specific knowledge in retrieved documents.","Overall, the proposed method allows retrieved and generated documents to complement each other to finalize a better query expansion.","We conduct extensive experiments on three information retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.","The results demonstrate that our method outperforms other baselines significantly."],"url":"http://arxiv.org/abs/2310.19056v1"}
{"created":"2023-10-29 16:02:46","title":"A Survey on Recent Named Entity Recognition and Relation Classification Methods with Focus on Few-Shot Learning Approaches","abstract":"Named entity recognition and relation classification are key stages for extracting information from unstructured text. Several natural language processing applications utilize the two tasks, such as information retrieval, knowledge graph construction and completion, question answering and other domain-specific applications, such as biomedical data mining. We present a survey of recent approaches in the two tasks with focus on few-shot learning approaches. Our work compares the main approaches followed in the two paradigms. Additionally, we report the latest metric scores in the two tasks with a structured analysis that considers the results in the few-shot learning scope.","sentences":["Named entity recognition and relation classification are key stages for extracting information from unstructured text.","Several natural language processing applications utilize the two tasks, such as information retrieval, knowledge graph construction and completion, question answering and other domain-specific applications, such as biomedical data mining.","We present a survey of recent approaches in the two tasks with focus on few-shot learning approaches.","Our work compares the main approaches followed in the two paradigms.","Additionally, we report the latest metric scores in the two tasks with a structured analysis that considers the results in the few-shot learning scope."],"url":"http://arxiv.org/abs/2310.19055v1"}
{"created":"2023-10-29 16:01:03","title":"Object-centric architectures enable efficient causal representation learning","abstract":"Causal representation learning has showed a variety of settings in which we can disentangle latent variables with identifiability guarantees (up to some reasonable equivalence class). Common to all of these approaches is the assumption that (1) the latent variables are represented as $d$-dimensional vectors, and (2) that the observations are the output of some injective generative function of these latent variables. While these assumptions appear benign, we show that when the observations are of multiple objects, the generative function is no longer injective and disentanglement fails in practice. We can address this failure by combining recent developments in object-centric learning and causal representation learning. By modifying the Slot Attention architecture arXiv:2006.15055, we develop an object-centric architecture that leverages weak supervision from sparse perturbations to disentangle each object's properties. This approach is more data-efficient in the sense that it requires significantly fewer perturbations than a comparable approach that encodes to a Euclidean space and we show that this approach successfully disentangles the properties of a set of objects in a series of simple image-based disentanglement experiments.","sentences":["Causal representation learning has showed a variety of settings in which we can disentangle latent variables with identifiability guarantees (up to some reasonable equivalence class).","Common to all of these approaches is the assumption that (1) the latent variables are represented as $d$-dimensional vectors, and (2) that the observations are the output of some injective generative function of these latent variables.","While these assumptions appear benign, we show that when the observations are of multiple objects, the generative function is no longer injective and disentanglement fails in practice.","We can address this failure by combining recent developments in object-centric learning and causal representation learning.","By modifying the Slot Attention architecture arXiv:2006.15055, we develop an object-centric architecture that leverages weak supervision from sparse perturbations to disentangle each object's properties.","This approach is more data-efficient in the sense that it requires significantly fewer perturbations than a comparable approach that encodes to a Euclidean space and we show that this approach successfully disentangles the properties of a set of objects in a series of simple image-based disentanglement experiments."],"url":"http://arxiv.org/abs/2310.19054v1"}
{"created":"2023-10-29 15:57:42","title":"Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations","abstract":"Nanophotonic structures have versatile applications including solar cells, anti-reflective coatings, electromagnetic interference shielding, optical filters, and light emitting diodes. To design and understand these nanophotonic structures, electrodynamic simulations are essential. These simulations enable us to model electromagnetic fields over time and calculate optical properties. In this work, we introduce frameworks and benchmarks to evaluate nanophotonic structures in the context of parametric structure design problems. The benchmarks are instrumental in assessing the performance of optimization algorithms and identifying an optimal structure based on target optical properties. Moreover, we explore the impact of varying grid sizes in electrodynamic simulations, shedding light on how evaluation fidelity can be strategically leveraged in enhancing structure designs.","sentences":["Nanophotonic structures have versatile applications including solar cells, anti-reflective coatings, electromagnetic interference shielding, optical filters, and light emitting diodes.","To design and understand these nanophotonic structures, electrodynamic simulations are essential.","These simulations enable us to model electromagnetic fields over time and calculate optical properties.","In this work, we introduce frameworks and benchmarks to evaluate nanophotonic structures in the context of parametric structure design problems.","The benchmarks are instrumental in assessing the performance of optimization algorithms and identifying an optimal structure based on target optical properties.","Moreover, we explore the impact of varying grid sizes in electrodynamic simulations, shedding light on how evaluation fidelity can be strategically leveraged in enhancing structure designs."],"url":"http://arxiv.org/abs/2310.19053v1"}
{"created":"2023-10-29 15:57:31","title":"Exploring the Emotional Landscape of Music: An Analysis of Valence Trends and Genre Variations in Spotify Music Data","abstract":"This paper conducts an intricate analysis of musical emotions and trends using Spotify music data, encompassing audio features and valence scores extracted through the Spotipi API. Employing regression modeling, temporal analysis, mood transitions, and genre investigation, the study uncovers patterns within music-emotion relationships. Regression models linear, support vector, random forest, and ridge, are employed to predict valence scores. Temporal analysis reveals shifts in valence distribution over time, while mood transition exploration illuminates emotional dynamics within playlists. The research contributes to nuanced insights into music's emotional fabric, enhancing comprehension of the interplay between music and emotions through years.","sentences":["This paper conducts an intricate analysis of musical emotions and trends using Spotify music data, encompassing audio features and valence scores extracted through the Spotipi API.","Employing regression modeling, temporal analysis, mood transitions, and genre investigation, the study uncovers patterns within music-emotion relationships.","Regression models linear, support vector, random forest, and ridge, are employed to predict valence scores.","Temporal analysis reveals shifts in valence distribution over time, while mood transition exploration illuminates emotional dynamics within playlists.","The research contributes to nuanced insights into music's emotional fabric, enhancing comprehension of the interplay between music and emotions through years."],"url":"http://arxiv.org/abs/2310.19052v1"}
{"created":"2023-10-29 15:44:52","title":"Large Language Models as Evolutionary Optimizers","abstract":"Evolutionary algorithms (EAs) have achieved remarkable success in tackling complex combinatorial optimization problems. However, EAs often demand carefully-designed operators with the aid of domain expertise to achieve satisfactory performance. In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers. The main advantage is that it requires minimal domain knowledge and human efforts, as well as no additional training of the model. This approach is referred to as LLM-driven EA (LMEA). Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions. Then, LMEA evaluates these new solutions and include them into the population for the next generation. LMEA is equipped with a self-adaptation mechanism that controls the temperature of the LLM. This enables it to balance between exploration and exploitation and prevents the search from getting stuck in local optima. We investigate the power of LMEA on the classical traveling salesman problems (TSPs) widely used in combinatorial optimization research. Notably, the results show that LMEA performs competitively to traditional heuristics in finding high-quality solutions on TSP instances with up to 20 nodes. Additionally, we also study the effectiveness of LLM-driven crossover/mutation and the self-adaptation mechanism in evolutionary search. In summary, our results reveal the great potentials of LLMs as evolutionary optimizers for solving combinatorial problems. We hope our research shall inspire future explorations on LLM-driven EAs for complex optimization challenges.","sentences":["Evolutionary algorithms (EAs) have achieved remarkable success in tackling complex combinatorial optimization problems.","However, EAs often demand carefully-designed operators with the aid of domain expertise to achieve satisfactory performance.","In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers.","The main advantage is that it requires minimal domain knowledge and human efforts, as well as no additional training of the model.","This approach is referred to as LLM-driven EA (LMEA).","Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions.","Then, LMEA evaluates these new solutions and include them into the population for the next generation.","LMEA is equipped with a self-adaptation mechanism that controls the temperature of the LLM.","This enables it to balance between exploration and exploitation and prevents the search from getting stuck in local optima.","We investigate the power of LMEA on the classical traveling salesman problems (TSPs) widely used in combinatorial optimization research.","Notably, the results show that LMEA performs competitively to traditional heuristics in finding high-quality solutions on TSP instances with up to 20 nodes.","Additionally, we also study the effectiveness of LLM-driven crossover/mutation and the self-adaptation mechanism in evolutionary search.","In summary, our results reveal the great potentials of LLMs as evolutionary optimizers for solving combinatorial problems.","We hope our research shall inspire future explorations on LLM-driven EAs for complex optimization challenges."],"url":"http://arxiv.org/abs/2310.19046v1"}
{"created":"2023-10-29 15:05:39","title":"Boosting Decision-Based Black-Box Adversarial Attack with Gradient Priors","abstract":"Decision-based methods have shown to be effective in black-box adversarial attacks, as they can obtain satisfactory performance and only require to access the final model prediction. Gradient estimation is a critical step in black-box adversarial attacks, as it will directly affect the query efficiency. Recent works have attempted to utilize gradient priors to facilitate score-based methods to obtain better results. However, these gradient priors still suffer from the edge gradient discrepancy issue and the successive iteration gradient direction issue, thus are difficult to simply extend to decision-based methods. In this paper, we propose a novel Decision-based Black-box Attack framework with Gradient Priors (DBA-GP), which seamlessly integrates the data-dependent gradient prior and time-dependent prior into the gradient estimation procedure. First, by leveraging the joint bilateral filter to deal with each random perturbation, DBA-GP can guarantee that the generated perturbations in edge locations are hardly smoothed, i.e., alleviating the edge gradient discrepancy, thus remaining the characteristics of the original image as much as possible. Second, by utilizing a new gradient updating strategy to automatically adjust the successive iteration gradient direction, DBA-GP can accelerate the convergence speed, thus improving the query efficiency. Extensive experiments have demonstrated that the proposed method outperforms other strong baselines significantly.","sentences":["Decision-based methods have shown to be effective in black-box adversarial attacks, as they can obtain satisfactory performance and only require to access the final model prediction.","Gradient estimation is a critical step in black-box adversarial attacks, as it will directly affect the query efficiency.","Recent works have attempted to utilize gradient priors to facilitate score-based methods to obtain better results.","However, these gradient priors still suffer from the edge gradient discrepancy issue and the successive iteration gradient direction issue, thus are difficult to simply extend to decision-based methods.","In this paper, we propose a novel Decision-based Black-box Attack framework with Gradient Priors (DBA-GP), which seamlessly integrates the data-dependent gradient prior and time-dependent prior into the gradient estimation procedure.","First, by leveraging the joint bilateral filter to deal with each random perturbation, DBA-GP can guarantee that the generated perturbations in edge locations are hardly smoothed, i.e., alleviating the edge gradient discrepancy, thus remaining the characteristics of the original image as much as possible.","Second, by utilizing a new gradient updating strategy to automatically adjust the successive iteration gradient direction, DBA-GP can accelerate the convergence speed, thus improving the query efficiency.","Extensive experiments have demonstrated that the proposed method outperforms other strong baselines significantly."],"url":"http://arxiv.org/abs/2310.19038v1"}
{"created":"2023-10-29 14:57:37","title":"Does Invariant Graph Learning via Environment Augmentation Learn Invariance?","abstract":"Invariant graph representation learning aims to learn the invariance among data from different environments for out-of-distribution generalization on graphs. As the graph environment partitions are usually expensive to obtain, augmenting the environment information has become the de facto approach. However, the usefulness of the augmented environment information has never been verified. In this work, we find that it is fundamentally impossible to learn invariant graph representations via environment augmentation without additional assumptions. Therefore, we develop a set of minimal assumptions, including variation sufficiency and variation consistency, for feasible invariant graph learning. We then propose a new framework Graph invAriant Learning Assistant (GALA). GALA incorporates an assistant model that needs to be sensitive to graph environment changes or distribution shifts. The correctness of the proxy predictions by the assistant model hence can differentiate the variations in spurious subgraphs. We show that extracting the maximally invariant subgraph to the proxy predictions provably identifies the underlying invariant subgraph for successful OOD generalization under the established minimal assumptions. Extensive experiments on datasets including DrugOOD with various graph distribution shifts confirm the effectiveness of GALA.","sentences":["Invariant graph representation learning aims to learn the invariance among data from different environments for out-of-distribution generalization on graphs.","As the graph environment partitions are usually expensive to obtain, augmenting the environment information has become the de facto approach.","However, the usefulness of the augmented environment information has never been verified.","In this work, we find that it is fundamentally impossible to learn invariant graph representations via environment augmentation without additional assumptions.","Therefore, we develop a set of minimal assumptions, including variation sufficiency and variation consistency, for feasible invariant graph learning.","We then propose a new framework Graph invAriant Learning Assistant (GALA).","GALA incorporates an assistant model that needs to be sensitive to graph environment changes or distribution shifts.","The correctness of the proxy predictions by the assistant model hence can differentiate the variations in spurious subgraphs.","We show that extracting the maximally invariant subgraph to the proxy predictions provably identifies the underlying invariant subgraph for successful OOD generalization under the established minimal assumptions.","Extensive experiments on datasets including DrugOOD with various graph distribution shifts confirm the effectiveness of GALA."],"url":"http://arxiv.org/abs/2310.19035v1"}
{"created":"2023-10-29 14:46:11","title":"ArBanking77: Intent Detection Neural Model and a New Dataset in Modern and Dialectical Arabic","abstract":"This paper presents the ArBanking77, a large Arabic dataset for intent detection in the banking domain. Our dataset was arabized and localized from the original English Banking77 dataset, which consists of 13,083 queries to ArBanking77 dataset with 31,404 queries in both Modern Standard Arabic (MSA) and Palestinian dialect, with each query classified into one of the 77 classes (intents). Furthermore, we present a neural model, based on AraBERT, fine-tuned on ArBanking77, which achieved an F1-score of 0.9209 and 0.8995 on MSA and Palestinian dialect, respectively. We performed extensive experimentation in which we simulated low-resource settings, where the model is trained on a subset of the data and augmented with noisy queries to simulate colloquial terms, mistakes and misspellings found in real NLP systems, especially live chat queries. The data and the models are publicly available at https://sina.birzeit.edu/arbanking77.","sentences":["This paper presents the ArBanking77, a large Arabic dataset for intent detection in the banking domain.","Our dataset was arabized and localized from the original English Banking77 dataset, which consists of 13,083 queries to ArBanking77 dataset with 31,404 queries in both Modern Standard Arabic (MSA) and Palestinian dialect, with each query classified into one of the 77 classes (intents).","Furthermore, we present a neural model, based on AraBERT, fine-tuned on ArBanking77, which achieved an F1-score of 0.9209 and 0.8995 on MSA and Palestinian dialect, respectively.","We performed extensive experimentation in which we simulated low-resource settings, where the model is trained on a subset of the data and augmented with noisy queries to simulate colloquial terms, mistakes and misspellings found in real NLP systems, especially live chat queries.","The data and the models are publicly available at https://sina.birzeit.edu/arbanking77."],"url":"http://arxiv.org/abs/2310.19034v1"}
