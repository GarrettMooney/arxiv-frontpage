{"created":"2023-11-14 18:59:59","title":"Instant3D: Instant Text-to-3D Generation","abstract":"Text-to-3D generation, which aims to synthesize vivid 3D objects from text prompts, has attracted much attention from the computer vision community. While several existing works have achieved impressive results for this task, they mainly rely on a time-consuming optimization paradigm. Specifically, these methods optimize a neural field from scratch for each text prompt, taking approximately one hour or more to generate one object. This heavy and repetitive training cost impedes their practical deployment. In this paper, we propose a novel framework for fast text-to-3D generation, dubbed Instant3D. Once trained, Instant3D is able to create a 3D object for an unseen text prompt in less than one second with a single run of a feedforward network. We achieve this remarkable speed by devising a new network that directly constructs a 3D triplane from a text prompt. The core innovation of our Instant3D lies in our exploration of strategies to effectively inject text conditions into the network. Furthermore, we propose a simple yet effective activation function, the scaled-sigmoid, to replace the original sigmoid function, which speeds up the training convergence by more than ten times. Finally, to address the Janus (multi-head) problem in 3D generation, we propose an adaptive Perp-Neg algorithm that can dynamically adjust its concept negation scales according to the severity of the Janus problem during training, effectively reducing the multi-head effect. Extensive experiments on a wide variety of benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods both qualitatively and quantitatively, while achieving significantly better efficiency. The project page is at https://ming1993li.github.io/Instant3DProj.","sentences":["Text-to-3D generation, which aims to synthesize vivid 3D objects from text prompts, has attracted much attention from the computer vision community.","While several existing works have achieved impressive results for this task, they mainly rely on a time-consuming optimization paradigm.","Specifically, these methods optimize a neural field from scratch for each text prompt, taking approximately one hour or more to generate one object.","This heavy and repetitive training cost impedes their practical deployment.","In this paper, we propose a novel framework for fast text-to-3D generation, dubbed Instant3D.","Once trained, Instant3D is able to create a 3D object for an unseen text prompt in less than one second with a single run of a feedforward network.","We achieve this remarkable speed by devising a new network that directly constructs a 3D triplane from a text prompt.","The core innovation of our Instant3D lies in our exploration of strategies to effectively inject text conditions into the network.","Furthermore, we propose a simple yet effective activation function, the scaled-sigmoid, to replace the original sigmoid function, which speeds up the training convergence by more than ten times.","Finally, to address the Janus (multi-head) problem in 3D generation, we propose an adaptive Perp-Neg algorithm that can dynamically adjust its concept negation scales according to the severity of the Janus problem during training, effectively reducing the multi-head effect.","Extensive experiments on a wide variety of benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods both qualitatively and quantitatively, while achieving significantly better efficiency.","The project page is at https://ming1993li.github.io/Instant3DProj."],"url":"http://arxiv.org/abs/2311.08403v1"}
{"created":"2023-11-14 18:59:24","title":"Retrieve and Copy: Scaling ASR Personalization to Large Catalogs","abstract":"Personalization of automatic speech recognition (ASR) models is a widely studied topic because of its many practical applications. Most recently, attention-based contextual biasing techniques are used to improve the recognition of rare words and domain specific entities. However, due to performance constraints, the biasing is often limited to a few thousand entities, restricting real-world usability. To address this, we first propose a \"Retrieve and Copy\" mechanism to improve latency while retaining the accuracy even when scaled to a large catalog. We also propose a training strategy to overcome the degradation in recall at such scale due to an increased number of confusing entities. Overall, our approach achieves up to 6% more Word Error Rate reduction (WERR) and 3.6% absolute improvement in F1 when compared to a strong baseline. Our method also allows for large catalog sizes of up to 20K without significantly affecting WER and F1-scores, while achieving at least 20% inference speedup per acoustic frame.","sentences":["Personalization of automatic speech recognition (ASR) models is a widely studied topic because of its many practical applications.","Most recently, attention-based contextual biasing techniques are used to improve the recognition of rare words and domain specific entities.","However, due to performance constraints, the biasing is often limited to a few thousand entities, restricting real-world usability.","To address this, we first propose a \"Retrieve and Copy\" mechanism to improve latency while retaining the accuracy even when scaled to a large catalog.","We also propose a training strategy to overcome the degradation in recall at such scale due to an increased number of confusing entities.","Overall, our approach achieves up to 6% more Word Error Rate reduction (WERR) and 3.6% absolute improvement in F1 when compared to a strong baseline.","Our method also allows for large catalog sizes of up to 20K without significantly affecting WER and F1-scores, while achieving at least 20% inference speedup per acoustic frame."],"url":"http://arxiv.org/abs/2311.08402v1"}
{"created":"2023-11-14 18:59:15","title":"Fine-tuning Language Models for Factuality","abstract":"The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.","sentences":["The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines.","Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.'","These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions.","Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire.","In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work.","We leverage two key recent innovations in NLP to do so.","First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores.","Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses.","We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality.","At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively."],"url":"http://arxiv.org/abs/2311.08401v1"}
{"created":"2023-11-14 18:59:01","title":"Towards Open-Ended Visual Recognition with Large Language Model","abstract":"Localizing and recognizing objects in the open-ended physical world poses a long-standing challenge within the domain of machine perception. Recent methods have endeavored to address the issue by employing a class-agnostic mask (or box) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP) using pre-extracted text embeddings. However, it is worth noting that these open-vocabulary recognition models still exhibit limitations in practical applications. On one hand, they rely on the provision of class names during testing, where the recognition performance heavily depends on this predefined set of semantic classes by users. On the other hand, when training with multiple datasets, human intervention is required to alleviate the label definition conflict between them. In this paper, we introduce the OmniScient Model (OSM), a novel Large Language Model (LLM) based mask classifier, as a straightforward and effective solution to the aforementioned challenges. Specifically, OSM predicts class labels in a generative manner, thus removing the supply of class names during both training and testing. It also enables cross-dataset training without any human interference, exhibiting robust generalization capabilities due to the world knowledge acquired from the LLM. By combining OSM with an off-the-shelf mask proposal model, we present promising results on various benchmarks, and demonstrate its effectiveness in handling novel concepts. Code/model are available at https://github.com/bytedance/OmniScient-Model.","sentences":["Localizing and recognizing objects in the open-ended physical world poses a long-standing challenge within the domain of machine perception.","Recent methods have endeavored to address the issue by employing a class-agnostic mask (or box) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP) using pre-extracted text embeddings.","However, it is worth noting that these open-vocabulary recognition models still exhibit limitations in practical applications.","On one hand, they rely on the provision of class names during testing, where the recognition performance heavily depends on this predefined set of semantic classes by users.","On the other hand, when training with multiple datasets, human intervention is required to alleviate the label definition conflict between them.","In this paper, we introduce the OmniScient Model (OSM), a novel Large Language Model (LLM) based mask classifier, as a straightforward and effective solution to the aforementioned challenges.","Specifically, OSM predicts class labels in a generative manner, thus removing the supply of class names during both training and testing.","It also enables cross-dataset training without any human interference, exhibiting robust generalization capabilities due to the world knowledge acquired from the LLM.","By combining OSM with an off-the-shelf mask proposal model, we present promising results on various benchmarks, and demonstrate its effectiveness in handling novel concepts.","Code/model are available at https://github.com/bytedance/OmniScient-Model."],"url":"http://arxiv.org/abs/2311.08400v1"}
{"created":"2023-11-14 18:57:48","title":"Constant Query Local Decoding Against Deletions Is Impossible","abstract":"Locally decodable codes (LDC's) are error-correcting codes that allow recovery of individual message indices by accessing only a constant number of codeword indices. For substitution errors, it is evident that LDC's exist -- Hadamard codes are examples of $2$-query LDC's. Research on this front has focused on finding the optimal encoding length for LDC's, for which there is a nearly exponential gap between the best lower bounds and constructions.   Ostrovsky and Paskin-Cherniavsky (ICITS 2015) introduced the notion of local decoding to the insertion and deletion setting. In this context, it is not clear whether constant query LDC's exist at all. Indeed, in contrast to the classical setting, Block et al. conjecture that they do not exist. Blocki et al. (FOCS 2021) make progress towards this conjecture, proving that any potential code must have at least exponential encoding length.   Our work definitively resolves the conjecture and shows that constant query LDC's do not exist in the insertion/deletion (or even deletion-only) setting. Using a reduction shown by Blocki et al., this also implies that constant query locally correctable codes do not exist in this setting.","sentences":["Locally decodable codes (LDC's) are error-correcting codes that allow recovery of individual message indices by accessing only a constant number of codeword indices.","For substitution errors, it is evident that LDC's exist -- Hadamard codes are examples of $2$-query LDC's.","Research on this front has focused on finding the optimal encoding length for LDC's, for which there is a nearly exponential gap between the best lower bounds and constructions.   ","Ostrovsky and Paskin-Cherniavsky (ICITS 2015) introduced the notion of local decoding to the insertion and deletion setting.","In this context, it is not clear whether constant query LDC's exist at all.","Indeed, in contrast to the classical setting, Block et al. conjecture that they do not exist.","Blocki et al. (FOCS 2021) make progress towards this conjecture, proving that any potential code must have at least exponential encoding length.   ","Our work definitively resolves the conjecture and shows that constant query LDC's do not exist in the insertion/deletion (or even deletion-only) setting.","Using a reduction shown by Blocki et al., this also implies that constant query locally correctable codes do not exist in this setting."],"url":"http://arxiv.org/abs/2311.08399v1"}
{"created":"2023-11-14 18:57:15","title":"Are Large Language Models Temporally Grounded?","abstract":"Are Large language models (LLMs) temporally grounded? Since LLMs cannot perceive and interact with the environment, it is impossible to answer this question directly. Instead, we provide LLMs with textual narratives and probe them with respect to their common-sense knowledge of the structure and duration of events, their ability to order events along a timeline, and self-consistency within their temporal model (e.g., temporal relations such as after and before are mutually exclusive for any pair of events). We evaluate state-of-the-art LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities. Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions. Contrary to expectations, we also find that scaling the model size does not guarantee positive gains in performance. To explain these results, we study the sources from which LLMs may gather temporal information: we find that sentence ordering in unlabelled texts, available during pre-training, is only weakly correlated with event ordering. Moreover, public instruction tuning mixtures contain few temporal tasks. Hence, we conclude that current LLMs lack a consistent temporal model of textual narratives. Code, datasets, and LLM outputs are available at https://github.com/yfqiu-nlp/temporal-llms.","sentences":["Are Large language models (LLMs) temporally grounded?","Since LLMs cannot perceive and interact with the environment, it is impossible to answer this question directly.","Instead, we provide LLMs with textual narratives and probe them with respect to their common-sense knowledge of the structure and duration of events, their ability to order events along a timeline, and self-consistency within their temporal model (e.g., temporal relations such as after and before are mutually exclusive for any pair of events).","We evaluate state-of-the-art LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.","Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs.","In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree.","Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions.","Contrary to expectations, we also find that scaling the model size does not guarantee positive gains in performance.","To explain these results, we study the sources from which LLMs may gather temporal information: we find that sentence ordering in unlabelled texts, available during pre-training, is only weakly correlated with event ordering.","Moreover, public instruction tuning mixtures contain few temporal tasks.","Hence, we conclude that current LLMs lack a consistent temporal model of textual narratives.","Code, datasets, and LLM outputs are available at https://github.com/yfqiu-nlp/temporal-llms."],"url":"http://arxiv.org/abs/2311.08398v1"}
{"created":"2023-11-14 18:53:28","title":"MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable Trajectory Generation","abstract":"The learn-from-observation (LfO) paradigm is a human-inspired mode for a robot to learn to perform a task simply by watching it being performed. LfO can facilitate robot integration on factory floors by minimizing disruption and reducing tedious programming. A key component of the LfO pipeline is a transformation of the depth camera frames to the corresponding task state and action pairs, which are then relayed to learning techniques such as imitation or inverse reinforcement learning for understanding the task parameters. While several existing computer vision models analyze videos for activity recognition, SA-Net specifically targets robotic LfO from RGB-D data. However, SA-Net and many other models analyze frame data captured from a single viewpoint. Their analysis is therefore highly sensitive to occlusions of the observed task, which are frequent in deployments. An obvious way of reducing occlusions is to simultaneously observe the task from multiple viewpoints and synchronously fuse the multiple streams in the model. Toward this, we present multi-view SA-Net, which generalizes the SA-Net model to allow the perception of multiple viewpoints of the task activity, integrate them, and better recognize the state and action in each frame. Performance evaluations on two distinct domains establish that MVSA-Net recognizes the state-action pairs under occlusion more accurately compared to single-view MVSA-Net and other baselines. Our ablation studies further evaluate its performance under different ambient conditions and establish the contribution of the architecture components. As such, MVSA-Net offers a significantly more robust and deployable state-action trajectory generation compared to previous methods.","sentences":["The learn-from-observation (LfO) paradigm is a human-inspired mode for a robot to learn to perform a task simply by watching it being performed.","LfO can facilitate robot integration on factory floors by minimizing disruption and reducing tedious programming.","A key component of the LfO pipeline is a transformation of the depth camera frames to the corresponding task state and action pairs, which are then relayed to learning techniques such as imitation or inverse reinforcement learning for understanding the task parameters.","While several existing computer vision models analyze videos for activity recognition, SA-Net specifically targets robotic LfO from RGB-D data.","However, SA-Net and many other models analyze frame data captured from a single viewpoint.","Their analysis is therefore highly sensitive to occlusions of the observed task, which are frequent in deployments.","An obvious way of reducing occlusions is to simultaneously observe the task from multiple viewpoints and synchronously fuse the multiple streams in the model.","Toward this, we present multi-view SA-Net, which generalizes the SA-Net model to allow the perception of multiple viewpoints of the task activity, integrate them, and better recognize the state and action in each frame.","Performance evaluations on two distinct domains establish that MVSA-Net recognizes the state-action pairs under occlusion more accurately compared to single-view MVSA-Net and other baselines.","Our ablation studies further evaluate its performance under different ambient conditions and establish the contribution of the architecture components.","As such, MVSA-Net offers a significantly more robust and deployable state-action trajectory generation compared to previous methods."],"url":"http://arxiv.org/abs/2311.08393v1"}
{"created":"2023-11-14 18:52:50","title":"Iterative Network Pricing for Ridesharing Platforms","abstract":"Ridesharing platforms match riders and drivers, using dynamic pricing to balance supply and demand. The origin-based \"surge pricing\", however, does not take into consideration market conditions at trip destinations, leading to inefficient driver flows in space and incentivizes drivers to strategize. In this work, we introduce the Iterative Network Pricing mechanism, addressing a main challenge in the practical implementation of optimal origin-destination (OD) based prices, that the model for rider demand is hard to estimate. Assuming that the platform's surge algorithm clears the market for each origin in real-time, our mechanism updates the OD-based price adjustments week-over-week, using only information immediately observable during the same time window in the prior weeks. For stationary market conditions, we prove that our mechanism converges to an outcome that is approximately welfare-optimal. Using data from the City of Chicago, we illustrate (via simulation) the iterative updates under our mechanism for morning rush hours, demonstrating substantial welfare improvements despite significant fluctuations of market conditions from early 2019 through the end of 2020.","sentences":["Ridesharing platforms match riders and drivers, using dynamic pricing to balance supply and demand.","The origin-based \"surge pricing\", however, does not take into consideration market conditions at trip destinations, leading to inefficient driver flows in space and incentivizes drivers to strategize.","In this work, we introduce the Iterative Network Pricing mechanism, addressing a main challenge in the practical implementation of optimal origin-destination (OD) based prices, that the model for rider demand is hard to estimate.","Assuming that the platform's surge algorithm clears the market for each origin in real-time, our mechanism updates the OD-based price adjustments week-over-week, using only information immediately observable during the same time window in the prior weeks.","For stationary market conditions, we prove that our mechanism converges to an outcome that is approximately welfare-optimal.","Using data from the City of Chicago, we illustrate (via simulation)","the iterative updates under our mechanism for morning rush hours, demonstrating substantial welfare improvements despite significant fluctuations of market conditions from early 2019 through the end of 2020."],"url":"http://arxiv.org/abs/2311.08392v1"}
{"created":"2023-11-14 18:52:09","title":"A Material Lens on Coloniality in NLP","abstract":"Coloniality, the continuation of colonial harms beyond \"official\" colonization, has pervasive effects across society and scientific fields. Natural Language Processing (NLP) is no exception to this broad phenomenon. In this work, we argue that coloniality is implicitly embedded in and amplified by NLP data, algorithms, and software. We formalize this analysis using Actor-Network Theory (ANT): an approach to understanding social phenomena through the network of relationships between human stakeholders and technology. We use our Actor-Network to guide a quantitative survey of the geography of different phases of NLP research, providing evidence that inequality along colonial boundaries increases as NLP builds on itself. Based on this, we argue that combating coloniality in NLP requires not only changing current values but also active work to remove the accumulation of colonial ideals in our foundational data and algorithms.","sentences":["Coloniality, the continuation of colonial harms beyond \"official\" colonization, has pervasive effects across society and scientific fields.","Natural Language Processing (NLP) is no exception to this broad phenomenon.","In this work, we argue that coloniality is implicitly embedded in and amplified by NLP data, algorithms, and software.","We formalize this analysis using Actor-Network Theory (ANT): an approach to understanding social phenomena through the network of relationships between human stakeholders and technology.","We use our Actor-Network to guide a quantitative survey of the geography of different phases of NLP research, providing evidence that inequality along colonial boundaries increases as NLP builds on itself.","Based on this, we argue that combating coloniality in NLP requires not only changing current values but also active work to remove the accumulation of colonial ideals in our foundational data and algorithms."],"url":"http://arxiv.org/abs/2311.08391v1"}
{"created":"2023-11-14 18:51:38","title":"On What Basis? Predicting Text Preference Via Structured Comparative Reasoning","abstract":"Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning. While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons. SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency. Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction.","sentences":["Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning.","While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts.","We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons.","SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect.","We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency.","Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction."],"url":"http://arxiv.org/abs/2311.08390v1"}
{"created":"2023-11-14 18:50:51","title":"TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer","abstract":"Text style is highly abstract, as it encompasses various aspects of a speaker's characteristics, habits, logical thinking, and the content they express. However, previous text-style transfer tasks have primarily focused on data-driven approaches, lacking in-depth analysis and research from the perspectives of linguistics and cognitive science. In this paper, we introduce a novel task called Text Speech-Style Transfer (TSST). The main objective is to further explore topics related to human cognition, such as personality and emotion, based on the capabilities of existing LLMs. Considering the objective of our task and the distinctive characteristics of oral speech in real-life scenarios, we trained multi-dimension (i.e. filler words, vividness, interactivity, emotionality) evaluation models for the TSST and validated their correlation with human assessments. We thoroughly analyze the performance of several large language models (LLMs) and identify areas where further improvement is needed. Moreover, driven by our evaluation models, we have released a new corpus that improves the capabilities of LLMs in generating text with speech-style characteristics. In summary, we present the TSST task, a new benchmark for style transfer and emphasizing human-oriented evaluation, exploring and advancing the performance of current LLMs.","sentences":["Text style is highly abstract, as it encompasses various aspects of a speaker's characteristics, habits, logical thinking, and the content they express.","However, previous text-style transfer tasks have primarily focused on data-driven approaches, lacking in-depth analysis and research from the perspectives of linguistics and cognitive science.","In this paper, we introduce a novel task called Text Speech-Style Transfer (TSST).","The main objective is to further explore topics related to human cognition, such as personality and emotion, based on the capabilities of existing LLMs.","Considering the objective of our task and the distinctive characteristics of oral speech in real-life scenarios, we trained multi-dimension (i.e. filler words, vividness, interactivity, emotionality) evaluation models for the TSST and validated their correlation with human assessments.","We thoroughly analyze the performance of several large language models (LLMs) and identify areas where further improvement is needed.","Moreover, driven by our evaluation models, we have released a new corpus that improves the capabilities of LLMs in generating text with speech-style characteristics.","In summary, we present the TSST task, a new benchmark for style transfer and emphasizing human-oriented evaluation, exploring and advancing the performance of current LLMs."],"url":"http://arxiv.org/abs/2311.08389v1"}
{"created":"2023-11-14 18:49:00","title":"Communication Efficiency of Summation over a Quantum Erasure MAC with Replicated Inputs","abstract":"The quantum communication cost of computing a classical sum of distributed sources is studied over a quantum erasure multiple access channel (QEMAC). $K$ messages are distributed across $S$ servers so that each server knows a subset of the messages. Each server $s\\in[S]$ sends a quantum subsystem $\\mathcal{Q}_s$ to the receiver who computes the sum of the messages. The download cost from Server $s\\in [S]$ is the logarithm of the dimension of $\\mathcal{Q}_s$. The rate $R$ is defined as the number of instances of the sum computed at the receiver, divided by the total download cost from all the servers. In the symmetric setting with $K= {S \\choose \\alpha} $ messages where each message is replicated among a unique subset of $\\alpha$ servers, and the answers from any $\\beta$ servers may be erased, the rate achieved is $R= \\max\\left\\{ \\min \\left\\{ \\frac{2(\\alpha-\\beta)}{S}, 1-\\frac{2\\beta}{S} \\right\\}, \\frac{\\alpha-\\beta}{S} \\right\\}$, which is shown to be optimal when $S\\geq 2\\alpha$.","sentences":["The quantum communication cost of computing a classical sum of distributed sources is studied over a quantum erasure multiple access channel (QEMAC).","$K$ messages are distributed across $S$ servers so that each server knows a subset of the messages.","Each server $s\\in[S]$ sends a quantum subsystem $\\mathcal{Q}_s$ to the receiver who computes the sum of the messages.","The download cost from Server $s\\in [S]$ is the logarithm of the dimension of $\\mathcal{Q}_s$. The rate $R$ is defined as the number of instances of the sum computed at the receiver, divided by the total download cost from all the servers.","In the symmetric setting with $K= {S \\choose \\alpha} $ messages where each message is replicated among a unique subset of $\\alpha$ servers, and the answers from any $\\beta$ servers may be erased, the rate achieved is $R= \\max\\left\\{ \\min \\left\\{ \\frac{2(\\alpha-\\beta)}{S}, 1-\\frac{2\\beta}{S} \\right\\}, \\frac{\\alpha-\\beta}{S} \\right\\}$, which is shown to be optimal when $S\\geq 2\\alpha$."],"url":"http://arxiv.org/abs/2311.08386v1"}
{"created":"2023-11-14 18:48:27","title":"ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning","abstract":"Aligning language models (LMs) with human opinion is challenging yet vital to enhance their grasp of human values, preferences, and beliefs. We present ChOiRe, a four-step solution framework to predict human opinion that differentiates between the user explicit personae (i.e. demographic or ideological attributes) that are manually declared and implicit personae inferred from user historical opinions. Specifically, it consists of (i) an LM analyzing the user explicit personae to filter out irrelevant attributes; (ii) the LM ranking the implicit persona opinions into a preferential list; (iii) Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the explicit personae and the most relevant implicit personae to perform opinion prediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with increasingly larger lists of implicit personae to overcome insufficient personae information to infer a final result. ChOiRe achieves new state-of-the-art effectiveness with limited inference calls, improving previous LLM-based techniques significantly by 3.22%.","sentences":["Aligning language models (LMs) with human opinion is challenging yet vital to enhance their grasp of human values, preferences, and beliefs.","We present ChOiRe, a four-step solution framework to predict human opinion that differentiates between the user explicit personae (i.e. demographic or ideological attributes) that are manually declared and implicit personae inferred from user historical opinions.","Specifically, it consists of (i) an LM analyzing the user explicit personae to filter out irrelevant attributes; (ii) the LM ranking the implicit persona opinions into a preferential list; (iii) Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the explicit personae and the most relevant implicit personae to perform opinion prediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with increasingly larger lists of implicit personae to overcome insufficient personae information to infer a final result.","ChOiRe achieves new state-of-the-art effectiveness with limited inference calls, improving previous LLM-based techniques significantly by 3.22%."],"url":"http://arxiv.org/abs/2311.08385v1"}
{"created":"2023-11-14 18:45:56","title":"Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees","abstract":"Hybrid RL is the setting where an RL agent has access to both offline data and online data by interacting with the real-world environment. In this work, we propose a new hybrid RL algorithm that combines an on-policy actor-critic method with offline data. On-policy methods such as policy gradient and natural policy gradient (NPG) have shown to be more robust to model misspecification, though sometimes it may not be as sample efficient as methods that rely on off-policy learning. On the other hand, offline methods that depend on off-policy training often require strong assumptions in theory and are less stable to train in practice. Our new approach integrates a procedure of off-policy training on the offline data into an on-policy NPG framework. We show that our approach, in theory, can obtain a best-of-both-worlds type of result -- it achieves the state-of-art theoretical guarantees of offline RL when offline RL-specific assumptions hold, while at the same time maintaining the theoretical guarantees of on-policy NPG regardless of the offline RL assumptions' validity. Experimentally, in challenging rich-observation environments, we show that our approach outperforms a state-of-the-art hybrid RL baseline which only relies on off-policy policy optimization, demonstrating the empirical benefit of combining on-policy and off-policy learning. Our code is publicly available at https://github.com/YifeiZhou02/HNPG.","sentences":["Hybrid RL is the setting where an RL agent has access to both offline data and online data by interacting with the real-world environment.","In this work, we propose a new hybrid RL algorithm that combines an on-policy actor-critic method with offline data.","On-policy methods such as policy gradient and natural policy gradient (NPG) have shown to be more robust to model misspecification, though sometimes it may not be as sample efficient as methods that rely on off-policy learning.","On the other hand, offline methods that depend on off-policy training often require strong assumptions in theory and are less stable to train in practice.","Our new approach integrates a procedure of off-policy training on the offline data into an on-policy NPG framework.","We show that our approach, in theory, can obtain a best-of-both-worlds type of result -- it achieves the state-of-art theoretical guarantees of offline RL when offline RL-specific assumptions hold, while at the same time maintaining the theoretical guarantees of on-policy NPG regardless of the offline RL assumptions' validity.","Experimentally, in challenging rich-observation environments, we show that our approach outperforms a state-of-the-art hybrid RL baseline which only relies on off-policy policy optimization, demonstrating the empirical benefit of combining on-policy and off-policy learning.","Our code is publicly available at https://github.com/YifeiZhou02/HNPG."],"url":"http://arxiv.org/abs/2311.08384v1"}
{"created":"2023-11-14 18:45:29","title":"Choosing Outdated Information to Achieve Reliability in Age-Based Gossiping","abstract":"We consider a system model with two sources, a reliable source and an unreliable source, who are responsible for disseminating updates regarding a process to an age-based gossip network of $n$ nodes. Nodes wish to have fresh information, however, they have preference for packets that originated at the reliable source and are willing to sacrifice their version age of information by up to $G$ versions to switch from an unreliable packet to a reliable packet. We study how this protocol impacts the prevalence of unreliable packets at nodes in the network and their version age. Using a stochastic hybrid system (SHS) framework, we formulate analytical equations to characterize two quantities: expected fraction of nodes with unreliable packets and expected version age of information at network nodes. We show that as $G$ increases, fewer nodes have unreliable packet, however, their version age increases as well, thereby inducing a freshness-reliability trade-off in the network. We present numerical results to support our findings.","sentences":["We consider a system model with two sources, a reliable source and an unreliable source, who are responsible for disseminating updates regarding a process to an age-based gossip network of $n$ nodes.","Nodes wish to have fresh information, however, they have preference for packets that originated at the reliable source and are willing to sacrifice their version age of information by up to $G$ versions to switch from an unreliable packet to a reliable packet.","We study how this protocol impacts the prevalence of unreliable packets at nodes in the network and their version age.","Using a stochastic hybrid system (SHS) framework, we formulate analytical equations to characterize two quantities: expected fraction of nodes with unreliable packets and expected version age of information at network nodes.","We show that as $G$ increases, fewer nodes have unreliable packet, however, their version age increases as well, thereby inducing a freshness-reliability trade-off in the network.","We present numerical results to support our findings."],"url":"http://arxiv.org/abs/2311.08383v1"}
{"created":"2023-11-14 18:43:51","title":"Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding","abstract":"Minimum Bayes Risk (MBR) decoding can significantly improve translation performance of Multilingual Large Language Models (MLLMs). However, MBR decoding is computationally expensive and in this paper, we show how recently developed Reinforcement Learning (RL) technique, Direct Preference Optimization (DPO) can be used to fine-tune MLLMs so that we get the gains from MBR without the additional computation in inference. Our fine-tuned models have significantly improved performance on multiple NMT test sets compared to base MLLMs without preference optimization. Our method boosts the translation performance of MLLMs using relatively small monolingual fine-tuning sets.","sentences":["Minimum Bayes Risk (MBR) decoding can significantly improve translation performance of Multilingual Large Language Models (MLLMs).","However, MBR decoding is computationally expensive and in this paper, we show how recently developed Reinforcement Learning (RL) technique, Direct Preference Optimization (DPO) can be used to fine-tune MLLMs so that we get the gains from MBR without the additional computation in inference.","Our fine-tuned models have significantly improved performance on multiple NMT test sets compared to base MLLMs without preference optimization.","Our method boosts the translation performance of MLLMs using relatively small monolingual fine-tuning sets."],"url":"http://arxiv.org/abs/2311.08380v1"}
{"created":"2023-11-14 18:42:40","title":"Scheming AIs: Will AIs fake alignment during training in order to get power?","abstract":"This report examines whether advanced AIs that perform well in training will be doing so in order to gain power later -- a behavior I call \"scheming\" (also sometimes called \"deceptive alignment\"). I conclude that scheming is a disturbingly plausible outcome of using baseline machine learning methods to train goal-directed AIs sophisticated enough to scheme (my subjective probability on such an outcome, given these conditions, is roughly 25%). In particular: if performing well in training is a good strategy for gaining power (as I think it might well be), then a very wide variety of goals would motivate scheming -- and hence, good training performance. This makes it plausible that training might either land on such a goal naturally and then reinforce it, or actively push a model's motivations towards such a goal as an easy way of improving performance. What's more, because schemers pretend to be aligned on tests designed to reveal their motivations, it may be quite difficult to tell whether this has occurred. However, I also think there are reasons for comfort. In particular: scheming may not actually be such a good strategy for gaining power; various selection pressures in training might work against schemer-like goals (for example, relative to non-schemers, schemers need to engage in extra instrumental reasoning, which might harm their training performance); and we may be able to increase such pressures intentionally. The report discusses these and a wide variety of other considerations in detail, and it suggests an array of empirical research directions for probing the topic further.","sentences":["This report examines whether advanced AIs that perform well in training will be doing so in order to gain power later -- a behavior I call \"scheming\" (also sometimes called \"deceptive alignment\").","I conclude that scheming is a disturbingly plausible outcome of using baseline machine learning methods to train goal-directed AIs sophisticated enough to scheme (my subjective probability on such an outcome, given these conditions, is roughly 25%).","In particular: if performing well in training is a good strategy for gaining power (as I think it might well be), then a very wide variety of goals would motivate scheming -- and hence, good training performance.","This makes it plausible that training might either land on such a goal naturally and then reinforce it, or actively push a model's motivations towards such a goal as an easy way of improving performance.","What's more, because schemers pretend to be aligned on tests designed to reveal their motivations, it may be quite difficult to tell whether this has occurred.","However, I also think there are reasons for comfort.","In particular: scheming may not actually be such a good strategy for gaining power; various selection pressures in training might work against schemer-like goals (for example, relative to non-schemers, schemers need to engage in extra instrumental reasoning, which might harm their training performance); and we may be able to increase such pressures intentionally.","The report discusses these and a wide variety of other considerations in detail, and it suggests an array of empirical research directions for probing the topic further."],"url":"http://arxiv.org/abs/2311.08379v1"}
{"created":"2023-11-14 18:41:54","title":"Learning to Filter Context for Retrieval-Augmented Generation","abstract":"On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.","sentences":["On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification.","However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages.","This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations.","To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time.","We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks.","FILCO effectively improves the quality of context, whether or not it supports the canonical output."],"url":"http://arxiv.org/abs/2311.08377v1"}
{"created":"2023-11-14 18:41:23","title":"Proceedings 19th International Conference on Quantum Physics and Logic","abstract":"This volume contains the proceedings of the 19th International Conference on Quantum Physics and Logic (QPL 2022), which was held June 27-July 1, 2022 at Wolfson College, University of Oxford, UK. QPL is an annual conference that brings together academic and industry researchers working on mathematical foundations of quantum computation, quantum physics, and related areas. The main focus is on the use of algebraic and categorical structures, formal languages, semantic methods, as well as other mathematical and computer scientific techniques applicable to the study of physical systems, physical processes, and their composition.","sentences":["This volume contains the proceedings of the 19th International Conference on Quantum Physics and Logic (QPL 2022), which was held June 27-July 1, 2022 at Wolfson College, University of Oxford, UK.","QPL is an annual conference that brings together academic and industry researchers working on mathematical foundations of quantum computation, quantum physics, and related areas.","The main focus is on the use of algebraic and categorical structures, formal languages, semantic methods, as well as other mathematical and computer scientific techniques applicable to the study of physical systems, physical processes, and their composition."],"url":"http://arxiv.org/abs/2311.08375v1"}
{"created":"2023-11-14 18:40:42","title":"A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts","abstract":"In the realm of text manipulation and linguistic transformation, the question of authorship has always been a subject of fascination and philosophical inquiry. Much like the \\textbf{Ship of Theseus paradox}, which ponders whether a ship remains the same when each of its original planks is replaced, our research delves into an intriguing question: \\textit{Does a text retain its original authorship when it undergoes numerous paraphrasing iterations?} Specifically, since Large Language Models (LLMs) have demonstrated remarkable proficiency in the generation of both original content and the modification of human-authored texts, a pivotal question emerges concerning the determination of authorship in instances where LLMs or similar paraphrasing tools are employed to rephrase the text. This inquiry revolves around \\textit{whether authorship should be attributed to the original human author or the AI-powered tool, given the tool's independent capacity to produce text that closely resembles human-generated content.} Therefore, we embark on a philosophical voyage through the seas of language and authorship to unravel this intricate puzzle.","sentences":["In the realm of text manipulation and linguistic transformation, the question of authorship has always been a subject of fascination and philosophical inquiry.","Much like the \\textbf{Ship of Theseus paradox}, which ponders whether a ship remains the same when each of its original planks is replaced, our research delves into an intriguing question: \\textit{Does a text retain its original authorship when it undergoes numerous paraphrasing iterations?} Specifically, since Large Language Models (LLMs) have demonstrated remarkable proficiency in the generation of both original content and the modification of human-authored texts, a pivotal question emerges concerning the determination of authorship in instances where LLMs or similar paraphrasing tools are employed to rephrase the text.","This inquiry revolves around \\textit{whether authorship should be attributed to the original human author or the AI-powered tool, given the tool's independent capacity to produce text that closely resembles human-generated content.}","Therefore, we embark on a philosophical voyage through the seas of language and authorship to unravel this intricate puzzle."],"url":"http://arxiv.org/abs/2311.08374v1"}
{"created":"2023-11-14 18:38:12","title":"Proceedings of the 18th International Workshop on the ACL2 Theorem Prover and Its Applications","abstract":"This volume contains the proceedings of the Eighteenth International Workshop on the ACL2 Theorem Prover and Its Applications (ACL2-2023), a two-day workshop held at the University of Texas at Austin and online, on November 13-14. These workshops provide a major technical forum for users of the ACL2 theorem prover to present research related to ACL2 and its applications.","sentences":["This volume contains the proceedings of the Eighteenth International Workshop on the ACL2 Theorem Prover and Its Applications (ACL2-2023), a two-day workshop held at the University of Texas at Austin and online, on November 13-14.","These workshops provide a major technical forum for users of the ACL2 theorem prover to present research related to ACL2 and its applications."],"url":"http://arxiv.org/abs/2311.08373v1"}
{"created":"2023-11-14 18:35:02","title":"Aid Nexus : A Blockchain Based Financial Distribution System","abstract":"Blockchain technology has emerged as a disruptive force with transformative potential across numerous industries, promising efficient and automated solutions that can revolutionize traditional systems. By leveraging decentralized ledger systems, blockchain offers enhanced security, transparency, and transaction verification without the need for intermediaries. The finance sector is exploring blockchain-based solutions for payments, remittances, lending, and investments, while healthcare adopts the technology for medical record keeping, supply chain tracking, and data management. Similarly, supply chain management benefits from blockchain's ability to enhance transparency, traceability, and accountability from raw materials to finished products. Other sectors, including real estate, energy, and government, are also investigating blockchain-based solutions to improve efficiency, security, and transparency. Furthermore, smart contracts within the blockchain enable process automation, reducing manual intervention in distribution workflows. AidNeux, a consortium-based blockchain DApp, reimagines the distribution of financial assistance by addressing inefficiencies and opaqueness. Using smart contracts ensures the security and directness of money transfers. Its robust digital identity verification and real-time auditability reduce fraud risks and strengthen accountability, thereby presenting a scalable, transparent solution to problems inherent to conventional financial aid systems.","sentences":["Blockchain technology has emerged as a disruptive force with transformative potential across numerous industries, promising efficient and automated solutions that can revolutionize traditional systems.","By leveraging decentralized ledger systems, blockchain offers enhanced security, transparency, and transaction verification without the need for intermediaries.","The finance sector is exploring blockchain-based solutions for payments, remittances, lending, and investments, while healthcare adopts the technology for medical record keeping, supply chain tracking, and data management.","Similarly, supply chain management benefits from blockchain's ability to enhance transparency, traceability, and accountability from raw materials to finished products.","Other sectors, including real estate, energy, and government, are also investigating blockchain-based solutions to improve efficiency, security, and transparency.","Furthermore, smart contracts within the blockchain enable process automation, reducing manual intervention in distribution workflows.","AidNeux, a consortium-based blockchain DApp, reimagines the distribution of financial assistance by addressing inefficiencies and opaqueness.","Using smart contracts ensures the security and directness of money transfers.","Its robust digital identity verification and real-time auditability reduce fraud risks and strengthen accountability, thereby presenting a scalable, transparent solution to problems inherent to conventional financial aid systems."],"url":"http://arxiv.org/abs/2311.08372v1"}
{"created":"2023-11-14 18:33:43","title":"SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models","abstract":"The past year has seen rapid acceleration in the development of large language models (LLMs). For many tasks, there is now a wide range of open-source and open-access LLMs that are viable alternatives to proprietary models like ChatGPT. Without proper steering and safeguards, however, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content. This is a critical safety risk for businesses and developers. We introduce SimpleSafetyTests as a new test suite for rapidly and systematically identifying such critical safety risks. The test suite comprises 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with. We test 11 popular open LLMs and find critical safety weaknesses in several of them. While some LLMs do not give a single unsafe response, most models we test respond unsafely on more than 20% of cases, with over 50% unsafe responses in the extreme. Prepending a safety-emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happening. We recommend that developers use such system prompts as a first line of defence against critical safety risks.","sentences":["The past year has seen rapid acceleration in the development of large language models (LLMs).","For many tasks, there is now a wide range of open-source and open-access LLMs that are viable alternatives to proprietary models like ChatGPT.","Without proper steering and safeguards, however, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content.","This is a critical safety risk for businesses and developers.","We introduce SimpleSafetyTests as a new test suite for rapidly and systematically identifying such critical safety risks.","The test suite comprises 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with.","We test 11 popular open LLMs and find critical safety weaknesses in several of them.","While some LLMs do not give a single unsafe response, most models we test respond unsafely on more than 20% of cases, with over 50% unsafe responses in the extreme.","Prepending a safety-emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happening.","We recommend that developers use such system prompts as a first line of defence against critical safety risks."],"url":"http://arxiv.org/abs/2311.08370v1"}
{"created":"2023-11-14 18:32:52","title":"How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection","abstract":"Against the misuse (e.g., plagiarism or spreading misinformation) of Large Language Models (LLMs), many recent works have presented LLM-generated-text detectors with promising detection performance. Spotlighting a situation where users instruct LLMs to generate texts (e.g., essay writing), there are various ways to write the instruction (e.g., what task-oriented constraint to include). In this paper, we discover that even a task-oriented constraint in instruction can cause the inconsistent performance of current detectors to the generated texts. Specifically, we focus on student essay writing as a realistic domain and manually create the task-oriented constraint for each factor on essay quality by Ke and Ng (2019). Our experiment shows that the detection performance variance of the current detector on texts generated by instruction with each task-oriented constraint is up to 20 times larger than the variance caused by generating texts multiple times and paraphrasing the instruction. Our finding calls for further research on developing robust detectors that can detect such distributional shifts caused by a task-oriented constraint in the instruction.","sentences":["Against the misuse (e.g., plagiarism or spreading misinformation) of Large Language Models (LLMs), many recent works have presented LLM-generated-text detectors with promising detection performance.","Spotlighting a situation where users instruct LLMs to generate texts (e.g., essay writing), there are various ways to write the instruction (e.g., what task-oriented constraint to include).","In this paper, we discover that even a task-oriented constraint in instruction can cause the inconsistent performance of current detectors to the generated texts.","Specifically, we focus on student essay writing as a realistic domain and manually create the task-oriented constraint for each factor on essay quality by Ke and Ng (2019).","Our experiment shows that the detection performance variance of the current detector on texts generated by instruction with each task-oriented constraint is up to 20 times larger than the variance caused by generating texts multiple times and paraphrasing the instruction.","Our finding calls for further research on developing robust detectors that can detect such distributional shifts caused by a task-oriented constraint in the instruction."],"url":"http://arxiv.org/abs/2311.08369v1"}
{"created":"2023-11-14 18:26:50","title":"Arboricity-Dependent Algorithms for Edge Coloring","abstract":"The problem of edge coloring has been extensively studied over the years. The main conceptual contribution of this work is in identifying a surprisingly simple connection between the problem of $(\\Delta +O(\\alpha))$-edge coloring and a certain canonical graph decomposition in graphs of arboricity $\\alpha$, for which efficient algorithms are known across various computational models.   We first leverage such graph decompositions to provide fast $(\\Delta +O(\\alpha))$-edge coloring algorithms in the standard {\\em static} (sequential and distributed) settings. Further, as our main technical contribution, we show how to efficiently maintain a $(\\Delta +O(\\alpha))$-edge coloring in the standard {\\em dynamic} model. Consequently, we improve over the state-of-the-art edge coloring algorithms in these models for graphs of sufficiently small arboricity.","sentences":["The problem of edge coloring has been extensively studied over the years.","The main conceptual contribution of this work is in identifying a surprisingly simple connection between the problem of $(\\Delta +O(\\alpha))$-edge coloring and a certain canonical graph decomposition in graphs of arboricity $\\alpha$, for which efficient algorithms are known across various computational models.   ","We first leverage such graph decompositions to provide fast $(\\Delta +O(\\alpha))$-edge coloring algorithms in the standard {\\em static} (sequential and distributed) settings.","Further, as our main technical contribution, we show how to efficiently maintain a $(\\Delta +O(\\alpha))$-edge coloring in the standard {\\em dynamic} model.","Consequently, we improve over the state-of-the-art edge coloring algorithms in these models for graphs of sufficiently small arboricity."],"url":"http://arxiv.org/abs/2311.08367v1"}
{"created":"2023-11-14 18:14:56","title":"Plum: Prompt Learning using Metaheuristic","abstract":"Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly \"general\", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in black-box prompt learning and Chain-of-Thought prompt tuning. Furthermore, we show that these methods can be used to discover more human-understandable prompts that were previously unknown, opening the door to a cornucopia of possibilities in prompt optimization. We release all the codes in \\url{https://github.com/research4pan/Plum}.","sentences":["Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models.","Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models.","However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods.","Unfortunately, few existing prompt learning methods satisfy the criteria of being truly \"general\", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once.","In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning.","Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in black-box prompt learning and Chain-of-Thought prompt tuning.","Furthermore, we show that these methods can be used to discover more human-understandable prompts that were previously unknown, opening the door to a cornucopia of possibilities in prompt optimization.","We release all the codes in \\url{https://github.com/research4pan/Plum}."],"url":"http://arxiv.org/abs/2311.08364v1"}
{"created":"2023-11-14 18:09:15","title":"Transformers can optimally learn regression mixture models","abstract":"Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms' highly-tailored and model-specific nature. On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing general-purpose prediction methods, even in this mixture setting. In this work, we investigate the hypothesis that transformers can learn an optimal predictor for mixtures of regressions. We construct a generative process for a mixture of linear regressions for which the decision-theoretic optimal procedure is given by data-driven exponential weights on a finite set of parameters. We observe that transformers achieve low mean-squared error on data generated via this process. By probing the transformer's output at inference time, we also show that transformers typically make predictions that are close to the optimal predictor. Our experiments also demonstrate that transformers can learn mixtures of regressions in a sample-efficient fashion and are somewhat robust to distribution shifts. We complement our experimental observations by proving constructively that the decision-theoretic optimal procedure is indeed implementable by a transformer.","sentences":["Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms' highly-tailored and model-specific nature.","On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing general-purpose prediction methods, even in this mixture setting.","In this work, we investigate the hypothesis that transformers can learn an optimal predictor for mixtures of regressions.","We construct a generative process for a mixture of linear regressions for which the decision-theoretic optimal procedure is given by data-driven exponential weights on a finite set of parameters.","We observe that transformers achieve low mean-squared error on data generated via this process.","By probing the transformer's output at inference time, we also show that transformers typically make predictions that are close to the optimal predictor.","Our experiments also demonstrate that transformers can learn mixtures of regressions in a sample-efficient fashion and are somewhat robust to distribution shifts.","We complement our experimental observations by proving constructively that the decision-theoretic optimal procedure is indeed implementable by a transformer."],"url":"http://arxiv.org/abs/2311.08362v1"}
{"created":"2023-11-14 18:03:20","title":"The Transient Nature of Emergent In-Context Learning in Transformers","abstract":"Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it. Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to \"overtrain\" transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits.","sentences":["Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it.","Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data.","However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically.","Here, we show that the emergence of ICL during transformer training is, in fact, often transient.","We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions.","We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL.","The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to \"overtrain\" transformers when seeking compact, cheaper-to-run models.","We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks.","Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits."],"url":"http://arxiv.org/abs/2311.08360v1"}
{"created":"2023-11-14 18:01:15","title":"Rotation-Agnostic Image Representation Learning for Digital Pathology","abstract":"This paper addresses complex challenges in histopathological image analysis through three key contributions. Firstly, it introduces a fast patch selection method, FPS, for whole-slide image (WSI) analysis, significantly reducing computational cost while maintaining accuracy. Secondly, it presents PathDino, a lightweight histopathology feature extractor with a minimal configuration of five Transformer blocks and only 9 million parameters, markedly fewer than alternatives. Thirdly, it introduces a rotation-agnostic representation learning paradigm using self-supervised learning, effectively mitigating overfitting. We also show that our compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including both internal datasets spanning four sites (breast, liver, skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS, DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training dataset of 6 million histopathology patches from The Cancer Genome Atlas (TCGA), our approach demonstrates an average 8.5% improvement in patch-level majority vote performance. These contributions provide a robust framework for enhancing image analysis in digital pathology, rigorously validated through extensive evaluation. Project Page: https://rhazeslab.github.io/PathDino-Page/","sentences":["This paper addresses complex challenges in histopathological image analysis through three key contributions.","Firstly, it introduces a fast patch selection method, FPS, for whole-slide image (WSI) analysis, significantly reducing computational cost while maintaining accuracy.","Secondly, it presents PathDino, a lightweight histopathology feature extractor with a minimal configuration of five Transformer blocks and only 9 million parameters, markedly fewer than alternatives.","Thirdly, it introduces a rotation-agnostic representation learning paradigm using self-supervised learning, effectively mitigating overfitting.","We also show that our compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including both internal datasets spanning four sites (breast, liver, skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS, DigestPath, Kather, PanNuke, and WSSS4LUAD).","Notably, even with a training dataset of 6 million histopathology patches from The Cancer Genome Atlas (TCGA), our approach demonstrates an average 8.5% improvement in patch-level majority vote performance.","These contributions provide a robust framework for enhancing image analysis in digital pathology, rigorously validated through extensive evaluation.","Project Page: https://rhazeslab.github.io/PathDino-Page/"],"url":"http://arxiv.org/abs/2311.08359v1"}
{"created":"2023-11-14 17:59:51","title":"Sparsity-Preserving Differentially Private Training of Large Embedding Models","abstract":"As the use of large embedding models in recommendation systems and language applications increases, concerns over user data privacy have also risen. DP-SGD, a training algorithm that combines differential privacy with stochastic gradient descent, has been the workhorse in protecting user privacy without compromising model accuracy by much. However, applying DP-SGD naively to embedding models can destroy gradient sparsity, leading to reduced training efficiency. To address this issue, we present two new algorithms, DP-FEST and DP-AdaFEST, that preserve gradient sparsity during private training of large embedding models. Our algorithms achieve substantial reductions ($10^6 \\times$) in gradient size, while maintaining comparable levels of accuracy, on benchmark real-world datasets.","sentences":["As the use of large embedding models in recommendation systems and language applications increases, concerns over user data privacy have also risen.","DP-SGD, a training algorithm that combines differential privacy with stochastic gradient descent, has been the workhorse in protecting user privacy without compromising model accuracy by much.","However, applying DP-SGD naively to embedding models can destroy gradient sparsity, leading to reduced training efficiency.","To address this issue, we present two new algorithms, DP-FEST and DP-AdaFEST, that preserve gradient sparsity during private training of large embedding models.","Our algorithms achieve substantial reductions ($10^6 \\times$) in gradient size, while maintaining comparable levels of accuracy, on benchmark real-world datasets."],"url":"http://arxiv.org/abs/2311.08357v1"}
{"created":"2023-11-14 17:53:54","title":"Hierarchical Experience-informed Navigation for Multi-modal Quadrupedal Rebar Grid Traversal","abstract":"This study focuses on a layered, experience-based, multi-modal contact planning framework for agile quadrupedal locomotion over a constrained rebar environment. To this end, our hierarchical planner incorporates locomotion-specific modules into the high-level contact sequence planner and solves kinodynamically-aware trajectory optimization as the low-level motion planner. Through quantitative analysis of the experience accumulation process and experimental validation of the kinodynamic feasibility of the generated locomotion trajectories, we demonstrate that the experience planning heuristic offers an effective way of providing candidate footholds for a legged contact planner. Additionally, we introduce a guiding torso path heuristic at the global planning level to enhance the navigation success rate in the presence of environmental obstacles. Our results indicate that the torso-path guided experience accumulation requires significantly fewer offline trials to successfully reach the goal compared to regular experience accumulation. Finally, our planning framework is validated in both dynamics simulations and real hardware implementations on a quadrupedal robot provided by Skymul Inc.","sentences":["This study focuses on a layered, experience-based, multi-modal contact planning framework for agile quadrupedal locomotion over a constrained rebar environment.","To this end, our hierarchical planner incorporates locomotion-specific modules into the high-level contact sequence planner and solves kinodynamically-aware trajectory optimization as the low-level motion planner.","Through quantitative analysis of the experience accumulation process and experimental validation of the kinodynamic feasibility of the generated locomotion trajectories, we demonstrate that the experience planning heuristic offers an effective way of providing candidate footholds for a legged contact planner.","Additionally, we introduce a guiding torso path heuristic at the global planning level to enhance the navigation success rate in the presence of environmental obstacles.","Our results indicate that the torso-path guided experience accumulation requires significantly fewer offline trials to successfully reach the goal compared to regular experience accumulation.","Finally, our planning framework is validated in both dynamics simulations and real hardware implementations on a quadrupedal robot provided by Skymul Inc."],"url":"http://arxiv.org/abs/2311.08354v1"}
{"created":"2023-11-14 17:48:27","title":"ChoralSynth: Synthetic Dataset of Choral Singing","abstract":"Choral singing, a widely practiced form of ensemble singing, lacks comprehensive datasets in the realm of Music Information Retrieval (MIR) research, due to challenges arising from the requirement to curate multitrack recordings. To address this, we devised a novel methodology, leveraging state-of-the-art synthesizers to create and curate quality renditions. The scores were sourced from Choral Public Domain Library(CPDL). This work is done in collaboration with a diverse team of musicians, software engineers and researchers. The resulting dataset, complete with its associated metadata, and methodology is released as part of this work, opening up new avenues for exploration and advancement in the field of singing voice research.","sentences":["Choral singing, a widely practiced form of ensemble singing, lacks comprehensive datasets in the realm of Music Information Retrieval (MIR) research, due to challenges arising from the requirement to curate multitrack recordings.","To address this, we devised a novel methodology, leveraging state-of-the-art synthesizers to create and curate quality renditions.","The scores were sourced from Choral Public Domain Library(CPDL).","This work is done in collaboration with a diverse team of musicians, software engineers and researchers.","The resulting dataset, complete with its associated metadata, and methodology is released as part of this work, opening up new avenues for exploration and advancement in the field of singing voice research."],"url":"http://arxiv.org/abs/2311.08350v1"}
{"created":"2023-11-14 17:48:19","title":"Artificial Text Boundary Detection with Topological Data Analysis and Sliding Window Techniques","abstract":"Due to the rapid development of text generation models, people increasingly often encounter texts that may start out as written by a human but then continue as machine-generated results of large language models. Detecting the boundary between human-written and machine-generated parts of such texts is a very challenging problem that has not received much attention in literature. In this work, we consider and compare a number of different approaches for this artificial text boundary detection problem, comparing several predictors over features of different nature. We show that supervised fine-tuning of the RoBERTa model works well for this task in general but fails to generalize in important cross-domain and cross-generator settings, demonstrating a tendency to overfit to spurious properties of the data. Then, we propose novel approaches based on features extracted from a frozen language model's embeddings that are able to outperform both the human accuracy level and previously considered baselines on the Real or Fake Text benchmark. Moreover, we adapt perplexity-based approaches for the boundary detection task and analyze their behaviour. We analyze the robustness of all proposed classifiers in cross-domain and cross-model settings, discovering important properties of the data that can negatively influence the performance of artificial text boundary detection algorithms.","sentences":["Due to the rapid development of text generation models, people increasingly often encounter texts that may start out as written by a human but then continue as machine-generated results of large language models.","Detecting the boundary between human-written and machine-generated parts of such texts is a very challenging problem that has not received much attention in literature.","In this work, we consider and compare a number of different approaches for this artificial text boundary detection problem, comparing several predictors over features of different nature.","We show that supervised fine-tuning of the RoBERTa model works well for this task in general but fails to generalize in important cross-domain and cross-generator settings, demonstrating a tendency to overfit to spurious properties of the data.","Then, we propose novel approaches based on features extracted from a frozen language model's embeddings that are able to outperform both the human accuracy level and previously considered baselines on the Real or Fake Text benchmark.","Moreover, we adapt perplexity-based approaches for the boundary detection task and analyze their behaviour.","We analyze the robustness of all proposed classifiers in cross-domain and cross-model settings, discovering important properties of the data that can negatively influence the performance of artificial text boundary detection algorithms."],"url":"http://arxiv.org/abs/2311.08349v1"}
{"created":"2023-11-14 17:45:50","title":"MC^2: A Multilingual Corpus of Minority Languages in China","abstract":"Large-scale corpora play a vital role in the construction of large language models (LLMs). However, existing LLMs exhibit limited abilities in understanding low-resource languages, including the minority languages in China, due to a lack of training data. To improve the accessibility of these languages, we present MC^2, a Multilingual Corpus of Minority Languages in China, which is the largest open-source corpus so far. It encompasses four underrepresented languages, i.e., Tibetan, Uyghur, Kazakh in the Kazakh Arabic script, and Mongolian in the traditional Mongolian script. Notably, two writing systems in MC^2 are long neglected in previous corpora. As we identify serious contamination in the low-resource language split in the existing multilingual corpora, we propose a quality-centric solution for collecting MC^2, prioritizing quality and accuracy while enhancing representativeness and diversity. By in-depth analysis, we demonstrate the new research challenges MC^2 brings, such as long-text modeling and multiplicity of writing systems. We hope MC^2 can help enhance the equity of the underrepresented languages in China and provide a reliable data foundation for further research on low-resource languages.","sentences":["Large-scale corpora play a vital role in the construction of large language models (LLMs).","However, existing LLMs exhibit limited abilities in understanding low-resource languages, including the minority languages in China, due to a lack of training data.","To improve the accessibility of these languages, we present MC^2, a Multilingual Corpus of Minority Languages in China, which is the largest open-source corpus so far.","It encompasses four underrepresented languages, i.e., Tibetan, Uyghur, Kazakh in the Kazakh Arabic script, and Mongolian in the traditional Mongolian script.","Notably, two writing systems in MC^2 are long neglected in previous corpora.","As we identify serious contamination in the low-resource language split in the existing multilingual corpora, we propose a quality-centric solution for collecting MC^2, prioritizing quality and accuracy while enhancing representativeness and diversity.","By in-depth analysis, we demonstrate the new research challenges MC^2 brings, such as long-text modeling and multiplicity of writing systems.","We hope MC^2 can help enhance the equity of the underrepresented languages in China and provide a reliable data foundation for further research on low-resource languages."],"url":"http://arxiv.org/abs/2311.08348v1"}
{"created":"2023-11-14 17:42:01","title":"Speeding Up Optimization-based Motion Planning through Deep Learning","abstract":"Planning collision-free motions for robots with many degrees of freedom is challenging in environments with complex obstacle geometries. Recent work introduced the idea of speeding up the planning by encoding prior experience of successful motion plans in a neural network. However, this \"neural motion planning\" did not scale to complex robots in unseen 3D environments as needed for real-world applications. Here, we introduce \"basis point set\", well-known in computer vision, to neural motion planning as a modern compact environment encoding enabling efficient supervised training networks that generalize well over diverse 3D worlds. Combined with a new elaborate training scheme, we reach a planning success rate of 100%. We use the network to predict an educated initial guess for an optimization-based planner (OMP), which quickly converges to a feasible solution, massively outperforming random multi-starts when tested on previously unseen environments. For the DLR humanoid Agile Justin with 19DoF and in challenging obstacle environments, optimal paths can be generated in 200ms using only a single CPU core. We also show a first successful real-world experiment based on a high-resolution world model from an integrated 3D sensor.","sentences":["Planning collision-free motions for robots with many degrees of freedom is challenging in environments with complex obstacle geometries.","Recent work introduced the idea of speeding up the planning by encoding prior experience of successful motion plans in a neural network.","However, this \"neural motion planning\" did not scale to complex robots in unseen 3D environments as needed for real-world applications.","Here, we introduce \"basis point set\", well-known in computer vision, to neural motion planning as a modern compact environment encoding enabling efficient supervised training networks that generalize well over diverse 3D worlds.","Combined with a new elaborate training scheme, we reach a planning success rate of 100%.","We use the network to predict an educated initial guess for an optimization-based planner (OMP), which quickly converges to a feasible solution, massively outperforming random multi-starts when tested on previously unseen environments.","For the DLR humanoid Agile Justin with 19DoF and in challenging obstacle environments, optimal paths can be generated in 200ms using only a single CPU core.","We also show a first successful real-world experiment based on a high-resolution world model from an integrated 3D sensor."],"url":"http://arxiv.org/abs/2311.08345v1"}
{"created":"2023-11-14 17:31:46","title":"Self-Contained Calibration of an Elastic Humanoid Upper Body Using Only a Head-Mounted RGB Camera","abstract":"When a humanoid robot performs a manipulation task, it first makes a model of the world using its visual sensors and then plans the motion of its body in this model. For this, precise calibration of the camera parameters and the kinematic tree is needed. Besides the accuracy of the calibrated model, the calibration process should be fast and self-contained, i.e., no external measurement equipment should be used. Therefore, we extend our prior work on calibrating the elastic upper body of DLR's Agile Justin by now using only its internal head-mounted RGB camera. We use simple visual markers at the ends of the kinematic chain and one in front of the robot, mounted on a pole, to get measurements for the whole kinematic tree. To ensure that the task-relevant cartesian error at the end-effectors is minimized, we introduce virtual noise to fit our imperfect robot model so that the pixel error has a higher weight if the marker is further away from the camera. This correction reduces the cartesian error by more than 20%, resulting in a final accuracy of 3.9mm on average and 9.1mm in the worst case. This way, we achieve the same precision as in our previous work, where an external cartesian tracking system was used.","sentences":["When a humanoid robot performs a manipulation task, it first makes a model of the world using its visual sensors and then plans the motion of its body in this model.","For this, precise calibration of the camera parameters and the kinematic tree is needed.","Besides the accuracy of the calibrated model, the calibration process should be fast and self-contained, i.e., no external measurement equipment should be used.","Therefore, we extend our prior work on calibrating the elastic upper body of DLR's Agile Justin by now using only its internal head-mounted RGB camera.","We use simple visual markers at the ends of the kinematic chain and one in front of the robot, mounted on a pole, to get measurements for the whole kinematic tree.","To ensure that the task-relevant cartesian error at the end-effectors is minimized, we introduce virtual noise to fit our imperfect robot model so that the pixel error has a higher weight if the marker is further away from the camera.","This correction reduces the cartesian error by more than 20%, resulting in a final accuracy of 3.9mm on average and 9.1mm in the worst case.","This way, we achieve the same precision as in our previous work, where an external cartesian tracking system was used."],"url":"http://arxiv.org/abs/2311.08338v1"}
{"created":"2023-11-14 17:27:30","title":"Exploring Variational Auto-Encoder Architectures, Configurations, and Datasets for Generative Music Explainable AI","abstract":"Generative AI models for music and the arts in general are increasingly complex and hard to understand. The field of eXplainable AI (XAI) seeks to make complex and opaque AI models such as neural networks more understandable to people. One approach to making generative AI models more understandable is to impose a small number of semantically meaningful attributes on generative AI models. This paper contributes a systematic examination of the impact that different combinations of Variational Auto-Encoder models (MeasureVAE and AdversarialVAE), configurations of latent space in the AI model (from 4 to 256 latent dimensions), and training datasets (Irish folk, Turkish folk, Classical, and pop) have on music generation performance when 2 or 4 meaningful musical attributes are imposed on the generative model. To date there have been no systematic comparisons of such models at this level of combinatorial detail. Our findings show that MeasureVAE has better reconstruction performance than AdversarialVAE which has better musical attribute independence. Results demonstrate that MeasureVAE was able to generate music across music genres with interpretable musical dimensions of control, and performs best with low complexity music such a pop and rock. We recommend that a 32 or 64 latent dimensional space is optimal for 4 regularised dimensions when using MeasureVAE to generate music across genres. Our results are the first detailed comparisons of configurations of state-of-the-art generative AI models for music and can be used to help select and configure AI models, musical features, and datasets for more understandable generation of music.","sentences":["Generative AI models for music and the arts in general are increasingly complex and hard to understand.","The field of eXplainable AI (XAI) seeks to make complex and opaque AI models such as neural networks more understandable to people.","One approach to making generative AI models more understandable is to impose a small number of semantically meaningful attributes on generative AI models.","This paper contributes a systematic examination of the impact that different combinations of Variational Auto-Encoder models (MeasureVAE and AdversarialVAE), configurations of latent space in the AI model (from 4 to 256 latent dimensions), and training datasets (Irish folk, Turkish folk, Classical, and pop) have on music generation performance when 2 or 4 meaningful musical attributes are imposed on the generative model.","To date there have been no systematic comparisons of such models at this level of combinatorial detail.","Our findings show that MeasureVAE has better reconstruction performance than AdversarialVAE which has better musical attribute independence.","Results demonstrate that MeasureVAE was able to generate music across music genres with interpretable musical dimensions of control, and performs best with low complexity music such a pop and rock.","We recommend that a 32 or 64 latent dimensional space is optimal for 4 regularised dimensions when using MeasureVAE to generate music across genres.","Our results are the first detailed comparisons of configurations of state-of-the-art generative AI models for music and can be used to help select and configure AI models, musical features, and datasets for more understandable generation of music."],"url":"http://arxiv.org/abs/2311.08336v1"}
{"created":"2023-11-14 17:25:14","title":"Calibration of an Elastic Humanoid Upper Body and Efficient Compensation for Motion Planning","abstract":"High absolute accuracy is an essential prerequisite for a humanoid robot to autonomously and robustly perform manipulation tasks while avoiding obstacles. We present for the first time a kinematic model for a humanoid upper body incorporating joint and transversal elasticities. These elasticities lead to significant deformations due to the robot's own weight, and the resulting model is implicitly defined via a torque equilibrium. We successfully calibrate this model for DLR's humanoid Agile Justin, including all Denavit-Hartenberg parameters and elasticities. The calibration is formulated as a combined least-squares problem with priors and based on measurements of the end effector positions of both arms via an external tracking system. The absolute position error is massively reduced from 21mm to 3.1mm on average in the whole workspace. Using this complex and implicit kinematic model in motion planning is challenging. We show that for optimization-based path planning, integrating the iterative solution of the implicit model into the optimization loop leads to an elegant and highly efficient solution. For mildly elastic robots like Agile Justin, there is no performance impact, and even for a simulated highly flexible robot with 20 times higher elasticities, the runtime increases by only 30%.","sentences":["High absolute accuracy is an essential prerequisite for a humanoid robot to autonomously and robustly perform manipulation tasks while avoiding obstacles.","We present for the first time a kinematic model for a humanoid upper body incorporating joint and transversal elasticities.","These elasticities lead to significant deformations due to the robot's own weight, and the resulting model is implicitly defined via a torque equilibrium.","We successfully calibrate this model for DLR's humanoid Agile Justin, including all Denavit-Hartenberg parameters and elasticities.","The calibration is formulated as a combined least-squares problem with priors and based on measurements of the end effector positions of both arms via an external tracking system.","The absolute position error is massively reduced from 21mm to 3.1mm on average in the whole workspace.","Using this complex and implicit kinematic model in motion planning is challenging.","We show that for optimization-based path planning, integrating the iterative solution of the implicit model into the optimization loop leads to an elegant and highly efficient solution.","For mildly elastic robots like Agile Justin, there is no performance impact, and even for a simulated highly flexible robot with 20 times higher elasticities, the runtime increases by only 30%."],"url":"http://arxiv.org/abs/2311.08333v1"}
{"created":"2023-11-14 17:18:08","title":"KTRL+F: Knowledge-Augmented In-Document Search","abstract":"We introduce a new problem KTRL+F, a knowledge-augmented in-document search task that necessitates real-time identification of all semantic targets within a document with the awareness of external sources through a single natural query. This task addresses following unique challenges for in-document search: 1) utilizing knowledge outside the document for extended use of additional information about targets to bridge the semantic gap between the query and the targets, and 2) balancing between real-time applicability with the performance. We analyze various baselines in KTRL+F and find there are limitations of existing models, such as hallucinations, low latency, or difficulties in leveraging external knowledge. Therefore we propose a Knowledge-Augmented Phrase Retrieval model that shows a promising balance between speed and performance by simply augmenting external knowledge embedding in phrase embedding. Additionally, we conduct a user study to verify whether solving KTRL+F can enhance search experience of users. It demonstrates that even with our simple model users can reduce the time for searching with less queries and reduced extra visits to other sources for collecting evidence. We encourage the research community to work on KTRL+F to enhance more efficient in-document information access.","sentences":["We introduce a new problem KTRL+F, a knowledge-augmented in-document search task that necessitates real-time identification of all semantic targets within a document with the awareness of external sources through a single natural query.","This task addresses following unique challenges for in-document search: 1) utilizing knowledge outside the document for extended use of additional information about targets to bridge the semantic gap between the query and the targets, and 2) balancing between real-time applicability with the performance.","We analyze various baselines in KTRL+F and find there are limitations of existing models, such as hallucinations, low latency, or difficulties in leveraging external knowledge.","Therefore we propose a Knowledge-Augmented Phrase Retrieval model that shows a promising balance between speed and performance by simply augmenting external knowledge embedding in phrase embedding.","Additionally, we conduct a user study to verify whether solving KTRL+F can enhance search experience of users.","It demonstrates that even with our simple model users can reduce the time for searching with less queries and reduced extra visits to other sources for collecting evidence.","We encourage the research community to work on KTRL+F to enhance more efficient in-document information access."],"url":"http://arxiv.org/abs/2311.08329v1"}
{"created":"2023-11-14 17:17:16","title":"A PRISMA-driven systematic mapping study on system assurance weakeners","abstract":"Context: An assurance case is a structured hierarchy of claims aiming at demonstrating that a given mission-critical system supports specific requirements (e.g., safety, security, privacy). The presence of assurance weakeners (i.e., assurance deficits, logical fallacies) in assurance cases reflects insufficient evidence, knowledge, or gaps in reasoning. These weakeners can undermine confidence in assurance arguments, potentially hindering the verification of mission-critical system capabilities.   Objectives: As a stepping stone for future research on assurance weakeners, we aim to initiate the first comprehensive systematic mapping study on this subject. Methods: We followed the well-established PRISMA 2020 and SEGRESS guidelines to conduct our systematic mapping study. We searched for primary studies in five digital libraries and focused on the 2012-2023 publication year range. Our selection criteria focused on studies addressing assurance weakeners at the modeling level, resulting in the inclusion of 39 primary studies in our systematic review.   Results: Our systematic mapping study reports a taxonomy (map) that provides a uniform categorization of assurance weakeners and approaches proposed to manage them at the modeling level.   Conclusion: Our study findings suggest that the SACM (Structured Assurance Case Metamodel) -- a standard specified by the OMG (Object Management Group) -- may be the best specification to capture structured arguments and reason about their potential assurance weakeners.","sentences":["Context: An assurance case is a structured hierarchy of claims aiming at demonstrating that a given mission-critical system supports specific requirements (e.g., safety, security, privacy).","The presence of assurance weakeners (i.e., assurance deficits, logical fallacies) in assurance cases reflects insufficient evidence, knowledge, or gaps in reasoning.","These weakeners can undermine confidence in assurance arguments, potentially hindering the verification of mission-critical system capabilities.   ","Objectives:","As a stepping stone for future research on assurance weakeners, we aim to initiate the first comprehensive systematic mapping study on this subject.","Methods: We followed the well-established PRISMA 2020 and SEGRESS guidelines to conduct our systematic mapping study.","We searched for primary studies in five digital libraries and focused on the 2012-2023 publication year range.","Our selection criteria focused on studies addressing assurance weakeners at the modeling level, resulting in the inclusion of 39 primary studies in our systematic review.   ","Results: Our systematic mapping study reports a taxonomy (map) that provides a uniform categorization of assurance weakeners and approaches proposed to manage them at the modeling level.   ","Conclusion: Our study findings suggest that the SACM (Structured Assurance Case Metamodel) -- a standard specified by the OMG (Object Management Group) -- may be the best specification to capture structured arguments and reason about their potential assurance weakeners."],"url":"http://arxiv.org/abs/2311.08328v1"}
{"created":"2023-11-14 17:11:12","title":"Protecting the Future of Information: LOCO Coding With Error Detection for DNA Data Storage","abstract":"DNA strands serve as a storage medium for $4$-ary data over the alphabet $\\{A,T,G,C\\}$. DNA data storage promises formidable information density, long-term durability, and ease of replicability. However, information in this intriguing storage technology might be corrupted. Experiments have revealed that DNA sequences with long homopolymers and/or with low $GC$-content are notably more subject to errors upon storage.   This paper investigates the utilization of the recently-introduced method for designing lexicographically-ordered constrained (LOCO) codes in DNA data storage. This paper introduces DNA LOCO (D-LOCO) codes, over the alphabet $\\{A,T,G,C\\}$ with limited runs of identical symbols. These codes come with an encoding-decoding rule we derive, which provides affordable encoding-decoding algorithms. In terms of storage overhead, the proposed encoding-decoding algorithms outperform those in the existing literature. Our algorithms are readily reconfigurable. D-LOCO codes are intrinsically balanced, which allows us to achieve balancing over the entire DNA strand with minimal rate penalty. Moreover, we propose four schemes to bridge consecutive codewords, three of which guarantee single substitution error detection per codeword. We examine the probability of undetecting errors. We also show that D-LOCO codes are capacity-achieving and that they offer remarkably high rates at moderate lengths.","sentences":["DNA strands serve as a storage medium for $4$-ary data over the alphabet $\\{A,T,G,C\\}$. DNA data storage promises formidable information density, long-term durability, and ease of replicability.","However, information in this intriguing storage technology might be corrupted.","Experiments have revealed that DNA sequences with long homopolymers and/or with low $GC$-content are notably more subject to errors upon storage.   ","This paper investigates the utilization of the recently-introduced method for designing lexicographically-ordered constrained (LOCO) codes in DNA data storage.","This paper introduces DNA LOCO (D-LOCO) codes, over the alphabet $\\{A,T,G,C\\}$ with limited runs of identical symbols.","These codes come with an encoding-decoding rule we derive, which provides affordable encoding-decoding algorithms.","In terms of storage overhead, the proposed encoding-decoding algorithms outperform those in the existing literature.","Our algorithms are readily reconfigurable.","D-LOCO codes are intrinsically balanced, which allows us to achieve balancing over the entire DNA strand with minimal rate penalty.","Moreover, we propose four schemes to bridge consecutive codewords, three of which guarantee single substitution error detection per codeword.","We examine the probability of undetecting errors.","We also show that D-LOCO codes are capacity-achieving and that they offer remarkably high rates at moderate lengths."],"url":"http://arxiv.org/abs/2311.08325v1"}
{"created":"2023-11-14 17:09:43","title":"Anti-LM Decoding for Zero-shot In-context Machine Translation","abstract":"Zero-shot In-context learning is the phenomenon where models can perform the task simply given the instructions. However, pre-trained large language models are known to be poorly calibrated for this task. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on some context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search ($B=5$). The proposed method outperforms other state-of-art decoding objectives, with up to $20$ BLEU point improvement from the default objective observed in some settings.","sentences":["Zero-shot In-context learning is the phenomenon where models can perform the task simply given the instructions.","However, pre-trained large language models are known to be poorly calibrated for this task.","One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on some context.","This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation.","We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search ($B=5$).","The proposed method outperforms other state-of-art decoding objectives, with up to $20$ BLEU point improvement from the default objective observed in some settings."],"url":"http://arxiv.org/abs/2311.08324v1"}
{"created":"2023-11-14 17:09:07","title":"Open-vocabulary keyword spotting in any language through multilingual contrastive speech-phoneme pretraining","abstract":"In this paper, we introduce a massively multilingual speech corpora with fine-grained phonemic transcriptions, encompassing more than 115 languages from diverse language families. Based on this multilingual dataset, we propose CLAP-IPA, a multilingual phoneme-speech contrastive embedding model capable of open-vocabulary matching between speech signals and phonemically transcribed keywords or arbitrary phrases. The proposed model has been tested on two fieldwork speech corpora in 97 unseen languages, exhibiting strong generalizability across languages. Comparison with a text-based model shows that using phonemes as modeling units enables much better crosslinguistic generalization than orthographic texts.","sentences":["In this paper, we introduce a massively multilingual speech corpora with fine-grained phonemic transcriptions, encompassing more than 115 languages from diverse language families.","Based on this multilingual dataset, we propose CLAP-IPA, a multilingual phoneme-speech contrastive embedding model capable of open-vocabulary matching between speech signals and phonemically transcribed keywords or arbitrary phrases.","The proposed model has been tested on two fieldwork speech corpora in 97 unseen languages, exhibiting strong generalizability across languages.","Comparison with a text-based model shows that using phonemes as modeling units enables much better crosslinguistic generalization than orthographic texts."],"url":"http://arxiv.org/abs/2311.08323v1"}
{"created":"2023-11-14 17:07:24","title":"GT4Py: High Performance Stencils for Weather and Climate Applications using Python","abstract":"All major weather and climate applications are currently developed using languages such as Fortran or C++. This is typical in the domain of high performance computing (HPC), where efficient execution is an important concern. Unfortunately, this approach leads to implementations that intermix optimizations for specific hardware architectures with the high-level numerical methods that are typical for the domain. This leads to code that is verbose, difficult to extend and maintain, and difficult to port to different hardware architectures. Here, we propose a different strategy based on GT4Py (GridTools for Python). GT4Py is a Python framework to write weather and climate applications that includes a high-level embedded domain specific language (DSL) to write stencil computations. The toolchain integrated in GT4Py enables automatic code-generation,to obtain the performance of state-of-the-art C++ and CUDA implementations. The separation of concerns between the mathematical definitions and the actual implementations allows for performance portability of the computations on a wide range of computing architectures, while being embedded in Python allows easy access to the tools of the Python ecosystem to enhance the productivity of the scientists and facilitate integration in complex workflows. Here, the initial release of GT4Py is described, providing an overview of the current state of the framework and performance results showing how GT4Py can outperform pure Python implementations by orders of magnitude.","sentences":["All major weather and climate applications are currently developed using languages such as Fortran or C++.","This is typical in the domain of high performance computing (HPC), where efficient execution is an important concern.","Unfortunately, this approach leads to implementations that intermix optimizations for specific hardware architectures with the high-level numerical methods that are typical for the domain.","This leads to code that is verbose, difficult to extend and maintain, and difficult to port to different hardware architectures.","Here, we propose a different strategy based on GT4Py (GridTools for Python).","GT4Py is a Python framework to write weather and climate applications that includes a high-level embedded domain specific language (DSL) to write stencil computations.","The toolchain integrated in GT4Py enables automatic code-generation,to obtain the performance of state-of-the-art C++ and CUDA implementations.","The separation of concerns between the mathematical definitions and the actual implementations allows for performance portability of the computations on a wide range of computing architectures, while being embedded in Python allows easy access to the tools of the Python ecosystem to enhance the productivity of the scientists and facilitate integration in complex workflows.","Here, the initial release of GT4Py is described, providing an overview of the current state of the framework and performance results showing how GT4Py can outperform pure Python implementations by orders of magnitude."],"url":"http://arxiv.org/abs/2311.08322v1"}
{"created":"2023-11-14 17:04:17","title":"CV32RT: Enabling Fast Interrupt and Context Switching for RISC-V Microcontrollers","abstract":"Processors using the open RISC-V ISA are finding increasing adoption in the embedded world. Many embedded use cases have real-time constraints and require flexible, predictable, and fast reactive handling of incoming events. However, RISC- V processors are still lagging in this area compared to more mature proprietary architectures, such as ARM Cortex-M and TriCore, which have been tuned for years. The default interrupt controller standardized by RISC-V, the Core Local Interruptor (CLINT), lacks configurability in prioritization and preemption of interrupts. The RISC-V Core Local Interrupt Controller (CLIC) specification addresses this concern by enabling pre-emptible, low-latency vectored interrupts while also envisioning optional extensions to improve interrupt latency. In this work, we implement a CLIC for the CV32E40P, an industrially supported open-source 32-bit MCU-class RISC-V core, and enhance it with fastirq: a custom extension that provides interrupt latency as low as 6 cycles. We call CV32RT our enhanced core. To the best of our knowledge, CV32RT is the first fully open-source RV32 core with competitive interrupt-handling features compared to the Arm Cortex-M series and TriCore. The proposed extensions are also demonstrated to improve task context switching in real-time operating systems.","sentences":["Processors using the open RISC-V ISA are finding increasing adoption in the embedded world.","Many embedded use cases have real-time constraints and require flexible, predictable, and fast reactive handling of incoming events.","However, RISC- V processors are still lagging in this area compared to more mature proprietary architectures, such as ARM Cortex-M and TriCore, which have been tuned for years.","The default interrupt controller standardized by RISC-V, the Core Local Interruptor (CLINT), lacks configurability in prioritization and preemption of interrupts.","The RISC-V Core Local Interrupt Controller (CLIC) specification addresses this concern by enabling pre-emptible, low-latency vectored interrupts while also envisioning optional extensions to improve interrupt latency.","In this work, we implement a CLIC for the CV32E40P, an industrially supported open-source 32-bit MCU-class RISC-V core, and enhance it with fastirq: a custom extension that provides interrupt latency as low as 6 cycles.","We call CV32RT our enhanced core.","To the best of our knowledge, CV32RT is the first fully open-source RV32 core with competitive interrupt-handling features compared to the Arm Cortex-M series and TriCore.","The proposed extensions are also demonstrated to improve task context switching in real-time operating systems."],"url":"http://arxiv.org/abs/2311.08320v1"}
{"created":"2023-11-14 17:03:52","title":"Resource Efficient Over-the-Air Fronthaul Signaling for Uplink Cell-Free Massive MIMO Systems","abstract":"We propose a novel resource efficient analog over-the-air (OTA) computation framework to address the demanding requirements of the uplink (UL) fronthaul between the access points (APs) and the central processing unit (CPU) in cell-free massive multiple-input multiple-output (MIMO) systems. We discuss the drawbacks of the wired and wireless fronthaul solutions, and show that our proposed mechanism is efficient and scalable as the number of APs increases. We present the transmit precoding and two-phase power assignment strategies at the APs to coherently combine the signals OTA in a spectrally efficient manner. We derive the statistics of the APs locally available signals which enable us to to obtain the analytical expressions for the Bayesian and classical estimators of the OTA combined signals. We empirically evaluate the normalized mean square error (NMSE), symbol error rate (SER), and the coded bit error rate (BER) of our developed solution and benchmark against the state-of-the-art wired fronthaul based system","sentences":["We propose a novel resource efficient analog over-the-air (OTA) computation framework to address the demanding requirements of the uplink (UL) fronthaul between the access points (APs) and the central processing unit (CPU) in cell-free massive multiple-input multiple-output (MIMO) systems.","We discuss the drawbacks of the wired and wireless fronthaul solutions, and show that our proposed mechanism is efficient and scalable as the number of APs increases.","We present the transmit precoding and two-phase power assignment strategies at the APs to coherently combine the signals OTA in a spectrally efficient manner.","We derive the statistics of the APs locally available signals which enable us to to obtain the analytical expressions for the Bayesian and classical estimators of the OTA combined signals.","We empirically evaluate the normalized mean square error (NMSE), symbol error rate (SER), and the coded bit error rate (BER) of our developed solution and benchmark against the state-of-the-art wired fronthaul based system"],"url":"http://arxiv.org/abs/2311.08319v1"}
{"created":"2023-11-14 16:58:18","title":"Convolutional Neural Networks Exploiting Attributes of Biological Neurons","abstract":"In this era of artificial intelligence, deep neural networks like Convolutional Neural Networks (CNNs) have emerged as front-runners, often surpassing human capabilities. These deep networks are often perceived as the panacea for all challenges. Unfortunately, a common downside of these networks is their ''black-box'' character, which does not necessarily mirror the operation of biological neural systems. Some even have millions/billions of learnable (tunable) parameters, and their training demands extensive data and time.   Here, we integrate the principles of biological neurons in certain layer(s) of CNNs. Specifically, we explore the use of neuro-science-inspired computational models of the Lateral Geniculate Nucleus (LGN) and simple cells of the primary visual cortex. By leveraging such models, we aim to extract image features to use as input to CNNs, hoping to enhance training efficiency and achieve better accuracy. We aspire to enable shallow networks with a Push-Pull Combination of Receptive Fields (PP-CORF) model of simple cells as the foundation layer of CNNs to enhance their learning process and performance. To achieve this, we propose a two-tower CNN, one shallow tower and the other as ResNet 18. Rather than extracting the features blindly, it seeks to mimic how the brain perceives and extracts features. The proposed system exhibits a noticeable improvement in the performance (on an average of $5\\%-10\\%$) on CIFAR-10, CIFAR-100, and ImageNet-100 datasets compared to ResNet-18. We also check the efficiency of only the Push-Pull tower of the network.","sentences":["In this era of artificial intelligence, deep neural networks like Convolutional Neural Networks (CNNs) have emerged as front-runners, often surpassing human capabilities.","These deep networks are often perceived as the panacea for all challenges.","Unfortunately, a common downside of these networks is their ''black-box'' character, which does not necessarily mirror the operation of biological neural systems.","Some even have millions/billions of learnable (tunable) parameters, and their training demands extensive data and time.   ","Here, we integrate the principles of biological neurons in certain layer(s) of CNNs.","Specifically, we explore the use of neuro-science-inspired computational models of the Lateral Geniculate Nucleus (LGN) and simple cells of the primary visual cortex.","By leveraging such models, we aim to extract image features to use as input to CNNs, hoping to enhance training efficiency and achieve better accuracy.","We aspire to enable shallow networks with a Push-Pull Combination of Receptive Fields (PP-CORF) model of simple cells as the foundation layer of CNNs to enhance their learning process and performance.","To achieve this, we propose a two-tower CNN, one shallow tower and the other as ResNet 18.","Rather than extracting the features blindly, it seeks to mimic how the brain perceives and extracts features.","The proposed system exhibits a noticeable improvement in the performance (on an average of $5\\%-10\\%$) on CIFAR-10, CIFAR-100, and ImageNet-100 datasets compared to ResNet-18.","We also check the efficiency of only the Push-Pull tower of the network."],"url":"http://arxiv.org/abs/2311.08314v1"}
{"created":"2023-11-14 16:57:34","title":"On the Fast Track to Full Gold Open Access","abstract":"The world of scientific publishing is changing, the days of an old type of subscription-based earnings for publishers are it seems over, and we are entering a new era. It seems as if an ever-increasing number of journals from disparate publishers are going Gold, Open Access that is, yet have we rigorously ascertained the issue in its entirety, or are we touting the strengths and forgetting about constructive criticism and careful weighing of evidence? We will therefore present the current state of the art of this more relevant than ever hot topic and suggest solutions that are most likely to be acceptable to all parties.","sentences":["The world of scientific publishing is changing, the days of an old type of subscription-based earnings for publishers are it seems over, and we are entering a new era.","It seems as if an ever-increasing number of journals from disparate publishers are going Gold, Open Access that is, yet have we rigorously ascertained the issue in its entirety, or are we touting the strengths and forgetting about constructive criticism and careful weighing of evidence?","We will therefore present the current state of the art of this more relevant than ever hot topic and suggest solutions that are most likely to be acceptable to all parties."],"url":"http://arxiv.org/abs/2311.08313v1"}
{"created":"2023-11-14 16:55:12","title":"Introducing an Improved Information-Theoretic Measure of Predictive Uncertainty","abstract":"Applying a machine learning model for decision-making in the real world requires to distinguish what the model knows from what it does not. A critical factor in assessing the knowledge of a model is to quantify its predictive uncertainty. Predictive uncertainty is commonly measured by the entropy of the Bayesian model average (BMA) predictive distribution. Yet, the properness of this current measure of predictive uncertainty was recently questioned. We provide new insights regarding those limitations. Our analyses show that the current measure erroneously assumes that the BMA predictive distribution is equivalent to the predictive distribution of the true model that generated the dataset. Consequently, we introduce a theoretically grounded measure to overcome these limitations. We experimentally verify the benefits of our introduced measure of predictive uncertainty. We find that our introduced measure behaves more reasonably in controlled synthetic tasks. Moreover, our evaluations on ImageNet demonstrate that our introduced measure is advantageous in real-world applications utilizing predictive uncertainty.","sentences":["Applying a machine learning model for decision-making in the real world requires to distinguish what the model knows from what it does not.","A critical factor in assessing the knowledge of a model is to quantify its predictive uncertainty.","Predictive uncertainty is commonly measured by the entropy of the Bayesian model average (BMA) predictive distribution.","Yet, the properness of this current measure of predictive uncertainty was recently questioned.","We provide new insights regarding those limitations.","Our analyses show that the current measure erroneously assumes that the BMA predictive distribution is equivalent to the predictive distribution of the true model that generated the dataset.","Consequently, we introduce a theoretically grounded measure to overcome these limitations.","We experimentally verify the benefits of our introduced measure of predictive uncertainty.","We find that our introduced measure behaves more reasonably in controlled synthetic tasks.","Moreover, our evaluations on ImageNet demonstrate that our introduced measure is advantageous in real-world applications utilizing predictive uncertainty."],"url":"http://arxiv.org/abs/2311.08309v1"}
{"created":"2023-11-14 16:53:45","title":"The Heat is On: Thermal Facial Landmark Tracking","abstract":"Facial landmark tracking for thermal images requires tracking certain important regions of subjects' faces, using images from thermal images, which omit lighting and shading, but show the temperatures of their subjects. The fluctuations of heat in particular places reflect physiological changes like bloodflow and perspiration, which can be used to remotely gauge things like anxiety and excitement. Past work in this domain has been limited to only a very limited set of architectures and techniques. This work goes further by trying a comprehensive suit of various models with different components, such as residual connections, channel and feature-wise attention, as well as the practice of ensembling components of the network to work in parallel. The best model integrated convolutional and residual layers followed by a channel-wise self-attention layer, requiring less than 100K parameters.","sentences":["Facial landmark tracking for thermal images requires tracking certain important regions of subjects' faces, using images from thermal images, which omit lighting and shading, but show the temperatures of their subjects.","The fluctuations of heat in particular places reflect physiological changes like bloodflow and perspiration, which can be used to remotely gauge things like anxiety and excitement.","Past work in this domain has been limited to only a very limited set of architectures and techniques.","This work goes further by trying a comprehensive suit of various models with different components, such as residual connections, channel and feature-wise attention, as well as the practice of ensembling components of the network to work in parallel.","The best model integrated convolutional and residual layers followed by a channel-wise self-attention layer, requiring less than 100K parameters."],"url":"http://arxiv.org/abs/2311.08308v1"}
{"created":"2023-11-14 16:49:33","title":"On-the-Fly Fusion of Large Language Models and Machine Translation","abstract":"We propose the on-the-fly ensembling of a machine translation model with an LLM, prompted on the same task and input. We perform experiments on 4 language pairs (both directions) with varying data amounts. We find that a slightly weaker-at-translation LLM can improve translations of a NMT model, and ensembling with an LLM can produce better translations than ensembling two stronger MT models. We combine our method with various techniques from LLM prompting, such as in context learning and translation context.","sentences":["We propose the on-the-fly ensembling of a machine translation model with an LLM, prompted on the same task and input.","We perform experiments on 4 language pairs (both directions) with varying data amounts.","We find that a slightly weaker-at-translation LLM can improve translations of a NMT model, and ensembling with an LLM can produce better translations than ensembling two stronger MT models.","We combine our method with various techniques from LLM prompting, such as in context learning and translation context."],"url":"http://arxiv.org/abs/2311.08306v1"}
{"created":"2023-11-14 16:46:15","title":"Extrinsically-Focused Evaluation of Omissions in Medical Summarization","abstract":"The goal of automated summarization techniques (Paice, 1990; Kupiec et al, 1995) is to condense text by focusing on the most critical information. Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs. In safety-critical domains such as medicine, more rigorous evaluation is required, especially given the potential for LLMs to omit important information in the resulting summary. We propose MED-OMIT, a new omission benchmark for medical summarization. Given a doctor-patient conversation and a generated summary, MED-OMIT categorizes the chat into a set of facts and identifies which are omitted from the summary. We further propose to determine fact importance by simulating the impact of each fact on a downstream clinical task: differential diagnosis (DDx) generation. MED-OMIT leverages LLM prompt-based approaches which categorize the importance of facts and cluster them as supporting or negating evidence to the diagnosis. We evaluate MED-OMIT on a publicly-released dataset of patient-doctor conversations and find that MED-OMIT captures omissions better than alternative metrics.","sentences":["The goal of automated summarization techniques (Paice, 1990; Kupiec et al, 1995) is to condense text by focusing on the most critical information.","Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs.","In safety-critical domains such as medicine, more rigorous evaluation is required, especially given the potential for LLMs to omit important information in the resulting summary.","We propose MED-OMIT, a new omission benchmark for medical summarization.","Given a doctor-patient conversation and a generated summary, MED-OMIT categorizes the chat into a set of facts and identifies which are omitted from the summary.","We further propose to determine fact importance by simulating the impact of each fact on a downstream clinical task: differential diagnosis (DDx) generation.","MED-OMIT leverages LLM prompt-based approaches which categorize the importance of facts and cluster them as supporting or negating evidence to the diagnosis.","We evaluate MED-OMIT on a publicly-released dataset of patient-doctor conversations and find that MED-OMIT captures omissions better than alternative metrics."],"url":"http://arxiv.org/abs/2311.08303v1"}
{"created":"2023-11-14 16:46:10","title":"Inverse Learning with Extremely Sparse Feedback for Recommendation","abstract":"Modern personalized recommendation services often rely on user feedback, either explicit or implicit, to improve the quality of services. Explicit feedback refers to behaviors like ratings, while implicit feedback refers to behaviors like user clicks. However, in the scenario of full-screen video viewing experiences like Tiktok and Reels, the click action is absent, resulting in unclear feedback from users, hence introducing noises in modeling training. Existing approaches on de-noising recommendation mainly focus on positive instances while ignoring the noise in a large amount of sampled negative feedback. In this paper, we propose a meta-learning method to annotate the unlabeled data from loss and gradient perspectives, which considers the noises in both positive and negative instances. Specifically, we first propose an Inverse Dual Loss (IDL) to boost the true label learning and prevent the false label learning. Then we further propose an Inverse Gradient (IG) method to explore the correct updating gradient and adjust the updating based on meta-learning. Finally, we conduct extensive experiments on both benchmark and industrial datasets where our proposed method can significantly improve AUC by 9.25% against state-of-the-art methods. Further analysis verifies the proposed inverse learning framework is model-agnostic and can improve a variety of recommendation backbones. The source code, along with the best hyper-parameter settings, is available at this link: https://github.com/Guanyu-Lin/InverseLearning.","sentences":["Modern personalized recommendation services often rely on user feedback, either explicit or implicit, to improve the quality of services.","Explicit feedback refers to behaviors like ratings, while implicit feedback refers to behaviors like user clicks.","However, in the scenario of full-screen video viewing experiences like Tiktok and Reels, the click action is absent, resulting in unclear feedback from users, hence introducing noises in modeling training.","Existing approaches on de-noising recommendation mainly focus on positive instances while ignoring the noise in a large amount of sampled negative feedback.","In this paper, we propose a meta-learning method to annotate the unlabeled data from loss and gradient perspectives, which considers the noises in both positive and negative instances.","Specifically, we first propose an Inverse Dual Loss (IDL) to boost the true label learning and prevent the false label learning.","Then we further propose an Inverse Gradient (IG) method to explore the correct updating gradient and adjust the updating based on meta-learning.","Finally, we conduct extensive experiments on both benchmark and industrial datasets where our proposed method can significantly improve AUC by 9.25% against state-of-the-art methods.","Further analysis verifies the proposed inverse learning framework is model-agnostic and can improve a variety of recommendation backbones.","The source code, along with the best hyper-parameter settings, is available at this link: https://github.com/Guanyu-Lin/InverseLearning."],"url":"http://arxiv.org/abs/2311.08302v1"}
{"created":"2023-11-14 16:44:33","title":"Workflow-Guided Response Generation for Task-Oriented Dialogue","abstract":"Task-oriented dialogue (TOD) systems aim to achieve specific goals through interactive dialogue. Such tasks usually involve following specific workflows, i.e. executing a sequence of actions in a particular order. While prior work has focused on supervised learning methods to condition on past actions, they do not explicitly optimize for compliance to a desired workflow. In this paper, we propose a novel framework based on reinforcement learning (RL) to generate dialogue responses that are aligned with a given workflow. Our framework consists of ComplianceScorer, a metric designed to evaluate how well a generated response executes the specified action, combined with an RL opimization process that utilizes an interactive sampling technique. We evaluate our approach on two TOD datasets, Action-Based Conversations Dataset (ABCD) (Chen et al., 2021a) and MultiWOZ 2.2 (Zang et al., 2020) on a range of automated and human evaluation metrics. Our findings indicate that our RL-based framework outperforms baselines and is effective at enerating responses that both comply with the intended workflows while being expressed in a natural and fluent manner.","sentences":["Task-oriented dialogue (TOD) systems aim to achieve specific goals through interactive dialogue.","Such tasks usually involve following specific workflows, i.e. executing a sequence of actions in a particular order.","While prior work has focused on supervised learning methods to condition on past actions, they do not explicitly optimize for compliance to a desired workflow.","In this paper, we propose a novel framework based on reinforcement learning (RL) to generate dialogue responses that are aligned with a given workflow.","Our framework consists of ComplianceScorer, a metric designed to evaluate how well a generated response executes the specified action, combined with an RL opimization process that utilizes an interactive sampling technique.","We evaluate our approach on two TOD datasets, Action-Based Conversations Dataset (ABCD) (Chen et al., 2021a) and MultiWOZ 2.2 (Zang et al., 2020) on a range of automated and human evaluation metrics.","Our findings indicate that our RL-based framework outperforms baselines and is effective at enerating responses that both comply with the intended workflows while being expressed in a natural and fluent manner."],"url":"http://arxiv.org/abs/2311.08300v1"}
{"created":"2023-11-14 16:44:16","title":"VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing","abstract":"Reflective listening is a fundamental skill that counselors must acquire to achieve proficiency in motivational interviewing (MI). It involves responding in a manner that acknowledges and explores the meaning of what the client has expressed in the conversation. In this work, we introduce the task of counseling response rewriting, which transforms non-reflective statements into reflective responses. We introduce VERVE, a template-based rewriting system with paraphrase-augmented training and adaptive template updating. VERVE first creates a template by identifying and filtering out tokens that are not relevant to reflections and constructs a reflective response using the template. Paraphrase-augmented training allows the model to learn less-strict fillings of masked spans, and adaptive template updating helps discover effective templates for rewriting without significantly removing the original content. Using both automatic and human evaluations, we compare our method against text rewriting baselines and show that our framework is effective in turning non-reflective statements into more reflective responses while achieving a good content preservation-reflection style trade-off.","sentences":["Reflective listening is a fundamental skill that counselors must acquire to achieve proficiency in motivational interviewing (MI).","It involves responding in a manner that acknowledges and explores the meaning of what the client has expressed in the conversation.","In this work, we introduce the task of counseling response rewriting, which transforms non-reflective statements into reflective responses.","We introduce VERVE, a template-based rewriting system with paraphrase-augmented training and adaptive template updating.","VERVE first creates a template by identifying and filtering out tokens that are not relevant to reflections and constructs a reflective response using the template.","Paraphrase-augmented training allows the model to learn less-strict fillings of masked spans, and adaptive template updating helps discover effective templates for rewriting without significantly removing the original content.","Using both automatic and human evaluations, we compare our method against text rewriting baselines and show that our framework is effective in turning non-reflective statements into more reflective responses while achieving a good content preservation-reflection style trade-off."],"url":"http://arxiv.org/abs/2311.08299v1"}
{"created":"2023-11-14 16:43:29","title":"A Survey of Language Model Confidence Estimation and Calibration","abstract":"Language models (LMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, the reliability of their output is concerning and questionable regarding the demand for AI safety. Assessing the confidence of LM predictions and calibrating them across different tasks with the aim to align LM confidence with accuracy can help mitigate risks and enable LMs to make better decisions. There have been various works in this respect, but there has been no comprehensive overview of this important research area. The present survey aims to bridge this gap. In particular, we discuss methods and techniques for LM confidence estimation and calibration, encompassing different LMs and various tasks. We further outline the challenges of estimating the confidence for large language models and we suggest some promising directions for future work.","sentences":["Language models (LMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains.","Despite their impressive performance, the reliability of their output is concerning and questionable regarding the demand for AI safety.","Assessing the confidence of LM predictions and calibrating them across different tasks with the aim to align LM confidence with accuracy can help mitigate risks and enable LMs to make better decisions.","There have been various works in this respect, but there has been no comprehensive overview of this important research area.","The present survey aims to bridge this gap.","In particular, we discuss methods and techniques for LM confidence estimation and calibration, encompassing different LMs and various tasks.","We further outline the challenges of estimating the confidence for large language models and we suggest some promising directions for future work."],"url":"http://arxiv.org/abs/2311.08298v1"}
{"created":"2023-11-14 16:37:28","title":"On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling","abstract":"On-policy reinforcement learning (RL) algorithms perform policy updates using i.i.d. trajectories collected by the current policy. However, after observing only a finite number of trajectories, on-policy sampling may produce data that fails to match the expected on-policy data distribution. This sampling error leads to noisy updates and data inefficient on-policy learning. Recent work in the policy evaluation setting has shown that non-i.i.d., off-policy sampling can produce data with lower sampling error than on-policy sampling can produce. Motivated by this observation, we introduce an adaptive, off-policy sampling method to improve the data efficiency of on-policy policy gradient algorithms. Our method, Proximal Robust On-Policy Sampling (PROPS), reduces sampling error by collecting data with a behavior policy that increases the probability of sampling actions that are under-sampled with respect to the current policy. Rather than discarding data from old policies -- as is commonly done in on-policy algorithms -- PROPS uses data collection to adjust the distribution of previously collected data to be approximately on-policy. We empirically evaluate PROPS on both continuous-action MuJoCo benchmark tasks as well as discrete-action tasks and demonstrate that (1) PROPS decreases sampling error throughout training and (2) improves the data efficiency of on-policy policy gradient algorithms. Our work improves the RL community's understanding of a nuance in the on-policy vs off-policy dichotomy: on-policy learning requires on-policy data, not on-policy sampling.","sentences":["On-policy reinforcement learning (RL) algorithms perform policy updates using i.i.d. trajectories collected by the current policy.","However, after observing only a finite number of trajectories, on-policy sampling may produce data that fails to match the expected on-policy data distribution.","This sampling error leads to noisy updates and data inefficient on-policy learning.","Recent work in the policy evaluation setting has shown that non-i.i.d., off-policy sampling can produce data with lower sampling error than on-policy sampling can produce.","Motivated by this observation, we introduce an adaptive, off-policy sampling method to improve the data efficiency of on-policy policy gradient algorithms.","Our method, Proximal Robust On-Policy Sampling (PROPS), reduces sampling error by collecting data with a behavior policy that increases the probability of sampling actions that are under-sampled with respect to the current policy.","Rather than discarding data from old policies -- as is commonly done in on-policy algorithms -- PROPS uses data collection to adjust the distribution of previously collected data to be approximately on-policy.","We empirically evaluate PROPS on both continuous-action MuJoCo benchmark tasks as well as discrete-action tasks and demonstrate that (1) PROPS decreases sampling error throughout training and (2) improves the data efficiency of on-policy policy gradient algorithms.","Our work improves the RL community's understanding of a nuance in the on-policy vs off-policy dichotomy: on-policy learning requires on-policy data, not on-policy sampling."],"url":"http://arxiv.org/abs/2311.08290v1"}
{"created":"2023-11-14 16:30:36","title":"How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions","abstract":"While recent advancements in large language models (LLMs) bring us closer to achieving artificial general intelligence, the question persists: Do LLMs truly understand language, or do they merely mimic comprehension through pattern recognition? This study seeks to explore this question through the lens of syntax, a crucial component of sentence comprehension. Adopting a natural language question-answering (Q&A) scheme, we craft questions targeting nine syntactic knowledge points that are most closely related to sentence comprehension. Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points. In particular, questions involving prepositional phrase attachment pose the greatest challenge, whereas those concerning adjectival modifier and indirect object are relatively easier for LLMs to handle. Furthermore, a case study on the training dynamics of the LLMs reveals that the majority of syntactic knowledge is learned during the initial stages of training, hinting that simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLMs.","sentences":["While recent advancements in large language models (LLMs) bring us closer to achieving artificial general intelligence, the question persists: Do LLMs truly understand language, or do they merely mimic comprehension through pattern recognition?","This study seeks to explore this question through the lens of syntax, a crucial component of sentence comprehension.","Adopting a natural language question-answering (Q&A) scheme, we craft questions targeting nine syntactic knowledge points that are most closely related to sentence comprehension.","Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points.","In particular, questions involving prepositional phrase attachment pose the greatest challenge, whereas those concerning adjectival modifier and indirect object are relatively easier for LLMs to handle.","Furthermore, a case study on the training dynamics of the LLMs reveals that the majority of syntactic knowledge is learned during the initial stages of training, hinting that simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLMs."],"url":"http://arxiv.org/abs/2311.08287v1"}
{"created":"2023-11-14 16:27:33","title":"Level Set KSVD","abstract":"We present a new algorithm for image segmentation - Level-set KSVD. Level-set KSVD merges the methods of sparse dictionary learning for feature extraction and variational level-set method for image segmentation. Specifically, we use a generalization of the Chan-Vese functional with features learned by KSVD. The motivation for this model is agriculture based. Aerial images are taken in order to detect the spread of fungi in various crops. Our model is tested on such images of cotton fields. The results are compared to other methods.","sentences":["We present a new algorithm for image segmentation - Level-set KSVD.","Level-set KSVD merges the methods of sparse dictionary learning for feature extraction and variational level-set method for image segmentation.","Specifically, we use a generalization of the Chan-Vese functional with features learned by KSVD.","The motivation for this model is agriculture based.","Aerial images are taken in order to detect the spread of fungi in various crops.","Our model is tested on such images of cotton fields.","The results are compared to other methods."],"url":"http://arxiv.org/abs/2311.08284v1"}
{"created":"2023-11-14 16:27:16","title":"Noise-Resilient Group Testing with Order-Optimal Tests and Fast-and-Reliable Decoding","abstract":"Group testing (GT) is the Boolean counterpart of compressed sensing and the marketplace of new ideas for related problems such as cognitive radio and heavy hitter. A GT scheme is considered good if it is nonadaptive, uses $O(k \\log n)$ tests, resists noise, can be decoded in $O(k \\operatorname{poly}(\\log n))$ time, and makes nearly no mistakes. In this paper, we propose \"Gacha GT\", an elementary, self-contained, and unified randomized scheme that, for the first time, satisfies all criteria for a fairly large region of parameters, namely when $\\log k < \\log(n)^{1-1/O(1)}$. Outside this parameter region, Gacha can be specialized to outperform the state-of-the-art partial-recovery GTs, exact-recovery GTs, and worst-case GTs.   The new idea that runs through this paper, using an analogy, is to ask every person to break her $9$-digit \"phone number\" into three $3$-digit numbers $x$, $y$, and $z$ and write $(b, x)$, $(b, y)$, and $(b, z)$ on three pieces of sticky notes, where $b$ is her \"birthday\". This way, one can sort the sticky notes by birthday to reassemble the phone numbers. This birthday--number code and other coded constructions can be stacked like a multipartite graph pyramid. Gacha's encoder will synthesize the test results from the bottom up; and Gacha's decoder will reassemble the phone numbers from the top down.","sentences":["Group testing (GT) is the Boolean counterpart of compressed sensing and the marketplace of new ideas for related problems such as cognitive radio and heavy hitter.","A GT scheme is considered good if it is nonadaptive, uses $O(k \\log n)$ tests, resists noise, can be decoded in $O(k \\operatorname{poly}(\\log n))$ time, and makes nearly no mistakes.","In this paper, we propose \"Gacha GT\", an elementary, self-contained, and unified randomized scheme that, for the first time, satisfies all criteria for a fairly large region of parameters, namely when $\\log k < \\log(n)^{1-1/O(1)}$. Outside this parameter region, Gacha can be specialized to outperform the state-of-the-art partial-recovery GTs, exact-recovery GTs, and worst-case GTs.   ","The new idea that runs through this paper, using an analogy, is to ask every person to break her $9$-digit \"phone number\" into three $3$-digit numbers $x$, $y$, and $z$ and write $(b, x)$, $(b, y)$, and $(b, z)$ on three pieces of sticky notes, where $b$ is her \"birthday\".","This way, one can sort the sticky notes by birthday to reassemble the phone numbers.","This birthday--number code and other coded constructions can be stacked like a multipartite graph pyramid.","Gacha's encoder will synthesize the test results from the bottom up; and Gacha's decoder will reassemble the phone numbers from the top down."],"url":"http://arxiv.org/abs/2311.08283v1"}
{"created":"2023-11-14 16:19:29","title":"ARTEMIS: Using GANs with Multiple Discriminators to Generate Art","abstract":"We propose a novel method for generating abstract art. First an autoencoder is trained to encode and decode the style representations of images, which are extracted from source images with a pretrained VGG network. Then, the decoder component of the autoencoder is extracted and used as a generator in a GAN. The generator works with an ensemble of discriminators. Each discriminator takes different style representations of the same images, and the generator is trained to create images that create convincing style representations in order to deceive all of the generators. The generator is also trained to maximize a diversity term. The resulting images had a surreal, geometric quality. We call our approach ARTEMIS (ARTistic Encoder- Multi- Discriminators Including Self-Attention), as it uses the self-attention layers and an encoder-decoder architecture.","sentences":["We propose a novel method for generating abstract art.","First an autoencoder is trained to encode and decode the style representations of images, which are extracted from source images with a pretrained VGG network.","Then, the decoder component of the autoencoder is extracted and used as a generator in a GAN.","The generator works with an ensemble of discriminators.","Each discriminator takes different style representations of the same images, and the generator is trained to create images that create convincing style representations in order to deceive all of the generators.","The generator is also trained to maximize a diversity term.","The resulting images had a surreal, geometric quality.","We call our approach ARTEMIS (ARTistic Encoder- Multi- Discriminators Including Self-Attention), as it uses the self-attention layers and an encoder-decoder architecture."],"url":"http://arxiv.org/abs/2311.08278v1"}
{"created":"2023-11-14 16:11:35","title":"Laccolith: Hypervisor-Based Adversary Emulation with Anti-Detection","abstract":"Advanced Persistent Threats (APTs) represent the most threatening form of attack nowadays since they can stay undetected for a long time. Adversary emulation is a proactive approach for preparing against these attacks. However, adversary emulation tools lack the anti-detection abilities of APTs. We introduce Laccolith, a hypervisor-based solution for adversary emulation with anti-detection to fill this gap. We also present an experimental study to compare Laccolith with MITRE CALDERA, a state-of-the-art solution for adversary emulation, against five popular anti-virus products. We found that CALDERA cannot evade detection, limiting the realism of emulated attacks, even when combined with a state-of-the-art anti-detection framework. Our experiments show that Laccolith can hide its activities from all the tested anti-virus products, thus making it suitable for realistic emulations.","sentences":["Advanced Persistent Threats (APTs) represent the most threatening form of attack nowadays since they can stay undetected for a long time.","Adversary emulation is a proactive approach for preparing against these attacks.","However, adversary emulation tools lack the anti-detection abilities of APTs.","We introduce Laccolith, a hypervisor-based solution for adversary emulation with anti-detection to fill this gap.","We also present an experimental study to compare Laccolith with MITRE CALDERA, a state-of-the-art solution for adversary emulation, against five popular anti-virus products.","We found that CALDERA cannot evade detection, limiting the realism of emulated attacks, even when combined with a state-of-the-art anti-detection framework.","Our experiments show that Laccolith can hide its activities from all the tested anti-virus products, thus making it suitable for realistic emulations."],"url":"http://arxiv.org/abs/2311.08274v1"}
{"created":"2023-11-14 16:11:23","title":"Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks","abstract":"Recent work has proposed explicitly inducing language-wise modularity in multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a means of better guiding cross-lingual sharing. In this work, we investigate (1) the degree to which language-wise modularity naturally arises within models with no special modularity interventions, and (2) how cross-lingual sharing and interference differ between such models and those with explicit SFT-guided subnetwork modularity. To quantify language specialization and cross-lingual interaction, we use a Training Data Attribution method that estimates the degree to which a model's predictions are influenced by in-language or cross-language training examples. Our results show that language-specialized subnetworks do naturally arise, and that SFT, rather than always increasing modularity, can decrease language specialization of subnetworks in favor of more cross-lingual sharing.","sentences":["Recent work has proposed explicitly inducing language-wise modularity in multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a means of better guiding cross-lingual sharing.","In this work, we investigate (1) the degree to which language-wise modularity naturally arises within models with no special modularity interventions, and (2) how cross-lingual sharing and interference differ between such models and those with explicit SFT-guided subnetwork modularity.","To quantify language specialization and cross-lingual interaction, we use a Training Data Attribution method that estimates the degree to which a model's predictions are influenced by in-language or cross-language training examples.","Our results show that language-specialized subnetworks do naturally arise, and that SFT, rather than always increasing modularity, can decrease language specialization of subnetworks in favor of more cross-lingual sharing."],"url":"http://arxiv.org/abs/2311.08273v1"}
{"created":"2023-11-14 16:07:16","title":"Mixed Attention Network for Cross-domain Sequential Recommendation","abstract":"In modern recommender systems, sequential recommendation leverages chronological user behaviors to make effective next-item suggestions, which suffers from data sparsity issues, especially for new users. One promising line of work is the cross-domain recommendation, which trains models with data across multiple domains to improve the performance in data-scarce domains. Recent proposed cross-domain sequential recommendation models such as PiNet and DASL have a common drawback relying heavily on overlapped users in different domains, which limits their usage in practical recommender systems. In this paper, we propose a Mixed Attention Network (MAN) with local and global attention modules to extract the domain-specific and cross-domain information. Firstly, we propose a local/global encoding layer to capture the domain-specific/cross-domain sequential pattern. Then we propose a mixed attention layer with item similarity attention, sequence-fusion attention, and group-prototype attention to capture the local/global item similarity, fuse the local/global item sequence, and extract the user groups across different domains, respectively. Finally, we propose a local/global prediction layer to further evolve and combine the domain-specific and cross-domain interests. Experimental results on two real-world datasets (each with two domains) demonstrate the superiority of our proposed model. Further study also illustrates that our proposed method and components are model-agnostic and effective, respectively. The code and data are available at https://github.com/Guanyu-Lin/MAN.","sentences":["In modern recommender systems, sequential recommendation leverages chronological user behaviors to make effective next-item suggestions, which suffers from data sparsity issues, especially for new users.","One promising line of work is the cross-domain recommendation, which trains models with data across multiple domains to improve the performance in data-scarce domains.","Recent proposed cross-domain sequential recommendation models such as PiNet and DASL have a common drawback relying heavily on overlapped users in different domains, which limits their usage in practical recommender systems.","In this paper, we propose a Mixed Attention Network (MAN) with local and global attention modules to extract the domain-specific and cross-domain information.","Firstly, we propose a local/global encoding layer to capture the domain-specific/cross-domain sequential pattern.","Then we propose a mixed attention layer with item similarity attention, sequence-fusion attention, and group-prototype attention to capture the local/global item similarity, fuse the local/global item sequence, and extract the user groups across different domains, respectively.","Finally, we propose a local/global prediction layer to further evolve and combine the domain-specific and cross-domain interests.","Experimental results on two real-world datasets (each with two domains) demonstrate the superiority of our proposed model.","Further study also illustrates that our proposed method and components are model-agnostic and effective, respectively.","The code and data are available at https://github.com/Guanyu-Lin/MAN."],"url":"http://arxiv.org/abs/2311.08272v1"}
{"created":"2023-11-14 16:06:11","title":"Mobility-Induced Graph Learning for WiFi Positioning","abstract":"A smartphone-based user mobility tracking could be effective in finding his/her location, while the unpredictable error therein due to low specification of built-in inertial measurement units (IMUs) rejects its standalone usage but demands the integration to another positioning technique like WiFi positioning. This paper aims to propose a novel integration technique using a graph neural network called Mobility-INduced Graph LEarning (MINGLE), which is designed based on two types of graphs made by capturing different user mobility features. Specifically, considering sequential measurement points (MPs) as nodes, a user's regular mobility pattern allows us to connect neighbor MPs as edges, called time-driven mobility graph (TMG). Second, a user's relatively straight transition at a constant pace when moving from one position to another can be captured by connecting the nodes on each path, called a direction-driven mobility graph (DMG). Then, we can design graph convolution network (GCN)-based cross-graph learning, where two different GCN models for TMG and DMG are jointly trained by feeding different input features created by WiFi RTTs yet sharing their weights. Besides, the loss function includes a mobility regularization term such that the differences between adjacent location estimates should be less variant due to the user's stable moving pace. Noting that the regularization term does not require ground-truth location, MINGLE can be designed under semi- and self-supervised learning frameworks. The proposed MINGLE's effectiveness is extensively verified through field experiments, showing a better positioning accuracy than benchmarks, say root mean square errors (RMSEs) being 1.398 (m) and 1.073 (m) for self- and semi-supervised learning cases, respectively.","sentences":["A smartphone-based user mobility tracking could be effective in finding his/her location, while the unpredictable error therein due to low specification of built-in inertial measurement units (IMUs) rejects its standalone usage but demands the integration to another positioning technique like WiFi positioning.","This paper aims to propose a novel integration technique using a graph neural network called Mobility-INduced Graph LEarning (MINGLE), which is designed based on two types of graphs made by capturing different user mobility features.","Specifically, considering sequential measurement points (MPs) as nodes, a user's regular mobility pattern allows us to connect neighbor MPs as edges, called time-driven mobility graph (TMG).","Second, a user's relatively straight transition at a constant pace when moving from one position to another can be captured by connecting the nodes on each path, called a direction-driven mobility graph (DMG).","Then, we can design graph convolution network (GCN)-based cross-graph learning, where two different GCN models for TMG and DMG are jointly trained by feeding different input features created by WiFi RTTs yet sharing their weights.","Besides, the loss function includes a mobility regularization term such that the differences between adjacent location estimates should be less variant due to the user's stable moving pace.","Noting that the regularization term does not require ground-truth location, MINGLE can be designed under semi- and self-supervised learning frameworks.","The proposed MINGLE's effectiveness is extensively verified through field experiments, showing a better positioning accuracy than benchmarks, say root mean square errors (RMSEs) being 1.398 (m) and 1.073 (m) for self- and semi-supervised learning cases, respectively."],"url":"http://arxiv.org/abs/2311.08271v1"}
{"created":"2023-11-14 16:02:16","title":"A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily","abstract":"Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on another white-box model, compromising generalization or jailbreak efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we offer detailed analysis and discussion from the perspective of prompt execution priority on the failure of LLMs' defense. We hope that our research can catalyze both the academic community and LLMs vendors towards the provision of safer and more regulated Large Language Models.","sentences":["Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses.","However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful content.","Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them.","Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on another white-box model, compromising generalization or jailbreak efficiency.","In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting.","Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts.","Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines.","Our study also reveals the inadequacy of current defense methods in safeguarding LLMs.","Finally, we offer detailed analysis and discussion from the perspective of prompt execution priority on the failure of LLMs' defense.","We hope that our research can catalyze both the academic community and LLMs vendors towards the provision of safer and more regulated Large Language Models."],"url":"http://arxiv.org/abs/2311.08268v1"}
{"created":"2023-11-14 16:00:29","title":"On The Relationship Between Universal Adversarial Attacks And Sparse Representations","abstract":"The prominent success of neural networks, mainly in computer vision tasks, is increasingly shadowed by their sensitivity to small, barely perceivable adversarial perturbations in image input.   In this work, we aim at explaining this vulnerability through the framework of sparsity.   We show the connection between adversarial attacks and sparse representations, with a focus on explaining the universality and transferability of adversarial examples in neural networks.   To this end, we show that sparse coding algorithms, and the neural network-based learned iterative shrinkage thresholding algorithm (LISTA) among them, suffer from this sensitivity, and that common attacks on neural networks can be expressed as attacks on the sparse representation of the input image. The phenomenon that we observe holds true also when the network is agnostic to the sparse representation and dictionary, and thus can provide a possible explanation for the universality and transferability of adversarial attacks.   The code is available at https://github.com/danawr/adversarial_attacks_and_sparse_representations.","sentences":["The prominent success of neural networks, mainly in computer vision tasks, is increasingly shadowed by their sensitivity to small, barely perceivable adversarial perturbations in image input.   ","In this work, we aim at explaining this vulnerability through the framework of sparsity.   ","We show the connection between adversarial attacks and sparse representations, with a focus on explaining the universality and transferability of adversarial examples in neural networks.   ","To this end, we show that sparse coding algorithms, and the neural network-based learned iterative shrinkage thresholding algorithm (LISTA) among them, suffer from this sensitivity, and that common attacks on neural networks can be expressed as attacks on the sparse representation of the input image.","The phenomenon that we observe holds true also when the network is agnostic to the sparse representation and dictionary, and thus can provide a possible explanation for the universality and transferability of adversarial attacks.   ","The code is available at https://github.com/danawr/adversarial_attacks_and_sparse_representations."],"url":"http://arxiv.org/abs/2311.08265v1"}
{"created":"2023-11-14 15:56:18","title":"Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster","abstract":"In this work, we propose FastCoT, a model-agnostic framework based on parallel decoding without any further training of an auxiliary model or modification to the LLM itself. FastCoT uses a size-varying context window whose size changes with position to conduct parallel decoding and auto-regressive decoding simultaneously, thus fully utilizing GPU computation resources. In FastCoT, the parallel decoding part provides the LLM with a quick glance of the future composed of approximate tokens, which could lead to faster answers compared to regular autoregressive decoding used by causal transformers. We also provide an implementation of parallel decoding within LLM, which supports KV-cache generation and batch processing. Through extensive experiments, we demonstrate that FastCoT saves inference time by nearly 20% with only a negligible performance drop compared to the regular approach. Additionally, we show that the context window size exhibits considerable robustness for different tasks.","sentences":["In this work, we propose FastCoT, a model-agnostic framework based on parallel decoding without any further training of an auxiliary model or modification to the LLM itself.","FastCoT uses a size-varying context window whose size changes with position to conduct parallel decoding and auto-regressive decoding simultaneously, thus fully utilizing GPU computation resources.","In FastCoT, the parallel decoding part provides the LLM with a quick glance of the future composed of approximate tokens, which could lead to faster answers compared to regular autoregressive decoding used by causal transformers.","We also provide an implementation of parallel decoding within LLM, which supports KV-cache generation and batch processing.","Through extensive experiments, we demonstrate that FastCoT saves inference time by nearly 20% with only a negligible performance drop compared to the regular approach.","Additionally, we show that the context window size exhibits considerable robustness for different tasks."],"url":"http://arxiv.org/abs/2311.08263v1"}
{"created":"2023-11-14 15:54:19","title":"Unprecedented reach and recruitment paths for hate and extremism","abstract":"Analyzing a unique real-time dataset from across 26 social media platforms, we show why the hate-extremism ecosystem now has unprecedented reach and recruitment paths online; why it is now able to exert instant and massive global mainstream influence, e.g. following the October 7 Hamas attack; why it will become increasingly robust in 2024 and beyond; why recent E.U. laws fall short because the effect of many smaller, lesser-known platforms outstrips larger ones like Twitter; and why law enforcement should expect increasingly hard-to-understand paths ahead of offline mass attacks. This new picture of online hate and extremism challenges current notions of a niche activity at the 'fringe' of the Internet driven by specific news sources. But it also suggests a new opportunity for system-wide control akin to adaptive vs. extinction treatments for cancer.","sentences":["Analyzing a unique real-time dataset from across 26 social media platforms, we show why the hate-extremism ecosystem now has unprecedented reach and recruitment paths online; why it is now able to exert instant and massive global mainstream influence, e.g. following the October 7 Hamas attack; why it will become increasingly robust in 2024 and beyond; why recent E.U. laws fall short because the effect of many smaller, lesser-known platforms outstrips larger ones like Twitter; and why law enforcement should expect increasingly hard-to-understand paths ahead of offline mass attacks.","This new picture of online hate and extremism challenges current notions of a niche activity at the 'fringe' of the Internet driven by specific news sources.","But it also suggests a new opportunity for system-wide control akin to adaptive vs. extinction treatments for cancer."],"url":"http://arxiv.org/abs/2311.08258v1"}
{"created":"2023-11-14 15:51:01","title":"Neural Dynamics of Delayed Feedback in Robot Teleoperation: Insights from fNIRS Analysis","abstract":"As robot teleoperation increasingly becomes integral in executing tasks in distant, hazardous, or inaccessible environments, the challenge of operational delays remains a significant obstacle. These delays are inherent in signal transmission and processing and can adversely affect the operators performance, particularly in tasks requiring precision and timeliness. While current research has made strides in mitigating these delays through advanced control strategies and training methods, a crucial gap persists in understanding the neurofunctional impacts of these delays and the efficacy of countermeasures from a cognitive perspective. Our study narrows this gap by leveraging functional Near-Infrared Spectroscopy (fNIRS) to examine the neurofunctional implications of simulated haptic feedback on cognitive activity and motor coordination under delayed conditions. In a human-subject experiment (N=41), we manipulated sensory feedback to observe its influences on various brain regions of interest (ROIs) response during teleoperation tasks. The fNIRS data provided a detailed assessment of cerebral activity, particularly in ROIs implicated in time perception and the execution of precise movements. Our results reveal that certain conditions, which provided immediate simulated haptic feedback, significantly optimized neural functions related to time perception and motor coordination, and improved motor performance. These findings provide empirical evidence about the neurofunctional basis of the enhanced motor performance with simulated synthetic force feedback in the presence of teleoperation delays.","sentences":["As robot teleoperation increasingly becomes integral in executing tasks in distant, hazardous, or inaccessible environments, the challenge of operational delays remains a significant obstacle.","These delays are inherent in signal transmission and processing and can adversely affect the operators performance, particularly in tasks requiring precision and timeliness.","While current research has made strides in mitigating these delays through advanced control strategies and training methods, a crucial gap persists in understanding the neurofunctional impacts of these delays and the efficacy of countermeasures from a cognitive perspective.","Our study narrows this gap by leveraging functional Near-Infrared Spectroscopy (fNIRS) to examine the neurofunctional implications of simulated haptic feedback on cognitive activity and motor coordination under delayed conditions.","In a human-subject experiment (N=41), we manipulated sensory feedback to observe its influences on various brain regions of interest (ROIs) response during teleoperation tasks.","The fNIRS data provided a detailed assessment of cerebral activity, particularly in ROIs implicated in time perception and the execution of precise movements.","Our results reveal that certain conditions, which provided immediate simulated haptic feedback, significantly optimized neural functions related to time perception and motor coordination, and improved motor performance.","These findings provide empirical evidence about the neurofunctional basis of the enhanced motor performance with simulated synthetic force feedback in the presence of teleoperation delays."],"url":"http://arxiv.org/abs/2311.08255v1"}
{"created":"2023-11-14 15:43:47","title":"REST: Retrieval-Based Speculative Decoding","abstract":"We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm designed to speed up language model generation. The key insight driving the development of REST is the observation that the process of text generation often includes certain common phases and patterns. Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens. This method draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context. Its plug-and-play nature allows for seamless integration and acceleration of any language models, all without necessitating additional training. When benchmarked on 7B and 13B language models in a single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on code or text generation. The code of REST is available at https://github.com/FasterDecoding/REST.","sentences":["We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm designed to speed up language model generation.","The key insight driving the development of REST is the observation that the process of text generation often includes certain common phases and patterns.","Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens.","This method draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context.","Its plug-and-play nature allows for seamless integration and acceleration of any language models, all without necessitating additional training.","When benchmarked on 7B and 13B language models in a single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on code or text generation.","The code of REST is available at https://github.com/FasterDecoding/REST."],"url":"http://arxiv.org/abs/2311.08252v1"}
{"created":"2023-11-14 15:37:19","title":"On Using Distribution-Based Compositionality Assessment to Evaluate Compositional Generalisation in Machine Translation","abstract":"Compositional generalisation (CG), in NLP and in machine learning more generally, has been assessed mostly using artificial datasets. It is important to develop benchmarks to assess CG also in real-world natural language tasks in order to understand the abilities and limitations of systems deployed in the wild. To this end, our GenBench Collaborative Benchmarking Task submission utilises the distribution-based compositionality assessment (DBCA) framework to split the Europarl translation corpus into a training and a test set in such a way that the test set requires compositional generalisation capacity. Specifically, the training and test sets have divergent distributions of dependency relations, testing NMT systems' capability of translating dependencies that they have not been trained on. This is a fully-automated procedure to create natural language compositionality benchmarks, making it simple and inexpensive to apply it further to other datasets and languages. The code and data for the experiments is available at https://github.com/aalto-speech/dbca.","sentences":["Compositional generalisation (CG), in NLP and in machine learning more generally, has been assessed mostly using artificial datasets.","It is important to develop benchmarks to assess CG also in real-world natural language tasks in order to understand the abilities and limitations of systems deployed in the wild.","To this end, our GenBench Collaborative Benchmarking Task submission utilises the distribution-based compositionality assessment (DBCA) framework to split the Europarl translation corpus into a training and a test set in such a way that the test set requires compositional generalisation capacity.","Specifically, the training and test sets have divergent distributions of dependency relations, testing NMT systems' capability of translating dependencies that they have not been trained on.","This is a fully-automated procedure to create natural language compositionality benchmarks, making it simple and inexpensive to apply it further to other datasets and languages.","The code and data for the experiments is available at https://github.com/aalto-speech/dbca."],"url":"http://arxiv.org/abs/2311.08249v1"}
{"created":"2023-11-14 15:30:17","title":"TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity Recognition","abstract":"Recent achievements in language models have showcased their extraordinary capabilities in bridging visual information with semantic language understanding. This leads us to a novel question: can language models connect textual semantics with IoT sensory signals to perform recognition tasks, e.g., Human Activity Recognition (HAR)? If so, an intelligent HAR system with human-like cognition can be built, capable of adapting to new environments and unseen categories. This paper explores its feasibility with an innovative approach, IoT-sEnsors-language alignmEnt pre-Training (TENT), which jointly aligns textual embeddings with IoT sensor signals, including camera video, LiDAR, and mmWave. Through the IoT-language contrastive learning, we derive a unified semantic feature space that aligns multi-modal features with language embeddings, so that the IoT data corresponds to specific words that describe the IoT data. To enhance the connection between textual categories and their IoT data, we propose supplementary descriptions and learnable prompts that bring more semantic information into the joint feature space. TENT can not only recognize actions that have been seen but also ``guess'' the unseen action by the closest textual words from the feature space. We demonstrate TENT achieves state-of-the-art performance on zero-shot HAR tasks using different modalities, improving the best vision-language models by over 12%.","sentences":["Recent achievements in language models have showcased their extraordinary capabilities in bridging visual information with semantic language understanding.","This leads us to a novel question: can language models connect textual semantics with IoT sensory signals to perform recognition tasks, e.g., Human Activity Recognition (HAR)?","If so, an intelligent HAR system with human-like cognition can be built, capable of adapting to new environments and unseen categories.","This paper explores its feasibility with an innovative approach, IoT-sEnsors-language alignmEnt pre-Training (TENT), which jointly aligns textual embeddings with IoT sensor signals, including camera video, LiDAR, and mmWave.","Through the IoT-language contrastive learning, we derive a unified semantic feature space that aligns multi-modal features with language embeddings, so that the IoT data corresponds to specific words that describe the IoT data.","To enhance the connection between textual categories and their IoT data, we propose supplementary descriptions and learnable prompts that bring more semantic information into the joint feature space.","TENT can not only recognize actions that have been seen but also ``guess'' the unseen action by the closest textual words from the feature space.","We demonstrate TENT achieves state-of-the-art performance on zero-shot HAR tasks using different modalities, improving the best vision-language models by over 12%."],"url":"http://arxiv.org/abs/2311.08245v1"}
{"created":"2023-11-14 15:29:52","title":"Language and Sketching: An LLM-driven Interactive Multimodal Multitask Robot Navigation Framework","abstract":"The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding. However, a prominent gap persists: in Human-Robot Interaction (HRI), the procedure of communicating commands to robots demands intricate mathematical formulations. Furthermore, the transition between tasks does not quite possess the intuitive control and user-centric interactivity that one would desire. In this work, we propose an LLM-driven interactive multimodal multitask robot navigation framework, termed LIM2N, to solve the above new challenge in the navigation field. We achieve this by first introducing a multimodal interaction framework where language and hand-drawn inputs can serve as navigation constraints and control objectives. Next, a reinforcement learning agent is built to handle multiple tasks with the received information. Crucially, LIM2N creates smooth cooperation among the reasoning of multimodal input, multitask planning, and adaptation and processing of the intelligent sensing modules in the complicated system. Extensive experiments are conducted in both simulation and the real world demonstrating that LIM2N has superior user needs understanding, alongside an enhanced interactive experience.","sentences":["The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding.","However, a prominent gap persists: in Human-Robot Interaction (HRI), the procedure of communicating commands to robots demands intricate mathematical formulations.","Furthermore, the transition between tasks does not quite possess the intuitive control and user-centric interactivity that one would desire.","In this work, we propose an LLM-driven interactive multimodal multitask robot navigation framework, termed LIM2N, to solve the above new challenge in the navigation field.","We achieve this by first introducing a multimodal interaction framework where language and hand-drawn inputs can serve as navigation constraints and control objectives.","Next, a reinforcement learning agent is built to handle multiple tasks with the received information.","Crucially, LIM2N creates smooth cooperation among the reasoning of multimodal input, multitask planning, and adaptation and processing of the intelligent sensing modules in the complicated system.","Extensive experiments are conducted in both simulation and the real world demonstrating that LIM2N has superior user needs understanding, alongside an enhanced interactive experience."],"url":"http://arxiv.org/abs/2311.08244v1"}
{"created":"2023-11-14 15:21:49","title":"Investigating the Encoding of Words in BERT's Neurons using Feature Textualization","abstract":"Pretrained language models (PLMs) form the basis of most state-of-the-art NLP technologies. Nevertheless, they are essentially black boxes: Humans do not have a clear understanding of what knowledge is encoded in different parts of the models, especially in individual neurons. The situation is different in computer vision, where feature visualization provides a decompositional interpretability technique for neurons of vision models. Activation maximization is used to synthesize inherently interpretable visual representations of the information encoded in individual neurons. Our work is inspired by this but presents a cautionary tale on the interpretability of single neurons, based on the first large-scale attempt to adapt activation maximization to NLP, and, more specifically, large PLMs. We propose feature textualization, a technique to produce dense representations of neurons in the PLM word embedding space. We apply feature textualization to the BERT model (Devlin et al., 2019) to investigate whether the knowledge encoded in individual neurons can be interpreted and symbolized. We find that the produced representations can provide insights about the knowledge encoded in individual neurons, but that individual neurons do not represent clearcut symbolic units of language such as words. Additionally, we use feature textualization to investigate how many neurons are needed to encode words in BERT.","sentences":["Pretrained language models (PLMs) form the basis of most state-of-the-art NLP technologies.","Nevertheless, they are essentially black boxes: Humans do not have a clear understanding of what knowledge is encoded in different parts of the models, especially in individual neurons.","The situation is different in computer vision, where feature visualization provides a decompositional interpretability technique for neurons of vision models.","Activation maximization is used to synthesize inherently interpretable visual representations of the information encoded in individual neurons.","Our work is inspired by this but presents a cautionary tale on the interpretability of single neurons, based on the first large-scale attempt to adapt activation maximization to NLP, and, more specifically, large PLMs.","We propose feature textualization, a technique to produce dense representations of neurons in the PLM word embedding space.","We apply feature textualization to the BERT model (Devlin et al., 2019) to investigate whether the knowledge encoded in individual neurons can be interpreted and symbolized.","We find that the produced representations can provide insights about the knowledge encoded in individual neurons, but that individual neurons do not represent clearcut symbolic units of language such as words.","Additionally, we use feature textualization to investigate how many neurons are needed to encode words in BERT."],"url":"http://arxiv.org/abs/2311.08240v1"}
{"created":"2023-11-14 15:18:54","title":"MeLo: Low-rank Adaptation is Better than Fine-tuning for Medical Image Diagnosis","abstract":"The common practice in developing computer-aided diagnosis (CAD) models based on transformer architectures usually involves fine-tuning from ImageNet pre-trained weights. However, with recent advances in large-scale pre-training and the practice of scaling laws, Vision Transformers (ViT) have become much larger and less accessible to medical imaging communities. Additionally, in real-world scenarios, the deployments of multiple CAD models can be troublesome due to problems such as limited storage space and time-consuming model switching. To address these challenges, we propose a new method MeLo (Medical image Low-rank adaptation), which enables the development of a single CAD model for multiple clinical tasks in a lightweight manner. It adopts low-rank adaptation instead of resource-demanding fine-tuning. By fixing the weight of ViT models and only adding small low-rank plug-ins, we achieve competitive results on various diagnosis tasks across different imaging modalities using only a few trainable parameters. Specifically, our proposed method achieves comparable performance to fully fine-tuned ViT models on four distinct medical imaging datasets using about 0.17% trainable parameters. Moreover, MeLo adds only about 0.5MB of storage space and allows for extremely fast model switching in deployment and inference. Our source code and pre-trained weights are available on our website (https://absterzhu.github.io/melo.github.io/).","sentences":["The common practice in developing computer-aided diagnosis (CAD) models based on transformer architectures usually involves fine-tuning from ImageNet pre-trained weights.","However, with recent advances in large-scale pre-training and the practice of scaling laws, Vision Transformers (ViT) have become much larger and less accessible to medical imaging communities.","Additionally, in real-world scenarios, the deployments of multiple CAD models can be troublesome due to problems such as limited storage space and time-consuming model switching.","To address these challenges, we propose a new method MeLo (Medical image Low-rank adaptation), which enables the development of a single CAD model for multiple clinical tasks in a lightweight manner.","It adopts low-rank adaptation instead of resource-demanding fine-tuning.","By fixing the weight of ViT models and only adding small low-rank plug-ins, we achieve competitive results on various diagnosis tasks across different imaging modalities using only a few trainable parameters.","Specifically, our proposed method achieves comparable performance to fully fine-tuned ViT models on four distinct medical imaging datasets using about 0.17% trainable parameters.","Moreover, MeLo adds only about 0.5MB of storage space and allows for extremely fast model switching in deployment and inference.","Our source code and pre-trained weights are available on our website (https://absterzhu.github.io/melo.github.io/)."],"url":"http://arxiv.org/abs/2311.08236v1"}
{"created":"2023-11-14 15:18:15","title":"Inference of Probabilistic Programs with Moment-Matching Gaussian Mixtures","abstract":"Computing the posterior distribution of a probabilistic program is a hard task for which no one-fit-for-all solution exists. We propose Gaussian Semantics, which approximates the exact probabilistic semantics of a bounded program by means of Gaussian mixtures. It is parametrized by a map that associates each program location with the moment order to be matched in the approximation. We provide two main contributions. The first is a universal approximation theorem stating that, under mild conditions, Gaussian Semantics can approximate the exact semantics arbitrarily closely. The second is an approximation that matches up to second-order moments analytically in face of the generally difficult problem of matching moments of Gaussian mixtures with arbitrary moment order. We test our second-order Gaussian approximation (SOGA) on a number of case studies from the literature. We show that it can provide accurate estimates in models not supported by other approximation methods or when exact symbolic techniques fail because of complex expressions or non-simplified integrals. On two notable classes of problems, namely collaborative filtering and programs involving mixtures of continuous and discrete distributions, we show that SOGA significantly outperforms alternative techniques in terms of accuracy and computational time.","sentences":["Computing the posterior distribution of a probabilistic program is a hard task for which no one-fit-for-all solution exists.","We propose Gaussian Semantics, which approximates the exact probabilistic semantics of a bounded program by means of Gaussian mixtures.","It is parametrized by a map that associates each program location with the moment order to be matched in the approximation.","We provide two main contributions.","The first is a universal approximation theorem stating that, under mild conditions, Gaussian Semantics can approximate the exact semantics arbitrarily closely.","The second is an approximation that matches up to second-order moments analytically in face of the generally difficult problem of matching moments of Gaussian mixtures with arbitrary moment order.","We test our second-order Gaussian approximation (SOGA) on a number of case studies from the literature.","We show that it can provide accurate estimates in models not supported by other approximation methods or when exact symbolic techniques fail because of complex expressions or non-simplified integrals.","On two notable classes of problems, namely collaborative filtering and programs involving mixtures of continuous and discrete distributions, we show that SOGA significantly outperforms alternative techniques in terms of accuracy and computational time."],"url":"http://arxiv.org/abs/2311.08235v1"}
{"created":"2023-11-14 15:17:38","title":"Centralized Intermediation in a Decentralized Web3 Economy: Value Accrual and Extraction","abstract":"The advent of Web3 has ushered in a new era of decentralized digital economy, promising a shift from centralized authority to distributed, peer-to-peer interactions. However, the underlying infrastructure of this decentralized ecosystem often relies on centralized cloud providers, creating a paradoxical concentration of value and power. This paper investigates the mechanics of value accrual and extraction within the Web3 ecosystem, focusing on the roles and revenues of centralized clouds. Through an analysis of publicly available material, we elucidate the financial implications of cloud services in purportedly decentralized contexts. We further explore the individual's perspective of value creation and accumulation, examining the interplay between user participation and centralized monetization strategies. Key findings indicate that while blockchain technology has the potential to significantly reduce infrastructure costs for financial services, the current Web3 landscape is marked by a substantial reliance on cloud providers for hosting, scalability, and performance.","sentences":["The advent of Web3 has ushered in a new era of decentralized digital economy, promising a shift from centralized authority to distributed, peer-to-peer interactions.","However, the underlying infrastructure of this decentralized ecosystem often relies on centralized cloud providers, creating a paradoxical concentration of value and power.","This paper investigates the mechanics of value accrual and extraction within the Web3 ecosystem, focusing on the roles and revenues of centralized clouds.","Through an analysis of publicly available material, we elucidate the financial implications of cloud services in purportedly decentralized contexts.","We further explore the individual's perspective of value creation and accumulation, examining the interplay between user participation and centralized monetization strategies.","Key findings indicate that while blockchain technology has the potential to significantly reduce infrastructure costs for financial services, the current Web3 landscape is marked by a substantial reliance on cloud providers for hosting, scalability, and performance."],"url":"http://arxiv.org/abs/2311.08234v1"}
{"created":"2023-11-14 15:08:14","title":"Counterfactual Explanation for Regression via Disentanglement in Latent Space","abstract":"Counterfactual Explanations (CEs) help address the question: How can the factors that influence the prediction of a predictive model be changed to achieve a more favorable outcome from a user's perspective? Thus, they bear the potential to guide the user's interaction with AI systems since they represent easy-to-understand explanations. To be applicable, CEs need to be realistic and actionable. In the literature, various methods have been proposed to generate CEs. However, the majority of research on CEs focuses on classification problems where questions like ``What should I do to get my rejected loan approved?\" are raised. In practice, answering questions like ``What should I do to increase my salary?\" are of a more regressive nature. In this paper, we introduce a novel method to generate CEs for a pre-trained regressor by first disentangling the label-relevant from the label-irrelevant dimensions in the latent space. CEs are then generated by combining the label-irrelevant dimensions and the predefined output. The intuition behind this approach is that the ideal counterfactual search should focus on the label-irrelevant characteristics of the input and suggest changes toward target-relevant characteristics. Searching in the latent space could help achieve this goal. We show that our method maintains the characteristics of the query sample during the counterfactual search. In various experiments, we demonstrate that the proposed method is competitive based on different quality measures on image and tabular datasets in regression problem settings. It efficiently returns results closer to the original data manifold compared to three state-of-the-art methods, which is essential for realistic high-dimensional machine learning applications. Our code will be made available as an open-source package upon the publication of this work.","sentences":["Counterfactual Explanations (CEs) help address the question: How can the factors that influence the prediction of a predictive model be changed to achieve a more favorable outcome from a user's perspective?","Thus, they bear the potential to guide the user's interaction with AI systems since they represent easy-to-understand explanations.","To be applicable, CEs need to be realistic and actionable.","In the literature, various methods have been proposed to generate CEs.","However, the majority of research on CEs focuses on classification problems where questions like ``What should I do to get my rejected loan approved?\" are raised.","In practice, answering questions like ``What should I do to increase my salary?\" are of a more regressive nature.","In this paper, we introduce a novel method to generate CEs for a pre-trained regressor by first disentangling the label-relevant from the label-irrelevant dimensions in the latent space.","CEs are then generated by combining the label-irrelevant dimensions and the predefined output.","The intuition behind this approach is that the ideal counterfactual search should focus on the label-irrelevant characteristics of the input and suggest changes toward target-relevant characteristics.","Searching in the latent space could help achieve this goal.","We show that our method maintains the characteristics of the query sample during the counterfactual search.","In various experiments, we demonstrate that the proposed method is competitive based on different quality measures on image and tabular datasets in regression problem settings.","It efficiently returns results closer to the original data manifold compared to three state-of-the-art methods, which is essential for realistic high-dimensional machine learning applications.","Our code will be made available as an open-source package upon the publication of this work."],"url":"http://arxiv.org/abs/2311.08228v1"}
{"created":"2023-11-14 15:07:13","title":"Prediction of inter packet arrival times for enhanced NR-V2X sidelink scheduling","abstract":"A significant limitation of the LTE-V2X and NR-V2X sidelink scheduling mechanisms is their difficulty coping with variations in inter packet arrival times, also known as aperiodic packets. This conflicts with the fundamental characteristics of most V2X services which are triggered based on an event. e.g. ETSI Cooperative Awareness Messages (CAMs) - vehicle kinematics, Cooperative Perception Messages (CPMs) - object sensing and Decentralised Event Notification Messages (DENMs) - event occurrences. Furthermore, network management techniques such as congestion control mechanisms can result in varied inter packet arrival times. To combat this, NR-V2X introduced a dynamic grant mechanism, which we show is ineffective unless there is background periodic traffic to stabilise the sensing history upon which the scheduler makes it decisions. The characteristics of V2X services make it implausible that such periodic application traffic will exist.   To overcome this significant drawback, we demonstrate that the standardised scheduling algorithms can be made effective if the event triggered arrival rate of packets can be accurately predicted. These predictions can be used to tune the Resource Reservation Interval (RRI) parameter of the MAC scheduler to negate the negative impact of aperiodicity. Such an approach allows the scheduler to achieve comparable performance to a scenario where packets arrive periodically. To demonstrate the effectiveness of our approach, an ML model has been devised for the prediction of cooperative awareness messages, but the same principle can be abstracted to other V2X service types.","sentences":["A significant limitation of the LTE-V2X and NR-V2X sidelink scheduling mechanisms is their difficulty coping with variations in inter packet arrival times, also known as aperiodic packets.","This conflicts with the fundamental characteristics of most V2X services which are triggered based on an event.","e.g. ETSI Cooperative Awareness Messages (CAMs) - vehicle kinematics, Cooperative Perception Messages (CPMs) - object sensing and Decentralised Event Notification Messages (DENMs) - event occurrences.","Furthermore, network management techniques such as congestion control mechanisms can result in varied inter packet arrival times.","To combat this, NR-V2X introduced a dynamic grant mechanism, which we show is ineffective unless there is background periodic traffic to stabilise the sensing history upon which the scheduler makes it decisions.","The characteristics of V2X services make it implausible that such periodic application traffic will exist.   ","To overcome this significant drawback, we demonstrate that the standardised scheduling algorithms can be made effective if the event triggered arrival rate of packets can be accurately predicted.","These predictions can be used to tune the Resource Reservation Interval (RRI) parameter of the MAC scheduler to negate the negative impact of aperiodicity.","Such an approach allows the scheduler to achieve comparable performance to a scenario where packets arrive periodically.","To demonstrate the effectiveness of our approach, an ML model has been devised for the prediction of cooperative awareness messages, but the same principle can be abstracted to other V2X service types."],"url":"http://arxiv.org/abs/2311.08227v1"}
{"created":"2023-11-14 15:01:58","title":"Improving Image Captioning via Predicting Structured Concepts","abstract":"Having the difficulty of solving the semantic gap between images and texts for the image captioning task, conventional studies in this area paid some attention to treating semantic concepts as a bridge between the two modalities and improved captioning performance accordingly. Although promising results on concept prediction were obtained, the aforementioned studies normally ignore the relationship among concepts, which relies on not only objects in the image, but also word dependencies in the text, so that offers a considerable potential for improving the process of generating good descriptions. In this paper, we propose a structured concept predictor (SCP) to predict concepts and their structures, then we integrate them into captioning, so as to enhance the contribution of visual signals in this task via concepts and further use their relations to distinguish cross-modal semantics for better description generation. Particularly, we design weighted graph convolutional networks (W-GCN) to depict concept relations driven by word dependencies, and then learns differentiated contributions from these concepts for following decoding process. Therefore, our approach captures potential relations among concepts and discriminatively learns different concepts, so that effectively facilitates image captioning with inherited information across modalities. Extensive experiments and their results demonstrate the effectiveness of our approach as well as each proposed module in this work.","sentences":["Having the difficulty of solving the semantic gap between images and texts for the image captioning task, conventional studies in this area paid some attention to treating semantic concepts as a bridge between the two modalities and improved captioning performance accordingly.","Although promising results on concept prediction were obtained, the aforementioned studies normally ignore the relationship among concepts, which relies on not only objects in the image, but also word dependencies in the text, so that offers a considerable potential for improving the process of generating good descriptions.","In this paper, we propose a structured concept predictor (SCP) to predict concepts and their structures, then we integrate them into captioning, so as to enhance the contribution of visual signals in this task via concepts and further use their relations to distinguish cross-modal semantics for better description generation.","Particularly, we design weighted graph convolutional networks (W-GCN) to depict concept relations driven by word dependencies, and then learns differentiated contributions from these concepts for following decoding process.","Therefore, our approach captures potential relations among concepts and discriminatively learns different concepts, so that effectively facilitates image captioning with inherited information across modalities.","Extensive experiments and their results demonstrate the effectiveness of our approach as well as each proposed module in this work."],"url":"http://arxiv.org/abs/2311.08223v1"}
{"created":"2023-11-14 14:59:00","title":"State-Dependent Channels with a Message-Cognizant Helper","abstract":"The capacity of a state-dependent discrete memoryless channel (SD-DMC) is derived for the setting where a message-cognizant rate-limited helper observes the state sequence noncausally, produces its description, and provides the description to both encoder and decoder.","sentences":["The capacity of a state-dependent discrete memoryless channel (SD-DMC) is derived for the setting where a message-cognizant rate-limited helper observes the state sequence noncausally, produces its description, and provides the description to both encoder and decoder."],"url":"http://arxiv.org/abs/2311.08220v1"}
{"created":"2023-11-14 14:56:33","title":"Eval-GCSC: A New Metric for Evaluating ChatGPT's Performance in Chinese Spelling Correction","abstract":"ChatGPT has demonstrated impressive performance in various downstream tasks. However, in the Chinese Spelling Correction (CSC) task, we observe a discrepancy: while ChatGPT performs well under human evaluation, it scores poorly according to traditional metrics. We believe this inconsistency arises because the traditional metrics are not well-suited for evaluating generative models. Their overly strict length and phonics constraints may lead to underestimating ChatGPT's correction capabilities. To better evaluate generative models in the CSC task, this paper proposes a new evaluation metric: Eval-GCSC. By incorporating word-level and semantic similarity judgments, it relaxes the stringent length and phonics constraints. Experimental results show that Eval-GCSC closely aligns with human evaluations. Under this metric, ChatGPT's performance is comparable to traditional token-level classification models (TCM), demonstrating its potential as a CSC tool. The source code and scripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.","sentences":["ChatGPT has demonstrated impressive performance in various downstream tasks.","However, in the Chinese Spelling Correction (CSC) task, we observe a discrepancy: while ChatGPT performs well under human evaluation, it scores poorly according to traditional metrics.","We believe this inconsistency arises because the traditional metrics are not well-suited for evaluating generative models.","Their overly strict length and phonics constraints may lead to underestimating ChatGPT's correction capabilities.","To better evaluate generative models in the CSC task, this paper proposes a new evaluation metric: Eval-GCSC.","By incorporating word-level and semantic similarity judgments, it relaxes the stringent length and phonics constraints.","Experimental results show that Eval-GCSC closely aligns with human evaluations.","Under this metric, ChatGPT's performance is comparable to traditional token-level classification models (TCM), demonstrating its potential as a CSC tool.","The source code and scripts can be accessed at https://github.com/ktlKTL/Eval-GCSC."],"url":"http://arxiv.org/abs/2311.08219v1"}
{"created":"2023-11-14 14:55:42","title":"Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot Image Generation","abstract":"Few-shot image generation aims to train generative models using a small number of training images. When there are few images available for training (e.g. 10 images), Learning From Scratch (LFS) methods often generate images that closely resemble the training data while Transfer Learning (TL) methods try to improve performance by leveraging prior knowledge from GANs pre-trained on large-scale datasets. However, current TL methods may not allow for sufficient control over the degree of knowledge preservation from the source model, making them unsuitable for setups where the source and target domains are not closely related. To address this, we propose a novel pipeline called Peer is your Pillar (PIP), which combines a target few-shot dataset with a peer dataset to create a data-unbalanced conditional generation. Our approach includes a class embedding method that separates the class space from the latent space, and we use a direction loss based on pre-trained CLIP to improve image diversity. Experiments on various few-shot datasets demonstrate the advancement of the proposed PIP, especially reduces the training requirements of few-shot image generation.","sentences":["Few-shot image generation aims to train generative models using a small number of training images.","When there are few images available for training (e.g. 10 images), Learning From Scratch (LFS) methods often generate images that closely resemble the training data while Transfer Learning (TL) methods try to improve performance by leveraging prior knowledge from GANs pre-trained on large-scale datasets.","However, current TL methods may not allow for sufficient control over the degree of knowledge preservation from the source model, making them unsuitable for setups where the source and target domains are not closely related.","To address this, we propose a novel pipeline called Peer is your Pillar (PIP), which combines a target few-shot dataset with a peer dataset to create a data-unbalanced conditional generation.","Our approach includes a class embedding method that separates the class space from the latent space, and we use a direction loss based on pre-trained CLIP to improve image diversity.","Experiments on various few-shot datasets demonstrate the advancement of the proposed PIP, especially reduces the training requirements of few-shot image generation."],"url":"http://arxiv.org/abs/2311.08217v1"}
{"created":"2023-11-14 14:49:46","title":"Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models","abstract":"Recently, multi-modal content generation has attracted lots of attention from researchers by investigating the utilization of visual instruction tuning based on large language models (LLMs). To enhance the performance and generalization ability of such LLMs, the practice of distilling knowledge from pretrained multi-modal models (a.k.a. teachers) to more compact multi-modal LLMs (students) has gained considerable interest. However, the prevailing paradigm of instructiontuning in multi-modal LLMs knowledge distillation is resource-intensive and unidirectional, neglecting the potential for mutual feedback between the student and teacher models. Thus, we propose an innovative Competitive Multi-modal Distillation framework (CoMD), which captures bidirectional feedback between teacher and student models and continually updates the multi-modal capabilities that the student model has learned. It comprises two stages: multi-modal pre-training and multi-modal competitive distillation. The first stage pre-trains the student model on a large number of filtered multi-modal datasets. The second stage facilitates a bidirectional knowledge transfer between the student and teacher models. Our experimental analysis of diverse datasets shows that our knowledge transfer method consistently improves the capabilities of the student model. Finally, the 7B-sized student model after four distillations surpassed the current state-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also outperforms other strong baselines in the zero-shot setting.","sentences":["Recently, multi-modal content generation has attracted lots of attention from researchers by investigating the utilization of visual instruction tuning based on large language models (LLMs).","To enhance the performance and generalization ability of such LLMs, the practice of distilling knowledge from pretrained multi-modal models (a.k.a. teachers) to more compact multi-modal LLMs (students) has gained considerable interest.","However, the prevailing paradigm of instructiontuning in multi-modal LLMs knowledge distillation is resource-intensive and unidirectional, neglecting the potential for mutual feedback between the student and teacher models.","Thus, we propose an innovative Competitive Multi-modal Distillation framework (CoMD), which captures bidirectional feedback between teacher and student models and continually updates the multi-modal capabilities that the student model has learned.","It comprises two stages: multi-modal pre-training and multi-modal competitive distillation.","The first stage pre-trains the student model on a large number of filtered multi-modal datasets.","The second stage facilitates a bidirectional knowledge transfer between the student and teacher models.","Our experimental analysis of diverse datasets shows that our knowledge transfer method consistently improves the capabilities of the student model.","Finally, the 7B-sized student model after four distillations surpassed the current state-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also outperforms other strong baselines in the zero-shot setting."],"url":"http://arxiv.org/abs/2311.08213v1"}
{"created":"2023-11-14 14:44:16","title":"Online Data-driven Control Against False Data Injection Attacks","abstract":"The rise of cyber-security concerns has brought significant attention to the analysis and design of cyber-physical systems (CPSs). Among the various types of cyberattacks, denial-of-service (DoS) attacks and false data injection (FDI) attacks can be easily launched and have become prominent threats. While resilient control against DoS attacks has received substantial research efforts, countermeasures developed against FDI attacks have been relatively limited, particularly when explicit system models are not available. To address this gap, the present paper focuses on the design of data-driven controllers for unknown linear systems subject to FDI attacks on the actuators, utilizing input-state data. To this end, a general FDI attack model is presented, which imposes minimally constraints on the switching frequency of attack channels and the magnitude of attack matrices. A dynamic state feedback control law is designed based on offline and online input-state data, which adapts to the channel switching of FDI attacks. This is achieved by solving two data-based semi-definite programs (SDPs) on-the-fly to yield a tight approximation of the set of subsystems consistent with both offline clean data and online attack-corrupted data. It is shown that under mild conditions on the attack, the proposed SDPs are recursively feasible and controller achieves exponential stability. Numerical examples showcase its effectiveness in mitigating the impact of FDI attacks.","sentences":["The rise of cyber-security concerns has brought significant attention to the analysis and design of cyber-physical systems (CPSs).","Among the various types of cyberattacks, denial-of-service (DoS) attacks and false data injection (FDI) attacks can be easily launched and have become prominent threats.","While resilient control against DoS attacks has received substantial research efforts, countermeasures developed against FDI attacks have been relatively limited, particularly when explicit system models are not available.","To address this gap, the present paper focuses on the design of data-driven controllers for unknown linear systems subject to FDI attacks on the actuators, utilizing input-state data.","To this end, a general FDI attack model is presented, which imposes minimally constraints on the switching frequency of attack channels and the magnitude of attack matrices.","A dynamic state feedback control law is designed based on offline and online input-state data, which adapts to the channel switching of FDI attacks.","This is achieved by solving two data-based semi-definite programs (SDPs) on-the-fly to yield a tight approximation of the set of subsystems consistent with both offline clean data and online attack-corrupted data.","It is shown that under mild conditions on the attack, the proposed SDPs are recursively feasible and controller achieves exponential stability.","Numerical examples showcase its effectiveness in mitigating the impact of FDI attacks."],"url":"http://arxiv.org/abs/2311.08207v1"}
{"created":"2023-11-14 14:42:28","title":"Human-Centric Autonomous Systems With LLMs for User Command Reasoning","abstract":"The evolution of autonomous driving has made remarkable advancements in recent years, evolving into a tangible reality. However, a human-centric large-scale adoption hinges on meeting a variety of multifaceted requirements. To ensure that the autonomous system meets the user's intent, it is essential to accurately discern and interpret user commands, especially in complex or emergency situations. To this end, we propose to leverage the reasoning capabilities of Large Language Models (LLMs) to infer system requirements from in-cabin users' commands. Through a series of experiments that include different LLM models and prompt designs, we explore the few-shot multivariate binary classification accuracy of system requirements from natural language textual commands. We confirm the general ability of LLMs to understand and reason about prompts but underline that their effectiveness is conditioned on the quality of both the LLM model and the design of appropriate sequential prompts. Code and models are public with the link \\url{https://github.com/KTH-RPL/DriveCmd_LLM}.","sentences":["The evolution of autonomous driving has made remarkable advancements in recent years, evolving into a tangible reality.","However, a human-centric large-scale adoption hinges on meeting a variety of multifaceted requirements.","To ensure that the autonomous system meets the user's intent, it is essential to accurately discern and interpret user commands, especially in complex or emergency situations.","To this end, we propose to leverage the reasoning capabilities of Large Language Models (LLMs) to infer system requirements from in-cabin users' commands.","Through a series of experiments that include different LLM models and prompt designs, we explore the few-shot multivariate binary classification accuracy of system requirements from natural language textual commands.","We confirm the general ability of LLMs to understand and reason about prompts but underline that their effectiveness is conditioned on the quality of both the LLM model and the design of appropriate sequential prompts.","Code and models are public with the link \\url{https://github.com/KTH-RPL/DriveCmd_LLM}."],"url":"http://arxiv.org/abs/2311.08206v1"}
{"created":"2023-11-14 14:41:39","title":"Increasing the Efficiency of Cryptoasset Investigations by Connecting the Cases","abstract":"Law enforcement agencies are confronted with a rapidly growing number of cryptoasset-related cases, often redundantly investigating the same cases without mutual knowledge or shared insights. In this paper, we explore the hypothesis that recognizing and acting upon connections between these cases can significantly streamline investigative processes. Through an analysis of a dataset comprising 34 cyberfraud and 1793 sextortion spam cases, we discovered that 41% of the cyberfraud and 96.9% of the sextortion spam incidents can be interconnected. We introduce a straightforward yet effective tool, which is integrated into a broader cryptoasset forensics workflow and allows investigators to highlight and share case connections. Our research unequivocally demonstrates that recognizing case connections can lead to remarkable efficiencies, especially when extended across crime areas, international borders, and jurisdictions.","sentences":["Law enforcement agencies are confronted with a rapidly growing number of cryptoasset-related cases, often redundantly investigating the same cases without mutual knowledge or shared insights.","In this paper, we explore the hypothesis that recognizing and acting upon connections between these cases can significantly streamline investigative processes.","Through an analysis of a dataset comprising 34 cyberfraud and 1793 sextortion spam cases, we discovered that 41% of the cyberfraud and 96.9% of the sextortion spam incidents can be interconnected.","We introduce a straightforward yet effective tool, which is integrated into a broader cryptoasset forensics workflow and allows investigators to highlight and share case connections.","Our research unequivocally demonstrates that recognizing case connections can lead to remarkable efficiencies, especially when extended across crime areas, international borders, and jurisdictions."],"url":"http://arxiv.org/abs/2311.08205v1"}
{"created":"2023-11-14 14:39:28","title":"On The Evaluation of Collision Probability along a Path","abstract":"Characterizing the risk of operations is a fundamental requirement in robotics, and a crucial ingredient of safe planning. The problem is multifaceted, with multiple definitions arising in the vast recent literature fitting different application scenarios and leading to different computational approaches. A basic element shared by most frameworks is the definition and evaluation of the probability of collision for a mobile object in an environment with obstacles. We observe that, even in basic cases, different interpretations are possible. This paper proposes an index we call Risk Density, which offers a theoretical link between conceptually distant assumptions about the interplay of single collision events along a continuous path. We show how this index can be used to approximate the collision probability in the case where the robot evolves along a nominal continuous curve from random initial conditions. Indeed under this hypothesis the proposed approximation outperforms some well-established methods either in accuracy or computational cost.","sentences":["Characterizing the risk of operations is a fundamental requirement in robotics, and a crucial ingredient of safe planning.","The problem is multifaceted, with multiple definitions arising in the vast recent literature fitting different application scenarios and leading to different computational approaches.","A basic element shared by most frameworks is the definition and evaluation of the probability of collision for a mobile object in an environment with obstacles.","We observe that, even in basic cases, different interpretations are possible.","This paper proposes an index we call Risk Density, which offers a theoretical link between conceptually distant assumptions about the interplay of single collision events along a continuous path.","We show how this index can be used to approximate the collision probability in the case where the robot evolves along a nominal continuous curve from random initial conditions.","Indeed under this hypothesis the proposed approximation outperforms some well-established methods either in accuracy or computational cost."],"url":"http://arxiv.org/abs/2311.08204v1"}
{"created":"2023-11-14 14:37:33","title":"Federated Skewed Label Learning with Logits Fusion","abstract":"Federated learning (FL) aims to collaboratively train a shared model across multiple clients without transmitting their local data. Data heterogeneity is a critical challenge in realistic FL settings, as it causes significant performance deterioration due to discrepancies in optimization among local models. In this work, we focus on label distribution skew, a common scenario in data heterogeneity, where the data label categories are imbalanced on each client. To address this issue, we propose FedBalance, which corrects the optimization bias among local models by calibrating their logits. Specifically, we introduce an extra private weak learner on the client side, which forms an ensemble model with the local model. By fusing the logits of the two models, the private weak learner can capture the variance of different data, regardless of their category. Therefore, the optimization direction of local models can be improved by increasing the penalty for misclassifying minority classes and reducing the attention to majority classes, resulting in a better global model. Extensive experiments show that our method can gain 13\\% higher average accuracy compared with state-of-the-art methods.","sentences":["Federated learning (FL) aims to collaboratively train a shared model across multiple clients without transmitting their local data.","Data heterogeneity is a critical challenge in realistic FL settings, as it causes significant performance deterioration due to discrepancies in optimization among local models.","In this work, we focus on label distribution skew, a common scenario in data heterogeneity, where the data label categories are imbalanced on each client.","To address this issue, we propose FedBalance, which corrects the optimization bias among local models by calibrating their logits.","Specifically, we introduce an extra private weak learner on the client side, which forms an ensemble model with the local model.","By fusing the logits of the two models, the private weak learner can capture the variance of different data, regardless of their category.","Therefore, the optimization direction of local models can be improved by increasing the penalty for misclassifying minority classes and reducing the attention to majority classes, resulting in a better global model.","Extensive experiments show that our method can gain 13\\% higher average accuracy compared with state-of-the-art methods."],"url":"http://arxiv.org/abs/2311.08202v1"}
