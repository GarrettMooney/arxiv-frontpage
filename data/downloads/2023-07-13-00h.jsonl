{"created":"2023-07-11 17:58:31","title":"Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives","abstract":"Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives. While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D representations made of a small set of textured primitives. Such representations are interpretable, easy to manipulate and suited for physics-based simulations. Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering. Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss. We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives. We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions. We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio. We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations. Code and video results are available at https://www.tmonnier.com/DBW .","sentences":["Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives.","While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D representations made of a small set of textured primitives.","Such representations are interpretable, easy to manipulate and suited for physics-based simulations.","Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering.","Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss.","We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives.","We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions.","We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio.","We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations.","Code and video results are available at https://www.tmonnier.com/DBW ."],"url":"http://arxiv.org/abs/2307.05473v1"}
{"created":"2023-07-11 17:56:22","title":"Scale Alone Does not Improve Mechanistic Interpretability in Vision Models","abstract":"In light of the recent widespread adoption of AI systems, understanding the internal information processing of neural networks has become increasingly critical. Most recently, machine vision has seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model size. We here ask whether this extraordinary increase in scale also positively impacts the field of mechanistic interpretability. In other words, has our understanding of the inner workings of scaled neural networks improved as well? We here use a psychophysical paradigm to quantify mechanistic interpretability for a diverse suite of models and find no scaling effect for interpretability - neither for model nor dataset size. Specifically, none of the nine investigated state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago. Latest-generation vision models appear even less interpretable than older architectures, hinting at a regression rather than improvement, with modern models sacrificing interpretability for accuracy. These results highlight the need for models explicitly designed to be mechanistically interpretable and the need for more helpful interpretability methods to increase our understanding of networks at an atomic level. We release a dataset containing more than 120'000 human responses from our psychophysical evaluation of 767 units across nine models. This dataset is meant to facilitate research on automated instead of human-based interpretability evaluations that can ultimately be leveraged to directly optimize the mechanistic interpretability of models.","sentences":["In light of the recent widespread adoption of AI systems, understanding the internal information processing of neural networks has become increasingly critical.","Most recently, machine vision has seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model size.","We here ask whether this extraordinary increase in scale also positively impacts the field of mechanistic interpretability.","In other words, has our understanding of the inner workings of scaled neural networks improved as well?","We here use a psychophysical paradigm to quantify mechanistic interpretability for a diverse suite of models and find no scaling effect for interpretability - neither for model nor dataset size.","Specifically, none of the nine investigated state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago.","Latest-generation vision models appear even less interpretable than older architectures, hinting at a regression rather than improvement, with modern models sacrificing interpretability for accuracy.","These results highlight the need for models explicitly designed to be mechanistically interpretable and the need for more helpful interpretability methods to increase our understanding of networks at an atomic level.","We release a dataset containing more than 120'000 human responses from our psychophysical evaluation of 767 units across nine models.","This dataset is meant to facilitate research on automated instead of human-based interpretability evaluations that can ultimately be leveraged to directly optimize the mechanistic interpretability of models."],"url":"http://arxiv.org/abs/2307.05471v1"}
{"created":"2023-07-11 17:53:43","title":"My3DGen: Building Lightweight Personalized 3D Generative Model","abstract":"Our paper presents My3DGen, a practical system for creating a personalized and lightweight 3D generative prior using as few as 10 images. My3DGen can reconstruct multi-view consistent images from an input test image, and generate novel appearances by interpolating between any two images of the same individual. While recent studies have demonstrated the effectiveness of personalized generative priors in producing high-quality 2D portrait reconstructions and syntheses, to the best of our knowledge, we are the first to develop a personalized 3D generative prior. Instead of fine-tuning a large pre-trained generative model with millions of parameters to achieve personalization, we propose a parameter-efficient approach. Our method involves utilizing a pre-trained model with fixed weights as a generic prior, while training a separate personalized prior through low-rank decomposition of the weights in each convolution and fully connected layer. However, parameter-efficient few-shot fine-tuning on its own often leads to overfitting. To address this, we introduce a regularization technique based on symmetry of human faces. This regularization enforces that novel view renderings of a training sample, rendered from symmetric poses, exhibit the same identity. By incorporating this symmetry prior, we enhance the quality of reconstruction and synthesis, particularly for non-frontal (profile) faces. Our final system combines low-rank fine-tuning with symmetry regularization and significantly surpasses the performance of pre-trained models, e.g. EG3D. It introduces only approximately 0.6 million additional parameters per identity compared to 31 million for full finetuning of the original model. As a result, our system achieves a 50-fold reduction in model size without sacrificing the quality of the generated 3D faces. Code will be available at our project page: https://luchaoqi.github.io/my3dgen.","sentences":["Our paper presents My3DGen, a practical system for creating a personalized and lightweight 3D generative prior using as few as 10 images.","My3DGen can reconstruct multi-view consistent images from an input test image, and generate novel appearances by interpolating between any two images of the same individual.","While recent studies have demonstrated the effectiveness of personalized generative priors in producing high-quality 2D portrait reconstructions and syntheses, to the best of our knowledge, we are the first to develop a personalized 3D generative prior.","Instead of fine-tuning a large pre-trained generative model with millions of parameters to achieve personalization, we propose a parameter-efficient approach.","Our method involves utilizing a pre-trained model with fixed weights as a generic prior, while training a separate personalized prior through low-rank decomposition of the weights in each convolution and fully connected layer.","However, parameter-efficient few-shot fine-tuning on its own often leads to overfitting.","To address this, we introduce a regularization technique based on symmetry of human faces.","This regularization enforces that novel view renderings of a training sample, rendered from symmetric poses, exhibit the same identity.","By incorporating this symmetry prior, we enhance the quality of reconstruction and synthesis, particularly for non-frontal (profile) faces.","Our final system combines low-rank fine-tuning with symmetry regularization and significantly surpasses the performance of pre-trained models, e.g. EG3D. It introduces only approximately 0.6 million additional parameters per identity compared to 31 million for full finetuning of the original model.","As a result, our system achieves a 50-fold reduction in model size without sacrificing the quality of the generated 3D faces.","Code will be available at our project page: https://luchaoqi.github.io/my3dgen."],"url":"http://arxiv.org/abs/2307.05468v1"}
{"created":"2023-07-11 17:50:15","title":"EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone","abstract":"Video-language pre-training (VLP) has become increasingly important due to its ability to generalize to various vision and language tasks. However, existing egocentric VLP frameworks utilize separate video and language encoders and learn task-specific cross-modal information only during fine-tuning, limiting the development of a unified system. In this work, we introduce the second generation of egocentric video-language pre-training (EgoVLPv2), a significant improvement from the previous generation, by incorporating cross-modal fusion directly into the video and language backbones. EgoVLPv2 learns strong video-text representation during pre-training and reuses the cross-modal attention modules to support different downstream tasks in a flexible and efficient manner, reducing fine-tuning costs. Moreover, our proposed fusion in the backbone strategy is more lightweight and compute-efficient than stacking additional fusion-specific layers. Extensive experiments on a wide range of VL tasks demonstrate the effectiveness of EgoVLPv2 by achieving consistent state-of-the-art performance over strong baselines across all downstream. Our project page can be found at https://shramanpramanick.github.io/EgoVLPv2/.","sentences":["Video-language pre-training (VLP) has become increasingly important due to its ability to generalize to various vision and language tasks.","However, existing egocentric VLP frameworks utilize separate video and language encoders and learn task-specific cross-modal information only during fine-tuning, limiting the development of a unified system.","In this work, we introduce the second generation of egocentric video-language pre-training (EgoVLPv2), a significant improvement from the previous generation, by incorporating cross-modal fusion directly into the video and language backbones.","EgoVLPv2 learns strong video-text representation during pre-training and reuses the cross-modal attention modules to support different downstream tasks in a flexible and efficient manner, reducing fine-tuning costs.","Moreover, our proposed fusion in the backbone strategy is more lightweight and compute-efficient than stacking additional fusion-specific layers.","Extensive experiments on a wide range of VL tasks demonstrate the effectiveness of EgoVLPv2 by achieving consistent state-of-the-art performance over strong baselines across all downstream.","Our project page can be found at https://shramanpramanick.github.io/EgoVLPv2/."],"url":"http://arxiv.org/abs/2307.05463v1"}
{"created":"2023-07-11 17:50:02","title":"Efficient 3D Articulated Human Generation with Layered Surface Volumes","abstract":"Access to high-quality and diverse 3D articulated digital human assets is crucial in various applications, ranging from virtual reality to social platforms. Generative approaches, such as 3D generative adversarial networks (GANs), are rapidly replacing laborious manual content creation tools. However, existing 3D GAN frameworks typically rely on scene representations that leverage either template meshes, which are fast but offer limited quality, or volumes, which offer high capacity but are slow to render, thereby limiting the 3D fidelity in GAN settings. In this work, we introduce layered surface volumes (LSVs) as a new 3D object representation for articulated digital humans. LSVs represent a human body using multiple textured mesh layers around a conventional template. These layers are rendered using alpha compositing with fast differentiable rasterization, and they can be interpreted as a volumetric representation that allocates its capacity to a manifold of finite thickness around the template. Unlike conventional single-layer templates that struggle with representing fine off-surface details like hair or accessories, our surface volumes naturally capture such details. LSVs can be articulated, and they exhibit exceptional efficiency in GAN settings, where a 2D generator learns to synthesize the RGBA textures for the individual layers. Trained on unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality and view-consistent 3D articulated digital humans without the need for view-inconsistent 2D upsampling networks.","sentences":["Access to high-quality and diverse 3D articulated digital human assets is crucial in various applications, ranging from virtual reality to social platforms.","Generative approaches, such as 3D generative adversarial networks (GANs), are rapidly replacing laborious manual content creation tools.","However, existing 3D GAN frameworks typically rely on scene representations that leverage either template meshes, which are fast but offer limited quality, or volumes, which offer high capacity but are slow to render, thereby limiting the 3D fidelity in GAN settings.","In this work, we introduce layered surface volumes (LSVs) as a new 3D object representation for articulated digital humans.","LSVs represent a human body using multiple textured mesh layers around a conventional template.","These layers are rendered using alpha compositing with fast differentiable rasterization, and they can be interpreted as a volumetric representation that allocates its capacity to a manifold of finite thickness around the template.","Unlike conventional single-layer templates that struggle with representing fine off-surface details like hair or accessories, our surface volumes naturally capture such details.","LSVs can be articulated, and they exhibit exceptional efficiency in GAN settings, where a 2D generator learns to synthesize the RGBA textures for the individual layers.","Trained on unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality and view-consistent 3D articulated digital humans without the need for view-inconsistent 2D upsampling networks."],"url":"http://arxiv.org/abs/2307.05462v1"}
{"created":"2023-07-11 17:33:03","title":"Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features","abstract":"A challenge towards developing NLP systems for the world's languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models' behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish. Our findings motivate the development of models that address these blind spots.","sentences":["A challenge towards developing NLP systems for the world's languages is understanding how they generalize to typological differences relevant for real-world applications.","To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models.","We use M2C to generate tests that probe models' behavior in light of specific linguistic features in 12 typologically diverse languages.","We evaluate state-of-the-art language models on the generated tests.","While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish.","Our findings motivate the development of models that address these blind spots."],"url":"http://arxiv.org/abs/2307.05454v1"}
{"created":"2023-07-11 17:27:43","title":"Detection Threshold of Audio Haptic Asynchrony in a Driving Context","abstract":"In order to provide perceptually accurate multimodal feedback during driving situations, it is vital to understand the threshold at which drivers are able to recognize asyncrony between multiple incoming Stimuli. In this work, we investigated and report the \\textit{detection threshold} (DT) of asynchrony between audio and haptic feedback, in the context of a force feedback steering wheel. We designed the experiment to loosely resemble a driving situation where the haptic feedback was provided through a steering wheel (\\textit{Sensodrive}), while the accompanying audio was played through noise cancelling headphones. Both feedbacks were designed to resemble rumble strips, that are generally installed on the side of major roadways as a safety tool. The results indicate that, for $50\\%$ of the participants, asynchrony was detectable outside the range of -75 ms and 110 ms, where the former is related to perceiving audio before haptic and vice versa for the latter. We were also able to concur with previous studies, which state that latency is perceivable at a lower threshold when audio precedes haptic stimuli.","sentences":["In order to provide perceptually accurate multimodal feedback during driving situations, it is vital to understand the threshold at which drivers are able to recognize asyncrony between multiple incoming Stimuli.","In this work, we investigated and report the \\textit{detection threshold} (DT) of asynchrony between audio and haptic feedback, in the context of a force feedback steering wheel.","We designed the experiment to loosely resemble a driving situation where the haptic feedback was provided through a steering wheel (\\textit{Sensodrive}), while the accompanying audio was played through noise cancelling headphones.","Both feedbacks were designed to resemble rumble strips, that are generally installed on the side of major roadways as a safety tool.","The results indicate that, for $50\\%$ of the participants, asynchrony was detectable outside the range of -75 ms and 110 ms, where the former is related to perceiving audio before haptic and vice versa for the latter.","We were also able to concur with previous studies, which state that latency is perceivable at a lower threshold when audio precedes haptic stimuli."],"url":"http://arxiv.org/abs/2307.05451v1"}
{"created":"2023-07-11 17:23:27","title":"On the hull and complementarity of one generator quasi-cyclic codes and four-circulant codes","abstract":"We study one generator quasi-cyclic codes and four-circulant codes, which are also quasi-cyclic but have two generators. We state the hull dimensions for both classes of codes in terms of the polynomials in their generating elements. We prove results such as the hull dimension of a four-circulant code is even and one-dimensional hull for double-circulant codes, which are special one generator codes, is not possible when the alphabet size $q$ is congruent to 3 mod 4. We also characterize linear complementary pairs among both classes of codes. Computational results on the code families in consideration are provided as well.","sentences":["We study one generator quasi-cyclic codes and four-circulant codes, which are also quasi-cyclic but have two generators.","We state the hull dimensions for both classes of codes in terms of the polynomials in their generating elements.","We prove results such as the hull dimension of a four-circulant code is even and one-dimensional hull for double-circulant codes, which are special one generator codes, is not possible when the alphabet size $q$ is congruent to 3 mod 4.","We also characterize linear complementary pairs among both classes of codes.","Computational results on the code families in consideration are provided as well."],"url":"http://arxiv.org/abs/2307.05449v1"}
{"created":"2023-07-11 17:23:23","title":"Polynomial-Time Linear-Swap Regret Minimization in Imperfect-Information Sequential Games","abstract":"No-regret learners seek to minimize the difference between the loss they cumulated through the actions they played, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. The size of the set of transformations considered by the learner determines a natural notion of rationality. As the set of transformations each learner considers grows, the strategies played by the learners recover more complex game-theoretic equilibria, including correlated equilibria in normal-form games and extensive-form correlated equilibria in extensive-form games. At the extreme, a no-swap-regret agent is one that minimizes regret against the set of all functions from the set of strategies to itself. While it is known that the no-swap-regret condition can be attained efficiently in nonsequential (normal-form) games, understanding what is the strongest notion of rationality that can be attained efficiently in the worst case in sequential (extensive-form) games is a longstanding open problem. In this paper we provide a positive result, by showing that it is possible, in any sequential game, to retain polynomial-time (in the game tree size) iterations while achieving sublinear regret with respect to all linear transformations of the mixed strategy space, a notion called no-linear-swap regret. This notion of hindsight rationality is as strong as no-swap-regret in nonsequential games, and stronger than no-trigger-regret in sequential games -- thereby proving the existence of a subset of extensive-form correlated equilibria robust to linear deviations, which we call linear-deviation correlated equilibria, that can be approached efficiently.","sentences":["No-regret learners seek to minimize the difference between the loss they cumulated through the actions they played, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function.","The size of the set of transformations considered by the learner determines a natural notion of rationality.","As the set of transformations each learner considers grows, the strategies played by the learners recover more complex game-theoretic equilibria, including correlated equilibria in normal-form games and extensive-form correlated equilibria in extensive-form games.","At the extreme, a no-swap-regret agent is one that minimizes regret against the set of all functions from the set of strategies to itself.","While it is known that the no-swap-regret condition can be attained efficiently in nonsequential (normal-form) games, understanding what is the strongest notion of rationality that can be attained efficiently in the worst case in sequential (extensive-form) games is a longstanding open problem.","In this paper we provide a positive result, by showing that it is possible, in any sequential game, to retain polynomial-time (in the game tree size) iterations while achieving sublinear regret with respect to all linear transformations of the mixed strategy space, a notion called no-linear-swap regret.","This notion of hindsight rationality is as strong as no-swap-regret in nonsequential games, and stronger than no-trigger-regret in sequential games -- thereby proving the existence of a subset of extensive-form correlated equilibria robust to linear deviations, which we call linear-deviation correlated equilibria, that can be approached efficiently."],"url":"http://arxiv.org/abs/2307.05448v1"}
{"created":"2023-07-11 17:22:22","title":"Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and Denoising","abstract":"Due to the low accuracy of object detection and recognition in many intelligent surveillance systems at nighttime, the quality of night images is crucial. Compared with the corresponding daytime image, nighttime image is characterized as low brightness, low contrast and high noise. In this paper, a bio-inspired image enhancement algorithm is proposed to convert a low illuminance image to a brighter and clear one. Different from existing bio-inspired algorithm, the proposed method doesn't use any training sequences, we depend on a novel chain of contrast enhancement and denoising algorithms without using any forms of recursive functions. Our method can largely improve the brightness and contrast of night images, besides, suppress noise. Then we implement on real experiment, and simulation experiment to test our algorithms. Both results show the advantages of proposed algorithm over contrast pair, Meylan and Retinex.","sentences":["Due to the low accuracy of object detection and recognition in many intelligent surveillance systems at nighttime, the quality of night images is crucial.","Compared with the corresponding daytime image, nighttime image is characterized as low brightness, low contrast and high noise.","In this paper, a bio-inspired image enhancement algorithm is proposed to convert a low illuminance image to a brighter and clear one.","Different from existing bio-inspired algorithm, the proposed method doesn't use any training sequences, we depend on a novel chain of contrast enhancement and denoising algorithms without using any forms of recursive functions.","Our method can largely improve the brightness and contrast of night images, besides, suppress noise.","Then we implement on real experiment, and simulation experiment to test our algorithms.","Both results show the advantages of proposed algorithm over contrast pair, Meylan and Retinex."],"url":"http://arxiv.org/abs/2307.05447v1"}
{"created":"2023-07-11 17:21:49","title":"Parameterized Results on Acyclic Matchings with Implications for Related Problems","abstract":"A matching $M$ in a graph $G$ is an \\emph{acyclic matching} if the subgraph of $G$ induced by the endpoints of the edges of $M$ is a forest. Given a graph $G$ and a positive integer $\\ell$, Acyclic Matching asks whether $G$ has an acyclic matching of size (i.e., the number of edges) at least $\\ell$. In this paper, we first prove that assuming $\\mathsf{W[1]\\nsubseteq FPT}$, there does not exist any $\\mathsf{FPT}$-approximation algorithm for Acyclic Matching that approximates it within a constant factor when the parameter is the size of the matching. Our reduction is general in the sense that it also asserts $\\mathsf{FPT}$-inapproximability for Induced Matching and Uniquely Restricted Matching as well. We also consider three below-guarantee parameters for Acyclic Matching, viz. $\\frac{n}{2}-\\ell$, $\\mathsf{MM(G)}-\\ell$, and $\\mathsf{IS(G)}-\\ell$, where $n$ is the number of vertices in $G$, $\\mathsf{MM(G)}$ is the matching number of $G$, and $\\mathsf{IS(G)}$ is the independence number of $G$. Furthermore, we show that Acyclic Matching does not exhibit a polynomial kernel with respect to vertex cover number (or vertex deletion distance to clique) plus the size of the matching unless $\\mathsf{NP}\\subseteq\\mathsf{coNP}\\slash\\mathsf{poly}$.","sentences":["A matching $M$ in a graph $G$ is an \\emph{acyclic matching} if the subgraph of $G$ induced by the endpoints of the edges of $M$ is a forest.","Given a graph $G$ and a positive integer $\\ell$, Acyclic Matching asks whether $G$ has an acyclic matching of size (i.e., the number of edges) at least $\\ell$. In this paper, we first prove that assuming $\\mathsf{W[1]\\nsubseteq FPT}$, there does not exist any $\\mathsf{FPT}$-approximation algorithm for Acyclic Matching that approximates it within a constant factor when the parameter is the size of the matching.","Our reduction is general in the sense that it also asserts $\\mathsf{FPT}$-inapproximability for Induced Matching and Uniquely Restricted Matching as well.","We also consider three below-guarantee parameters for Acyclic Matching, viz.","$\\frac{n}{2}-\\ell$, $\\mathsf{MM(G)}-\\ell$, and $\\mathsf{IS(G)}-\\ell$, where $n$ is the number of vertices in $G$, $\\mathsf{MM(G)}$ is the matching number of $G$, and $\\mathsf{IS(G)}$ is the independence number of $G$. Furthermore, we show that Acyclic Matching does not exhibit a polynomial kernel with respect to vertex cover number (or vertex deletion distance to clique) plus the size of the matching unless $\\mathsf{NP}\\subseteq\\mathsf{coNP}\\slash\\mathsf{poly}$."],"url":"http://arxiv.org/abs/2307.05446v1"}
{"created":"2023-07-11 17:09:26","title":"Testing for Reviewer Anchoring in Peer Review: A Randomized Controlled Trial","abstract":"Peer review frequently follows a process where reviewers first provide initial reviews, authors respond to these reviews, then reviewers update their reviews based on the authors' response. There is mixed evidence regarding whether this process is useful, including frequent anecdotal complaints that reviewers insufficiently update their scores. In this study, we aim to investigate whether reviewers anchor to their original scores when updating their reviews, which serves as a potential explanation for the lack of updates in reviewer scores.   We design a novel randomized controlled trial to test if reviewers exhibit anchoring. In the experimental condition, participants initially see a flawed version of a paper that is later corrected, while in the control condition, participants only see the correct version. We take various measures to ensure that in the absence of anchoring, reviewers in the experimental group should revise their scores to be identically distributed to the scores from the control group. Furthermore, we construct the reviewed paper to maximize the difference between the flawed and corrected versions, and employ deception to hide the true experiment purpose.   Our randomized controlled trial consists of 108 researchers as participants. First, we find that our intervention was successful at creating a difference in perceived paper quality between the flawed and corrected versions: Using a permutation test with the Mann-Whitney U statistic, we find that the experimental group's initial scores are lower than the control group's scores in both the Evaluation category (Vargha-Delaney A=0.64, p=0.0096) and Overall score (A=0.59, p=0.058). Next, we test for anchoring by comparing the experimental group's revised scores with the control group's scores. We find no significant evidence of anchoring in either the Overall (A=0.50, p=0.61) or Evaluation category (A=0.49, p=0.61).","sentences":["Peer review frequently follows a process where reviewers first provide initial reviews, authors respond to these reviews, then reviewers update their reviews based on the authors' response.","There is mixed evidence regarding whether this process is useful, including frequent anecdotal complaints that reviewers insufficiently update their scores.","In this study, we aim to investigate whether reviewers anchor to their original scores when updating their reviews, which serves as a potential explanation for the lack of updates in reviewer scores.   ","We design a novel randomized controlled trial to test if reviewers exhibit anchoring.","In the experimental condition, participants initially see a flawed version of a paper that is later corrected, while in the control condition, participants only see the correct version.","We take various measures to ensure that in the absence of anchoring, reviewers in the experimental group should revise their scores to be identically distributed to the scores from the control group.","Furthermore, we construct the reviewed paper to maximize the difference between the flawed and corrected versions, and employ deception to hide the true experiment purpose.   ","Our randomized controlled trial consists of 108 researchers as participants.","First, we find that our intervention was successful at creating a difference in perceived paper quality between the flawed and corrected versions: Using a permutation test with the Mann-Whitney U statistic, we find that the experimental group's initial scores are lower than the control group's scores in both the Evaluation category (Vargha-Delaney A=0.64, p=0.0096) and Overall score (A=0.59, p=0.058).","Next, we test for anchoring by comparing the experimental group's revised scores with the control group's scores.","We find no significant evidence of anchoring in either the Overall (A=0.50, p=0.61) or Evaluation category (A=0.49, p=0.61)."],"url":"http://arxiv.org/abs/2307.05443v1"}
{"created":"2023-07-11 17:06:52","title":"ISLTranslate: Dataset for Translating Indian Sign Language","abstract":"Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language. We provide a detailed analysis of the dataset. To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation.","sentences":["Sign languages are the primary means of communication for many hard-of-hearing people worldwide.","Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems.","However, there is a dearth of sign language resources for the Indian sign language.","This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs.","To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language.","We provide a detailed analysis of the dataset.","To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation."],"url":"http://arxiv.org/abs/2307.05440v1"}
{"created":"2023-07-11 17:05:23","title":"Metropolis Sampling for Constrained Diffusion Models","abstract":"Denoising diffusion models have recently emerged as the predominant paradigm for generative modelling. Their extension to Riemannian manifolds has facilitated their application to an array of problems in the natural sciences. Yet, in many practical settings, such manifolds are defined by a set of constraints and are not covered by the existing (Riemannian) diffusion model methodology. Recent work has attempted to address this issue by employing novel noising processes based on logarithmic barrier methods or reflected Brownian motions. However, the associated samplers are computationally burdensome as the complexity of the constraints increases. In this paper, we introduce an alternative simple noising scheme based on Metropolis sampling that affords substantial gains in computational efficiency and empirical performance compared to the earlier samplers. Of independent interest, we prove that this new process corresponds to a valid discretisation of the reflected Brownian motion. We demonstrate the scalability and flexibility of our approach on a range of problem settings with convex and non-convex constraints, including applications from geospatial modelling, robotics and protein design.","sentences":["Denoising diffusion models have recently emerged as the predominant paradigm for generative modelling.","Their extension to Riemannian manifolds has facilitated their application to an array of problems in the natural sciences.","Yet, in many practical settings, such manifolds are defined by a set of constraints and are not covered by the existing (Riemannian) diffusion model methodology.","Recent work has attempted to address this issue by employing novel noising processes based on logarithmic barrier methods or reflected Brownian motions.","However, the associated samplers are computationally burdensome as the complexity of the constraints increases.","In this paper, we introduce an alternative simple noising scheme based on Metropolis sampling that affords substantial gains in computational efficiency and empirical performance compared to the earlier samplers.","Of independent interest, we prove that this new process corresponds to a valid discretisation of the reflected Brownian motion.","We demonstrate the scalability and flexibility of our approach on a range of problem settings with convex and non-convex constraints, including applications from geospatial modelling, robotics and protein design."],"url":"http://arxiv.org/abs/2307.05439v1"}
{"created":"2023-07-11 17:02:21","title":"Improving the Security of Smartwatch Payment with Deep Learning","abstract":"Making contactless payments using a smartwatch is increasingly popular, but this payment medium lacks traditional biometric security measures such as facial or fingerprint recognition. In 2022, Sturgess et al. proposed WatchAuth, a system for authenticating smartwatch payments using the physical gesture of reaching towards a payment terminal. While effective, the system requires the user to undergo a burdensome enrolment period to achieve acceptable error levels. In this dissertation, we explore whether applications of deep learning can reduce the number of gestures a user must provide to enrol into an authentication system for smartwatch payment. We firstly construct a deep-learned authentication system that outperforms the current state-of-the-art, including in a scenario where the target user has provided a limited number of gestures. We then develop a regularised autoencoder model for generating synthetic user-specific gestures. We show that using these gestures in training improves classification ability for an authentication system. Through this technique we can reduce the number of gestures required to enrol a user into a WatchAuth-like system without negatively impacting its error rates.","sentences":["Making contactless payments using a smartwatch is increasingly popular, but this payment medium lacks traditional biometric security measures such as facial or fingerprint recognition.","In 2022, Sturgess et al. proposed WatchAuth, a system for authenticating smartwatch payments using the physical gesture of reaching towards a payment terminal.","While effective, the system requires the user to undergo a burdensome enrolment period to achieve acceptable error levels.","In this dissertation, we explore whether applications of deep learning can reduce the number of gestures a user must provide to enrol into an authentication system for smartwatch payment.","We firstly construct a deep-learned authentication system that outperforms the current state-of-the-art, including in a scenario where the target user has provided a limited number of gestures.","We then develop a regularised autoencoder model for generating synthetic user-specific gestures.","We show that using these gestures in training improves classification ability for an authentication system.","Through this technique we can reduce the number of gestures required to enrol a user into a WatchAuth-like system without negatively impacting its error rates."],"url":"http://arxiv.org/abs/2307.05437v1"}
{"created":"2023-07-11 16:57:17","title":"One-Versus-Others Attention: Scalable Multimodal Integration","abstract":"Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \\choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with the number of modalities and requires only $n$ attention operations, thus offering a significant reduction in computational complexity compared to existing cross-modal attention algorithms. Using three diverse real-world datasets as well as an additional simulation experiment, we show that our method improves performance compared to popular fusion techniques while decreasing computation costs.","sentences":["Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving.","Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images).","However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion.","Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities.","For $n$ modalities, computing attention will result in $n \\choose 2$ operations, potentially requiring considerable amounts of computational resources.","To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with the number of modalities and requires only $n$ attention operations, thus offering a significant reduction in computational complexity compared to existing cross-modal attention algorithms.","Using three diverse real-world datasets as well as an additional simulation experiment, we show that our method improves performance compared to popular fusion techniques while decreasing computation costs."],"url":"http://arxiv.org/abs/2307.05435v1"}
{"created":"2023-07-11 16:52:22","title":"Self-Supervised Learning with Lie Symmetries for Partial Differential Equations","abstract":"Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation models for PDEs.","sentences":["Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering.","Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete.","In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision.","Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers.","We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation models for PDEs."],"url":"http://arxiv.org/abs/2307.05432v1"}
{"created":"2023-07-11 16:41:02","title":"Let's shake on it: Extracting secure shared keys from Wi-Fi CSI","abstract":"A shared secret key is necessary for encrypted communications. Since Wi-Fi relies on OFDM, we suggest a method to generate such a key by utilizing Wi-Fi's channel state information (CSI). CSI is typically reciprocal but very sensitive to location: While the legitimate Alice and Bob observe the same CSI, an eavesdropper Eve observes an uncorrelated CSI when positioned over 0.5 wavelength away. We show that if endpoint Bob is shaken, sufficient diversity is induced in the CSI so that it can serve as a source of true randomness. Then we show that the CSI among neighboring sub-carriers is correlated, so we select a small set of judiciously-spaced sub-carriers, and use a majority rule around each. We demonstrate that Alice and Bob observe a 5-15\\% bit mismatch rate (BMR) in the extracted bitstream while Eve observes a BMR of around 50\\% even when placed within 10cm of Alice. We employ the cryptography-oriented definition of min-entropy to estimate the number of secure bits within the bitstream, and use the Cascade algorithm of quantum-key-distribution to reconcile Alice and Bob's bitstreams, while quantifying the number of bits leaked by the algorithm. Accounting for both the min-entropy and the cascade leakage we quantify the Secured Bit Generation Rate of our method.   We conducted extensive tests in an indoor environment. Our system exhibits a secure bit generation rate of 1.2--1.6 %secure bits per packet, at distances ranging from 0.5m--9m, and can generate a secure shared 128-bit key with 20sec of device shaking.","sentences":["A shared secret key is necessary for encrypted communications.","Since Wi-Fi relies on OFDM, we suggest a method to generate such a key by utilizing Wi-Fi's channel state information (CSI).","CSI is typically reciprocal but very sensitive to location: While the legitimate Alice and Bob observe the same CSI, an eavesdropper Eve observes an uncorrelated CSI when positioned over 0.5 wavelength away.","We show that if endpoint Bob is shaken, sufficient diversity is induced in the CSI so that it can serve as a source of true randomness.","Then we show that the CSI among neighboring sub-carriers is correlated, so we select a small set of judiciously-spaced sub-carriers, and use a majority rule around each.","We demonstrate that Alice and Bob observe a 5-15\\% bit mismatch rate (BMR) in the extracted bitstream while Eve observes a BMR of around 50\\% even when placed within 10cm of Alice.","We employ the cryptography-oriented definition of min-entropy to estimate the number of secure bits within the bitstream, and use the Cascade algorithm of quantum-key-distribution to reconcile Alice and Bob's bitstreams, while quantifying the number of bits leaked by the algorithm.","Accounting for both the min-entropy and the cascade leakage we quantify the Secured Bit Generation Rate of our method.   ","We conducted extensive tests in an indoor environment.","Our system exhibits a secure bit generation rate of 1.2--1.6 %secure bits per packet, at distances ranging from 0.5m--9m, and can generate a secure shared 128-bit key with 20sec of device shaking."],"url":"http://arxiv.org/abs/2307.05423v1"}
{"created":"2023-07-11 16:39:43","title":"Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection","abstract":"This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario. The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features. To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics. To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples. Then, the five metrics are computed by using the output labels of the corresponding synthetic samples. One contribution of this work is the use of a tiny clean validation dataset. Having the computed five metrics, five novelty detectors are trained from the validation dataset. A meta novelty detector fuses the output of the five trained novelty detectors to generate a meta confidence score. During online testing, our method determines if online samples are poisoned or not via assessing their meta confidence scores output by the meta novelty detector. We show the efficacy of our methodology through a broad range of backdoor attacks, including ablation studies and comparison to existing approaches. Our methodology is promising since the proposed five metrics quantify the inherent differences between clean and poisoned samples. Additionally, our detection method can be incrementally improved by appending more metrics that may be proposed to address future advanced attacks.","sentences":["This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario.","The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features.","To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics.","To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples.","Then, the five metrics are computed by using the output labels of the corresponding synthetic samples.","One contribution of this work is the use of a tiny clean validation dataset.","Having the computed five metrics, five novelty detectors are trained from the validation dataset.","A meta novelty detector fuses the output of the five trained novelty detectors to generate a meta confidence score.","During online testing, our method determines if online samples are poisoned or not via assessing their meta confidence scores output by the meta novelty detector.","We show the efficacy of our methodology through a broad range of backdoor attacks, including ablation studies and comparison to existing approaches.","Our methodology is promising since the proposed five metrics quantify the inherent differences between clean and poisoned samples.","Additionally, our detection method can be incrementally improved by appending more metrics that may be proposed to address future advanced attacks."],"url":"http://arxiv.org/abs/2307.05422v1"}
{"created":"2023-07-11 16:35:04","title":"Channel Selection for Wi-Fi 7 Multi-Link Operation via Optimistic-Weighted VDN and Parallel Transfer Reinforcement Learning","abstract":"Dense and unplanned IEEE 802.11 Wireless Fidelity(Wi-Fi) deployments and the continuous increase of throughput and latency stringent services for users have led to machine learning algorithms to be considered as promising techniques in the industry and the academia. Specifically, the ongoing IEEE 802.11be EHT -- Extremely High Throughput, known as Wi-Fi 7 -- amendment propose, for the first time, Multi-Link Operation (MLO). Among others, this new feature will increase the complexity of channel selection due the novel multiple interfaces proposal. In this paper, we present a Parallel Transfer Reinforcement Learning (PTRL)-based cooperative Multi-Agent Reinforcement Learning (MARL) algorithm named Parallel Transfer Reinforcement Learning Optimistic-Weighted Value Decomposition Networks (oVDN) to improve intelligent channel selection in IEEE 802.11be MLO-capable networks. Additionally, we compare the impact of different parallel transfer learning alternatives and a centralized non-transfer MARL baseline. Two PTRL methods are presented: Multi-Agent System (MAS) Joint Q-function Transfer, where the joint Q-function is transferred and MAS Best/Worst Experience Transfer where the best and worst experiences are transferred among MASs. Simulation results show that oVDNg -- only the best experiences are utilized -- is the best algorithm variant. Moreover, oVDNg offers a gain up to 3%, 7.2% and 11% when compared with VDN, VDN-nonQ and non-PTRL baselines. Furthermore, oVDNg experienced a reward convergence gain in the 5 GHz interface of 33.3% over oVDNb and oVDN where only worst and both types of experiences are considered, respectively. Finally, our best PTRL alternative showed an improvement over the non-PTRL baseline in terms of speed of convergence up to 40 episodes and reward up to 135%.","sentences":["Dense and unplanned IEEE 802.11 Wireless Fidelity(Wi-Fi) deployments and the continuous increase of throughput and latency stringent services for users have led to machine learning algorithms to be considered as promising techniques in the industry and the academia.","Specifically, the ongoing IEEE 802.11be EHT -- Extremely High Throughput, known as Wi-Fi 7 -- amendment propose, for the first time, Multi-Link Operation (MLO).","Among others, this new feature will increase the complexity of channel selection due the novel multiple interfaces proposal.","In this paper, we present a Parallel Transfer Reinforcement Learning (PTRL)-based cooperative Multi-Agent Reinforcement Learning (MARL) algorithm named Parallel Transfer Reinforcement Learning Optimistic-Weighted Value Decomposition Networks (oVDN) to improve intelligent channel selection in IEEE 802.11be MLO-capable networks.","Additionally, we compare the impact of different parallel transfer learning alternatives and a centralized non-transfer MARL baseline.","Two PTRL methods are presented: Multi-Agent System (MAS) Joint Q-function Transfer, where the joint Q-function is transferred and MAS Best/Worst Experience Transfer where the best and worst experiences are transferred among MASs.","Simulation results show that oVDNg -- only the best experiences are utilized -- is the best algorithm variant.","Moreover, oVDNg offers a gain up to 3%, 7.2% and 11% when compared with VDN, VDN-nonQ and non-PTRL baselines.","Furthermore, oVDNg experienced a reward convergence gain in the 5 GHz interface of 33.3% over oVDNb and oVDN where only worst and both types of experiences are considered, respectively.","Finally, our best PTRL alternative showed an improvement over the non-PTRL baseline in terms of speed of convergence up to 40 episodes and reward up to 135%."],"url":"http://arxiv.org/abs/2307.05419v1"}
{"created":"2023-07-11 16:32:58","title":"Optimizing Scientific Data Transfer on Globus with Error-bounded Lossy Compression","abstract":"The increasing volume and velocity of science data necessitate the frequent movement of enormous data volumes as part of routine research activities. As a result, limited wide-area bandwidth often leads to bottlenecks in research progress. However, in many cases, consuming applications (e.g., for analysis, visualization, and machine learning) can achieve acceptable performance on reduced-precision data, and thus researchers may wish to compromise on data precision to reduce transfer and storage costs. Error-bounded lossy compression presents a promising approach as it can significantly reduce data volumes while preserving data integrity based on user-specified error bounds. In this paper, we propose a novel data transfer framework called Ocelot that integrates error-bounded lossy compression into the Globus data transfer infrastructure. We note four key contributions: (1) Ocelot is the first integration of lossy compression in Globus to significantly improve scientific data transfer performance over wide area network (WAN). (2) We propose an effective machine-learning based lossy compression quality estimation model that can predict the quality of error-bounded lossy compressors, which is fundamental to ensure that transferred data are acceptable to users. (3) We develop optimized strategies to reduce the compression time overhead, counter the compute-node waiting time, and improve transfer speed for compressed files. (4) We perform evaluations using many real-world scientific applications across different domains and distributed Globus endpoints. Our experiments show that Ocelot can improve dataset transfer performance substantially, and the quality of lossy compression (time, ratio and data distortion) can be predicted accurately for the purpose of quality assurance.","sentences":["The increasing volume and velocity of science data necessitate the frequent movement of enormous data volumes as part of routine research activities.","As a result, limited wide-area bandwidth often leads to bottlenecks in research progress.","However, in many cases, consuming applications (e.g., for analysis, visualization, and machine learning) can achieve acceptable performance on reduced-precision data, and thus researchers may wish to compromise on data precision to reduce transfer and storage costs.","Error-bounded lossy compression presents a promising approach as it can significantly reduce data volumes while preserving data integrity based on user-specified error bounds.","In this paper, we propose a novel data transfer framework called Ocelot that integrates error-bounded lossy compression into the Globus data transfer infrastructure.","We note four key contributions: (1) Ocelot is the first integration of lossy compression in Globus to significantly improve scientific data transfer performance over wide area network (WAN).","(2) We propose an effective machine-learning based lossy compression quality estimation model that can predict the quality of error-bounded lossy compressors, which is fundamental to ensure that transferred data are acceptable to users.","(3) We develop optimized strategies to reduce the compression time overhead, counter the compute-node waiting time, and improve transfer speed for compressed files.","(4) We perform evaluations using many real-world scientific applications across different domains and distributed Globus endpoints.","Our experiments show that Ocelot can improve dataset transfer performance substantially, and the quality of lossy compression (time, ratio and data distortion) can be predicted accurately for the purpose of quality assurance."],"url":"http://arxiv.org/abs/2307.05416v1"}
{"created":"2023-07-11 16:30:45","title":"Duncode Characters Shorter","abstract":"This paper investigates the employment of various encoders in text transformation, converting characters into bytes. It discusses local encoders such as ASCII and GB-2312, which encode specific characters into shorter bytes, and universal encoders like UTF-8 and UTF-16, which can encode the complete Unicode set with greater space requirements and are gaining widespread acceptance. Other encoders, including SCSU, BOCU-1, and binary encoders, however, lack self-synchronizing capabilities. Duncode is introduced as an innovative encoding method that aims to encode the entire Unicode character set with high space efficiency, akin to local encoders. It has the potential to compress multiple characters of a string into a Duncode unit using fewer bytes. Despite offering less self-synchronizing identification information, Duncode surpasses UTF8 in terms of space efficiency. The application is available at \\url{https://github.com/laohur/duncode}. Additionally, we have developed a benchmark for evaluating character encoders across different languages. It encompasses 179 languages and can be accessed at \\url{https://github.com/laohur/wiki2txt}.","sentences":["This paper investigates the employment of various encoders in text transformation, converting characters into bytes.","It discusses local encoders such as ASCII and GB-2312, which encode specific characters into shorter bytes, and universal encoders like UTF-8 and UTF-16, which can encode the complete Unicode set with greater space requirements and are gaining widespread acceptance.","Other encoders, including SCSU, BOCU-1, and binary encoders, however, lack self-synchronizing capabilities.","Duncode is introduced as an innovative encoding method that aims to encode the entire Unicode character set with high space efficiency, akin to local encoders.","It has the potential to compress multiple characters of a string into a Duncode unit using fewer bytes.","Despite offering less self-synchronizing identification information, Duncode surpasses UTF8 in terms of space efficiency.","The application is available at \\url{https://github.com/laohur/duncode}.","Additionally, we have developed a benchmark for evaluating character encoders across different languages.","It encompasses 179 languages and can be accessed at \\url{https://github.com/laohur/wiki2txt}."],"url":"http://arxiv.org/abs/2307.05414v1"}
{"created":"2023-07-11 16:25:09","title":"BLUEX: A benchmark based on Brazilian Leading Universities Entrance eXams","abstract":"One common trend in recent studies of language models (LMs) is the use of standardized tests for evaluation. However, despite being the fifth most spoken language worldwide, few such evaluations have been conducted in Portuguese. This is mainly due to the lack of high-quality datasets available to the community for carrying out evaluations in Portuguese. To address this gap, we introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset of entrance exams from the two leading universities in Brazil: UNICAMP and USP. The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects. Furthermore, BLUEX includes a collection of recently administered exams that are unlikely to be included in the training data of many popular LMs as of 2023. The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning. We describe the creation and characteristics of BLUEX and establish a benchmark through experiments with state-of-the-art LMs, demonstrating its potential for advancing the state-of-the-art in natural language understanding and reasoning in Portuguese. The data and relevant code can be found at https://github.com/Portuguese-Benchmark-Datasets/BLUEX","sentences":["One common trend in recent studies of language models (LMs) is the use of standardized tests for evaluation.","However, despite being the fifth most spoken language worldwide, few such evaluations have been conducted in Portuguese.","This is mainly due to the lack of high-quality datasets available to the community for carrying out evaluations in Portuguese.","To address this gap, we introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset of entrance exams from the two leading universities in Brazil: UNICAMP and USP.","The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects.","Furthermore, BLUEX includes a collection of recently administered exams that are unlikely to be included in the training data of many popular LMs as of 2023.","The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning.","We describe the creation and characteristics of BLUEX and establish a benchmark through experiments with state-of-the-art LMs, demonstrating its potential for advancing the state-of-the-art in natural language understanding and reasoning in Portuguese.","The data and relevant code can be found at https://github.com/Portuguese-Benchmark-Datasets/BLUEX"],"url":"http://arxiv.org/abs/2307.05410v1"}
{"created":"2023-07-11 16:23:19","title":"3D detection of roof sections from a single satellite image and application to LOD2-building reconstruction","abstract":"Reconstructing urban areas in 3D out of satellite raster images has been a long-standing and challenging goal of both academical and industrial research. The rare methods today achieving this objective at a Level Of Details $2$ rely on procedural approaches based on geometry, and need stereo images and/or LIDAR data as input. We here propose a method for urban 3D reconstruction named KIBS(\\textit{Keypoints Inference By Segmentation}), which comprises two novel features: i) a full deep learning approach for the 3D detection of the roof sections, and ii) only one single (non-orthogonal) satellite raster image as model input. This is achieved in two steps: i) by a Mask R-CNN model performing a 2D segmentation of the buildings' roof sections, and after blending these latter segmented pixels within the RGB satellite raster image, ii) by another identical Mask R-CNN model inferring the heights-to-ground of the roof sections' corners via panoptic segmentation, unto full 3D reconstruction of the buildings and city. We demonstrate the potential of the KIBS method by reconstructing different urban areas in a few minutes, with a Jaccard index for the 2D segmentation of individual roof sections of $88.55\\%$ and $75.21\\%$ on our two data sets resp., and a height's mean error of such correctly segmented pixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets resp., hence within the LOD2 precision range.","sentences":["Reconstructing urban areas in 3D out of satellite raster images has been a long-standing and challenging goal of both academical and industrial research.","The rare methods today achieving this objective at a Level Of Details $2$ rely on procedural approaches based on geometry, and need stereo images and/or LIDAR data as input.","We here propose a method for urban 3D reconstruction named KIBS(\\textit{Keypoints Inference By Segmentation}), which comprises two novel features: i) a full deep learning approach for the 3D detection of the roof sections, and ii) only one single (non-orthogonal) satellite raster image as model input.","This is achieved in two steps: i) by a Mask R-CNN model performing a 2D segmentation of the buildings' roof sections, and after blending these latter segmented pixels within the RGB satellite raster image, ii) by another identical Mask R-CNN model inferring the heights-to-ground of the roof sections' corners via panoptic segmentation, unto full 3D reconstruction of the buildings and city.","We demonstrate the potential of the KIBS method by reconstructing different urban areas in a few minutes, with a Jaccard index for the 2D segmentation of individual roof sections of $88.55\\%$ and $75.21\\%$ on our two data sets resp., and a height's mean error of such correctly segmented pixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets resp., hence within the LOD2 precision range."],"url":"http://arxiv.org/abs/2307.05409v1"}
{"created":"2023-07-11 16:12:15","title":"Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores","abstract":"Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores, while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL.","sentences":["Interactive reinforcement learning has shown promise in learning complex robotic tasks.","However, the process can be human-intensive due to the requirement of large amount of interactive feedback.","This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning.","Our key insight is that scores can yield significantly more data than pairwise preferences.","Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment.","To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme.","This enables the learning paradigm to be insensitive to imperfect or unreliable scores.","We extensively evaluate our method on robotic locomotion and manipulation tasks.","The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores, while requiring less feedback compared to pairwise preference learning methods.","The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL."],"url":"http://arxiv.org/abs/2307.05405v1"}
{"created":"2023-07-11 16:06:40","title":"Complexity results for matching cut problems in graphs without long induced paths","abstract":"In a graph, a (perfect) matching cut is an edge cut that is a (perfect) matching. Matching Cut (MC), respectively, Perfect Matching Cut (PMC), is the problem of deciding whether a given graph has a matching cut, respectively, a perfect matching cut. The Disconnected Perfect Matching problem (DPM) is to decide if a graph has a perfect matching that contains a matching cut. Solving an open problem recently posed in [Lucke, Paulusma, Ries (ISAAC 2022), and Feghali, Lucke, Paulusma, Ries (arXiv:2212.12317)], we show that PMC is NP-complete in graphs without induced 14-vertex path $P_{14}$. Our reduction also works simultaneously for MC and DPM, improving the previous hardness results of MC on $P_{19}$-free graphs and of DPM on $P_{23}$-free graphs to $P_{14}$-free graphs for both problems.   Actually, we prove a slightly stronger result: within $P_{14}$-free graphs, it is hard to distinguish between (i) those without matching cuts and those in which every matching cut is a perfect matching cut, (ii) those without perfect matching cuts and those in which every matching cut is a perfect matching cut, and (iii) those without disconnected perfect matchings and those in which every matching cut is a perfect matching cut.   Moreover, assuming the Exponential Time Hypothesis, none of these problems can be solved in time $2^{o(n)}$ for $n$-vertex $P_{14}$-free input graphs.   We also consider the problems in graphs without long induced cycles. It is known that MC is polynomially solvable in graphs without induced cycles of length at least 5 [Moshi (JGT 1989)]. We point out that the same holds for DPM.","sentences":["In a graph, a (perfect) matching cut is an edge cut that is a (perfect) matching.","Matching Cut (MC), respectively, Perfect Matching Cut (PMC), is the problem of deciding whether a given graph has a matching cut, respectively, a perfect matching cut.","The Disconnected Perfect Matching problem (DPM) is to decide if a graph has a perfect matching that contains a matching cut.","Solving an open problem recently posed in [Lucke, Paulusma, Ries (ISAAC 2022), and Feghali, Lucke, Paulusma, Ries (arXiv:2212.12317)], we show that PMC is NP-complete in graphs without induced 14-vertex path $P_{14}$. Our reduction also works simultaneously for MC and DPM, improving the previous hardness results of MC on $P_{19}$-free graphs and of DPM on $P_{23}$-free graphs to $P_{14}$-free graphs for both problems.   ","Actually, we prove a slightly stronger result: within $P_{14}$-free graphs, it is hard to distinguish between (i) those without matching cuts and those in which every matching cut is a perfect matching cut, (ii) those without perfect matching cuts and those in which every matching cut is a perfect matching cut, and (iii) those without disconnected perfect matchings and those in which every matching cut is a perfect matching cut.   ","Moreover, assuming the Exponential Time Hypothesis, none of these problems can be solved in time $2^{o(n)}$ for $n$-vertex $P_{14}$-free input graphs.   ","We also consider the problems in graphs without long induced cycles.","It is known that MC is polynomially solvable in graphs without induced cycles of length at least 5 [Moshi (JGT 1989)].","We point out that the same holds for DPM."],"url":"http://arxiv.org/abs/2307.05402v1"}
{"created":"2023-07-11 16:01:44","title":"Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform","abstract":"Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately. We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods.","sentences":["Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks.","Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately.","Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios.","For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately.","We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments.","The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods."],"url":"http://arxiv.org/abs/2307.05399v1"}
{"created":"2023-07-11 15:57:51","title":"On the Vulnerability of DeepFake Detectors to Attacks Generated by Denoising Diffusion Models","abstract":"The detection of malicious Deepfakes is a constantly evolving problem, that requires continuous monitoring of detectors, to ensure they are able to detect image manipulations generated by the latest emerging models. In this paper, we present a preliminary study that investigates the vulnerability of single-image Deepfake detectors to attacks created by a representative of the newest generation of generative methods, i.e. Denoising Diffusion Models (DDMs). Our experiments are run on FaceForensics++, a commonly used benchmark dataset, consisting of Deepfakes generated with various techniques for face swapping and face reenactment. The analysis shows, that reconstructing existing Deepfakes with only one denoising diffusion step significantly decreases the accuracy of all tested detectors, without introducing visually perceptible image changes.","sentences":["The detection of malicious Deepfakes is a constantly evolving problem, that requires continuous monitoring of detectors, to ensure they are able to detect image manipulations generated by the latest emerging models.","In this paper, we present a preliminary study that investigates the vulnerability of single-image Deepfake detectors to attacks created by a representative of the newest generation of generative methods, i.e. Denoising Diffusion Models (DDMs).","Our experiments are run on FaceForensics++, a commonly used benchmark dataset, consisting of Deepfakes generated with various techniques for face swapping and face reenactment.","The analysis shows, that reconstructing existing Deepfakes with only one denoising diffusion step significantly decreases the accuracy of all tested detectors, without introducing visually perceptible image changes."],"url":"http://arxiv.org/abs/2307.05397v1"}
{"created":"2023-07-11 15:57:15","title":"Handwritten Text Recognition Using Convolutional Neural Network","abstract":"OCR (Optical Character Recognition) is a technology that offers comprehensive alphanumeric recognition of handwritten and printed characters at electronic speed by merely scanning the document. Recently, the understanding of visual data has been termed Intelligent Character Recognition (ICR). Intelligent Character Recognition (ICR) is the OCR module that can convert scans of handwritten or printed characters into ASCII text. ASCII data is the standard format for data encoding in electronic communication. ASCII assigns standard numeric values to letters, numeral, symbols, white-spaces and other characters. In more technical terms, OCR is the process of using an electronic device to transform 2-Dimensional textual information into machine-encoded text. Anything that contains text both machine written or handwritten can be scanned either through a scanner or just simply a picture of the text is enough for the recognition system to distinguish the text. The goal of this papers is to show the results of a Convolutional Neural Network model which has been trained on National Institute of Science and Technology (NIST) dataset containing over a 100,000 images. The network learns from the features extracted from the images and use it to generate the probability of each class to which the picture belongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.","sentences":["OCR (Optical Character Recognition) is a technology that offers comprehensive alphanumeric recognition of handwritten and printed characters at electronic speed by merely scanning the document.","Recently, the understanding of visual data has been termed Intelligent Character Recognition (ICR).","Intelligent Character Recognition (ICR) is the OCR module that can convert scans of handwritten or printed characters into ASCII text.","ASCII data is the standard format for data encoding in electronic communication.","ASCII assigns standard numeric values to letters, numeral, symbols, white-spaces and other characters.","In more technical terms, OCR is the process of using an electronic device to transform 2-Dimensional textual information into machine-encoded text.","Anything that contains text both machine written or handwritten can be scanned either through a scanner or just simply a picture of the text is enough for the recognition system to distinguish the text.","The goal of this papers is to show the results of a Convolutional Neural Network model which has been trained on National Institute of Science and Technology (NIST) dataset containing over a 100,000 images.","The network learns from the features extracted from the images and use it to generate the probability of each class to which the picture belongs to.","We have achieved an accuracy of 90.54% with a loss of 2.53%."],"url":"http://arxiv.org/abs/2307.05396v1"}
{"created":"2023-07-11 15:46:20","title":"From academic to media capital: To what extent does the scientific reputation of universities translate into Wikipedia attention?","abstract":"Universities face increasing demands to improve their visibility, public outreach, and online presence. There is a broad consensus that scientific reputation significantly increases the attention universities receive. However, in most cases estimates of scientific reputation are based on composite or weighted indicators and absolute positions in university rankings. In this study, we adopt a more granular approach to assessment of universities' scientific performance using a multidimensional set of indicators from the Leiden Ranking and testing their individual effects on university Wikipedia page views. We distinguish between international and local attention and find a positive association between research performance and Wikipedia attention which holds for regions and linguistic areas. Additional analysis shows that productivity, scientific impact, and international collaboration have a curvilinear effect on universities' Wikipedia attention. This finding suggests that there may be other factors than scientific reputation driving the general public's interest in universities. Our study adds to a growing stream of work which views altmetrics as tools to deepen science-society interactions rather than direct measures of impact and recognition of scientific outputs.","sentences":["Universities face increasing demands to improve their visibility, public outreach, and online presence.","There is a broad consensus that scientific reputation significantly increases the attention universities receive.","However, in most cases estimates of scientific reputation are based on composite or weighted indicators and absolute positions in university rankings.","In this study, we adopt a more granular approach to assessment of universities' scientific performance using a multidimensional set of indicators from the Leiden Ranking and testing their individual effects on university Wikipedia page views.","We distinguish between international and local attention and find a positive association between research performance and Wikipedia attention which holds for regions and linguistic areas.","Additional analysis shows that productivity, scientific impact, and international collaboration have a curvilinear effect on universities' Wikipedia attention.","This finding suggests that there may be other factors than scientific reputation driving the general public's interest in universities.","Our study adds to a growing stream of work which views altmetrics as tools to deepen science-society interactions rather than direct measures of impact and recognition of scientific outputs."],"url":"http://arxiv.org/abs/2307.05366v1"}
{"created":"2023-07-11 15:45:03","title":"Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators","abstract":"Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored for unlabeled instances in each client. We further formulate the client model training as bi-level optimization that adaptively optimizes the model in the client with two regulators. Theoretically, we show the convergence guarantee of the dual regulators. Empirically, we demonstrate that FedDure is superior to the existing methods across a wide range of settings, notably by more than 11% on CIFAR-10 and CINIC-10 datasets.","sentences":["Federated learning has become a popular method to learn from decentralized heterogeneous data.","Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients.","Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client.","This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data.","To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.}","FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored for unlabeled instances in each client.","We further formulate the client model training as bi-level optimization that adaptively optimizes the model in the client with two regulators.","Theoretically, we show the convergence guarantee of the dual regulators.","Empirically, we demonstrate that FedDure is superior to the existing methods across a wide range of settings, notably by more than 11% on CIFAR-10 and CINIC-10 datasets."],"url":"http://arxiv.org/abs/2307.05358v1"}
{"created":"2023-07-11 15:44:01","title":"GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts","abstract":"In the context of the rapid development of large language models, we have meticulously trained and introduced the GujiBERT and GujiGPT language models, which are foundational models specifically designed for intelligent information processing of ancient texts. These models have been trained on an extensive dataset that encompasses both simplified and traditional Chinese characters, allowing them to effectively handle various natural language processing tasks related to ancient books, including but not limited to automatic sentence segmentation, punctuation, word segmentation, part-of-speech tagging, entity recognition, and automatic translation. Notably, these models have exhibited exceptional performance across a range of validation tasks using publicly available datasets. Our research findings highlight the efficacy of employing self-supervised methods to further train the models using classical text corpora, thus enhancing their capability to tackle downstream tasks. Moreover, it is worth emphasizing that the choice of font, the scale of the corpus, and the initial model selection all exert significant influence over the ultimate experimental outcomes. To cater to the diverse text processing preferences of researchers in digital humanities and linguistics, we have developed three distinct categories comprising a total of nine model variations. We believe that by sharing these foundational language models specialized in the domain of ancient texts, we can facilitate the intelligent processing and scholarly exploration of ancient literary works and, consequently, contribute to the global dissemination of China's rich and esteemed traditional culture in this new era.","sentences":["In the context of the rapid development of large language models, we have meticulously trained and introduced the GujiBERT and GujiGPT language models, which are foundational models specifically designed for intelligent information processing of ancient texts.","These models have been trained on an extensive dataset that encompasses both simplified and traditional Chinese characters, allowing them to effectively handle various natural language processing tasks related to ancient books, including but not limited to automatic sentence segmentation, punctuation, word segmentation, part-of-speech tagging, entity recognition, and automatic translation.","Notably, these models have exhibited exceptional performance across a range of validation tasks using publicly available datasets.","Our research findings highlight the efficacy of employing self-supervised methods to further train the models using classical text corpora, thus enhancing their capability to tackle downstream tasks.","Moreover, it is worth emphasizing that the choice of font, the scale of the corpus, and the initial model selection all exert significant influence over the ultimate experimental outcomes.","To cater to the diverse text processing preferences of researchers in digital humanities and linguistics, we have developed three distinct categories comprising a total of nine model variations.","We believe that by sharing these foundational language models specialized in the domain of ancient texts, we can facilitate the intelligent processing and scholarly exploration of ancient literary works and, consequently, contribute to the global dissemination of China's rich and esteemed traditional culture in this new era."],"url":"http://arxiv.org/abs/2307.05354v1"}
{"created":"2023-07-11 15:26:49","title":"Explaining Competitive-Level Programming Solutions using LLMs","abstract":"In this paper, we approach competitive-level programming problem-solving as a composite task of reasoning and code generation. We propose a novel method to automatically annotate natural language explanations to \\textit{<problem, solution>} pairs. We show that despite poor performance in solving competitive-level programming problems, state-of-the-art LLMs exhibit a strong capacity in describing and explaining solutions. Our explanation generation methodology can generate a structured solution explanation for the problem containing descriptions and analysis. To evaluate the quality of the annotated explanations, we examine their effectiveness in two aspects: 1) satisfying the human programming expert who authored the oracle solution, and 2) aiding LLMs in solving problems more effectively. The experimental results on the CodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities in describing the solution are comparable, GPT-4 shows a better understanding of the key idea behind the solution.","sentences":["In this paper, we approach competitive-level programming problem-solving as a composite task of reasoning and code generation.","We propose a novel method to automatically annotate natural language explanations to \\textit{<problem, solution>} pairs.","We show that despite poor performance in solving competitive-level programming problems, state-of-the-art LLMs exhibit a strong capacity in describing and explaining solutions.","Our explanation generation methodology can generate a structured solution explanation for the problem containing descriptions and analysis.","To evaluate the quality of the annotated explanations, we examine their effectiveness in two aspects: 1) satisfying the human programming expert who authored the oracle solution, and 2) aiding LLMs in solving problems more effectively.","The experimental results on the CodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities in describing the solution are comparable, GPT-4 shows a better understanding of the key idea behind the solution."],"url":"http://arxiv.org/abs/2307.05337v1"}
{"created":"2023-07-11 15:19:47","title":"ProgGP: From GuitarPro Tablature Neural Generation To Progressive Metal Production","abstract":"Recent work in the field of symbolic music generation has shown value in using a tokenization based on the GuitarPro format, a symbolic representation supporting guitar expressive attributes, as an input and output representation. We extend this work by fine-tuning a pre-trained Transformer model on ProgGP, a custom dataset of 173 progressive metal songs, for the purposes of creating compositions from that genre through a human-AI partnership. Our model is able to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We examine the validity of the generated music using a mixed methods approach by combining quantitative analyses following a computational musicology paradigm and qualitative analyses following a practice-based research paradigm. Finally, we demonstrate the value of the model by using it as a tool to create a progressive metal song, fully produced and mixed by a human metal producer based on AI-generated music.","sentences":["Recent work in the field of symbolic music generation has shown value in using a tokenization based on the GuitarPro format, a symbolic representation supporting guitar expressive attributes, as an input and output representation.","We extend this work by fine-tuning a pre-trained Transformer model on ProgGP, a custom dataset of 173 progressive metal songs, for the purposes of creating compositions from that genre through a human-AI partnership.","Our model is able to generate multiple guitar, bass guitar, drums, piano and orchestral parts.","We examine the validity of the generated music using a mixed methods approach by combining quantitative analyses following a computational musicology paradigm and qualitative analyses following a practice-based research paradigm.","Finally, we demonstrate the value of the model by using it as a tool to create a progressive metal song, fully produced and mixed by a human metal producer based on AI-generated music."],"url":"http://arxiv.org/abs/2307.05328v1"}
{"created":"2023-07-11 15:11:06","title":"Self-supervised adversarial masking for 3D point cloud representation learning","abstract":"Self-supervised methods have been proven effective for learning deep representations of 3D point cloud data. Although recent methods in this domain often rely on random masking of inputs, the results of this approach can be improved. We introduce PointCAM, a novel adversarial method for learning a masking function for point clouds. Our model utilizes a self-distillation framework with an online tokenizer for 3D point clouds. Compared to previous techniques that optimize patch-level and object-level objectives, we postulate applying an auxiliary network that learns how to select masks instead of choosing them randomly. Our results show that the learned masking function achieves state-of-the-art or competitive performance on various downstream tasks. The source code is available at https://github.com/szacho/pointcam.","sentences":["Self-supervised methods have been proven effective for learning deep representations of 3D point cloud data.","Although recent methods in this domain often rely on random masking of inputs, the results of this approach can be improved.","We introduce PointCAM, a novel adversarial method for learning a masking function for point clouds.","Our model utilizes a self-distillation framework with an online tokenizer for 3D point clouds.","Compared to previous techniques that optimize patch-level and object-level objectives, we postulate applying an auxiliary network that learns how to select masks instead of choosing them randomly.","Our results show that the learned masking function achieves state-of-the-art or competitive performance on various downstream tasks.","The source code is available at https://github.com/szacho/pointcam."],"url":"http://arxiv.org/abs/2307.05325v1"}
{"created":"2023-07-11 15:11:02","title":"ShredGP: Guitarist Style-Conditioned Tablature Generation","abstract":"GuitarPro format tablatures are a type of digital music notation that encapsulates information about guitar playing techniques and fingerings. We introduce ShredGP, a GuitarPro tablature generative Transformer-based model conditioned to imitate the style of four distinct iconic electric guitarists. In order to assess the idiosyncrasies of each guitar player, we adopt a computational musicology methodology by analysing features computed from the tokens yielded by the DadaGP encoding scheme. Statistical analyses of the features evidence significant differences between the four guitarists. We trained two variants of the ShredGP model, one using a multi-instrument corpus, the other using solo guitar data. We present a BERT-based model for guitar player classification and use it to evaluate the generated examples. Overall, results from the classifier show that ShredGP is able to generate content congruent with the style of the targeted guitar player. Finally, we reflect on prospective applications for ShredGP for human-AI music interaction.","sentences":["GuitarPro format tablatures are a type of digital music notation that encapsulates information about guitar playing techniques and fingerings.","We introduce ShredGP, a GuitarPro tablature generative Transformer-based model conditioned to imitate the style of four distinct iconic electric guitarists.","In order to assess the idiosyncrasies of each guitar player, we adopt a computational musicology methodology by analysing features computed from the tokens yielded by the DadaGP encoding scheme.","Statistical analyses of the features evidence significant differences between the four guitarists.","We trained two variants of the ShredGP model, one using a multi-instrument corpus, the other using solo guitar data.","We present a BERT-based model for guitar player classification and use it to evaluate the generated examples.","Overall, results from the classifier show that ShredGP is able to generate content congruent with the style of the targeted guitar player.","Finally, we reflect on prospective applications for ShredGP for human-AI music interaction."],"url":"http://arxiv.org/abs/2307.05324v1"}
{"created":"2023-07-11 15:09:10","title":"Class Instance Balanced Learning for Long-Tailed Classification","abstract":"The long-tailed image classification task remains important in the development of deep neural networks as it explicitly deals with large imbalances in the class frequencies of the training data. While uncommon in engineered datasets, this imbalance is almost always present in real-world data. Previous approaches have shown that combining cross-entropy and contrastive learning can improve performance on the long-tailed task, but they do not explore the tradeoff between head and tail classes. We propose a novel class instance balanced loss (CIBL), which reweights the relative contributions of a cross-entropy and a contrastive loss as a function of the frequency of class instances in the training batch. This balancing favours the contrastive loss for more common classes, leading to a learned classifier with a more balanced performance across all class frequencies. Furthermore, increasing the relative weight on the contrastive head shifts performance from common (head) to rare (tail) classes, allowing the user to skew the performance towards these classes if desired. We also show that changing the linear classifier head with a cosine classifier yields a network that can be trained to similar performance in substantially fewer epochs. We obtain competitive results on both CIFAR-100-LT and ImageNet-LT.","sentences":["The long-tailed image classification task remains important in the development of deep neural networks as it explicitly deals with large imbalances in the class frequencies of the training data.","While uncommon in engineered datasets, this imbalance is almost always present in real-world data.","Previous approaches have shown that combining cross-entropy and contrastive learning can improve performance on the long-tailed task, but they do not explore the tradeoff between head and tail classes.","We propose a novel class instance balanced loss (CIBL), which reweights the relative contributions of a cross-entropy and a contrastive loss as a function of the frequency of class instances in the training batch.","This balancing favours the contrastive loss for more common classes, leading to a learned classifier with a more balanced performance across all class frequencies.","Furthermore, increasing the relative weight on the contrastive head shifts performance from common (head) to rare (tail) classes, allowing the user to skew the performance towards these classes if desired.","We also show that changing the linear classifier head with a cosine classifier yields a network that can be trained to similar performance in substantially fewer epochs.","We obtain competitive results on both CIFAR-100-LT and ImageNet-LT."],"url":"http://arxiv.org/abs/2307.05322v1"}
{"created":"2023-07-11 15:01:42","title":"Automatic Generation of Semantic Parts for Face Image Synthesis","abstract":"Semantic image synthesis (SIS) refers to the problem of generating realistic imagery given a semantic segmentation mask that defines the spatial layout of object classes. Most of the approaches in the literature, other than the quality of the generated images, put effort in finding solutions to increase the generation diversity in terms of style i.e. texture. However, they all neglect a different feature, which is the possibility of manipulating the layout provided by the mask. Currently, the only way to do so is manually by means of graphical users interfaces. In this paper, we describe a network architecture to address the problem of automatically manipulating or generating the shape of object classes in semantic segmentation masks, with specific focus on human faces. Our proposed model allows embedding the mask class-wise into a latent space where each class embedding can be independently edited. Then, a bi-directional LSTM block and a convolutional decoder output a new, locally manipulated mask. We report quantitative and qualitative results on the CelebMask-HQ dataset, which show our model can both faithfully reconstruct and modify a segmentation mask at the class level. Also, we show our model can be put before a SIS generator, opening the way to a fully automatic generation control of both shape and texture. Code available at https://github.com/TFonta/Semantic-VAE.","sentences":["Semantic image synthesis (SIS) refers to the problem of generating realistic imagery given a semantic segmentation mask that defines the spatial layout of object classes.","Most of the approaches in the literature, other than the quality of the generated images, put effort in finding solutions to increase the generation diversity in terms of style i.e. texture.","However, they all neglect a different feature, which is the possibility of manipulating the layout provided by the mask.","Currently, the only way to do so is manually by means of graphical users interfaces.","In this paper, we describe a network architecture to address the problem of automatically manipulating or generating the shape of object classes in semantic segmentation masks, with specific focus on human faces.","Our proposed model allows embedding the mask class-wise into a latent space where each class embedding can be independently edited.","Then, a bi-directional LSTM block and a convolutional decoder output a new, locally manipulated mask.","We report quantitative and qualitative results on the CelebMask-HQ dataset, which show our model can both faithfully reconstruct and modify a segmentation mask at the class level.","Also, we show our model can be put before a SIS generator, opening the way to a fully automatic generation control of both shape and texture.","Code available at https://github.com/TFonta/Semantic-VAE."],"url":"http://arxiv.org/abs/2307.05317v1"}
{"created":"2023-07-11 15:00:11","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering","abstract":"Medical visual question answering (VQA) is a challenging task that requires answering clinical questions of a given medical image, by taking consider of both visual and language information. However, due to the small scale of training data for medical VQA, pre-training fine-tuning paradigms have been a commonly used solution to improve model generalization performance. In this paper, we present a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimodal and multimodal contrastive losses, along with masked language modeling and image text matching as pretraining objectives. The pre-trained model is then transferred to downstream medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA) performance on three publicly available medical VQA datasets with significant accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we conduct a comprehensive analysis to validate the effectiveness of different components of the approach and study different pre-training settings. Our codes and models are available at https://github.com/pengfeiliHEU/MUMC.","sentences":["Medical visual question answering (VQA) is a challenging task that requires answering clinical questions of a given medical image, by taking consider of both visual and language information.","However, due to the small scale of training data for medical VQA, pre-training fine-tuning paradigms have been a commonly used solution to improve model generalization performance.","In this paper, we present a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimodal and multimodal contrastive losses, along with masked language modeling and image text matching as pretraining objectives.","The pre-trained model is then transferred to downstream medical VQA tasks.","The proposed approach achieves state-of-the-art (SOTA) performance on three publicly available medical VQA datasets with significant accuracy improvements of 2.2%, 14.7%, and 1.7% respectively.","Besides, we conduct a comprehensive analysis to validate the effectiveness of different components of the approach and study different pre-training settings.","Our codes and models are available at https://github.com/pengfeiliHEU/MUMC."],"url":"http://arxiv.org/abs/2307.05314v1"}
{"created":"2023-07-11 14:45:19","title":"Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration","abstract":"Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assigning multiple, fine-grained personas in LLMs elicits better problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP effectively elicits internal knowledge acquisition abilities, reduces hallucination, and maintains strong reasoning capabilities. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.","sentences":["Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation.","Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning.","In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas.","A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks.","By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs.","We have discovered that assigning multiple, fine-grained personas in LLMs elicits better problem-solving abilities compared to using a single or fixed number of personas.","We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types.","Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP effectively elicits internal knowledge acquisition abilities, reduces hallucination, and maintains strong reasoning capabilities.","Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git."],"url":"http://arxiv.org/abs/2307.05300v1"}
{"created":"2023-07-11 14:43:25","title":"Discovering Symbolic Laws Directly from Trajectories with Hamiltonian Graph Neural Networks","abstract":"The time evolution of physical systems is described by differential equations, which depend on abstract quantities like energy and force. Traditionally, these quantities are derived as functionals based on observables such as positions and velocities. Discovering these governing symbolic laws is the key to comprehending the interactions in nature. Here, we present a Hamiltonian graph neural network (HGNN), a physics-enforced GNN that learns the dynamics of systems directly from their trajectory. We demonstrate the performance of HGNN on n-springs, n-pendulums, gravitational systems, and binary Lennard Jones systems; HGNN learns the dynamics in excellent agreement with the ground truth from small amounts of data. We also evaluate the ability of HGNN to generalize to larger system sizes, and to hybrid spring-pendulum system that is a combination of two original systems (spring and pendulum) on which the models are trained independently. Finally, employing symbolic regression on the learned HGNN, we infer the underlying equations relating the energy functionals, even for complex systems such as the binary Lennard-Jones liquid. Our framework facilitates the interpretable discovery of interaction laws directly from physical system trajectories. Furthermore, this approach can be extended to other systems with topology-dependent dynamics, such as cells, polydisperse gels, or deformable bodies.","sentences":["The time evolution of physical systems is described by differential equations, which depend on abstract quantities like energy and force.","Traditionally, these quantities are derived as functionals based on observables such as positions and velocities.","Discovering these governing symbolic laws is the key to comprehending the interactions in nature.","Here, we present a Hamiltonian graph neural network (HGNN), a physics-enforced GNN that learns the dynamics of systems directly from their trajectory.","We demonstrate the performance of HGNN on n-springs, n-pendulums, gravitational systems, and binary Lennard Jones systems; HGNN learns the dynamics in excellent agreement with the ground truth from small amounts of data.","We also evaluate the ability of HGNN to generalize to larger system sizes, and to hybrid spring-pendulum system that is a combination of two original systems (spring and pendulum) on which the models are trained independently.","Finally, employing symbolic regression on the learned HGNN, we infer the underlying equations relating the energy functionals, even for complex systems such as the binary Lennard-Jones liquid.","Our framework facilitates the interpretable discovery of interaction laws directly from physical system trajectories.","Furthermore, this approach can be extended to other systems with topology-dependent dynamics, such as cells, polydisperse gels, or deformable bodies."],"url":"http://arxiv.org/abs/2307.05299v1"}
{"created":"2023-07-11 14:38:50","title":"Optimization of Rate-Splitting Multiple Access in Beyond Diagonal RIS-assisted URLLC Systems","abstract":"This paper proposes a general optimization framework for rate splitting multiple access (RSMA) in beyond diagonal (BD) reconfigurable intelligent surface (RIS) assisted ultra-reliable low-latency communications (URLLC) systems. This framework can solve a large family of optimization problems in which the objective and/or constraints are linear functions of the rates and/or energy efficiency (EE) of users. Using this framework, we show that RSMA and RIS can be mutually beneficial tools when the system is overloaded, i.e., when the number of users per cell is higher than the number of base station (BS) antennas. Additionally, we show that the benefits of RSMA increase when the packets are shorter and/or the reliability constraint is more stringent. Furthermore, we show that the RSMA benefits increase with the number of users per cell and decrease with the number of BS antennas. Finally, we show that RIS (either diagonal or BD) can highly improve the system performance, and BD-RIS outperforms regular RIS.","sentences":["This paper proposes a general optimization framework for rate splitting multiple access (RSMA) in beyond diagonal (BD) reconfigurable intelligent surface (RIS) assisted ultra-reliable low-latency communications (URLLC) systems.","This framework can solve a large family of optimization problems in which the objective and/or constraints are linear functions of the rates and/or energy efficiency (EE) of users.","Using this framework, we show that RSMA and RIS can be mutually beneficial tools when the system is overloaded, i.e., when the number of users per cell is higher than the number of base station (BS) antennas.","Additionally, we show that the benefits of RSMA increase when the packets are shorter and/or the reliability constraint is more stringent.","Furthermore, we show that the RSMA benefits increase with the number of users per cell and decrease with the number of BS antennas.","Finally, we show that RIS (either diagonal or BD) can highly improve the system performance, and BD-RIS outperforms regular RIS."],"url":"http://arxiv.org/abs/2307.05295v1"}
{"created":"2023-07-11 14:28:33","title":"Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in Autonomous Vehicle Safety","abstract":"Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving. While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems. Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios. To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator. This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame. Furthermore, an end-to-end short-term trajectory prediction model using convolutional neural networks (CNN) and long short-term memory (LSTM) networks has also been developed. This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment. In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community. Our datasets are publicly available on https://github.com/navigatinguncertainty.","sentences":["Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving.","While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems.","Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios.","To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator.","This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame.","Furthermore, an end-to-end short-term trajectory prediction model using convolutional neural networks (CNN) and long short-term memory (LSTM) networks has also been developed.","This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment.","In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community.","Our datasets are publicly available on https://github.com/navigatinguncertainty."],"url":"http://arxiv.org/abs/2307.05288v1"}
{"created":"2023-07-11 14:25:10","title":"On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets","abstract":"Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithmic and data-based interventions. Our testbed highlights the importance of future research that builds an understanding of how distributions differ.","sentences":["Different distribution shifts require different algorithmic and operational interventions.","Methodological research must be grounded by the specific shifts they address.","Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes.","We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent.","To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over.","Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithmic and data-based interventions.","Our testbed highlights the importance of future research that builds an understanding of how distributions differ."],"url":"http://arxiv.org/abs/2307.05284v1"}
{"created":"2023-07-11 14:24:32","title":"On the Identity and Group Problems for Complex Heisenberg Matrices","abstract":"We study the Identity Problem, the problem of determining if a finitely generated semigroup of matrices contains the identity matrix; see Problem 3 (Chapter 10.3) in ``Unsolved Problems in Mathematical Systems and Control Theory'' by Blondel and Megretski (2004). This fundamental problem is known to be undecidable for $\\mathbb{Z}^{4 \\times 4}$ and decidable for $\\mathbb{Z}^{2 \\times 2}$. The Identity Problem has been recently shown to be in polynomial time by Dong for the Heisenberg group over complex numbers in any fixed dimension with the use of Lie algebra and the Baker-Campbell-Hausdorff formula. We develop alternative proof techniques for the problem making a step forward towards more general problems such as the Membership Problem. We extend our techniques to show that the fundamental problem of determining if a given set of Heisenberg matrices generates a group, can also be decided in polynomial time.","sentences":["We study the Identity Problem, the problem of determining if a finitely generated semigroup of matrices contains the identity matrix; see Problem 3 (Chapter 10.3) in ``Unsolved Problems in Mathematical Systems and Control Theory'' by Blondel and Megretski (2004).","This fundamental problem is known to be undecidable for $\\mathbb{Z}^{4 \\times 4}$ and decidable for $\\mathbb{Z}^{2 \\times 2}$.","The Identity Problem has been recently shown to be in polynomial time by Dong for the Heisenberg group over complex numbers in any fixed dimension with the use of Lie algebra and the Baker-Campbell-Hausdorff formula.","We develop alternative proof techniques for the problem making a step forward towards more general problems such as the Membership Problem.","We extend our techniques to show that the fundamental problem of determining if a given set of Heisenberg matrices generates a group, can also be decided in polynomial time."],"url":"http://arxiv.org/abs/2307.05283v1"}
{"created":"2023-07-11 14:21:46","title":"Introducing Asynchronicity to Probabilistic Hyperproperties","abstract":"Probabilistic hyperproperties express probabilistic relations between different executions of systems with uncertain behavior. HyperPCTL allows to formalize such properties, where quantification over probabilistic schedulers resolves potential non-determinism. In this paper we propose an extension named AHyperPCTL to additionally introduce asynchronicity between the observed executions by quantifying over stutter-schedulers, which may randomly decide to delay scheduler decisions by idling. To our knowledge, this is the first asynchronous extension of a probabilistic branching-time hyperlogic. We show that AHyperPCTL can express interesting information-flow security policies, and propose a model checking algorithm for a decidable fragment.","sentences":["Probabilistic hyperproperties express probabilistic relations between different executions of systems with uncertain behavior.","HyperPCTL allows to formalize such properties, where quantification over probabilistic schedulers resolves potential non-determinism.","In this paper we propose an extension named AHyperPCTL to additionally introduce asynchronicity between the observed executions by quantifying over stutter-schedulers, which may randomly decide to delay scheduler decisions by idling.","To our knowledge, this is the first asynchronous extension of a probabilistic branching-time hyperlogic.","We show that AHyperPCTL can express interesting information-flow security policies, and propose a model checking algorithm for a decidable fragment."],"url":"http://arxiv.org/abs/2307.05282v1"}
{"created":"2023-07-11 14:20:29","title":"Smart Environment for Adaptive Learning of Cybersecurity Skills","abstract":"Hands-on computing education requires a realistic learning environment that enables students to gain and deepen their skills. Available learning environments, including virtual and physical labs, provide students with real-world computer systems but rarely adapt the learning environment to individual students of various proficiency and background. We designed a unique and novel smart environment for adaptive training of cybersecurity skills. The environment collects a variety of student data to assign a suitable learning path through the training. To enable such adaptiveness, we proposed, developed, and deployed a new tutor model and a training format. We evaluated the learning environment using two different adaptive trainings attended by 114 students of various proficiency. The results show students were assigned tasks with a more appropriate difficulty, which enabled them to successfully complete the training. Students reported that they enjoyed the training, felt the training difficulty was appropriately designed, and would attend more training sessions like these. Instructors can use the environment for teaching any topic involving real-world computer networks and systems because it is not tailored to particular training. We freely released the software along with exemplary training so that other instructors can adopt the innovations in their teaching practice.","sentences":["Hands-on computing education requires a realistic learning environment that enables students to gain and deepen their skills.","Available learning environments, including virtual and physical labs, provide students with real-world computer systems but rarely adapt the learning environment to individual students of various proficiency and background.","We designed a unique and novel smart environment for adaptive training of cybersecurity skills.","The environment collects a variety of student data to assign a suitable learning path through the training.","To enable such adaptiveness, we proposed, developed, and deployed a new tutor model and a training format.","We evaluated the learning environment using two different adaptive trainings attended by 114 students of various proficiency.","The results show students were assigned tasks with a more appropriate difficulty, which enabled them to successfully complete the training.","Students reported that they enjoyed the training, felt the training difficulty was appropriately designed, and would attend more training sessions like these.","Instructors can use the environment for teaching any topic involving real-world computer networks and systems because it is not tailored to particular training.","We freely released the software along with exemplary training so that other instructors can adopt the innovations in their teaching practice."],"url":"http://arxiv.org/abs/2307.05281v1"}
{"created":"2023-07-11 14:18:25","title":"A Mixed Reality System for Interaction\\\\with Heterogeneous Robotic Systems","abstract":"The growing spread of robots for service and industrial purposes calls for versatile, intuitive and portable interaction approaches. In particular, in industrial environments, operators should be able to interact with robots in a fast, effective, and possibly effortless manner. To this end, reality enhancement techniques have been used to achieve efficient management and simplify interactions, in particular in manufacturing and logistics processes. Building upon this, in this paper we propose a system based on mixed reality that allows a ubiquitous interface for heterogeneous robotic systems in dynamic scenarios, where users are involved in different tasks and need to interact with different robots. By means of mixed reality, users can interact with a robot through manipulation of its virtual replica, which is always colocated with the user and is extracted when interaction is needed. The system has been tested in a simulated intralogistics setting, where different robots are present and require sporadic intervention by human operators, who are involved in other tasks. In our setting we consider the presence of drones and AGVs with different levels of autonomy, calling for different user interventions. The proposed approach has been validated in virtual reality, considering quantitative and qualitative assessment of performance and user's feedback.","sentences":["The growing spread of robots for service and industrial purposes calls for versatile, intuitive and portable interaction approaches.","In particular, in industrial environments, operators should be able to interact with robots in a fast, effective, and possibly effortless manner.","To this end, reality enhancement techniques have been used to achieve efficient management and simplify interactions, in particular in manufacturing and logistics processes.","Building upon this, in this paper we propose a system based on mixed reality that allows a ubiquitous interface for heterogeneous robotic systems in dynamic scenarios, where users are involved in different tasks and need to interact with different robots.","By means of mixed reality, users can interact with a robot through manipulation of its virtual replica, which is always colocated with the user and is extracted when interaction is needed.","The system has been tested in a simulated intralogistics setting, where different robots are present and require sporadic intervention by human operators, who are involved in other tasks.","In our setting we consider the presence of drones and AGVs with different levels of autonomy, calling for different user interventions.","The proposed approach has been validated in virtual reality, considering quantitative and qualitative assessment of performance and user's feedback."],"url":"http://arxiv.org/abs/2307.05280v1"}
{"created":"2023-07-11 14:11:24","title":"Unbiased Scene Graph Generation via Two-stage Causal Modeling","abstract":"Despite the impressive performance of recent unbiased Scene Graph Generation (SGG) methods, the current debiasing literature mainly focuses on the long-tailed distribution problem, whereas it overlooks another source of bias, i.e., semantic confusion, which makes the SGG model prone to yield false predictions for similar relationships. In this paper, we explore a debiasing procedure for the SGG task leveraging causal inference. Our central insight is that the Sparse Mechanism Shift (SMS) in causality allows independent intervention on multiple biases, thereby potentially preserving head category performance while pursuing the prediction of high-informative tail relationships. However, the noisy datasets lead to unobserved confounders for the SGG task, and thus the constructed causal models are always causal-insufficient to benefit from SMS. To remedy this, we propose Two-stage Causal Modeling (TsCM) for the SGG task, which takes the long-tailed distribution and semantic confusion as confounders to the Structural Causal Model (SCM) and then decouples the causal intervention into two stages. The first stage is causal representation learning, where we use a novel Population Loss (P-Loss) to intervene in the semantic confusion confounder. The second stage introduces the Adaptive Logit Adjustment (AL-Adjustment) to eliminate the long-tailed distribution confounder to complete causal calibration learning. These two stages are model agnostic and thus can be used in any SGG model that seeks unbiased predictions. Comprehensive experiments conducted on the popular SGG backbones and benchmarks show that our TsCM can achieve state-of-the-art performance in terms of mean recall rate. Furthermore, TsCM can maintain a higher recall rate than other debiasing methods, which indicates that our method can achieve a better tradeoff between head and tail relationships.","sentences":["Despite the impressive performance of recent unbiased Scene Graph Generation (SGG) methods, the current debiasing literature mainly focuses on the long-tailed distribution problem, whereas it overlooks another source of bias, i.e., semantic confusion, which makes the SGG model prone to yield false predictions for similar relationships.","In this paper, we explore a debiasing procedure for the SGG task leveraging causal inference.","Our central insight is that the Sparse Mechanism Shift (SMS) in causality allows independent intervention on multiple biases, thereby potentially preserving head category performance while pursuing the prediction of high-informative tail relationships.","However, the noisy datasets lead to unobserved confounders for the SGG task, and thus the constructed causal models are always causal-insufficient to benefit from SMS.","To remedy this, we propose Two-stage Causal Modeling (TsCM) for the SGG task, which takes the long-tailed distribution and semantic confusion as confounders to the Structural Causal Model (SCM) and then decouples the causal intervention into two stages.","The first stage is causal representation learning, where we use a novel Population Loss (P-Loss) to intervene in the semantic confusion confounder.","The second stage introduces the Adaptive Logit Adjustment (AL-Adjustment) to eliminate the long-tailed distribution confounder to complete causal calibration learning.","These two stages are model agnostic and thus can be used in any SGG model that seeks unbiased predictions.","Comprehensive experiments conducted on the popular SGG backbones and benchmarks show that our TsCM can achieve state-of-the-art performance in terms of mean recall rate.","Furthermore, TsCM can maintain a higher recall rate than other debiasing methods, which indicates that our method can achieve a better tradeoff between head and tail relationships."],"url":"http://arxiv.org/abs/2307.05276v1"}
{"created":"2023-07-11 14:08:51","title":"CareFall: Automatic Fall Detection through Wearable Devices and AI Methods","abstract":"The aging population has led to a growing number of falls in our society, affecting global public health worldwide. This paper presents CareFall, an automatic Fall Detection System (FDS) based on wearable devices and Artificial Intelligence (AI) methods. CareFall considers the accelerometer and gyroscope time signals extracted from a smartwatch. Two different approaches are used for feature extraction and classification: i) threshold-based, and ii) machine learning-based. Experimental results on two public databases show that the machine learning-based approach, which combines accelerometer and gyroscope information, outperforms the threshold-based approach in terms of accuracy, sensitivity, and specificity. This research contributes to the design of smart and user-friendly solutions to mitigate the negative consequences of falls among older people.","sentences":["The aging population has led to a growing number of falls in our society, affecting global public health worldwide.","This paper presents CareFall, an automatic Fall Detection System (FDS) based on wearable devices and Artificial Intelligence (AI) methods.","CareFall considers the accelerometer and gyroscope time signals extracted from a smartwatch.","Two different approaches are used for feature extraction and classification: i) threshold-based, and ii) machine learning-based.","Experimental results on two public databases show that the machine learning-based approach, which combines accelerometer and gyroscope information, outperforms the threshold-based approach in terms of accuracy, sensitivity, and specificity.","This research contributes to the design of smart and user-friendly solutions to mitigate the negative consequences of falls among older people."],"url":"http://arxiv.org/abs/2307.05275v1"}
{"created":"2023-07-11 14:02:06","title":"Temporal Graphs Anomaly Emergence Detection: Benchmarking For Social Media Interactions","abstract":"Temporal graphs have become an essential tool for analyzing complex dynamic systems with multiple agents. Detecting anomalies in temporal graphs is crucial for various applications, including identifying emerging trends, monitoring network security, understanding social dynamics, tracking disease outbreaks, and understanding financial dynamics. In this paper, we present a comprehensive benchmarking study that compares 12 data-driven methods for anomaly detection in temporal graphs. We conduct experiments on two temporal graphs extracted from Twitter and Facebook, aiming to identify anomalies in group interactions. Surprisingly, our study reveals an unclear pattern regarding the best method for such tasks, highlighting the complexity and challenges involved in anomaly emergence detection in large and dynamic systems. The results underscore the need for further research and innovative approaches to effectively detect emerging anomalies in dynamic systems represented as temporal graphs.","sentences":["Temporal graphs have become an essential tool for analyzing complex dynamic systems with multiple agents.","Detecting anomalies in temporal graphs is crucial for various applications, including identifying emerging trends, monitoring network security, understanding social dynamics, tracking disease outbreaks, and understanding financial dynamics.","In this paper, we present a comprehensive benchmarking study that compares 12 data-driven methods for anomaly detection in temporal graphs.","We conduct experiments on two temporal graphs extracted from Twitter and Facebook, aiming to identify anomalies in group interactions.","Surprisingly, our study reveals an unclear pattern regarding the best method for such tasks, highlighting the complexity and challenges involved in anomaly emergence detection in large and dynamic systems.","The results underscore the need for further research and innovative approaches to effectively detect emerging anomalies in dynamic systems represented as temporal graphs."],"url":"http://arxiv.org/abs/2307.05268v1"}
{"created":"2023-07-11 13:58:20","title":"Computing minimal distinguishing Hennessy-Milner formulas is NP-hard, but variants are tractable","abstract":"We study the problem of computing minimal distinguishing formulas for non-bisimilar states in finite LTSs. We show that this is NP-hard if the size of the formula must be minimal. Similarly, the existence of a short distinguishing trace is NP-complete. However, we can provide polynomial algorithms, if minimality is formulated as the minimal number of nested modalities, and it can even be extended by recursively requiring a minimal number of nested negations. A prototype implementation shows that the generated formulas are much smaller than those generated by the method introduced by Cleaveland.","sentences":["We study the problem of computing minimal distinguishing formulas for non-bisimilar states in finite LTSs.","We show that this is NP-hard if the size of the formula must be minimal.","Similarly, the existence of a short distinguishing trace is NP-complete.","However, we can provide polynomial algorithms, if minimality is formulated as the minimal number of nested modalities, and it can even be extended by recursively requiring a minimal number of nested negations.","A prototype implementation shows that the generated formulas are much smaller than those generated by the method introduced by Cleaveland."],"url":"http://arxiv.org/abs/2307.05265v1"}
{"created":"2023-07-11 13:53:13","title":"Pegasus Simulator: An Isaac Sim Framework for Multiple Aerial Vehicles Simulation","abstract":"Developing and testing novel control and motion planning algorithms for aerial vehicles can be a challenging task, with the robotics community relying more than ever on 3D simulation technologies to evaluate the performance of new algorithms in a variety of conditions and environments. In this work, we introduce the Pegasus Simulator, a modular framework implemented as an NVIDIA Isaac Sim extension that enables real-time simulation of multiple multirotor vehicles in photo-realistic environments, while providing out-of-the-box integration with the widely adopted PX4-Autopilot and ROS2 through its modular implementation and intuitive graphical user interface. To demonstrate some of its capabilities, a nonlinear controller was implemented and simulation results for two drones performing aggressive flight maneuvers are presented. Code and documentation for this framework are also provided as supplementary material.","sentences":["Developing and testing novel control and motion planning algorithms for aerial vehicles can be a challenging task, with the robotics community relying more than ever on 3D simulation technologies to evaluate the performance of new algorithms in a variety of conditions and environments.","In this work, we introduce the Pegasus Simulator, a modular framework implemented as an NVIDIA Isaac Sim extension that enables real-time simulation of multiple multirotor vehicles in photo-realistic environments, while providing out-of-the-box integration with the widely adopted PX4-Autopilot and ROS2 through its modular implementation and intuitive graphical user interface.","To demonstrate some of its capabilities, a nonlinear controller was implemented and simulation results for two drones performing aggressive flight maneuvers are presented.","Code and documentation for this framework are also provided as supplementary material."],"url":"http://arxiv.org/abs/2307.05263v1"}
{"created":"2023-07-11 13:51:12","title":"U-CREAT: Unsupervised Case Retrieval using Events extrAcTion","abstract":"The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (Indian and Canadian), and it shows state-of-the-art performance on the benchmarks for both the legal systems (IL-PCR and COLIEE corpora).","sentences":["The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case.","To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus.","Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents.","In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction).","We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems.","Our proposed system is generic, we show that it generalizes across two different legal systems (Indian and Canadian), and it shows state-of-the-art performance on the benchmarks for both the legal systems (IL-PCR and COLIEE corpora)."],"url":"http://arxiv.org/abs/2307.05260v1"}
{"created":"2023-07-11 13:47:26","title":"Integrated Planning in Hospitals: A Review","abstract":"Efficient planning of scarce resources in hospitals is a challenging task for which a large variety of Operations Research and Management Science approaches have been developed since the 1950s. While efficient planning of single resources such as operating rooms, beds, or specific types of staff can already lead to enormous efficiency gains, integrated planning of several resources has been shown to hold even greater potential, and a large number of integrated planning approaches have been presented in the literature over the past decades.   This paper provides the first literature review that focuses specifically on the Operations Research and Management Science literature related to integrated planning of different resources in hospitals. We collect the relevant literature and analyze it regarding different aspects such as uncertainty modeling and the use of real-life data. Several cross comparisons reveal interesting insights concerning, e.g., relations between the modeling and solution methods used and the practical implementation of the approaches developed. Moreover, we provide a high-level taxonomy for classifying different resource-focused integration approaches and point out gaps in the literature as well as promising directions for future research.","sentences":["Efficient planning of scarce resources in hospitals is a challenging task for which a large variety of Operations Research and Management Science approaches have been developed since the 1950s.","While efficient planning of single resources such as operating rooms, beds, or specific types of staff can already lead to enormous efficiency gains, integrated planning of several resources has been shown to hold even greater potential, and a large number of integrated planning approaches have been presented in the literature over the past decades.   ","This paper provides the first literature review that focuses specifically on the Operations Research and Management Science literature related to integrated planning of different resources in hospitals.","We collect the relevant literature and analyze it regarding different aspects such as uncertainty modeling and the use of real-life data.","Several cross comparisons reveal interesting insights concerning, e.g., relations between the modeling and solution methods used and the practical implementation of the approaches developed.","Moreover, we provide a high-level taxonomy for classifying different resource-focused integration approaches and point out gaps in the literature as well as promising directions for future research."],"url":"http://arxiv.org/abs/2307.05258v1"}
{"created":"2023-07-11 13:36:07","title":"OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification","abstract":"Active learning (AL) is an effective approach to select the most informative samples to label so as to reduce the annotation cost. Existing AL methods typically work under the closed-set assumption, i.e., all classes existing in the unlabeled sample pool need to be classified by the target model. However, in some practical clinical tasks, the unlabeled pool may contain not only the target classes that need to be fine-grainedly classified, but also non-target classes that are irrelevant to the clinical tasks. Existing AL methods cannot work well in this scenario because they tend to select a large number of non-target samples. In this paper, we formulate this scenario as an open-set AL problem and propose an efficient framework, OpenAL, to address the challenge of querying samples from an unlabeled pool with both target class and non-target class samples. Experiments on fine-grained classification of pathology images show that OpenAL can significantly improve the query quality of target class samples and achieve higher performance than current state-of-the-art AL methods. Code is available at https://github.com/miccaiif/OpenAL.","sentences":["Active learning (AL) is an effective approach to select the most informative samples to label so as to reduce the annotation cost.","Existing AL methods typically work under the closed-set assumption, i.e., all classes existing in the unlabeled sample pool need to be classified by the target model.","However, in some practical clinical tasks, the unlabeled pool may contain not only the target classes that need to be fine-grainedly classified, but also non-target classes that are irrelevant to the clinical tasks.","Existing AL methods cannot work well in this scenario because they tend to select a large number of non-target samples.","In this paper, we formulate this scenario as an open-set AL problem and propose an efficient framework, OpenAL, to address the challenge of querying samples from an unlabeled pool with both target class and non-target class samples.","Experiments on fine-grained classification of pathology images show that OpenAL can significantly improve the query quality of target class samples and achieve higher performance than current state-of-the-art AL methods.","Code is available at https://github.com/miccaiif/OpenAL."],"url":"http://arxiv.org/abs/2307.05254v1"}
{"created":"2023-07-11 13:35:27","title":"MAP- and MLE-Based Teaching","abstract":"Imagine a learner L who tries to infer a hidden concept from a collection of observations. Building on the work [4] of Ferri et al., we assume the learner to be parameterized by priors P(c) and by c-conditional likelihoods P(z|c) where c ranges over all concepts in a given class C and z ranges over all observations in an observation set Z. L is called a MAP-learner (resp. an MLE-learner) if it thinks of a collection S of observations as a random sample and returns the concept with the maximum a-posteriori probability (resp. the concept which maximizes the c-conditional likelihood of S). Depending on whether L assumes that S is obtained from ordered or unordered sampling resp. from sampling with or without replacement, we can distinguish four different sampling modes. Given a target concept c in C, a teacher for a MAP-learner L aims at finding a smallest collection of observations that causes L to return c. This approach leads in a natural manner to various notions of a MAP- or MLE-teaching dimension of a concept class C. Our main results are: We show that this teaching model has some desirable monotonicity properties. We clarify how the four sampling modes are related to each other. As for the (important!) special case, where concepts are subsets of a domain and observations are 0,1-labeled examples, we obtain some additional results. First of all, we characterize the MAP- and MLE-teaching dimension associated with an optimally parameterized MAP-learner graph-theoretically. From this central result, some other ones are easy to derive. It is shown, for instance, that the MLE-teaching dimension is either equal to the MAP-teaching dimension or exceeds the latter by 1. It is shown furthermore that these dimensions can be bounded from above by the so-called antichain number, the VC-dimension and related combinatorial parameters. Moreover they can be computed in polynomial time.","sentences":["Imagine a learner L who tries to infer a hidden concept from a collection of observations.","Building on the work [4] of Ferri et al., we assume the learner to be parameterized by priors P(c) and by c-conditional likelihoods P(z|c) where c ranges over all concepts in a given class C and z ranges over all observations in an observation set Z. L is called a MAP-learner (resp.","an MLE-learner) if it thinks of a collection S of observations as a random sample and returns the concept with the maximum a-posteriori probability (resp.","the concept which maximizes the c-conditional likelihood of S).","Depending on whether L assumes that S is obtained from ordered or unordered sampling resp.","from sampling with or without replacement, we can distinguish four different sampling modes.","Given a target concept c in C, a teacher for a MAP-learner L aims at finding a smallest collection of observations that causes L to return c.","This approach leads in a natural manner to various notions of a MAP- or MLE-teaching dimension of a concept class C. Our main results are: We show that this teaching model has some desirable monotonicity properties.","We clarify how the four sampling modes are related to each other.","As for the (important!)","special case, where concepts are subsets of a domain and observations are 0,1-labeled examples, we obtain some additional results.","First of all, we characterize the MAP- and MLE-teaching dimension associated with an optimally parameterized MAP-learner graph-theoretically.","From this central result, some other ones are easy to derive.","It is shown, for instance, that the MLE-teaching dimension is either equal to the MAP-teaching dimension or exceeds the latter by 1.","It is shown furthermore that these dimensions can be bounded from above by the so-called antichain number, the VC-dimension and related combinatorial parameters.","Moreover they can be computed in polynomial time."],"url":"http://arxiv.org/abs/2307.05252v1"}
{"created":"2023-07-11 13:16:04","title":"Does pre-training on brain-related tasks results in better deep-learning-based brain age biomarkers?","abstract":"Brain age prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker. Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects. In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction. More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data. Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer's disease. Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers.","sentences":["Brain age prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker.","Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects.","In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction.","More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data.","Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer's disease.","Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers."],"url":"http://arxiv.org/abs/2307.05241v1"}
{"created":"2023-07-11 13:09:11","title":"Quantitative Comparison of Nearest Neighbor Search Algorithms","abstract":"We compare the performance of three nearest neighbor search algorithms: the Orchard, ball tree, and VP-tree algorithms. These algorithms are commonly used for nearest-neighbor searches and are known for their efficiency in large datasets. We analyze the fraction of distances computed in relation to the size of the dataset and its dimension. For each algorithm we derive a fitting function for the efficiency as a function to set size and dimension. The article aims to provide a comprehensive analysis of the performance of these algorithms and help researchers and practitioners choose the best algorithm for their specific application.","sentences":["We compare the performance of three nearest neighbor search algorithms: the Orchard, ball tree, and VP-tree algorithms.","These algorithms are commonly used for nearest-neighbor searches and are known for their efficiency in large datasets.","We analyze the fraction of distances computed in relation to the size of the dataset and its dimension.","For each algorithm we derive a fitting function for the efficiency as a function to set size and dimension.","The article aims to provide a comprehensive analysis of the performance of these algorithms and help researchers and practitioners choose the best algorithm for their specific application."],"url":"http://arxiv.org/abs/2307.05235v1"}
{"created":"2023-07-11 13:06:42","title":"A Survey From Distributed Machine Learning to Distributed Deep Learning","abstract":"Artificial intelligence has achieved significant success in handling complex tasks in recent years. This success is due to advances in machine learning algorithms and hardware acceleration. In order to obtain more accurate results and solve more complex problems, algorithms must be trained with more data. This huge amount of data could be time-consuming to process and require a great deal of computation. This solution could be achieved by distributing the data and algorithm across several machines, which is known as distributed machine learning. There has been considerable effort put into distributed machine learning algorithms, and different methods have been proposed so far. In this article, we present a comprehensive summary of the current state-of-the-art in the field through the review of these algorithms. We divide this algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of studies worked on this algorithms. As a result, most of the articles we discussed here belong to this category. Based on our investigation of algorithms, we highlight limitations that should be addressed in future research.","sentences":["Artificial intelligence has achieved significant success in handling complex tasks in recent years.","This success is due to advances in machine learning algorithms and hardware acceleration.","In order to obtain more accurate results and solve more complex problems, algorithms must be trained with more data.","This huge amount of data could be time-consuming to process and require a great deal of computation.","This solution could be achieved by distributing the data and algorithm across several machines, which is known as distributed machine learning.","There has been considerable effort put into distributed machine learning algorithms, and different methods have been proposed so far.","In this article, we present a comprehensive summary of the current state-of-the-art in the field through the review of these algorithms.","We divide this algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups.","Distributed deep learning has gained more attention in recent years and most of studies worked on this algorithms.","As a result, most of the articles we discussed here belong to this category.","Based on our investigation of algorithms, we highlight limitations that should be addressed in future research."],"url":"http://arxiv.org/abs/2307.05232v1"}
{"created":"2023-07-11 12:48:55","title":"Attribute Controlled Dialogue Prompting","abstract":"Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.","sentences":["Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks.","However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation.","In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation.","Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation.","Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters."],"url":"http://arxiv.org/abs/2307.05228v1"}
{"created":"2023-07-11 12:47:04","title":"Energy Efficient Personalized Hand-Gesture Recognition with Neuromorphic Computing","abstract":"Hand gestures are a form of non-verbal communication that is used in social interaction and it is therefore required for more natural human-robot interaction. Neuromorphic (brain-inspired) computing offers a low-power solution for Spiking neural networks (SNNs) that can be used for the classification and recognition of gestures. This article introduces the preliminary results of a novel methodology for training spiking convolutional neural networks for hand-gesture recognition so that a humanoid robot with integrated neuromorphic hardware will be able to personalise the interaction with a user according to the shown hand gesture. It also describes other approaches that could improve the overall performance of the model.","sentences":["Hand gestures are a form of non-verbal communication that is used in social interaction and it is therefore required for more natural human-robot interaction.","Neuromorphic (brain-inspired) computing offers a low-power solution for Spiking neural networks (SNNs) that can be used for the classification and recognition of gestures.","This article introduces the preliminary results of a novel methodology for training spiking convolutional neural networks for hand-gesture recognition so that a humanoid robot with integrated neuromorphic hardware will be able to personalise the interaction with a user according to the shown hand gesture.","It also describes other approaches that could improve the overall performance of the model."],"url":"http://arxiv.org/abs/2307.05225v1"}
{"created":"2023-07-11 12:46:39","title":"Reliable Packet Detection for Random Access Networks: Analysis, Benchmark, and Optimization","abstract":"This paper reexamines and fundamentally improves the Schmidl-and-Cox (S&C) algorithm, which is extensively used for packet detection in wireless networks, and enhances its adaptability for multi-antenna receivers. First, we introduce a new \"compensated autocorrelation\" metric, providing a more analytically tractable solution with precise expressions for false-alarm and missed-detection probabilities. Second, this paper proposes the Pareto comparison principle for fair benchmarking packet-detection algorithms, considering both false alarms and missed detections simultaneously. Third, with the Pareto benchmarking scheme, we experimentally confirm that the performance of S&C can be greatly improved by taking only the real part and discarding the imaginary part of the autocorrelation, leading to the novel real-part S&C (RP-S&C) scheme. Fourth, and perhaps most importantly, we utilize the compensated autocorrelation metric we newly put forth to extend the single-antenna algorithm to multi-antenna scenarios through a weighted-sum approach. Two optimization problems, minimizing false-alarm and missed-detection probabilities respectively, are formulated and solutions are provided. Our experimental results reveal that the optimal weights for false alarms (WFA) scheme is more desirable than the optimal weights for missed detections (WMD) due to its simplicity, reliability, and superior performance. This study holds considerable implications for the design and deployment of packet-detection schemes in random-access networks.","sentences":["This paper reexamines and fundamentally improves the Schmidl-and-Cox (S&C) algorithm, which is extensively used for packet detection in wireless networks, and enhances its adaptability for multi-antenna receivers.","First, we introduce a new \"compensated autocorrelation\" metric, providing a more analytically tractable solution with precise expressions for false-alarm and missed-detection probabilities.","Second, this paper proposes the Pareto comparison principle for fair benchmarking packet-detection algorithms, considering both false alarms and missed detections simultaneously.","Third, with the Pareto benchmarking scheme, we experimentally confirm that the performance of S&C can be greatly improved by taking only the real part and discarding the imaginary part of the autocorrelation, leading to the novel real-part S&C (RP-S&C) scheme.","Fourth, and perhaps most importantly, we utilize the compensated autocorrelation metric we newly put forth to extend the single-antenna algorithm to multi-antenna scenarios through a weighted-sum approach.","Two optimization problems, minimizing false-alarm and missed-detection probabilities respectively, are formulated and solutions are provided.","Our experimental results reveal that the optimal weights for false alarms (WFA) scheme is more desirable than the optimal weights for missed detections (WMD) due to its simplicity, reliability, and superior performance.","This study holds considerable implications for the design and deployment of packet-detection schemes in random-access networks."],"url":"http://arxiv.org/abs/2307.05224v1"}
{"created":"2023-07-11 12:45:39","title":"Generative Pretraining in Multimodality","abstract":"We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.","sentences":["We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context.","This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process.","First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence.","Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence.","This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs.","Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation.","Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models.","Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance."],"url":"http://arxiv.org/abs/2307.05222v1"}
{"created":"2023-07-11 12:44:06","title":"MinkSORT: A 3D deep feature extractor using sparse convolutions to improve 3D multi-object tracking in greenhouse tomato plants","abstract":"The agro-food industry is turning to robots to address the challenge of labour shortage. However, agro-food environments pose difficulties for robots due to high variation and occlusions. In the presence of these challenges, accurate world models, with information about object location, shape, and properties, are crucial for robots to perform tasks accurately. Building such models is challenging due to the complex and unique nature of agro-food environments, and errors in the model can lead to task execution issues. In this paper, we propose MinkSORT, a novel method for generating tracking features using a 3D sparse convolutional network in a deepSORT-like approach to improve the accuracy of world models in agro-food environments. We evaluated our feature extractor network using real-world data collected in a tomato greenhouse, which significantly improved the performance of our baseline model that tracks tomato positions in 3D using a Kalman filter and Mahalanobis distance. Our deep learning feature extractor improved the HOTA from 42.8% to 44.77%, the association accuracy from 32.55% to 35.55%, and the MOTA from 57.63% to 58.81%. We also evaluated different contrastive loss functions for training our deep learning feature extractor and demonstrated that our approach leads to improved performance in terms of three separate precision and recall detection outcomes. Our method improves world model accuracy, enabling robots to perform tasks such as harvesting and plant maintenance with greater efficiency and accuracy, which is essential for meeting the growing demand for food in a sustainable manner.","sentences":["The agro-food industry is turning to robots to address the challenge of labour shortage.","However, agro-food environments pose difficulties for robots due to high variation and occlusions.","In the presence of these challenges, accurate world models, with information about object location, shape, and properties, are crucial for robots to perform tasks accurately.","Building such models is challenging due to the complex and unique nature of agro-food environments, and errors in the model can lead to task execution issues.","In this paper, we propose MinkSORT, a novel method for generating tracking features using a 3D sparse convolutional network in a deepSORT-like approach to improve the accuracy of world models in agro-food environments.","We evaluated our feature extractor network using real-world data collected in a tomato greenhouse, which significantly improved the performance of our baseline model that tracks tomato positions in 3D using a Kalman filter and Mahalanobis distance.","Our deep learning feature extractor improved the HOTA from 42.8% to 44.77%, the association accuracy from 32.55% to 35.55%, and the MOTA from 57.63% to 58.81%.","We also evaluated different contrastive loss functions for training our deep learning feature extractor and demonstrated that our approach leads to improved performance in terms of three separate precision and recall detection outcomes.","Our method improves world model accuracy, enabling robots to perform tasks such as harvesting and plant maintenance with greater efficiency and accuracy, which is essential for meeting the growing demand for food in a sustainable manner."],"url":"http://arxiv.org/abs/2307.05219v1"}
{"created":"2023-07-11 12:43:42","title":"Probabilistic Operational Correspondence (Technical Report)","abstract":"Encodings are the main way to compare process calculi. By applying quality criteria to encodings we analyse their quality and rule out trivial or meaningless encodings. Thereby, operational correspondence is one of the most common and most important quality criteria. It ensures that processes and their translations have the same abstract behaviour. We analyse probabilistic versions of operational correspondence to enable such a verification for probabilistic systems. Concretely, we present three versions of probabilistic operational correspondence: weak, middle, and strong. We show the relevance of the weaker version using an encoding from a sublanguage of probabilistic CCS into the probabilistic pi-calculus. Moreover, we map this version of probabilistic operational correspondence onto a probabilistic behavioural relation that directly relates source and target terms. Then we can analyse the quality of the criterion by analysing the relation it induces between a source term and its translation. For the second version of probabilistic operational correspondence we proceed in the opposite direction. We start with a standard simulation relation for probabilistic systems and map it onto a probabilistic operational correspondence criterion. This technical report contains the proofs to the lemmata and theorems of [8] as well as some additional material.","sentences":["Encodings are the main way to compare process calculi.","By applying quality criteria to encodings we analyse their quality and rule out trivial or meaningless encodings.","Thereby, operational correspondence is one of the most common and most important quality criteria.","It ensures that processes and their translations have the same abstract behaviour.","We analyse probabilistic versions of operational correspondence to enable such a verification for probabilistic systems.","Concretely, we present three versions of probabilistic operational correspondence: weak, middle, and strong.","We show the relevance of the weaker version using an encoding from a sublanguage of probabilistic CCS into the probabilistic pi-calculus.","Moreover, we map this version of probabilistic operational correspondence onto a probabilistic behavioural relation that directly relates source and target terms.","Then we can analyse the quality of the criterion by analysing the relation it induces between a source term and its translation.","For the second version of probabilistic operational correspondence we proceed in the opposite direction.","We start with a standard simulation relation for probabilistic systems and map it onto a probabilistic operational correspondence criterion.","This technical report contains the proofs to the lemmata and theorems of [8] as well as some additional material."],"url":"http://arxiv.org/abs/2307.05218v1"}
{"created":"2023-07-11 12:43:23","title":"Supervised Attention Using Homophily in Graph Neural Networks","abstract":"Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.","sentences":["Graph neural networks have become the standard approach for dealing with learning problems on graphs.","Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks.","In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism.","However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance.","In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label.","We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models."],"url":"http://arxiv.org/abs/2307.05217v1"}
{"created":"2023-07-11 12:40:45","title":"Words fixing the kernel network and maximum independent sets in graphs","abstract":"The simple greedy algorithm to find a maximal independent set of a graph can be viewed as a sequential update of a Boolean network, where the update function at each vertex is the conjunction of all the negated variables in its neighbourhood. In general, the convergence of the so-called kernel network is complex. A word (sequence of vertices) fixes the kernel network if applying the updates sequentially according to that word. We prove that determining whether a word fixes the kernel network is coNP-complete. We also consider the so-called permis, which are permutation words that fix the kernel network. We exhibit large classes of graphs that have a permis, but we also construct many graphs without a permis.","sentences":["The simple greedy algorithm to find a maximal independent set of a graph can be viewed as a sequential update of a Boolean network, where the update function at each vertex is the conjunction of all the negated variables in its neighbourhood.","In general, the convergence of the so-called kernel network is complex.","A word (sequence of vertices) fixes the kernel network if applying the updates sequentially according to that word.","We prove that determining whether a word fixes the kernel network is coNP-complete.","We also consider the so-called permis, which are permutation words that fix the kernel network.","We exhibit large classes of graphs that have a permis, but we also construct many graphs without a permis."],"url":"http://arxiv.org/abs/2307.05216v1"}
{"created":"2023-07-11 12:32:13","title":"Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning","abstract":"Many real-world optimization problems contain unknown parameters that must be predicted prior to solving. To train the predictive machine learning (ML) models involved, the commonly adopted approach focuses on maximizing predictive accuracy. However, this approach does not always lead to the minimization of the downstream task loss. Decision-focused learning (DFL) is a recently proposed paradigm whose goal is to train the ML model by directly minimizing the task loss. However, state-of-the-art DFL methods are limited by the assumptions they make about the structure of the optimization problem (e.g., that the problem is linear) and by the fact that can only predict parameters that appear in the objective function. In this work, we address these limitations by instead predicting \\textit{distributions} over parameters and adopting score function gradient estimation (SFGE) to compute decision-focused updates to the predictive model, thereby widening the applicability of DFL. Our experiments show that by using SFGE we can: (1) deal with predictions that occur both in the objective function and in the constraints; and (2) effectively tackle two-stage stochastic optimization problems.","sentences":["Many real-world optimization problems contain unknown parameters that must be predicted prior to solving.","To train the predictive machine learning (ML) models involved, the commonly adopted approach focuses on maximizing predictive accuracy.","However, this approach does not always lead to the minimization of the downstream task loss.","Decision-focused learning (DFL) is a recently proposed paradigm whose goal is to train the ML model by directly minimizing the task loss.","However, state-of-the-art DFL methods are limited by the assumptions they make about the structure of the optimization problem (e.g., that the problem is linear) and by the fact that can only predict parameters that appear in the objective function.","In this work, we address these limitations by instead predicting \\textit{distributions} over parameters and adopting score function gradient estimation (SFGE) to compute decision-focused updates to the predictive model, thereby widening the applicability of DFL.","Our experiments show that by using SFGE we can: (1) deal with predictions that occur both in the objective function and in the constraints; and (2) effectively tackle two-stage stochastic optimization problems."],"url":"http://arxiv.org/abs/2307.05213v1"}
{"created":"2023-07-11 12:28:05","title":"Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning","abstract":"Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.","sentences":["Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes.","To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics.","Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions.","These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer.","Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains."],"url":"http://arxiv.org/abs/2307.05209v1"}
{"created":"2023-07-11 12:24:15","title":"Application-aware Energy Attack Mitigation in the Battery-less Internet of Things","abstract":"We study how to mitigate the effects of energy attacks in the batteryless Internet of Things (IoT). Battery-less IoT devices live and die with ambient energy, as they use energy harvesting to power their operation. They are employed in a multitude of applications, including safety-critical ones such as biomedical implants. Due to scarce energy intakes and limited energy buffers, their executions become intermittent, alternating periods of active operation with periods of recharging their energy buffers. Experimental evidence exists that shows how controlling ambient energy allows an attacker to steer a device execution in unintended ways: energy provisioning effectively becomes an attack vector. We design, implement, and evaluate a mitigation system for energy attacks. By taking into account the specific application requirements and the output of an attack detection module, we tune task execution rates and optimize energy management. This ensures continued application execution in the event of an energy attack. When a device is under attack, our solution ensures the execution of 23.3% additional application cycles compared to the baselines we consider and increases task schedulability by at least 21%, while enabling a 34% higher peripheral availability.","sentences":["We study how to mitigate the effects of energy attacks in the batteryless Internet of Things (IoT).","Battery-less IoT devices live and die with ambient energy, as they use energy harvesting to power their operation.","They are employed in a multitude of applications, including safety-critical ones such as biomedical implants.","Due to scarce energy intakes and limited energy buffers, their executions become intermittent, alternating periods of active operation with periods of recharging their energy buffers.","Experimental evidence exists that shows how controlling ambient energy allows an attacker to steer a device execution in unintended ways: energy provisioning effectively becomes an attack vector.","We design, implement, and evaluate a mitigation system for energy attacks.","By taking into account the specific application requirements and the output of an attack detection module, we tune task execution rates and optimize energy management.","This ensures continued application execution in the event of an energy attack.","When a device is under attack, our solution ensures the execution of 23.3% additional application cycles compared to the baselines we consider and increases task schedulability by at least 21%, while enabling a 34% higher peripheral availability."],"url":"http://arxiv.org/abs/2307.05206v1"}
{"created":"2023-07-11 12:10:42","title":"The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework","abstract":"In the context of label-efficient learning on video data, the distillation method and the structural design of the teacher-student architecture have a significant impact on knowledge distillation. However, the relationship between these factors has been overlooked in previous research. To address this gap, we propose a new weakly supervised learning framework for knowledge distillation in video classification that is designed to improve the efficiency and accuracy of the student model. Our approach leverages the concept of substage-based learning to distill knowledge based on the combination of student substages and the correlation of corresponding substages. We also employ the progressive cascade training method to address the accuracy loss caused by the large capacity gap between the teacher and the student. Additionally, we propose a pseudo-label optimization strategy to improve the initial data label. To optimize the loss functions of different distillation substages during the training process, we introduce a new loss method based on feature distribution. We conduct extensive experiments on both real and simulated data sets, demonstrating that our proposed approach outperforms existing distillation methods in terms of knowledge distillation for video classification tasks. Our proposed substage-based distillation approach has the potential to inform future research on label-efficient learning for video data.","sentences":["In the context of label-efficient learning on video data, the distillation method and the structural design of the teacher-student architecture have a significant impact on knowledge distillation.","However, the relationship between these factors has been overlooked in previous research.","To address this gap, we propose a new weakly supervised learning framework for knowledge distillation in video classification that is designed to improve the efficiency and accuracy of the student model.","Our approach leverages the concept of substage-based learning to distill knowledge based on the combination of student substages and the correlation of corresponding substages.","We also employ the progressive cascade training method to address the accuracy loss caused by the large capacity gap between the teacher and the student.","Additionally, we propose a pseudo-label optimization strategy to improve the initial data label.","To optimize the loss functions of different distillation substages during the training process, we introduce a new loss method based on feature distribution.","We conduct extensive experiments on both real and simulated data sets, demonstrating that our proposed approach outperforms existing distillation methods in terms of knowledge distillation for video classification tasks.","Our proposed substage-based distillation approach has the potential to inform future research on label-efficient learning for video data."],"url":"http://arxiv.org/abs/2307.05201v1"}
{"created":"2023-07-11 12:09:14","title":"Reject option models comprising out-of-distribution detection","abstract":"The optimal prediction strategy for out-of-distribution (OOD) setups is a fundamental question in machine learning. In this paper, we address this question and present several contributions. We propose three reject option models for OOD setups: the Cost-based model, the Bounded TPR-FPR model, and the Bounded Precision-Recall model. These models extend the standard reject option models used in non-OOD setups and define the notion of an optimal OOD selective classifier. We establish that all the proposed models, despite their different formulations, share a common class of optimal strategies. Motivated by the optimal strategy, we introduce double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection. The experimental results consistently demonstrate the superior performance of this simple strategy compared to state-of-the-art methods. Additionally, we propose novel evaluation metrics derived from the definition of the optimal strategy under the proposed OOD rejection models. These new metrics provide a comprehensive and reliable assessment of OOD methods without the deficiencies observed in existing evaluation approaches.","sentences":["The optimal prediction strategy for out-of-distribution (OOD) setups is a fundamental question in machine learning.","In this paper, we address this question and present several contributions.","We propose three reject option models for OOD setups: the Cost-based model, the Bounded TPR-FPR model, and the Bounded Precision-Recall model.","These models extend the standard reject option models used in non-OOD setups and define the notion of an optimal OOD selective classifier.","We establish that all the proposed models, despite their different formulations, share a common class of optimal strategies.","Motivated by the optimal strategy, we introduce double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection.","The experimental results consistently demonstrate the superior performance of this simple strategy compared to state-of-the-art methods.","Additionally, we propose novel evaluation metrics derived from the definition of the optimal strategy under the proposed OOD rejection models.","These new metrics provide a comprehensive and reliable assessment of OOD methods without the deficiencies observed in existing evaluation approaches."],"url":"http://arxiv.org/abs/2307.05199v1"}
{"created":"2023-07-11 11:59:04","title":"Membership Inference Attacks on DNNs using Adversarial Perturbations","abstract":"Several membership inference (MI) attacks have been proposed to audit a target DNN. Given a set of subjects, MI attacks tell which subjects the target DNN has seen during training. This work focuses on the post-training MI attacks emphasizing high confidence membership detection -- True Positive Rates (TPR) at low False Positive Rates (FPR). Current works in this category -- likelihood ratio attack (LiRA) and enhanced MI attack (EMIA) -- only perform well on complex datasets (e.g., CIFAR-10 and Imagenet) where the target DNN overfits its train set, but perform poorly on simpler datasets (0% TPR by both attacks on Fashion-MNIST, 2% and 0% TPR respectively by LiRA and EMIA on MNIST at 1% FPR). To address this, firstly, we unify current MI attacks by presenting a framework divided into three stages -- preparation, indication and decision. Secondly, we utilize the framework to propose two novel attacks: (1) Adversarial Membership Inference Attack (AMIA) efficiently utilizes the membership and the non-membership information of the subjects while adversarially minimizing a novel loss function, achieving 6% TPR on both Fashion-MNIST and MNIST datasets; and (2) Enhanced AMIA (E-AMIA) combines EMIA and AMIA to achieve 8% and 4% TPRs on Fashion-MNIST and MNIST datasets respectively, at 1% FPR. Thirdly, we introduce two novel augmented indicators that positively leverage the loss information in the Gaussian neighborhood of a subject. This improves TPR of all four attacks on average by 2.5% and 0.25% respectively on Fashion-MNIST and MNIST datasets at 1% FPR. Finally, we propose simple, yet novel, evaluation metric, the running TPR average (RTA) at a given FPR, that better distinguishes different MI attacks in the low FPR region. We also show that AMIA and E-AMIA are more transferable to the unknown DNNs (other than the target DNN) and are more robust to DP-SGD training as compared to LiRA and EMIA.","sentences":["Several membership inference (MI) attacks have been proposed to audit a target DNN.","Given a set of subjects, MI attacks tell which subjects the target DNN has seen during training.","This work focuses on the post-training MI attacks emphasizing high confidence membership detection -- True Positive Rates (TPR) at low False Positive Rates (FPR).","Current works in this category -- likelihood ratio attack (LiRA) and enhanced MI attack (EMIA) -- only perform well on complex datasets (e.g., CIFAR-10 and Imagenet) where the target DNN overfits its train set, but perform poorly on simpler datasets (0% TPR by both attacks on Fashion-MNIST, 2% and 0% TPR respectively by LiRA and EMIA on MNIST at 1% FPR).","To address this, firstly, we unify current MI attacks by presenting a framework divided into three stages -- preparation, indication and decision.","Secondly, we utilize the framework to propose two novel attacks: (1) Adversarial Membership Inference Attack (AMIA) efficiently utilizes the membership and the non-membership information of the subjects while adversarially minimizing a novel loss function, achieving 6% TPR on both Fashion-MNIST and MNIST datasets; and (2) Enhanced AMIA (E-AMIA) combines EMIA and AMIA to achieve 8% and 4% TPRs on Fashion-MNIST and MNIST datasets respectively, at 1% FPR.","Thirdly, we introduce two novel augmented indicators that positively leverage the loss information in the Gaussian neighborhood of a subject.","This improves TPR of all four attacks on average by 2.5% and 0.25% respectively on Fashion-MNIST and MNIST datasets at 1% FPR.","Finally, we propose simple, yet novel, evaluation metric, the running TPR average (RTA) at a given FPR, that better distinguishes different MI attacks in the low FPR region.","We also show that AMIA and E-AMIA are more transferable to the unknown DNNs (other than the target DNN) and are more robust to DP-SGD training as compared to LiRA and EMIA."],"url":"http://arxiv.org/abs/2307.05193v1"}
{"created":"2023-07-11 11:53:25","title":"Using Linear Regression for Iteratively Training Neural Networks","abstract":"We present a simple linear regression based approach for learning the weights and biases of a neural network, as an alternative to standard gradient based backpropagation. The present work is exploratory in nature, and we restrict the description and experiments to (i) simple feedforward neural networks, (ii) scalar (single output) regression problems, and (iii) invertible activation functions. However, the approach is intended to be extensible to larger, more complex architectures. The key idea is the observation that the input to every neuron in a neural network is a linear combination of the activations of neurons in the previous layer, as well as the parameters (weights and biases) of the layer. If we are able to compute the ideal total input values to every neuron by working backwards from the output, we can formulate the learning problem as a linear least squares problem which iterates between updating the parameters and the activation values. We present an explicit algorithm that implements this idea, and we show that (at least for simple problems) the approach is more stable and faster than gradient-based backpropagation.","sentences":["We present a simple linear regression based approach for learning the weights and biases of a neural network, as an alternative to standard gradient based backpropagation.","The present work is exploratory in nature, and we restrict the description and experiments to (i) simple feedforward neural networks, (ii) scalar (single output) regression problems, and (iii) invertible activation functions.","However, the approach is intended to be extensible to larger, more complex architectures.","The key idea is the observation that the input to every neuron in a neural network is a linear combination of the activations of neurons in the previous layer, as well as the parameters (weights and biases) of the layer.","If we are able to compute the ideal total input values to every neuron by working backwards from the output, we can formulate the learning problem as a linear least squares problem which iterates between updating the parameters and the activation values.","We present an explicit algorithm that implements this idea, and we show that (at least for simple problems)","the approach is more stable and faster than gradient-based backpropagation."],"url":"http://arxiv.org/abs/2307.05189v1"}
{"created":"2023-07-11 11:35:40","title":"Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery","abstract":"Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with Co-Attention gaTed Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse heterogeneous features from visual and textual sources. The fused embedding will feed a standard Data-Efficient Image Transformer (DeiT) module, before the parallel classifier and detector for joint prediction. We conduct the experimental validation on public surgical videos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results highlight the superior performance and robustness of our proposed model compared to the state-of-the-art approaches. Ablation studies further prove the outstanding performance of all the proposed components. The proposed method provides a promising solution for surgical scene understanding, and opens up a primary step in the Artificial Intelligence (AI)-based VQLA system for surgical training. Our code is publicly available.","sentences":["Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery.","However, experts are often busy with clinical and academic work, and have little time to give guidance.","Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers.","In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks.","Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos.","We propose an end-to-end Transformer with Co-Attention gaTed Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not require feature extraction through detection models.","The CAT-ViL embedding module is designed to fuse heterogeneous features from visual and textual sources.","The fused embedding will feed a standard Data-Efficient Image Transformer (DeiT) module, before the parallel classifier and detector for joint prediction.","We conduct the experimental validation on public surgical videos from MICCAI EndoVis Challenge 2017 and 2018.","The experimental results highlight the superior performance and robustness of our proposed model compared to the state-of-the-art approaches.","Ablation studies further prove the outstanding performance of all the proposed components.","The proposed method provides a promising solution for surgical scene understanding, and opens up a primary step in the Artificial Intelligence (AI)-based VQLA system for surgical training.","Our code is publicly available."],"url":"http://arxiv.org/abs/2307.05182v1"}
{"created":"2023-07-11 11:32:12","title":"ResMatch: Residual Attention Learning for Local Feature Matching","abstract":"Attention-based graph neural networks have made great progress in feature matching learning. However, insight of how attention mechanism works for feature matching is lacked in the literature. In this paper, we rethink cross- and self-attention from the viewpoint of traditional feature matching and filtering. In order to facilitate the learning of matching and filtering, we inject the similarity of descriptors and relative positions into cross- and self-attention score, respectively. In this way, the attention can focus on learning residual matching and filtering functions with reference to the basic functions of measuring visual and spatial correlation. Moreover, we mine intra- and inter-neighbors according to the similarity of descriptors and relative positions. Then sparse attention for each point can be performed only within its neighborhoods to acquire higher computation efficiency. Feature matching networks equipped with our full and sparse residual attention learning strategies are termed ResMatch and sResMatch respectively. Extensive experiments, including feature matching, pose estimation and visual localization, confirm the superiority of our networks.","sentences":["Attention-based graph neural networks have made great progress in feature matching learning.","However, insight of how attention mechanism works for feature matching is lacked in the literature.","In this paper, we rethink cross- and self-attention from the viewpoint of traditional feature matching and filtering.","In order to facilitate the learning of matching and filtering, we inject the similarity of descriptors and relative positions into cross- and self-attention score, respectively.","In this way, the attention can focus on learning residual matching and filtering functions with reference to the basic functions of measuring visual and spatial correlation.","Moreover, we mine intra- and inter-neighbors according to the similarity of descriptors and relative positions.","Then sparse attention for each point can be performed only within its neighborhoods to acquire higher computation efficiency.","Feature matching networks equipped with our full and sparse residual attention learning strategies are termed ResMatch and sResMatch respectively.","Extensive experiments, including feature matching, pose estimation and visual localization, confirm the superiority of our networks."],"url":"http://arxiv.org/abs/2307.05180v1"}
{"created":"2023-07-11 11:12:06","title":"Mao-Zedong At SemEval-2023 Task 4: Label Represention Multi-Head Attention Model With Contrastive Learning-Enhanced Nearest Neighbor Mechanism For Multi-Label Text Classification","abstract":"The study of human values is essential in both practical and theoretical domains. With the development of computational linguistics, the creation of large-scale datasets has made it possible to automatically recognize human values accurately. SemEval 2023 Task 4\\cite{kiesel:2023} provides a set of arguments and 20 types of human values that are implicitly expressed in each argument. In this paper, we present our team's solution. We use the Roberta\\cite{liu_roberta_2019} model to obtain the word vector encoding of the document and propose a multi-head attention mechanism to establish connections between specific labels and semantic components. Furthermore, we use a contrastive learning-enhanced K-nearest neighbor mechanism\\cite{su_contrastive_2022} to leverage existing instance information for prediction. Our approach achieved an F1 score of 0.533 on the test set and ranked fourth on the leaderboard.","sentences":["The study of human values is essential in both practical and theoretical domains.","With the development of computational linguistics, the creation of large-scale datasets has made it possible to automatically recognize human values accurately.","SemEval 2023 Task 4\\cite{kiesel:2023} provides a set of arguments and 20 types of human values that are implicitly expressed in each argument.","In this paper, we present our team's solution.","We use the Roberta\\cite{liu_roberta_2019} model to obtain the word vector encoding of the document and propose a multi-head attention mechanism to establish connections between specific labels and semantic components.","Furthermore, we use a contrastive learning-enhanced K-nearest neighbor mechanism\\cite{su_contrastive_2022} to leverage existing instance information for prediction.","Our approach achieved an F1 score of 0.533 on the test set and ranked fourth on the leaderboard."],"url":"http://arxiv.org/abs/2307.05174v1"}
{"created":"2023-07-11 11:05:17","title":"Enriching Verbal Feedback from Usability Testing: Automatic Linking of Thinking-Aloud Recordings and Stimulus using Eye Tracking and Mouse Data","abstract":"The think aloud method is an important and commonly used tool for usability optimization. However, analyzing think aloud data could be time consuming. In this paper, we put forth an automatic analysis of verbal protocols and test the link between spoken feedback and the stimulus using eye tracking and mouse tracking. The gained data - user feedback linked to a specific area of the stimulus - could be used to let an expert review the feedback on specific web page elements or to visualize on which parts of the web page the feedback was given. Specifically, we test if participants fixate on or point with the mouse to the content of the webpage that they are verbalizing. During the testing, participants were shown three websites and asked to verbally give their opinion. The verbal responses, along with the eye and cursor movements were recorded. We compared the hit rate, defined as the percentage of verbally mentioned areas of interest (AOIs) that were fixated with gaze or pointed to with the mouse. The results revealed a significantly higher hit rate for the gaze compared to the mouse data. Further investigation revealed that, while the mouse was mostly used passively to scroll, the gaze was often directed towards relevant AOIs, thus establishing a strong association between spoken words and stimuli. Therefore, eye tracking data possibly provides more detailed information and more valuable insights about the verbalizations compared to the mouse data.","sentences":["The think aloud method is an important and commonly used tool for usability optimization.","However, analyzing think aloud data could be time consuming.","In this paper, we put forth an automatic analysis of verbal protocols and test the link between spoken feedback and the stimulus using eye tracking and mouse tracking.","The gained data - user feedback linked to a specific area of the stimulus - could be used to let an expert review the feedback on specific web page elements or to visualize on which parts of the web page the feedback was given.","Specifically, we test if participants fixate on or point with the mouse to the content of the webpage that they are verbalizing.","During the testing, participants were shown three websites and asked to verbally give their opinion.","The verbal responses, along with the eye and cursor movements were recorded.","We compared the hit rate, defined as the percentage of verbally mentioned areas of interest (AOIs) that were fixated with gaze or pointed to with the mouse.","The results revealed a significantly higher hit rate for the gaze compared to the mouse data.","Further investigation revealed that, while the mouse was mostly used passively to scroll, the gaze was often directed towards relevant AOIs, thus establishing a strong association between spoken words and stimuli.","Therefore, eye tracking data possibly provides more detailed information and more valuable insights about the verbalizations compared to the mouse data."],"url":"http://arxiv.org/abs/2307.05171v1"}
{"created":"2023-07-11 11:05:10","title":"Neural Quantile Optimization for Edge-Cloud Computing","abstract":"We seek the best traffic allocation scheme for the edge-cloud computing network that satisfies constraints and minimizes the cost based on burstable billing. First, for a fixed network topology, we formulate a family of integer programming problems with random parameters describing the various traffic demands. Then, to overcome the difficulty caused by the discrete feature of the problem, we generalize the Gumbel-softmax reparameterization method to induce an unconstrained continuous optimization problem as a regularized continuation of the discrete problem. Finally, we introduce the Gumbel-softmax sampling network to solve the optimization problems via unsupervised learning. The network structure reflects the edge-cloud computing topology and is trained to minimize the expectation of the cost function for unconstrained continuous optimization problems. The trained network works as an efficient traffic allocation scheme sampler, remarkably outperforming the random strategy in feasibility and cost function value. Besides testing the quality of the output allocation scheme, we examine the generalization property of the network by increasing the time steps and the number of users. We also feed the solution to existing integer optimization solvers as initial conditions and verify the warm-starts can accelerate the short-time iteration process. The framework is general with solid performance, and the decoupled feature of the random neural networks is adequate for practical implementations.","sentences":["We seek the best traffic allocation scheme for the edge-cloud computing network that satisfies constraints and minimizes the cost based on burstable billing.","First, for a fixed network topology, we formulate a family of integer programming problems with random parameters describing the various traffic demands.","Then, to overcome the difficulty caused by the discrete feature of the problem, we generalize the Gumbel-softmax reparameterization method to induce an unconstrained continuous optimization problem as a regularized continuation of the discrete problem.","Finally, we introduce the Gumbel-softmax sampling network to solve the optimization problems via unsupervised learning.","The network structure reflects the edge-cloud computing topology and is trained to minimize the expectation of the cost function for unconstrained continuous optimization problems.","The trained network works as an efficient traffic allocation scheme sampler, remarkably outperforming the random strategy in feasibility and cost function value.","Besides testing the quality of the output allocation scheme, we examine the generalization property of the network by increasing the time steps and the number of users.","We also feed the solution to existing integer optimization solvers as initial conditions and verify the warm-starts can accelerate the short-time iteration process.","The framework is general with solid performance, and the decoupled feature of the random neural networks is adequate for practical implementations."],"url":"http://arxiv.org/abs/2307.05170v1"}
{"created":"2023-07-11 10:53:45","title":"A Non-Custodial Wallet for CBDC: Design Challenges and Opportunities","abstract":"Central Bank Digital Currency (CBDC) is a novel form of money that could be issued and regulated by central banks, offering benefits such as programmability, security, and privacy. However, the design of a CBDC system presents numerous technical and social challenges. This paper presents the design and prototype of a non-custodial wallet, a device that enables users to store and spend CBDC in various contexts. To address the challenges of designing a CBDC system, we conducted a series of workshops with internal and external stakeholders, using methods such as storytelling, metaphors, and provotypes to communicate CBDC concepts, elicit user feedback and critique, and incorporate normative values into the technical design. We derived basic guidelines for designing CBDC systems that balance technical and social aspects, and reflect user needs and values. Our paper contributes to the CBDC discourse by demonstrating a practical example of how CBDC could be used in everyday life and by highlighting the importance of a user-centred approach.","sentences":["Central Bank Digital Currency (CBDC) is a novel form of money that could be issued and regulated by central banks, offering benefits such as programmability, security, and privacy.","However, the design of a CBDC system presents numerous technical and social challenges.","This paper presents the design and prototype of a non-custodial wallet, a device that enables users to store and spend CBDC in various contexts.","To address the challenges of designing a CBDC system, we conducted a series of workshops with internal and external stakeholders, using methods such as storytelling, metaphors, and provotypes to communicate CBDC concepts, elicit user feedback and critique, and incorporate normative values into the technical design.","We derived basic guidelines for designing CBDC systems that balance technical and social aspects, and reflect user needs and values.","Our paper contributes to the CBDC discourse by demonstrating a practical example of how CBDC could be used in everyday life and by highlighting the importance of a user-centred approach."],"url":"http://arxiv.org/abs/2307.05167v1"}
{"created":"2023-07-11 10:41:41","title":"A Mapping Study of Machine Learning Methods for Remaining Useful Life Estimation of Lead-Acid Batteries","abstract":"Energy storage solutions play an increasingly important role in modern infrastructure and lead-acid batteries are among the most commonly used in the rechargeable category. Due to normal degradation over time, correctly determining the battery's State of Health (SoH) and Remaining Useful Life (RUL) contributes to enhancing predictive maintenance, reliability, and longevity of battery systems. Besides improving the cost savings, correct estimation of the SoH can lead to reduced pollution though reuse of retired batteries. This paper presents a mapping study of the state-of-the-art in machine learning methods for estimating the SoH and RUL of lead-acid batteries. These two indicators are critical in the battery management systems of electric vehicles, renewable energy systems, and other applications that rely heavily on this battery technology. In this study, we analyzed the types of machine learning algorithms employed for estimating SoH and RUL, and evaluated their performance in terms of accuracy and inference time. Additionally, this mapping identifies and analyzes the most commonly used combinations of sensors in specific applications, such as vehicular batteries. The mapping concludes by highlighting potential gaps and opportunities for future research, which lays the foundation for further advancements in the field.","sentences":["Energy storage solutions play an increasingly important role in modern infrastructure and lead-acid batteries are among the most commonly used in the rechargeable category.","Due to normal degradation over time, correctly determining the battery's State of Health (SoH) and Remaining Useful Life (RUL) contributes to enhancing predictive maintenance, reliability, and longevity of battery systems.","Besides improving the cost savings, correct estimation of the SoH can lead to reduced pollution though reuse of retired batteries.","This paper presents a mapping study of the state-of-the-art in machine learning methods for estimating the SoH and RUL of lead-acid batteries.","These two indicators are critical in the battery management systems of electric vehicles, renewable energy systems, and other applications that rely heavily on this battery technology.","In this study, we analyzed the types of machine learning algorithms employed for estimating SoH and RUL, and evaluated their performance in terms of accuracy and inference time.","Additionally, this mapping identifies and analyzes the most commonly used combinations of sensors in specific applications, such as vehicular batteries.","The mapping concludes by highlighting potential gaps and opportunities for future research, which lays the foundation for further advancements in the field."],"url":"http://arxiv.org/abs/2307.05163v1"}
{"created":"2023-07-11 10:38:58","title":"SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization","abstract":"Finetuning Large Language Models helps improve the results for domain-specific use cases. End-to-end finetuning of large language models is time and resource intensive and has high storage requirements to store the finetuned version of the large language model. Parameter Efficient Fine Tuning (PEFT) methods address the time and resource challenges by keeping the large language model as a fixed base and add additional layers, which the PEFT methods finetune. This paper demonstrates the evaluation results for one such PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization. The evaluation results show that LoRA works at par with end-to-end finetuning for a large language model. The paper presents the evaluations done for solving both the Subtask A and B from ImageCLEFmedical {https://www.imageclef.org/2023/medical}","sentences":["Finetuning Large Language Models helps improve the results for domain-specific use cases.","End-to-end finetuning of large language models is time and resource intensive and has high storage requirements to store the finetuned version of the large language model.","Parameter Efficient Fine Tuning (PEFT) methods address the time and resource challenges by keeping the large language model as a fixed base and add additional layers, which the PEFT methods finetune.","This paper demonstrates the evaluation results for one such PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.","The evaluation results show that LoRA works at par with end-to-end finetuning for a large language model.","The paper presents the evaluations done for solving both the Subtask A and B from ImageCLEFmedical {https://www.imageclef.org/2023/medical}"],"url":"http://arxiv.org/abs/2307.05162v1"}
{"created":"2023-07-11 10:37:57","title":"On the Effectiveness of Speech Self-supervised Learning for Music","abstract":"Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent speech models such as wav2vec2.0 have shown promise in music modelling. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms.","sentences":["Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications.","However, its efficacy in music information retrieval (MIR) still remains largely unexplored.","While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent speech models such as wav2vec2.0 have shown promise in music modelling.","Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited.","We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively.","We train $12$ SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks.","Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech.","However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information.","Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms."],"url":"http://arxiv.org/abs/2307.05161v1"}
{"created":"2023-07-11 10:30:33","title":"A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings","abstract":"Predicting where a person is looking is a complex task, requiring to understand not only the person's gaze and scene content, but also the 3D scene structure and the person's situation (are they manipulating? interacting or observing others? attentive?) to detect obstructions in the line of sight or apply attention priors that humans typically have when observing others. In this paper, we hypothesize that identifying and leveraging such priors can be better achieved through the exploitation of explicitly derived multimodal cues such as depth and pose. We thus propose a modular multimodal architecture allowing to combine these cues using an attention mechanism. The architecture can naturally be exploited in privacy-sensitive situations such as surveillance and health, where personally identifiable information cannot be released. We perform extensive experiments on the GazeFollow and VideoAttentionTarget public datasets, obtaining state-of-the-art performance and demonstrating very competitive results in the privacy setting case.","sentences":["Predicting where a person is looking is a complex task, requiring to understand not only the person's gaze and scene content, but also the 3D scene structure and the person's situation (are they manipulating?","interacting or observing others?","attentive?) to detect obstructions in the line of sight or apply attention priors that humans typically have when observing others.","In this paper, we hypothesize that identifying and leveraging such priors can be better achieved through the exploitation of explicitly derived multimodal cues such as depth and pose.","We thus propose a modular multimodal architecture allowing to combine these cues using an attention mechanism.","The architecture can naturally be exploited in privacy-sensitive situations such as surveillance and health, where personally identifiable information cannot be released.","We perform extensive experiments on the GazeFollow and VideoAttentionTarget public datasets, obtaining state-of-the-art performance and demonstrating very competitive results in the privacy setting case."],"url":"http://arxiv.org/abs/2307.05158v1"}
{"created":"2023-07-11 10:26:05","title":"Stable Normative Explanations: From Argumentation to Deontic Logic","abstract":"This paper examines how a notion of stable explanation developed elsewhere in Defeasible Logic can be expressed in the context of formal argumentation. With this done, we discuss the deontic meaning of this reconstruction and show how to build from argumentation neighborhood structures for deontic logic where this notion of explanation can be characterised. Some direct complexity results are offered.","sentences":["This paper examines how a notion of stable explanation developed elsewhere in Defeasible Logic can be expressed in the context of formal argumentation.","With this done, we discuss the deontic meaning of this reconstruction and show how to build from argumentation neighborhood structures for deontic logic where this notion of explanation can be characterised.","Some direct complexity results are offered."],"url":"http://arxiv.org/abs/2307.05156v1"}
{"created":"2023-07-11 10:14:41","title":"ExFaceGAN: Exploring Identity Directions in GAN's Learned Latent Space for Synthetic Identity Generation","abstract":"Deep generative models have recently presented impressive results in generating realistic face images of random synthetic identities. To generate multiple samples of a certain synthetic identity, several previous works proposed to disentangle the latent space of GANs by incorporating additional supervision or regularization, enabling the manipulation of certain attributes, e.g. identity, hairstyle, pose, or expression. Most of these works require designing special loss functions and training dedicated network architectures. Others proposed to disentangle specific factors in unconditional pretrained GANs latent spaces to control their output, which also requires supervision by attribute classifiers. Moreover, these attributes are entangled in GAN's latent space, making it difficult to manipulate them without affecting the identity information. We propose in this work a framework, ExFaceGAN, to disentangle identity information in state-of-the-art pretrained GANs latent spaces, enabling the generation of multiple samples of any synthetic identity. The variations in our generated images are not limited to specific attributes as ExFaceGAN explicitly aims at disentangling identity information, while other visual attributes are randomly drawn from a learned GAN latent space. As an example of the practical benefit of our ExFaceGAN, we empirically prove that data generated by ExFaceGAN can be successfully used to train face recognition models.","sentences":["Deep generative models have recently presented impressive results in generating realistic face images of random synthetic identities.","To generate multiple samples of a certain synthetic identity, several previous works proposed to disentangle the latent space of GANs by incorporating additional supervision or regularization, enabling the manipulation of certain attributes, e.g. identity, hairstyle, pose, or expression.","Most of these works require designing special loss functions and training dedicated network architectures.","Others proposed to disentangle specific factors in unconditional pretrained GANs latent spaces to control their output, which also requires supervision by attribute classifiers.","Moreover, these attributes are entangled in GAN's latent space, making it difficult to manipulate them without affecting the identity information.","We propose in this work a framework, ExFaceGAN, to disentangle identity information in state-of-the-art pretrained GANs latent spaces, enabling the generation of multiple samples of any synthetic identity.","The variations in our generated images are not limited to specific attributes as ExFaceGAN explicitly aims at disentangling identity information, while other visual attributes are randomly drawn from a learned GAN latent space.","As an example of the practical benefit of our ExFaceGAN, we empirically prove that data generated by ExFaceGAN can be successfully used to train face recognition models."],"url":"http://arxiv.org/abs/2307.05151v1"}
{"created":"2023-07-11 10:13:25","title":"A Modal Logic for Explaining some Graph Neural Networks","abstract":"In this paper, we propose a modal logic in which counting modalities appear in linear inequalities. We show that each formula can be transformed into an equivalent graph neural network (GNN). We also show that each GNN can be transformed into a formula. We show that the satisfiability problem is decidable. We also discuss some variants that are in PSPACE.","sentences":["In this paper, we propose a modal logic in which counting modalities appear in linear inequalities.","We show that each formula can be transformed into an equivalent graph neural network (GNN).","We also show that each GNN can be transformed into a formula.","We show that the satisfiability problem is decidable.","We also discuss some variants that are in PSPACE."],"url":"http://arxiv.org/abs/2307.05150v1"}
{"created":"2023-07-11 10:04:52","title":"Tests4Py: A Benchmark for System Testing","abstract":"Benchmarks are among the main drivers of progress in software engineering research, especially in software testing and debugging. However, current benchmarks in this field could be better suited for specific research tasks, as they rely on weak system oracles like crash detection, come with few unit tests only, need more elaborative research, or cannot verify the outcome of system tests.   Our Tests4Py benchmark addresses these issues. It is derived from the popular BugsInPy benchmark, including 30 bugs from 5 real-world Python applications. Each subject in Tests4Py comes with an oracle to verify the functional correctness of system inputs. Besides, it enables the generation of system tests and unit tests, allowing for qualitative studies by investigating essential aspects of test sets and extensive evaluations. These opportunities make Tests4Py a next-generation benchmark for research in test generation, debugging, and automatic program repair.","sentences":["Benchmarks are among the main drivers of progress in software engineering research, especially in software testing and debugging.","However, current benchmarks in this field could be better suited for specific research tasks, as they rely on weak system oracles like crash detection, come with few unit tests only, need more elaborative research, or cannot verify the outcome of system tests.   ","Our Tests4Py benchmark addresses these issues.","It is derived from the popular BugsInPy benchmark, including 30 bugs from 5 real-world Python applications.","Each subject in Tests4Py comes with an oracle to verify the functional correctness of system inputs.","Besides, it enables the generation of system tests and unit tests, allowing for qualitative studies by investigating essential aspects of test sets and extensive evaluations.","These opportunities make Tests4Py a next-generation benchmark for research in test generation, debugging, and automatic program repair."],"url":"http://arxiv.org/abs/2307.05147v1"}
{"created":"2023-07-11 09:47:54","title":"Process-Algebraic Models of Multi-Writer Multi-Reader Non-Atomic Registers","abstract":"We present process-algebraic models of multi-writer multi-reader safe, regular and atomic registers. We establish the relationship between our models and alternative versions presented in the literature. We use our models to formally analyse by model checking to what extent several well-known mutual exclusion algorithms are robust for relaxed atomicity requirements. Our analyses refute correctness claims made about some of these algorithms in the literature.","sentences":["We present process-algebraic models of multi-writer multi-reader safe, regular and atomic registers.","We establish the relationship between our models and alternative versions presented in the literature.","We use our models to formally analyse by model checking to what extent several well-known mutual exclusion algorithms are robust for relaxed atomicity requirements.","Our analyses refute correctness claims made about some of these algorithms in the literature."],"url":"http://arxiv.org/abs/2307.05143v1"}
{"created":"2023-07-11 09:34:15","title":"Deep Probabilistic Movement Primitives with a Bayesian Aggregator","abstract":"Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive architecture that encodes all the operations above and uses a Bayesian context aggregator that allows a more sound context conditioning and blending. Our results demonstrate our approach can scale to reproduce complex motions on a larger variety of input choices compared to baselines while maintaining operations of linear movement primitives provide.","sentences":["Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations.","Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object).","Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations.","However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications.","This paper proposes a deep movement primitive architecture that encodes all the operations above and uses a Bayesian context aggregator that allows a more sound context conditioning and blending.","Our results demonstrate our approach can scale to reproduce complex motions on a larger variety of input choices compared to baselines while maintaining operations of linear movement primitives provide."],"url":"http://arxiv.org/abs/2307.05141v1"}
{"created":"2023-07-11 09:27:07","title":"SecFlow: Adaptive Security-Aware Workflow Management System in Multi-Cloud Environments","abstract":"In this paper, we propose an architecture for a security-aware workflow management system (WfMS) we call SecFlow in answer to the recent developments of combining workflow management systems with Cloud environments and the still lacking abilities of such systems to ensure the security and privacy of cloud-based workflows. The SecFlow architecture focuses on full workflow life cycle coverage as, in addition to the existing approaches to design security-aware processes, there is a need to fill in the gap of maintaining security properties of workflows during their execution phase. To address this gap, we derive the requirements for such a security-aware WfMS and design a system architecture that meets these requirements. SecFlow integrates key functional components such as secure model construction, security-aware service selection, security violation detection, and adaptive response mechanisms while considering all potential malicious parties in multi-tenant and cloud-based WfMS.","sentences":["In this paper, we propose an architecture for a security-aware workflow management system (WfMS) we call SecFlow in answer to the recent developments of combining workflow management systems with Cloud environments and the still lacking abilities of such systems to ensure the security and privacy of cloud-based workflows.","The SecFlow architecture focuses on full workflow life cycle coverage as, in addition to the existing approaches to design security-aware processes, there is a need to fill in the gap of maintaining security properties of workflows during their execution phase.","To address this gap, we derive the requirements for such a security-aware WfMS and design a system architecture that meets these requirements.","SecFlow integrates key functional components such as secure model construction, security-aware service selection, security violation detection, and adaptive response mechanisms while considering all potential malicious parties in multi-tenant and cloud-based WfMS."],"url":"http://arxiv.org/abs/2307.05137v1"}
{"created":"2023-07-11 09:27:00","title":"Unveiling the invisible: Enhanced detection and analysis deteriorated areas in solar PV modules using unsupervised sensing algorithms and 3D augmented reality","abstract":"Solar Photovoltaic (PV) is increasingly being used to address the global concern of energy security. However, hot spot and snail trails in PV modules caused mostly by crakes reduce their efficiency and power capacity. This article presents a groundbreaking methodology for automatically identifying and analyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV) modules, leveraging unsupervised sensing algorithms and 3D Augmented Reality (AR) visualization. By transforming the traditional methods of diagnosis and repair, our approach not only enhances efficiency but also substantially cuts down the cost of PV system maintenance. Validated through computer simulations and real-world image datasets, the proposed framework accurately identifies dirty regions, emphasizing the critical role of regular maintenance in optimizing the power capacity of solar PV modules. Our immediate objective is to leverage drone technology for real-time, automatic solar panel detection, significantly boosting the efficacy of PV maintenance. The proposed methodology could revolutionize solar PV maintenance, enabling swift, precise anomaly detection without human intervention. This could result in significant cost savings, heightened energy production, and improved overall performance of solar PV systems. Moreover, the novel combination of unsupervised sensing algorithms with 3D AR visualization heralds new opportunities for further research and development in solar PV maintenance.","sentences":["Solar Photovoltaic (PV) is increasingly being used to address the global concern of energy security.","However, hot spot and snail trails in PV modules caused mostly by crakes reduce their efficiency and power capacity.","This article presents a groundbreaking methodology for automatically identifying and analyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV) modules, leveraging unsupervised sensing algorithms and 3D Augmented Reality (AR) visualization.","By transforming the traditional methods of diagnosis and repair, our approach not only enhances efficiency but also substantially cuts down the cost of PV system maintenance.","Validated through computer simulations and real-world image datasets, the proposed framework accurately identifies dirty regions, emphasizing the critical role of regular maintenance in optimizing the power capacity of solar PV modules.","Our immediate objective is to leverage drone technology for real-time, automatic solar panel detection, significantly boosting the efficacy of PV maintenance.","The proposed methodology could revolutionize solar PV maintenance, enabling swift, precise anomaly detection without human intervention.","This could result in significant cost savings, heightened energy production, and improved overall performance of solar PV systems.","Moreover, the novel combination of unsupervised sensing algorithms with 3D AR visualization heralds new opportunities for further research and development in solar PV maintenance."],"url":"http://arxiv.org/abs/2307.05136v1"}
{"created":"2023-07-11 09:23:05","title":"TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation","abstract":"The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically depending on the latent noise used as a seed for the images. We also quantify the influence of the number of concepts in the prompt, their order as well as their (color) attributes. Finally, our method allows us to identify some latent seeds that produce better images than others, opening novel directions of research on this understudied topic.","sentences":["The progress in the generation of synthetic images has made it crucial to assess their quality.","While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt.","Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered.","In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images.","It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color.","We conducted a study on several recent T2I models about various aspects.","An additional interesting result we obtained with our approach is that image quality can vary drastically depending on the latent noise used as a seed for the images.","We also quantify the influence of the number of concepts in the prompt, their order as well as their (color) attributes.","Finally, our method allows us to identify some latent seeds that produce better images than others, opening novel directions of research on this understudied topic."],"url":"http://arxiv.org/abs/2307.05134v1"}
{"created":"2023-07-11 09:20:33","title":"Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering","abstract":"This is an overview of the eleventh edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2023. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks b and Synergy, and a new task (MedProcNER) on semantic annotation of clinical content in Spanish with medical procedures, which have a critical role in medical practice. In this edition of BioASQ, 28 competing teams submitted the results of more than 150 distinct systems in total for the three different shared tasks of the challenge. Similarly to previous editions, most of the participating systems achieved competitive performance, suggesting the continuous advancement of the state-of-the-art in the field.","sentences":["This is an overview of the eleventh edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2023.","BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering.","This year, BioASQ consisted of new editions of the two established tasks b and Synergy, and a new task (MedProcNER) on semantic annotation of clinical content in Spanish with medical procedures, which have a critical role in medical practice.","In this edition of BioASQ, 28 competing teams submitted the results of more than 150 distinct systems in total for the three different shared tasks of the challenge.","Similarly to previous editions, most of the participating systems achieved competitive performance, suggesting the continuous advancement of the state-of-the-art in the field."],"url":"http://arxiv.org/abs/2307.05131v1"}
{"created":"2023-07-11 09:11:22","title":"DFR: Depth from Rotation by Uncalibrated Image Rectification with Latitudinal Motion Assumption","abstract":"Despite the increasing prevalence of rotating-style capture (e.g., surveillance cameras), conventional stereo rectification techniques frequently fail due to the rotation-dominant motion and small baseline between views. In this paper, we tackle the challenge of performing stereo rectification for uncalibrated rotating cameras. To that end, we propose Depth-from-Rotation (DfR), a novel image rectification solution that analytically rectifies two images with two-point correspondences and serves for further depth estimation. Specifically, we model the motion of a rotating camera as the camera rotates on a sphere with fixed latitude. The camera's optical axis lies perpendicular to the sphere's surface. We call this latitudinal motion assumption. Then we derive a 2-point analytical solver from directly computing the rectified transformations on the two images. We also present a self-adaptive strategy to reduce the geometric distortion after rectification. Extensive synthetic and real data experiments demonstrate that the proposed method outperforms existing works in effectiveness and efficiency by a significant margin.","sentences":["Despite the increasing prevalence of rotating-style capture (e.g., surveillance cameras), conventional stereo rectification techniques frequently fail due to the rotation-dominant motion and small baseline between views.","In this paper, we tackle the challenge of performing stereo rectification for uncalibrated rotating cameras.","To that end, we propose Depth-from-Rotation (DfR), a novel image rectification solution that analytically rectifies two images with two-point correspondences and serves for further depth estimation.","Specifically, we model the motion of a rotating camera as the camera rotates on a sphere with fixed latitude.","The camera's optical axis lies perpendicular to the sphere's surface.","We call this latitudinal motion assumption.","Then we derive a 2-point analytical solver from directly computing the rectified transformations on the two images.","We also present a self-adaptive strategy to reduce the geometric distortion after rectification.","Extensive synthetic and real data experiments demonstrate that the proposed method outperforms existing works in effectiveness and efficiency by a significant margin."],"url":"http://arxiv.org/abs/2307.05129v1"}
{"created":"2023-07-11 09:10:16","title":"One-Shot Learning for Periocular Recognition: Exploring the Effect of Domain Adaptation and Data Bias on Deep Representations","abstract":"One weakness of machine-learning algorithms is the need to train the models for a new task. This presents a specific challenge for biometric recognition due to the dynamic nature of databases and, in some instances, the reliance on subject collaboration for data collection. In this paper, we investigate the behavior of deep representations in widely used CNN models under extreme data scarcity for One-Shot periocular recognition, a biometric recognition task. We analyze the outputs of CNN layers as identity-representing feature vectors. We examine the impact of Domain Adaptation on the network layers' output for unseen data and evaluate the method's robustness concerning data normalization and generalization of the best-performing layer. We improved state-of-the-art results that made use of networks trained with biometric datasets with millions of images and fine-tuned for the target periocular dataset by utilizing out-of-the-box CNNs trained for the ImageNet Recognition Challenge and standard computer vision algorithms. For example, for the Cross-Eyed dataset, we could reduce the EER by 67% and 79% (from 1.70% and 3.41% to 0.56% and 0.71%) in the Close-World and Open-World protocols, respectively, for the periocular case. We also demonstrate that traditional algorithms like SIFT can outperform CNNs in situations with limited data or scenarios where the network has not been trained with the test classes like the Open-World mode. SIFT alone was able to reduce the EER by 64% and 71.6% (from 1.7% and 3.41% to 0.6% and 0.97%) for Cross-Eyed in the Close-World and Open-World protocols, respectively, and a reduction of 4.6% (from 3.94% to 3.76%) in the PolyU database for the Open-World and single biometric case.","sentences":["One weakness of machine-learning algorithms is the need to train the models for a new task.","This presents a specific challenge for biometric recognition due to the dynamic nature of databases and, in some instances, the reliance on subject collaboration for data collection.","In this paper, we investigate the behavior of deep representations in widely used CNN models under extreme data scarcity for One-Shot periocular recognition, a biometric recognition task.","We analyze the outputs of CNN layers as identity-representing feature vectors.","We examine the impact of Domain Adaptation on the network layers' output for unseen data and evaluate the method's robustness concerning data normalization and generalization of the best-performing layer.","We improved state-of-the-art results that made use of networks trained with biometric datasets with millions of images and fine-tuned for the target periocular dataset by utilizing out-of-the-box CNNs trained for the ImageNet Recognition Challenge and standard computer vision algorithms.","For example, for the Cross-Eyed dataset, we could reduce the EER by 67% and 79% (from 1.70% and 3.41% to 0.56% and 0.71%) in the Close-World and Open-World protocols, respectively, for the periocular case.","We also demonstrate that traditional algorithms like SIFT can outperform CNNs in situations with limited data or scenarios where the network has not been trained with the test classes like the Open-World mode.","SIFT alone was able to reduce the EER by 64% and 71.6% (from 1.7% and 3.41% to 0.6% and 0.97%) for Cross-Eyed in the Close-World and Open-World protocols, respectively, and a reduction of 4.6% (from 3.94% to 3.76%) in the PolyU database for the Open-World and single biometric case."],"url":"http://arxiv.org/abs/2307.05128v1"}
{"created":"2023-07-11 09:05:13","title":"Optimal Coordinated Transmit Beamforming for Networked Integrated Sensing and Communications","abstract":"This paper studies a multi-antenna networked integrated sensing and communications (ISAC) system, in which a set of multi-antenna base stations (BSs) employ the coordinated transmit beamforming to serve multiple single-antenna communication users (CUs) and perform joint target detection by exploiting the reflected signals simultaneously. To facilitate target sensing, the BSs transmit dedicated sensing signals combined with their information signals. Accordingly, we consider two types of CU receivers with and without the capability of canceling the interference from the dedicated sensing signals, respectively. In addition, we investigate two scenarios with and without time synchronization among the BSs. For the scenario with synchronization, the BSs can exploit the target-reflected signals over both the direct links (BS-to-target-to-originated BS links) and the cross-links (BS-to-target-to-other BSs links) for joint detection, while in the unsynchronized scenario, the BSs can only utilize the target-reflected signals over the direct links. For each scenario under different types of CU receivers, we optimize the coordinated transmit beamforming at the BSs to maximize the minimum detection probability over a particular targeted area, while guaranteeing the required minimum signal-to-interference-plus-noise ratio (SINR) constraints at the CUs. These SINR-constrained detection probability maximization problems are recast as non-convex quadratically constrained quadratic programs (QCQPs), which are then optimally solved via the semi-definite relaxation (SDR) technique.","sentences":["This paper studies a multi-antenna networked integrated sensing and communications (ISAC) system, in which a set of multi-antenna base stations (BSs) employ the coordinated transmit beamforming to serve multiple single-antenna communication users (CUs) and perform joint target detection by exploiting the reflected signals simultaneously.","To facilitate target sensing, the BSs transmit dedicated sensing signals combined with their information signals.","Accordingly, we consider two types of CU receivers with and without the capability of canceling the interference from the dedicated sensing signals, respectively.","In addition, we investigate two scenarios with and without time synchronization among the BSs.","For the scenario with synchronization, the BSs can exploit the target-reflected signals over both the direct links (BS-to-target-to-originated BS links) and the cross-links (BS-to-target-to-other BSs links) for joint detection, while in the unsynchronized scenario, the BSs can only utilize the target-reflected signals over the direct links.","For each scenario under different types of CU receivers, we optimize the coordinated transmit beamforming at the BSs to maximize the minimum detection probability over a particular targeted area, while guaranteeing the required minimum signal-to-interference-plus-noise ratio (SINR) constraints at the CUs.","These SINR-constrained detection probability maximization problems are recast as non-convex quadratically constrained quadratic programs (QCQPs), which are then optimally solved via the semi-definite relaxation (SDR) technique."],"url":"http://arxiv.org/abs/2307.05127v1"}
{"created":"2023-07-11 09:01:49","title":"Enhancing Continuous Time Series Modelling with a Latent ODE-LSTM Approach","abstract":"Due to their dynamic properties such as irregular sampling rate and high-frequency sampling, Continuous Time Series (CTS) are found in many applications. Since CTS with irregular sampling rate are difficult to model with standard Recurrent Neural Networks (RNNs), RNNs have been generalised to have continuous-time hidden dynamics defined by a Neural Ordinary Differential Equation (Neural ODE), leading to the ODE-RNN model. Another approach that provides a better modelling is that of the Latent ODE model, which constructs a continuous-time model where a latent state is defined at all times. The Latent ODE model uses a standard RNN as the encoder and a Neural ODE as the decoder. However, since the RNN encoder leads to difficulties with missing data and ill-defined latent variables, a Latent ODE-RNN model has recently been proposed that uses a ODE-RNN model as the encoder instead. Both the Latent ODE and Latent ODE-RNN models are difficult to train due to the vanishing and exploding gradients problem. To overcome this problem, the main contribution of this paper is to propose and illustrate a new model based on a new Latent ODE using an ODE-LSTM (Long Short-Term Memory) network as an encoder -- the Latent ODE-LSTM model. To limit the growth of the gradients the Norm Gradient Clipping strategy was embedded on the Latent ODE-LSTM model. The performance evaluation of the new Latent ODE-LSTM (with and without Norm Gradient Clipping) for modelling CTS with regular and irregular sampling rates is then demonstrated. Numerical experiments show that the new Latent ODE-LSTM performs better than Latent ODE-RNNs and can avoid the vanishing and exploding gradients during training.","sentences":["Due to their dynamic properties such as irregular sampling rate and high-frequency sampling, Continuous Time Series (CTS) are found in many applications.","Since CTS with irregular sampling rate are difficult to model with standard Recurrent Neural Networks (RNNs), RNNs have been generalised to have continuous-time hidden dynamics defined by a Neural Ordinary Differential Equation (Neural ODE), leading to the ODE-RNN model.","Another approach that provides a better modelling is that of the Latent ODE model, which constructs a continuous-time model where a latent state is defined at all times.","The Latent ODE model uses a standard RNN as the encoder and a Neural ODE as the decoder.","However, since the RNN encoder leads to difficulties with missing data and ill-defined latent variables, a Latent ODE-RNN model has recently been proposed that uses a ODE-RNN model as the encoder instead.","Both the Latent ODE and Latent ODE-RNN models are difficult to train due to the vanishing and exploding gradients problem.","To overcome this problem, the main contribution of this paper is to propose and illustrate a new model based on a new Latent ODE using an ODE-LSTM (Long Short-Term Memory) network as an encoder -- the Latent ODE-LSTM model.","To limit the growth of the gradients the Norm Gradient Clipping strategy was embedded on the Latent ODE-LSTM model.","The performance evaluation of the new Latent ODE-LSTM (with and without Norm Gradient Clipping) for modelling CTS with regular and irregular sampling rates is then demonstrated.","Numerical experiments show that the new Latent ODE-LSTM performs better than Latent ODE-RNNs and can avoid the vanishing and exploding gradients during training."],"url":"http://arxiv.org/abs/2307.05126v1"}
{"created":"2023-07-11 08:56:53","title":"Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer","abstract":"How to obtain informative representations of transactions and then perform the identification of fraudulent transactions is a crucial part of ensuring financial security. Recent studies apply Graph Neural Networks (GNNs) to the transaction fraud detection problem. Nevertheless, they encounter challenges in effectively learning spatial-temporal information due to structural limitations. Moreover, few prior GNN-based detectors have recognized the significance of incorporating global information, which encompasses similar behavioral patterns and offers valuable insights for discriminative representation learning. Therefore, we propose a novel heterogeneous graph neural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for transaction fraud detection problems. Specifically, we design a temporal encoding strategy to capture temporal dependencies and incorporate it into the graph neural network framework, enhancing spatial-temporal information modeling and improving expressive ability. Furthermore, we introduce a transformer module to learn local and global information. Pairwise node-node interactions overcome the limitation of the GNN structure and build up the interactions with the target node and long-distance ones. Experimental results on two financial datasets compared to general GNN models and GNN-based fraud detectors demonstrate that our proposed method STA-GT is effective on the transaction fraud detection task.","sentences":["How to obtain informative representations of transactions and then perform the identification of fraudulent transactions is a crucial part of ensuring financial security.","Recent studies apply Graph Neural Networks (GNNs) to the transaction fraud detection problem.","Nevertheless, they encounter challenges in effectively learning spatial-temporal information due to structural limitations.","Moreover, few prior GNN-based detectors have recognized the significance of incorporating global information, which encompasses similar behavioral patterns and offers valuable insights for discriminative representation learning.","Therefore, we propose a novel heterogeneous graph neural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for transaction fraud detection problems.","Specifically, we design a temporal encoding strategy to capture temporal dependencies and incorporate it into the graph neural network framework, enhancing spatial-temporal information modeling and improving expressive ability.","Furthermore, we introduce a transformer module to learn local and global information.","Pairwise node-node interactions overcome the limitation of the GNN structure and build up the interactions with the target node and long-distance ones.","Experimental results on two financial datasets compared to general GNN models and GNN-based fraud detectors demonstrate that our proposed method STA-GT is effective on the transaction fraud detection task."],"url":"http://arxiv.org/abs/2307.05121v1"}
{"created":"2023-07-11 08:51:53","title":"$\\ell_p$-Regression in the Arbitrary Partition Model of Communication","abstract":"We consider the randomized communication complexity of the distributed $\\ell_p$-regression problem in the coordinator model, for $p\\in (0,2]$. In this problem, there is a coordinator and $s$ servers. The $i$-th server receives $A^i\\in\\{-M, -M+1, \\ldots, M\\}^{n\\times d}$ and $b^i\\in\\{-M, -M+1, \\ldots, M\\}^n$ and the coordinator would like to find a $(1+\\epsilon)$-approximate solution to $\\min_{x\\in\\mathbb{R}^n} \\|(\\sum_i A^i)x - (\\sum_i b^i)\\|_p$. Here $M \\leq \\mathrm{poly}(nd)$ for convenience. This model, where the data is additively shared across servers, is commonly referred to as the arbitrary partition model.   We obtain significantly improved bounds for this problem. For $p = 2$, i.e., least squares regression, we give the first optimal bound of $\\tilde{\\Theta}(sd^2 + sd/\\epsilon)$ bits.   For $p \\in (1,2)$,we obtain an $\\tilde{O}(sd^2/\\epsilon + sd/\\mathrm{poly}(\\epsilon))$ upper bound. Notably, for $d$ sufficiently large, our leading order term only depends linearly on $1/\\epsilon$ rather than quadratically. We also show communication lower bounds of $\\Omega(sd^2 + sd/\\epsilon^2)$ for $p\\in (0,1]$ and $\\Omega(sd^2 + sd/\\epsilon)$ for $p\\in (1,2]$. Our bounds considerably improve previous bounds due to (Woodruff et al. COLT, 2013) and (Vempala et al., SODA, 2020).","sentences":["We consider the randomized communication complexity of the distributed $\\ell_p$-regression problem in the coordinator model, for $p\\in (0,2]$. In this problem, there is a coordinator and $s$ servers.","The $i$-th server receives $A^i\\in\\{-M, -M+1, \\ldots, M\\}^{n\\times d}$ and $b^i\\in\\{-M, -M+1, \\ldots, M\\}^n$ and the coordinator would like to find a $(1+\\epsilon)$-approximate solution to $\\min_{x\\in\\mathbb{R}^n} \\|(\\sum_i A^i)x - (\\sum_i b^i)\\|_p$.","Here $M \\leq \\mathrm{poly}(nd)$ for convenience.","This model, where the data is additively shared across servers, is commonly referred to as the arbitrary partition model.   ","We obtain significantly improved bounds for this problem.","For $p = 2$, i.e., least squares regression, we give the first optimal bound of $\\tilde{\\Theta}(sd^2 + sd/\\epsilon)$ bits.   ","For $p \\in (1,2)$,we obtain an $\\tilde{O}(sd^2/\\epsilon + sd/\\mathrm{poly}(\\epsilon))$ upper bound.","Notably, for $d$ sufficiently large, our leading order term only depends linearly on $1/\\epsilon$ rather than quadratically.","We also show communication lower bounds of $\\Omega(sd^2 + sd/\\epsilon^2)$ for $p\\in (0,1]$ and $\\Omega(sd^2 + sd/\\epsilon)$ for $p\\in (1,2]$. Our bounds considerably improve previous bounds due to (Woodruff et al. COLT, 2013) and (Vempala et al., SODA, 2020)."],"url":"http://arxiv.org/abs/2307.05117v1"}
{"created":"2023-07-11 08:45:46","title":"Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark)","abstract":"This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario reasoning, aiming to close the gap in artificial neural networks' ability to reason in everyday contexts. In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich information on real-life scenarios, human behaviors, and character roles. The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its quality. Experiments are conducted using state-of-the-art language models, such as gpt3.5-turbo and instruction fine-tuned llama models, to test the performance in LSR-Benchmark. The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine learning models in comprehending daily human life.","sentences":["This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario reasoning, aiming to close the gap in artificial neural networks' ability to reason in everyday contexts.","In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich information on real-life scenarios, human behaviors, and character roles.","The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its quality.","Experiments are conducted using state-of-the-art language models, such as gpt3.5-turbo and instruction fine-tuned llama models, to test the performance in LSR-Benchmark.","The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine learning models in comprehending daily human life."],"url":"http://arxiv.org/abs/2307.05113v1"}
{"created":"2023-07-11 08:36:12","title":"Conformalization of Sparse Generalized Linear Models","abstract":"Given a sequence of observable variables $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$, the conformal prediction method estimates a confidence set for $y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merely assuming that the joint distribution of the data is permutation invariant. Although attractive, computing such a set is computationally infeasible in most regression problems. Indeed, in these cases, the unknown variable $y_{n+1}$ can take an infinite number of possible candidate values, and generating conformal sets requires retraining a predictive model for each candidate. In this paper, we focus on a sparse linear model with only a subset of variables for prediction and use numerical continuation techniques to approximate the solution path efficiently. The critical property we exploit is that the set of selected variables is invariant under a small perturbation of the input data. Therefore, it is sufficient to enumerate and refit the model only at the change points of the set of active features and smoothly interpolate the rest of the solution via a Predictor-Corrector mechanism. We show how our path-following algorithm accurately approximates conformal prediction sets and illustrate its performance using synthetic and real data examples.","sentences":["Given a sequence of observable variables $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$, the conformal prediction method estimates a confidence set for $y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merely assuming that the joint distribution of the data is permutation invariant.","Although attractive, computing such a set is computationally infeasible in most regression problems.","Indeed, in these cases, the unknown variable $y_{n+1}$ can take an infinite number of possible candidate values, and generating conformal sets requires retraining a predictive model for each candidate.","In this paper, we focus on a sparse linear model with only a subset of variables for prediction and use numerical continuation techniques to approximate the solution path efficiently.","The critical property we exploit is that the set of selected variables is invariant under a small perturbation of the input data.","Therefore, it is sufficient to enumerate and refit the model only at the change points of the set of active features and smoothly interpolate the rest of the solution via a Predictor-Corrector mechanism.","We show how our path-following algorithm accurately approximates conformal prediction sets and illustrate its performance using synthetic and real data examples."],"url":"http://arxiv.org/abs/2307.05109v1"}
{"created":"2023-07-11 08:34:11","title":"Optimizing Feature Extraction for Symbolic Music","abstract":"This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score. In this regard, we propose a novel feature extraction tool, named musif, and evaluate its efficacy on various repertoires and file formats, including MIDI, MusicXML, and **kern. Musif approximates existing tools such as jSymbolic and music21 in terms of computational efficiency while attempting to enhance the usability for custom feature development. The proposed tool also enhances classification accuracy when combined with other sets of features. We demonstrate the contribution of each set of features and the computational resources they require. Our findings indicate that the optimal tool for feature extraction is a combination of the best features from each tool rather than those of a single one. To facilitate future research in music information retrieval, we release the source code of the tool and benchmarks.","sentences":["This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score.","In this regard, we propose a novel feature extraction tool, named musif, and evaluate its efficacy on various repertoires and file formats, including MIDI, MusicXML, and **kern.","Musif approximates existing tools such as jSymbolic and music21 in terms of computational efficiency while attempting to enhance the usability for custom feature development.","The proposed tool also enhances classification accuracy when combined with other sets of features.","We demonstrate the contribution of each set of features and the computational resources they require.","Our findings indicate that the optimal tool for feature extraction is a combination of the best features from each tool rather than those of a single one.","To facilitate future research in music information retrieval, we release the source code of the tool and benchmarks."],"url":"http://arxiv.org/abs/2307.05107v1"}
{"created":"2023-07-11 08:30:57","title":"Tree-Based Scenario Classification: A Formal Framework for Coverage Analysis on Test Drives of Autonomous Vehicles","abstract":"Scenario-based testing is envisioned as a key approach for the safety assurance of autonomous vehicles. In scenario-based testing, relevant (driving) scenarios are the basis of tests. Many recent works focus on specification, variation, generation and execution of individual scenarios. In this work, we address the open challenges of classifying sets of scenarios and measuring coverage of theses scenarios in recorded test drives. Technically, we define logic-based classifiers that compute features of scenarios on complex data streams and combine these classifiers into feature trees that describe sets of scenarios. We demonstrate the expressiveness and effectiveness of our approach by defining a scenario classifier for urban driving and evaluating it on data recorded from simulations.","sentences":["Scenario-based testing is envisioned as a key approach for the safety assurance of autonomous vehicles.","In scenario-based testing, relevant (driving) scenarios are the basis of tests.","Many recent works focus on specification, variation, generation and execution of individual scenarios.","In this work, we address the open challenges of classifying sets of scenarios and measuring coverage of theses scenarios in recorded test drives.","Technically, we define logic-based classifiers that compute features of scenarios on complex data streams and combine these classifiers into feature trees that describe sets of scenarios.","We demonstrate the expressiveness and effectiveness of our approach by defining a scenario classifier for urban driving and evaluating it on data recorded from simulations."],"url":"http://arxiv.org/abs/2307.05106v1"}
{"created":"2023-07-11 08:26:08","title":"A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI","abstract":"Explainable Artificial Intelligence (XAI) has gained significant attention recently as the demand for transparency and interpretability of machine learning models has increased. In particular, XAI for time series data has become increasingly important in finance, healthcare, and climate science. However, evaluating the quality of explanations, such as attributions provided by XAI techniques, remains challenging. This paper provides an in-depth analysis of using perturbations to evaluate attributions extracted from time series models. A perturbation analysis involves systematically modifying the input data and evaluating the impact on the attributions generated by the XAI method. We apply this approach to several state-of-the-art XAI techniques and evaluate their performance on three time series classification datasets. Our results demonstrate that the perturbation analysis approach can effectively evaluate the quality of attributions and provide insights into the strengths and limitations of XAI techniques. Such an approach can guide the selection of XAI methods for time series data, e.g., focusing on return time rather than precision, and facilitate the development of more reliable and interpretable machine learning models for time series analysis.","sentences":["Explainable Artificial Intelligence (XAI) has gained significant attention recently as the demand for transparency and interpretability of machine learning models has increased.","In particular, XAI for time series data has become increasingly important in finance, healthcare, and climate science.","However, evaluating the quality of explanations, such as attributions provided by XAI techniques, remains challenging.","This paper provides an in-depth analysis of using perturbations to evaluate attributions extracted from time series models.","A perturbation analysis involves systematically modifying the input data and evaluating the impact on the attributions generated by the XAI method.","We apply this approach to several state-of-the-art XAI techniques and evaluate their performance on three time series classification datasets.","Our results demonstrate that the perturbation analysis approach can effectively evaluate the quality of attributions and provide insights into the strengths and limitations of XAI techniques.","Such an approach can guide the selection of XAI methods for time series data, e.g., focusing on return time rather than precision, and facilitate the development of more reliable and interpretable machine learning models for time series analysis."],"url":"http://arxiv.org/abs/2307.05104v1"}
{"created":"2023-07-11 08:24:25","title":"Rational Solutions of Parametric First-Order Algebraic Differential Equations","abstract":"In this paper we give a procedure for finding rational solutions of a given first-order ODE with functional and constant coefficients which occur in a rational way. We derive an associated system with the same solvability, and sufficient and necessary conditions for the existence of rational solutions are given. In the case where all parametric coefficients are constant, we give an algorithm to compute the rational solutions. In the case where one functional coefficient appears, we algorithmically find rational general solutions which rationally depend on the appearing transcendental constant. In the other cases, the presented procedure is not completely algorithmic.","sentences":["In this paper we give a procedure for finding rational solutions of a given first-order ODE with functional and constant coefficients which occur in a rational way.","We derive an associated system with the same solvability, and sufficient and necessary conditions for the existence of rational solutions are given.","In the case where all parametric coefficients are constant, we give an algorithm to compute the rational solutions.","In the case where one functional coefficient appears, we algorithmically find rational general solutions which rationally depend on the appearing transcendental constant.","In the other cases, the presented procedure is not completely algorithmic."],"url":"http://arxiv.org/abs/2307.05102v1"}
{"created":"2023-07-11 08:20:53","title":"Generative Contrastive Graph Learning for Recommendation","abstract":"By treating users' interactions as a user-item graph, graph learning models have been widely deployed in Collaborative Filtering(CF) based recommendation. Recently, researchers have introduced Graph Contrastive Learning(GCL) techniques into CF to alleviate the sparse supervision issue, which first constructs contrastive views by data augmentations and then provides self-supervised signals by maximizing the mutual information between contrastive views. Despite the effectiveness, we argue that current GCL-based recommendation models are still limited as current data augmentation techniques, either structure augmentation or feature augmentation. First, structure augmentation randomly dropout nodes or edges, which is easy to destroy the intrinsic nature of the user-item graph. Second, feature augmentation imposes the same scale noise augmentation on each node, which neglects the unique characteristics of nodes on the graph. To tackle the above limitations, we propose a novel Variational Graph Generative-Contrastive Learning(VGCL) framework for recommendation. Specifically, we leverage variational graph reconstruction to estimate a Gaussian distribution of each node, then generate multiple contrastive views through multiple samplings from the estimated distributions, which builds a bridge between generative and contrastive learning. Besides, the estimated variances are tailored to each node, which regulates the scale of contrastive loss for each node on optimization. Considering the similarity of the estimated distributions, we propose a cluster-aware twofold contrastive learning, a node-level to encourage consistency of a node's contrastive views and a cluster-level to encourage consistency of nodes in a cluster. Finally, extensive experimental results on three public datasets clearly demonstrate the effectiveness of the proposed model.","sentences":["By treating users' interactions as a user-item graph, graph learning models have been widely deployed in Collaborative Filtering(CF) based recommendation.","Recently, researchers have introduced Graph Contrastive Learning(GCL) techniques into CF to alleviate the sparse supervision issue, which first constructs contrastive views by data augmentations and then provides self-supervised signals by maximizing the mutual information between contrastive views.","Despite the effectiveness, we argue that current GCL-based recommendation models are still limited as current data augmentation techniques, either structure augmentation or feature augmentation.","First, structure augmentation randomly dropout nodes or edges, which is easy to destroy the intrinsic nature of the user-item graph.","Second, feature augmentation imposes the same scale noise augmentation on each node, which neglects the unique characteristics of nodes on the graph.","To tackle the above limitations, we propose a novel Variational Graph Generative-Contrastive Learning(VGCL) framework for recommendation.","Specifically, we leverage variational graph reconstruction to estimate a Gaussian distribution of each node, then generate multiple contrastive views through multiple samplings from the estimated distributions, which builds a bridge between generative and contrastive learning.","Besides, the estimated variances are tailored to each node, which regulates the scale of contrastive loss for each node on optimization.","Considering the similarity of the estimated distributions, we propose a cluster-aware twofold contrastive learning, a node-level to encourage consistency of a node's contrastive views and a cluster-level to encourage consistency of nodes in a cluster.","Finally, extensive experimental results on three public datasets clearly demonstrate the effectiveness of the proposed model."],"url":"http://arxiv.org/abs/2307.05100v1"}
{"created":"2023-07-11 08:10:58","title":"The smarty4covid dataset and knowledge base: a framework enabling interpretable analysis of audio signals","abstract":"Harnessing the power of Artificial Intelligence (AI) and m-health towards detecting new bio-markers indicative of the onset and progress of respiratory abnormalities/conditions has greatly attracted the scientific and research interest especially during COVID-19 pandemic. The smarty4covid dataset contains audio signals of cough (4,676), regular breathing (4,665), deep breathing (4,695) and voice (4,291) as recorded by means of mobile devices following a crowd-sourcing approach. Other self reported information is also included (e.g. COVID-19 virus tests), thus providing a comprehensive dataset for the development of COVID-19 risk detection models. The smarty4covid dataset is released in the form of a web-ontology language (OWL) knowledge base enabling data consolidation from other relevant datasets, complex queries and reasoning. It has been utilized towards the development of models able to: (i) extract clinically informative respiratory indicators from regular breathing records, and (ii) identify cough, breath and voice segments in crowd-sourced audio recordings. A new framework utilizing the smarty4covid OWL knowledge base towards generating counterfactual explanations in opaque AI-based COVID-19 risk detection models is proposed and validated.","sentences":["Harnessing the power of Artificial Intelligence (AI) and m-health towards detecting new bio-markers indicative of the onset and progress of respiratory abnormalities/conditions has greatly attracted the scientific and research interest especially during COVID-19 pandemic.","The smarty4covid dataset contains audio signals of cough (4,676), regular breathing (4,665), deep breathing (4,695) and voice (4,291) as recorded by means of mobile devices following a crowd-sourcing approach.","Other self reported information is also included (e.g. COVID-19 virus tests), thus providing a comprehensive dataset for the development of COVID-19 risk detection models.","The smarty4covid dataset is released in the form of a web-ontology language (OWL) knowledge base enabling data consolidation from other relevant datasets, complex queries and reasoning.","It has been utilized towards the development of models able to: (i) extract clinically informative respiratory indicators from regular breathing records, and (ii) identify cough, breath and voice segments in crowd-sourced audio recordings.","A new framework utilizing the smarty4covid OWL knowledge base towards generating counterfactual explanations in opaque AI-based COVID-19 risk detection models is proposed and validated."],"url":"http://arxiv.org/abs/2307.05096v1"}
{"created":"2023-07-11 08:07:10","title":"ATWM: Defense against adversarial malware based on adversarial training","abstract":"Deep learning technology has made great achievements in the field of image. In order to defend against malware attacks, researchers have proposed many Windows malware detection models based on deep learning. However, deep learning models are vulnerable to adversarial example attacks. Malware can generate adversarial malware with the same malicious function to attack the malware detection model and evade detection of the model. Currently, many adversarial defense studies have been proposed, but existing adversarial defense studies are based on image sample and cannot be directly applied to malware sample. Therefore, this paper proposes an adversarial malware defense method based on adversarial training. This method uses preprocessing to defend simple adversarial examples to reduce the difficulty of adversarial training. Moreover, this method improves the adversarial defense capability of the model through adversarial training. We experimented with three attack methods in two sets of datasets, and the results show that the method in this paper can improve the adversarial defense capability of the model without reducing the accuracy of the model.","sentences":["Deep learning technology has made great achievements in the field of image.","In order to defend against malware attacks, researchers have proposed many Windows malware detection models based on deep learning.","However, deep learning models are vulnerable to adversarial example attacks.","Malware can generate adversarial malware with the same malicious function to attack the malware detection model and evade detection of the model.","Currently, many adversarial defense studies have been proposed, but existing adversarial defense studies are based on image sample and cannot be directly applied to malware sample.","Therefore, this paper proposes an adversarial malware defense method based on adversarial training.","This method uses preprocessing to defend simple adversarial examples to reduce the difficulty of adversarial training.","Moreover, this method improves the adversarial defense capability of the model through adversarial training.","We experimented with three attack methods in two sets of datasets, and the results show that the method in this paper can improve the adversarial defense capability of the model without reducing the accuracy of the model."],"url":"http://arxiv.org/abs/2307.05095v1"}
{"created":"2023-07-11 07:57:58","title":"Forward Dynamics Estimation from Data-Driven Inverse Dynamics Learning","abstract":"In this paper, we propose to estimate the forward dynamics equations of mechanical systems by learning a model of the inverse dynamics and estimating individual dynamics components from it. We revisit the classical formulation of rigid body dynamics in order to extrapolate the physical dynamical components, such as inertial and gravitational components, from an inverse dynamics model. After estimating the dynamical components, the forward dynamics can be computed in closed form as a function of the learned inverse dynamics. We tested the proposed method with several machine learning models based on Gaussian Process Regression and compared them with the standard approach of learning the forward dynamics directly. Results on two simulated robotic manipulators, a PANDA Franka Emika and a UR10, show the effectiveness of the proposed method in learning the forward dynamics, both in terms of accuracy as well as in opening the possibility of using more structured~models.","sentences":["In this paper, we propose to estimate the forward dynamics equations of mechanical systems by learning a model of the inverse dynamics and estimating individual dynamics components from it.","We revisit the classical formulation of rigid body dynamics in order to extrapolate the physical dynamical components, such as inertial and gravitational components, from an inverse dynamics model.","After estimating the dynamical components, the forward dynamics can be computed in closed form as a function of the learned inverse dynamics.","We tested the proposed method with several machine learning models based on Gaussian Process Regression and compared them with the standard approach of learning the forward dynamics directly.","Results on two simulated robotic manipulators, a PANDA Franka Emika and a UR10, show the effectiveness of the proposed method in learning the forward dynamics, both in terms of accuracy as well as in opening the possibility of using more structured~models."],"url":"http://arxiv.org/abs/2307.05093v1"}
{"created":"2023-07-11 07:52:06","title":"Offline and Online Optical Flow Enhancement for Deep Video Compression","abstract":"Video compression relies heavily on exploiting the temporal redundancy between video frames, which is usually achieved by estimating and using the motion information. The motion information is represented as optical flows in most of the existing deep video compression networks. Indeed, these networks often adopt pre-trained optical flow estimation networks for motion estimation. The optical flows, however, may be less suitable for video compression due to the following two factors. First, the optical flow estimation networks were trained to perform inter-frame prediction as accurately as possible, but the optical flows themselves may cost too many bits to encode. Second, the optical flow estimation networks were trained on synthetic data, and may not generalize well enough to real-world videos. We address the twofold limitations by enhancing the optical flows in two stages: offline and online. In the offline stage, we fine-tune a trained optical flow estimation network with the motion information provided by a traditional (non-deep) video compression scheme, e.g. H.266/VVC, as we believe the motion information of H.266/VVC achieves a better rate-distortion trade-off. In the online stage, we further optimize the latent features of the optical flows with a gradient descent-based algorithm for the video to be compressed, so as to enhance the adaptivity of the optical flows. We conduct experiments on a state-of-the-art deep video compression scheme, DCVC. Experimental results demonstrate that the proposed offline and online enhancement together achieves on average 12.8% bitrate saving on the tested videos, without increasing the model or computational complexity of the decoder side.","sentences":["Video compression relies heavily on exploiting the temporal redundancy between video frames, which is usually achieved by estimating and using the motion information.","The motion information is represented as optical flows in most of the existing deep video compression networks.","Indeed, these networks often adopt pre-trained optical flow estimation networks for motion estimation.","The optical flows, however, may be less suitable for video compression due to the following two factors.","First, the optical flow estimation networks were trained to perform inter-frame prediction as accurately as possible, but the optical flows themselves may cost too many bits to encode.","Second, the optical flow estimation networks were trained on synthetic data, and may not generalize well enough to real-world videos.","We address the twofold limitations by enhancing the optical flows in two stages: offline and online.","In the offline stage, we fine-tune a trained optical flow estimation network with the motion information provided by a traditional (non-deep) video compression scheme, e.g. H.266/VVC, as we believe the motion information of H.266/VVC achieves a better rate-distortion trade-off.","In the online stage, we further optimize the latent features of the optical flows with a gradient descent-based algorithm for the video to be compressed, so as to enhance the adaptivity of the optical flows.","We conduct experiments on a state-of-the-art deep video compression scheme, DCVC.","Experimental results demonstrate that the proposed offline and online enhancement together achieves on average 12.8% bitrate saving on the tested videos, without increasing the model or computational complexity of the decoder side."],"url":"http://arxiv.org/abs/2307.05092v1"}
{"created":"2023-07-11 07:37:56","title":"SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View Representation","abstract":"SAR images are highly sensitive to observation configurations, and they exhibit significant variations across different viewing angles, making it challenging to represent and learn their anisotropic features. As a result, deep learning methods often generalize poorly across different view angles. Inspired by the concept of neural radiance fields (NeRF), this study combines SAR imaging mechanisms with neural networks to propose a novel NeRF model for SAR image generation. Following the mapping and projection pinciples, a set of SAR images is modeled implicitly as a function of attenuation coefficients and scattering intensities in the 3D imaging space through a differentiable rendering equation. SAR-NeRF is then constructed to learn the distribution of attenuation coefficients and scattering intensities of voxels, where the vectorized form of 3D voxel SAR rendering equation and the sampling relationship between the 3D space voxels and the 2D view ray grids are analytically derived. Through quantitative experiments on various datasets, we thoroughly assess the multi-view representation and generalization capabilities of SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can significantly improve SAR target classification performance under few-shot learning setup, where a 10-type classification accuracy of 91.6\\% can be achieved by using only 12 images per class.","sentences":["SAR images are highly sensitive to observation configurations, and they exhibit significant variations across different viewing angles, making it challenging to represent and learn their anisotropic features.","As a result, deep learning methods often generalize poorly across different view angles.","Inspired by the concept of neural radiance fields (NeRF), this study combines SAR imaging mechanisms with neural networks to propose a novel NeRF model for SAR image generation.","Following the mapping and projection pinciples, a set of SAR images is modeled implicitly as a function of attenuation coefficients and scattering intensities in the 3D imaging space through a differentiable rendering equation.","SAR-NeRF is then constructed to learn the distribution of attenuation coefficients and scattering intensities of voxels, where the vectorized form of 3D voxel SAR rendering equation and the sampling relationship between the 3D space voxels and the 2D view ray grids are analytically derived.","Through quantitative experiments on various datasets, we thoroughly assess the multi-view representation and generalization capabilities of SAR-NeRF.","Additionally, it is found that SAR-NeRF augumented dataset can significantly improve SAR target classification performance under few-shot learning setup, where a 10-type classification accuracy of 91.6\\% can be achieved by using only 12 images per class."],"url":"http://arxiv.org/abs/2307.05087v1"}
{"created":"2023-07-11 07:32:12","title":"Vacaspati: A Diverse Corpus of Bangla Literature","abstract":"Bangla (or Bengali) is the fifth most spoken language globally; yet, the state-of-the-art NLP in Bangla is lagging for even simple tasks such as lemmatization, POS tagging, etc. This is partly due to lack of a varied quality corpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla literature. The literary works are collected from various websites; only those works that are publicly available without copyright violations or restrictions are collected. We believe that published literature captures the features of a language much better than newspapers, blogs or social media posts which tend to follow only a certain literary pattern and, therefore, miss out on language variety. Our corpus Vacaspati is varied from multiple aspects, including type of composition, topic, author, time, space, etc. It contains more than 11 million sentences and 115 million words. We also built a word embedding model, Vac-FT, using FastText from Vacaspati as well as trained an Electra model, Vac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only a fraction of resources compared to other state-of-the-art transformer models and yet performs either better or similar on various downstream tasks. On multiple downstream tasks, Vac-FT outperforms other FastText-based models. We also demonstrate the efficacy of Vacaspati as a corpus by showing that similar models built from other corpora are not as effective. The models are available at https://bangla.iitk.ac.in/.","sentences":["Bangla (or Bengali) is the fifth most spoken language globally; yet, the state-of-the-art NLP in Bangla is lagging for even simple tasks such as lemmatization, POS tagging, etc.","This is partly due to lack of a varied quality corpus.","To alleviate this need, we build Vacaspati, a diverse corpus of Bangla literature.","The literary works are collected from various websites; only those works that are publicly available without copyright violations or restrictions are collected.","We believe that published literature captures the features of a language much better than newspapers, blogs or social media posts which tend to follow only a certain literary pattern and, therefore, miss out on language variety.","Our corpus Vacaspati is varied from multiple aspects, including type of composition, topic, author, time, space, etc.","It contains more than 11 million sentences and 115 million words.","We also built a word embedding model, Vac-FT, using FastText from Vacaspati as well as trained an Electra model, Vac-BERT, using the corpus.","Vac-BERT has far fewer parameters and requires only a fraction of resources compared to other state-of-the-art transformer models and yet performs either better or similar on various downstream tasks.","On multiple downstream tasks, Vac-FT outperforms other FastText-based models.","We also demonstrate the efficacy of Vacaspati as a corpus by showing that similar models built from other corpora are not as effective.","The models are available at https://bangla.iitk.ac.in/."],"url":"http://arxiv.org/abs/2307.05083v1"}
{"created":"2023-07-11 07:31:58","title":"OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning","abstract":"This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT's meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2 LLM. The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology, enabling their adaptation and utilization in various LLM-based systems. This versatile approach opens up new possibilities for NLP and dialogue systems, empowering developers to enhance the performance and functionality of chatbot systems across different domains and languages.","sentences":["This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM).","The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.","The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems.","The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation.","By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses.","The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2 LLM.","The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology, enabling their adaptation and utilization in various LLM-based systems.","This versatile approach opens up new possibilities for NLP and dialogue systems, empowering developers to enhance the performance and functionality of chatbot systems across different domains and languages."],"url":"http://arxiv.org/abs/2307.05082v1"}
{"created":"2023-07-11 07:29:18","title":"Argumentative Segmentation Enhancement for Legal Summarization","abstract":"We use the combination of argumentative zoning [1] and a legal argumentative scheme to create legal argumentative segments. Based on the argumentative segmentation, we propose a novel task of classifying argumentative segments of legal case decisions. GPT-3.5 is used to generate summaries based on argumentative segments. In terms of automatic evaluation metrics, our method generates higher quality argumentative summaries while leaving out less relevant context as compared to GPT-4 and non-GPT models.","sentences":["We use the combination of argumentative zoning [1] and a legal argumentative scheme to create legal argumentative segments.","Based on the argumentative segmentation, we propose a novel task of classifying argumentative segments of legal case decisions.","GPT-3.5 is used to generate summaries based on argumentative segments.","In terms of automatic evaluation metrics, our method generates higher quality argumentative summaries while leaving out less relevant context as compared to GPT-4 and non-GPT models."],"url":"http://arxiv.org/abs/2307.05081v1"}
{"created":"2023-07-11 07:29:09","title":"Estimating label quality and errors in semantic segmentation data via any model","abstract":"The labor-intensive annotation process of semantic segmentation datasets is often prone to errors, since humans struggle to label every pixel correctly. We study algorithms to automatically detect such annotation errors, in particular methods to score label quality, such that the images with the lowest scores are least likely to be correctly labeled. This helps prioritize what data to review in order to ensure a high-quality training/evaluation dataset, which is critical in sensitive applications such as medical imaging and autonomous vehicles. Widely applicable, our label quality scores rely on probabilistic predictions from a trained segmentation model -- any model architecture and training procedure can be utilized. Here we study 7 different label quality scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation model to detect annotation errors in a version of the SYNTHIA dataset. Precision-recall evaluations reveal a score -- the soft-minimum of the model-estimated likelihoods of each pixel's annotated class -- that is particularly effective to identify images that are mislabeled, across multiple types of annotation error.","sentences":["The labor-intensive annotation process of semantic segmentation datasets is often prone to errors, since humans struggle to label every pixel correctly.","We study algorithms to automatically detect such annotation errors, in particular methods to score label quality, such that the images with the lowest scores are least likely to be correctly labeled.","This helps prioritize what data to review in order to ensure a high-quality training/evaluation dataset, which is critical in sensitive applications such as medical imaging and autonomous vehicles.","Widely applicable, our label quality scores rely on probabilistic predictions from a trained segmentation model -- any model architecture and training procedure can be utilized.","Here we study 7 different label quality scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation model to detect annotation errors in a version of the SYNTHIA dataset.","Precision-recall evaluations reveal a score -- the soft-minimum of the model-estimated likelihoods of each pixel's annotated class -- that is particularly effective to identify images that are mislabeled, across multiple types of annotation error."],"url":"http://arxiv.org/abs/2307.05080v1"}
{"created":"2023-07-11 07:24:26","title":"Selling Data to a Competitor (Extended Abstract)","abstract":"We study the costs and benefits of selling data to a competitor. Although selling all consumers' data may decrease total firm profits, there exist other selling mechanisms -- in which only some consumers' data is sold -- that render both firms better off. We identify the profit-maximizing mechanism, and show that the benefit to firms comes at a cost to consumers. We then construct Pareto-improving mechanisms, in which each consumers' welfare, as well as both firms' profits, increase. Finally, we show that consumer opt-in can serve as an instrument to induce firms to choose a Pareto-improving mechanism over a profit-maximizing one.","sentences":["We study the costs and benefits of selling data to a competitor.","Although selling all consumers' data may decrease total firm profits, there exist other selling mechanisms -- in which only some consumers' data is sold -- that render both firms better off.","We identify the profit-maximizing mechanism, and show that the benefit to firms comes at a cost to consumers.","We then construct Pareto-improving mechanisms, in which each consumers' welfare, as well as both firms' profits, increase.","Finally, we show that consumer opt-in can serve as an instrument to induce firms to choose a Pareto-improving mechanism over a profit-maximizing one."],"url":"http://arxiv.org/abs/2307.05078v1"}
{"created":"2023-07-11 07:19:24","title":"Incentive Engineering for Concurrent Games","abstract":"We consider the problem of incentivising desirable behaviours in multi-agent systems by way of taxation schemes. Our study employs the concurrent games model: in this model, each agent is primarily motivated to seek the satisfaction of a goal, expressed as a Linear Temporal Logic (LTL) formula; secondarily, agents seek to minimise costs, where costs are imposed based on the actions taken by agents in different states of the game. In this setting, we consider an external principal who can influence agents' preferences by imposing taxes (additional costs) on the actions chosen by agents in different states. The principal imposes taxation schemes to motivate agents to choose a course of action that will lead to the satisfaction of their goal, also expressed as an LTL formula. However, taxation schemes are limited in their ability to influence agents' preferences: an agent will always prefer to satisfy its goal rather than otherwise, no matter what the costs. The fundamental question that we study is whether the principal can impose a taxation scheme such that, in the resulting game, the principal's goal is satisfied in at least one or all runs of the game that could arise by agents choosing to follow game-theoretic equilibrium strategies. We consider two different types of taxation schemes: in a static scheme, the same tax is imposed on a state-action profile pair in all circumstances, while in a dynamic scheme, the principal can choose to vary taxes depending on the circumstances. We investigate the main game-theoretic properties of this model as well as the computational complexity of the relevant decision problems.","sentences":["We consider the problem of incentivising desirable behaviours in multi-agent systems by way of taxation schemes.","Our study employs the concurrent games model: in this model, each agent is primarily motivated to seek the satisfaction of a goal, expressed as a Linear Temporal Logic (LTL) formula; secondarily, agents seek to minimise costs, where costs are imposed based on the actions taken by agents in different states of the game.","In this setting, we consider an external principal who can influence agents' preferences by imposing taxes (additional costs) on the actions chosen by agents in different states.","The principal imposes taxation schemes to motivate agents to choose a course of action that will lead to the satisfaction of their goal, also expressed as an LTL formula.","However, taxation schemes are limited in their ability to influence agents' preferences: an agent will always prefer to satisfy its goal rather than otherwise, no matter what the costs.","The fundamental question that we study is whether the principal can impose a taxation scheme such that, in the resulting game, the principal's goal is satisfied in at least one or all runs of the game that could arise by agents choosing to follow game-theoretic equilibrium strategies.","We consider two different types of taxation schemes: in a static scheme, the same tax is imposed on a state-action profile pair in all circumstances, while in a dynamic scheme, the principal can choose to vary taxes depending on the circumstances.","We investigate the main game-theoretic properties of this model as well as the computational complexity of the relevant decision problems."],"url":"http://arxiv.org/abs/2307.05076v1"}
{"created":"2023-07-11 07:18:15","title":"Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images","abstract":"Removing multiple degradations, such as haze, rain, and blur, from real-world images poses a challenging and illposed problem. Recently, unified models that can handle different degradations have been proposed and yield promising results. However, these approaches focus on synthetic images and experience a significant performance drop when applied to realworld images. In this paper, we introduce Uni-Removal, a twostage semi-supervised framework for addressing the removal of multiple degradations in real-world images using a unified model and parameters. In the knowledge transfer stage, Uni-Removal leverages a supervised multi-teacher and student architecture in the knowledge transfer stage to facilitate learning from pretrained teacher networks specialized in different degradation types. A multi-grained contrastive loss is introduced to enhance learning from feature and image spaces. In the domain adaptation stage, unsupervised fine-tuning is performed by incorporating an adversarial discriminator on real-world images. The integration of an extended multi-grained contrastive loss and generative adversarial loss enables the adaptation of the student network from synthetic to real-world domains. Extensive experiments on real-world degraded datasets demonstrate the effectiveness of our proposed method. We compare our Uni-Removal framework with state-of-the-art supervised and unsupervised methods, showcasing its promising results in real-world image dehazing, deraining, and deblurring simultaneously.","sentences":["Removing multiple degradations, such as haze, rain, and blur, from real-world images poses a challenging and illposed problem.","Recently, unified models that can handle different degradations have been proposed and yield promising results.","However, these approaches focus on synthetic images and experience a significant performance drop when applied to realworld images.","In this paper, we introduce Uni-Removal, a twostage semi-supervised framework for addressing the removal of multiple degradations in real-world images using a unified model and parameters.","In the knowledge transfer stage, Uni-Removal leverages a supervised multi-teacher and student architecture in the knowledge transfer stage to facilitate learning from pretrained teacher networks specialized in different degradation types.","A multi-grained contrastive loss is introduced to enhance learning from feature and image spaces.","In the domain adaptation stage, unsupervised fine-tuning is performed by incorporating an adversarial discriminator on real-world images.","The integration of an extended multi-grained contrastive loss and generative adversarial loss enables the adaptation of the student network from synthetic to real-world domains.","Extensive experiments on real-world degraded datasets demonstrate the effectiveness of our proposed method.","We compare our Uni-Removal framework with state-of-the-art supervised and unsupervised methods, showcasing its promising results in real-world image dehazing, deraining, and deblurring simultaneously."],"url":"http://arxiv.org/abs/2307.05075v1"}
{"created":"2023-07-11 07:16:22","title":"Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain","abstract":"Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large. In this paper, we propose a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain. Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question. To retrieve questions sharing similar intents with input questions, we propose two strategies for assisting retrieval. Firstly, we leverage LLMs to simplify the original questions, unifying the syntax and thereby clarifying the users' intentions. To generate executable and accurate SQLs without human intervention, we design a dynamic revision chain which iteratively adapts fine-grained feedback from the previously generated SQL. Experimental results on three Text-to-SQL benchmarks demonstrate the superiority of our method over strong baseline models.","sentences":["Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases.","Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL.","However, it faces challenges with strict SQL syntax requirements.","Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large.","In this paper, we propose a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain.","Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question.","To retrieve questions sharing similar intents with input questions, we propose two strategies for assisting retrieval.","Firstly, we leverage LLMs to simplify the original questions, unifying the syntax and thereby clarifying the users' intentions.","To generate executable and accurate SQLs without human intervention, we design a dynamic revision chain which iteratively adapts fine-grained feedback from the previously generated SQL.","Experimental results on three Text-to-SQL benchmarks demonstrate the superiority of our method over strong baseline models."],"url":"http://arxiv.org/abs/2307.05074v1"}
{"created":"2023-07-11 07:15:28","title":"Knowledge-wh and False Belief Sensitivity: A Logical Study (An Extended Abstract)","abstract":"In epistemic logic, a way to deal with knowledge-wh is to interpret them as a kind of mention-some knowledge (MS-knowledge). But philosophers and linguists have challenged both the sufficiency and necessity of such an account: some argue that knowledge-wh has, in addition to MS-knowledge, also a sensitivity to false belief (FS); others argue that knowledge-wh might only imply mention-some true belief (MS-true belief). In this paper, we offer a logical study for all these different accounts. We apply the technique of bundled operators, and introduce four different bundled operators: $[\\mathsf{tB}^\\mathtt{MS}]^x \\phi := \\exists x (\\mathsf{[B]} \\phi \\wedge \\phi)$, $[\\mathsf{tB}^\\mathtt{MS}_\\mathtt{FS}]^x \\phi := \\exists x (\\mathsf{[B]} \\phi \\wedge \\phi) \\wedge \\forall x (\\mathsf{[B]} \\phi \\to \\phi)$, $[\\mathsf{K}^\\mathtt{MS}]^x \\phi := \\exists x \\mathsf{[K]} \\phi$ and $[\\mathsf{K}^\\mathtt{MS}_\\mathtt{FS}]^x \\phi := \\exists x \\mathsf{[K]} \\phi \\wedge \\forall x (\\mathsf{[B]} \\phi \\to \\phi)$, which characterize the notions of MS-true belief, MS-true belief with FS, MS-knowledge and MS-knowledge with FS respectively. We axiomatize the four logics which take the above operators (as well as $\\mathsf{[K]}$) as primitive modalities on the class of $S4.2$-constant-domain models, and compare the patterns of reasoning in the obtained logics, in order to show how the four accounts of knowledge-wh differ from each other, as well as what they have in common.","sentences":["In epistemic logic, a way to deal with knowledge-wh is to interpret them as a kind of mention-some knowledge (MS-knowledge).","But philosophers and linguists have challenged both the sufficiency and necessity of such an account: some argue that knowledge-wh has, in addition to MS-knowledge, also a sensitivity to false belief (FS); others argue that knowledge-wh might only imply mention-some true belief (MS-true belief).","In this paper, we offer a logical study for all these different accounts.","We apply the technique of bundled operators, and introduce four different bundled operators: $[\\mathsf{tB}^\\mathtt{MS}]^x \\phi := \\exists x (\\mathsf{[B]} \\phi \\wedge \\phi)$, $[\\mathsf{tB}^\\mathtt{MS}_\\mathtt{FS}]^x \\phi := \\exists x (\\mathsf{[B]} \\phi \\wedge \\phi)","\\wedge \\forall x (\\mathsf{[B]} \\phi \\to \\phi)$, $[\\mathsf{K}^\\mathtt{MS}]^x \\phi := \\exists x \\mathsf{[K]} \\phi$ and $[\\mathsf{K}^\\mathtt{MS}_\\mathtt{FS}]^x \\phi := \\exists x \\mathsf{[K]} \\phi \\wedge \\forall x (\\mathsf{[B]} \\phi \\to \\phi)$, which characterize the notions of MS-true belief, MS-true belief with FS, MS-knowledge and MS-knowledge with FS respectively.","We axiomatize the four logics which take the above operators (as well as $\\mathsf{[K]}$) as primitive modalities on the class of $S4.2$-constant-domain models, and compare the patterns of reasoning in the obtained logics, in order to show how the four accounts of knowledge-wh differ from each other, as well as what they have in common."],"url":"http://arxiv.org/abs/2307.05073v1"}
{"created":"2023-07-11 07:15:11","title":"Aggregating Credences into Beliefs: Agenda Conditions for Impossibility Results","abstract":"Binarizing belief aggregation addresses how to rationally aggregate individual probabilistic beliefs into collective binary beliefs. Similar to the development of judgment aggregation theory, formulating axiomatic requirements, proving impossibility theorems, and identifying exact agenda conditions of impossibility theorems are natural and important research topics in binarizing belief aggregation. Building on our previous research on impossibility theorems, we use an agenda-theoretic approach to generalize the results and to determine the necessary and sufficient level of logical interconnection between the issues in an agenda for the impossibility theorems to arise. We demonstrate that (1) path-connectedness and even-negatability constitute the exact agenda condition for the oligarchy result stating that binarizing belief aggregation satisfying proposition-wise independence and deductive closure of collective beliefs yields the oligarchies under minor conditions; (2) negation-connectedness is the condition for the triviality result obtained by adding anonymity to the oligarchy result; and (3) blockedness is the condition for the impossibility result, which follows by adding completeness and consistency of collective beliefs. Moreover, we compare these novel findings with existing agenda-theoretic characterization theorems in judgment aggregation and belief binarization.","sentences":["Binarizing belief aggregation addresses how to rationally aggregate individual probabilistic beliefs into collective binary beliefs.","Similar to the development of judgment aggregation theory, formulating axiomatic requirements, proving impossibility theorems, and identifying exact agenda conditions of impossibility theorems are natural and important research topics in binarizing belief aggregation.","Building on our previous research on impossibility theorems, we use an agenda-theoretic approach to generalize the results and to determine the necessary and sufficient level of logical interconnection between the issues in an agenda for the impossibility theorems to arise.","We demonstrate that (1) path-connectedness and even-negatability constitute the exact agenda condition for the oligarchy result stating that binarizing belief aggregation satisfying proposition-wise independence and deductive closure of collective beliefs yields the oligarchies under minor conditions; (2) negation-connectedness is the condition for the triviality result obtained by adding anonymity to the oligarchy result; and (3) blockedness is the condition for the impossibility result, which follows by adding completeness and consistency of collective beliefs.","Moreover, we compare these novel findings with existing agenda-theoretic characterization theorems in judgment aggregation and belief binarization."],"url":"http://arxiv.org/abs/2307.05072v1"}
{"created":"2023-07-11 07:14:53","title":"Mining for Unknown Unknowns","abstract":"Unknown unknowns are future relevant contingencies that lack an ex ante description. While there are numerous retrospective accounts showing that significant gains or losses might have been achieved or avoided had such contingencies been previously uncovered, getting hold of unknown unknowns still remains elusive, both in practice and conceptually. Using Formal Concept Analysis (FCA) - a subfield of lattice theory which is increasingly applied for mining and organizing data - this paper introduces a simple framework to systematically think out of the box and direct the search for unknown unknowns.","sentences":["Unknown unknowns are future relevant contingencies that lack an ex ante description.","While there are numerous retrospective accounts showing that significant gains or losses might have been achieved or avoided had such contingencies been previously uncovered, getting hold of unknown unknowns still remains elusive, both in practice and conceptually.","Using Formal Concept Analysis (FCA) - a subfield of lattice theory which is increasingly applied for mining and organizing data - this paper introduces a simple framework to systematically think out of the box and direct the search for unknown unknowns."],"url":"http://arxiv.org/abs/2307.05071v1"}
{"created":"2023-07-11 07:14:11","title":"A Logic-Based Analysis of Responsibility","abstract":"This paper presents a logic-based framework to analyze responsibility, which I refer to as intentional epistemic act-utilitarian stit theory (IEAUST). To be precise, IEAUST is used to model and syntactically characterize various modes of responsibility, where by 'modes of responsibility' I mean instances of Broersen's three categories of responsibility (causal, informational, and motivational responsibility), cast against the background of particular deontic contexts. IEAUST is obtained by integrating a modal language to express the following components of responsibility on stit models: agency, epistemic notions, intentionality, and different senses of obligation. With such a language, I characterize the components of responsibility using particular formulas. Then, adopting a compositional approach -- where complex modalities are built out of more basic ones -- these characterizations of the components are used to formalize the aforementioned modes of responsibility.","sentences":["This paper presents a logic-based framework to analyze responsibility, which I refer to as intentional epistemic act-utilitarian stit theory (IEAUST).","To be precise, IEAUST is used to model and syntactically characterize various modes of responsibility, where by 'modes of responsibility' I mean instances of Broersen's three categories of responsibility (causal, informational, and motivational responsibility), cast against the background of particular deontic contexts.","IEAUST is obtained by integrating a modal language to express the following components of responsibility on stit models: agency, epistemic notions, intentionality, and different senses of obligation.","With such a language, I characterize the components of responsibility using particular formulas.","Then, adopting a compositional approach -- where complex modalities are built out of more basic ones -- these characterizations of the components are used to formalize the aforementioned modes of responsibility."],"url":"http://arxiv.org/abs/2307.05070v1"}
{"created":"2023-07-11 07:13:52","title":"Cognitive Bias and Belief Revision","abstract":"In this paper we formalise three types of cognitive bias within the framework of belief revision: confirmation bias, framing bias, and anchoring bias. We interpret them generally, as restrictions on the process of iterated revision, and we apply them to three well-known belief revision methods: conditioning, lexicographic revision, and minimal revision. We investigate the reliability of biased belief revision methods in truth tracking. We also run computer simulations to assess the performance of biased belief revision in random scenarios.","sentences":["In this paper we formalise three types of cognitive bias within the framework of belief revision: confirmation bias, framing bias, and anchoring bias.","We interpret them generally, as restrictions on the process of iterated revision, and we apply them to three well-known belief revision methods: conditioning, lexicographic revision, and minimal revision.","We investigate the reliability of biased belief revision methods in truth tracking.","We also run computer simulations to assess the performance of biased belief revision in random scenarios."],"url":"http://arxiv.org/abs/2307.05069v1"}
{"created":"2023-07-11 07:13:29","title":"A Theory of Bounded Inductive Rationality","abstract":"The dominant theories of rational choice assume logical omniscience. That is, they assume that when facing a decision problem, an agent can perform all relevant computations and determine the truth value of all relevant logical/mathematical claims. This assumption is unrealistic when, for example, we offer bets on remote digits of pi or when an agent faces a computationally intractable planning problem. Furthermore, the assumption of logical omniscience creates contradictions in cases where the environment can contain descriptions of the agent itself. Importantly, strategic interactions as studied in game theory are decision problems in which a rational agent is predicted by its environment (the other players). In this paper, we develop a theory of rational decision making that does not assume logical omniscience. We consider agents who repeatedly face decision problems (including ones like betting on digits of pi or games against other agents). The main contribution of this paper is to provide a sensible theory of rationality for such agents. Roughly, we require that a boundedly rational inductive agent tests each efficiently computable hypothesis infinitely often and follows those hypotheses that keep their promises of high rewards. We then prove that agents that are rational in this sense have other desirable properties. For example, they learn to value random and pseudo-random lotteries at their expected reward. Finally, we consider strategic interactions between different agents and prove a folk theorem for what strategies bounded rational inductive agents can converge to.","sentences":["The dominant theories of rational choice assume logical omniscience.","That is, they assume that when facing a decision problem, an agent can perform all relevant computations and determine the truth value of all relevant logical/mathematical claims.","This assumption is unrealistic when, for example, we offer bets on remote digits of pi or when an agent faces a computationally intractable planning problem.","Furthermore, the assumption of logical omniscience creates contradictions in cases where the environment can contain descriptions of the agent itself.","Importantly, strategic interactions as studied in game theory are decision problems in which a rational agent is predicted by its environment (the other players).","In this paper, we develop a theory of rational decision making that does not assume logical omniscience.","We consider agents who repeatedly face decision problems (including ones like betting on digits of pi or games against other agents).","The main contribution of this paper is to provide a sensible theory of rationality for such agents.","Roughly, we require that a boundedly rational inductive agent tests each efficiently computable hypothesis infinitely often and follows those hypotheses that keep their promises of high rewards.","We then prove that agents that are rational in this sense have other desirable properties.","For example, they learn to value random and pseudo-random lotteries at their expected reward.","Finally, we consider strategic interactions between different agents and prove a folk theorem for what strategies bounded rational inductive agents can converge to."],"url":"http://arxiv.org/abs/2307.05068v1"}
{"created":"2023-07-11 07:13:09","title":"Exploiting Asymmetry in Logic Puzzles: Using ZDDs for Symbolic Model Checking Dynamic Epistemic Logic","abstract":"Binary decision diagrams (BDDs) are widely used to mitigate the state-explosion problem in model checking. A variation of BDDs are Zero-suppressed Decision Diagrams (ZDDs) which omit variables that must be false, instead of omitting variables that do not matter. We use ZDDs to symbolically encode Kripke models used in Dynamic Epistemic Logic, a framework to reason about knowledge and information dynamics in multi-agent systems. We compare the memory usage of different ZDD variants for three well-known examples from the literature: the Muddy Children, the Sum and Product puzzle and the Dining Cryptographers. Our implementation is based on the existing model checker SMCDEL and the CUDD library. Our results show that replacing BDDs with the right variant of ZDDs can significantly reduce memory usage. This suggests that ZDDs are a useful tool for model checking multi-agent systems.","sentences":["Binary decision diagrams (BDDs) are widely used to mitigate the state-explosion problem in model checking.","A variation of BDDs are Zero-suppressed Decision Diagrams (ZDDs) which omit variables that must be false, instead of omitting variables that do not matter.","We use ZDDs to symbolically encode Kripke models used in Dynamic Epistemic Logic, a framework to reason about knowledge and information dynamics in multi-agent systems.","We compare the memory usage of different ZDD variants for three well-known examples from the literature: the Muddy Children, the Sum and Product puzzle and the Dining Cryptographers.","Our implementation is based on the existing model checker SMCDEL and the CUDD library.","Our results show that replacing BDDs with the right variant of ZDDs can significantly reduce memory usage.","This suggests that ZDDs are a useful tool for model checking multi-agent systems."],"url":"http://arxiv.org/abs/2307.05067v1"}
{"created":"2023-07-11 07:12:51","title":"Tableaux for the Logic of Strategically Knowing How","abstract":"The logic of goal-directed knowing-how extends the standard epistemic logic with an operator of knowing-how. The knowing-how operator is interpreted as that there exists a strategy such that the agent knows that the strategy can make sure that p. This paper presents a tableau procedure for the multi-agent version of the logic of strategically knowing-how and shows the soundness and completeness of this tableau procedure. This paper also shows that the satisfiability problem of the logic can be decided in PSPACE.","sentences":["The logic of goal-directed knowing-how extends the standard epistemic logic with an operator of knowing-how.","The knowing-how operator is interpreted as that there exists a strategy such that the agent knows that the strategy can make sure that p.","This paper presents a tableau procedure for the multi-agent version of the logic of strategically knowing-how and shows the soundness and completeness of this tableau procedure.","This paper also shows that the satisfiability problem of the logic can be decided in PSPACE."],"url":"http://arxiv.org/abs/2307.05066v1"}
{"created":"2023-07-11 07:12:30","title":"Metatickles and Death in Damascus","abstract":"The prescriptions of our two most prominent strands of decision theory, evidential and causal, differ in a general class of problems known as Newcomb problems. In these, evidential decision theory prescribes choosing a dominated act. Attempts have been made at reconciling the two theories by relying on additional requirements such as ratification (Jeffrey 1983) or \"tickles\" (Eells 1982). It has been argued that such attempts have failed (Lewis 1981a; Skyrms 1982). More recently, Huttegger (forthcoming) has developed a version of deliberative decision theory that reconciles the prescriptions of the evidentialist and causalist. In this paper, I extend this framework to problems characterised by decision instability, and show that it cannot deliver a resolute answer under a plausible specification of the tickle. I prove that there exists a robust method of determining whether the specification of the tickle matters for all two-state, two-act problems whose payoff tables exhibit some basic mathematical relationships. One upshot is that we have a principled way of knowing ex-ante whether a reconciliation of evidential and causal decision theory is plausible for a wide range of decision problems under this framework. Another upshot is that the tickle approach needs further work to achieve full reconciliation.","sentences":["The prescriptions of our two most prominent strands of decision theory, evidential and causal, differ in a general class of problems known as Newcomb problems.","In these, evidential decision theory prescribes choosing a dominated act.","Attempts have been made at reconciling the two theories by relying on additional requirements such as ratification (Jeffrey 1983) or \"tickles\" (Eells 1982).","It has been argued that such attempts have failed (Lewis 1981a; Skyrms 1982).","More recently, Huttegger (forthcoming) has developed a version of deliberative decision theory that reconciles the prescriptions of the evidentialist and causalist.","In this paper, I extend this framework to problems characterised by decision instability, and show that it cannot deliver a resolute answer under a plausible specification of the tickle.","I prove that there exists a robust method of determining whether the specification of the tickle matters for all two-state, two-act problems whose payoff tables exhibit some basic mathematical relationships.","One upshot is that we have a principled way of knowing ex-ante whether a reconciliation of evidential and causal decision theory is plausible for a wide range of decision problems under this framework.","Another upshot is that the tickle approach needs further work to achieve full reconciliation."],"url":"http://arxiv.org/abs/2307.05065v1"}
{"created":"2023-07-11 07:11:51","title":"An Acceptance Semantics for Stable Modal Knowledge","abstract":"We observe some puzzling linguistic data concerning ordinary knowledge ascriptions that embed an epistemic (im)possibility claim. We conclude that it is untenable to jointly endorse both classical logic and a pair of intuitively attractive theses: the thesis that knowledge ascriptions are always veridical and a `negative transparency' thesis that reduces knowledge of a simple negated `might' claim to an epistemic claim without modal content. We motivate a strategy for answering the trade-off: preserve veridicality and (generalized) negative transparency, while abandoning the general validity of contraposition. We survey and criticize various approaches for incorporating veridicality into domain semantics, a paradigmatic `information-sensitive' framework for capturing negative transparency and, more generally, the non-classical behavior of sentences with epistemic modals. We then present a novel information-sensitive semantics that successfully executes our favored strategy: stable acceptance semantics.","sentences":["We observe some puzzling linguistic data concerning ordinary knowledge ascriptions that embed an epistemic (im)possibility claim.","We conclude that it is untenable to jointly endorse both classical logic and a pair of intuitively attractive theses: the thesis that knowledge ascriptions are always veridical and a `negative transparency' thesis that reduces knowledge of a simple negated `might' claim to an epistemic claim without modal content.","We motivate a strategy for answering the trade-off: preserve veridicality and (generalized) negative transparency, while abandoning the general validity of contraposition.","We survey and criticize various approaches for incorporating veridicality into domain semantics, a paradigmatic `information-sensitive' framework for capturing negative transparency and, more generally, the non-classical behavior of sentences with epistemic modals.","We then present a novel information-sensitive semantics that successfully executes our favored strategy: stable acceptance semantics."],"url":"http://arxiv.org/abs/2307.05064v1"}
{"created":"2023-07-11 07:10:55","title":"A \"Game of Like\" : Online Social Network Sharing As Strategic Interaction","abstract":"We argue that behavioral science models of online content-sharing overlook the role of strategic interactions between users. Borrowing from accuracy-nudges studies decision-theoretic models, we propose a basic game model and explore special cases with idealized parameter settings to identify refinements necessary to capture real-world online social network behavior. Anticipating those refinements, we sketch a strategic analysis of content amplification and draw a connection between Keynes's beauty contest analogy and recent social-epistemological work on echo chambers. We conclude on the model's prospects from analytical and empirical perspectives.","sentences":["We argue that behavioral science models of online content-sharing overlook the role of strategic interactions between users.","Borrowing from accuracy-nudges studies decision-theoretic models, we propose a basic game model and explore special cases with idealized parameter settings to identify refinements necessary to capture real-world online social network behavior.","Anticipating those refinements, we sketch a strategic analysis of content amplification and draw a connection between Keynes's beauty contest analogy and recent social-epistemological work on echo chambers.","We conclude on the model's prospects from analytical and empirical perspectives."],"url":"http://arxiv.org/abs/2307.05063v1"}
{"created":"2023-07-11 07:10:39","title":"System of Spheres-based Two Level Credibility-limited Revisions","abstract":"Two level credibility-limited revision is a non-prioritized revision operation. When revising by a two level credibility-limited revision, two levels of credibility and one level of incredibility are considered. When revising by a sentence at the highest level of credibility, the operator behaves as a standard revision, if the sentence is at the second level of credibility, then the outcome of the revision process coincides with a standard contraction by the negation of that sentence. If the sentence is not credible, then the original belief set remains unchanged. In this paper, we propose a construction for two level credibility-limited revision operators based on Grove's systems of spheres and present an axiomatic characterization for these operators.","sentences":["Two level credibility-limited revision is a non-prioritized revision operation.","When revising by a two level credibility-limited revision, two levels of credibility and one level of incredibility are considered.","When revising by a sentence at the highest level of credibility, the operator behaves as a standard revision, if the sentence is at the second level of credibility, then the outcome of the revision process coincides with a standard contraction by the negation of that sentence.","If the sentence is not credible, then the original belief set remains unchanged.","In this paper, we propose a construction for two level credibility-limited revision operators based on Grove's systems of spheres and present an axiomatic characterization for these operators."],"url":"http://arxiv.org/abs/2307.05062v1"}
{"created":"2023-07-11 07:10:19","title":"Maximizing Social Welfare in Score-Based Social Distance Games","abstract":"Social distance games have been extensively studied as a coalition formation model where the utilities of agents in each coalition were captured using a utility function u that took into account distances in a given social network. In this paper, we consider a non-normalized score-based definition of social distance games where the utility function u_v depends on a generic scoring vector v, which may be customized to match the specifics of each individual application scenario.   As our main technical contribution, we establish the tractability of computing a welfare-maximizing partitioning of the agents into coalitions on tree-like networks, for every score-based function u_v. We provide more efficient algorithms when dealing with specific choices of u_v or simpler networks, and also extend all of these results to computing coalitions that are Nash stable or individually rational. We view these results as a further strong indication of the usefulness of the proposed score-based utility function: even on very simple networks, the problem of computing a welfare-maximizing partitioning into coalitions remains open for the originally considered canonical function u.","sentences":["Social distance games have been extensively studied as a coalition formation model where the utilities of agents in each coalition were captured using a utility function u that took into account distances in a given social network.","In this paper, we consider a non-normalized score-based definition of social distance games where the utility function u_v depends on a generic scoring vector v, which may be customized to match the specifics of each individual application scenario.   ","As our main technical contribution, we establish the tractability of computing a welfare-maximizing partitioning of the agents into coalitions on tree-like networks, for every score-based function u_v.","We provide more efficient algorithms when dealing with specific choices of u_v or simpler networks, and also extend all of these results to computing coalitions that are Nash stable or individually rational.","We view these results as a further strong indication of the usefulness of the proposed score-based utility function: even on very simple networks, the problem of computing a welfare-maximizing partitioning into coalitions remains open for the originally considered canonical function u."],"url":"http://arxiv.org/abs/2307.05061v1"}
{"created":"2023-07-11 07:10:01","title":"Satisfiability of Arbitrary Public Announcement Logic with Common Knowledge is $\u03a3^1_1$-hard","abstract":"Arbitrary Public Announcement Logic with Common Knowledge (APALC) is an extension of Public Announcement Logic with common knowledge modality and quantifiers over announcements. We show that the satisfiability problem of APALC on S5-models, as well as that of two other related logics with quantification and common knowledge, is $\\Sigma^1_1$-hard. This implies that neither the validities nor the satisfiable formulas of APALC are recursively enumerable. Which, in turn, implies that APALC is not finitely axiomatisable.","sentences":["Arbitrary Public Announcement Logic with Common Knowledge (APALC) is an extension of Public Announcement Logic with common knowledge modality and quantifiers over announcements.","We show that the satisfiability problem of APALC on S5-models, as well as that of two other related logics with quantification and common knowledge, is $\\Sigma^1_1$-hard.","This implies that neither the validities nor the satisfiable formulas of APALC are recursively enumerable.","Which, in turn, implies that APALC is not finitely axiomatisable."],"url":"http://arxiv.org/abs/2307.05060v1"}
{"created":"2023-07-11 07:08:34","title":"On Imperfect Recall in Multi-Agent Influence Diagrams","abstract":"Multi-agent influence diagrams (MAIDs) are a popular game-theoretic model based on Bayesian networks. In some settings, MAIDs offer significant advantages over extensive-form game representations. Previous work on MAIDs has assumed that agents employ behavioural policies, which set independent conditional probability distributions over actions for each of their decisions. In settings with imperfect recall, however, a Nash equilibrium in behavioural policies may not exist. We overcome this by showing how to solve MAIDs with forgetful and absent-minded agents using mixed policies and two types of correlated equilibrium. We also analyse the computational complexity of key decision problems in MAIDs, and explore tractable cases. Finally, we describe applications of MAIDs to Markov games and team situations, where imperfect recall is often unavoidable.","sentences":["Multi-agent influence diagrams (MAIDs) are a popular game-theoretic model based on Bayesian networks.","In some settings, MAIDs offer significant advantages over extensive-form game representations.","Previous work on MAIDs has assumed that agents employ behavioural policies, which set independent conditional probability distributions over actions for each of their decisions.","In settings with imperfect recall, however, a Nash equilibrium in behavioural policies may not exist.","We overcome this by showing how to solve MAIDs with forgetful and absent-minded agents using mixed policies and two types of correlated equilibrium.","We also analyse the computational complexity of key decision problems in MAIDs, and explore tractable cases.","Finally, we describe applications of MAIDs to Markov games and team situations, where imperfect recall is often unavoidable."],"url":"http://arxiv.org/abs/2307.05059v1"}
{"created":"2023-07-11 07:07:33","title":"Comparing the Update Expressivity of Communication Patterns and Action Models","abstract":"Any kind of dynamics in dynamic epistemic logic can be represented as an action model. Right? Wrong! In this contribution we prove that the update expressivity of communication patterns is incomparable to that of action models. Action models, as update mechanisms, were proposed by Baltag, Moss, and Solecki in 1998 and have remained the nearly universally accepted update mechanism in dynamic epistemic logics since then. Alternatives, such as arrow updates that were proposed by Kooi and Renne in 2011, have update equivalent action models. More recently, the picture is shifting. Communication patterns are update mechanisms originally proposed in some form or other by Agotnes and Wang in 2017 (as resolving distributed knowledge), by Baltag and Smets in 2020 (as reading events), and by Velazquez, Castaneda, and Rosenblueth in 2021 (as communication patterns). All these logics have the same expressivity as the base logic of distributed knowledge. However, their update expressivity, the relation between pointed epistemic models induced by such an update, was conjectured to be different from that of action model logic. Indeed, we show that action model logic and communication pattern logic are incomparable in update expressivity. We also show that, given a history-based semantics and when restricted to (static) interpreted systems, action model logic is (strictly) more update expressive than communication pattern logic. Our results are relevant for distributed computing wherein oblivious models involve arbitrary iteration of communication patterns.","sentences":["Any kind of dynamics in dynamic epistemic logic can be represented as an action model.","Right?","Wrong!","In this contribution we prove that the update expressivity of communication patterns is incomparable to that of action models.","Action models, as update mechanisms, were proposed by Baltag, Moss, and Solecki in 1998 and have remained the nearly universally accepted update mechanism in dynamic epistemic logics since then.","Alternatives, such as arrow updates that were proposed by Kooi and Renne in 2011, have update equivalent action models.","More recently, the picture is shifting.","Communication patterns are update mechanisms originally proposed in some form or other by Agotnes and Wang in 2017 (as resolving distributed knowledge), by Baltag and Smets in 2020 (as reading events), and by Velazquez, Castaneda, and Rosenblueth in 2021 (as communication patterns).","All these logics have the same expressivity as the base logic of distributed knowledge.","However, their update expressivity, the relation between pointed epistemic models induced by such an update, was conjectured to be different from that of action model logic.","Indeed, we show that action model logic and communication pattern logic are incomparable in update expressivity.","We also show that, given a history-based semantics and when restricted to (static) interpreted systems, action model logic is (strictly) more update expressive than communication pattern logic.","Our results are relevant for distributed computing wherein oblivious models involve arbitrary iteration of communication patterns."],"url":"http://arxiv.org/abs/2307.05057v1"}
{"created":"2023-07-11 07:06:54","title":"Epistemic Logics of Structured Intensional Groups","abstract":"Epistemic logics of intensional groups lift the assumption that membership in a group of agents is common knowledge. Instead of being represented directly as a set of agents, intensional groups are represented by a property that may change its extension from world to world. Several authors have considered versions of the intensional group framework where group-specifying properties are articulated using structured terms of a language, such as the language of Boolean algebras or of description logic. In this paper we formulate a general semantic framework for epistemic logics of structured intensional groups, develop the basic theory leading to completeness-via-canonicity results, and show that several frameworks presented in the literature correspond to special cases of the general framework.","sentences":["Epistemic logics of intensional groups lift the assumption that membership in a group of agents is common knowledge.","Instead of being represented directly as a set of agents, intensional groups are represented by a property that may change its extension from world to world.","Several authors have considered versions of the intensional group framework where group-specifying properties are articulated using structured terms of a language, such as the language of Boolean algebras or of description logic.","In this paper we formulate a general semantic framework for epistemic logics of structured intensional groups, develop the basic theory leading to completeness-via-canonicity results, and show that several frameworks presented in the literature correspond to special cases of the general framework."],"url":"http://arxiv.org/abs/2307.05056v1"}
{"created":"2023-07-11 07:06:31","title":"Comparing Social Network Dynamic Operators","abstract":"Numerous logics have been developed to reason either about threshold-induced opinion diffusion in a network, or about similarity-driven network structure evolution, or about both. In this paper, we first introduce a logic containing different dynamic operators to capture changes that are 'asynchronous' (opinion change only, network-link change only) and changes that are 'synchronous' (both at the same time). Second, we show that synchronous operators cannot, in general, be replaced by asynchronous operators and vice versa. Third, we characterise the class of models on which the synchronous operator can be reduced to sequences of asynchronous operators.","sentences":["Numerous logics have been developed to reason either about threshold-induced opinion diffusion in a network, or about similarity-driven network structure evolution, or about both.","In this paper, we first introduce a logic containing different dynamic operators to capture changes that are 'asynchronous' (opinion change only, network-link change only) and changes that are 'synchronous' (both at the same time).","Second, we show that synchronous operators cannot, in general, be replaced by asynchronous operators and vice versa.","Third, we characterise the class of models on which the synchronous operator can be reduced to sequences of asynchronous operators."],"url":"http://arxiv.org/abs/2307.05055v1"}
{"created":"2023-07-11 07:03:29","title":"Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps","abstract":"We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boosting ICL performance is task-dependent, with limited benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks. These insights are critical for understanding the functionality of LLMs and guiding the development of effective demonstrations, which is increasingly relevant in light of the growing use of LLMs in applications such as ChatGPT. Our research code is publicly available at https://github.com/paihengxu/XICL.","sentences":["We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs).","Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed.","We build on previous work, which offers mixed findings on how these elements influence ICL.","To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis.","Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs.","Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels.","Finally, we find that the effectiveness of complementary explanations in boosting ICL performance is task-dependent, with limited benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.","These insights are critical for understanding the functionality of LLMs and guiding the development of effective demonstrations, which is increasingly relevant in light of the growing use of LLMs in applications such as ChatGPT.","Our research code is publicly available at https://github.com/paihengxu/XICL."],"url":"http://arxiv.org/abs/2307.05052v1"}
{"created":"2023-07-11 06:57:42","title":"An Abstract Look at Awareness Models and Their Dynamics","abstract":"This work builds upon a well-established research tradition on modal logics of awareness. One of its aims is to export tools and techniques to other areas within modal logic. To this end, we illustrate a number of significant bridges with abstract argumentation, justification logics, the epistemic logic of knowing-what and deontic logic, where basic notions and definitional concepts can be expressed in terms of the awareness operator combined with the box modality. Furthermore, these conceptual links point to interesting properties of awareness sets beyond those standardly assumed in awareness logics, i.e. positive and negative introspection. We show that the properties we list are characterised by corresponding canonical formulas, so as to obtain a series of off-the-shelf axiomatisations for them. As a second focus, we investigate the general dynamics of this framework by means of event models. Of specific interest in this context is to know under which conditions, given a model that satisfies some property, the update with an event model keeps it within the intended class. This is known as the closure problem in general dynamic epistemic logics. As a main contribution, we prove a number of closure theorems providing sufficient conditions for the preservation of our properties. Again, these results enable us to axiomatize our dynamic logics by means of reduction axioms.","sentences":["This work builds upon a well-established research tradition on modal logics of awareness.","One of its aims is to export tools and techniques to other areas within modal logic.","To this end, we illustrate a number of significant bridges with abstract argumentation, justification logics, the epistemic logic of knowing-what and deontic logic, where basic notions and definitional concepts can be expressed in terms of the awareness operator combined with the box modality.","Furthermore, these conceptual links point to interesting properties of awareness sets beyond those standardly assumed in awareness logics, i.e. positive and negative introspection.","We show that the properties we list are characterised by corresponding canonical formulas, so as to obtain a series of off-the-shelf axiomatisations for them.","As a second focus, we investigate the general dynamics of this framework by means of event models.","Of specific interest in this context is to know under which conditions, given a model that satisfies some property, the update with an event model keeps it within the intended class.","This is known as the closure problem in general dynamic epistemic logics.","As a main contribution, we prove a number of closure theorems providing sufficient conditions for the preservation of our properties.","Again, these results enable us to axiomatize our dynamic logics by means of reduction axioms."],"url":"http://arxiv.org/abs/2307.05049v1"}
{"created":"2023-07-11 06:52:49","title":"A Blockchain-based two Factor Honeytoken Authentication System","abstract":"This paper extends and advances our recently introduced two-factor Honeytoken authentication method by incorporating blockchain technology. This novel approach strengthens the authentication method to prevent many attacks including tampering attacks. Evaluation results show that integrating blockchain into the Honeytoken method could improve performance and operational efficiency.","sentences":["This paper extends and advances our recently introduced two-factor Honeytoken authentication method by incorporating blockchain technology.","This novel approach strengthens the authentication method to prevent many attacks including tampering attacks.","Evaluation results show that integrating blockchain into the Honeytoken method could improve performance and operational efficiency."],"url":"http://arxiv.org/abs/2307.05047v1"}
{"created":"2023-07-11 06:50:49","title":"Epistemic Syllogistic: First Steps","abstract":"Aristotle's discussions on modal syllogistic have often been viewed as error-prone and have garnered significant attention in the literature due to historical and philosophical interests. However, from a contemporary standpoint, they also introduced natural fragments of first-order modal logic, warranting a comprehensive technical analysis. In this paper, drawing inspiration from the natural logic program, we propose and examine several variants of modal syllogistic within the epistemic context, thereby coining the term Epistemic Syllogistic. Specifically, we concentrate on the de re interpretation of epistemic syllogisms containing non-trivial yet natural expressions such as \"all things known to be A are also known to be not B.\" We explore the epistemic apodeictic syllogistic and its extensions, which accommodate more complex terms. Our main contributions include several axiomatizations of these logics, with completeness proofs that may be of independent interest.","sentences":["Aristotle's discussions on modal syllogistic have often been viewed as error-prone and have garnered significant attention in the literature due to historical and philosophical interests.","However, from a contemporary standpoint, they also introduced natural fragments of first-order modal logic, warranting a comprehensive technical analysis.","In this paper, drawing inspiration from the natural logic program, we propose and examine several variants of modal syllogistic within the epistemic context, thereby coining the term Epistemic Syllogistic.","Specifically, we concentrate on the de re interpretation of epistemic syllogisms containing non-trivial yet natural expressions such as \"all things known to be A are also known to be not B.\" We explore the epistemic apodeictic syllogistic and its extensions, which accommodate more complex terms.","Our main contributions include several axiomatizations of these logics, with completeness proofs that may be of independent interest."],"url":"http://arxiv.org/abs/2307.05043v1"}
{"created":"2023-07-11 06:50:07","title":"Implicit Knowledge in Unawareness Structures -- Extended Abstract","abstract":"Awareness structures by Fagin and Halpern (1988) (FH) feature a syntactic awareness correspondence and accessibility relations modeling implicit knowledge. They are a flexible model of unawareness, and best interpreted from a outside modeler's perspective. Unawareness structures by Heifetz, Meier, and Schipper (2006, 2008) (HMS) model awareness by a lattice of state-spaces and explicit knowledge via a possibility correspondence. They can be interpreted as providing the subjective views of agents. Open questions include (1) how implicit knowledge can be defined in HMS structures, and (2) in which way FH structures can be extended to model the agents' subjective views. In this paper, we address (1) by showing how to derive implicit knowledge from explicit knowledge in HMS models. We also introduce a variant of HMS models that instead of explicit knowledge, takes implicit knowledge and awareness as primitives. Further, we address (2) by introducing a category of FH models that are modally equivalent relative to sublanguages and can be interpreted as agents' subjective views depending on their awareness. These constructions allow us to show an equivalence between HMS and FH models. As a corollary, we obtain soundness and completeness of HMS models with respect to the Logic of Propositional Awareness, based on a language featuring both implicit and explicit knowledge.","sentences":["Awareness structures by Fagin and Halpern (1988) (FH) feature a syntactic awareness correspondence and accessibility relations modeling implicit knowledge.","They are a flexible model of unawareness, and best interpreted from a outside modeler's perspective.","Unawareness structures by Heifetz, Meier, and Schipper (2006, 2008) (HMS) model awareness by a lattice of state-spaces and explicit knowledge via a possibility correspondence.","They can be interpreted as providing the subjective views of agents.","Open questions include (1) how implicit knowledge can be defined in HMS structures, and (2) in which way FH structures can be extended to model the agents' subjective views.","In this paper, we address (1) by showing how to derive implicit knowledge from explicit knowledge in HMS models.","We also introduce a variant of HMS models that instead of explicit knowledge, takes implicit knowledge and awareness as primitives.","Further, we address (2) by introducing a category of FH models that are modally equivalent relative to sublanguages and can be interpreted as agents' subjective views depending on their awareness.","These constructions allow us to show an equivalence between HMS and FH models.","As a corollary, we obtain soundness and completeness of HMS models with respect to the Logic of Propositional Awareness, based on a language featuring both implicit and explicit knowledge."],"url":"http://arxiv.org/abs/2307.05041v1"}
{"created":"2023-07-11 06:49:47","title":"Simple Axioms for Local Properties","abstract":"Correspondence theory allows us to create sound and complete axiomatizations for modal logic on frames with certain properties. For example, if we restrict ourselves to transitive frames we should add the axiom $\\square \\phi \\rightarrow \\square\\square\\phi$ which, among other things, can be interpreted as positive introspection. One limitation of this technique is that the frame property and the axiom are assumed to hold globally, i.e., the relation is transitive throughout the frame, and the agent's knowledge satisfies positive introspection in every world.   In a modal logic with local properties, we can reason about properties that are not global. So, for example, transitivity might hold only in certain parts of the model and, as a result, the agent's knowledge might satisfy positive introspection in some worlds but not in others. Van Ditmarsch et al. (2012) introduced sound and complete axiomatizations for modal logics with certain local properties. Unfortunately, those axiomatizations are rather complex. Here, we introduce far simpler axiomatizations for a wide range of local properties.","sentences":["Correspondence theory allows us to create sound and complete axiomatizations for modal logic on frames with certain properties.","For example, if we restrict ourselves to transitive frames we should add the axiom $\\square \\phi \\rightarrow \\square\\square\\phi$ which, among other things, can be interpreted as positive introspection.","One limitation of this technique is that the frame property and the axiom are assumed to hold globally, i.e., the relation is transitive throughout the frame, and the agent's knowledge satisfies positive introspection in every world.   ","In a modal logic with local properties, we can reason about properties that are not global.","So, for example, transitivity might hold only in certain parts of the model and, as a result, the agent's knowledge might satisfy positive introspection in some worlds but not in others.","Van Ditmarsch et al. (2012) introduced sound and complete axiomatizations for modal logics with certain local properties.","Unfortunately, those axiomatizations are rather complex.","Here, we introduce far simpler axiomatizations for a wide range of local properties."],"url":"http://arxiv.org/abs/2307.05040v1"}
{"created":"2023-07-11 06:40:27","title":"Disentangled Contrastive Image Translation for Nighttime Surveillance","abstract":"Nighttime surveillance suffers from degradation due to poor illumination and arduous human annotations. It is challengable and remains a security risk at night. Existing methods rely on multi-spectral images to perceive objects in the dark, which are troubled by low resolution and color absence. We argue that the ultimate solution for nighttime surveillance is night-to-day translation, or Night2Day, which aims to translate a surveillance scene from nighttime to the daytime while maintaining semantic consistency. To achieve this, this paper presents a Disentangled Contrastive (DiCo) learning method. Specifically, to address the poor and complex illumination in the nighttime scenes, we propose a learnable physical prior, i.e., the color invariant, which provides a stable perception of a highly dynamic night environment and can be incorporated into the learning pipeline of neural networks. Targeting the surveillance scenes, we develop a disentangled representation, which is an auxiliary pretext task that separates surveillance scenes into the foreground and background with contrastive learning. Such a strategy can extract the semantics without supervision and boost our model to achieve instance-aware translation. Finally, we incorporate all the modules above into generative adversarial networks and achieve high-fidelity translation. This paper also contributes a new surveillance dataset called NightSuR. It includes six scenes to support the study on nighttime surveillance. This dataset collects nighttime images with different properties of nighttime environments, such as flare and extreme darkness. Extensive experiments demonstrate that our method outperforms existing works significantly. The dataset and source code will be released on GitHub soon.","sentences":["Nighttime surveillance suffers from degradation due to poor illumination and arduous human annotations.","It is challengable and remains a security risk at night.","Existing methods rely on multi-spectral images to perceive objects in the dark, which are troubled by low resolution and color absence.","We argue that the ultimate solution for nighttime surveillance is night-to-day translation, or Night2Day, which aims to translate a surveillance scene from nighttime to the daytime while maintaining semantic consistency.","To achieve this, this paper presents a Disentangled Contrastive (DiCo) learning method.","Specifically, to address the poor and complex illumination in the nighttime scenes, we propose a learnable physical prior, i.e., the color invariant, which provides a stable perception of a highly dynamic night environment and can be incorporated into the learning pipeline of neural networks.","Targeting the surveillance scenes, we develop a disentangled representation, which is an auxiliary pretext task that separates surveillance scenes into the foreground and background with contrastive learning.","Such a strategy can extract the semantics without supervision and boost our model to achieve instance-aware translation.","Finally, we incorporate all the modules above into generative adversarial networks and achieve high-fidelity translation.","This paper also contributes a new surveillance dataset called NightSuR.","It includes six scenes to support the study on nighttime surveillance.","This dataset collects nighttime images with different properties of nighttime environments, such as flare and extreme darkness.","Extensive experiments demonstrate that our method outperforms existing works significantly.","The dataset and source code will be released on GitHub soon."],"url":"http://arxiv.org/abs/2307.05038v1"}
{"created":"2023-07-11 06:29:31","title":"Neural-Symbolic Recommendation with Graph-Enhanced Information","abstract":"The recommendation system is not only a problem of inductive statistics from data but also a cognitive task that requires reasoning ability. The most advanced graph neural networks have been widely used in recommendation systems because they can capture implicit structured information from graph-structured data. However, like most neural network algorithms, they only learn matching patterns from a perception perspective. Some researchers use user behavior for logic reasoning to achieve recommendation prediction from the perspective of cognitive reasoning, but this kind of reasoning is a local one and ignores implicit information on a global scale. In this work, we combine the advantages of graph neural networks and propositional logic operations to construct a neuro-symbolic recommendation model with both global implicit reasoning ability and local explicit logic reasoning ability. We first build an item-item graph based on the principle of adjacent interaction and use graph neural networks to capture implicit information in global data. Then we transform user behavior into propositional logic expressions to achieve recommendations from the perspective of cognitive reasoning. Extensive experiments on five public datasets show that our proposed model outperforms several state-of-the-art methods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].","sentences":["The recommendation system is not only a problem of inductive statistics from data but also a cognitive task that requires reasoning ability.","The most advanced graph neural networks have been widely used in recommendation systems because they can capture implicit structured information from graph-structured data.","However, like most neural network algorithms, they only learn matching patterns from a perception perspective.","Some researchers use user behavior for logic reasoning to achieve recommendation prediction from the perspective of cognitive reasoning, but this kind of reasoning is a local one and ignores implicit information on a global scale.","In this work, we combine the advantages of graph neural networks and propositional logic operations to construct a neuro-symbolic recommendation model with both global implicit reasoning ability and local explicit logic reasoning ability.","We first build an item-item graph based on the principle of adjacent interaction and use graph neural networks to capture implicit information in global data.","Then we transform user behavior into propositional logic expressions to achieve recommendations from the perspective of cognitive reasoning.","Extensive experiments on five public datasets show that our proposed model outperforms several state-of-the-art methods, source code is avaliable at [https://github.com/hanzo2020/GNNLR]."],"url":"http://arxiv.org/abs/2307.05036v1"}
{"created":"2023-07-11 06:19:25","title":"Number Systems for Deep Neural Network Architectures: A Survey","abstract":"Deep neural networks (DNNs) have become an enabling component for a myriad of artificial intelligence applications. DNNs have shown sometimes superior performance, even compared to humans, in cases such as self-driving, health applications, etc. Because of their computational complexity, deploying DNNs in resource-constrained devices still faces many challenges related to computing complexity, energy efficiency, latency, and cost. To this end, several research directions are being pursued by both academia and industry to accelerate and efficiently implement DNNs. One important direction is determining the appropriate data representation for the massive amount of data involved in DNN processing. Using conventional number systems has been found to be sub-optimal for DNNs. Alternatively, a great body of research focuses on exploring suitable number systems. This article aims to provide a comprehensive survey and discussion about alternative number systems for more efficient representations of DNN data. Various number systems (conventional/unconventional) exploited for DNNs are discussed. The impact of these number systems on the performance and hardware design of DNNs is considered. In addition, this paper highlights the challenges associated with each number system and various solutions that are proposed for addressing them. The reader will be able to understand the importance of an efficient number system for DNN, learn about the widely used number systems for DNN, understand the trade-offs between various number systems, and consider various design aspects that affect the impact of number systems on DNN performance. In addition, the recent trends and related research opportunities will be highlighted","sentences":["Deep neural networks (DNNs) have become an enabling component for a myriad of artificial intelligence applications.","DNNs have shown sometimes superior performance, even compared to humans, in cases such as self-driving, health applications, etc.","Because of their computational complexity, deploying DNNs in resource-constrained devices still faces many challenges related to computing complexity, energy efficiency, latency, and cost.","To this end, several research directions are being pursued by both academia and industry to accelerate and efficiently implement DNNs.","One important direction is determining the appropriate data representation for the massive amount of data involved in DNN processing.","Using conventional number systems has been found to be sub-optimal for DNNs.","Alternatively, a great body of research focuses on exploring suitable number systems.","This article aims to provide a comprehensive survey and discussion about alternative number systems for more efficient representations of DNN data.","Various number systems (conventional/unconventional) exploited for DNNs are discussed.","The impact of these number systems on the performance and hardware design of DNNs is considered.","In addition, this paper highlights the challenges associated with each number system and various solutions that are proposed for addressing them.","The reader will be able to understand the importance of an efficient number system for DNN, learn about the widely used number systems for DNN, understand the trade-offs between various number systems, and consider various design aspects that affect the impact of number systems on DNN performance.","In addition, the recent trends and related research opportunities will be highlighted"],"url":"http://arxiv.org/abs/2307.05035v1"}
{"created":"2023-07-11 06:18:07","title":"Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference","abstract":"We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers. After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.","sentences":["We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic.","We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014).","To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009).","We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis.","Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules.","We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios.","We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers.","After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers."],"url":"http://arxiv.org/abs/2307.05034v2"}
{"created":"2023-07-11 06:15:12","title":"Towards Anytime Optical Flow Estimation with Event Cameras","abstract":"Event cameras are capable of responding to log-brightness changes in microseconds. Its characteristic of producing responses only to the changing region is particularly suitable for optical flow estimation. In contrast to the super low-latency response speed of event cameras, existing datasets collected via event cameras, however, only provide limited frame rate optical flow ground truth, (e.g., at 10Hz), greatly restricting the potential of event-driven optical flow. To address this challenge, we put forward a high-frame-rate, low-latency event representation Unified Voxel Grid, sequentially fed into the network bin by bin. We then propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce high-frame-rate event optical flow with only low-frame-rate optical flow ground truth for supervision. The key component of our EVA-Flow is the stacked Spatiotemporal Motion Refinement (SMR) module, which predicts temporally-dense optical flow and enhances the accuracy via spatial-temporal motion refinement. The time-dense feature warping utilized in the SMR module provides implicit supervision for the intermediate optical flow. Additionally, we introduce the Rectified Flow Warp Loss (RFWL) for the unsupervised evaluation of intermediate optical flow in the absence of ground truth. This is, to the best of our knowledge, the first work focusing on anytime optical flow estimation via event cameras. A comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow achieves competitive performance, super-low-latency (5ms), fastest inference (9.2ms), time-dense motion estimation (200Hz), and strong generalization. Our code will be available at https://github.com/Yaozhuwa/EVA-Flow.","sentences":["Event cameras are capable of responding to log-brightness changes in microseconds.","Its characteristic of producing responses only to the changing region is particularly suitable for optical flow estimation.","In contrast to the super low-latency response speed of event cameras, existing datasets collected via event cameras, however, only provide limited frame rate optical flow ground truth, (e.g., at 10Hz), greatly restricting the potential of event-driven optical flow.","To address this challenge, we put forward a high-frame-rate, low-latency event representation Unified Voxel Grid, sequentially fed into the network bin by bin.","We then propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce high-frame-rate event optical flow with only low-frame-rate optical flow ground truth for supervision.","The key component of our EVA-Flow is the stacked Spatiotemporal Motion Refinement (SMR) module, which predicts temporally-dense optical flow and enhances the accuracy via spatial-temporal motion refinement.","The time-dense feature warping utilized in the SMR module provides implicit supervision for the intermediate optical flow.","Additionally, we introduce the Rectified Flow Warp Loss (RFWL) for the unsupervised evaluation of intermediate optical flow in the absence of ground truth.","This is, to the best of our knowledge, the first work focusing on anytime optical flow estimation via event cameras.","A comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow achieves competitive performance, super-low-latency (5ms), fastest inference (9.2ms), time-dense motion estimation (200Hz), and strong generalization.","Our code will be available at https://github.com/Yaozhuwa/EVA-Flow."],"url":"http://arxiv.org/abs/2307.05033v1"}
{"created":"2023-07-11 06:05:06","title":"FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms","abstract":"This thesis explores open-sourced machine learning (ML) model explanation tools to understand whether these tools can allow a layman to visualize, understand, and suggest intuitive remedies to unfairness in ML-based decision-support systems. Machine learning models trained on datasets biased against minority groups are increasingly used to guide life-altering social decisions, prompting the urgent need to study their logic for unfairness. Due to this problem's impact on vast populations of the general public, it is critical for the layperson -- not just subject matter experts in social justice or machine learning experts -- to understand the nature of unfairness within these algorithms and the potential trade-offs. Existing research on fairness in machine learning focuses mostly on the mathematical definitions and tools to understand and remedy unfair models, with some directly citing user-interactive tools as necessary for future work. This thesis presents FairLay-ML, a proof-of-concept GUI integrating some of the most promising tools to provide intuitive explanations for unfair logic in ML models by integrating existing research tools (e.g. Local Interpretable Model-Agnostic Explanations) with existing ML-focused GUI (e.g. Python Streamlit). We test FairLay-ML using models of various accuracy and fairness generated by an unfairness detector tool, Parfait-ML, and validate our results using Themis. Our study finds that the technology stack used for FairLay-ML makes it easy to install and provides real-time black-box explanations of pre-trained models to users. Furthermore, the explanations provided translate to actionable remedies.","sentences":["This thesis explores open-sourced machine learning (ML) model explanation tools to understand whether these tools can allow a layman to visualize, understand, and suggest intuitive remedies to unfairness in ML-based decision-support systems.","Machine learning models trained on datasets biased against minority groups are increasingly used to guide life-altering social decisions, prompting the urgent need to study their logic for unfairness.","Due to this problem's impact on vast populations of the general public, it is critical for the layperson -- not just subject matter experts in social justice or machine learning experts -- to understand the nature of unfairness within these algorithms and the potential trade-offs.","Existing research on fairness in machine learning focuses mostly on the mathematical definitions and tools to understand and remedy unfair models, with some directly citing user-interactive tools as necessary for future work.","This thesis presents FairLay-ML, a proof-of-concept GUI integrating some of the most promising tools to provide intuitive explanations for unfair logic in ML models by integrating existing research tools (e.g. Local Interpretable Model-Agnostic Explanations) with existing ML-focused GUI (e.g. Python Streamlit).","We test FairLay-ML using models of various accuracy and fairness generated by an unfairness detector tool, Parfait-ML, and validate our results using Themis.","Our study finds that the technology stack used for FairLay-ML makes it easy to install and provides real-time black-box explanations of pre-trained models to users.","Furthermore, the explanations provided translate to actionable remedies."],"url":"http://arxiv.org/abs/2307.05029v1"}
{"created":"2023-07-11 05:58:20","title":"Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels","abstract":"In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robustness to noisy training labels while generalizing to clean data. These algorithms often incorporate sophisticated techniques, such as noise modeling, label correction, and co-training. In this study, we demonstrate that a simple baseline using cross-entropy loss, combined with widely used regularization strategies like learning rate decay, model weights average, and data augmentations, can outperform state-of-the-art methods. Our findings suggest that employing a combination of regularization strategies can be more effective than intricate algorithms in tackling the challenges of learning with noisy labels. While some of these regularization strategies have been utilized in previous noisy label learning research, their full potential has not been thoroughly explored. Our results encourage a reevaluation of benchmarks for learning with noisy labels and prompt reconsideration of the role of specialized learning algorithms designed for training with noisy labels.","sentences":["In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robustness to noisy training labels while generalizing to clean data.","These algorithms often incorporate sophisticated techniques, such as noise modeling, label correction, and co-training.","In this study, we demonstrate that a simple baseline using cross-entropy loss, combined with widely used regularization strategies like learning rate decay, model weights average, and data augmentations, can outperform state-of-the-art methods.","Our findings suggest that employing a combination of regularization strategies can be more effective than intricate algorithms in tackling the challenges of learning with noisy labels.","While some of these regularization strategies have been utilized in previous noisy label learning research, their full potential has not been thoroughly explored.","Our results encourage a reevaluation of benchmarks for learning with noisy labels and prompt reconsideration of the role of specialized learning algorithms designed for training with noisy labels."],"url":"http://arxiv.org/abs/2307.05025v1"}
{"created":"2023-07-11 05:56:21","title":"Best Arm Identification Based Beam Acquisition in Stationary and Abruptly Changing Environments","abstract":"We study the initial beam acquisition problem in millimeter wave (mm-wave) networks from the perspective of best arm identification in multi-armed bandits (MABs). For the stationary environment, we propose a novel algorithm called concurrent beam exploration, CBE, in which multiple beams are grouped based on the beam indices and are simultaneously activated to detect the presence of the user. The best beam is then identified using a Hamming decoding strategy. For the case of orthogonal and highly directional thin beams, we characterize the performance of CBE in terms of the probability of missed detection and false alarm in a beam group (BG). Leveraging this, we derive the probability of beam selection error and prove that CBE outperforms the state-of-the-art strategies in this metric.   Then, for the abruptly changing environments, e.g., in the case of moving blockages, we characterize the performance of the classical sequential halving (SH) algorithm. In particular, we derive the conditions on the distribution of the change for which the beam selection error is exponentially bounded. In case the change is restricted to a subset of the beams, we devise a strategy called K-sequential halving and exhaustive search, K-SHES, that leads to an improved bound for the beam selection error as compared to SH. This policy is particularly useful when a near-optimal beam becomes optimal during the beam-selection procedure due to abruptly changing channel conditions. Finally, we demonstrate the efficacy of the proposed scheme by employing it in a tandem beam refinement and data transmission scheme.","sentences":["We study the initial beam acquisition problem in millimeter wave (mm-wave) networks from the perspective of best arm identification in multi-armed bandits (MABs).","For the stationary environment, we propose a novel algorithm called concurrent beam exploration, CBE, in which multiple beams are grouped based on the beam indices and are simultaneously activated to detect the presence of the user.","The best beam is then identified using a Hamming decoding strategy.","For the case of orthogonal and highly directional thin beams, we characterize the performance of CBE in terms of the probability of missed detection and false alarm in a beam group (BG).","Leveraging this, we derive the probability of beam selection error and prove that CBE outperforms the state-of-the-art strategies in this metric.   ","Then, for the abruptly changing environments, e.g., in the case of moving blockages, we characterize the performance of the classical sequential halving (SH) algorithm.","In particular, we derive the conditions on the distribution of the change for which the beam selection error is exponentially bounded.","In case the change is restricted to a subset of the beams, we devise a strategy called K-sequential halving and exhaustive search, K-SHES, that leads to an improved bound for the beam selection error as compared to SH.","This policy is particularly useful when a near-optimal beam becomes optimal during the beam-selection procedure due to abruptly changing channel conditions.","Finally, we demonstrate the efficacy of the proposed scheme by employing it in a tandem beam refinement and data transmission scheme."],"url":"http://arxiv.org/abs/2307.05023v1"}
{"created":"2023-07-11 05:33:46","title":"Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification","abstract":"Decisions made by convolutional neural networks(CNN) can be understood and explained by visualizing discriminative regions on images. To this end, Class Activation Map (CAM) based methods were proposed as powerful interpretation tools, making the prediction of deep learning models more explainable, transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM, Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with fully-connected (FC) layers as a classifier. It is worth noting that many deep learning models classify images without FC layers, e.g., few-shot learning image classification, contrastive learning image classification, and image retrieval tasks. In this work, a post-hoc interpretation tool named feature activation map (FAM) is proposed, which can interpret deep learning models without FC layers as a classifier. In the proposed FAM algorithm, the channel-wise contribution weights are derived from the similarity scores between two image embeddings. The activation maps are linearly combined with the corresponding normalized contribution weights, forming the explanation map for visualization. The quantitative and qualitative experiments conducted on ten deep learning models for few-shot image classification, contrastive learning image classification and image retrieval tasks demonstrate the effectiveness of the proposed FAM algorithm.","sentences":["Decisions made by convolutional neural networks(CNN) can be understood and explained by visualizing discriminative regions on images.","To this end, Class Activation Map (CAM) based methods were proposed as powerful interpretation tools, making the prediction of deep learning models more explainable, transparent, and trustworthy.","However, all the CAM-based methods (e.g., CAM, Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with fully-connected (FC) layers as a classifier.","It is worth noting that many deep learning models classify images without FC layers, e.g., few-shot learning image classification, contrastive learning image classification, and image retrieval tasks.","In this work, a post-hoc interpretation tool named feature activation map (FAM) is proposed, which can interpret deep learning models without FC layers as a classifier.","In the proposed FAM algorithm, the channel-wise contribution weights are derived from the similarity scores between two image embeddings.","The activation maps are linearly combined with the corresponding normalized contribution weights, forming the explanation map for visualization.","The quantitative and qualitative experiments conducted on ten deep learning models for few-shot image classification, contrastive learning image classification and image retrieval tasks demonstrate the effectiveness of the proposed FAM algorithm."],"url":"http://arxiv.org/abs/2307.05017v1"}
{"created":"2023-07-11 05:32:21","title":"TRansPose: Large-Scale Multispectral Dataset for Transparent Object","abstract":"Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventional vision sensors due to their unique material properties, not being well perceived from RGB or depth cameras. Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape information for transparent objects. In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal infrared (TIR) images, and object poses to promote transparent object research. The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks, ground-truth poses, and completed depth information. The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator. Spanning 87 sequences, TRansPose covers various challenging real-life scenarios, including objects filled with water, diverse lighting conditions, heavy clutter, non-transparent or translucent containers, objects in plastic bags, and multi-stacked objects. TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset","sentences":["Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventional vision sensors due to their unique material properties, not being well perceived from RGB or depth cameras.","Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape information for transparent objects.","In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal infrared (TIR) images, and object poses to promote transparent object research.","The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects.","It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks, ground-truth poses, and completed depth information.","The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator.","Spanning 87 sequences, TRansPose covers various challenging real-life scenarios, including objects filled with water, diverse lighting conditions, heavy clutter, non-transparent or translucent containers, objects in plastic bags, and multi-stacked objects.","TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset"],"url":"http://arxiv.org/abs/2307.05016v1"}
{"created":"2023-07-11 05:17:42","title":"Test-Time Training on Video Streams","abstract":"Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using synthetic videos. We conceptualize locality as the advantage of online over offline TTT. We analyze the role of locality with ablations and a theory based on bias-variance trade-off.","sentences":["Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time.","Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders.","We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order.","Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before.","Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets.","The relative improvement is 45% and 66% for instance and panoptic segmentation.","Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order.","This differs from previous findings using synthetic videos.","We conceptualize locality as the advantage of online over offline TTT.","We analyze the role of locality with ablations and a theory based on bias-variance trade-off."],"url":"http://arxiv.org/abs/2307.05014v1"}
{"created":"2023-07-11 03:57:00","title":"Improving RNN-Transducers with Acoustic LookAhead","abstract":"RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end model for speech to text conversion because of their high accuracy and streaming capabilities. A typical RNN-T independently encodes the input audio and the text context, and combines the two encodings by a thin joint network. While this architecture provides SOTA streaming accuracy, it also makes the model vulnerable to strong LM biasing which manifests as multi-step hallucination of text without acoustic evidence. In this paper we propose LookAhead that makes text representations more acoustically grounded by looking ahead into the future within the audio input. This technique yields a significant 5%-20% relative reduction in word error rate on both in-domain and out-of-domain evaluation sets.","sentences":["RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end model for speech to text conversion because of their high accuracy and streaming capabilities.","A typical RNN-T independently encodes the input audio and the text context, and combines the two encodings by a thin joint network.","While this architecture provides SOTA streaming accuracy, it also makes the model vulnerable to strong LM biasing which manifests as multi-step hallucination of text without acoustic evidence.","In this paper we propose LookAhead that makes text representations more acoustically grounded by looking ahead into the future within the audio input.","This technique yields a significant 5%-20% relative reduction in word error rate on both in-domain and out-of-domain evaluation sets."],"url":"http://arxiv.org/abs/2307.05006v1"}
{"created":"2023-07-11 03:53:46","title":"Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning","abstract":"This paper proposes a generative probabilistic model integrating emergent communication and multi-agent reinforcement learning. The agents plan their actions by probabilistic inference, called control as inference, and communicate using messages that are latent variables and estimated based on the planned actions. Through these messages, each agent can send information about its actions and know information about the actions of another agent. Therefore, the agents change their actions according to the estimated messages to achieve cooperative tasks. This inference of messages can be considered as communication, and this procedure can be formulated by the Metropolis-Hasting naming game. Through experiments in the grid world environment, we show that the proposed PGM can infer meaningful messages to achieve the cooperative task.","sentences":["This paper proposes a generative probabilistic model integrating emergent communication and multi-agent reinforcement learning.","The agents plan their actions by probabilistic inference, called control as inference, and communicate using messages that are latent variables and estimated based on the planned actions.","Through these messages, each agent can send information about its actions and know information about the actions of another agent.","Therefore, the agents change their actions according to the estimated messages to achieve cooperative tasks.","This inference of messages can be considered as communication, and this procedure can be formulated by the Metropolis-Hasting naming game.","Through experiments in the grid world environment, we show that the proposed PGM can infer meaningful messages to achieve the cooperative task."],"url":"http://arxiv.org/abs/2307.05004v1"}
{"created":"2023-07-11 03:40:10","title":"Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar","abstract":"Rendering photorealistic and dynamically moving human heads is crucial for ensuring a pleasant and immersive experience in AR/VR and video conferencing applications. However, existing methods often struggle to model challenging facial regions (e.g., mouth interior, eyes, hair/beard), resulting in unrealistic and blurry results. In this paper, we propose {\\fullname} ({\\name}), a method that adopts the neural point representation as well as the neural volume rendering process and discards the predefined connectivity and hard correspondence imposed by mesh-based approaches. Specifically, the neural points are strategically constrained around the surface of the target expression via a high-resolution UV displacement map, achieving increased modeling capacity and more accurate control. We introduce three technical innovations to improve the rendering and training efficiency: a patch-wise depth-guided (shading point) sampling strategy, a lightweight radiance decoding process, and a Grid-Error-Patch (GEP) ray sampling strategy during training. By design, our {\\name} is better equipped to handle topologically changing regions and thin structures while also ensuring accurate expression control when animating avatars. Experiments conducted on three subjects from the Multiface dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods, especially in handling challenging facial regions.","sentences":["Rendering photorealistic and dynamically moving human heads is crucial for ensuring a pleasant and immersive experience in AR/VR and video conferencing applications.","However, existing methods often struggle to model challenging facial regions (e.g., mouth interior, eyes, hair/beard), resulting in unrealistic and blurry results.","In this paper, we propose {\\fullname} ({\\name}), a method that adopts the neural point representation as well as the neural volume rendering process and discards the predefined connectivity and hard correspondence imposed by mesh-based approaches.","Specifically, the neural points are strategically constrained around the surface of the target expression via a high-resolution UV displacement map, achieving increased modeling capacity and more accurate control.","We introduce three technical innovations to improve the rendering and training efficiency: a patch-wise depth-guided (shading point) sampling strategy, a lightweight radiance decoding process, and a Grid-Error-Patch (GEP) ray sampling strategy during training.","By design, our {\\name} is better equipped to handle topologically changing regions and thin structures while also ensuring accurate expression control when animating avatars.","Experiments conducted on three subjects from the Multiface dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods, especially in handling challenging facial regions."],"url":"http://arxiv.org/abs/2307.05000v1"}
{"created":"2023-07-11 03:32:20","title":"Selective Sampling and Imitation Learning via Online Regression","abstract":"We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.   Our algorithm for selective sampling leverages function approximation, and relies on an online regression oracle w.r.t.~the given model class to predict actions, and to decide whether to query the expert for its label. On the theoretical side, the regret bound of our algorithm is upper bounded by the regret of the online regression oracle, while the query complexity additionally depends on the eluder dimension of the model class. We complement this with a lower bound that demonstrates that our results are tight. We extend our selective sampling algorithm for IL with general function approximation and provide bounds on both the regret and the number of queries made to the noisy expert. A key novelty here is that our regret and query complexity bounds only depend on the number of times the optimal policy (and not the noisy expert, or the learner) go to states that have a small margin.","sentences":["We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback.","While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications.","In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful.","In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback.","Our contributions are twofold:","First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries.","Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.   ","Our algorithm for selective sampling leverages function approximation, and relies on an online regression oracle w.r.t.~the given model class to predict actions, and to decide whether to query the expert for its label.","On the theoretical side, the regret bound of our algorithm is upper bounded by the regret of the online regression oracle, while the query complexity additionally depends on the eluder dimension of the model class.","We complement this with a lower bound that demonstrates that our results are tight.","We extend our selective sampling algorithm for IL with general function approximation and provide bounds on both the regret and the number of queries made to the noisy expert.","A key novelty here is that our regret and query complexity bounds only depend on the number of times the optimal policy (and not the noisy expert, or the learner) go to states that have a small margin."],"url":"http://arxiv.org/abs/2307.04998v1"}
{"created":"2023-07-11 03:24:54","title":"Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning","abstract":"Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body of text data).Using the Reinforcement Learning-based recommender system we could leverage the graph traversal path leading to the recommendation as a way to generate interpretations (Path Directed Reasoning (PDR)). In the XGBoost-based approach, one can also provide explainable results using post-hoc methods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I am Five).Importantly, our approach offers explainable results, promoting better decision-making. This study underscores the potential of combining advanced machine learning techniques with KG-driven insights to bolster experience in customer relationship management.","sentences":["Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications.","For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions.","While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.","To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company.","The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers.","Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body of text data).Using the Reinforcement Learning-based recommender system we could leverage the graph traversal path leading to the recommendation as a way to generate interpretations (Path Directed Reasoning (PDR)).","In the XGBoost-based approach, one can also provide explainable results using post-hoc methods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I am Five).Importantly, our approach offers explainable results, promoting better decision-making.","This study underscores the potential of combining advanced machine learning techniques with KG-driven insights to bolster experience in customer relationship management."],"url":"http://arxiv.org/abs/2307.04996v1"}
{"created":"2023-07-11 03:17:40","title":"PowerFusion: A Tensor Compiler with Explicit Data Movement Description and Instruction-level Graph IR","abstract":"Deep neural networks (DNNs) are of critical use in different domains. To accelerate DNN computation, tensor compilers are proposed to generate efficient code on different domain-specific accelerators. Existing tensor compilers mainly focus on optimizing computation efficiency. However, memory access is becoming a key performance bottleneck because the computational performance of accelerators is increasing much faster than memory performance. The lack of direct description of memory access and data dependence in current tensor compilers' intermediate representation (IR) brings significant challenges to generate memory-efficient code.   In this paper, we propose IntelliGen, a tensor compiler that can generate high-performance code for memory-intensive operators by considering both computation and data movement optimizations. IntelliGen represent a DNN program using GIR, which includes primitives indicating its computation, data movement, and parallel strategies. This information will be further composed as an instruction-level dataflow graph to perform holistic optimizations by searching different memory access patterns and computation operations, and generating memory-efficient code on different hardware. We evaluate IntelliGen on NVIDIA GPU, AMD GPU, and Cambricon MLU, showing speedup up to 1.97x, 2.93x, and 16.91x(1.28x, 1.23x, and 2.31x on average), respectively, compared to current most performant frameworks.","sentences":["Deep neural networks (DNNs) are of critical use in different domains.","To accelerate DNN computation, tensor compilers are proposed to generate efficient code on different domain-specific accelerators.","Existing tensor compilers mainly focus on optimizing computation efficiency.","However, memory access is becoming a key performance bottleneck because the computational performance of accelerators is increasing much faster than memory performance.","The lack of direct description of memory access and data dependence in current tensor compilers' intermediate representation (IR) brings significant challenges to generate memory-efficient code.   ","In this paper, we propose IntelliGen, a tensor compiler that can generate high-performance code for memory-intensive operators by considering both computation and data movement optimizations.","IntelliGen represent a DNN program using GIR, which includes primitives indicating its computation, data movement, and parallel strategies.","This information will be further composed as an instruction-level dataflow graph to perform holistic optimizations by searching different memory access patterns and computation operations, and generating memory-efficient code on different hardware.","We evaluate IntelliGen on NVIDIA GPU, AMD GPU, and Cambricon MLU, showing speedup up to 1.97x, 2.93x, and 16.91x(1.28x, 1.23x, and 2.31x on average), respectively, compared to current most performant frameworks."],"url":"http://arxiv.org/abs/2307.04995v1"}
{"created":"2023-07-11 03:02:44","title":"Monotone deep Boltzmann machines","abstract":"Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods ever studied, are multi-layered probabilistic models governed by a pairwise energy function that describes the likelihood of all variables/nodes in the network. In practice, DBMs are often constrained, i.e., via the \\emph{restricted} Boltzmann machine (RBM) architecture (which does not permit intra-layer connections), in order to allow for more efficient inference. In this work, we revisit the generic DBM approach, and ask the question: are there other possible restrictions to their design that would enable efficient (approximate) inference? In particular, we develop a new class of restricted model, the monotone DBM, which allows for arbitrary self-connection in each layer, but restricts the \\emph{weights} in a manner that guarantees the existence and global uniqueness of a mean-field fixed point. To do this, we leverage tools from the recently-proposed monotone Deep Equilibrium model and show that a particular choice of activation results in a fixed-point iteration that gives a variational mean-field solution. While this approach is still largely conceptual, it is the first architecture that allows for efficient approximate inference in fully-general weight structures for DBMs. We apply this approach to simple deep convolutional Boltzmann architectures and demonstrate that it allows for tasks such as the joint completion and classification of images, within a single deep probabilistic setting, while avoiding the pitfalls of mean-field inference in traditional RBMs.","sentences":["Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods ever studied, are multi-layered probabilistic models governed by a pairwise energy function that describes the likelihood of all variables/nodes in the network.","In practice, DBMs are often constrained, i.e., via the \\emph{restricted} Boltzmann machine (RBM) architecture (which does not permit intra-layer connections), in order to allow for more efficient inference.","In this work, we revisit the generic DBM approach, and ask the question: are there other possible restrictions to their design that would enable efficient (approximate) inference?","In particular, we develop a new class of restricted model, the monotone DBM, which allows for arbitrary self-connection in each layer, but restricts the \\emph{weights} in a manner that guarantees the existence and global uniqueness of a mean-field fixed point.","To do this, we leverage tools from the recently-proposed monotone Deep Equilibrium model and show that a particular choice of activation results in a fixed-point iteration that gives a variational mean-field solution.","While this approach is still largely conceptual, it is the first architecture that allows for efficient approximate inference in fully-general weight structures for DBMs.","We apply this approach to simple deep convolutional Boltzmann architectures and demonstrate that it allows for tasks such as the joint completion and classification of images, within a single deep probabilistic setting, while avoiding the pitfalls of mean-field inference in traditional RBMs."],"url":"http://arxiv.org/abs/2307.04990v1"}
{"created":"2023-07-11 02:58:10","title":"Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation","abstract":"The practical utility of causality in decision-making is widely recognized, with causal discovery and inference being inherently intertwined. Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference. To address this gap, we evaluate six established baseline causal discovery methods and a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation. Through the implementation of a robust evaluation procedure, we offer valuable insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios. Furthermore, the results of our study demonstrate that GFlowNets possess the capability to effectively capture a wide range of useful and diverse ATE modes.","sentences":["The practical utility of causality in decision-making is widely recognized, with causal discovery and inference being inherently intertwined.","Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference.","To address this gap, we evaluate six established baseline causal discovery methods and a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation.","Through the implementation of a robust evaluation procedure, we offer valuable insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios.","Furthermore, the results of our study demonstrate that GFlowNets possess the capability to effectively capture a wide range of useful and diverse ATE modes."],"url":"http://arxiv.org/abs/2307.04988v1"}
{"created":"2023-07-11 02:52:32","title":"Epidemic Modeling with Generative Agents","abstract":"This study offers a new paradigm of individual-level modeling to address the grand challenge of incorporating human behavior in epidemic models. Using generative artificial intelligence in an agent-based epidemic model, each agent is empowered to make its own reasonings and decisions via connecting to a large language model such as ChatGPT. Through various simulation experiments, we present compelling evidence that generative agents mimic real-world behaviors such as quarantining when sick and self-isolation when cases rise. Collectively, the agents demonstrate patterns akin to multiple waves observed in recent pandemics followed by an endemic period. Moreover, the agents successfully flatten the epidemic curve. This study creates potential to improve dynamic system modeling by offering a way to represent human brain, reasoning, and decision making.","sentences":["This study offers a new paradigm of individual-level modeling to address the grand challenge of incorporating human behavior in epidemic models.","Using generative artificial intelligence in an agent-based epidemic model, each agent is empowered to make its own reasonings and decisions via connecting to a large language model such as ChatGPT.","Through various simulation experiments, we present compelling evidence that generative agents mimic real-world behaviors such as quarantining when sick and self-isolation when cases rise.","Collectively, the agents demonstrate patterns akin to multiple waves observed in recent pandemics followed by an endemic period.","Moreover, the agents successfully flatten the epidemic curve.","This study creates potential to improve dynamic system modeling by offering a way to represent human brain, reasoning, and decision making."],"url":"http://arxiv.org/abs/2307.04986v1"}
{"created":"2023-07-11 02:39:46","title":"A Multi-view Impartial Decision Network for Frontotemporal Dementia Diagnosis","abstract":"Frontotemporal Dementia (FTD) diagnosis has been successfully progress using deep learning techniques. However, current FTD identification methods suffer from two limitations. Firstly, they do not exploit the potential of multi-view functional magnetic resonance imaging (fMRI) for classifying FTD. Secondly, they do not consider the reliability of the multi-view FTD diagnosis. To address these limitations, we propose a reliable multi-view impartial decision network (MID-Net) for FTD diagnosis in fMRI. Our MID-Net provides confidence for each view and generates a reliable prediction without any conflict. To achieve this, we employ multiple expert models to extract evidence from the abundant neural network information contained in fMRI images. We then introduce the Dirichlet Distribution to characterize the expert class probability distribution from an evidence level. Additionally, a novel Impartial Decision Maker (IDer) is proposed to combine the different opinions inductively to arrive at an unbiased prediction without additional computation cost. Overall, our MID-Net dynamically integrates the decisions of different experts on FTD disease, especially when dealing with multi-view high-conflict cases. Extensive experiments on a high-quality FTD fMRI dataset demonstrate that our model outperforms previous methods and provides high uncertainty for hard-to-classify examples. We believe that our approach represents a significant step toward the deployment of reliable FTD decision-making under multi-expert conditions. We will release the codes for reproduction after acceptance.","sentences":["Frontotemporal Dementia (FTD) diagnosis has been successfully progress using deep learning techniques.","However, current FTD identification methods suffer from two limitations.","Firstly, they do not exploit the potential of multi-view functional magnetic resonance imaging (fMRI) for classifying FTD.","Secondly, they do not consider the reliability of the multi-view FTD diagnosis.","To address these limitations, we propose a reliable multi-view impartial decision network (MID-Net) for FTD diagnosis in fMRI.","Our MID-Net provides confidence for each view and generates a reliable prediction without any conflict.","To achieve this, we employ multiple expert models to extract evidence from the abundant neural network information contained in fMRI images.","We then introduce the Dirichlet Distribution to characterize the expert class probability distribution from an evidence level.","Additionally, a novel Impartial Decision Maker (IDer) is proposed to combine the different opinions inductively to arrive at an unbiased prediction without additional computation cost.","Overall, our MID-Net dynamically integrates the decisions of different experts on FTD disease, especially when dealing with multi-view high-conflict cases.","Extensive experiments on a high-quality FTD fMRI dataset demonstrate that our model outperforms previous methods and provides high uncertainty for hard-to-classify examples.","We believe that our approach represents a significant step toward the deployment of reliable FTD decision-making under multi-expert conditions.","We will release the codes for reproduction after acceptance."],"url":"http://arxiv.org/abs/2307.04981v1"}
{"created":"2023-07-11 02:35:26","title":"Diffusion idea exploration for art generation","abstract":"Cross-Modal learning tasks have picked up pace in recent times. With plethora of applications in diverse areas, generation of novel content using multiple modalities of data has remained a challenging problem. To address the same, various generative modelling techniques have been proposed for specific tasks. Novel and creative image generation is one important aspect for industrial application which could help as an arm for novel content generation. Techniques proposed previously used Generative Adversarial Network(GAN), autoregressive models and Variational Autoencoders (VAE) for accomplishing similar tasks. These approaches are limited in their capability to produce images guided by either text instructions or rough sketch images decreasing the overall performance of image generator. We used state of the art diffusion models to generate creative art by primarily leveraging text with additional support of rough sketches. Diffusion starts with a pattern of random dots and slowly converts that pattern into a design image using the guiding information fed into the model. Diffusion models have recently outperformed other generative models in image generation tasks using cross modal data as guiding information. The initial experiments for this task of novel image generation demonstrated promising qualitative results.","sentences":["Cross-Modal learning tasks have picked up pace in recent times.","With plethora of applications in diverse areas, generation of novel content using multiple modalities of data has remained a challenging problem.","To address the same, various generative modelling techniques have been proposed for specific tasks.","Novel and creative image generation is one important aspect for industrial application which could help as an arm for novel content generation.","Techniques proposed previously used Generative Adversarial Network(GAN), autoregressive models and Variational Autoencoders (VAE) for accomplishing similar tasks.","These approaches are limited in their capability to produce images guided by either text instructions or rough sketch images decreasing the overall performance of image generator.","We used state of the art diffusion models to generate creative art by primarily leveraging text with additional support of rough sketches.","Diffusion starts with a pattern of random dots and slowly converts that pattern into a design image using the guiding information fed into the model.","Diffusion models have recently outperformed other generative models in image generation tasks using cross modal data as guiding information.","The initial experiments for this task of novel image generation demonstrated promising qualitative results."],"url":"http://arxiv.org/abs/2307.04978v1"}
{"created":"2023-07-11 02:33:43","title":"Model-Driven Sensing-Node Selection and Power Allocation for Tracking Maneuvering Targets in Perceptive Mobile Networks","abstract":"Maneuvering target tracking will be an important service of future wireless networks to assist innovative applications such as intelligent transportation. However, tracking maneuvering targets by cellular networks faces many challenges. For example, the dense network and high-speed targets make the selection of the sensing nodes (SNs), e.g., base stations, and the associated power allocation very difficult, given the stringent latency requirement of sensing applications. Existing methods have demonstrated engaging tracking performance, but with very high computational complexity. In this paper, we propose a model-driven deep learning approach for SN selection to meet the latency requirement. To this end, we first propose an iterative SN selection method by jointly exploiting the majorization-minimization (MM) framework and the alternating direction method of multipliers (ADMM). Then, we unfold the iterative algorithm as a deep neural network (DNN) and prove its convergence. The proposed model-driven method has a low computational complexity, because the number of layers is less than the number of iterations required by the original algorithm, and each layer only involves simple matrix-vector additions/multiplications. Finally, we propose an efficient power allocation method based on fixed point (FP) water filling (WF) and solve the joint SN selection and power allocation problem under the alternative optimization framework. Simulation results show that the proposed method achieves better performance than the conventional optimization-based methods with much lower computational complexity.","sentences":["Maneuvering target tracking will be an important service of future wireless networks to assist innovative applications such as intelligent transportation.","However, tracking maneuvering targets by cellular networks faces many challenges.","For example, the dense network and high-speed targets make the selection of the sensing nodes (SNs), e.g., base stations, and the associated power allocation very difficult, given the stringent latency requirement of sensing applications.","Existing methods have demonstrated engaging tracking performance, but with very high computational complexity.","In this paper, we propose a model-driven deep learning approach for SN selection to meet the latency requirement.","To this end, we first propose an iterative SN selection method by jointly exploiting the majorization-minimization (MM) framework and the alternating direction method of multipliers (ADMM).","Then, we unfold the iterative algorithm as a deep neural network (DNN) and prove its convergence.","The proposed model-driven method has a low computational complexity, because the number of layers is less than the number of iterations required by the original algorithm, and each layer only involves simple matrix-vector additions/multiplications.","Finally, we propose an efficient power allocation method based on fixed point (FP) water filling (WF) and solve the joint SN selection and power allocation problem under the alternative optimization framework.","Simulation results show that the proposed method achieves better performance than the conventional optimization-based methods with much lower computational complexity."],"url":"http://arxiv.org/abs/2307.04977v1"}
{"created":"2023-07-11 02:27:45","title":"SAM-U: Multi-box prompts triggered uncertainty estimation for reliable SAM in medical image","abstract":"Recently, Segmenting Anything has taken an important step towards general artificial intelligence. At the same time, its reliability and fairness have also attracted great attention, especially in the field of health care. In this study, we propose multi-box prompts triggered uncertainty estimation for SAM cues to demonstrate the reliability of segmented lesions or tissues. We estimate the distribution of SAM predictions via Monte Carlo with prior distribution parameters, which employs different prompts as formulation of test-time augmentation. Our experimental results found that multi-box prompts augmentation improve the SAM performance, and endowed each pixel with uncertainty. This provides the first paradigm for a reliable SAM.","sentences":["Recently, Segmenting Anything has taken an important step towards general artificial intelligence.","At the same time, its reliability and fairness have also attracted great attention, especially in the field of health care.","In this study, we propose multi-box prompts triggered uncertainty estimation for SAM cues to demonstrate the reliability of segmented lesions or tissues.","We estimate the distribution of SAM predictions via Monte Carlo with prior distribution parameters, which employs different prompts as formulation of test-time augmentation.","Our experimental results found that multi-box prompts augmentation improve the SAM performance, and endowed each pixel with uncertainty.","This provides the first paradigm for a reliable SAM."],"url":"http://arxiv.org/abs/2307.04973v1"}
{"created":"2023-07-11 01:55:24","title":"Secrets of RLHF in Large Language Models Part I: PPO","abstract":"Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes","sentences":["Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence.","Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant.","Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.","Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities.","However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs.","The stable training of RLHF has still been a puzzle.","In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training.","We identify policy constraints being the key factor for the effective implementation of the PPO algorithm.","Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model.","Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.","The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment.","Therefore, we are eager to release technical reports, reward models and PPO codes"],"url":"http://arxiv.org/abs/2307.04964v1"}
{"created":"2023-07-11 01:53:19","title":"DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization","abstract":"DL compiler's primary function is to translate DNN programs written in high-level DL frameworks such as PyTorch and TensorFlow into portable executables. These executables can then be flexibly executed by the deployed host programs. However, existing DL compilers rely on a tracing mechanism, which involves feeding a runtime input to a neural network program and tracing the program execution paths to generate the computational graph necessary for compilation. Unfortunately, this mechanism falls short when dealing with modern dynamic neural networks (DyNNs) that possess varying computational graphs depending on the inputs. Consequently, conventional DL compilers struggle to accurately compile DyNNs into executable code. To address this limitation, we propose \\tool, a general approach that enables any existing DL compiler to successfully compile DyNNs. \\tool tackles the dynamic nature of DyNNs by introducing a compilation mechanism that redistributes the control and data flow of the original DNN programs during the compilation process. Specifically, \\tool develops program analysis and program transformation techniques to convert a dynamic neural network into multiple sub-neural networks. Each sub-neural network is devoid of conditional statements and is compiled independently. Furthermore, \\tool synthesizes a host module that models the control flow of the DyNNs and facilitates the invocation of the sub-neural networks. Our evaluation demonstrates the effectiveness of \\tool, achieving a 100\\% success rate in compiling all dynamic neural networks. Moreover, the compiled executables generated by \\tool exhibit significantly improved performance, running between $1.12\\times$ and $20.21\\times$ faster than the original DyNNs executed on general-purpose DL frameworks.","sentences":["DL compiler's primary function is to translate DNN programs written in high-level DL frameworks such as PyTorch and TensorFlow into portable executables.","These executables can then be flexibly executed by the deployed host programs.","However, existing DL compilers rely on a tracing mechanism, which involves feeding a runtime input to a neural network program and tracing the program execution paths to generate the computational graph necessary for compilation.","Unfortunately, this mechanism falls short when dealing with modern dynamic neural networks (DyNNs) that possess varying computational graphs depending on the inputs.","Consequently, conventional DL compilers struggle to accurately compile DyNNs into executable code.","To address this limitation, we propose \\tool, a general approach that enables any existing DL compiler to successfully compile DyNNs.","\\tool tackles the dynamic nature of DyNNs by introducing a compilation mechanism that redistributes the control and data flow of the original DNN programs during the compilation process.","Specifically, \\tool develops program analysis and program transformation techniques to convert a dynamic neural network into multiple sub-neural networks.","Each sub-neural network is devoid of conditional statements and is compiled independently.","Furthermore, \\tool synthesizes a host module that models the control flow of the DyNNs and facilitates the invocation of the sub-neural networks.","Our evaluation demonstrates the effectiveness of \\tool, achieving a 100\\% success rate in compiling all dynamic neural networks.","Moreover, the compiled executables generated by \\tool exhibit significantly improved performance, running between $1.12\\times$ and $20.21\\times$ faster than the original DyNNs executed on general-purpose DL frameworks."],"url":"http://arxiv.org/abs/2307.04963v1"}
{"created":"2023-07-11 01:52:08","title":"Intrinsically motivated graph exploration using network theories of human curiosity","abstract":"Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bear particular relevance for recommender systems. We demonstrate that curiosity-based recommendations are more predictive of human behavior than PageRank centrality for several real-world graph datasets, including MovieLens, Amazon Books, and Wikispeedia.","sentences":["Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards.","When the environment is naturally represented as a graph, how to guide exploration best remains an open question.","In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory.","The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment.","We use these proposed features as rewards for graph neural-network-based reinforcement learning.","On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training.","Our method computes more efficiently than the greedy evaluation of the relevant topological properties.","The proposed intrinsic motivations bear particular relevance for recommender systems.","We demonstrate that curiosity-based recommendations are more predictive of human behavior than PageRank centrality for several real-world graph datasets, including MovieLens, Amazon Books, and Wikispeedia."],"url":"http://arxiv.org/abs/2307.04962v1"}
{"created":"2023-07-11 01:51:43","title":"Still Waters Run Deep: Extend THz Coverage with Non-Intelligent Reflecting Surface","abstract":"Large reflection and diffraction losses in the Terahertz (THz) band give rise to degraded coverage abilities in non-line-of-sight (NLoS) areas. To overcome this, a non-intelligent reflecting surface (NIRS) can be used, which is essentially a rough surface made by metal materials. NIRS is not only able to enhance received power in large NLoS areas through rich reflections and scattering, but also costless and super-easy to fabricate and implement. In this article, we first thoroughly compare NIRS with the lively discussed intelligent reflecting surface (IRS) and point out the unique advantages of NIRS over IRS. Furthermore, experimental results are elaborated to show the effectiveness of NIRS in improving coverage. Last but not least, open problems and future directions are highlighted to inspire future research efforts on NIRS.","sentences":["Large reflection and diffraction losses in the Terahertz (THz) band give rise to degraded coverage abilities in non-line-of-sight (NLoS) areas.","To overcome this, a non-intelligent reflecting surface (NIRS) can be used, which is essentially a rough surface made by metal materials.","NIRS is not only able to enhance received power in large NLoS areas through rich reflections and scattering, but also costless and super-easy to fabricate and implement.","In this article, we first thoroughly compare NIRS with the lively discussed intelligent reflecting surface (IRS) and point out the unique advantages of NIRS over IRS.","Furthermore, experimental results are elaborated to show the effectiveness of NIRS in improving coverage.","Last but not least, open problems and future directions are highlighted to inspire future research efforts on NIRS."],"url":"http://arxiv.org/abs/2307.04961v1"}
{"created":"2023-07-11 01:42:10","title":"Simple Reference Immutability for System F-sub","abstract":"Reference immutability is a type based technique for taming mutation that has long been studied in the context of object-oriented languages, like Java. Recently, though, languages like Scala have blurred the lines between functional programming languages and object oriented programming languages. We explore how reference immutability interacts with features commonly found in these hybrid languages, in particular with higher-order functions -- polymorphism -- and subtyping. We construct a calculus System F-sub-M which encodes a reference immutability system as a simple extension of F-sub and prove that it satisfies the standard soundness and immutability safety properties.","sentences":["Reference immutability is a type based technique for taming mutation that has long been studied in the context of object-oriented languages, like Java.","Recently, though, languages like Scala have blurred the lines between functional programming languages and object oriented programming languages.","We explore how reference immutability interacts with features commonly found in these hybrid languages, in particular with higher-order functions -- polymorphism -- and subtyping.","We construct a calculus System F-sub-M which encodes a reference immutability system as a simple extension of F-sub and prove that it satisfies the standard soundness and immutability safety properties."],"url":"http://arxiv.org/abs/2307.04960v1"}
{"created":"2023-07-11 01:20:09","title":"Reinforcement Learning with Non-Cumulative Objective","abstract":"In reinforcement learning, the objective is almost always defined as a \\emph{cumulative} function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision process under which the globally optimal convergence of the generalized Bellman updates can be guaranteed. We demonstrate the idea experimentally with the bottleneck objective, i.e., the objectives determined by the minimum reward along the process, on classical optimal control and reinforcement learning tasks, as well as on two network routing problems on maximizing the flow rates.","sentences":["In reinforcement learning, the objective is almost always defined as a \\emph{cumulative} function over the rewards along the process.","However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards.","In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives.","Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation.","To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective.","Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision process under which the globally optimal convergence of the generalized Bellman updates can be guaranteed.","We demonstrate the idea experimentally with the bottleneck objective, i.e., the objectives determined by the minimum reward along the process, on classical optimal control and reinforcement learning tasks, as well as on two network routing problems on maximizing the flow rates."],"url":"http://arxiv.org/abs/2307.04957v1"}
{"created":"2023-07-11 01:17:00","title":"PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation","abstract":"Visual anomaly detection is essential and commonly used for many tasks in the field of computer vision. Recent anomaly detection datasets mainly focus on industrial automated inspection, medical image analysis and video surveillance. In order to broaden the application and research of anomaly detection in unmanned supermarkets and smart manufacturing, we introduce the supermarket goods anomaly detection (GoodsAD) dataset. It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories. Each category contains several common different types of anomalies such as deformation, surface damage and opened. Anomalies contain both texture changes and structural changes. It follows the unsupervised setting and only normal (defect-free) images are used for training. Pixel-precise ground truth regions are provided for all anomalies. Moreover, we also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods. This initial benchmark indicates that some methods which perform well on the industrial anomaly detection dataset (e.g., MVTec AD), show poor performance on our dataset. This is a comprehensive, multi-object dataset for supermarket goods anomaly detection that focuses on real-world applications.","sentences":["Visual anomaly detection is essential and commonly used for many tasks in the field of computer vision.","Recent anomaly detection datasets mainly focus on industrial automated inspection, medical image analysis and video surveillance.","In order to broaden the application and research of anomaly detection in unmanned supermarkets and smart manufacturing, we introduce the supermarket goods anomaly detection (GoodsAD) dataset.","It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories.","Each category contains several common different types of anomalies such as deformation, surface damage and opened.","Anomalies contain both texture changes and structural changes.","It follows the unsupervised setting and only normal (defect-free) images are used for training.","Pixel-precise ground truth regions are provided for all anomalies.","Moreover, we also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods.","This initial benchmark indicates that some methods which perform well on the industrial anomaly detection dataset (e.g., MVTec AD), show poor performance on our dataset.","This is a comprehensive, multi-object dataset for supermarket goods anomaly detection that focuses on real-world applications."],"url":"http://arxiv.org/abs/2307.04956v1"}
{"created":"2023-07-11 00:56:44","title":"Hybrid hidden Markov LSTM for short-term traffic flow prediction","abstract":"Deep learning (DL) methods have outperformed parametric models such as historical average, ARIMA and variants in predicting traffic variables into short and near-short future, that are critical for traffic management. Specifically, recurrent neural network (RNN) and its variants (e.g. long short-term memory) are designed to retain long-term temporal correlations and therefore are suitable for modeling sequences. However, multi-regime models assume the traffic system to evolve through multiple states (say, free-flow, congestion in traffic) with distinct characteristics, and hence, separate models are trained to characterize the traffic dynamics within each regime. For instance, Markov-switching models with a hidden Markov model (HMM) for regime identification is capable of capturing complex dynamic patterns and non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an observation sequence from a set of latent or, hidden state variables. In LSTM, the latent variable is computed in a deterministic manner from the current observation and the previous latent variable, while, in HMM, the set of latent variables is a Markov chain. Inspired by research in natural language processing, a hybrid hidden Markov-LSTM model that is capable of learning complementary features in traffic data is proposed for traffic flow prediction. Results indicate significant performance gains in using hybrid architecture compared to conventional methods such as Markov switching ARIMA and LSTM.","sentences":["Deep learning (DL) methods have outperformed parametric models such as historical average, ARIMA and variants in predicting traffic variables into short and near-short future, that are critical for traffic management.","Specifically, recurrent neural network (RNN) and its variants (e.g. long short-term memory) are designed to retain long-term temporal correlations and therefore are suitable for modeling sequences.","However, multi-regime models assume the traffic system to evolve through multiple states (say, free-flow, congestion in traffic) with distinct characteristics, and hence, separate models are trained to characterize the traffic dynamics within each regime.","For instance, Markov-switching models with a hidden Markov model (HMM) for regime identification is capable of capturing complex dynamic patterns and non-stationarity.","Interestingly, both HMM and LSTM can be used for modeling an observation sequence from a set of latent or, hidden state variables.","In LSTM, the latent variable is computed in a deterministic manner from the current observation and the previous latent variable, while, in HMM, the set of latent variables is a Markov chain.","Inspired by research in natural language processing, a hybrid hidden Markov-LSTM model that is capable of learning complementary features in traffic data is proposed for traffic flow prediction.","Results indicate significant performance gains in using hybrid architecture compared to conventional methods such as Markov switching ARIMA and LSTM."],"url":"http://arxiv.org/abs/2307.04954v1"}
{"created":"2023-07-11 00:46:59","title":"Compact Twice Fusion Network for Edge Detection","abstract":"The significance of multi-scale features has been gradually recognized by the edge detection community. However, the fusion of multi-scale features increases the complexity of the model, which is not friendly to practical application. In this work, we propose a Compact Twice Fusion Network (CTFN) to fully integrate multi-scale features while maintaining the compactness of the model. CTFN includes two lightweight multi-scale feature fusion modules: a Semantic Enhancement Module (SEM) that can utilize the semantic information contained in coarse-scale features to guide the learning of fine-scale features, and a Pseudo Pixel-level Weighting (PPW) module that aggregate the complementary merits of multi-scale features by assigning weights to all features. Notwithstanding all this, the interference of texture noise makes the correct classification of some pixels still a challenge. For these hard samples, we propose a novel loss function, coined Dynamic Focal Loss, which reshapes the standard cross-entropy loss and dynamically adjusts the weights to correct the distribution of hard samples. We evaluate our method on three datasets, i.e., BSDS500, NYUDv2, and BIPEDv2. Compared with state-of-the-art methods, CTFN achieves competitive accuracy with less parameters and computational cost. Apart from the backbone, CTFN requires only 0.1M additional parameters, which reduces its computation cost to just 60% of other state-of-the-art methods. The codes are available at https://github.com/Li-yachuan/CTFN-pytorch-master.","sentences":["The significance of multi-scale features has been gradually recognized by the edge detection community.","However, the fusion of multi-scale features increases the complexity of the model, which is not friendly to practical application.","In this work, we propose a Compact Twice Fusion Network (CTFN) to fully integrate multi-scale features while maintaining the compactness of the model.","CTFN includes two lightweight multi-scale feature fusion modules: a Semantic Enhancement Module (SEM) that can utilize the semantic information contained in coarse-scale features to guide the learning of fine-scale features, and a Pseudo Pixel-level Weighting (PPW) module that aggregate the complementary merits of multi-scale features by assigning weights to all features.","Notwithstanding all this, the interference of texture noise makes the correct classification of some pixels still a challenge.","For these hard samples, we propose a novel loss function, coined Dynamic Focal Loss, which reshapes the standard cross-entropy loss and dynamically adjusts the weights to correct the distribution of hard samples.","We evaluate our method on three datasets, i.e., BSDS500, NYUDv2, and BIPEDv2.","Compared with state-of-the-art methods, CTFN achieves competitive accuracy with less parameters and computational cost.","Apart from the backbone, CTFN requires only 0.1M additional parameters, which reduces its computation cost to just 60% of other state-of-the-art methods.","The codes are available at https://github.com/Li-yachuan/CTFN-pytorch-master."],"url":"http://arxiv.org/abs/2307.04952v1"}
{"created":"2023-07-11 00:21:38","title":"DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization","abstract":"Inverse problems generally require a regularizer or prior for a good solution. A recent trend is to train a convolutional net to denoise images, and use this net as a prior when solving the inverse problem. Several proposals depend on a singular value decomposition of the forward operator, and several others backpropagate through the denoising net at runtime. Here we propose a simpler approach that combines the traditional gradient-based minimization of reconstruction error with denoising. Noise is also added at each step, so the iterative dynamics resembles a Langevin or diffusion process. Both the level of added noise and the size of the denoising step decay exponentially with time. We apply our method to the problem of tomographic reconstruction from electron micrographs acquired at multiple tilt angles. With empirical studies using simulated tilt views, we find parameter settings for our method that produce good results. We show that high accuracy can be achieved with as few as 50 denoising steps. We also compare with DDRM and DPS, more complex diffusion methods of the kinds mentioned above. These methods are less accurate (as measured by MSE and SSIM) for our tomography problem, even after the generation hyperparameters are optimized. Finally we extend our method to reconstruction of arbitrary-sized images and show results on 128 $\\times$ 1568 pixel images","sentences":["Inverse problems generally require a regularizer or prior for a good solution.","A recent trend is to train a convolutional net to denoise images, and use this net as a prior when solving the inverse problem.","Several proposals depend on a singular value decomposition of the forward operator, and several others backpropagate through the denoising net at runtime.","Here we propose a simpler approach that combines the traditional gradient-based minimization of reconstruction error with denoising.","Noise is also added at each step, so the iterative dynamics resembles a Langevin or diffusion process.","Both the level of added noise and the size of the denoising step decay exponentially with time.","We apply our method to the problem of tomographic reconstruction from electron micrographs acquired at multiple tilt angles.","With empirical studies using simulated tilt views, we find parameter settings for our method that produce good results.","We show that high accuracy can be achieved with as few as 50 denoising steps.","We also compare with DDRM and DPS, more complex diffusion methods of the kinds mentioned above.","These methods are less accurate (as measured by MSE and SSIM) for our tomography problem, even after the generation hyperparameters are optimized.","Finally we extend our method to reconstruction of arbitrary-sized images and show results on 128 $\\times$ 1568 pixel images"],"url":"http://arxiv.org/abs/2307.04946v1"}
{"created":"2023-07-11 00:21:21","title":"What do LLMs need to Synthesize Correct Router Configurations?","abstract":"We investigate whether Large Language Models (e.g., GPT-4) can synthesize correct router configurations with reduced manual effort. We find GPT-4 works very badly by itself, producing promising draft configurations but with egregious errors in topology, syntax, and semantics. Our strategy, that we call Verified Prompt Programming, is to combine GPT-4 with verifiers, and use localized feedback from the verifier to automatically correct errors. Verification requires a specification and actionable localized feedback to be effective. We show results for two use cases: translating from Cisco to Juniper configurations on a single router, and implementing no-transit policy on multiple routers. While human input is still required, if we define the leverage as the number of automated prompts to the number of human prompts, our experiments show a leverage of 10X for Juniper translation, and 6X for implementing no-transit policy, ending with verified configurations.","sentences":["We investigate whether Large Language Models (e.g., GPT-4) can synthesize correct router configurations with reduced manual effort.","We find GPT-4 works very badly by itself, producing promising draft configurations but with egregious errors in topology, syntax, and semantics.","Our strategy, that we call Verified Prompt Programming, is to combine GPT-4 with verifiers, and use localized feedback from the verifier to automatically correct errors.","Verification requires a specification and actionable localized feedback to be effective.","We show results for two use cases: translating from Cisco to Juniper configurations on a single router, and implementing no-transit policy on multiple routers.","While human input is still required, if we define the leverage as the number of automated prompts to the number of human prompts, our experiments show a leverage of 10X for Juniper translation, and 6X for implementing no-transit policy, ending with verified configurations."],"url":"http://arxiv.org/abs/2307.04945v1"}
{"created":"2023-07-11 00:14:45","title":"Benchmarking Algorithms for Federated Domain Generalization","abstract":"While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges. Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario. Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity. To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty. We then apply our methodology to evaluate 13 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG. Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client heterogeneity, or more realistic datasets. Please check our extendable benchmark code here: https://github.com/inouye-lab/FedDG_Benchmark.","sentences":["While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges.","Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario.","Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity.","To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty.","We then apply our methodology to evaluate 13 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG.","Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client heterogeneity, or more realistic datasets.","Please check our extendable benchmark code here: https://github.com/inouye-lab/FedDG_Benchmark."],"url":"http://arxiv.org/abs/2307.04942v1"}
{"created":"2023-07-11 00:03:28","title":"MG3MConv: Multi-Grained Matrix-Multiplication-Mapping Convolution Algorithm toward the SW26010 Processor","abstract":"As the core of artificial intelligence applications, the research of convolution has become a hot topic in high performance computing. With the rapid development of the emerging SW26010 processor in artificial intelligence, there is an urgent need for high-performance convolution algorithms on the processor. However, the current support of convolution on SW26010 is still rudimentary. The only studies provide sufficient runtime peak performance but lack the adaptability to various convolution scenes. To perfect convolution algorithms on SW26010, we propose a multi-grained matrix-multiplication-mapping convolution algorithm called MG3MConv, which targets the architectural features of SW26010. MG3MConv supports diversified mapping schemes of convolution tasks based on the concept of the thread block proposed in this paper. All the architecture-oriented optimization methods are elaborately designed from four levels to fully exploit the hardware efficiency of SW26010. The experiments show that the hardware efficiency of MG3MConv can reach 84.78% in max, which is 1.75 times compared with that of cuDNN based on NVIDIA K80m GPU. Moreover, MG3MConv can overperform cuDNN in most convolution scenes. We also use six representative CNNs as real-world cases, and the hardware efficiency of MG3MConv reaches up to 67.04% on the VGG network model, which is 1.37 times and 1.96 times that of cuDNN and swDNN, respectively.","sentences":["As the core of artificial intelligence applications, the research of convolution has become a hot topic in high performance computing.","With the rapid development of the emerging SW26010 processor in artificial intelligence, there is an urgent need for high-performance convolution algorithms on the processor.","However, the current support of convolution on SW26010 is still rudimentary.","The only studies provide sufficient runtime peak performance but lack the adaptability to various convolution scenes.","To perfect convolution algorithms on SW26010, we propose a multi-grained matrix-multiplication-mapping convolution algorithm called MG3MConv, which targets the architectural features of SW26010. MG3MConv supports diversified mapping schemes of convolution tasks based on the concept of the thread block proposed in this paper.","All the architecture-oriented optimization methods are elaborately designed from four levels to fully exploit the hardware efficiency of SW26010.","The experiments show that the hardware efficiency of MG3MConv can reach 84.78% in max, which is 1.75 times compared with that of cuDNN based on NVIDIA K80m GPU.","Moreover, MG3MConv can overperform cuDNN in most convolution scenes.","We also use six representative CNNs as real-world cases, and the hardware efficiency of MG3MConv reaches up to 67.04% on the VGG network model, which is 1.37 times and 1.96 times that of cuDNN and swDNN, respectively."],"url":"http://arxiv.org/abs/2307.04941v1"}
{"created":"2023-07-10 23:28:03","title":"Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective","abstract":"Graph neural networks have shown great ability in representation (GNNs) learning on graphs, facilitating various tasks. Despite their great performance in modeling graphs, recent works show that GNNs tend to inherit and amplify the bias from training data, causing concerns of the adoption of GNNs in high-stake scenarios. Hence, many efforts have been taken for fairness-aware GNNs. However, most existing fair GNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies. Motivated by causal theory, there are several attempts utilizing graph counterfactual fairness to mitigate root causes of unfairness. However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation. In this paper, we take a causal view on fair graph learning problem. Guided by the casual analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid non-realistic counterfactuals and adopt selected counterfactuals to learn fair node representations for node classification task. Extensive experiments on synthetic and real-world datasets show the effectiveness of CAF.","sentences":["Graph neural networks have shown great ability in representation (GNNs) learning on graphs, facilitating various tasks.","Despite their great performance in modeling graphs, recent works show that GNNs tend to inherit and amplify the bias from training data, causing concerns of the adoption of GNNs in high-stake scenarios.","Hence, many efforts have been taken for fairness-aware GNNs.","However, most existing fair GNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies.","Motivated by causal theory, there are several attempts utilizing graph counterfactual fairness to mitigate root causes of unfairness.","However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation.","In this paper, we take a causal view on fair graph learning problem.","Guided by the casual analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid non-realistic counterfactuals and adopt selected counterfactuals to learn fair node representations for node classification task.","Extensive experiments on synthetic and real-world datasets show the effectiveness of CAF."],"url":"http://arxiv.org/abs/2307.04937v1"}
{"created":"2023-07-10 22:28:33","title":"Probabilistic Counterexample Guidance for Safer Reinforcement Learning","abstract":"Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the risk of safety violations during the subsequent online exploration. We demonstrate our method's effectiveness in reducing safety violations during online exploration in preliminary experiments by an average of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with previous related work, while achieving comparable cumulative rewards with respect to unrestricted exploration and alternative approaches.","sentences":["Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs.","Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states.","However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging.","In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement.","Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration.","We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the risk of safety violations during the subsequent online exploration.","We demonstrate our method's effectiveness in reducing safety violations during online exploration in preliminary experiments by an average of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with previous related work, while achieving comparable cumulative rewards with respect to unrestricted exploration and alternative approaches."],"url":"http://arxiv.org/abs/2307.04927v1"}
{"created":"2023-07-10 22:19:36","title":"Model-checking parametric lock-sharing systems against regular constraints","abstract":"In parametric lock-sharing systems processes can spawn new processes to run in parallel, and can create new locks. The behavior of every process is given by a pushdown automaton. We consider infinite behaviors of such systems under strong process fairness condition. A result of a potentially infinite execution of a system is a limit configuration, that is a potentially infinite tree. The verification problem is to determine if a given system has a limit configuration satisfying a given regular property. This formulation of the problem encompasses verification of reachability as well as of many liveness properties. We show that this verification problem, while undecidable in general, is decidable for nested lock usage. We show Exptime-completeness of the verification problem. The main source of complexity is the number of parameters in the spawn operation. If the number of parameters is bounded, our algorithm works in Ptime for properties expressed by parity automata with a fixed number of ranks.","sentences":["In parametric lock-sharing systems processes can spawn new processes to run in parallel, and can create new locks.","The behavior of every process is given by a pushdown automaton.","We consider infinite behaviors of such systems under strong process fairness condition.","A result of a potentially infinite execution of a system is a limit configuration, that is a potentially infinite tree.","The verification problem is to determine if a given system has a limit configuration satisfying a given regular property.","This formulation of the problem encompasses verification of reachability as well as of many liveness properties.","We show that this verification problem, while undecidable in general, is decidable for nested lock usage.","We show Exptime-completeness of the verification problem.","The main source of complexity is the number of parameters in the spawn operation.","If the number of parameters is bounded, our algorithm works in Ptime for properties expressed by parity automata with a fixed number of ranks."],"url":"http://arxiv.org/abs/2307.04925v1"}
{"created":"2023-07-10 22:14:56","title":"Ranking with Long-Term Constraints","abstract":"The feedback that users provide through their choices (e.g., clicks, purchases) is one of the most common types of data readily available for training search and recommendation algorithms. However, myopically training systems based on choice data may only improve short-term engagement, but not the long-term sustainability of the platform and the long-term benefits to its users, content providers, and other stakeholders. In this paper, we thus develop a new framework in which decision makers (e.g., platform operators, regulators, users) can express long-term goals for the behavior of the platform (e.g., fairness, revenue distribution, legal requirements). These goals take the form of exposure or impact targets that go well beyond individual sessions, and we provide new control-based algorithms to achieve these goals. In particular, the controllers are designed to achieve the stated long-term goals with minimum impact on short-term engagement. Beyond the principled theoretical derivation of the controllers, we evaluate the algorithms on both synthetic and real-world data. While all controllers perform well, we find that they provide interesting trade-offs in efficiency, robustness, and the ability to plan ahead.","sentences":["The feedback that users provide through their choices (e.g., clicks, purchases) is one of the most common types of data readily available for training search and recommendation algorithms.","However, myopically training systems based on choice data may only improve short-term engagement, but not the long-term sustainability of the platform and the long-term benefits to its users, content providers, and other stakeholders.","In this paper, we thus develop a new framework in which decision makers (e.g., platform operators, regulators, users) can express long-term goals for the behavior of the platform (e.g., fairness, revenue distribution, legal requirements).","These goals take the form of exposure or impact targets that go well beyond individual sessions, and we provide new control-based algorithms to achieve these goals.","In particular, the controllers are designed to achieve the stated long-term goals with minimum impact on short-term engagement.","Beyond the principled theoretical derivation of the controllers, we evaluate the algorithms on both synthetic and real-world data.","While all controllers perform well, we find that they provide interesting trade-offs in efficiency, robustness, and the ability to plan ahead."],"url":"http://arxiv.org/abs/2307.04923v1"}
{"created":"2023-07-10 22:00:24","title":"Enhanced Food Availability can Deteriorate Fitness through Excessive Scrounging","abstract":"In group foraging situations, the conventional expectation is that increased food availability would enhance consumption, especially when animals prioritize maximizing their food intake. This paper challenges this conventional wisdom by conducting an in-depth game-theoretic analysis of a basic producer-scrounger model, in which animals must choose between intensive food searching as producers or moderate searching while relying on group members as scroungers. Surprisingly, our study reveals that, under certain circumstances, increasing food availability can amplify the inclination to scrounge to such an extent that it paradoxically leads to a reduction in animals' food consumption compared to scenarios with limited food availability. We further illustrate a similar phenomenon in a model capturing free-riding dynamics among workers in a company. We demonstrate that, under certain reward mechanisms, enhancing workers' production capacities can inadvertently trigger a surge in free-riding behavior, leading to both diminished group productivity and reduced individual payoffs. Our findings underscore the significance of contextual factors when comprehending and predicting the impact of resource availability on individual and collective outcomes.","sentences":["In group foraging situations, the conventional expectation is that increased food availability would enhance consumption, especially when animals prioritize maximizing their food intake.","This paper challenges this conventional wisdom by conducting an in-depth game-theoretic analysis of a basic producer-scrounger model, in which animals must choose between intensive food searching as producers or moderate searching while relying on group members as scroungers.","Surprisingly, our study reveals that, under certain circumstances, increasing food availability can amplify the inclination to scrounge to such an extent that it paradoxically leads to a reduction in animals' food consumption compared to scenarios with limited food availability.","We further illustrate a similar phenomenon in a model capturing free-riding dynamics among workers in a company.","We demonstrate that, under certain reward mechanisms, enhancing workers' production capacities can inadvertently trigger a surge in free-riding behavior, leading to both diminished group productivity and reduced individual payoffs.","Our findings underscore the significance of contextual factors when comprehending and predicting the impact of resource availability on individual and collective outcomes."],"url":"http://arxiv.org/abs/2307.04920v1"}
{"created":"2023-07-10 21:51:06","title":"Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators","abstract":"We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS). The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing. The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances. The compliance of the impedance controller makes the robot safer for interactions with humans and the environment. We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object.","sentences":["We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS).","The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing.","The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances.","The compliance of the impedance controller makes the robot safer for interactions with humans and the environment.","We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm.","The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object."],"url":"http://arxiv.org/abs/2307.04918v1"}
{"created":"2023-07-10 21:49:42","title":"Unlimited Sampling of Bandpass Signals: Computational Demodulation via Undersampling","abstract":"Bandpass signals are an important sub-class of bandlimited signals that naturally arise in a number of application areas but their high-frequency content poses an acquisition challenge. Consequently, \"Bandpass Sampling Theory\" has been investigated and applied in the literature. In this paper, we consider the problem of modulo sampling of bandpass signals with the main goal of sampling and recovery of high dynamic range inputs. Our work is inspired by the Unlimited Sensing Framework (USF). In the USF, the modulo operation folds high dynamic range inputs into low dynamic range, modulo samples. This fundamentally avoids signal clipping. Given that the output of the modulo nonlinearity is non-bandlimited, bandpass sampling conditions never hold true. Yet, we show that bandpass signals can be recovered from a modulo representation despite the inevitable aliasing. Our main contribution includes proof of sampling theorems for recovery of bandpass signals from an undersampled representation, reaching sub-Nyquist sampling rates. On the recovery front, by considering both time-and frequency-domain perspectives, we provide a holistic view of the modulo bandpass sampling problem. On the hardware front, we include ideal, non-ideal and generalized modulo folding architectures that arise in the hardware implementation of modulo analog-to-digital converters. Numerical simulations corroborate our theoretical results. Bridging the theory-practice gap, we validate our results using hardware experiments, thus demonstrating the practical effectiveness of our methods.","sentences":["Bandpass signals are an important sub-class of bandlimited signals that naturally arise in a number of application areas but their high-frequency content poses an acquisition challenge.","Consequently, \"Bandpass Sampling Theory\" has been investigated and applied in the literature.","In this paper, we consider the problem of modulo sampling of bandpass signals with the main goal of sampling and recovery of high dynamic range inputs.","Our work is inspired by the Unlimited Sensing Framework (USF).","In the USF, the modulo operation folds high dynamic range inputs into low dynamic range, modulo samples.","This fundamentally avoids signal clipping.","Given that the output of the modulo nonlinearity is non-bandlimited, bandpass sampling conditions never hold true.","Yet, we show that bandpass signals can be recovered from a modulo representation despite the inevitable aliasing.","Our main contribution includes proof of sampling theorems for recovery of bandpass signals from an undersampled representation, reaching sub-Nyquist sampling rates.","On the recovery front, by considering both time-and frequency-domain perspectives, we provide a holistic view of the modulo bandpass sampling problem.","On the hardware front, we include ideal, non-ideal and generalized modulo folding architectures that arise in the hardware implementation of modulo analog-to-digital converters.","Numerical simulations corroborate our theoretical results.","Bridging the theory-practice gap, we validate our results using hardware experiments, thus demonstrating the practical effectiveness of our methods."],"url":"http://arxiv.org/abs/2307.04917v1"}
{"created":"2023-07-10 21:49:30","title":"Rapid Deforestation and Burned Area Detection using Deep Multimodal Learning on Satellite Imagery","abstract":"Deforestation estimation and fire detection in the Amazon forest poses a significant challenge due to the vast size of the area and the limited accessibility. However, these are crucial problems that lead to severe environmental consequences, including climate change, global warming, and biodiversity loss. To effectively address this problem, multimodal satellite imagery and remote sensing offer a promising solution for estimating deforestation and detecting wildfire in the Amazonia region. This research paper introduces a new curated dataset and a deep learning-based approach to solve these problems using convolutional neural networks (CNNs) and comprehensive data processing techniques. Our dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites. We design the dataset considering different spatial and temporal resolution requirements. Our method successfully achieves high-precision deforestation estimation and burned area detection on unseen images from the region. Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation","sentences":["Deforestation estimation and fire detection in the Amazon forest poses a significant challenge due to the vast size of the area and the limited accessibility.","However, these are crucial problems that lead to severe environmental consequences, including climate change, global warming, and biodiversity loss.","To effectively address this problem, multimodal satellite imagery and remote sensing offer a promising solution for estimating deforestation and detecting wildfire in the Amazonia region.","This research paper introduces a new curated dataset and a deep learning-based approach to solve these problems using convolutional neural networks (CNNs) and comprehensive data processing techniques.","Our dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.","We design the dataset considering different spatial and temporal resolution requirements.","Our method successfully achieves high-precision deforestation estimation and burned area detection on unseen images from the region.","Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation"],"url":"http://arxiv.org/abs/2307.04916v1"}
{"created":"2023-07-10 21:43:12","title":"Coded Distributed Image Classification","abstract":"In this paper, we present a coded computation (CC) scheme for distributed computation of the inference phase of machine learning (ML) tasks, specifically, the task of image classification. Building upon Agrawal et al.~2022, the proposed scheme combines the strengths of deep learning and Lagrange interpolation technique to mitigate the effect of straggling workers, and recovers approximate results with reasonable accuracy using outputs from any $R$ out of $N$ workers, where $R\\leq N$. Our proposed scheme guarantees a minimum recovery threshold $R$ for non-polynomial problems, which can be adjusted as a tunable parameter in the system. Moreover, unlike existing schemes, our scheme maintains flexibility with respect to worker availability and system design. We propose two system designs for our CC scheme that allows flexibility in distributing the computational load between the master and the workers based on the accessibility of input data. Our experimental results demonstrate the superiority of our scheme compared to the state-of-the-art CC schemes for image classification tasks, and pave the path for designing new schemes for distributed computation of any general ML classification tasks.","sentences":["In this paper, we present a coded computation (CC) scheme for distributed computation of the inference phase of machine learning (ML) tasks, specifically, the task of image classification.","Building upon Agrawal et al.~2022, the proposed scheme combines the strengths of deep learning and Lagrange interpolation technique to mitigate the effect of straggling workers, and recovers approximate results with reasonable accuracy using outputs from any $R$ out of $N$ workers, where $R\\leq N$. Our proposed scheme guarantees a minimum recovery threshold $R$ for non-polynomial problems, which can be adjusted as a tunable parameter in the system.","Moreover, unlike existing schemes, our scheme maintains flexibility with respect to worker availability and system design.","We propose two system designs for our CC scheme that allows flexibility in distributing the computational load between the master and the workers based on the accessibility of input data.","Our experimental results demonstrate the superiority of our scheme compared to the state-of-the-art CC schemes for image classification tasks, and pave the path for designing new schemes for distributed computation of any general ML classification tasks."],"url":"http://arxiv.org/abs/2307.04915v1"}
{"created":"2023-07-10 21:28:26","title":"Self-Diagnosis and Large Language Models: A New Front for Medical Misinformation","abstract":"Improving healthcare quality and access remains a critical concern for countries worldwide. Consequently, the rise of large language models (LLMs) has erupted a wealth of discussion around healthcare applications among researchers and consumers alike. While the ability of these models to pass medical exams has been used to argue in favour of their use in medical training and diagnosis, the impact of their inevitable use as a self-diagnostic tool and their role in spreading healthcare misinformation has not been evaluated. In this work, we critically evaluate LLMs' capabilities from the lens of a general user self-diagnosing, as well as the means through which LLMs may aid in the spread of medical misinformation. To accomplish this, we develop a testing methodology which can be used to evaluate responses to open-ended questions mimicking real-world use cases. In doing so, we reveal that a) these models perform worse than previously known, and b) they exhibit peculiar behaviours, including overconfidence when stating incorrect recommendations, which increases the risk of spreading medical misinformation.","sentences":["Improving healthcare quality and access remains a critical concern for countries worldwide.","Consequently, the rise of large language models (LLMs) has erupted a wealth of discussion around healthcare applications among researchers and consumers alike.","While the ability of these models to pass medical exams has been used to argue in favour of their use in medical training and diagnosis, the impact of their inevitable use as a self-diagnostic tool and their role in spreading healthcare misinformation has not been evaluated.","In this work, we critically evaluate LLMs' capabilities from the lens of a general user self-diagnosing, as well as the means through which LLMs may aid in the spread of medical misinformation.","To accomplish this, we develop a testing methodology which can be used to evaluate responses to open-ended questions mimicking real-world use cases.","In doing so, we reveal that a) these models perform worse than previously known, and b) they exhibit peculiar behaviours, including overconfidence when stating incorrect recommendations, which increases the risk of spreading medical misinformation."],"url":"http://arxiv.org/abs/2307.04910v1"}
{"created":"2023-07-10 21:26:43","title":"Planar Curve Registration using Bayesian Inversion","abstract":"We study parameterisation-independent closed planar curve matching as a Bayesian inverse problem. The motion of the curve is modelled via a curve on the diffeomorphism group acting on the ambient space, leading to a large deformation diffeomorphic metric mapping (LDDMM) functional penalising the kinetic energy of the deformation. We solve Hamilton's equations for the curve matching problem using the Wu-Xu element [S. Wu, J. Xu, Nonconforming finite element spaces for $2m^\\text{th}$ order partial differential equations on $\\mathbb{R}^n$ simplicial grids when $m=n+1$, Mathematics of Computation 88 (316) (2019) 531-551] which provides mesh-independent Lipschitz constants for the forward motion of the curve, and solve the inverse problem for the momentum using Bayesian inversion. Since this element is not affine-equivalent we provide a pullback theory which expedites the implementation and efficiency of the forward map. We adopt ensemble Kalman inversion using a negative Sobolev norm mismatch penalty to measure the discrepancy between the target and the ensemble mean shape. We provide several numerical examples to validate the approach.","sentences":["We study parameterisation-independent closed planar curve matching as a Bayesian inverse problem.","The motion of the curve is modelled via a curve on the diffeomorphism group acting on the ambient space, leading to a large deformation diffeomorphic metric mapping (LDDMM) functional penalising the kinetic energy of the deformation.","We solve Hamilton's equations for the curve matching problem using the Wu-Xu element [S. Wu, J. Xu, Nonconforming finite element spaces for $2m^\\text{th}$ order partial differential equations on $\\mathbb{R}^n$ simplicial grids when $m=n+1$, Mathematics of Computation 88 (316) (2019) 531-551] which provides mesh-independent Lipschitz constants for the forward motion of the curve, and solve the inverse problem for the momentum using Bayesian inversion.","Since this element is not affine-equivalent we provide a pullback theory which expedites the implementation and efficiency of the forward map.","We adopt ensemble Kalman inversion using a negative Sobolev norm mismatch penalty to measure the discrepancy between the target and the ensemble mean shape.","We provide several numerical examples to validate the approach."],"url":"http://arxiv.org/abs/2307.04909v1"}
{"created":"2023-07-10 21:16:46","title":"SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation","abstract":"SimpleMTOD is a simple language model which recasts several sub-tasks in multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is built on a large-scale transformer-based auto-regressive architecture, which has already proven to be successful in uni-modal task-oriented dialogues, and effectively leverages transfer learning from pre-trained GPT-2. In-order to capture the semantics of visual scenes, we introduce both local and de-localized tokens for objects within a scene. De-localized tokens represent the type of an object rather than the specific object itself and so possess a consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0 test-std dataset while performing on par in other multimodal sub-tasks: Disambiguation, Coreference Resolution, and Dialog State Tracking. This is despite taking a minimalist approach for extracting visual (and non-visual) information. In addition the model does not rely on task-specific architectural changes such as classification heads.","sentences":["SimpleMTOD is a simple language model which recasts several sub-tasks in multimodal task-oriented dialogues as sequence prediction tasks.","SimpleMTOD is built on a large-scale transformer-based auto-regressive architecture, which has already proven to be successful in uni-modal task-oriented dialogues, and effectively leverages transfer learning from pre-trained GPT-2.","In-order to capture the semantics of visual scenes, we introduce both local and de-localized tokens for objects within a scene.","De-localized tokens represent the type of an object rather than the specific object itself and so possess a consistent meaning across the dataset.","SimpleMTOD achieves a state-of-the-art BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0 test-std dataset while performing on par in other multimodal sub-tasks: Disambiguation, Coreference Resolution, and Dialog State Tracking.","This is despite taking a minimalist approach for extracting visual (and non-visual) information.","In addition the model does not rely on task-specific architectural changes such as classification heads."],"url":"http://arxiv.org/abs/2307.04907v1"}
{"created":"2023-07-10 21:08:52","title":"FedYolo: Augmenting Federated Learning with Pretrained Transformers","abstract":"The growth and diversity of machine learning applications motivate a rethinking of learning with mobile and edge devices. How can we address diverse client goals and learn with scarce heterogeneous data? While federated learning aims to address these issues, it has challenges hindering a unified solution. Large transformer models have been shown to work across a variety of tasks achieving remarkable few-shot adaptation. This raises the question: Can clients use a single general-purpose model, rather than custom models for each task, while obeying device and network constraints? In this work, we investigate pretrained transformers (PTF) to achieve these on-device learning goals and thoroughly explore the roles of model size and modularity, where the latter refers to adaptation through modules such as prompts or adapters. Focusing on federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy gaps between alternative approaches and improves heterogeneity robustness. Scale allows clients to run more local SGD epochs which can significantly reduce the number of communication rounds. At the extreme, clients can achieve respectable accuracy locally highlighting the potential of fully-local learning. (2) Modularity, by design, enables $>$100$\\times$ less communication in bits. Surprisingly, it also boosts the generalization capability of local adaptation methods and the robustness of smaller PTFs. Finally, it enables clients to solve multiple unrelated tasks simultaneously using a single PTF, whereas full updates are prone to catastrophic forgetting. These insights on scale and modularity motivate a new federated learning approach we call \"You Only Load Once\" (FedYolo): The clients load a full PTF model once and all future updates are accomplished through communication-efficient modules with limited catastrophic-forgetting, where each task is assigned to its own module.","sentences":["The growth and diversity of machine learning applications motivate a rethinking of learning with mobile and edge devices.","How can we address diverse client goals and learn with scarce heterogeneous data?","While federated learning aims to address these issues, it has challenges hindering a unified solution.","Large transformer models have been shown to work across a variety of tasks achieving remarkable few-shot adaptation.","This raises the question: Can clients use a single general-purpose model, rather than custom models for each task, while obeying device and network constraints?","In this work, we investigate pretrained transformers (PTF) to achieve these on-device learning goals and thoroughly explore the roles of model size and modularity, where the latter refers to adaptation through modules such as prompts or adapters.","Focusing on federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy gaps between alternative approaches and improves heterogeneity robustness.","Scale allows clients to run more local SGD epochs which can significantly reduce the number of communication rounds.","At the extreme, clients can achieve respectable accuracy locally highlighting the potential of fully-local learning.","(2) Modularity, by design, enables $>$100$\\times$ less communication in bits.","Surprisingly, it also boosts the generalization capability of local adaptation methods and the robustness of smaller PTFs.","Finally, it enables clients to solve multiple unrelated tasks simultaneously using a single PTF, whereas full updates are prone to catastrophic forgetting.","These insights on scale and modularity motivate a new federated learning approach we call \"You Only Load Once\" (FedYolo): The clients load a full PTF model once and all future updates are accomplished through communication-efficient modules with limited catastrophic-forgetting, where each task is assigned to its own module."],"url":"http://arxiv.org/abs/2307.04905v1"}
{"created":"2023-07-10 21:07:30","title":"An evolutionary game with environmental feedback and players' opinions","abstract":"Evolutionary games are a developing sub-field of game theory. This branch of game theory is used in the study of the adaptation of large, but finite, populations of agents to changes in the environment. It assumes that each agent has no significant influence on the system. Many scientific areas use the theory of evolutionary games. In particular, it is used in biology, medicine and the modelling of wireless networks. In this paper we study an evolutionary game with two levels of interaction between population agents. At the first level, changes in the population state depend on changes in the environment and on increasing or decreasing the resources available to the agents. At the second level, the populations state changes according to how the agents evaluate the state of the environment. These levels form a hierarchical structure. A change in one parameter of the system, which is responsible for the state of the environment, the population or the opinions of the agents, causes a change in the other elements of the system. The study involves the analysis of a modified evolutionary game taking into account the influence of the environment and the opinions of the agents. It also involves the development of computational methods in MATLAB and two sets of numerical experiments.","sentences":["Evolutionary games are a developing sub-field of game theory.","This branch of game theory is used in the study of the adaptation of large, but finite, populations of agents to changes in the environment.","It assumes that each agent has no significant influence on the system.","Many scientific areas use the theory of evolutionary games.","In particular, it is used in biology, medicine and the modelling of wireless networks.","In this paper we study an evolutionary game with two levels of interaction between population agents.","At the first level, changes in the population state depend on changes in the environment and on increasing or decreasing the resources available to the agents.","At the second level, the populations state changes according to how the agents evaluate the state of the environment.","These levels form a hierarchical structure.","A change in one parameter of the system, which is responsible for the state of the environment, the population or the opinions of the agents, causes a change in the other elements of the system.","The study involves the analysis of a modified evolutionary game taking into account the influence of the environment and the opinions of the agents.","It also involves the development of computational methods in MATLAB and two sets of numerical experiments."],"url":"http://arxiv.org/abs/2307.04902v1"}
