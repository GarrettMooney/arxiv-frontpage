{"created":"2023-12-05 18:59:58","title":"ReconFusion: 3D Reconstruction with Diffusion Priors","abstract":"3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches.","sentences":["3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes.","However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process.","We present ReconFusion to reconstruct real-world scenes using only a few photos.","Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images.","Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions.","We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches."],"url":"http://arxiv.org/abs/2312.02981v1"}
{"created":"2023-12-05 18:59:55","title":"GPT4Point: A Unified Framework for Point-Language Understanding and Generation","abstract":"Multimodal Large Language Models (MLLMs) have excelled in 2D image-text comprehension and image generation, but their understanding of the 3D world is notably deficient, limiting progress in 3D language understanding and generation. To solve this problem, we introduce GPT4Point, an innovative groundbreaking point-language multimodal model designed specifically for unified 3D object understanding and generation within the MLLM framework. GPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text reference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point is equipped with advanced capabilities for controllable 3D generation, it can get high-quality results through a low-quality point-text feature maintaining the geometric shapes and colors. To support the expansive needs of 3D object-text pairs, we develop Pyramid-XL, a point-language dataset annotation engine. It constructs a large-scale database over 1M objects of varied text granularity levels from the Objaverse-XL dataset, essential for training GPT4Point. A comprehensive benchmark has been proposed to evaluate 3D point-language understanding capabilities. In extensive evaluations, GPT4Point has demonstrated superior performance in understanding and generation.","sentences":["Multimodal Large Language Models (MLLMs) have excelled in 2D image-text comprehension and image generation, but their understanding of the 3D world is notably deficient, limiting progress in 3D language understanding and generation.","To solve this problem, we introduce GPT4Point, an innovative groundbreaking point-language multimodal model designed specifically for unified 3D object understanding and generation within the MLLM framework.","GPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text reference tasks such as point-cloud captioning and Q&A.","Additionally, GPT4Point is equipped with advanced capabilities for controllable 3D generation, it can get high-quality results through a low-quality point-text feature maintaining the geometric shapes and colors.","To support the expansive needs of 3D object-text pairs, we develop Pyramid-XL, a point-language dataset annotation engine.","It constructs a large-scale database over 1M objects of varied text granularity levels from the Objaverse-XL dataset, essential for training GPT4Point.","A comprehensive benchmark has been proposed to evaluate 3D point-language understanding capabilities.","In extensive evaluations, GPT4Point has demonstrated superior performance in understanding and generation."],"url":"http://arxiv.org/abs/2312.02980v1"}
{"created":"2023-12-05 18:59:45","title":"Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World","abstract":"Reinforcement learning (RL) with dense rewards and imitation learning (IL) with human-generated trajectories are the most widely used approaches for training modern embodied agents. RL requires extensive reward shaping and auxiliary losses and is often too slow and ineffective for long-horizon tasks. While IL with human supervision is effective, collecting human trajectories at scale is extremely expensive. In this work, we show that imitating shortest-path planners in simulation produces agents that, given a language instruction, can proficiently navigate, explore, and manipulate objects in both simulation and in the real world using only RGB sensors (no depth map or GPS coordinates). This surprising result is enabled by our end-to-end, transformer-based, SPOC architecture, powerful visual encoders paired with extensive image augmentation, and the dramatic scale and diversity of our training data: millions of frames of shortest-path-expert trajectories collected inside approximately 200,000 procedurally generated houses containing 40,000 unique 3D assets. Our models, data, training code, and newly proposed 10-task benchmarking suite CHORES will be open-sourced.","sentences":["Reinforcement learning (RL) with dense rewards and imitation learning (IL) with human-generated trajectories are the most widely used approaches for training modern embodied agents.","RL requires extensive reward shaping and auxiliary losses and is often too slow and ineffective for long-horizon tasks.","While IL with human supervision is effective, collecting human trajectories at scale is extremely expensive.","In this work, we show that imitating shortest-path planners in simulation produces agents that, given a language instruction, can proficiently navigate, explore, and manipulate objects in both simulation and in the real world using only RGB sensors (no depth map or GPS coordinates).","This surprising result is enabled by our end-to-end, transformer-based, SPOC architecture, powerful visual encoders paired with extensive image augmentation, and the dramatic scale and diversity of our training data: millions of frames of shortest-path-expert trajectories collected inside approximately 200,000 procedurally generated houses containing 40,000 unique 3D assets.","Our models, data, training code, and newly proposed 10-task benchmarking suite CHORES will be open-sourced."],"url":"http://arxiv.org/abs/2312.02976v1"}
{"created":"2023-12-05 18:59:23","title":"Dexterous Functional Grasping","abstract":"While there have been significant strides in dexterous manipulation, most of it is limited to benchmark tasks like in-hand reorientation which are of limited utility in the real world. The main benefit of dexterous hands over two-fingered ones is their ability to pickup tools and other objects (including thin ones) and grasp them firmly to apply force. However, this task requires both a complex understanding of functional affordances as well as precise low-level control. While prior work obtains affordances from human data this approach doesn't scale to low-level control. Similarly, simulation training cannot give the robot an understanding of real-world semantics. In this paper, we aim to combine the best of both worlds to accomplish functional grasping for in-the-wild objects. We use a modular approach. First, affordances are obtained by matching corresponding regions of different objects and then a low-level policy trained in sim is run to grasp it. We propose a novel application of eigengrasps to reduce the search space of RL using a small amount of human data and find that it leads to more stable and physically realistic motion. We find that eigengrasp action space beats baselines in simulation and outperforms hardcoded grasping in real and matches or outperforms a trained human teleoperator. Results visualizations and videos at https://dexfunc.github.io/","sentences":["While there have been significant strides in dexterous manipulation, most of it is limited to benchmark tasks like in-hand reorientation which are of limited utility in the real world.","The main benefit of dexterous hands over two-fingered ones is their ability to pickup tools and other objects (including thin ones) and grasp them firmly to apply force.","However, this task requires both a complex understanding of functional affordances as well as precise low-level control.","While prior work obtains affordances from human data this approach doesn't scale to low-level control.","Similarly, simulation training cannot give the robot an understanding of real-world semantics.","In this paper, we aim to combine the best of both worlds to accomplish functional grasping for in-the-wild objects.","We use a modular approach.","First, affordances are obtained by matching corresponding regions of different objects and then a low-level policy trained in sim is run to grasp it.","We propose a novel application of eigengrasps to reduce the search space of RL using a small amount of human data and find that it leads to more stable and physically realistic motion.","We find that eigengrasp action space beats baselines in simulation and outperforms hardcoded grasping in real and matches or outperforms a trained human teleoperator.","Results visualizations and videos at https://dexfunc.github.io/"],"url":"http://arxiv.org/abs/2312.02975v1"}
{"created":"2023-12-05 18:59:16","title":"Describing Differences in Image Sets with Natural Language","abstract":"How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two $\\textbf{sets}$ of images, which we term Set Difference Captioning. This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.","sentences":["How do two sets of images differ?","Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical.","To aid in this discovery process, we explore the task of automatically describing the differences between two $\\textbf{sets}$ of images, which we term Set Difference Captioning.","This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets.","We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP.","To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions.","We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable.","Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights."],"url":"http://arxiv.org/abs/2312.02974v1"}
{"created":"2023-12-05 18:59:14","title":"GauHuman: Articulated Gaussian Splatting from Monocular Human Videos","abstract":"We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians.","sentences":["We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame.","Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost.","Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up.","Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed.","Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians."],"url":"http://arxiv.org/abs/2312.02973v1"}
{"created":"2023-12-05 18:58:26","title":"Alchemist: Parametric Control of Material Properties with Diffusion Models","abstract":"We propose a method to control material attributes of objects like roughness, metallic, albedo, and transparency in real images. Our method capitalizes on the generative prior of text-to-image models known for photorealism, employing a scalar value and instructions to alter low-level material properties. Addressing the lack of datasets with controlled material attributes, we generated an object-centric synthetic dataset with physically-based materials. Fine-tuning a modified pre-trained text-to-image model on this synthetic dataset enables us to edit material properties in real-world images while preserving all other attributes. We show the potential application of our model to material edited NeRFs.","sentences":["We propose a method to control material attributes of objects like roughness, metallic, albedo, and transparency in real images.","Our method capitalizes on the generative prior of text-to-image models known for photorealism, employing a scalar value and instructions to alter low-level material properties.","Addressing the lack of datasets with controlled material attributes, we generated an object-centric synthetic dataset with physically-based materials.","Fine-tuning a modified pre-trained text-to-image model on this synthetic dataset enables us to edit material properties in real-world images while preserving all other attributes.","We show the potential application of our model to material edited NeRFs."],"url":"http://arxiv.org/abs/2312.02970v1"}
{"created":"2023-12-05 18:57:40","title":"Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models","abstract":"Listwise rerankers based on large language models (LLM) are the zero-shot state-of-the-art. However, current works in this direction all depend on the GPT models, making it a single point of failure in scientific reproducibility. Moreover, it raises the concern that the current research findings only hold for GPT models but not LLM in general. In this work, we lift this pre-condition and build for the first time effective listwise rerankers without any form of dependency on GPT. Our passage retrieval experiments show that our best list se reranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves 97% effectiveness of the ones built on GPT-4. Our results also show that the existing training datasets, which were expressly constructed for pointwise ranking, are insufficient for building such listwise rerankers. Instead, high-quality listwise ranking data is required and crucial, calling for further work on building human-annotated listwise data resources.","sentences":["Listwise rerankers based on large language models (LLM) are the zero-shot state-of-the-art.","However, current works in this direction all depend on the GPT models, making it a single point of failure in scientific reproducibility.","Moreover, it raises the concern that the current research findings only hold for GPT models but not LLM in general.","In this work, we lift this pre-condition and build for the first time effective listwise rerankers without any form of dependency on GPT.","Our passage retrieval experiments show that our best list se reranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves 97% effectiveness of the ones built on GPT-4.","Our results also show that the existing training datasets, which were expressly constructed for pointwise ranking, are insufficient for building such listwise rerankers.","Instead, high-quality listwise ranking data is required and crucial, calling for further work on building human-annotated listwise data resources."],"url":"http://arxiv.org/abs/2312.02969v1"}
{"created":"2023-12-05 18:56:06","title":"AmbiGen: Generating Ambigrams from Pre-trained Diffusion Model","abstract":"Ambigrams are calligraphic designs that have different meanings depending on the viewing orientation. Creating ambigrams is a challenging task even for skilled artists, as it requires maintaining the meaning under two different viewpoints at the same time. In this work, we propose to generate ambigrams by distilling a large-scale vision and language diffusion model, namely DeepFloyd IF, to optimize the letters' outline for legibility in the two viewing orientations. Empirically, we demonstrate that our approach outperforms existing ambigram generation methods. On the 500 most common words in English, our method achieves more than an 11.6% increase in word accuracy and at least a 41.9% reduction in edit distance.","sentences":["Ambigrams are calligraphic designs that have different meanings depending on the viewing orientation.","Creating ambigrams is a challenging task even for skilled artists, as it requires maintaining the meaning under two different viewpoints at the same time.","In this work, we propose to generate ambigrams by distilling a large-scale vision and language diffusion model, namely DeepFloyd IF, to optimize the letters' outline for legibility in the two viewing orientations.","Empirically, we demonstrate that our approach outperforms existing ambigram generation methods.","On the 500 most common words in English, our method achieves more than an 11.6% increase in word accuracy and at least a 41.9% reduction in edit distance."],"url":"http://arxiv.org/abs/2312.02967v1"}
{"created":"2023-12-05 18:54:03","title":"Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection","abstract":"Semi-supervised object detection is crucial for 3D scene understanding, efficiently addressing the limitation of acquiring large-scale 3D bounding box annotations. Existing methods typically employ a teacher-student framework with pseudo-labeling to leverage unlabeled point clouds. However, producing reliable pseudo-labels in a diverse 3D space still remains challenging. In this work, we propose Diffusion-SS3D, a new perspective of enhancing the quality of pseudo-labels via the diffusion model for semi-supervised 3D object detection. Specifically, we include noises to produce corrupted 3D object size and class label distributions, and then utilize the diffusion model as a denoising process to obtain bounding box outputs. Moreover, we integrate the diffusion model into the teacher-student framework, so that the denoised bounding boxes can be used to improve pseudo-label generation, as well as the entire semi-supervised learning process. We conduct experiments on the ScanNet and SUN RGB-D benchmark datasets to demonstrate that our approach achieves state-of-the-art performance against existing methods. We also present extensive analysis to understand how our diffusion model design affects performance in semi-supervised learning.","sentences":["Semi-supervised object detection is crucial for 3D scene understanding, efficiently addressing the limitation of acquiring large-scale 3D bounding box annotations.","Existing methods typically employ a teacher-student framework with pseudo-labeling to leverage unlabeled point clouds.","However, producing reliable pseudo-labels in a diverse 3D space still remains challenging.","In this work, we propose Diffusion-SS3D, a new perspective of enhancing the quality of pseudo-labels via the diffusion model for semi-supervised 3D object detection.","Specifically, we include noises to produce corrupted 3D object size and class label distributions, and then utilize the diffusion model as a denoising process to obtain bounding box outputs.","Moreover, we integrate the diffusion model into the teacher-student framework, so that the denoised bounding boxes can be used to improve pseudo-label generation, as well as the entire semi-supervised learning process.","We conduct experiments on the ScanNet and SUN RGB-D benchmark datasets to demonstrate that our approach achieves state-of-the-art performance against existing methods.","We also present extensive analysis to understand how our diffusion model design affects performance in semi-supervised learning."],"url":"http://arxiv.org/abs/2312.02966v1"}
{"created":"2023-12-05 18:50:12","title":"MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures","abstract":"In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while remarkable progress has been made with models trained on large-scale synthetic and real-captured object data like Objaverse and MVImgNet, a similar level of progress has not been observed in the domain of human-centric tasks partially due to the lack of a large-scale human dataset. Existing datasets of high-fidelity 3D human capture continue to be mid-sized due to the significant challenges in acquiring large-scale high-quality 3D human data. To bridge this gap, we present MVHumanNet, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using a multi-view human capture system, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot studies on view-consistent action recognition, human NeRF reconstruction, text-driven view-unconstrained human image generation, as well as 2D view-unconstrained human image and 3D avatar generation. Extensive experiments demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet data with annotations will foster further innovations in the domain of 3D human-centric tasks at scale.","sentences":["In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets.","However, in the realm of 3D vision, while remarkable progress has been made with models trained on large-scale synthetic and real-captured object data like Objaverse and MVImgNet, a similar level of progress has not been observed in the domain of human-centric tasks partially due to the lack of a large-scale human dataset.","Existing datasets of high-fidelity 3D human capture continue to be mid-sized due to the significant challenges in acquiring large-scale high-quality 3D human data.","To bridge this gap, we present MVHumanNet, a dataset that comprises multi-view human action sequences of 4,500 human identities.","The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using a multi-view human capture system, which facilitates easily scalable data collection.","Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions.","To explore the potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot studies on view-consistent action recognition, human NeRF reconstruction, text-driven view-unconstrained human image generation, as well as 2D view-unconstrained human image and 3D avatar generation.","Extensive experiments demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet.","As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet data with annotations will foster further innovations in the domain of 3D human-centric tasks at scale."],"url":"http://arxiv.org/abs/2312.02963v1"}
{"created":"2023-12-05 18:49:54","title":"Predicting Horizontal Gene Transfers with Perfect Transfer Networks","abstract":"Horizontal gene transfer inference approaches are usually based on gene sequences: parametric methods search for patterns that deviate from a particular genomic signature, while phylogenetic methods use sequences to reconstruct the gene and species trees. However, it is well-known that sequences have difficulty identifying ancient transfers since mutations have enough time to erase all evidence of such events. In this work, we ask whether character-based methods can predict gene transfers. Their advantage over sequences is that homologous genes can have low DNA similarity, but still have retained enough important common motifs that allow them to have common character traits, for instance the same functional or expression profile. A phylogeny that has two separate clades that acquired the same character independently might indicate the presence of a transfer even in the absence of sequence similarity. We introduce perfect transfer networks, which are phylogenetic networks that can explain the character diversity of a set of taxa under the assumption that characters have unique births, and that once a character is gained it is rarely lost. Examples of such traits include transposable elements, biochemical markers and emergence of organelles, just to name a few. We study the differences between our model and two similar models: perfect phylogenetic networks and ancestral recombination networks. Our goals are to initiate a study on the structural and algorithmic properties of perfect transfer networks. We then show that in polynomial time, one can decide whether a given network is a valid explanation for a set of taxa, and show how, for a given tree, one can add transfer edges to it so that it explains a set of taxa. We finally provide lower and upper bounds on the number of transfers required to explain a set of taxa, in the worst case.","sentences":["Horizontal gene transfer inference approaches are usually based on gene sequences: parametric methods search for patterns that deviate from a particular genomic signature, while phylogenetic methods use sequences to reconstruct the gene and species trees.","However, it is well-known that sequences have difficulty identifying ancient transfers since mutations have enough time to erase all evidence of such events.","In this work, we ask whether character-based methods can predict gene transfers.","Their advantage over sequences is that homologous genes can have low DNA similarity, but still have retained enough important common motifs that allow them to have common character traits, for instance the same functional or expression profile.","A phylogeny that has two separate clades that acquired the same character independently might indicate the presence of a transfer even in the absence of sequence similarity.","We introduce perfect transfer networks, which are phylogenetic networks that can explain the character diversity of a set of taxa under the assumption that characters have unique births, and that once a character is gained it is rarely lost.","Examples of such traits include transposable elements, biochemical markers and emergence of organelles, just to name a few.","We study the differences between our model and two similar models: perfect phylogenetic networks and ancestral recombination networks.","Our goals are to initiate a study on the structural and algorithmic properties of perfect transfer networks.","We then show that in polynomial time, one can decide whether a given network is a valid explanation for a set of taxa, and show how, for a given tree, one can add transfer edges to it so that it explains a set of taxa.","We finally provide lower and upper bounds on the number of transfers required to explain a set of taxa, in the worst case."],"url":"http://arxiv.org/abs/2312.02962v1"}
{"created":"2023-12-05 18:41:03","title":"Classification for everyone : Building geography agnostic models for fairer recognition","abstract":"In this paper, we analyze different methods to mitigate inherent geographical biases present in state of the art image classification models. We first quantitatively present this bias in two datasets - The Dollar Street Dataset and ImageNet, using images with location information. We then present different methods which can be employed to reduce this bias. Finally, we analyze the effectiveness of the different techniques on making these models more robust to geographical locations of the images.","sentences":["In this paper, we analyze different methods to mitigate inherent geographical biases present in state of the art image classification models.","We first quantitatively present this bias in two datasets - The Dollar Street Dataset and ImageNet, using images with location information.","We then present different methods which can be employed to reduce this bias.","Finally, we analyze the effectiveness of the different techniques on making these models more robust to geographical locations of the images."],"url":"http://arxiv.org/abs/2312.02957v1"}
{"created":"2023-12-05 18:40:39","title":"Switch Points of Bi-Persistence Matching Distance","abstract":"In multi-parameter persistence, the matching distance is defined as the supremum of weighted bottleneck distances on the barcodes given by the restriction of persistence modules to lines with a positive slope. In the case of finitely presented bi-persistence modules, all the available methods to compute the matching distance are based on restricting the computation to lines through pairs from a finite set of points in the plane. Some of these points are determined by the filtration data as they are entrance values of critical simplices. However, these critical values alone are not sufficient for the matching distance computation and it is necessary to add so-called switch points, i.e. points such that on a line through any of them, the bottleneck matching switches the matched pair.   This paper is devoted to the algorithmic computation of the set of switch points given a set of critical values. We find conditions under which a candidate switch point is erroneous or superfluous. The obtained conditions are turned into algorithms that have been implemented. With this, we analyze how the size of the set of switch points increases as the number of critical values increases, and how it varies depending on the distribution of critical values. Experiments are carried out on various types of bi-persistence modules.","sentences":["In multi-parameter persistence, the matching distance is defined as the supremum of weighted bottleneck distances on the barcodes given by the restriction of persistence modules to lines with a positive slope.","In the case of finitely presented bi-persistence modules, all the available methods to compute the matching distance are based on restricting the computation to lines through pairs from a finite set of points in the plane.","Some of these points are determined by the filtration data as they are entrance values of critical simplices.","However, these critical values alone are not sufficient for the matching distance computation and it is necessary to add so-called switch points, i.e. points such that on a line through any of them, the bottleneck matching switches the matched pair.   ","This paper is devoted to the algorithmic computation of the set of switch points given a set of critical values.","We find conditions under which a candidate switch point is erroneous or superfluous.","The obtained conditions are turned into algorithms that have been implemented.","With this, we analyze how the size of the set of switch points increases as the number of critical values increases, and how it varies depending on the distribution of critical values.","Experiments are carried out on various types of bi-persistence modules."],"url":"http://arxiv.org/abs/2312.02955v1"}
{"created":"2023-12-05 18:29:31","title":"LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models","abstract":"With the recent significant advancements in large multi-modal models (LMMs), the importance of their grounding capability in visual chat is increasingly recognized. Despite recent efforts to enable LMMs to support grounding, their capabilities for grounding and chat are usually separate, and their chat performance drops dramatically when asked to ground. The problem is the lack of a dataset for grounded visual chat (GVC). Existing grounding datasets only contain short captions. To address this issue, we have created GVC data that allows for the combination of grounding and chat capabilities. To better evaluate the GVC capabilities, we have introduced a benchmark called Grounding-Bench. Additionally, we have proposed a model design that can support GVC and various types of visual prompts by connecting segmentation models with language models. Experimental results demonstrate that our model outperforms other LMMs on Grounding-Bench. Furthermore, our model achieves competitive performance on classic grounding benchmarks like RefCOCO/+/g and Flickr30K Entities. Our code will be released at https://github.com/UX-Decoder/LLaVA-Grounding .","sentences":["With the recent significant advancements in large multi-modal models (LMMs), the importance of their grounding capability in visual chat is increasingly recognized.","Despite recent efforts to enable LMMs to support grounding, their capabilities for grounding and chat are usually separate, and their chat performance drops dramatically when asked to ground.","The problem is the lack of a dataset for grounded visual chat (GVC).","Existing grounding datasets only contain short captions.","To address this issue, we have created GVC data that allows for the combination of grounding and chat capabilities.","To better evaluate the GVC capabilities, we have introduced a benchmark called Grounding-Bench.","Additionally, we have proposed a model design that can support GVC and various types of visual prompts by connecting segmentation models with language models.","Experimental results demonstrate that our model outperforms other LMMs on Grounding-Bench.","Furthermore, our model achieves competitive performance on classic grounding benchmarks like RefCOCO/+/g and Flickr30K Entities.","Our code will be released at https://github.com/UX-Decoder/LLaVA-Grounding ."],"url":"http://arxiv.org/abs/2312.02949v1"}
{"created":"2023-12-05 18:10:46","title":"An alternating peak-optimization method for optimal trajectory generation of quadrotor drones","abstract":"In this paper, we propose an alternating optimization method to address a time-optimal trajectory generation problem. Different from the existing solutions, our approach introduces a new formulation that minimizes the overall trajectory running time while maintaining the polynomial smoothness constraints and incorporating hard limits on motion derivatives to ensure feasibility. To address this problem, an alternating peak-optimization method is developed, which splits the optimization process into two sub-optimizations: the first sub-optimization optimizes polynomial coefficients for smoothness, and the second sub-optimization adjusts the time allocated to each trajectory segment. These are alternated until a feasible minimum-time solution is found. We offer a comprehensive set of simulations and experiments to showcase the superior performance of our approach in comparison to existing methods.   A collection of demonstration videos with real drone flying experiments can be accessed at https://www.youtube.com/playlist?list=PLQGtPFK17zUYkwFT-fr0a8E49R8Uq712l .","sentences":["In this paper, we propose an alternating optimization method to address a time-optimal trajectory generation problem.","Different from the existing solutions, our approach introduces a new formulation that minimizes the overall trajectory running time while maintaining the polynomial smoothness constraints and incorporating hard limits on motion derivatives to ensure feasibility.","To address this problem, an alternating peak-optimization method is developed, which splits the optimization process into two sub-optimizations: the first sub-optimization optimizes polynomial coefficients for smoothness, and the second sub-optimization adjusts the time allocated to each trajectory segment.","These are alternated until a feasible minimum-time solution is found.","We offer a comprehensive set of simulations and experiments to showcase the superior performance of our approach in comparison to existing methods.   ","A collection of demonstration videos with real drone flying experiments can be accessed at https://www.youtube.com/playlist?list=PLQGtPFK17zUYkwFT-fr0a8E49R8Uq712l ."],"url":"http://arxiv.org/abs/2312.02944v1"}
{"created":"2023-12-05 18:07:45","title":"Synergistic Perception and Control Simplex for Verifiable Safe Vertical Landing","abstract":"Perception, Planning, and Control form the essential components of autonomy in advanced air mobility. This work advances the holistic integration of these components to enhance the performance and robustness of the complete cyber-physical system. We adapt Perception Simplex, a system for verifiable collision avoidance amidst obstacle detection faults, to the vertical landing maneuver for autonomous air mobility vehicles. We improve upon this system by replacing static assumptions of control capabilities with dynamic confirmation, i.e., real-time confirmation of control limitations of the system, ensuring reliable fulfillment of safety maneuvers and overrides, without dependence on overly pessimistic assumptions. Parameters defining control system capabilities and limitations, e.g., maximum deceleration, are continuously tracked within the system and used to make safety-critical decisions. We apply these techniques to propose a verifiable collision avoidance solution for autonomous aerial mobility vehicles operating in cluttered and potentially unsafe environments.","sentences":["Perception, Planning, and Control form the essential components of autonomy in advanced air mobility.","This work advances the holistic integration of these components to enhance the performance and robustness of the complete cyber-physical system.","We adapt Perception Simplex, a system for verifiable collision avoidance amidst obstacle detection faults, to the vertical landing maneuver for autonomous air mobility vehicles.","We improve upon this system by replacing static assumptions of control capabilities with dynamic confirmation, i.e., real-time confirmation of control limitations of the system, ensuring reliable fulfillment of safety maneuvers and overrides, without dependence on overly pessimistic assumptions.","Parameters defining control system capabilities and limitations, e.g., maximum deceleration, are continuously tracked within the system and used to make safety-critical decisions.","We apply these techniques to propose a verifiable collision avoidance solution for autonomous aerial mobility vehicles operating in cluttered and potentially unsafe environments."],"url":"http://arxiv.org/abs/2312.02937v1"}
{"created":"2023-12-05 18:05:59","title":"Drag-A-Video: Non-rigid Video Editing with Point-based Interaction","abstract":"Video editing is a challenging task that requires manipulating videos on both the spatial and temporal dimensions. Existing methods for video editing mainly focus on changing the appearance or style of the objects in the video, while keeping their structures unchanged. However, there is no existing method that allows users to interactively ``drag'' any points of instances on the first frame to precisely reach the target points with other frames consistently deformed. In this paper, we propose a new diffusion-based method for interactive point-based video manipulation, called Drag-A-Video. Our method allows users to click pairs of handle points and target points as well as masks on the first frame of an input video. Then, our method transforms the inputs into point sets and propagates these sets across frames. To precisely modify the contents of the video, we employ a new video-level motion supervision to update the features of the video and introduce the latent offsets to achieve this update at multiple denoising timesteps. We propose a temporal-consistent point tracking module to coordinate the movement of the points in the handle point sets. We demonstrate the effectiveness and flexibility of our method on various videos. The website of our work is available here: https://drag-a-video.github.io/.","sentences":["Video editing is a challenging task that requires manipulating videos on both the spatial and temporal dimensions.","Existing methods for video editing mainly focus on changing the appearance or style of the objects in the video, while keeping their structures unchanged.","However, there is no existing method that allows users to interactively ``drag'' any points of instances on the first frame to precisely reach the target points with other frames consistently deformed.","In this paper, we propose a new diffusion-based method for interactive point-based video manipulation, called Drag-A-Video.","Our method allows users to click pairs of handle points and target points as well as masks on the first frame of an input video.","Then, our method transforms the inputs into point sets and propagates these sets across frames.","To precisely modify the contents of the video, we employ a new video-level motion supervision to update the features of the video and introduce the latent offsets to achieve this update at multiple denoising timesteps.","We propose a temporal-consistent point tracking module to coordinate the movement of the points in the handle point sets.","We demonstrate the effectiveness and flexibility of our method on various videos.","The website of our work is available here: https://drag-a-video.github.io/."],"url":"http://arxiv.org/abs/2312.02936v1"}
{"created":"2023-12-05 18:05:14","title":"WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation","abstract":"Generating multi-camera street-view videos is critical for augmenting autonomous driving datasets, addressing the urgent demand for extensive and varied data. Due to the limitations in diversity and challenges in handling lighting conditions, traditional rendering-based methods are increasingly being supplanted by diffusion-based methods. However, a significant challenge in diffusion-based methods is ensuring that the generated sensor data preserve both intra-world consistency and inter-sensor coherence. To address these challenges, we combine an additional explicit world volume and propose the World Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system is specifically designed to leverage 4D world volume as a foundational element for video generation. Our model operates in two distinct phases: (i) envisioning the future 4D temporal world volume based on vehicle control sequences, and (ii) generating multi-camera videos, informed by this envisioned 4D temporal world volume and sensor interconnectivity. The incorporation of the 4D world volume empowers WoVoGen not only to generate high-quality street-view videos in response to vehicle control inputs but also to facilitate scene editing tasks.","sentences":["Generating multi-camera street-view videos is critical for augmenting autonomous driving datasets, addressing the urgent demand for extensive and varied data.","Due to the limitations in diversity and challenges in handling lighting conditions, traditional rendering-based methods are increasingly being supplanted by diffusion-based methods.","However, a significant challenge in diffusion-based methods is ensuring that the generated sensor data preserve both intra-world consistency and inter-sensor coherence.","To address these challenges, we combine an additional explicit world volume and propose the World Volume-aware Multi-camera Driving Scene Generator (WoVoGen).","This system is specifically designed to leverage 4D world volume as a foundational element for video generation.","Our model operates in two distinct phases: (i) envisioning the future 4D temporal world volume based on vehicle control sequences, and (ii) generating multi-camera videos, informed by this envisioned 4D temporal world volume and sensor interconnectivity.","The incorporation of the 4D world volume empowers WoVoGen not only to generate high-quality street-view videos in response to vehicle control inputs but also to facilitate scene editing tasks."],"url":"http://arxiv.org/abs/2312.02934v1"}
{"created":"2023-12-05 18:03:13","title":"WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words","abstract":"Training on multiple modalities of input can augment the capabilities of a language model. Here, we ask whether such a training regime can improve the quality and efficiency of these systems as well. We focus on text--audio and introduce Whisbert, which is inspired by the text--image approach of FLAVA \\citep{singh_flava_2022}. In accordance with Babylm \\citep{warstadt2023papers} guidelines, we pretrain Whisbert on a dataset comprising only 100 million words plus their corresponding speech from the word-aligned version of the People's Speech dataset \\citep{galvez_peoples_2021}. To assess the impact of multimodality, we compare versions of the model that are trained on text only and on both audio and text simultaneously. We find that while Whisbert is able to perform well on multimodal masked modeling and surpasses the Babylm baselines in most benchmark tasks, it struggles to optimize its complex objective and outperform its text-only Whisbert baseline.","sentences":["Training on multiple modalities of input can augment the capabilities of a language model.","Here, we ask whether such a training regime can improve the quality and efficiency of these systems as well.","We focus on text--audio and introduce Whisbert, which is inspired by the text--image approach of FLAVA \\citep{singh_flava_2022}.","In accordance with Babylm \\citep{warstadt2023papers} guidelines, we pretrain Whisbert on a dataset comprising only 100 million words plus their corresponding speech from the word-aligned version of the People's Speech dataset \\citep{galvez_peoples_2021}.","To assess the impact of multimodality, we compare versions of the model that are trained on text only and on both audio and text simultaneously.","We find that while Whisbert is able to perform well on multimodal masked modeling and surpasses the Babylm baselines in most benchmark tasks, it struggles to optimize its complex objective and outperform its text-only Whisbert baseline."],"url":"http://arxiv.org/abs/2312.02931v1"}
{"created":"2023-12-05 17:59:52","title":"LivePhoto: Real Image Animation with Text-guided Motion Control","abstract":"Despite the recent progress in text-to-video generation, existing studies usually overlook the issue that only spatial contents but not temporal motions in synthesized videos are under the control of text. Towards such a challenge, this work presents a practical system, named LivePhoto, which allows users to animate an image of their interest with text descriptions. We first establish a strong baseline that helps a well-learned text-to-image generator (i.e., Stable Diffusion) take an image as a further input. We then equip the improved generator with a motion module for temporal modeling and propose a carefully designed training pipeline to better link texts and motions. In particular, considering the facts that (1) text can only describe motions roughly (e.g., regardless of the moving speed) and (2) text may include both content and motion descriptions, we introduce a motion intensity estimation module as well as a text re-weighting module to reduce the ambiguity of text-to-motion mapping. Empirical evidence suggests that our approach is capable of well decoding motion-related textual instructions into videos, such as actions, camera movements, or even conjuring new contents from thin air (e.g., pouring water into an empty glass). Interestingly, thanks to the proposed intensity learning mechanism, our system offers users an additional control signal (i.e., the motion intensity) besides text for video customization.","sentences":["Despite the recent progress in text-to-video generation, existing studies usually overlook the issue that only spatial contents but not temporal motions in synthesized videos are under the control of text.","Towards such a challenge, this work presents a practical system, named LivePhoto, which allows users to animate an image of their interest with text descriptions.","We first establish a strong baseline that helps a well-learned text-to-image generator (i.e., Stable Diffusion) take an image as a further input.","We then equip the improved generator with a motion module for temporal modeling and propose a carefully designed training pipeline to better link texts and motions.","In particular, considering the facts that (1) text can only describe motions roughly (e.g., regardless of the moving speed) and (2) text may include both content and motion descriptions, we introduce a motion intensity estimation module as well as a text re-weighting module to reduce the ambiguity of text-to-motion mapping.","Empirical evidence suggests that our approach is capable of well decoding motion-related textual instructions into videos, such as actions, camera movements, or even conjuring new contents from thin air (e.g., pouring water into an empty glass).","Interestingly, thanks to the proposed intensity learning mechanism, our system offers users an additional control signal (i.e., the motion intensity) besides text for video customization."],"url":"http://arxiv.org/abs/2312.02928v1"}
{"created":"2023-12-05 17:50:55","title":"Split & Merge: Unlocking the Potential of Visual Adapters via Sparse Training","abstract":"With the rapid growth in the scale of pre-trained foundation models, parameter-efficient fine-tuning techniques have gained significant attention, among which Adapter Tuning is the most widely used. Despite achieving efficiency, Adapter Tuning still underperforms full fine-tuning, and the performance improves at the cost of an increase in parameters. Recent efforts address this issue by pruning the original adapters, but it also introduces training instability and suboptimal performance on certain datasets. Motivated by this, we propose Mixture of Sparse Adapters, or MoSA, as a novel Adapter Tuning method to fully unleash the potential of each parameter in the adapter. We first split the standard adapter into multiple non-overlapping modules, then stochastically activate modules for sparse training, and finally merge them to form a complete adapter after tuning. In this way, MoSA can achieve significantly better performance than standard adapters without any additional computational or storage overhead. Furthermore, we propose a hierarchical sparse strategy to better leverage limited training data. Extensive experiments on a series of 27 visual tasks demonstrate that MoSA consistently outperforms other Adapter Tuning methods as well as other baselines by a significant margin. Furthermore, in two challenging scenarios with low-resource and multi-task settings, MoSA achieves satisfactory results, further demonstrating the effectiveness of our design. Our code will be released.","sentences":["With the rapid growth in the scale of pre-trained foundation models, parameter-efficient fine-tuning techniques have gained significant attention, among which Adapter Tuning is the most widely used.","Despite achieving efficiency, Adapter Tuning still underperforms full fine-tuning, and the performance improves at the cost of an increase in parameters.","Recent efforts address this issue by pruning the original adapters, but it also introduces training instability and suboptimal performance on certain datasets.","Motivated by this, we propose Mixture of Sparse Adapters, or MoSA, as a novel Adapter Tuning method to fully unleash the potential of each parameter in the adapter.","We first split the standard adapter into multiple non-overlapping modules, then stochastically activate modules for sparse training, and finally merge them to form a complete adapter after tuning.","In this way, MoSA can achieve significantly better performance than standard adapters without any additional computational or storage overhead.","Furthermore, we propose a hierarchical sparse strategy to better leverage limited training data.","Extensive experiments on a series of 27 visual tasks demonstrate that MoSA consistently outperforms other Adapter Tuning methods as well as other baselines by a significant margin.","Furthermore, in two challenging scenarios with low-resource and multi-task settings, MoSA achieves satisfactory results, further demonstrating the effectiveness of our design.","Our code will be released."],"url":"http://arxiv.org/abs/2312.02923v1"}
{"created":"2023-12-05 17:50:34","title":"Cyber Insurance for Cyber Resilience","abstract":"Cyber insurance is a complementary mechanism to further reduce the financial impact on the systems after their effort in defending against cyber attacks and implementing resilience mechanism to maintain the system-level operator even though the attacker is already in the system. This chapter presents a review of the quantitative cyber insurance design framework that takes into account the incentives as well as the perceptual aspects of multiple parties. The design framework builds on the correlation between state-of-the-art attacker vectors and defense mechanisms. In particular, we propose the notion of residual risks to characterize the goal of cyber insurance design. By elaborating the insurer's observations necessary for the modeling of the cyber insurance contract, we make comparison between the design strategies of the insurer under scenarios with different monitoring rules. These distinct but practical scenarios give rise to the concept of the intensity of the moral hazard issue. Using the modern techniques in quantifying the risk preferences of individuals, we link the economic impacts of perception manipulation with moral hazard. With the joint design of cyber insurance design and risk perceptions, cyber resilience can be enhanced under mild assumptions on the monitoring of insurees' actions. Finally, we discuss possible extensions on the cyber insurance design framework to more sophisticated settings and the regulations to strengthen the cyber insurance markets.","sentences":["Cyber insurance is a complementary mechanism to further reduce the financial impact on the systems after their effort in defending against cyber attacks and implementing resilience mechanism to maintain the system-level operator even though the attacker is already in the system.","This chapter presents a review of the quantitative cyber insurance design framework that takes into account the incentives as well as the perceptual aspects of multiple parties.","The design framework builds on the correlation between state-of-the-art attacker vectors and defense mechanisms.","In particular, we propose the notion of residual risks to characterize the goal of cyber insurance design.","By elaborating the insurer's observations necessary for the modeling of the cyber insurance contract, we make comparison between the design strategies of the insurer under scenarios with different monitoring rules.","These distinct but practical scenarios give rise to the concept of the intensity of the moral hazard issue.","Using the modern techniques in quantifying the risk preferences of individuals, we link the economic impacts of perception manipulation with moral hazard.","With the joint design of cyber insurance design and risk perceptions, cyber resilience can be enhanced under mild assumptions on the monitoring of insurees' actions.","Finally, we discuss possible extensions on the cyber insurance design framework to more sophisticated settings and the regulations to strengthen the cyber insurance markets."],"url":"http://arxiv.org/abs/2312.02921v1"}
{"created":"2023-12-05 17:47:33","title":"Fine-grained Controllable Video Generation via Object Appearance and Context","abstract":"Text-to-video generation has shown promising results. However, by taking only natural languages as input, users often face difficulties in providing detailed information to precisely control the model's output. In this work, we propose fine-grained controllable video generation (FACTOR) to achieve detailed control. Specifically, FACTOR aims to control objects' appearances and context, including their location and category, in conjunction with the text prompt. To achieve detailed control, we propose a unified framework to jointly inject control signals into the existing text-to-video model. Our model consists of a joint encoder and adaptive cross-attention layers. By optimizing the encoder and the inserted layer, we adapt the model to generate videos that are aligned with both text prompts and fine-grained control. Compared to existing methods relying on dense control signals such as edge maps, we provide a more intuitive and user-friendly interface to allow object-level fine-grained control. Our method achieves controllability of object appearances without finetuning, which reduces the per-subject optimization efforts for the users. Extensive experiments on standard benchmark datasets and user-provided inputs validate that our model obtains a 70% improvement in controllability metrics over competitive baselines.","sentences":["Text-to-video generation has shown promising results.","However, by taking only natural languages as input, users often face difficulties in providing detailed information to precisely control the model's output.","In this work, we propose fine-grained controllable video generation (FACTOR) to achieve detailed control.","Specifically, FACTOR aims to control objects' appearances and context, including their location and category, in conjunction with the text prompt.","To achieve detailed control, we propose a unified framework to jointly inject control signals into the existing text-to-video model.","Our model consists of a joint encoder and adaptive cross-attention layers.","By optimizing the encoder and the inserted layer, we adapt the model to generate videos that are aligned with both text prompts and fine-grained control.","Compared to existing methods relying on dense control signals such as edge maps, we provide a more intuitive and user-friendly interface to allow object-level fine-grained control.","Our method achieves controllability of object appearances without finetuning, which reduces the per-subject optimization efforts for the users.","Extensive experiments on standard benchmark datasets and user-provided inputs validate that our model obtains a 70% improvement in controllability metrics over competitive baselines."],"url":"http://arxiv.org/abs/2312.02919v1"}
{"created":"2023-12-05 17:47:11","title":"Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration","abstract":"Despite substantial progress, all-in-one image restoration (IR) grapples with persistent challenges in handling intricate real-world degradations. This paper introduces MPerceiver: a novel multimodal prompt learning approach that harnesses Stable Diffusion (SD) priors to enhance adaptiveness, generalizability and fidelity for all-in-one image restoration. Specifically, we develop a dual-branch module to master two types of SD prompts: textual for holistic representation and visual for multiscale detail representation. Both prompts are dynamically adjusted by degradation predictions from the CLIP image encoder, enabling adaptive responses to diverse unknown degradations. Moreover, a plug-in detail refinement module improves restoration fidelity via direct encoder-to-decoder information transformation. To assess our method, MPerceiver is trained on 9 tasks for all-in-one IR and outperforms state-of-the-art task-specific methods across most tasks. Post multitask pre-training, MPerceiver attains a generalized representation in low-level vision, exhibiting remarkable zero-shot and few-shot capabilities in unseen tasks. Extensive experiments on 16 IR tasks and 26 benchmarks underscore the superiority of MPerceiver in terms of adaptiveness, generalizability and fidelity.","sentences":["Despite substantial progress, all-in-one image restoration (IR) grapples with persistent challenges in handling intricate real-world degradations.","This paper introduces MPerceiver: a novel multimodal prompt learning approach that harnesses Stable Diffusion (SD) priors to enhance adaptiveness, generalizability and fidelity for all-in-one image restoration.","Specifically, we develop a dual-branch module to master two types of SD prompts: textual for holistic representation and visual for multiscale detail representation.","Both prompts are dynamically adjusted by degradation predictions from the CLIP image encoder, enabling adaptive responses to diverse unknown degradations.","Moreover, a plug-in detail refinement module improves restoration fidelity via direct encoder-to-decoder information transformation.","To assess our method, MPerceiver is trained on 9 tasks for all-in-one IR and outperforms state-of-the-art task-specific methods across most tasks.","Post multitask pre-training, MPerceiver attains a generalized representation in low-level vision, exhibiting remarkable zero-shot and few-shot capabilities in unseen tasks.","Extensive experiments on 16 IR tasks and 26 benchmarks underscore the superiority of MPerceiver in terms of adaptiveness, generalizability and fidelity."],"url":"http://arxiv.org/abs/2312.02918v1"}
{"created":"2023-12-05 17:46:52","title":"MIND: Multi-Task Incremental Network Distillation","abstract":"The recent surge in pervasive devices generating dynamic data streams has underscored the necessity for learning systems to adapt to data distributional shifts continually. To tackle this challenge, the research community has put forth a spectrum of methodologies, including the demanding pursuit of class-incremental learning without replay data. In this study, we present MIND, a parameter isolation method that aims to significantly enhance the performance of replay-free solutions and achieve state-of-the-art results on several widely studied datasets. Our approach introduces two main contributions: two alternative distillation procedures that significantly improve the efficiency of MIND increasing the accumulated knowledge of each sub-network, and the optimization of the BachNorm layers across tasks inside the sub-networks. Overall, MIND outperforms all the state-of-the-art methods for rehearsal-free Class-Incremental learning (with an increment in classification accuracy of approx. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx. +40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each contribution to demonstrate its impact on performance improvement. Our results showcase the superior performance of MIND indicating its potential for addressing the challenges posed by Class-incremental and Domain-Incremental learning in resource-constrained environments.","sentences":["The recent surge in pervasive devices generating dynamic data streams has underscored the necessity for learning systems to adapt to data distributional shifts continually.","To tackle this challenge, the research community has put forth a spectrum of methodologies, including the demanding pursuit of class-incremental learning without replay data.","In this study, we present MIND, a parameter isolation method that aims to significantly enhance the performance of replay-free solutions and achieve state-of-the-art results on several widely studied datasets.","Our approach introduces two main contributions: two alternative distillation procedures that significantly improve the efficiency of MIND increasing the accumulated knowledge of each sub-network, and the optimization of the BachNorm layers across tasks inside the sub-networks.","Overall, MIND outperforms all the state-of-the-art methods for rehearsal-free Class-Incremental learning (with an increment in classification accuracy of approx.","+6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx.","+40% accuracy in Domain-Incremental scenarios.","Moreover, we ablated each contribution to demonstrate its impact on performance improvement.","Our results showcase the superior performance of MIND indicating its potential for addressing the challenges posed by Class-incremental and Domain-Incremental learning in resource-constrained environments."],"url":"http://arxiv.org/abs/2312.02916v1"}
{"created":"2023-12-05 17:39:19","title":"Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training","abstract":"In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.","sentences":["In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition.","Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain.","UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective.","We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos.","Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains.","We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results."],"url":"http://arxiv.org/abs/2312.02914v1"}
{"created":"2023-12-05 17:38:02","title":"Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions","abstract":"Conversational question-answering (CQA) systems aim to create interactive search systems that effectively retrieve information by interacting with users. To replicate human-to-human conversations, existing work uses human annotators to play the roles of the questioner (student) and the answerer (teacher). Despite its effectiveness, challenges exist as human annotation is time-consuming, inconsistent, and not scalable. To address this issue and investigate the applicability of large language models (LLMs) in CQA simulation, we propose a simulation framework that employs zero-shot learner LLMs for simulating teacher-student interactions. Our framework involves two LLMs interacting on a specific topic, with the first LLM acting as a student, generating questions to explore a given search topic. The second LLM plays the role of a teacher by answering questions and is equipped with additional information, including a text on the given topic. We implement both the student and teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness of LLMs in simulating CQA interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the simulated data from various perspectives. We begin by evaluating the teacher's performance through both automatic and human assessment. Next, we evaluate the performance of the student, analyzing and comparing the disparities between questions generated by the LLM and those generated by humans. Furthermore, we conduct extensive analyses to thoroughly examine the LLM performance by benchmarking state-of-the-art reading comprehension models on both datasets. Our results reveal that the teacher LLM generates lengthier answers that tend to be more accurate and complete. The student LLM generates more diverse questions, covering more aspects of a given topic.","sentences":["Conversational question-answering (CQA) systems aim to create interactive search systems that effectively retrieve information by interacting with users.","To replicate human-to-human conversations, existing work uses human annotators to play the roles of the questioner (student) and the answerer (teacher).","Despite its effectiveness, challenges exist as human annotation is time-consuming, inconsistent, and not scalable.","To address this issue and investigate the applicability of large language models (LLMs) in CQA simulation, we propose a simulation framework that employs zero-shot learner LLMs for simulating teacher-student interactions.","Our framework involves two LLMs interacting on a specific topic, with the first LLM acting as a student, generating questions to explore a given search topic.","The second LLM plays the role of a teacher by answering questions and is equipped with additional information, including a text on the given topic.","We implement both the student and teacher by zero-shot prompting the GPT-4 model.","To assess the effectiveness of LLMs in simulating CQA interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the simulated data from various perspectives.","We begin by evaluating the teacher's performance through both automatic and human assessment.","Next, we evaluate the performance of the student, analyzing and comparing the disparities between questions generated by the LLM and those generated by humans.","Furthermore, we conduct extensive analyses to thoroughly examine the LLM performance by benchmarking state-of-the-art reading comprehension models on both datasets.","Our results reveal that the teacher LLM generates lengthier answers that tend to be more accurate and complete.","The student LLM generates more diverse questions, covering more aspects of a given topic."],"url":"http://arxiv.org/abs/2312.02913v1"}
{"created":"2023-12-05 17:36:34","title":"Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers","abstract":"Adversarial attacks have highlighted the vulnerability of classifiers based on machine learning for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) tasks. An adversarial attack perturbs SAR images of on-ground targets such that the classifiers are misled into making incorrect predictions. However, many existing attacking techniques rely on arbitrary manipulation of SAR images while overlooking the feasibility of executing the attacks on real-world SAR imagery. Instead, adversarial attacks should be able to be implemented by physical actions, for example, placing additional false objects as scatterers around the on-ground target to perturb the SAR image and fool the SAR ATR.   In this paper, we propose the On-Target Scatterer Attack (OTSA), a scatterer-based physical adversarial attack. To ensure the feasibility of its physical execution, we enforce a constraint on the positioning of the scatterers. Specifically, we restrict the scatterers to be placed only on the target instead of in the shadow regions or the background. To achieve this, we introduce a positioning score based on Gaussian kernels and formulate an optimization problem for our OTSA attack. Using a gradient ascent method to solve the optimization problem, the OTSA can generate a vector of parameters describing the positions, shapes, sizes and amplitudes of the scatterers to guide the physical execution of the attack that will mislead SAR image classifiers. The experimental results show that our attack obtains significantly higher success rates under the positioning constraint compared with the existing method.","sentences":["Adversarial attacks have highlighted the vulnerability of classifiers based on machine learning for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) tasks.","An adversarial attack perturbs SAR images of on-ground targets such that the classifiers are misled into making incorrect predictions.","However, many existing attacking techniques rely on arbitrary manipulation of SAR images while overlooking the feasibility of executing the attacks on real-world SAR imagery.","Instead, adversarial attacks should be able to be implemented by physical actions, for example, placing additional false objects as scatterers around the on-ground target to perturb the SAR image and fool the SAR ATR.   ","In this paper, we propose the On-Target Scatterer Attack (OTSA), a scatterer-based physical adversarial attack.","To ensure the feasibility of its physical execution, we enforce a constraint on the positioning of the scatterers.","Specifically, we restrict the scatterers to be placed only on the target instead of in the shadow regions or the background.","To achieve this, we introduce a positioning score based on Gaussian kernels and formulate an optimization problem for our OTSA attack.","Using a gradient ascent method to solve the optimization problem, the OTSA can generate a vector of parameters describing the positions, shapes, sizes and amplitudes of the scatterers to guide the physical execution of the attack that will mislead SAR image classifiers.","The experimental results show that our attack obtains significantly higher success rates under the positioning constraint compared with the existing method."],"url":"http://arxiv.org/abs/2312.02912v1"}
{"created":"2023-12-05 17:26:28","title":"Uncovering Patterns of Participant-Invariant Influence in Networks","abstract":"In this paper, we explore the nature of influence in a network. The concept of participant-invariant influence is derived from an influence matrix M specifically designed to explore this phenomenon. Through nonnegative matrix factorization approximation, we managed to extract a participant-invariant matrix H representing a shared pattern that all participants must obey. The acquired H is highly field-related and can be further utilized to cluster factual networks. Our discovery of the unveiled participant-independent influence within network dynamics opens up new avenues for further research on network behavior and its implications.","sentences":["In this paper, we explore the nature of influence in a network.","The concept of participant-invariant influence is derived from an influence matrix M specifically designed to explore this phenomenon.","Through nonnegative matrix factorization approximation, we managed to extract a participant-invariant matrix H representing a shared pattern that all participants must obey.","The acquired H is highly field-related and can be further utilized to cluster factual networks.","Our discovery of the unveiled participant-independent influence within network dynamics opens up new avenues for further research on network behavior and its implications."],"url":"http://arxiv.org/abs/2312.02906v1"}
{"created":"2023-12-05 17:19:22","title":"HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting","abstract":"3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, the first model to use 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit representation from 3DGS with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent final color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, which surpasses baselines by up to ~2dB, while accelerating rendering speed by over x10.","sentences":["3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields.","Real-time rendering is a highly desirable goal for real-world applications.","We propose HeadGaS, the first model to use 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation.","In this paper we introduce a hybrid model that extends the explicit representation from 3DGS with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent final color and opacity values.","We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, which surpasses baselines by up to ~2dB, while accelerating rendering speed by over x10."],"url":"http://arxiv.org/abs/2312.02902v1"}
{"created":"2023-12-05 17:15:16","title":"Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive Review","abstract":"Due to the advent and increase in the popularity of the Internet, people have been producing and disseminating textual data in several ways, such as reviews, social media posts, and news articles. As a result, numerous researchers have been working on discovering patterns in textual data, especially because social media posts function as social sensors, indicating peoples' opinions, interests, etc. However, most tasks regarding natural language processing are addressed using traditional machine learning methods and static datasets. This setting can lead to several problems, such as an outdated dataset, which may not correspond to reality, and an outdated model, which has its performance degrading over time. Concept drift is another aspect that emphasizes these issues, which corresponds to data distribution and pattern changes. In a text stream scenario, it is even more challenging due to its characteristics, such as the high speed and data arriving sequentially. In addition, models for this type of scenario must adhere to the constraints mentioned above while learning from the stream by storing texts for a limited time and consuming low memory. In this study, we performed a systematic literature review regarding concept drift adaptation in text stream scenarios. Considering well-defined criteria, we selected 40 papers to unravel aspects such as text drift categories, types of text drift detection, model update mechanism, the addressed stream mining tasks, types of text representations, and text representation update mechanism. In addition, we discussed drift visualization and simulation and listed real-world datasets used in the selected papers. Therefore, this paper comprehensively reviews the concept drift adaptation in text stream mining scenarios.","sentences":["Due to the advent and increase in the popularity of the Internet, people have been producing and disseminating textual data in several ways, such as reviews, social media posts, and news articles.","As a result, numerous researchers have been working on discovering patterns in textual data, especially because social media posts function as social sensors, indicating peoples' opinions, interests, etc.","However, most tasks regarding natural language processing are addressed using traditional machine learning methods and static datasets.","This setting can lead to several problems, such as an outdated dataset, which may not correspond to reality, and an outdated model, which has its performance degrading over time.","Concept drift is another aspect that emphasizes these issues, which corresponds to data distribution and pattern changes.","In a text stream scenario, it is even more challenging due to its characteristics, such as the high speed and data arriving sequentially.","In addition, models for this type of scenario must adhere to the constraints mentioned above while learning from the stream by storing texts for a limited time and consuming low memory.","In this study, we performed a systematic literature review regarding concept drift adaptation in text stream scenarios.","Considering well-defined criteria, we selected 40 papers to unravel aspects such as text drift categories, types of text drift detection, model update mechanism, the addressed stream mining tasks, types of text representations, and text representation update mechanism.","In addition, we discussed drift visualization and simulation and listed real-world datasets used in the selected papers.","Therefore, this paper comprehensively reviews the concept drift adaptation in text stream mining scenarios."],"url":"http://arxiv.org/abs/2312.02901v1"}
{"created":"2023-12-05 17:09:59","title":"Perspectives from Naive Participants and Experienced Social Science Researchers on Addressing Embodiment in a Virtual Cyberball Task","abstract":"We describe the design of an immersive virtual Cyberball task that included avatar customization, and user feedback on this design. We first created a prototype of an avatar customization template and added it to a Cyberball prototype built in the Unity3D game engine. Then, we conducted in-depth user testing and feedback sessions with 15 Cyberball stakeholders: five naive participants with no prior knowledge of Cyberball and ten experienced researchers with extensive experience using the Cyberball paradigm. We report the divergent perspectives of the two groups on the following design insights; designing for intuitive use, inclusivity, and realistic experiences versus minimalism. Participant responses shed light on how system design problems may contribute to or perpetuate negative experiences when customizing avatars. They also demonstrate the value of considering multiple stakeholders' feedback in the design process for virtual reality, presenting a more comprehensive view in designing future Cyberball prototypes and interactive systems for social science research.","sentences":["We describe the design of an immersive virtual Cyberball task that included avatar customization, and user feedback on this design.","We first created a prototype of an avatar customization template and added it to a Cyberball prototype built in the Unity3D game engine.","Then, we conducted in-depth user testing and feedback sessions with 15 Cyberball stakeholders: five naive participants with no prior knowledge of Cyberball and ten experienced researchers with extensive experience using the Cyberball paradigm.","We report the divergent perspectives of the two groups on the following design insights; designing for intuitive use, inclusivity, and realistic experiences versus minimalism.","Participant responses shed light on how system design problems may contribute to or perpetuate negative experiences when customizing avatars.","They also demonstrate the value of considering multiple stakeholders' feedback in the design process for virtual reality, presenting a more comprehensive view in designing future Cyberball prototypes and interactive systems for social science research."],"url":"http://arxiv.org/abs/2312.02897v1"}
{"created":"2023-12-05 17:06:59","title":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models","abstract":"Large Multimodal Models (LMMs) such as GPT-4V and LLaVA have shown remarkable capabilities in visual reasoning with common image styles. However, their robustness against diverse style shifts, crucial for practical applications, remains largely unexplored. In this paper, we propose a new benchmark, BenchLMM, to assess the robustness of LMMs against three different styles: artistic image style, imaging sensor style, and application style, where each style has five sub-styles. Utilizing BenchLMM, we comprehensively evaluate state-of-the-art LMMs and reveal: 1) LMMs generally suffer performance degradation when working with other styles; 2) An LMM performs better than another model in common style does not guarantee its superior performance in other styles; 3) LMMs' reasoning capability can be enhanced by prompting LMMs to predict the style first, based on which we propose a versatile and training-free method for improving LMMs; 4) An intelligent LMM is expected to interpret the causes of its errors when facing stylistic variations. We hope that our benchmark and analysis can shed new light on developing more intelligent and versatile LMMs.","sentences":["Large Multimodal Models (LMMs) such as GPT-4V and LLaVA have shown remarkable capabilities in visual reasoning with common image styles.","However, their robustness against diverse style shifts, crucial for practical applications, remains largely unexplored.","In this paper, we propose a new benchmark, BenchLMM, to assess the robustness of LMMs against three different styles: artistic image style, imaging sensor style, and application style, where each style has five sub-styles.","Utilizing BenchLMM, we comprehensively evaluate state-of-the-art LMMs and reveal: 1) LMMs generally suffer performance degradation when working with other styles; 2) An LMM performs better than another model in common style does not guarantee its superior performance in other styles; 3) LMMs' reasoning capability can be enhanced by prompting LMMs to predict the style first, based on which we propose a versatile and training-free method for improving LMMs; 4) An intelligent LMM is expected to interpret the causes of its errors when facing stylistic variations.","We hope that our benchmark and analysis can shed new light on developing more intelligent and versatile LMMs."],"url":"http://arxiv.org/abs/2312.02896v1"}
{"created":"2023-12-05 16:53:20","title":"Zero Trust for Cyber Resilience","abstract":"The increased connectivity and potential insider threats make traditional network defense vulnerable. Instead of assuming that everything behind the security perimeter is safe, the zero-trust security model verifies every incoming request before granting access. This chapter draws attention to the cyber resilience within the zero-trust model. We introduce the evolution from traditional perimeter-based security to zero trust and discuss their difference. Two key elements of the zero-trust engine are trust evaluation (TE) and policy engine (PE). We introduce the design of the two components and discuss how their interplay would contribute to cyber resilience. Dynamic game theory and learning are applied as quantitative approaches to achieve automated zero-trust cyber resilience. Several case studies and implementations are introduced to illustrate the benefits of such a security model.","sentences":["The increased connectivity and potential insider threats make traditional network defense vulnerable.","Instead of assuming that everything behind the security perimeter is safe, the zero-trust security model verifies every incoming request before granting access.","This chapter draws attention to the cyber resilience within the zero-trust model.","We introduce the evolution from traditional perimeter-based security to zero trust and discuss their difference.","Two key elements of the zero-trust engine are trust evaluation (TE) and policy engine (PE).","We introduce the design of the two components and discuss how their interplay would contribute to cyber resilience.","Dynamic game theory and learning are applied as quantitative approaches to achieve automated zero-trust cyber resilience.","Several case studies and implementations are introduced to illustrate the benefits of such a security model."],"url":"http://arxiv.org/abs/2312.02882v1"}
{"created":"2023-12-05 16:52:20","title":"PULSAR: Simultaneous Many-Row Activation for Reliable and High-Performance Computing in Off-the-Shelf DRAM Chips","abstract":"Data movement between the processor and the main memory is a first-order obstacle against improving performance and energy efficiency in modern systems. To address this obstacle, Processing-using-Memory (PuM) is a promising approach where bulk-bitwise operations are performed leveraging intrinsic analog properties within the DRAM array and massive parallelism across DRAM columns. Unfortunately, 1) modern off-the-shelf DRAM chips do not officially support PuM operations, and 2) existing techniques of performing PuM operations on off-the-shelf DRAM chips suffer from two key limitations. First, these techniques have low success rates, i.e., only a small fraction of DRAM columns can correctly execute PuM operations because they operate beyond manufacturer-recommended timing constraints, causing these operations to be highly susceptible to noise and process variation. Second, these techniques have limited compute primitives, preventing them from fully leveraging parallelism across DRAM columns and thus hindering their performance benefits.   We propose PULSAR, a new technique to enable high-success-rate and high-performance PuM operations in off-the-shelf DRAM chips. PULSAR leverages our new observation that a carefully crafted sequence of DRAM commands simultaneously activates up to 32 DRAM rows. PULSAR overcomes the limitations of existing techniques by 1) replicating the input data to improve the success rate and 2) enabling new bulk bitwise operations (e.g., many-input majority, Multi-RowInit, and Bulk-Write) to improve the performance.   Our analysis on 120 off-the-shelf DDR4 chips from two major manufacturers shows that PULSAR achieves a 24.18% higher success rate and 121% higher performance over seven arithmetic-logic operations compared to FracDRAM, a state-of-the-art off-the-shelf DRAM-based PuM technique.","sentences":["Data movement between the processor and the main memory is a first-order obstacle against improving performance and energy efficiency in modern systems.","To address this obstacle, Processing-using-Memory (PuM) is a promising approach where bulk-bitwise operations are performed leveraging intrinsic analog properties within the DRAM array and massive parallelism across DRAM columns.","Unfortunately, 1) modern off-the-shelf DRAM chips do not officially support PuM operations, and 2) existing techniques of performing PuM operations on off-the-shelf DRAM chips suffer from two key limitations.","First, these techniques have low success rates, i.e., only a small fraction of DRAM columns can correctly execute PuM operations because they operate beyond manufacturer-recommended timing constraints, causing these operations to be highly susceptible to noise and process variation.","Second, these techniques have limited compute primitives, preventing them from fully leveraging parallelism across DRAM columns and thus hindering their performance benefits.   ","We propose PULSAR, a new technique to enable high-success-rate and high-performance PuM operations in off-the-shelf DRAM chips.","PULSAR leverages our new observation that a carefully crafted sequence of DRAM commands simultaneously activates up to 32 DRAM rows.","PULSAR overcomes the limitations of existing techniques by 1) replicating the input data to improve the success rate and 2) enabling new bulk bitwise operations (e.g., many-input majority, Multi-RowInit, and Bulk-Write) to improve the performance.   ","Our analysis on 120 off-the-shelf DDR4 chips from two major manufacturers shows that PULSAR achieves a 24.18% higher success rate and 121% higher performance over seven arithmetic-logic operations compared to FracDRAM, a state-of-the-art off-the-shelf DRAM-based PuM technique."],"url":"http://arxiv.org/abs/2312.02880v1"}
{"created":"2023-12-05 16:48:17","title":"Towards More Practical Group Activity Detection: A New Benchmark and Model","abstract":"Group activity detection (GAD) is the task of identifying members of each group and classifying the activity of the group at the same time in a video. While GAD has been studied recently, there is still much room for improvement in both dataset and methodology due to their limited capability to address practical GAD scenarios. To resolve these issues, we first present a new dataset, dubbed Caf\\'e. Unlike existing datasets, Caf\\'e is constructed primarily for GAD and presents more practical evaluation scenarios and metrics, as well as being large-scale and providing rich annotations. Along with the dataset, we propose a new GAD model that deals with an unknown number of groups and latent group members efficiently and effectively. We evaluated our model on three datasets including Caf\\'e, where it outperformed previous work in terms of both accuracy and inference speed. Both our dataset and code base will be open to the public to promote future research on GAD.","sentences":["Group activity detection (GAD) is the task of identifying members of each group and classifying the activity of the group at the same time in a video.","While GAD has been studied recently, there is still much room for improvement in both dataset and methodology due to their limited capability to address practical GAD scenarios.","To resolve these issues, we first present a new dataset, dubbed Caf\\'e.","Unlike existing datasets, Caf\\'e is constructed primarily for GAD and presents more practical evaluation scenarios and metrics, as well as being large-scale and providing rich annotations.","Along with the dataset, we propose a new GAD model that deals with an unknown number of groups and latent group members efficiently and effectively.","We evaluated our model on three datasets including Caf\\'e, where it outperformed previous work in terms of both accuracy and inference speed.","Both our dataset and code base will be open to the public to promote future research on GAD."],"url":"http://arxiv.org/abs/2312.02878v1"}
{"created":"2023-12-05 16:47:46","title":"A Dynamic Network for Efficient Point Cloud Registration","abstract":"For the point cloud registration task, a significant challenge arises from non-overlapping points that consume extensive computational resources while negatively affecting registration accuracy. In this paper, we introduce a dynamic approach, widely utilized to improve network efficiency in computer vision tasks, to the point cloud registration task. We employ an iterative registration process on point cloud data multiple times to identify regions where matching points cluster, ultimately enabling us to remove noisy points. Specifically, we begin with deep global sampling to perform coarse global registration. Subsequently, we employ the proposed refined node proposal module to further narrow down the registration region and perform local registration. Furthermore, we utilize a spatial consistency-based classifier to evaluate the results of each registration stage. The model terminates once it reaches sufficient confidence, avoiding unnecessary computations. Extended experiments demonstrate that our model significantly reduces time consumption compared to other methods with similar results, achieving a speed improvement of over 41% on indoor dataset (3DMatch) and 33% on outdoor datasets (KITTI) while maintaining competitive registration recall requirements.","sentences":["For the point cloud registration task, a significant challenge arises from non-overlapping points that consume extensive computational resources while negatively affecting registration accuracy.","In this paper, we introduce a dynamic approach, widely utilized to improve network efficiency in computer vision tasks, to the point cloud registration task.","We employ an iterative registration process on point cloud data multiple times to identify regions where matching points cluster, ultimately enabling us to remove noisy points.","Specifically, we begin with deep global sampling to perform coarse global registration.","Subsequently, we employ the proposed refined node proposal module to further narrow down the registration region and perform local registration.","Furthermore, we utilize a spatial consistency-based classifier to evaluate the results of each registration stage.","The model terminates once it reaches sufficient confidence, avoiding unnecessary computations.","Extended experiments demonstrate that our model significantly reduces time consumption compared to other methods with similar results, achieving a speed improvement of over 41% on indoor dataset (3DMatch) and 33% on outdoor datasets (KITTI) while maintaining competitive registration recall requirements."],"url":"http://arxiv.org/abs/2312.02877v1"}
{"created":"2023-12-05 16:39:41","title":"Toward autocorrection of chemical process flowsheets using large language models","abstract":"The process engineering domain widely uses Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (P&IDs) to represent process flows and equipment configurations. However, the P&IDs and PFDs, hereafter called flowsheets, can contain errors causing safety hazards, inefficient operation, and unnecessary expenses. Correcting and verifying flowsheets is a tedious, manual process. We propose a novel generative AI methodology for automatically identifying errors in flowsheets and suggesting corrections to the user, i.e., autocorrecting flowsheets. Inspired by the breakthrough of Large Language Models (LLMs) for grammatical autocorrection of human language, we investigate LLMs for the autocorrection of flowsheets. The input to the model is a potentially erroneous flowsheet and the output of the model are suggestions for a corrected flowsheet. We train our autocorrection model on a synthetic dataset in a supervised manner. The model achieves a top-1 accuracy of 80% and a top-5 accuracy of 84% on an independent test dataset of synthetically generated flowsheets. The results suggest that the model can learn to autocorrect the synthetic flowsheets. We envision that flowsheet autocorrection will become a useful tool for chemical engineers.","sentences":["The process engineering domain widely uses Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (P&IDs) to represent process flows and equipment configurations.","However, the P&IDs and PFDs, hereafter called flowsheets, can contain errors causing safety hazards, inefficient operation, and unnecessary expenses.","Correcting and verifying flowsheets is a tedious, manual process.","We propose a novel generative AI methodology for automatically identifying errors in flowsheets and suggesting corrections to the user, i.e., autocorrecting flowsheets.","Inspired by the breakthrough of Large Language Models (LLMs) for grammatical autocorrection of human language, we investigate LLMs for the autocorrection of flowsheets.","The input to the model is a potentially erroneous flowsheet and the output of the model are suggestions for a corrected flowsheet.","We train our autocorrection model on a synthetic dataset in a supervised manner.","The model achieves a top-1 accuracy of 80% and a top-5 accuracy of 84% on an independent test dataset of synthetically generated flowsheets.","The results suggest that the model can learn to autocorrect the synthetic flowsheets.","We envision that flowsheet autocorrection will become a useful tool for chemical engineers."],"url":"http://arxiv.org/abs/2312.02873v1"}
{"created":"2023-12-05 16:39:32","title":"Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction","abstract":"In the context of autonomous driving, pedestrian crossing prediction is a key component for improving road safety. Presently, the focus of these predictions extends beyond achieving trustworthy results; it is shifting towards the explainability and interpretability of these predictions. This research introduces a novel neuro-symbolic approach that combines deep learning and fuzzy logic for an explainable and interpretable pedestrian crossing prediction. We have developed an explainable predictor (ExPedCross), which utilizes a set of explainable features and employs a fuzzy inference system to predict whether the pedestrian will cross or not. Our approach was evaluated on both the PIE and JAAD datasets. The results offer experimental insights into achieving explainability and interpretability in the pedestrian crossing prediction task. Furthermore, the testing results yield a set of guidelines and recommendations regarding the process of dataset selection, feature selection, and explainability.","sentences":["In the context of autonomous driving, pedestrian crossing prediction is a key component for improving road safety.","Presently, the focus of these predictions extends beyond achieving trustworthy results; it is shifting towards the explainability and interpretability of these predictions.","This research introduces a novel neuro-symbolic approach that combines deep learning and fuzzy logic for an explainable and interpretable pedestrian crossing prediction.","We have developed an explainable predictor (ExPedCross), which utilizes a set of explainable features and employs a fuzzy inference system to predict whether the pedestrian will cross or not.","Our approach was evaluated on both the PIE and JAAD datasets.","The results offer experimental insights into achieving explainability and interpretability in the pedestrian crossing prediction task.","Furthermore, the testing results yield a set of guidelines and recommendations regarding the process of dataset selection, feature selection, and explainability."],"url":"http://arxiv.org/abs/2312.02872v1"}
{"created":"2023-12-05 16:39:24","title":"Attention-enhanced neural differential equations for physics-informed deep learning of ion transport","abstract":"Species transport models typically combine partial differential equations (PDEs) with relations from hindered transport theory to quantify electromigrative, convective, and diffusive transport through complex nanoporous systems; however, these formulations are frequently substantial simplifications of the governing dynamics, leading to the poor generalization performance of PDE-based models. Given the growing interest in deep learning methods for the physical sciences, we develop a machine learning-based approach to characterize ion transport across nanoporous membranes. Our proposed framework centers around attention-enhanced neural differential equations that incorporate electroneutrality-based inductive biases to improve generalization performance relative to conventional PDE-based methods. In addition, we study the role of the attention mechanism in illuminating physically-meaningful ion-pairing relationships across diverse mixture compositions. Further, we investigate the importance of pre-training on simulated data from PDE-based models, as well as the performance benefits from hard vs. soft inductive biases. Our results indicate that physics-informed deep learning solutions can outperform their classical PDE-based counterparts and provide promising avenues for modelling complex transport phenomena across diverse applications.","sentences":["Species transport models typically combine partial differential equations (PDEs) with relations from hindered transport theory to quantify electromigrative, convective, and diffusive transport through complex nanoporous systems; however, these formulations are frequently substantial simplifications of the governing dynamics, leading to the poor generalization performance of PDE-based models.","Given the growing interest in deep learning methods for the physical sciences, we develop a machine learning-based approach to characterize ion transport across nanoporous membranes.","Our proposed framework centers around attention-enhanced neural differential equations that incorporate electroneutrality-based inductive biases to improve generalization performance relative to conventional PDE-based methods.","In addition, we study the role of the attention mechanism in illuminating physically-meaningful ion-pairing relationships across diverse mixture compositions.","Further, we investigate the importance of pre-training on simulated data from PDE-based models, as well as the performance benefits from hard vs. soft inductive biases.","Our results indicate that physics-informed deep learning solutions can outperform their classical PDE-based counterparts and provide promising avenues for modelling complex transport phenomena across diverse applications."],"url":"http://arxiv.org/abs/2312.02871v1"}
{"created":"2023-12-05 16:36:27","title":"Can a Tabula Recta provide security in the XXI century?","abstract":"In the not so unlikely scenario of total compromise of computers accessible to a group of users, they might be tempted to resort to human-computable paper-and-pencil cryptographic methods aided by a classic Tabula Recta, which helps to perform addition and subtraction directly with letters. But do these classic algorithms, or some new ones using the same simple tools, have any chance against computer-aided cryptanalysis? In this paper I discuss how some human-computable algorithms can indeed afford sufficient security in this situation, drawing conclusions from computer-based statistical analysis. Three kinds of algorithms are discussed: those that concentrate entropy from shared text sources, stream ciphers based on arithmetic of non-binary spaces, and hash-like algorithms that may be used to generate a password from a challenge text.","sentences":["In the not so unlikely scenario of total compromise of computers accessible to a group of users, they might be tempted to resort to human-computable paper-and-pencil cryptographic methods aided by a classic Tabula Recta, which helps to perform addition and subtraction directly with letters.","But do these classic algorithms, or some new ones using the same simple tools, have any chance against computer-aided cryptanalysis?","In this paper I discuss how some human-computable algorithms can indeed afford sufficient security in this situation, drawing conclusions from computer-based statistical analysis.","Three kinds of algorithms are discussed: those that concentrate entropy from shared text sources, stream ciphers based on arithmetic of non-binary spaces, and hash-like algorithms that may be used to generate a password from a challenge text."],"url":"http://arxiv.org/abs/2312.02869v1"}
{"created":"2023-12-05 16:27:51","title":"Semi-Supervised Health Index Monitoring with Feature Generation and Fusion","abstract":"The Health Index (HI) is crucial for evaluating system health, aiding tasks like anomaly detection and predicting remaining useful life for systems demanding high safety and reliability. Tight monitoring is crucial for achieving high precision at a lower cost, with applications such as spray coating. Obtaining HI labels in real-world applications is often cost-prohibitive, requiring continuous, precise health measurements. Therefore, it is more convenient to leverage run-to failure datasets that may provide potential indications of machine wear condition, making it necessary to apply semi-supervised tools for HI construction. In this study, we adapt the Deep Semi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use the DeepSAD embedding as a condition indicators to address interpretability challenges and sensitivity to system-specific factors. Then, we introduce a diversity loss to enrich condition indicators. We employ an alternating projection algorithm with isotonic constraints to transform the DeepSAD embedding into a normalized HI with an increasing trend. Validation on the PHME 2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates meaningful HIs estimations. Our methodology is then applied to monitor wear states of thermal spray coatings using high-frequency voltage. Our contributions create opportunities for more accessible and reliable HI estimation, particularly in cases where obtaining ground truth HI labels is unfeasible.","sentences":["The Health Index (HI) is crucial for evaluating system health, aiding tasks like anomaly detection and predicting remaining useful life for systems demanding high safety and reliability.","Tight monitoring is crucial for achieving high precision at a lower cost, with applications such as spray coating.","Obtaining HI labels in real-world applications is often cost-prohibitive, requiring continuous, precise health measurements.","Therefore, it is more convenient to leverage run-to failure datasets that may provide potential indications of machine wear condition, making it necessary to apply semi-supervised tools for HI construction.","In this study, we adapt the Deep Semi-supervised Anomaly Detection (DeepSAD) method for HI construction.","We use the DeepSAD embedding as a condition indicators to address interpretability challenges and sensitivity to system-specific factors.","Then, we introduce a diversity loss to enrich condition indicators.","We employ an alternating projection algorithm with isotonic constraints to transform the DeepSAD embedding into a normalized HI with an increasing trend.","Validation on the PHME 2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates meaningful HIs estimations.","Our methodology is then applied to monitor wear states of thermal spray coatings using high-frequency voltage.","Our contributions create opportunities for more accessible and reliable HI estimation, particularly in cases where obtaining ground truth HI labels is unfeasible."],"url":"http://arxiv.org/abs/2312.02867v1"}
{"created":"2023-12-05 16:15:53","title":"A look at the Kolmogorov complexity of finite groupoids and algebras","abstract":"The incompressibility method is a counting argument in the framework of algorithmic complexity that permits discovering properties that are satisfied by most objects of a class. This paper gives a preliminary insight into Kolmogorov's complexity of groupoids and some algebras. The incompressibility method shows that almost all the groupoids are asymmetric and simple: Only trivial or constant homomorphisms are possible. However, highly random groupoids allow subgroupoids with interesting restrictions that reveal intrinsic structural properties. We also study the issue of the algebraic varieties and wonder which equational identities allow randomness.","sentences":["The incompressibility method is a counting argument in the framework of algorithmic complexity that permits discovering properties that are satisfied by most objects of a class.","This paper gives a preliminary insight into Kolmogorov's complexity of groupoids and some algebras.","The incompressibility method shows that almost all the groupoids are asymmetric and simple: Only trivial or constant homomorphisms are possible.","However, highly random groupoids allow subgroupoids with interesting restrictions that reveal intrinsic structural properties.","We also study the issue of the algebraic varieties and wonder which equational identities allow randomness."],"url":"http://arxiv.org/abs/2312.02863v1"}
{"created":"2023-12-05 16:13:50","title":"Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring","abstract":"Through past experiences deploying what we call usable ML (one step beyond explainable ML, including both explanations and other augmenting information) to real-world domains, we have learned three key lessons. First, many organizations are beginning to hire people who we call ``bridges'' because they bridge the gap between ML developers and domain experts, and these people fill a valuable role in developing usable ML applications. Second, a configurable system that enables easily iterating on usable ML interfaces during collaborations with bridges is key. Finally, there is a need for continuous, in-deployment evaluations to quantify the real-world impact of usable ML. Throughout this paper, we apply these lessons to the task of wind turbine monitoring, an essential task in the renewable energy domain. Turbine engineers and data analysts must decide whether to perform costly in-person investigations on turbines to prevent potential cases of brakepad failure, and well-tuned usable ML interfaces can aid with this decision-making process. Through the applications of our lessons to this task, we hope to demonstrate the potential real-world impact of usable ML in the renewable energy domain.","sentences":["Through past experiences deploying what we call usable ML (one step beyond explainable ML, including both explanations and other augmenting information) to real-world domains, we have learned three key lessons.","First, many organizations are beginning to hire people who we call ``bridges'' because they bridge the gap between ML developers and domain experts, and these people fill a valuable role in developing usable ML applications.","Second, a configurable system that enables easily iterating on usable ML interfaces during collaborations with bridges is key.","Finally, there is a need for continuous, in-deployment evaluations to quantify the real-world impact of usable ML.","Throughout this paper, we apply these lessons to the task of wind turbine monitoring, an essential task in the renewable energy domain.","Turbine engineers and data analysts must decide whether to perform costly in-person investigations on turbines to prevent potential cases of brakepad failure, and well-tuned usable ML interfaces can aid with this decision-making process.","Through the applications of our lessons to this task, we hope to demonstrate the potential real-world impact of usable ML in the renewable energy domain."],"url":"http://arxiv.org/abs/2312.02859v1"}
{"created":"2023-12-05 16:13:34","title":"Towards Causal Representations of Climate Model Data","abstract":"Climate models, such as Earth system models (ESMs), are crucial for simulating future climate change based on projected Shared Socioeconomic Pathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated and invaluable, machine learning-based emulators trained on existing simulation data can project additional climate scenarios much faster and are computationally efficient. However, they often lack generalizability and interpretability. This work delves into the potential of causal representation learning, specifically the \\emph{Causal Discovery with Single-parent Decoding} (CDSD) method, which could render climate model emulation efficient \\textit{and} interpretable. We evaluate CDSD on multiple climate datasets, focusing on emissions, temperature, and precipitation. Our findings shed light on the challenges, limitations, and promise of using CDSD as a stepping stone towards more interpretable and robust climate model emulation.","sentences":["Climate models, such as Earth system models (ESMs), are crucial for simulating future climate change based on projected Shared Socioeconomic Pathways (SSP) greenhouse gas emissions scenarios.","While ESMs are sophisticated and invaluable, machine learning-based emulators trained on existing simulation data can project additional climate scenarios much faster and are computationally efficient.","However, they often lack generalizability and interpretability.","This work delves into the potential of causal representation learning, specifically the \\emph{Causal Discovery with Single-parent Decoding} (CDSD) method, which could render climate model emulation efficient \\textit{and} interpretable.","We evaluate CDSD on multiple climate datasets, focusing on emissions, temperature, and precipitation.","Our findings shed light on the challenges, limitations, and promise of using CDSD as a stepping stone towards more interpretable and robust climate model emulation."],"url":"http://arxiv.org/abs/2312.02858v1"}
{"created":"2023-12-05 16:11:52","title":"Exploring Error Bits for Memory Failure Prediction: An In-Depth Correlative Study","abstract":"In large-scale datacenters, memory failure is a common cause of server crashes, with uncorrectable errors (UEs) being a major indicator of Dual Inline Memory Module (DIMM) defects. Existing approaches primarily focus on predicting UEs using correctable errors (CEs), without fully considering the information provided by error bits. However, error bit patterns have a strong correlation with the occurrence of uncorrectable errors (UEs). In this paper, we present a comprehensive study on the correlation between CEs and UEs, specifically emphasizing the importance of spatio-temporal error bit information. Our analysis reveals a strong correlation between spatio-temporal error bits and UE occurrence. Through evaluations using real-world datasets, we demonstrate that our approach significantly improves prediction performance by 15% in F1-score compared to the state-of-the-art algorithms. Overall, our approach effectively reduces the number of virtual machine interruptions caused by UEs by approximately 59%.","sentences":["In large-scale datacenters, memory failure is a common cause of server crashes, with uncorrectable errors (UEs) being a major indicator of Dual Inline Memory Module (DIMM) defects.","Existing approaches primarily focus on predicting UEs using correctable errors (CEs), without fully considering the information provided by error bits.","However, error bit patterns have a strong correlation with the occurrence of uncorrectable errors (UEs).","In this paper, we present a comprehensive study on the correlation between CEs and UEs, specifically emphasizing the importance of spatio-temporal error bit information.","Our analysis reveals a strong correlation between spatio-temporal error bits and UE occurrence.","Through evaluations using real-world datasets, we demonstrate that our approach significantly improves prediction performance by 15% in F1-score compared to the state-of-the-art algorithms.","Overall, our approach effectively reduces the number of virtual machine interruptions caused by UEs by approximately 59%."],"url":"http://arxiv.org/abs/2312.02855v1"}
{"created":"2023-12-05 16:09:31","title":"Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems","abstract":"Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation. In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments. Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions. At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability. By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation. We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation.","sentences":["Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation.","In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments.","Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions.","At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability.","By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation.","We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation."],"url":"http://arxiv.org/abs/2312.02852v1"}
{"created":"2023-12-05 15:57:40","title":"A Review of Password-less User Authentication Schemes","abstract":"Since the demise of the password was predicted in 2004, different attempts in industry and academia have been made to create an alternative for the use of passwords in authentication, without compromising on security and user experience. This review examines password-less authentication schemes that have been proposed since after the death knell was placed on passwords in 2004. We start with a brief discussion of the requirements of authentication systems and then identify various password-less authentication proposals to date. We then evaluate the truly password-less and practical schemes using a framework that examines authentication credentials based on their impact on user experience, overall security, and ease of deployment. The findings of this review observe a difficulty in balancing security with a user experience compared to that of passwords in new password-less schemes, providing the opportunity for new applied research to leverage existing knowledge and combine technologies and techniques in innovative ways that can address this imbalance.","sentences":["Since the demise of the password was predicted in 2004, different attempts in industry and academia have been made to create an alternative for the use of passwords in authentication, without compromising on security and user experience.","This review examines password-less authentication schemes that have been proposed since after the death knell was placed on passwords in 2004.","We start with a brief discussion of the requirements of authentication systems and then identify various password-less authentication proposals to date.","We then evaluate the truly password-less and practical schemes using a framework that examines authentication credentials based on their impact on user experience, overall security, and ease of deployment.","The findings of this review observe a difficulty in balancing security with a user experience compared to that of passwords in new password-less schemes, providing the opportunity for new applied research to leverage existing knowledge and combine technologies and techniques in innovative ways that can address this imbalance."],"url":"http://arxiv.org/abs/2312.02845v1"}
{"created":"2023-12-05 15:53:24","title":"Are Vision Transformers More Data Hungry Than Newborn Visual Systems?","abstract":"Vision transformers (ViTs) are top performing models on many computer vision benchmarks and can accurately predict human behavior on object recognition tasks. However, researchers question the value of using ViTs as models of biological learning because ViTs are thought to be more data hungry than brains, with ViTs requiring more training data to reach similar levels of performance. To test this assumption, we directly compared the learning abilities of ViTs and animals, by performing parallel controlled rearing experiments on ViTs and newborn chicks. We first raised chicks in impoverished visual environments containing a single object, then simulated the training data available in those environments by building virtual animal chambers in a video game engine. We recorded the first-person images acquired by agents moving through the virtual chambers and used those images to train self supervised ViTs that leverage time as a teaching signal, akin to biological visual systems. When ViTs were trained through the eyes of newborn chicks, the ViTs solved the same view invariant object recognition tasks as the chicks. Thus, ViTs were not more data hungry than newborn visual systems: both learned view invariant object representations in impoverished visual environments. The flexible and generic attention based learning mechanism in ViTs combined with the embodied data streams available to newborn animals appears sufficient to drive the development of animal-like object recognition.","sentences":["Vision transformers (ViTs) are top performing models on many computer vision benchmarks and can accurately predict human behavior on object recognition tasks.","However, researchers question the value of using ViTs as models of biological learning because ViTs are thought to be more data hungry than brains, with ViTs requiring more training data to reach similar levels of performance.","To test this assumption, we directly compared the learning abilities of ViTs and animals, by performing parallel controlled rearing experiments on ViTs and newborn chicks.","We first raised chicks in impoverished visual environments containing a single object, then simulated the training data available in those environments by building virtual animal chambers in a video game engine.","We recorded the first-person images acquired by agents moving through the virtual chambers and used those images to train self supervised ViTs that leverage time as a teaching signal, akin to biological visual systems.","When ViTs were trained through the eyes of newborn chicks, the ViTs solved the same view invariant object recognition tasks as the chicks.","Thus, ViTs were not more data hungry than newborn visual systems: both learned view invariant object representations in impoverished visual environments.","The flexible and generic attention based learning mechanism in ViTs combined with the embodied data streams available to newborn animals appears sufficient to drive the development of animal-like object recognition."],"url":"http://arxiv.org/abs/2312.02843v1"}
{"created":"2023-12-05 15:45:19","title":"Low-complexity Linear Multicast Beamforming for Cache-aided MIMO Communications","abstract":"A practical and scalable multicast beamformer design in multi-input multi-output~(MIMO) coded caching~(CC) systems is introduced in this paper. The proposed approach allows multicast transmission to multiple groups with partially overlapping user sets using receiver dimensions to distinguish between different group-specific streams. Additionally, it provides flexibility in accommodating various parameter configurations of the MIMO-CC setup and overcomes practical limitations, such as the requirement to use successive interference cancellation~(SIC) at the receiver, while achieving the same degrees-of-freedom~(DoF). To evaluate the proposed scheme, we define the symmetric rate as the sum rate of the partially overlapping streams received per user, comprising a linear multistream multicast transmission vector and the linear minimum mean square error~(LMMSE) receiver. The resulting non-convex symmetric rate maximization problem is solved using alternative optimization and successive convex approximation~(SCA). Moreover, a fast iterative Lagrangian-based algorithm is developed, significantly reducing the computational overhead compared to previous designs. The effectiveness of our proposed method is demonstrated by extensive simulations.","sentences":["A practical and scalable multicast beamformer design in multi-input multi-output~(MIMO) coded caching~(CC) systems is introduced in this paper.","The proposed approach allows multicast transmission to multiple groups with partially overlapping user sets using receiver dimensions to distinguish between different group-specific streams.","Additionally, it provides flexibility in accommodating various parameter configurations of the MIMO-CC setup and overcomes practical limitations, such as the requirement to use successive interference cancellation~(SIC) at the receiver, while achieving the same degrees-of-freedom~(DoF).","To evaluate the proposed scheme, we define the symmetric rate as the sum rate of the partially overlapping streams received per user, comprising a linear multistream multicast transmission vector and the linear minimum mean square error~(LMMSE) receiver.","The resulting non-convex symmetric rate maximization problem is solved using alternative optimization and successive convex approximation~(SCA).","Moreover, a fast iterative Lagrangian-based algorithm is developed, significantly reducing the computational overhead compared to previous designs.","The effectiveness of our proposed method is demonstrated by extensive simulations."],"url":"http://arxiv.org/abs/2312.02839v1"}
{"created":"2023-12-05 15:26:14","title":"Detection of Seismic Infrasonic Elephant Rumbles Using Spectrogram-Based Machine Learning","abstract":"This paper presents an effective method of identifying elephant rumbles in infrasonic seismic signals. The design and implementation of electronic circuitry to amplify, filter, and digitize the seismic signals captured through geophones are presented. A collection of seismic infrasonic elephant rumbles was collected at a free-ranging area of an elephant orphanage in Sri Lanka. The seismic rumbles were converted to spectrograms, and several methods were used for spectral feature extraction. Using LasyPredict, the features extracted using different methods were fed into their corresponding machine-learning algorithms to train them for automatic seismic rumble identification. It was found that the Mel frequency cepstral coefficient (MFCC) together with the Ridge classifier machine learning algorithm produced the best performance in identifying seismic elephant rumbles. A novel method for denoising the spectrum that leads to enhanced accuracy in identifying seismic rumbles is also presented.","sentences":["This paper presents an effective method of identifying elephant rumbles in infrasonic seismic signals.","The design and implementation of electronic circuitry to amplify, filter, and digitize the seismic signals captured through geophones are presented.","A collection of seismic infrasonic elephant rumbles was collected at a free-ranging area of an elephant orphanage in Sri Lanka.","The seismic rumbles were converted to spectrograms, and several methods were used for spectral feature extraction.","Using LasyPredict, the features extracted using different methods were fed into their corresponding machine-learning algorithms to train them for automatic seismic rumble identification.","It was found that the Mel frequency cepstral coefficient (MFCC) together with the Ridge classifier machine learning algorithm produced the best performance in identifying seismic elephant rumbles.","A novel method for denoising the spectrum that leads to enhanced accuracy in identifying seismic rumbles is also presented."],"url":"http://arxiv.org/abs/2312.02831v1"}
{"created":"2023-12-05 15:25:45","title":"MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition","abstract":"With the advent of deep learning, progressively larger neural networks have been designed to solve complex tasks. We take advantage of these capacity-rich models to lower the cost of inference by exploiting computation in superposition. To reduce the computational burden per input, we propose Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling many inputs at once. MIMONets augment various deep neural network architectures with variable binding mechanisms to represent an arbitrary number of inputs in a compositional data structure via fixed-width distributed representations. Accordingly, MIMONets adapt nonlinear neural transformations to process the data structure holistically, leading to a speedup nearly proportional to the number of superposed input items in the data structure. After processing in superposition, an unbinding mechanism recovers each transformed input of interest. MIMONets also provide a dynamic trade-off between accuracy and throughput by an instantaneous on-demand switching between a set of accuracy-throughput operating points, yet within a single set of fixed parameters. We apply the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical evaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy delta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and CIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining a high average accuracy within a [-1.07, -3.43]% delta on the long range arena benchmark. Finally, we provide mathematical bounds on the interference between superposition channels in MIMOFormer. Our code is available at https://github.com/IBM/multiple-input-multiple-output-nets.","sentences":["With the advent of deep learning, progressively larger neural networks have been designed to solve complex tasks.","We take advantage of these capacity-rich models to lower the cost of inference by exploiting computation in superposition.","To reduce the computational burden per input, we propose Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling many inputs at once.","MIMONets augment various deep neural network architectures with variable binding mechanisms to represent an arbitrary number of inputs in a compositional data structure via fixed-width distributed representations.","Accordingly, MIMONets adapt nonlinear neural transformations to process the data structure holistically, leading to a speedup nearly proportional to the number of superposed input items in the data structure.","After processing in superposition, an unbinding mechanism recovers each transformed input of interest.","MIMONets also provide a dynamic trade-off between accuracy and throughput by an instantaneous on-demand switching between a set of accuracy-throughput operating points, yet within a single set of fixed parameters.","We apply the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMOFormer, respectively.","Empirical evaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy delta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and CIFAR100.","Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining a high average accuracy within a [-1.07, -3.43]% delta on the long range arena benchmark.","Finally, we provide mathematical bounds on the interference between superposition channels in MIMOFormer.","Our code is available at https://github.com/IBM/multiple-input-multiple-output-nets."],"url":"http://arxiv.org/abs/2312.02829v1"}
{"created":"2023-12-05 15:22:26","title":"Computing $k$-Crossing Visibility through $k$-levels","abstract":"Let $\\mathcal{A}$ be an arrangement of straight lines in the plane (or planes in $\\mathbb{R}^3$). The $k$-crossing visibility of a point $p$ on $\\mathcal{A}$ is the set of point $q$ in elements of $\\mathcal{A}$ such that the segment $pq$ intersects at most $k$ elements of $\\mathcal{A}$. In this paper, we obtain algorithms for computing the $k$-crossing visibility. In particular we obtain $O(n\\log n + kn)$ and $O(n\\log n + k^2n)$ time algorithms, for arrangements of lines in the plane and planes in $\\mathbb{R}^3$; which are optimal for $k=\\Omega(\\log n)$ and $k=\\Omega(\\sqrt{\\log n})$, respectively. We also introduce another algorithm for computing $k$-crossing visibilities on polygons, which reaches the same asymptotical time as the one presented by Bahoo et al. The ideas introduced in this paper can be easily adapted for obtaining $k$-crossing visibilities on other arrangements whose $(\\leq k)$-level is known.","sentences":["Let $\\mathcal{A}$ be an arrangement of straight lines in the plane (or planes in $\\mathbb{R}^3$).","The $k$-crossing visibility of a point $p$ on $\\mathcal{A}$ is the set of point $q$ in elements of $\\mathcal{A}$ such that the segment $pq$ intersects at most $k$ elements of $\\mathcal{A}$. In this paper, we obtain algorithms for computing the $k$-crossing visibility.","In particular we obtain $O(n\\log n + kn)$ and $O(n\\log n + k^2n)$ time algorithms, for arrangements of lines in the plane and planes in $\\mathbb{R}^3$; which are optimal for $k=\\Omega(\\log n)$ and $k=\\Omega(\\sqrt{\\log n})$, respectively.","We also introduce another algorithm for computing $k$-crossing visibilities on polygons, which reaches the same asymptotical time as the one presented by Bahoo et al.","The ideas introduced in this paper can be easily adapted for obtaining $k$-crossing visibilities on other arrangements whose $(\\leq k)$-level is known."],"url":"http://arxiv.org/abs/2312.02827v1"}
{"created":"2023-12-05 15:19:29","title":"Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault Diagnosis","abstract":"Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be an effective and flexible solution, attracting extensive research. Deep neural networks can learn rich representations from vast amounts of representative labeled data for various applications. In IFD, they achieve high classification performance from signals in an end-to-end manner, without requiring extensive domain knowledge. However, deep learning models usually only perform well on the data distribution they have been trained on. When applied to a different distribution, they may experience performance drops. This is also observed in IFD, where assets are often operated in working conditions different from those in which labeled data have been collected. Unsupervised domain adaptation (UDA) deals with the scenario where labeled data are available in a source domain, and only unlabeled data are available in a target domain, where domains may correspond to operating conditions. Recent methods rely on training with confident pseudo-labels for target samples. However, the confidence-based selection of pseudo-labels is hindered by poorly calibrated confidence estimates in the target domain, primarily due to over-confident predictions, which limits the quality of pseudo-labels and leads to error accumulation. In this paper, we propose a novel UDA method called Calibrated Adaptive Teacher (CAT), where we propose to calibrate the predictions of the teacher network throughout the self-training process, leveraging post-hoc calibration techniques. We evaluate CAT on domain-adaptive IFD and perform extensive experiments on the Paderborn benchmark for bearing fault diagnosis under varying operating conditions. Our proposed method achieves state-of-the-art performance on most transfer tasks.","sentences":["Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be an effective and flexible solution, attracting extensive research.","Deep neural networks can learn rich representations from vast amounts of representative labeled data for various applications.","In IFD, they achieve high classification performance from signals in an end-to-end manner, without requiring extensive domain knowledge.","However, deep learning models usually only perform well on the data distribution they have been trained on.","When applied to a different distribution, they may experience performance drops.","This is also observed in IFD, where assets are often operated in working conditions different from those in which labeled data have been collected.","Unsupervised domain adaptation (UDA) deals with the scenario where labeled data are available in a source domain, and only unlabeled data are available in a target domain, where domains may correspond to operating conditions.","Recent methods rely on training with confident pseudo-labels for target samples.","However, the confidence-based selection of pseudo-labels is hindered by poorly calibrated confidence estimates in the target domain, primarily due to over-confident predictions, which limits the quality of pseudo-labels and leads to error accumulation.","In this paper, we propose a novel UDA method called Calibrated Adaptive Teacher (CAT), where we propose to calibrate the predictions of the teacher network throughout the self-training process, leveraging post-hoc calibration techniques.","We evaluate CAT on domain-adaptive IFD and perform extensive experiments on the Paderborn benchmark for bearing fault diagnosis under varying operating conditions.","Our proposed method achieves state-of-the-art performance on most transfer tasks."],"url":"http://arxiv.org/abs/2312.02826v1"}
{"created":"2023-12-05 15:11:06","title":"Energy-consistent integration of mechanical systems based on Livens principle","abstract":"In this work we make us of Livens principle (sometimes also referred to as Hamilton-Pontryagin principle) in order to obtain a novel structure-preserving integrator for mechanical systems. In contrast to the canonical Hamiltonian equations of motion, the Euler-Lagrange equations pertaining to Livens principle circumvent the need to invert the mass matrix. This is an essential advantage with respect to singular mass matrices, which can yield severe difficulties for the modelling and simulation of multibody systems. Moreover, Livens principle unifies both Lagrangian and Hamiltonian viewpoints on mechanics. Additionally, the present framework avoids the need to set up the system's Hamiltonian. The novel scheme algorithmically conserves a general energy function and aims at the preservation of momentum maps corresponding to symmetries of the system. We present an extension to mechanical systems subject to holonomic constraints. The performance of the newly devised method is studied in representative examples.","sentences":["In this work we make us of Livens principle (sometimes also referred to as Hamilton-Pontryagin principle) in order to obtain a novel structure-preserving integrator for mechanical systems.","In contrast to the canonical Hamiltonian equations of motion, the Euler-Lagrange equations pertaining to Livens principle circumvent the need to invert the mass matrix.","This is an essential advantage with respect to singular mass matrices, which can yield severe difficulties for the modelling and simulation of multibody systems.","Moreover, Livens principle unifies both Lagrangian and Hamiltonian viewpoints on mechanics.","Additionally, the present framework avoids the need to set up the system's Hamiltonian.","The novel scheme algorithmically conserves a general energy function and aims at the preservation of momentum maps corresponding to symmetries of the system.","We present an extension to mechanical systems subject to holonomic constraints.","The performance of the newly devised method is studied in representative examples."],"url":"http://arxiv.org/abs/2312.02825v1"}
{"created":"2023-12-05 15:06:04","title":"RotaTR: Detection Transformer for Dense and Rotated Object","abstract":"Detecting the objects in dense and rotated scenes is a challenging task. Recent works on this topic are mostly based on Faster RCNN or Retinanet. As they are highly dependent on the pre-set dense anchors and the NMS operation, the approach is indirect and suboptimal.The end-to-end DETR-based detectors have achieved great success in horizontal object detection and many other areas like segmentation, tracking, action recognition and etc.However, the DETR-based detectors perform poorly on dense rotated target tasks and perform worse than most modern CNN-based detectors. In this paper, we find the most significant reason for the poor performance is that the original attention can not accurately focus on the oriented targets. Accordingly, we propose Rotated object detection TRansformer (RotaTR) as an extension of DETR to oriented detection. Specifically, we design Rotation Sensitive deformable (RSDeform) attention to enhance the DETR's ability to detect oriented targets. It is used to build the feature alignment module and rotation-sensitive decoder for our model. We test RotaTR on four challenging-oriented benchmarks. It shows a great advantage in detecting dense and oriented objects compared to the original DETR. It also achieves competitive results when compared to the state-of-the-art.","sentences":["Detecting the objects in dense and rotated scenes is a challenging task.","Recent works on this topic are mostly based on Faster RCNN or Retinanet.","As they are highly dependent on the pre-set dense anchors and the NMS operation, the approach is indirect and suboptimal.","The end-to-end DETR-based detectors have achieved great success in horizontal object detection and many other areas like segmentation, tracking, action recognition and etc.","However, the DETR-based detectors perform poorly on dense rotated target tasks and perform worse than most modern CNN-based detectors.","In this paper, we find the most significant reason for the poor performance is that the original attention can not accurately focus on the oriented targets.","Accordingly, we propose Rotated object detection TRansformer (RotaTR) as an extension of DETR to oriented detection.","Specifically, we design Rotation Sensitive deformable (RSDeform) attention to enhance the DETR's ability to detect oriented targets.","It is used to build the feature alignment module and rotation-sensitive decoder for our model.","We test RotaTR on four challenging-oriented benchmarks.","It shows a great advantage in detecting dense and oriented objects compared to the original DETR.","It also achieves competitive results when compared to the state-of-the-art."],"url":"http://arxiv.org/abs/2312.02821v1"}
{"created":"2023-12-05 15:03:27","title":"Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix","abstract":"In multilingual translation research, the comprehension and utilization of language families are of paramount importance. Nevertheless, clustering languages based solely on their ancestral families can yield suboptimal results due to variations in the datasets employed during the model's training phase. To mitigate this challenge, we introduce an innovative method that leverages the fisher information matrix (FIM) to cluster language families, anchored on the multilingual translation model's characteristics. We hypothesize that language pairs with similar effects on model parameters exhibit a considerable degree of linguistic congruence and should thus be grouped cohesively. This concept has led us to define pseudo language families. We provide an in-depth discussion regarding the inception and application of these pseudo language families. Empirical evaluations reveal that employing these pseudo language families enhances performance over conventional language families in adapting a multilingual translation model to unfamiliar language pairs. The proposed methodology may also be extended to scenarios requiring language similarity measurements. The source code and associated scripts can be accessed at https://github.com/ecoli-hit/PseudoFamily.","sentences":["In multilingual translation research, the comprehension and utilization of language families are of paramount importance.","Nevertheless, clustering languages based solely on their ancestral families can yield suboptimal results due to variations in the datasets employed during the model's training phase.","To mitigate this challenge, we introduce an innovative method that leverages the fisher information matrix (FIM) to cluster language families, anchored on the multilingual translation model's characteristics.","We hypothesize that language pairs with similar effects on model parameters exhibit a considerable degree of linguistic congruence and should thus be grouped cohesively.","This concept has led us to define pseudo language families.","We provide an in-depth discussion regarding the inception and application of these pseudo language families.","Empirical evaluations reveal that employing these pseudo language families enhances performance over conventional language families in adapting a multilingual translation model to unfamiliar language pairs.","The proposed methodology may also be extended to scenarios requiring language similarity measurements.","The source code and associated scripts can be accessed at https://github.com/ecoli-hit/PseudoFamily."],"url":"http://arxiv.org/abs/2312.02820v1"}
{"created":"2023-12-05 15:03:15","title":"Deterministic Guidance Diffusion Model for Probabilistic Weather Forecasting","abstract":"Weather forecasting requires not only accuracy but also the ability to perform probabilistic prediction. However, deterministic weather forecasting methods do not support probabilistic predictions, and conversely, probabilistic models tend to be less accurate. To address these challenges, in this paper, we introduce the \\textbf{\\textit{D}}eterministic \\textbf{\\textit{G}}uidance \\textbf{\\textit{D}}iffusion \\textbf{\\textit{M}}odel (DGDM) for probabilistic weather forecasting, integrating benefits of both deterministic and probabilistic approaches. During the forward process, both the deterministic and probabilistic models are trained end-to-end. In the reverse process, weather forecasting leverages the predicted result from the deterministic model, using as an intermediate starting point for the probabilistic model. By fusing deterministic models with probabilistic models in this manner, DGDM is capable of providing accurate forecasts while also offering probabilistic predictions. To evaluate DGDM, we assess it on the global weather forecasting dataset (WeatherBench) and the common video frame prediction benchmark (Moving MNIST). We also introduce and evaluate the Pacific Northwest Windstorm (PNW)-Typhoon weather satellite dataset to verify the effectiveness of DGDM in high-resolution regional forecasting. As a result of our experiments, DGDM achieves state-of-the-art results not only in global forecasting but also in regional forecasting. The code is available at: \\url{https://github.com/DongGeun-Yoon/DGDM}.","sentences":["Weather forecasting requires not only accuracy but also the ability to perform probabilistic prediction.","However, deterministic weather forecasting methods do not support probabilistic predictions, and conversely, probabilistic models tend to be less accurate.","To address these challenges, in this paper, we introduce the \\textbf{\\textit{D}}eterministic \\textbf{\\textit{G}}uidance \\textbf{\\textit{D}}iffusion \\textbf{\\textit{M}}odel (DGDM) for probabilistic weather forecasting, integrating benefits of both deterministic and probabilistic approaches.","During the forward process, both the deterministic and probabilistic models are trained end-to-end.","In the reverse process, weather forecasting leverages the predicted result from the deterministic model, using as an intermediate starting point for the probabilistic model.","By fusing deterministic models with probabilistic models in this manner, DGDM is capable of providing accurate forecasts while also offering probabilistic predictions.","To evaluate DGDM, we assess it on the global weather forecasting dataset (WeatherBench) and the common video frame prediction benchmark (Moving MNIST).","We also introduce and evaluate the Pacific Northwest Windstorm (PNW)-Typhoon weather satellite dataset to verify the effectiveness of DGDM in high-resolution regional forecasting.","As a result of our experiments, DGDM achieves state-of-the-art results not only in global forecasting but also in regional forecasting.","The code is available at: \\url{https://github.com/DongGeun-Yoon/DGDM}."],"url":"http://arxiv.org/abs/2312.02819v1"}
{"created":"2023-12-05 14:56:55","title":"BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models","abstract":"Diffusion models have made tremendous progress in text-driven image and video generation. Now text-to-image foundation models are widely applied to various downstream image synthesis tasks, such as controllable image generation and image editing, while downstream video synthesis tasks are less explored for several reasons. First, it requires huge memory and compute overhead to train a video generation foundation model. Even with video foundation models, additional costly training is still required for downstream video synthesis tasks. Second, although some works extend image diffusion models into videos in a training-free manner, temporal consistency cannot be well kept. Finally, these adaption methods are specifically designed for one task and fail to generalize to different downstream video synthesis tasks. To mitigate these issues, we propose a training-free general-purpose video synthesis framework, coined as BIVDiff, via bridging specific image diffusion models and general text-to-video foundation diffusion models. Specifically, we first use an image diffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video generation, then perform Mixed Inversion on the generated video, and finally input the inverted latents into the video diffusion model for temporal smoothing. Decoupling image and video models enables flexible image model selection for different purposes, which endows the framework with strong task generalization and high efficiency. To validate the effectiveness and general use of BIVDiff, we perform a wide range of video generation tasks, including controllable video generation video editing, video inpainting and outpainting. Our project page is available at https://bivdiff.github.io.","sentences":["Diffusion models have made tremendous progress in text-driven image and video generation.","Now text-to-image foundation models are widely applied to various downstream image synthesis tasks, such as controllable image generation and image editing, while downstream video synthesis tasks are less explored for several reasons.","First, it requires huge memory and compute overhead to train a video generation foundation model.","Even with video foundation models, additional costly training is still required for downstream video synthesis tasks.","Second, although some works extend image diffusion models into videos in a training-free manner, temporal consistency cannot be well kept.","Finally, these adaption methods are specifically designed for one task and fail to generalize to different downstream video synthesis tasks.","To mitigate these issues, we propose a training-free general-purpose video synthesis framework, coined as BIVDiff, via bridging specific image diffusion models and general text-to-video foundation diffusion models.","Specifically, we first use an image diffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video generation, then perform Mixed Inversion on the generated video, and finally input the inverted latents into the video diffusion model for temporal smoothing.","Decoupling image and video models enables flexible image model selection for different purposes, which endows the framework with strong task generalization and high efficiency.","To validate the effectiveness and general use of BIVDiff, we perform a wide range of video generation tasks, including controllable video generation video editing, video inpainting and outpainting.","Our project page is available at https://bivdiff.github.io."],"url":"http://arxiv.org/abs/2312.02813v1"}
{"created":"2023-12-05 14:56:12","title":"Simulating Vision Impairment in Virtual Reality -- A Comparison of Visual Task Performance with Real and Simulated Tunnel Vision","abstract":"Purpose: In this work, we explore the potential and limitations of simulating gaze-contingent tunnel vision conditions using Virtual Reality (VR) with built-in eye tracking technology. This approach promises an easy and accessible way of expanding study populations and test groups for visual training, visual aids, or accessibility evaluations. However, it is crucial to assess the validity and reliability of simulating these types of visual impairments and evaluate the extend to which participants with simulated tunnel vision can represent real patients. Methods: Two age-matched participant groups were acquired: The first group (n=8 aged 20-60, average 49.1, sd 13.2) consisted of patients diagnosed with Retinitis pigmentosa (RP). The second group (n=8, aged 27-59, average 46.5, sd 10.8) consisted of visually healthy participants with simulated tunnel vision. Both groups carried out different visual tasks in a virtual environment for 30 minutes per day over the course of four weeks. Task performances as well as gaze characteristics were evaluated in both groups over the course of the study. Results: Using the \"two one-sided tests for equivalence\" method, the two groups were found to perform similar in all three visual tasks. Significant differences between groups were found in different aspects of their gaze behavior, though most of these aspects seem to converge over time. Conclusion: Our study evaluates the potential and limitations of using Virtual Reality technology to simulate the effects of tunnel vision within controlled virtual environments. We find that the simulation accurately represents performance of RP patients in the context of group averages, but fails to fully replicate effects on gaze behavior.","sentences":["Purpose: In this work, we explore the potential and limitations of simulating gaze-contingent tunnel vision conditions using Virtual Reality (VR) with built-in eye tracking technology.","This approach promises an easy and accessible way of expanding study populations and test groups for visual training, visual aids, or accessibility evaluations.","However, it is crucial to assess the validity and reliability of simulating these types of visual impairments and evaluate the extend to which participants with simulated tunnel vision can represent real patients.","Methods: Two age-matched participant groups were acquired: The first group (n=8 aged 20-60, average 49.1, sd 13.2) consisted of patients diagnosed with Retinitis pigmentosa (RP).","The second group (n=8, aged 27-59, average 46.5, sd 10.8) consisted of visually healthy participants with simulated tunnel vision.","Both groups carried out different visual tasks in a virtual environment for 30 minutes per day over the course of four weeks.","Task performances as well as gaze characteristics were evaluated in both groups over the course of the study.","Results: Using the \"two one-sided tests for equivalence\" method, the two groups were found to perform similar in all three visual tasks.","Significant differences between groups were found in different aspects of their gaze behavior, though most of these aspects seem to converge over time.","Conclusion: Our study evaluates the potential and limitations of using Virtual Reality technology to simulate the effects of tunnel vision within controlled virtual environments.","We find that the simulation accurately represents performance of RP patients in the context of group averages, but fails to fully replicate effects on gaze behavior."],"url":"http://arxiv.org/abs/2312.02812v1"}
{"created":"2023-12-05 14:52:33","title":"Using the SP!CE Framework to Code Influence Campaign Activity on Social Media: Case Study on the 2022 Brazilian Presidential Election","abstract":"We describe a case study in the use of the Structured Process for Information Campaign Enhancement (SP!CE, version 2.1) to evaluate influence campaigns present in the 2nd round of the Brazilian presidential election in 2022 October. SP!CE is a US-military focused framework for describing both friendly and adversary actions in influence campaigns, and is inter-operable with the Disinformation Analysis and Risk Management (DISARM) framework. The purpose of the case study is to demonstrate how SP!CE can be used to describe influence campaign behaviors. We selected the Brazilian election as the target of the case study as it is known that there were significant amounts of mis- and disinformation present on social media during the campaigns. Our goal was to demonstrate how SP!CE could be applied in such a context, showing how social media content could be aligned with information campaign behaviors and how such an alignment can be used to analyze which mis- and disinformation narratives were in play. Additionally, we aim to provide insights on best practices regarding how to apply the framework in further research. We release the coding and screenshots of the relevant social media posts to support future research.","sentences":["We describe a case study in the use of the Structured Process for Information Campaign Enhancement (SP!CE, version 2.1) to evaluate influence campaigns present in the 2nd round of the Brazilian presidential election in 2022 October.","SP!CE is a US-military focused framework for describing both friendly and adversary actions in influence campaigns, and is inter-operable with the Disinformation Analysis and Risk Management (DISARM) framework.","The purpose of the case study is to demonstrate how SP!CE can be used to describe influence campaign behaviors.","We selected the Brazilian election as the target of the case study as it is known that there were significant amounts of mis- and disinformation present on social media during the campaigns.","Our goal was to demonstrate how SP!CE could be applied in such a context, showing how social media content could be aligned with information campaign behaviors and how such an alignment can be used to analyze which mis- and disinformation narratives were in play.","Additionally, we aim to provide insights on best practices regarding how to apply the framework in further research.","We release the coding and screenshots of the relevant social media posts to support future research."],"url":"http://arxiv.org/abs/2312.02810v1"}
{"created":"2023-12-05 14:44:58","title":"Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems","abstract":"Stochastic networks and queueing systems often lead to Markov decision processes (MDPs) with large state and action spaces as well as nonconvex objective functions, which hinders the convergence of many reinforcement learning (RL) algorithms. Policy-gradient methods perform well on MDPs with large state and action spaces, but they sometimes experience slow convergence due to the high variance of the gradient estimator. In this paper, we show that some of these difficulties can be circumvented by exploiting the structure of the underlying MDP. We first introduce a new family of gradient estimators called score-aware gradient estimators (SAGEs). When the stationary distribution of the MDP belongs to an exponential family parametrized by the policy parameters, SAGEs allow us to estimate the policy gradient without relying on value-function estimation, contrary to classical policy-gradient methods like actor-critic. To demonstrate their applicability, we examine two common control problems arising in stochastic networks and queueing systems whose stationary distributions have a product-form, a special case of exponential families. As a second contribution, we show that, under appropriate assumptions, the policy under a SAGE-based policy-gradient method has a large probability of converging to an optimal policy, provided that it starts sufficiently close to it, even with a nonconvex objective function and multiple maximizers. Our key assumptions are that, locally around a maximizer, a nondegeneracy property of the Hessian of the objective function holds and a Lyapunov function exists. Finally, we conduct a numerical comparison between a SAGE-based policy-gradient method and an actor-critic algorithm. The results demonstrate that the SAGE-based method finds close-to-optimal policies more rapidly, highlighting its superior performance over the traditional actor-critic method.","sentences":["Stochastic networks and queueing systems often lead to Markov decision processes (MDPs) with large state and action spaces as well as nonconvex objective functions, which hinders the convergence of many reinforcement learning (RL) algorithms.","Policy-gradient methods perform well on MDPs with large state and action spaces, but they sometimes experience slow convergence due to the high variance of the gradient estimator.","In this paper, we show that some of these difficulties can be circumvented by exploiting the structure of the underlying MDP.","We first introduce a new family of gradient estimators called score-aware gradient estimators (SAGEs).","When the stationary distribution of the MDP belongs to an exponential family parametrized by the policy parameters, SAGEs allow us to estimate the policy gradient without relying on value-function estimation, contrary to classical policy-gradient methods like actor-critic.","To demonstrate their applicability, we examine two common control problems arising in stochastic networks and queueing systems whose stationary distributions have a product-form, a special case of exponential families.","As a second contribution, we show that, under appropriate assumptions, the policy under a SAGE-based policy-gradient method has a large probability of converging to an optimal policy, provided that it starts sufficiently close to it, even with a nonconvex objective function and multiple maximizers.","Our key assumptions are that, locally around a maximizer, a nondegeneracy property of the Hessian of the objective function holds and a Lyapunov function exists.","Finally, we conduct a numerical comparison between a SAGE-based policy-gradient method and an actor-critic algorithm.","The results demonstrate that the SAGE-based method finds close-to-optimal policies more rapidly, highlighting its superior performance over the traditional actor-critic method."],"url":"http://arxiv.org/abs/2312.02804v1"}
{"created":"2023-12-05 14:44:08","title":"Leveraging Domain Adaptation and Data Augmentation to Improve Qur'anic IR in English and Arabic","abstract":"In this work, we approach the problem of Qur'anic information retrieval (IR) in Arabic and English. Using the latest state-of-the-art methods in neural IR, we research what helps to tackle this task more efficiently. Training retrieval models requires a lot of data, which is difficult to obtain for training in-domain. Therefore, we commence with training on a large amount of general domain data and then continue training on in-domain data. To handle the lack of in-domain data, we employed a data augmentation technique, which considerably improved results in MRR@10 and NDCG@5 metrics, setting the state-of-the-art in Qur'anic IR for both English and Arabic. The absence of an Islamic corpus and domain-specific model for IR task in English motivated us to address this lack of resources and take preliminary steps of the Islamic corpus compilation and domain-specific language model (LM) pre-training, which helped to improve the performance of the retrieval models that use the domain-specific LM as the shared backbone. We examined several language models (LMs) in Arabic to select one that efficiently deals with the Qur'anic IR task. Besides transferring successful experiments from English to Arabic, we conducted additional experiments with retrieval task in Arabic to amortize the scarcity of general domain datasets used to train the retrieval models. Handling Qur'anic IR task combining English and Arabic allowed us to enhance the comparison and share valuable insights across models and languages.","sentences":["In this work, we approach the problem of Qur'anic information retrieval (IR) in Arabic and English.","Using the latest state-of-the-art methods in neural IR, we research what helps to tackle this task more efficiently.","Training retrieval models requires a lot of data, which is difficult to obtain for training in-domain.","Therefore, we commence with training on a large amount of general domain data and then continue training on in-domain data.","To handle the lack of in-domain data, we employed a data augmentation technique, which considerably improved results in MRR@10 and NDCG@5 metrics, setting the state-of-the-art in Qur'anic IR for both English and Arabic.","The absence of an Islamic corpus and domain-specific model for IR task in English motivated us to address this lack of resources and take preliminary steps of the Islamic corpus compilation and domain-specific language model (LM) pre-training, which helped to improve the performance of the retrieval models that use the domain-specific LM as the shared backbone.","We examined several language models (LMs) in Arabic to select one that efficiently deals with the Qur'anic IR task.","Besides transferring successful experiments from English to Arabic, we conducted additional experiments with retrieval task in Arabic to amortize the scarcity of general domain datasets used to train the retrieval models.","Handling Qur'anic IR task combining English and Arabic allowed us to enhance the comparison and share valuable insights across models and languages."],"url":"http://arxiv.org/abs/2312.02803v1"}
{"created":"2023-12-05 14:35:11","title":"Weakly Supervised Detection of Hallucinations in LLM Activations","abstract":"We propose an auditing method to identify whether a large language model (LLM) encodes patterns such as hallucinations in its internal states, which may propagate to downstream tasks. We introduce a weakly supervised auditing technique using a subset scanning approach to detect anomalous patterns in LLM activations from pre-trained models. Importantly, our method does not need knowledge of the type of patterns a-priori. Instead, it relies on a reference dataset devoid of anomalies during testing. Further, our approach enables the identification of pivotal nodes responsible for encoding these patterns, which may offer crucial insights for fine-tuning specific sub-networks for bias mitigation. We introduce two new scanning methods to handle LLM activations for anomalous sentences that may deviate from the expected distribution in either direction. Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally. Importantly, our scanning approach, without prior exposure to false statements, performs comparably to a fully supervised out-of-distribution classifier.","sentences":["We propose an auditing method to identify whether a large language model (LLM) encodes patterns such as hallucinations in its internal states, which may propagate to downstream tasks.","We introduce a weakly supervised auditing technique using a subset scanning approach to detect anomalous patterns in LLM activations from pre-trained models.","Importantly, our method does not need knowledge of the type of patterns a-priori.","Instead, it relies on a reference dataset devoid of anomalies during testing.","Further, our approach enables the identification of pivotal nodes responsible for encoding these patterns, which may offer crucial insights for fine-tuning specific sub-networks for bias mitigation.","We introduce two new scanning methods to handle LLM activations for anomalous sentences that may deviate from the expected distribution in either direction.","Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally.","Importantly, our scanning approach, without prior exposure to false statements, performs comparably to a fully supervised out-of-distribution classifier."],"url":"http://arxiv.org/abs/2312.02798v1"}
{"created":"2023-12-05 14:20:46","title":"A Low-cost, High-impact Node Injection Approach for Attacking Social Network Alignment","abstract":"Social network alignment (SNA) holds significant importance for various downstream applications, prompting numerous professionals to develop and share SNA tools. Unfortunately, these tools can be exploited by malicious actors to integrate sensitive user information, posing cybersecurity risks. While many researchers have explored attacking SNA (ASNA) through a network modification attack way, practical feasibility remains a challenge. This paper introduces a novel approach, the node injection attack. To overcome the problem of modeling and solving within a limited time and balancing costs and benefits, we propose a low-cost, high-impact node injection attack via dynamic programming (DPNIA) framework. DPNIA models ASNA as a problem of maximizing the number of confirmed incorrect correspondent node pairs who have a greater similarity scores than the pairs between existing nodes, making ASNA solvable. Meanwhile, it employs a cross-network evaluation method to identify node vulnerability, facilitating a progressive attack from easy to difficult. Additionally, it utilizes an optimal injection strategy searching method, based on dynamic programming, to determine which links should be added between injected nodes and existing nodes, thereby achieving a high impact for attack effectiveness at a low cost. Experiments on four real-world datasets consistently demonstrate that DPNIA consistently and significantly outperforms various attack baselines.","sentences":["Social network alignment (SNA) holds significant importance for various downstream applications, prompting numerous professionals to develop and share SNA tools.","Unfortunately, these tools can be exploited by malicious actors to integrate sensitive user information, posing cybersecurity risks.","While many researchers have explored attacking SNA (ASNA) through a network modification attack way, practical feasibility remains a challenge.","This paper introduces a novel approach, the node injection attack.","To overcome the problem of modeling and solving within a limited time and balancing costs and benefits, we propose a low-cost, high-impact node injection attack via dynamic programming (DPNIA) framework.","DPNIA models ASNA as a problem of maximizing the number of confirmed incorrect correspondent node pairs who have a greater similarity scores than the pairs between existing nodes, making ASNA solvable.","Meanwhile, it employs a cross-network evaluation method to identify node vulnerability, facilitating a progressive attack from easy to difficult.","Additionally, it utilizes an optimal injection strategy searching method, based on dynamic programming, to determine which links should be added between injected nodes and existing nodes, thereby achieving a high impact for attack effectiveness at a low cost.","Experiments on four real-world datasets consistently demonstrate that DPNIA consistently and significantly outperforms various attack baselines."],"url":"http://arxiv.org/abs/2312.02790v1"}
{"created":"2023-12-05 14:14:27","title":"Large Language Models on Graphs: A Comprehensive Survey","abstract":"Large language models (LLMs), such as ChatGPT and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data are associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data are paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graph scenarios (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-rich graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we mention the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.","sentences":["Large language models (LLMs), such as ChatGPT and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning).","While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data are associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data are paired with rich textual information (e.g., molecules with descriptions).","Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graph scenarios (i.e., graph-based reasoning).","In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs.","We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-rich graphs, and text-paired graphs.","We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models.","Furthermore, we mention the real-world applications of such methods and summarize open-source codes and benchmark datasets.","Finally, we conclude with potential future research directions in this fast-growing field.","The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs."],"url":"http://arxiv.org/abs/2312.02783v1"}
{"created":"2023-12-05 14:12:38","title":"PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features","abstract":"Speech-driven 3D facial animation has improved a lot recently while most related works only utilize acoustic modality and neglect the influence of visual and textual cues, leading to unsatisfactory results in terms of precision and coherence. We argue that visual and textual cues are not trivial information. Therefore, we present a novel framework, namely PMMTalk, using complementary Pseudo Multi-Modal features for improving the accuracy of facial animation. The framework entails three modules: PMMTalk encoder, cross-modal alignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder employs the off-the-shelf talking head generation architecture and speech recognition technology to extract visual and textual information from speech, respectively. Subsequently, the cross-modal alignment module aligns the audio-image-text features at temporal and semantic levels. Then PMMTalk decoder is employed to predict lip-syncing facial blendshape coefficients. Contrary to prior methods, PMMTalk only requires an additional random reference face image but yields more accurate results. Additionally, it is artist-friendly as it seamlessly integrates into standard animation production workflows by introducing facial blendshape coefficients. Finally, given the scarcity of 3D talking face datasets, we introduce a large-scale 3D Chinese Audio-Visual Facial Animation (3D-CAVFA) dataset. Extensive experiments and user studies show that our approach outperforms the state of the art. We recommend watching the supplementary video.","sentences":["Speech-driven 3D facial animation has improved a lot recently while most related works only utilize acoustic modality and neglect the influence of visual and textual cues, leading to unsatisfactory results in terms of precision and coherence.","We argue that visual and textual cues are not trivial information.","Therefore, we present a novel framework, namely PMMTalk, using complementary Pseudo Multi-Modal features for improving the accuracy of facial animation.","The framework entails three modules: PMMTalk encoder, cross-modal alignment module, and PMMTalk decoder.","Specifically, the PMMTalk encoder employs the off-the-shelf talking head generation architecture and speech recognition technology to extract visual and textual information from speech, respectively.","Subsequently, the cross-modal alignment module aligns the audio-image-text features at temporal and semantic levels.","Then PMMTalk decoder is employed to predict lip-syncing facial blendshape coefficients.","Contrary to prior methods, PMMTalk only requires an additional random reference face image but yields more accurate results.","Additionally, it is artist-friendly as it seamlessly integrates into standard animation production workflows by introducing facial blendshape coefficients.","Finally, given the scarcity of 3D talking face datasets, we introduce a large-scale 3D Chinese Audio-Visual Facial Animation (3D-CAVFA) dataset.","Extensive experiments and user studies show that our approach outperforms the state of the art.","We recommend watching the supplementary video."],"url":"http://arxiv.org/abs/2312.02781v1"}
{"created":"2023-12-05 14:12:15","title":"Scaling Laws for Adversarial Attacks on Language Model Activations","abstract":"We explore a class of adversarial attacks targeting the activations of language models. By manipulating a relatively small subset of model activations, $a$, we demonstrate the ability to control the exact prediction of a significant number (in some cases up to 1000) of subsequent tokens $t$. We empirically verify a scaling law where the maximum number of target tokens $t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose activations the attacker controls as $t_\\mathrm{max} = \\kappa a$. We find that the number of bits of control in the input space needed to control a single bit in the output space (what we call attack resistance $\\chi$) is remarkably constant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of model sizes for different language models. Compared to attacks on tokens, attacks on activations are predictably much stronger, however, we identify a surprising regularity where one bit of input steered either via activations or via tokens is able to exert control over a similar amount of output bits. This gives support for the hypothesis that adversarial attacks are a consequence of dimensionality mismatch between the input and output spaces. A practical implication of the ease of attacking language model activations instead of tokens is for multi-modal and selected retrieval models, where additional data sources are added as activations directly, sidestepping the tokenized input. This opens up a new, broad attack surface. By using language models as a controllable test-bed to study adversarial attacks, we were able to experiment with input-output dimensions that are inaccessible in computer vision, especially where the output dimension dominates.","sentences":["We explore a class of adversarial attacks targeting the activations of language models.","By manipulating a relatively small subset of model activations, $a$, we demonstrate the ability to control the exact prediction of a significant number (in some cases up to 1000) of subsequent tokens $t$. We empirically verify a scaling law where the maximum number of target tokens $t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose activations the attacker controls as $t_\\mathrm{max} = \\kappa a$.","We find that the number of bits of control in the input space needed to control a single bit in the output space (what we call attack resistance $\\chi$) is remarkably constant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of model sizes for different language models.","Compared to attacks on tokens, attacks on activations are predictably much stronger, however, we identify a surprising regularity where one bit of input steered either via activations or via tokens is able to exert control over a similar amount of output bits.","This gives support for the hypothesis that adversarial attacks are a consequence of dimensionality mismatch between the input and output spaces.","A practical implication of the ease of attacking language model activations instead of tokens is for multi-modal and selected retrieval models, where additional data sources are added as activations directly, sidestepping the tokenized input.","This opens up a new, broad attack surface.","By using language models as a controllable test-bed to study adversarial attacks, we were able to experiment with input-output dimensions that are inaccessible in computer vision, especially where the output dimension dominates."],"url":"http://arxiv.org/abs/2312.02780v1"}
{"created":"2023-12-05 14:10:00","title":"On Age of Information and Energy-Transfer in a STAR-RIS-assisted System","abstract":"Battery-limited devices and time-sensitive applications are considered as key players in the forthcoming wireless sensor network. Therefore, the main goal of the network is two-fold; Charge battery-limited devices, and provide status updates to users where information-freshness matters. In this paper, a multi-antenna base station (BS) in assistance of simultaneously-transmitting-and-reflecting reconfigurable intelligent surface (STAR-RIS) transmits power to energy-harvesting devices while controlling status update performance at information-users by analyzing age of information (AoI) metric. Therefore, we derive a scheduling policy at BS, and analyze joint transmit beamforming and amplitude-phase optimization at BS and STAR-RIS, respectively, to reduce average sum-AoI for the time-sensitive information-users while satisfying minimum required energy at energy-harvesting users. Moreover, two different energy-splitting and mode switching policies at STAR-RIS are studied. Then, by use of an alternating optimization algorithm, the optimization problem is studied and non-convexity of the problem is tackled by using the successive convex approximation technique. Through numerical results, AoI-metric and energy harvesting requirements of the network are analyzed versus different parameters such as number of antennas at BS, size of STAR-RIS, and transmitted power to highlight how we can improve two fold performance of the system by utilizing STAR-RIS compared to the conventional RIS structure.","sentences":["Battery-limited devices and time-sensitive applications are considered as key players in the forthcoming wireless sensor network.","Therefore, the main goal of the network is two-fold; Charge battery-limited devices, and provide status updates to users where information-freshness matters.","In this paper, a multi-antenna base station (BS) in assistance of simultaneously-transmitting-and-reflecting reconfigurable intelligent surface (STAR-RIS) transmits power to energy-harvesting devices while controlling status update performance at information-users by analyzing age of information (AoI) metric.","Therefore, we derive a scheduling policy at BS, and analyze joint transmit beamforming and amplitude-phase optimization at BS and STAR-RIS, respectively, to reduce average sum-AoI for the time-sensitive information-users while satisfying minimum required energy at energy-harvesting users.","Moreover, two different energy-splitting and mode switching policies at STAR-RIS are studied.","Then, by use of an alternating optimization algorithm, the optimization problem is studied and non-convexity of the problem is tackled by using the successive convex approximation technique.","Through numerical results, AoI-metric and energy harvesting requirements of the network are analyzed versus different parameters such as number of antennas at BS, size of STAR-RIS, and transmitted power to highlight how we can improve two fold performance of the system by utilizing STAR-RIS compared to the conventional RIS structure."],"url":"http://arxiv.org/abs/2312.02776v1"}
