{"created":"2023-08-23 17:59:11","title":"CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images","abstract":"We present a method for teaching machines to understand and model the underlying spatial common sense of diverse human-object interactions in 3D in a self-supervised way. This is a challenging task, as there exist specific manifolds of the interactions that can be considered human-like and natural, but the human pose and the geometry of objects can vary even for similar interactions. Such diversity makes the annotating task of 3D interactions difficult and hard to scale, which limits the potential to reason about that in a supervised way. One way of learning the 3D spatial relationship between humans and objects during interaction is by showing multiple 2D images captured from different viewpoints when humans interact with the same type of objects. The core idea of our method is to leverage a generative model that produces high-quality 2D images from an arbitrary text prompt input as an \"unbounded\" data generator with effective controllability and view diversity. Despite its imperfection of the image quality over real images, we demonstrate that the synthesized images are sufficient to learn the 3D human-object spatial relations. We present multiple strategies to leverage the synthesized images, including (1) the first method to leverage a generative image model for 3D human-object spatial relation learning; (2) a framework to reason about the 3D spatial relations from inconsistent 2D cues in a self-supervised manner via 3D occupancy reasoning with pose canonicalization; (3) semantic clustering to disambiguate different types of interactions with the same object types; and (4) a novel metric to assess the quality of 3D spatial learning of interaction. Project Page: https://jellyheadandrew.github.io/projects/chorus","sentences":["We present a method for teaching machines to understand and model the underlying spatial common sense of diverse human-object interactions in 3D in a self-supervised way.","This is a challenging task, as there exist specific manifolds of the interactions that can be considered human-like and natural, but the human pose and the geometry of objects can vary even for similar interactions.","Such diversity makes the annotating task of 3D interactions difficult and hard to scale, which limits the potential to reason about that in a supervised way.","One way of learning the 3D spatial relationship between humans and objects during interaction is by showing multiple 2D images captured from different viewpoints when humans interact with the same type of objects.","The core idea of our method is to leverage a generative model that produces high-quality 2D images from an arbitrary text prompt input as an \"unbounded\" data generator with effective controllability and view diversity.","Despite its imperfection of the image quality over real images, we demonstrate that the synthesized images are sufficient to learn the 3D human-object spatial relations.","We present multiple strategies to leverage the synthesized images, including (1) the first method to leverage a generative image model for 3D human-object spatial relation learning; (2) a framework to reason about the 3D spatial relations from inconsistent 2D cues in a self-supervised manner via 3D occupancy reasoning with pose canonicalization; (3) semantic clustering to disambiguate different types of interactions with the same object types; and (4) a novel metric to assess the quality of 3D spatial learning of interaction.","Project Page: https://jellyheadandrew.github.io/projects/chorus"],"url":"http://arxiv.org/abs/2308.12288v1"}
{"created":"2023-08-23 17:58:40","title":"Devising and Detecting Phishing: large language models vs. Smaller Human Models","abstract":"AI programs, built using large language models, make it possible to automatically create phishing emails based on a few data points about a user. They stand in contrast to traditional phishing emails that hackers manually design using general rules gleaned from experience. The V-Triad is an advanced set of rules for manually designing phishing emails to exploit our cognitive heuristics and biases. In this study, we compare the performance of phishing emails created automatically by GPT-4 and manually using the V-Triad. We also combine GPT-4 with the V-Triad to assess their combined potential. A fourth group, exposed to generic phishing emails, was our control group. We utilized a factorial approach, sending emails to 112 randomly selected participants recruited for the study. The control group emails received a click-through rate between 19-28%, the GPT-generated emails 30-44%, emails generated by the V-Triad 69-79%, and emails generated by GPT and the V-Triad 43-81%. Each participant was asked to explain for why they pressed or did not press a link in the email. These answers often contradict each other, highlighting the need for personalized content. The cues that make one person avoid phishing emails make another person fall for them. Next, we used four popular large language models (GPT, Claude, PaLM, and LLaMA) to detect the intention of phishing emails and compare the results to human detection. The language models demonstrated a strong ability to detect malicious intent, even in non-obvious phishing emails. They sometimes surpassed human detection, although often being slightly less accurate than humans.","sentences":["AI programs, built using large language models, make it possible to automatically create phishing emails based on a few data points about a user.","They stand in contrast to traditional phishing emails that hackers manually design using general rules gleaned from experience.","The V-Triad is an advanced set of rules for manually designing phishing emails to exploit our cognitive heuristics and biases.","In this study, we compare the performance of phishing emails created automatically by GPT-4 and manually using the V-Triad.","We also combine GPT-4 with the V-Triad to assess their combined potential.","A fourth group, exposed to generic phishing emails, was our control group.","We utilized a factorial approach, sending emails to 112 randomly selected participants recruited for the study.","The control group emails received a click-through rate between 19-28%, the GPT-generated emails 30-44%, emails generated by the V-Triad 69-79%, and emails generated by GPT and the V-Triad 43-81%.","Each participant was asked to explain for why they pressed or did not press a link in the email.","These answers often contradict each other, highlighting the need for personalized content.","The cues that make one person avoid phishing emails make another person fall for them.","Next, we used four popular large language models (GPT, Claude, PaLM, and LLaMA) to detect the intention of phishing emails and compare the results to human detection.","The language models demonstrated a strong ability to detect malicious intent, even in non-obvious phishing emails.","They sometimes surpassed human detection, although often being slightly less accurate than humans."],"url":"http://arxiv.org/abs/2308.12287v1"}
{"created":"2023-08-23 17:58:14","title":"D4: Improving LLM Pretraining via Document De-Duplication and Diversification","abstract":"Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.","sentences":["Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora.","While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash.","Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale.","Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training).","Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data."],"url":"http://arxiv.org/abs/2308.12284v1"}
{"created":"2023-08-23 17:50:57","title":"Extended Linear Regression: A Kalman Filter Approach for Minimizing Loss via Area Under the Curve","abstract":"This research enhances linear regression models by integrating a Kalman filter and analysing curve areas to minimize loss. The goal is to develop an optimal linear regression equation using stochastic gradient descent (SGD) for weight updating. Our approach involves a stepwise process, starting with user-defined parameters. The linear regression model is trained using SGD, tracking weights and loss separately and zipping them finally. A Kalman filter is then trained based on weight and loss arrays to predict the next consolidated weights. Predictions result from multiplying input averages with weights, evaluated for loss to form a weight-versus-loss curve. The curve's equation is derived using the two-point formula, and area under the curve is calculated via integration. The linear regression equation with minimum area becomes the optimal curve for prediction. Benefits include avoiding constant weight updates via gradient descent and working with partial datasets, unlike methods needing the entire set. However, computational complexity should be considered. The Kalman filter's accuracy might diminish beyond a certain prediction range.","sentences":["This research enhances linear regression models by integrating a Kalman filter and analysing curve areas to minimize loss.","The goal is to develop an optimal linear regression equation using stochastic gradient descent (SGD) for weight updating.","Our approach involves a stepwise process, starting with user-defined parameters.","The linear regression model is trained using SGD, tracking weights and loss separately and zipping them finally.","A Kalman filter is then trained based on weight and loss arrays to predict the next consolidated weights.","Predictions result from multiplying input averages with weights, evaluated for loss to form a weight-versus-loss curve.","The curve's equation is derived using the two-point formula, and area under the curve is calculated via integration.","The linear regression equation with minimum area becomes the optimal curve for prediction.","Benefits include avoiding constant weight updates via gradient descent and working with partial datasets, unlike methods needing the entire set.","However, computational complexity should be considered.","The Kalman filter's accuracy might diminish beyond a certain prediction range."],"url":"http://arxiv.org/abs/2308.12280v1"}
{"created":"2023-08-23 17:50:50","title":"On-Manifold Projected Gradient Descent","abstract":"This work provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, along with nonlinear projections from input space onto these class manifolds. The tools are applied to the setting of neural network image classifiers, where we generate novel, on-manifold data samples, and implement a projected gradient descent algorithm for on-manifold adversarial training. The susceptibility of neural networks (NNs) to adversarial attack highlights the brittle nature of NN decision boundaries in input space. Introducing adversarial examples during training has been shown to reduce the susceptibility of NNs to adversarial attack; however, it has also been shown to reduce the accuracy of the classifier if the examples are not valid examples for that class. Realistic \"on-manifold\" examples have been previously generated from class manifolds in the latent of an autoencoder. Our work explores these phenomena in a geometric and computational setting that is much closer to the raw, high-dimensional input space than can be provided by VAE or other black box dimensionality reductions. We employ conformally invariant diffusion maps (CIDM) to approximate class manifolds in diffusion coordinates, and develop the Nystr\\\"{o}m projection to project novel points onto class manifolds in this setting. On top of the manifold approximation, we leverage the spectral exterior calculus (SEC) to determine geometric quantities such as tangent vectors of the manifold. We use these tools to obtain adversarial examples that reside on a class manifold, yet fool a classifier. These misclassifications then become explainable in terms of human-understandable manipulations within the data, by expressing the on-manifold adversary in the semantic basis on the manifold.","sentences":["This work provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, along with nonlinear projections from input space onto these class manifolds.","The tools are applied to the setting of neural network image classifiers, where we generate novel, on-manifold data samples, and implement a projected gradient descent algorithm for on-manifold adversarial training.","The susceptibility of neural networks (NNs) to adversarial attack highlights the brittle nature of NN decision boundaries in input space.","Introducing adversarial examples during training has been shown to reduce the susceptibility of NNs to adversarial attack; however, it has also been shown to reduce the accuracy of the classifier if the examples are not valid examples for that class.","Realistic \"on-manifold\" examples have been previously generated from class manifolds in the latent of an autoencoder.","Our work explores these phenomena in a geometric and computational setting that is much closer to the raw, high-dimensional input space than can be provided by VAE or other black box dimensionality reductions.","We employ conformally invariant diffusion maps (CIDM) to approximate class manifolds in diffusion coordinates, and develop the Nystr\\\"{o}m projection to project novel points onto class manifolds in this setting.","On top of the manifold approximation, we leverage the spectral exterior calculus (SEC) to determine geometric quantities such as tangent vectors of the manifold.","We use these tools to obtain adversarial examples that reside on a class manifold, yet fool a classifier.","These misclassifications then become explainable in terms of human-understandable manipulations within the data, by expressing the on-manifold adversary in the semantic basis on the manifold."],"url":"http://arxiv.org/abs/2308.12279v1"}
{"created":"2023-08-23 17:49:51","title":"Operational requirements for localization in autonomous vehicles","abstract":"Autonomous vehicles (AVs) need to determine their position and orientation accurately with respect to global coordinate system or local features under different scene geometries, traffic conditions and environmental conditions. \\cite{reid2019localization} provides a comprehensive framework for the localization requirements for AVs. However, the framework is too restrictive whereby - (a) only a very small deviation from the lane is tolerated (one every $10^{8}$ hours), (b) all roadway types are considered same without any attention to restriction provided by the environment onto the localization and (c) the temporal nature of the location and orientation is not considered in the requirements. In this research, we present a more practical view of the localization requirement aimed at keeping the AV safe during an operation. We present the following novel contributions - (a) we propose a deviation penalty as a cumulative distribution function of the Weibull distribution which starts from the adjacent lane boundary, (b) we customize the parameters of the deviation penalty according to the current roadway type, particular lane boundary that the ego vehicle is against and roadway curvature and (c) we update the deviation penalty based on the available gap in the adjacent lane. We postulate that this formulation can provide a more robust and achievable view of the localization requirements than previous research while focusing on safety.","sentences":["Autonomous vehicles (AVs) need to determine their position and orientation accurately with respect to global coordinate system or local features under different scene geometries, traffic conditions and environmental conditions.","\\cite{reid2019localization} provides a comprehensive framework for the localization requirements for AVs.","However, the framework is too restrictive whereby - (a) only a very small deviation from the lane is tolerated (one every $10^{8}$ hours), (b) all roadway types are considered same without any attention to restriction provided by the environment onto the localization and (c) the temporal nature of the location and orientation is not considered in the requirements.","In this research, we present a more practical view of the localization requirement aimed at keeping the AV safe during an operation.","We present the following novel contributions - (a) we propose a deviation penalty as a cumulative distribution function of the Weibull distribution which starts from the adjacent lane boundary, (b) we customize the parameters of the deviation penalty according to the current roadway type, particular lane boundary that the ego vehicle is against and roadway curvature and (c) we update the deviation penalty based on the available gap in the adjacent lane.","We postulate that this formulation can provide a more robust and achievable view of the localization requirements than previous research while focusing on safety."],"url":"http://arxiv.org/abs/2308.12277v1"}
{"created":"2023-08-23 17:47:35","title":"A Model for Integrating Generative AI into Course Content Development","abstract":"As Generative AI (GenAI) models continue to gain prominence, a new frontier is emerging in the field of computer science education. Results from initial anonymous surveys reveal that nearly half (48.5%) of our students now turn to GenAI for academic assignments, highlighting its growing role in modern education. With educators facing challenges in creating dynamic and unique course content, the potential of GenAI becomes evident. It offers not only a quicker method for content development but also paves the way for diversified, high-quality educational resources, countering traditional cheating methods and catering to varied student needs. Key questions thus arise: \"How can GenAI assist instructors in creating meaningful content and problems quickly, and can it reduce the instructional staff's workload?\"   Addressing the first question, we unveil the \"GenAI Content Generation Framework\". This novel tool equips educators to tap into the prowess of GenAI for course content design. The framework presents a systematic and practical blueprint for generating university-level course material through chat-based GenAI. Drawing from our first-hand experiences, we provide strategic guidance on formulating inquiries and organizing GenAI sessions to elicit quality content that aligns with specific educational goals and context.   Our work stands apart by outlining a specific workflow and offering concrete suggestions for harnessing GenAI in course material development, backed by a strong case for its adoption. Armed with the framework and insights presented in this paper, educators and course content developers can move forward with assurance, tapping into GenAI's vast potential for innovative content creation.","sentences":["As Generative AI (GenAI) models continue to gain prominence, a new frontier is emerging in the field of computer science education.","Results from initial anonymous surveys reveal that nearly half (48.5%) of our students now turn to GenAI for academic assignments, highlighting its growing role in modern education.","With educators facing challenges in creating dynamic and unique course content, the potential of GenAI becomes evident.","It offers not only a quicker method for content development but also paves the way for diversified, high-quality educational resources, countering traditional cheating methods and catering to varied student needs.","Key questions thus arise: \"How can GenAI assist instructors in creating meaningful content and problems quickly, and can it reduce the instructional staff's workload?\"   Addressing the first question, we unveil the \"GenAI Content Generation Framework\".","This novel tool equips educators to tap into the prowess of GenAI for course content design.","The framework presents a systematic and practical blueprint for generating university-level course material through chat-based GenAI.","Drawing from our first-hand experiences, we provide strategic guidance on formulating inquiries and organizing GenAI sessions to elicit quality content that aligns with specific educational goals and context.   ","Our work stands apart by outlining a specific workflow and offering concrete suggestions for harnessing GenAI in course material development, backed by a strong case for its adoption.","Armed with the framework and insights presented in this paper, educators and course content developers can move forward with assurance, tapping into GenAI's vast potential for innovative content creation."],"url":"http://arxiv.org/abs/2308.12276v1"}
{"created":"2023-08-23 17:40:35","title":"Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models","abstract":"Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guided reinforcement learning approach. We discovered that the suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by a factor of many times using datasets that show the usefulness of NLP in sensitive fields, such as mental health.","sentences":["Foundational Language Models (FLMs) have advanced natural language processing (NLP) research.","Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation.","While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty.","Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble.","In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets.","We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs.","We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guided reinforcement learning approach.","We discovered that the suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by a factor of many times using datasets that show the usefulness of NLP in sensitive fields, such as mental health."],"url":"http://arxiv.org/abs/2308.12272v1"}
{"created":"2023-08-23 17:39:58","title":"A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces","abstract":"Since thermal imagery offers a unique modality to investigate pain, the U.S. National Institutes of Health (NIH) has collected a large and diverse set of cancer patient facial thermograms for AI-based pain research. However, differing angles from camera capture between thermal and visible sensors has led to misalignment between Visible-Thermal (VT) images. We modernize the classic computer vision task of image registration by applying and modifying a generative alignment algorithm to register VT cancer faces, without the need for a reference or alignment parameters. By registering VT faces, we demonstrate that the quality of thermal images produced in the generative AI downstream task of Visible-to-Thermal (V2T) image translation significantly improves up to 52.5\\%, than without registration. Images in this paper have been approved by the NIH NCI for public dissemination.","sentences":["Since thermal imagery offers a unique modality to investigate pain, the U.S. National Institutes of Health (NIH) has collected a large and diverse set of cancer patient facial thermograms for AI-based pain research.","However, differing angles from camera capture between thermal and visible sensors has led to misalignment between Visible-Thermal (VT) images.","We modernize the classic computer vision task of image registration by applying and modifying a generative alignment algorithm to register VT cancer faces, without the need for a reference or alignment parameters.","By registering VT faces, we demonstrate that the quality of thermal images produced in the generative AI downstream task of Visible-to-Thermal (V2T) image translation significantly improves up to 52.5\\%, than without registration.","Images in this paper have been approved by the NIH NCI for public dissemination."],"url":"http://arxiv.org/abs/2308.12271v1"}
{"created":"2023-08-23 17:37:51","title":"Language Reward Modulation for Pretraining Reinforcement Learning","abstract":"Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\\textbf{LA}$nguage Reward $\\textbf{M}$odulated $\\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy. Our VLM pretraining approach, which is a departure from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench.","sentences":["Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years.","In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards.","Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL.","Concretely, we propose $\\textbf{LA}$nguage Reward $\\textbf{M}$odulated $\\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\\textit{pretraining}$ utility for RL as opposed to a downstream task reward.","LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment.","LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy.","Our VLM pretraining approach, which is a departure from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench."],"url":"http://arxiv.org/abs/2308.12270v1"}
{"created":"2023-08-23 17:35:16","title":"Bugsplainer: Leveraging Code Structures to Explain Software Bugs with Neural Machine Translation","abstract":"Software bugs cost the global economy billions of dollars each year and take up ~50% of the development time. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a novel web-based debugging solution that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer leverages code structures to reason about a bug and employs the fine-tuned version of a text generation model, CodeT5, to generate the explanations.   Tool video: https://youtu.be/xga-ScvULpk","sentences":["Software bugs cost the global economy billions of dollars each year and take up ~50% of the development time.","Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code.","Over the last five decades, there has been significant research on automatically finding or correcting software bugs.","However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task.","In this paper, we propose Bugsplainer, a novel web-based debugging solution that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits.","Bugsplainer leverages code structures to reason about a bug and employs the fine-tuned version of a text generation model, CodeT5, to generate the explanations.   ","Tool video: https://youtu.be/xga-ScvULpk"],"url":"http://arxiv.org/abs/2308.12267v1"}
{"created":"2023-08-23 17:34:43","title":"Age of Gossip on Generalized Rings","abstract":"We consider a gossip network consisting of a source forwarding updates and $n$ nodes placed geometrically in a ring formation. Each node gossips with $f(n)$ nodes on either side, thus communicating with $2f(n)$ nodes in total. $f(n)$ is a sub-linear, non-decreasing and positive function. The source keeps updates of a process, that might be generated or observed, and shares them with the nodes in the ring network. The nodes in the ring network communicate with their neighbors and disseminate these version updates using a push-style gossip strategy. We use the version age metric to quantify the timeliness of information at the nodes. Prior to this work, it was shown that the version age scales as $O(n^{\\frac{1}{2}})$ in a ring network, i.e., when $f(n)=1$, and as $O(\\log{n})$ in a fully-connected network, i.e., when $2f(n)=n-1$. In this paper, we find an upper bound for the average version age for a set of nodes in such a network in terms of the number of nodes $n$ and the number of gossiped neighbors $2 f(n)$. We show that if $f(n) = \\Omega(\\frac{n}{\\log^2{n}})$, then the version age still scales as $\\theta(\\log{n})$. We also show that if $f(n)$ is a rational function, then the version age also scales as a rational function. In particular, if $f(n)=n^\\alpha$, then version age is $O(n^\\frac{1-\\alpha}{2})$. Finally, through numerical calculations we verify that, for all practical purposes, if $f(n) = \\Omega(n^{0.6})$, the version age scales as $O(\\log{n})$.","sentences":["We consider a gossip network consisting of a source forwarding updates and $n$ nodes placed geometrically in a ring formation.","Each node gossips with $f(n)$ nodes on either side, thus communicating with $2f(n)$ nodes in total.","$f(n)$ is a sub-linear, non-decreasing and positive function.","The source keeps updates of a process, that might be generated or observed, and shares them with the nodes in the ring network.","The nodes in the ring network communicate with their neighbors and disseminate these version updates using a push-style gossip strategy.","We use the version age metric to quantify the timeliness of information at the nodes.","Prior to this work, it was shown that the version age scales as $O(n^{\\frac{1}{2}})$ in a ring network, i.e., when $f(n)=1$, and as $O(\\log{n})$ in a fully-connected network, i.e., when $2f(n)=n-1$. In this paper, we find an upper bound for the average version age for a set of nodes in such a network in terms of the number of nodes $n$ and the number of gossiped neighbors $2 f(n)$.","We show that if $f(n) = \\Omega(\\frac{n}{\\log^2{n}})$, then the version age still scales as $\\theta(\\log{n})$. We also show that if $f(n)$ is a rational function, then the version age also scales as a rational function.","In particular, if $f(n)=n^\\alpha$, then version age is $O(n^\\frac{1-\\alpha}{2})$. Finally, through numerical calculations we verify that, for all practical purposes, if $f(n) = \\Omega(n^{0.6})$, the version age scales as $O(\\log{n})$."],"url":"http://arxiv.org/abs/2308.12266v1"}
{"created":"2023-08-23 17:32:56","title":"Network Navigation with Online Delays is PSPACE-complete","abstract":"In public transport networks disruptions may occur and lead to travel delays. It is thus interesting to determine whether a traveler can be resilient to delays that occur unexpectedly, ensuring that they can reach their destination in time regardless. We model this as a game between the traveler and a delay-introducing adversary. We study the computational complexity of the problem of deciding whether the traveler has a winning strategy in this game. Our main result is that this problem is PSPACE-complete.","sentences":["In public transport networks disruptions may occur and lead to travel delays.","It is thus interesting to determine whether a traveler can be resilient to delays that occur unexpectedly, ensuring that they can reach their destination in time regardless.","We model this as a game between the traveler and a delay-introducing adversary.","We study the computational complexity of the problem of deciding whether the traveler has a winning strategy in this game.","Our main result is that this problem is PSPACE-complete."],"url":"http://arxiv.org/abs/2308.12265v1"}
{"created":"2023-08-23 17:32:06","title":"FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning","abstract":"With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, namely TensorFlow. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow APIs' energy profiles. Furthermore, we elaborate on the considerations, issues, and challenges that one needs to consider while designing and implementing a fine-grained energy consumption measurement tool. We hope this work will facilitate further advances in DL energy measurement and the development of energy-aware practices for DL systems.","sentences":["With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern.","Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems.","However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area.","In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement.","Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs.","FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability.","We assess FECoM's capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, namely TensorFlow.","Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow APIs' energy profiles.","Furthermore, we elaborate on the considerations, issues, and challenges that one needs to consider while designing and implementing a fine-grained energy consumption measurement tool.","We hope this work will facilitate further advances in DL energy measurement and the development of energy-aware practices for DL systems."],"url":"http://arxiv.org/abs/2308.12264v1"}
{"created":"2023-08-23 17:28:21","title":"Prompt2Model: Generating Deployable Models from Natural Language Instructions","abstract":"Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.","sentences":["Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples.","However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs.","In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.","This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets.","Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller.","We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment.","Prompt2Model is available open-source at https://github.com/neulab/prompt2model."],"url":"http://arxiv.org/abs/2308.12261v1"}
{"created":"2023-08-23 17:20:37","title":"Innovating Computer Programming Pedagogy: The AI-Lab Framework for Generative AI Adoption","abstract":"Over the last year, the ascent of Generative AI (GenAI) has raised concerns about its impact on core skill development, such as problem-solving and algorithmic thinking, in Computer Science students. Preliminary anonymous surveys show that at least 48.5% of our students use GenAI for homework. With the proliferation of these tools, the academic community must contemplate the appropriate role of these tools in education. Neglecting this might culminate in a phenomenon we term the \"Junior-Year Wall,\" where students struggle in advanced courses due to prior over-dependence on GenAI. Instead of discouraging GenAI use, which may unintentionally foster covert usage, our research seeks to answer: \"How can educators guide students' interactions with GenAI to preserve core skill development during their foundational academic years?\"   We introduce \"AI-Lab,\" a pedagogical framework for guiding students in effectively leveraging GenAI within core collegiate programming courses. This framework accentuates GenAI's benefits and potential as a pedagogical instrument. By identifying and rectifying GenAI's errors, students enrich their learning process. Moreover, AI-Lab presents opportunities to use GenAI for tailored support such as topic introductions, detailed examples, corner case identification, rephrased explanations, and debugging assistance. Importantly, the framework highlights the risks of GenAI over-dependence, aiming to intrinsically motivate students towards balanced usage. This approach is premised on the idea that mere warnings of GenAI's potential failures may be misconstrued as instructional shortcomings rather than genuine tool limitations.   Additionally, AI-Lab offers strategies for formulating prompts to elicit high-quality GenAI responses. For educators, AI-Lab provides mechanisms to explore students' perceptions of GenAI's role in their learning experience.","sentences":["Over the last year, the ascent of Generative AI (GenAI) has raised concerns about its impact on core skill development, such as problem-solving and algorithmic thinking, in Computer Science students.","Preliminary anonymous surveys show that at least 48.5% of our students use GenAI for homework.","With the proliferation of these tools, the academic community must contemplate the appropriate role of these tools in education.","Neglecting this might culminate in a phenomenon we term the \"Junior-Year Wall,\" where students struggle in advanced courses due to prior over-dependence on GenAI.","Instead of discouraging GenAI use, which may unintentionally foster covert usage, our research seeks to answer: \"How can educators guide students' interactions with GenAI to preserve core skill development during their foundational academic years?\"   ","We introduce \"AI-Lab,\" a pedagogical framework for guiding students in effectively leveraging GenAI within core collegiate programming courses.","This framework accentuates GenAI's benefits and potential as a pedagogical instrument.","By identifying and rectifying GenAI's errors, students enrich their learning process.","Moreover, AI-Lab presents opportunities to use GenAI for tailored support such as topic introductions, detailed examples, corner case identification, rephrased explanations, and debugging assistance.","Importantly, the framework highlights the risks of GenAI over-dependence, aiming to intrinsically motivate students towards balanced usage.","This approach is premised on the idea that mere warnings of GenAI's potential failures may be misconstrued as instructional shortcomings rather than genuine tool limitations.   ","Additionally, AI-Lab offers strategies for formulating prompts to elicit high-quality GenAI responses.","For educators, AI-Lab provides mechanisms to explore students' perceptions of GenAI's role in their learning experience."],"url":"http://arxiv.org/abs/2308.12258v1"}
{"created":"2023-08-23 17:16:07","title":"Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders","abstract":"Sequential recommenders have been widely used in industry due to their strength in modeling user preferences. While these models excel at learning a user's positive interests, less attention has been paid to learning from negative user feedback. Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user. However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions. In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a \"not-to-recommend\" loss function that optimizes for the log-likelihood of not recommending items with negative feedback. We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system. Furthermore, we address a challenge in measuring recommender responsiveness to negative feedback by developing a counterfactual simulation framework to compare recommender responses between different user actions, showing improved responsiveness from the modeling change.","sentences":["Sequential recommenders have been widely used in industry due to their strength in modeling user preferences.","While these models excel at learning a user's positive interests, less attention has been paid to learning from negative user feedback.","Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user.","However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions.","In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a \"not-to-recommend\" loss function that optimizes for the log-likelihood of not recommending items with negative feedback.","We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system.","Furthermore, we address a challenge in measuring recommender responsiveness to negative feedback by developing a counterfactual simulation framework to compare recommender responses between different user actions, showing improved responsiveness from the modeling change."],"url":"http://arxiv.org/abs/2308.12256v1"}
{"created":"2023-08-23 17:01:53","title":"How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy","abstract":"End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a cartpole.","sentences":["End-to-end learning has emerged as a major paradigm for developing autonomous systems.","Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance.","A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve.","Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states.","To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift.","These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction.","We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a cartpole."],"url":"http://arxiv.org/abs/2308.12252v1"}
{"created":"2023-08-23 16:48:04","title":"How to Protect Copyright Data in Optimization of Large Language Models?","abstract":"Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.   In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.","sentences":["Large language models (LLMs) and generative AI have played a transformative role in computer research and applications.","Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted.","LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.   ","In this paper, we show that large language model training and optimization can be seen as a softmax regression problem.","We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data.","This establishes a theoretical method of training large language models in a way that avoids generating copyright data."],"url":"http://arxiv.org/abs/2308.12247v1"}
{"created":"2023-08-23 16:42:27","title":"Multi-Objective Optimization for Sparse Deep Neural Network Training","abstract":"Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economical and also ecological) sustainability issue of DNN models, with a particular focus on Deep Multi-Task models, which are typically designed with a very large number of weights to perform equally well on multiple tasks. Through experiments conducted on two Machine Learning datasets, we demonstrate the possibility of adaptively sparsifying the model during training without significantly impacting its performance, if we are willing to apply task-specific adaptations to the network weights. Code is available at https://github.com/salomonhotegni/MDMTN.","sentences":["Different conflicting optimization criteria arise naturally in various Deep Learning scenarios.","These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity.","The usual approach is a simple weighting of the criteria, which formally only works in the convex setting.","In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks.","By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems.","The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints.","Our work aims to address the (economical and also ecological) sustainability issue of DNN models, with a particular focus on Deep Multi-Task models, which are typically designed with a very large number of weights to perform equally well on multiple tasks.","Through experiments conducted on two Machine Learning datasets, we demonstrate the possibility of adaptively sparsifying the model during training without significantly impacting its performance, if we are willing to apply task-specific adaptations to the network weights.","Code is available at https://github.com/salomonhotegni/MDMTN."],"url":"http://arxiv.org/abs/2308.12243v1"}
{"created":"2023-08-23 16:39:14","title":"Recent Developments in Pandora's Box Problem: Variants and Applications","abstract":"In 1979, Weitzman introduced Pandora's box problem as a framework for sequential search with costly inspections. Recently, there has been a surge of interest in Pandora's box problem, particularly among researchers working at the intersection of economics and computation. This survey provides an overview of the recent literature on Pandora's box problem, including its latest extensions and applications in areas such as market design, decision theory, and machine learning.","sentences":["In 1979, Weitzman introduced Pandora's box problem as a framework for sequential search with costly inspections.","Recently, there has been a surge of interest in Pandora's box problem, particularly among researchers working at the intersection of economics and computation.","This survey provides an overview of the recent literature on Pandora's box problem, including its latest extensions and applications in areas such as market design, decision theory, and machine learning."],"url":"http://arxiv.org/abs/2308.12242v1"}
{"created":"2023-08-23 16:32:54","title":"LLMRec: Benchmarking Large Language Models on Recommendation Task","abstract":"Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec.","sentences":["Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models.","However, the application of LLMs in the recommendation domain has not been thoroughly investigated.","To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks.","Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization.","Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability.","The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation.","However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks.","We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results.","We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance.","Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec."],"url":"http://arxiv.org/abs/2308.12241v1"}
{"created":"2023-08-23 16:25:13","title":"NimbRo wins ANA Avatar XPRIZE Immersive Telepresence Competition: Human-Centric Evaluation and Lessons Learned","abstract":"Robotic avatar systems can enable immersive telepresence with locomotion, manipulation, and communication capabilities. We present such an avatar system, based on the key components of immersive 3D visualization and transparent force-feedback telemanipulation. Our avatar robot features an anthropomorphic upper body with dexterous hands. The remote human operator drives the arms and fingers through an exoskeleton-based operator station, which provides force feedback both at the wrist and for each finger. The robot torso is mounted on a holonomic base, providing omnidirectional locomotion on flat floors, controlled using a 3D rudder device. Finally, the robot features a 6D movable head with stereo cameras, which stream images to a VR display worn by the operator. Movement latency is hidden using spherical rendering. The head also carries a telepresence screen displaying an animated image of the operator's face, enabling direct interaction with remote persons. Our system won the \\$10M ANA Avatar XPRIZE competition, which challenged teams to develop intuitive and immersive avatar systems that could be operated by briefly trained judges. We analyze our successful participation in the semifinals and finals and provide insight into our operator training and lessons learned. In addition, we evaluate our system in a user study that demonstrates its intuitive and easy usability.","sentences":["Robotic avatar systems can enable immersive telepresence with locomotion, manipulation, and communication capabilities.","We present such an avatar system, based on the key components of immersive 3D visualization and transparent force-feedback telemanipulation.","Our avatar robot features an anthropomorphic upper body with dexterous hands.","The remote human operator drives the arms and fingers through an exoskeleton-based operator station, which provides force feedback both at the wrist and for each finger.","The robot torso is mounted on a holonomic base, providing omnidirectional locomotion on flat floors, controlled using a 3D rudder device.","Finally, the robot features a 6D movable head with stereo cameras, which stream images to a VR display worn by the operator.","Movement latency is hidden using spherical rendering.","The head also carries a telepresence screen displaying an animated image of the operator's face, enabling direct interaction with remote persons.","Our system won the \\$10M ANA Avatar XPRIZE competition, which challenged teams to develop intuitive and immersive avatar systems that could be operated by briefly trained judges.","We analyze our successful participation in the semifinals and finals and provide insight into our operator training and lessons learned.","In addition, we evaluate our system in a user study that demonstrates its intuitive and easy usability."],"url":"http://arxiv.org/abs/2308.12238v1"}
{"created":"2023-08-23 16:16:11","title":"MolGrapher: Graph-based Visual Recognition of Chemical Structures","abstract":"The automatic analysis of chemical literature has immense potential to accelerate the discovery of new materials and drugs. Much of the critical information in patent documents and scientific articles is contained in figures, depicting the molecule structures. However, automatically parsing the exact chemical structure is a formidable challenge, due to the amount of detailed information, the diversity of drawing styles, and the need for training data. In this work, we introduce MolGrapher to recognize chemical structures visually. First, a deep keypoint detector detects the atoms. Second, we treat all candidate atoms and bonds as nodes and put them in a graph. This construct allows a natural graph representation of the molecule. Last, we classify atom and bond nodes in the graph with a Graph Neural Network. To address the lack of real training data, we propose a synthetic data generation pipeline producing diverse and realistic results. In addition, we introduce a large-scale benchmark of annotated real molecule images, USPTO-30K, to spur research on this critical topic. Extensive experiments on five datasets show that our approach significantly outperforms classical and learning-based methods in most settings. Code, models, and datasets are available.","sentences":["The automatic analysis of chemical literature has immense potential to accelerate the discovery of new materials and drugs.","Much of the critical information in patent documents and scientific articles is contained in figures, depicting the molecule structures.","However, automatically parsing the exact chemical structure is a formidable challenge, due to the amount of detailed information, the diversity of drawing styles, and the need for training data.","In this work, we introduce MolGrapher to recognize chemical structures visually.","First, a deep keypoint detector detects the atoms.","Second, we treat all candidate atoms and bonds as nodes and put them in a graph.","This construct allows a natural graph representation of the molecule.","Last, we classify atom and bond nodes in the graph with a Graph Neural Network.","To address the lack of real training data, we propose a synthetic data generation pipeline producing diverse and realistic results.","In addition, we introduce a large-scale benchmark of annotated real molecule images, USPTO-30K, to spur research on this critical topic.","Extensive experiments on five datasets show that our approach significantly outperforms classical and learning-based methods in most settings.","Code, models, and datasets are available."],"url":"http://arxiv.org/abs/2308.12234v1"}
{"created":"2023-08-23 16:09:28","title":"Electromagnets Under the Table: an Unobtrusive Magnetic Navigation System for Microsurgery","abstract":"Miniature magnetic tools have the potential to enable minimally invasive surgical techniques to be applied to space-restricted surgical procedures in areas such as neurosurgery. However, typical magnetic navigation systems, which create the magnetic fields to drive such tools, either cannot generate large enough fields, or surround the patient in a way that obstructs surgeon access to the patient. This paper introduces the design of a magnetic navigation system with eight electromagnets arranged completely under the operating table, to endow the system with maximal workspace accessibility, which allows the patient to lie down on the top surface of the system without any constraints. The found optimal geometric layout of the electromagnets maximizes the field strength and uniformity over a reasonable neurosurgical operating volume. The system can generate non-uniform magnetic fields up to 38 mT along the x and y axes and 47 mT along the z axis at a working distance of 120 mm away from the actuation system workbench, deep enough to deploy magnetic microsurgical tools in the brain. The forces which can be exerted on millimeter-scale magnets used in prototype neurosurgical tools are validated experimentally. Due to its large workspace, this system could be used to control milli-robots in a variety of surgical applications.","sentences":["Miniature magnetic tools have the potential to enable minimally invasive surgical techniques to be applied to space-restricted surgical procedures in areas such as neurosurgery.","However, typical magnetic navigation systems, which create the magnetic fields to drive such tools, either cannot generate large enough fields, or surround the patient in a way that obstructs surgeon access to the patient.","This paper introduces the design of a magnetic navigation system with eight electromagnets arranged completely under the operating table, to endow the system with maximal workspace accessibility, which allows the patient to lie down on the top surface of the system without any constraints.","The found optimal geometric layout of the electromagnets maximizes the field strength and uniformity over a reasonable neurosurgical operating volume.","The system can generate non-uniform magnetic fields up to 38 mT along the x and y axes and 47 mT along the z axis at a working distance of 120 mm away from the actuation system workbench, deep enough to deploy magnetic microsurgical tools in the brain.","The forces which can be exerted on millimeter-scale magnets used in prototype neurosurgical tools are validated experimentally.","Due to its large workspace, this system could be used to control milli-robots in a variety of surgical applications."],"url":"http://arxiv.org/abs/2308.12228v1"}
{"created":"2023-08-23 16:01:50","title":"Critical Learning Periods Emerge Even in Deep Linear Networks","abstract":"Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show analytically and in simulations that the learning of features is tied to competition between sources. Finally, we extend our analysis to multi-task learning to show that pre-training on certain tasks can damage the transfer performance on new tasks, and show how this depends on the relationship between tasks and the duration of the pre-training stage. To the best of our knowledge, our work provides the first analytically tractable model that sheds light into why critical learning periods emerge in biological and artificial networks.","sentences":["Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations.","Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems.","This suggests that critical periods may be fundamental to learning and not an accident of biology.","Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details.","To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment.","We show that critical periods depend on the depth of the model and structure of the data distribution.","We also show analytically and in simulations that the learning of features is tied to competition between sources.","Finally, we extend our analysis to multi-task learning to show that pre-training on certain tasks can damage the transfer performance on new tasks, and show how this depends on the relationship between tasks and the duration of the pre-training stage.","To the best of our knowledge, our work provides the first analytically tractable model that sheds light into why critical learning periods emerge in biological and artificial networks."],"url":"http://arxiv.org/abs/2308.12221v1"}
{"created":"2023-08-23 16:01:12","title":"Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning","abstract":"The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning","sentences":["The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models.","Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts.","This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners.","We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections.","We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks.","Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks.","We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning"],"url":"http://arxiv.org/abs/2308.12219v1"}
{"created":"2023-08-23 15:56:26","title":"CIParsing: Unifying Causality Properties into Multiple Human Parsing","abstract":"Existing methods of multiple human parsing (MHP) apply statistical models to acquire underlying associations between images and labeled body parts. However, acquired associations often contain many spurious correlations that degrade model generalization, leading statistical models to be vulnerable to visually contextual variations in images (e.g., unseen image styles/external interventions). To tackle this, we present a causality inspired parsing paradigm termed CIParsing, which follows fundamental causal principles involving two causal properties for human parsing (i.e., the causal diversity and the causal invariance). Specifically, we assume that an input image is constructed by a mix of causal factors (the characteristics of body parts) and non-causal factors (external contexts), where only the former ones cause the generation process of human parsing.Since causal/non-causal factors are unobservable, a human parser in proposed CIParsing is required to construct latent representations of causal factors and learns to enforce representations to satisfy the causal properties. In this way, the human parser is able to rely on causal factors w.r.t relevant evidence rather than non-causal factors w.r.t spurious correlations, thus alleviating model degradation and yielding improved parsing ability. Notably, the CIParsing is designed in a plug-and-play fashion and can be integrated into any existing MHP models. Extensive experiments conducted on two widely used benchmarks demonstrate the effectiveness and generalizability of our method.","sentences":["Existing methods of multiple human parsing (MHP) apply statistical models to acquire underlying associations between images and labeled body parts.","However, acquired associations often contain many spurious correlations that degrade model generalization, leading statistical models to be vulnerable to visually contextual variations in images (e.g., unseen image styles/external interventions).","To tackle this, we present a causality inspired parsing paradigm termed CIParsing, which follows fundamental causal principles involving two causal properties for human parsing (i.e., the causal diversity and the causal invariance).","Specifically, we assume that an input image is constructed by a mix of causal factors (the characteristics of body parts) and non-causal factors (external contexts), where only the former ones cause the generation process of human parsing.","Since causal/non-causal factors are unobservable, a human parser in proposed CIParsing is required to construct latent representations of causal factors and learns to enforce representations to satisfy the causal properties.","In this way, the human parser is able to rely on causal factors w.r.t relevant evidence rather than non-causal factors w.r.t spurious correlations, thus alleviating model degradation and yielding improved parsing ability.","Notably, the CIParsing is designed in a plug-and-play fashion and can be integrated into any existing MHP models.","Extensive experiments conducted on two widely used benchmarks demonstrate the effectiveness and generalizability of our method."],"url":"http://arxiv.org/abs/2308.12218v1"}
{"created":"2023-08-23 15:52:45","title":"SG-Former: Self-guided Transformer with Evolving Token Reallocation","abstract":"Vision Transformer has demonstrated impressive success across various vision tasks. However, its heavy computation cost, which grows quadratically with respect to the token sequence length, largely limits its power in handling large feature maps. To alleviate the computation cost, previous works rely on either fine-grained self-attentions restricted to local small regions, or global self-attentions but to shorten the sequence length resulting in coarse granularity. In this paper, we propose a novel model, termed as Self-guided Transformer~(SG-Former), towards effective global self-attention with adaptive fine granularity. At the heart of our approach is to utilize a significance map, which is estimated through hybrid-scale self-attention and evolves itself during training, to reallocate tokens based on the significance of each region. Intuitively, we assign more tokens to the salient regions for achieving fine-grained attention, while allocating fewer tokens to the minor regions in exchange for efficiency and global receptive fields. The proposed SG-Former achieves performance superior to state of the art: our base size model achieves \\textbf{84.7\\%} Top-1 accuracy on ImageNet-1K, \\textbf{51.2mAP} bbAP on CoCo, \\textbf{52.7mIoU} on ADE20K surpassing the Swin Transformer by \\textbf{+1.3\\% / +2.7 mAP/ +3 mIoU}, with lower computation costs and fewer parameters. The code is available at \\href{https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former}","sentences":["Vision Transformer has demonstrated impressive success across various vision tasks.","However, its heavy computation cost, which grows quadratically with respect to the token sequence length, largely limits its power in handling large feature maps.","To alleviate the computation cost, previous works rely on either fine-grained self-attentions restricted to local small regions, or global self-attentions but to shorten the sequence length resulting in coarse granularity.","In this paper, we propose a novel model, termed as Self-guided Transformer~(SG-Former), towards effective global self-attention with adaptive fine granularity.","At the heart of our approach is to utilize a significance map, which is estimated through hybrid-scale self-attention and evolves itself during training, to reallocate tokens based on the significance of each region.","Intuitively, we assign more tokens to the salient regions for achieving fine-grained attention, while allocating fewer tokens to the minor regions in exchange for efficiency and global receptive fields.","The proposed SG-Former achieves performance superior to state of the art: our base size model achieves \\textbf{84.7\\%} Top-1 accuracy on ImageNet-1K, \\textbf{51.2mAP} bbAP on CoCo, \\textbf{52.7mIoU} on ADE20K surpassing the Swin Transformer by \\textbf{+1.3\\% / +2.7 mAP/ +3 mIoU}, with lower computation costs and fewer parameters.","The code is available at \\href{https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former}"],"url":"http://arxiv.org/abs/2308.12216v1"}
{"created":"2023-08-23 15:52:20","title":"The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection","abstract":"We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is for future work to avoid the pitfalls that we identify.","sentences":["We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study.","We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field.","We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability.","We find significant shortcomings in the literature that call into question claimed performance and practicality.","Detection tasks are often meaningfully distinct from the challenges that online services actually face.","Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training.","Data and code availability is poor.","Models do not generalize well to out-of-domain data.","Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems.","Our aim is for future work to avoid the pitfalls that we identify."],"url":"http://arxiv.org/abs/2308.12215v1"}
{"created":"2023-08-23 15:51:36","title":"CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No","abstract":"Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying \"no\" (\\textbf{CLIPN}), which empowers the logic of saying \"no\" within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable \"no\" prompt and a \"no\" text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with \"no\" prompts, thereby enabling it to identify unknown samples. Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from \"no\" prompts and the text encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6 OOD datasets) for the OOD detection task demonstrate that CLIPN, based on ViT-B-16, outperforms 7 well-used algorithms by at least 2.34\\% and 11.64\\% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN can serve as a solid foundation for effectively leveraging CLIP in downstream OOD tasks. The code is available on https://github.com/xmed-lab/CLIPN}{https://github.com/xmed-lab/CLIPN.","sentences":["Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes.","Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers.","However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention.","This paper presents a novel method, namely CLIP saying \"no\" (\\textbf{CLIPN}), which empowers the logic of saying \"no\" within CLIP.","Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts.","Specifically, we design a novel learnable \"no\" prompt and a \"no\" text encoder to capture negation semantics within images.","Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with \"no\" prompts, thereby enabling it to identify unknown samples.","Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from \"no\" prompts and the text encoder.","Experimental results on 9 benchmark datasets (3 ID datasets and 6 OOD datasets) for the OOD detection task demonstrate that CLIPN, based on ViT-B-16, outperforms 7 well-used algorithms by at least 2.34\\% and 11.64\\% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K.","Our CLIPN can serve as a solid foundation for effectively leveraging CLIP in downstream OOD tasks.","The code is available on https://github.com/xmed-lab/CLIPN}{https://github.com/xmed-lab/CLIPN."],"url":"http://arxiv.org/abs/2308.12213v1"}
{"created":"2023-08-23 15:50:51","title":"ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy","abstract":"Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under user-level DP compared to baseline methods. To the best of our knowledge, our work is the first FL framework that effectively provides user-level DP in the general cross-silo FL setting.","sentences":["Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy.","Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL.","However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown.","In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos.","Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches.","We provide a theoretical analysis of the algorithm's privacy and utility.","Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks.","Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under user-level DP compared to baseline methods.","To the best of our knowledge, our work is the first FL framework that effectively provides user-level DP in the general cross-silo FL setting."],"url":"http://arxiv.org/abs/2308.12210v1"}
{"created":"2023-08-23 15:47:56","title":"A Heuristic Informative-Path-Planning Algorithm for Autonomous Mapping of Unknown Areas","abstract":"Informative path planning algorithms are of paramount importance in applications like disaster management to efficiently gather information through a priori unknown environments. This is, however, a complex problem that involves finding a globally optimal path that gathers the maximum amount of information (e.g., the largest map with a minimum travelling distance) while using partial and uncertain local measurements. This paper addresses this problem by proposing a novel heuristic algorithm that continuously estimates the potential mapping gain for different sub-areas across the partially created map, and then uses these estimations to locally navigate the robot. Furthermore, this paper presents a novel algorithm to calculate a benchmark solution, where the map is a priori known to the planar, to evaluate the efficacy of the developed heuristic algorithm over different test scenarios. The findings indicate that the efficiency of the proposed algorithm, measured in terms of the mapped area per unit of travelling distance, ranges from 70% to 80% of the benchmark solution in various test scenarios. In essence, the algorithm demonstrates the capability to generate paths that come close to the globally optimal path provided by the benchmark solution.","sentences":["Informative path planning algorithms are of paramount importance in applications like disaster management to efficiently gather information through a priori unknown environments.","This is, however, a complex problem that involves finding a globally optimal path that gathers the maximum amount of information (e.g., the largest map with a minimum travelling distance) while using partial and uncertain local measurements.","This paper addresses this problem by proposing a novel heuristic algorithm that continuously estimates the potential mapping gain for different sub-areas across the partially created map, and then uses these estimations to locally navigate the robot.","Furthermore, this paper presents a novel algorithm to calculate a benchmark solution, where the map is a priori known to the planar, to evaluate the efficacy of the developed heuristic algorithm over different test scenarios.","The findings indicate that the efficiency of the proposed algorithm, measured in terms of the mapped area per unit of travelling distance, ranges from 70% to 80% of the benchmark solution in various test scenarios.","In essence, the algorithm demonstrates the capability to generate paths that come close to the globally optimal path provided by the benchmark solution."],"url":"http://arxiv.org/abs/2308.12209v1"}
{"created":"2023-08-23 15:39:42","title":"Curriculum Learning with Adam: The Devil Is in the Wrong Details","abstract":"Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chosen hyperparameters. As such, our results contribute to understanding why CL methods work, but at the same time urge caution when claiming positive results.","sentences":["Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress.","However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success.","In this paper, we explore why.","Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP.","A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm.","We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chosen hyperparameters.","As such, our results contribute to understanding why CL methods work, but at the same time urge caution when claiming positive results."],"url":"http://arxiv.org/abs/2308.12202v1"}
{"created":"2023-08-23 15:39:18","title":"A foolish consistency? Aligning interface objects hinders location recall and may induce collinear errors","abstract":"During our nearly constant use of digital devices, perhaps our most frequent need is to visually identify icons representing our content and invoke the actions to manipulate them. Almost since the inception of user interface design in the 1970s, with rare exception it has become the tendency for programmers to prescribe the arrangement these things in uniform rectilinear rows and columns. This was imported from theories for print design and ultimately brought into widespread practice for graphical user interfaces (GUIs). Whether consistent rectilinearity actually does better than less rectilinear arrangements to maximize selection efficiency has not been challenged on considerations of speed or any other measure. In a series of four experiments, we explore how alignment may in fact discourage easy recallability of screen object locations and hence increase search intensity. A second objective is to present a methodological model where we deliberately attempt to begin with psychophysical cognitive evidence at the environmental schematic low end (beginning with contextual cueing paradigm), then move progressively upwards in naturalism in the experiments to something that approximates actual human work at the higher end, all along attempting to keep one important environmental property constant. Two experiments using contextual cueing paradigm confirm that collinearly aligned arrays do not encourage recallability of location, while noncollinear arrays appear to create traces that can be recalled automatically. Two other experiments give further demonstration of explicit recollection and location recall, showing that collinear arrangements may in fact induce location recall errors to neighboring collinear objects. We discuss surrounding theoretical, historical, and practical questions.","sentences":["During our nearly constant use of digital devices, perhaps our most frequent need is to visually identify icons representing our content and invoke the actions to manipulate them.","Almost since the inception of user interface design in the 1970s, with rare exception it has become the tendency for programmers to prescribe the arrangement these things in uniform rectilinear rows and columns.","This was imported from theories for print design and ultimately brought into widespread practice for graphical user interfaces (GUIs).","Whether consistent rectilinearity actually does better than less rectilinear arrangements to maximize selection efficiency has not been challenged on considerations of speed or any other measure.","In a series of four experiments, we explore how alignment may in fact discourage easy recallability of screen object locations and hence increase search intensity.","A second objective is to present a methodological model where we deliberately attempt to begin with psychophysical cognitive evidence at the environmental schematic low end (beginning with contextual cueing paradigm), then move progressively upwards in naturalism in the experiments to something that approximates actual human work at the higher end, all along attempting to keep one important environmental property constant.","Two experiments using contextual cueing paradigm confirm that collinearly aligned arrays do not encourage recallability of location, while noncollinear arrays appear to create traces that can be recalled automatically.","Two other experiments give further demonstration of explicit recollection and location recall, showing that collinear arrangements may in fact induce location recall errors to neighboring collinear objects.","We discuss surrounding theoretical, historical, and practical questions."],"url":"http://arxiv.org/abs/2308.12201v1"}
{"created":"2023-08-23 15:38:26","title":"Towards Real-Time Analysis of Broadcast Badminton Videos","abstract":"Analysis of player movements is a crucial subset of sports analysis. Existing player movement analysis methods use recorded videos after the match is over. In this work, we propose an end-to-end framework for player movement analysis for badminton matches on live broadcast match videos. We only use the visual inputs from the match and, unlike other approaches which use multi-modal sensor data, our approach uses only visual cues. We propose a method to calculate the on-court distance covered by both the players from the video feed of a live broadcast badminton match. To perform this analysis, we focus on the gameplay by removing replays and other redundant parts of the broadcast match. We then perform player tracking to identify and track the movements of both players in each frame. Finally, we calculate the distance covered by each player and the average speed with which they move on the court. We further show a heatmap of the areas covered by the player on the court which is useful for analyzing the gameplay of the player. Our proposed framework was successfully used to analyze live broadcast matches in real-time during the Premier Badminton League 2019 (PBL 2019), with commentators and broadcasters appreciating the utility.","sentences":["Analysis of player movements is a crucial subset of sports analysis.","Existing player movement analysis methods use recorded videos after the match is over.","In this work, we propose an end-to-end framework for player movement analysis for badminton matches on live broadcast match videos.","We only use the visual inputs from the match and, unlike other approaches which use multi-modal sensor data, our approach uses only visual cues.","We propose a method to calculate the on-court distance covered by both the players from the video feed of a live broadcast badminton match.","To perform this analysis, we focus on the gameplay by removing replays and other redundant parts of the broadcast match.","We then perform player tracking to identify and track the movements of both players in each frame.","Finally, we calculate the distance covered by each player and the average speed with which they move on the court.","We further show a heatmap of the areas covered by the player on the court which is useful for analyzing the gameplay of the player.","Our proposed framework was successfully used to analyze live broadcast matches in real-time during the Premier Badminton League 2019 (PBL 2019), with commentators and broadcasters appreciating the utility."],"url":"http://arxiv.org/abs/2308.12199v1"}
{"created":"2023-08-23 15:35:02","title":"Inferring Human Intentions from Predicted Action Probabilities","abstract":"Predicting the next action that a human is most likely to perform is key to human-AI collaboration and has consequently attracted increasing research interests in recent years. An important factor for next action prediction are human intentions: If the AI agent knows the intention it can predict future actions and plan collaboration more effectively. Existing Bayesian methods for this task struggle with complex visual input while deep neural network (DNN) based methods do not provide uncertainty quantifications. In this work we combine both approaches for the first time and show that the predicted next action probabilities contain information that can be used to infer the underlying intention. We propose a two-step approach to human intention prediction: While a DNN predicts the probabilities of the next action, MCMC-based Bayesian inference is used to infer the underlying intention from these predictions. This approach not only allows for independent design of the DNN architecture but also the subsequently fast, design-independent inference of human intentions. We evaluate our method using a series of experiments on the Watch-And-Help (WAH) and a keyboard and mouse interaction dataset. Our results show that our approach can accurately predict human intentions from observed actions and the implicit information contained in next action probabilities. Furthermore, we show that our approach can predict the correct intention even if only few actions have been observed.","sentences":["Predicting the next action that a human is most likely to perform is key to human-AI collaboration and has consequently attracted increasing research interests in recent years.","An important factor for next action prediction are human intentions: If the AI agent knows the intention it can predict future actions and plan collaboration more effectively.","Existing Bayesian methods for this task struggle with complex visual input while deep neural network (DNN) based methods do not provide uncertainty quantifications.","In this work we combine both approaches for the first time and show that the predicted next action probabilities contain information that can be used to infer the underlying intention.","We propose a two-step approach to human intention prediction: While a DNN predicts the probabilities of the next action, MCMC-based Bayesian inference is used to infer the underlying intention from these predictions.","This approach not only allows for independent design of the DNN architecture but also the subsequently fast, design-independent inference of human intentions.","We evaluate our method using a series of experiments on the Watch-And-Help (WAH) and a keyboard and mouse interaction dataset.","Our results show that our approach can accurately predict human intentions from observed actions and the implicit information contained in next action probabilities.","Furthermore, we show that our approach can predict the correct intention even if only few actions have been observed."],"url":"http://arxiv.org/abs/2308.12194v1"}
{"created":"2023-08-23 15:30:44","title":"Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques","abstract":"This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep. Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models.","sentences":["This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques.","They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model.","To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds.","We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees.","In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states.","In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples.","These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep.","Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models."],"url":"http://arxiv.org/abs/2308.12192v1"}
{"created":"2023-08-23 15:27:50","title":"Sign Language Translation with Iterative Prototype","abstract":"This paper presents IP-SLT, a simple yet effective framework for sign language translation (SLT). Our IP-SLT adopts a recurrent structure and enhances the semantic representation (prototype) of the input sign language video via an iterative refinement manner. Our idea mimics the behavior of human reading, where a sentence can be digested repeatedly, till reaching accurate understanding. Technically, IP-SLT consists of feature extraction, prototype initialization, and iterative prototype refinement. The initialization module generates the initial prototype based on the visual feature extracted by the feature extraction module. Then, the iterative refinement module leverages the cross-attention mechanism to polish the previous prototype by aggregating it with the original video feature. Through repeated refinement, the prototype finally converges to a more stable and accurate state, leading to a fluent and appropriate translation. In addition, to leverage the sequential dependence of prototypes, we further propose an iterative distillation loss to compress the knowledge of the final iteration into previous ones. As the autoregressive decoding process is executed only once in inference, our IP-SLT is ready to improve various SLT systems with acceptable overhead. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the IP-SLT.","sentences":["This paper presents IP-SLT, a simple yet effective framework for sign language translation (SLT).","Our IP-SLT adopts a recurrent structure and enhances the semantic representation (prototype) of the input sign language video via an iterative refinement manner.","Our idea mimics the behavior of human reading, where a sentence can be digested repeatedly, till reaching accurate understanding.","Technically, IP-SLT consists of feature extraction, prototype initialization, and iterative prototype refinement.","The initialization module generates the initial prototype based on the visual feature extracted by the feature extraction module.","Then, the iterative refinement module leverages the cross-attention mechanism to polish the previous prototype by aggregating it with the original video feature.","Through repeated refinement, the prototype finally converges to a more stable and accurate state, leading to a fluent and appropriate translation.","In addition, to leverage the sequential dependence of prototypes, we further propose an iterative distillation loss to compress the knowledge of the final iteration into previous ones.","As the autoregressive decoding process is executed only once in inference, our IP-SLT is ready to improve various SLT systems with acceptable overhead.","Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the IP-SLT."],"url":"http://arxiv.org/abs/2308.12191v1"}
{"created":"2023-08-23 15:25:17","title":"Development and external validation of a lung cancer risk estimation tool using gradient-boosting","abstract":"Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates. In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years. The study utilized two datasets, the PLCO (n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer. Additionally, a focus was placed on mitigating bias caused by censored data. Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees. The ML model was trained on the pre-processed PLCO dataset and tested on the NLST dataset. The model incorporated features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer. The model was well-calibrated (Brier score=0.044). ROC-AUC was 82% on the PLCO dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When compared to the USPSTF guidelines for lung cancer screening, our model provided the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2% vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years. By utilizing risk factors and clinical data, individuals can assess their risk and make informed decisions regarding lung cancer screening. This research contributes to the efforts in early detection and prevention strategies, aiming to reduce lung cancer-related mortality rates.","sentences":["Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates.","In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years.","The study utilized two datasets, the PLCO (n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer.","Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer.","Additionally, a focus was placed on mitigating bias caused by censored data.","Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees.","The ML model was trained on the pre-processed PLCO dataset and tested on the NLST dataset.","The model incorporated features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer.","The model was well-calibrated (Brier score=0.044).","ROC-AUC was 82% on the PLCO dataset and 70% on the NLST dataset.","PR-AUC was 29% and 11% respectively.","When compared to the USPSTF guidelines for lung cancer screening, our model provided the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2% vs. 3.1% on the NLST dataset.","The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years.","By utilizing risk factors and clinical data, individuals can assess their risk and make informed decisions regarding lung cancer screening.","This research contributes to the efforts in early detection and prevention strategies, aiming to reduce lung cancer-related mortality rates."],"url":"http://arxiv.org/abs/2308.12188v1"}
{"created":"2023-08-23 14:57:41","title":"In-Hand Cube Reconfiguration: Simplified","abstract":"We present a simple approach to in-hand cube reconfiguration. By simplifying planning, control, and perception as much as possible, while maintaining robust and general performance, we gain insights into the inherent complexity of in-hand cube reconfiguration. We also demonstrate the effectiveness of combining GOFAI-based planning with the exploitation of environmental constraints and inherently compliant end-effectors in the context of dexterous manipulation. The proposed system outperforms a substantially more complex system for cube reconfiguration based on deep learning and accurate physical simulation, contributing arguments to the discussion about what the most promising approach to general manipulation might be. Project website: https://rbo.gitlab-pages.tu-berlin.de/robotics/simpleIHM/","sentences":["We present a simple approach to in-hand cube reconfiguration.","By simplifying planning, control, and perception as much as possible, while maintaining robust and general performance, we gain insights into the inherent complexity of in-hand cube reconfiguration.","We also demonstrate the effectiveness of combining GOFAI-based planning with the exploitation of environmental constraints and inherently compliant end-effectors in the context of dexterous manipulation.","The proposed system outperforms a substantially more complex system for cube reconfiguration based on deep learning and accurate physical simulation, contributing arguments to the discussion about what the most promising approach to general manipulation might be.","Project website: https://rbo.gitlab-pages.tu-berlin.de/robotics/simpleIHM/"],"url":"http://arxiv.org/abs/2308.12178v1"}
{"created":"2023-08-23 14:56:53","title":"On the Existence of EFX (and Pareto-Optimal) Allocations for Binary Chores","abstract":"We study the problem of allocating a group of indivisible chores among agents while each chore has a binary marginal. We focus on the fairness criteria of envy-freeness up to any item (EFX) and investigate the existence of EFX allocations. We show that when agents have additive binary cost functions, there exist EFX and Pareto-optimal (PO) allocations that can be computed in polynomial time. To the best of our knowledge, this is the first setting of a general number of agents that admits EFX and PO allocations, before which EFX and PO allocations have only been shown to exist for three bivalued agents. We further consider more general cost functions: cancelable and general monotone (both with binary marginal). We show that EFX allocations exist and can be computed for binary cancelable chores, but EFX is incompatible with PO. For general binary marginal functions, we propose an algorithm that computes (partial) envy-free (EF) allocations with at most $n-1$ unallocated items.","sentences":["We study the problem of allocating a group of indivisible chores among agents while each chore has a binary marginal.","We focus on the fairness criteria of envy-freeness up to any item (EFX) and investigate the existence of EFX allocations.","We show that when agents have additive binary cost functions, there exist EFX and Pareto-optimal (PO) allocations that can be computed in polynomial time.","To the best of our knowledge, this is the first setting of a general number of agents that admits EFX and PO allocations, before which EFX and PO allocations have only been shown to exist for three bivalued agents.","We further consider more general cost functions: cancelable and general monotone (both with binary marginal).","We show that EFX allocations exist and can be computed for binary cancelable chores, but EFX is incompatible with PO.","For general binary marginal functions, we propose an algorithm that computes (partial) envy-free (EF) allocations with at most $n-1$ unallocated items."],"url":"http://arxiv.org/abs/2308.12177v1"}
{"created":"2023-08-23 14:53:38","title":"Unsupervised anomalies detection in IIoT edge devices networks using federated learning","abstract":"In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT/ IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT/IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.","sentences":["In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules.","However, some owners are reluctant of availing their data out of the company due to data security concerns.","Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself.","In this scenario, data is not share over the network for training purpose.","Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session.","The devices could be chosen at random, and a device can be aborted.","The resulting models are sent to the coordinating server and then average models from the devices that finished training.","The process is repeated until a desired model accuracy is achieved.","By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitive data for the owners.","In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT/ IIoT device networks.","The results were almost the same as the centralized machine learning approach.","We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training.","This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT/IIoT gadgets developed using Fedavg.","Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work."],"url":"http://arxiv.org/abs/2308.12175v1"}
{"created":"2023-08-23 14:25:22","title":"NPF-200: A Multi-Modal Eye Fixation Dataset and Method for Non-Photorealistic Videos","abstract":"Non-photorealistic videos are in demand with the wave of the metaverse, but lack of sufficient research studies. This work aims to take a step forward to understand how humans perceive non-photorealistic videos with eye fixation (\\ie, saliency detection), which is critical for enhancing media production, artistic design, and game user experience. To fill in the gap of missing a suitable dataset for this research line, we present NPF-200, the first large-scale multi-modal dataset of purely non-photorealistic videos with eye fixations. Our dataset has three characteristics: 1) it contains soundtracks that are essential according to vision and psychological studies; 2) it includes diverse semantic content and videos are of high-quality; 3) it has rich motions across and within videos. We conduct a series of analyses to gain deeper insights into this task and compare several state-of-the-art methods to explore the gap between natural images and non-photorealistic data. Additionally, as the human attention system tends to extract visual and audio features with different frequencies, we propose a universal frequency-aware multi-modal non-photorealistic saliency detection model called NPSNet, demonstrating the state-of-the-art performance of our task. The results uncover strengths and weaknesses of multi-modal network design and multi-domain training, opening up promising directions for future works. {Our dataset and code can be found at \\url{https://github.com/Yangziyu/NPF200}}.","sentences":["Non-photorealistic videos are in demand with the wave of the metaverse, but lack of sufficient research studies.","This work aims to take a step forward to understand how humans perceive non-photorealistic videos with eye fixation (\\ie, saliency detection), which is critical for enhancing media production, artistic design, and game user experience.","To fill in the gap of missing a suitable dataset for this research line, we present NPF-200, the first large-scale multi-modal dataset of purely non-photorealistic videos with eye fixations.","Our dataset has three characteristics: 1) it contains soundtracks that are essential according to vision and psychological studies; 2) it includes diverse semantic content and videos are of high-quality; 3) it has rich motions across and within videos.","We conduct a series of analyses to gain deeper insights into this task and compare several state-of-the-art methods to explore the gap between natural images and non-photorealistic data.","Additionally, as the human attention system tends to extract visual and audio features with different frequencies, we propose a universal frequency-aware multi-modal non-photorealistic saliency detection model called NPSNet, demonstrating the state-of-the-art performance of our task.","The results uncover strengths and weaknesses of multi-modal network design and multi-domain training, opening up promising directions for future works.","{Our dataset and code can be found at \\url{https://github.com/Yangziyu/NPF200}}."],"url":"http://arxiv.org/abs/2308.12163v1"}
{"created":"2023-08-23 14:23:48","title":"Incremental Property Directed Reachability","abstract":"Property Directed Reachability (PDR) is a widely used technique for formal verification of hardware and software systems. This paper presents an incremental version of PDR (IPDR), which enables the automatic verification of system instances of incremental complexity. The proposed algorithm leverages the concept of incremental SAT solvers to reuse verification results from previously verified system instances, thereby accelerating the verification process. The new algorithm supports both incremental constraining and relaxing; i.e., starting from an over-constrained instance that is gradually relaxed.   To validate the effectiveness of the proposed algorithm, we implemented IPDR and experimentally evaluate it on two different problem domains. First, we consider a circuit pebbling problem, where the number of pebbles is both constrained and relaxed. Second, we explore parallel program instances, progressively increasing the allowed number of interleavings. The experimental results demonstrate significant performance improvements compared to Z3's PDR implementation SPACER. Experiments also show that the incremental approach succeeds in reusing a substantial amount of clauses between instances, for both the constraining and relaxing algorithm.","sentences":["Property Directed Reachability (PDR) is a widely used technique for formal verification of hardware and software systems.","This paper presents an incremental version of PDR (IPDR), which enables the automatic verification of system instances of incremental complexity.","The proposed algorithm leverages the concept of incremental SAT solvers to reuse verification results from previously verified system instances, thereby accelerating the verification process.","The new algorithm supports both incremental constraining and relaxing; i.e., starting from an over-constrained instance that is gradually relaxed.   ","To validate the effectiveness of the proposed algorithm, we implemented IPDR and experimentally evaluate it on two different problem domains.","First, we consider a circuit pebbling problem, where the number of pebbles is both constrained and relaxed.","Second, we explore parallel program instances, progressively increasing the allowed number of interleavings.","The experimental results demonstrate significant performance improvements compared to Z3's PDR implementation SPACER.","Experiments also show that the incremental approach succeeds in reusing a substantial amount of clauses between instances, for both the constraining and relaxing algorithm."],"url":"http://arxiv.org/abs/2308.12162v1"}
{"created":"2023-08-23 14:18:56","title":"A Visualization System for Hexahedral Mesh Quality Study","abstract":"In this paper, we introduce a new 3D hex mesh visual analysis system that emphasizes poor-quality areas with an aggregated glyph, highlights overlapping elements, and provides detailed boundary error inspection in three forms. By supporting multi-level analysis through multiple views, our system effectively evaluates various mesh models and compares the performance of mesh generation and optimization algorithms for hexahedral meshes.","sentences":["In this paper, we introduce a new 3D hex mesh visual analysis system that emphasizes poor-quality areas with an aggregated glyph, highlights overlapping elements, and provides detailed boundary error inspection in three forms.","By supporting multi-level analysis through multiple views, our system effectively evaluates various mesh models and compares the performance of mesh generation and optimization algorithms for hexahedral meshes."],"url":"http://arxiv.org/abs/2308.12158v1"}
{"created":"2023-08-23 14:18:44","title":"Evaluation of Faithfulness Using the Longest Supported Subsequence","abstract":"As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarization dataset across six different models. Finally, we compare several popular Large Language Models (LLMs) for faithfulness using this metric. We release the human-annotated dataset built for predicting LSS and our fine-tuned model for evaluating faithfulness.","sentences":["As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering.","Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers.","In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS).","Using a new human-annotated dataset, we finetune a model to generate LSS.","We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not.","Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset.","Our metric consistently outperforms other metrics on a summarization dataset across six different models.","Finally, we compare several popular Large Language Models (LLMs) for faithfulness using this metric.","We release the human-annotated dataset built for predicting LSS and our fine-tuned model for evaluating faithfulness."],"url":"http://arxiv.org/abs/2308.12157v1"}
{"created":"2023-08-23 14:17:44","title":"Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals","abstract":"This paper discusses the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS). The proposed approach presents a novel multimodal learning framework that combines ME and PS, including a 1D separable and mixable depthwise inception network, a standardised normal distribution weighted feature fusion method, and depth/physiology guided attention modules for multimodal learning. Experimental results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance.","sentences":["This paper discusses the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS).","The proposed approach presents a novel multimodal learning framework that combines ME and PS, including a 1D separable and mixable depthwise inception network, a standardised normal distribution weighted feature fusion method, and depth/physiology guided attention modules for multimodal learning.","Experimental results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance."],"url":"http://arxiv.org/abs/2308.12156v1"}
{"created":"2023-08-23 14:05:08","title":"Survey of adaptive containerization architectures for HPC","abstract":"Containers offer an array of advantages that benefit research reproducibility and portability across groups and systems. As container tools mature, container security improves, and High-performance computing (HPC) and cloud system tools converge, supercomputing centers are increasingly integrating containers in their workflows. The technology selection process requires sufficient information on the diverse tools available, yet the majority of research into containers still focuses on cloud environments. We consider an adaptive containerization approach, with a focus on accelerating the deployment of applications and workflows on HPC systems using containers. To this end, we discuss the specific HPC requirements regarding container tools, and analyze the entire containerization stack, including container engines and registries, in-depth. Finally, we consider various orchestrator and HPC workload manager integration scenarios.","sentences":["Containers offer an array of advantages that benefit research reproducibility and portability across groups and systems.","As container tools mature, container security improves, and High-performance computing (HPC) and cloud system tools converge, supercomputing centers are increasingly integrating containers in their workflows.","The technology selection process requires sufficient information on the diverse tools available, yet the majority of research into containers still focuses on cloud environments.","We consider an adaptive containerization approach, with a focus on accelerating the deployment of applications and workflows on HPC systems using containers.","To this end, we discuss the specific HPC requirements regarding container tools, and analyze the entire containerization stack, including container engines and registries, in-depth.","Finally, we consider various orchestrator and HPC workload manager integration scenarios."],"url":"http://arxiv.org/abs/2308.12147v1"}
{"created":"2023-08-23 14:00:58","title":"A Probabilistic Fluctuation based Membership Inference Attack for Generative Models","abstract":"Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.","sentences":["Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model.","MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models.","Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models.","However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice.","Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon.","Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record.","Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records.","We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline."],"url":"http://arxiv.org/abs/2308.12143v1"}
{"created":"2023-08-23 13:56:38","title":"Aparecium: Revealing Secrets from Physical Photographs","abstract":"Watermarking is a crucial tool for safeguarding copyrights and can serve as a more aesthetically pleasing alternative to QR codes. In recent years, watermarking methods based on deep learning have proved superior robustness against complex physical distortions than traditional watermarking methods. However, they have certain limitations that render them less effective in practice. For instance, current solutions necessitate physical photographs to be rectangular for accurate localization, cannot handle physical bending or folding, and require the hidden area to be completely captured at a close distance and small angle. To overcome these challenges, we propose a novel deep watermarking framework dubbed \\textit{Aparecium}. Specifically, we preprocess secrets (i.e., watermarks) into a pattern and then embed it into the cover image, which is symmetrical to the final decoding-then-extracting process. To capture the watermarked region from complex physical scenarios, a locator is also introduced. Besides, we adopt a three-stage training strategy for training convergence. Extensive experiments demonstrate that \\textit{Aparecium} is not only robust against different digital distortions, but also can resist various physical distortions, such as screen-shooting and printing-shooting, even in severe cases including different shapes, curvature, folding, incompleteness, long distances, and big angles while maintaining high visual quality. Furthermore, some ablation studies are also conducted to verify our design.","sentences":["Watermarking is a crucial tool for safeguarding copyrights and can serve as a more aesthetically pleasing alternative to QR codes.","In recent years, watermarking methods based on deep learning have proved superior robustness against complex physical distortions than traditional watermarking methods.","However, they have certain limitations that render them less effective in practice.","For instance, current solutions necessitate physical photographs to be rectangular for accurate localization, cannot handle physical bending or folding, and require the hidden area to be completely captured at a close distance and small angle.","To overcome these challenges, we propose a novel deep watermarking framework dubbed \\textit{Aparecium}.","Specifically, we preprocess secrets (i.e., watermarks) into a pattern and then embed it into the cover image, which is symmetrical to the final decoding-then-extracting process.","To capture the watermarked region from complex physical scenarios, a locator is also introduced.","Besides, we adopt a three-stage training strategy for training convergence.","Extensive experiments demonstrate that \\textit{Aparecium} is not only robust against different digital distortions, but also can resist various physical distortions, such as screen-shooting and printing-shooting, even in severe cases including different shapes, curvature, folding, incompleteness, long distances, and big angles while maintaining high visual quality.","Furthermore, some ablation studies are also conducted to verify our design."],"url":"http://arxiv.org/abs/2308.12141v1"}
{"created":"2023-08-23 13:54:15","title":"Mesh Conflation of Oblique Photogrammetric Models using Virtual Cameras and Truncated Signed Distance Field","abstract":"Conflating/stitching 2.5D raster digital surface models (DSM) into a large one has been a running practice in geoscience applications, however, conflating full-3D mesh models, such as those from oblique photogrammetry, is extremely challenging. In this letter, we propose a novel approach to address this challenge by conflating multiple full-3D oblique photogrammetric models into a single, and seamless mesh for high-resolution site modeling. Given two or more individually collected and created photogrammetric meshes, we first propose to create a virtual camera field (with a panoramic field of view) to incubate virtual spaces represented by Truncated Signed Distance Field (TSDF), an implicit volumetric field friendly for linear 3D fusion; then we adaptively leverage the truncated bound of meshes in TSDF to conflate them into a single and accurate full 3D site model. With drone-based 3D meshes, we show that our approach significantly improves upon traditional methods for model conflations, to drive new potentials to create excessively large and accurate full 3D mesh models in support of geoscience and environmental applications.","sentences":["Conflating/stitching 2.5D raster digital surface models (DSM) into a large one has been a running practice in geoscience applications, however, conflating full-3D mesh models, such as those from oblique photogrammetry, is extremely challenging.","In this letter, we propose a novel approach to address this challenge by conflating multiple full-3D oblique photogrammetric models into a single, and seamless mesh for high-resolution site modeling.","Given two or more individually collected and created photogrammetric meshes, we first propose to create a virtual camera field (with a panoramic field of view) to incubate virtual spaces represented by Truncated Signed Distance Field (TSDF), an implicit volumetric field friendly for linear 3D fusion; then we adaptively leverage the truncated bound of meshes in TSDF to conflate them into a single and accurate full 3D site model.","With drone-based 3D meshes, we show that our approach significantly improves upon traditional methods for model conflations, to drive new potentials to create excessively large and accurate full 3D mesh models in support of geoscience and environmental applications."],"url":"http://arxiv.org/abs/2308.12139v1"}
{"created":"2023-08-23 13:51:54","title":"Select-and-Combine (SAC): A Novel Multi-Stereo Depth Fusion Algorithm for Point Cloud Generation via Efficient Local Markov Netlets","abstract":"Many practical systems for image-based surface reconstruction employ a stereo/multi-stereo paradigm, due to its ability to scale for large scenes and its ease of implementation for out-of-core operations. In this process, multiple and abundant depth maps from stereo matching must be combined and fused into a single, consistent, and clean point cloud. However, the noises and outliers caused by stereo matching and the heterogenous geometric errors of the poses present a challenge for existing fusion algorithms, since they mostly assume Gaussian errors and predict fused results based on data from local spatial neighborhoods, which may inherit uncertainties from multiple depths resulting in lowered accuracy. In this paper, we propose a novel depth fusion paradigm, that instead of numerically fusing points from multiple depth maps, selects the best depth map per point, and combines them into a single and clean point cloud. This paradigm, called select-and-combine (SAC), is achieved through modeling the point level fusion using local Markov Netlets, a micro-network over point across neighboring views for depth/view selection, followed by a Netlets collapse process for point combination. The Markov Netlets are optimized such that they can inherently leverage spatial consistencies among depth maps of neighboring views, thus they can address errors beyond Gaussian ones. Our experiment results show that our approach outperforms existing depth fusion approaches by increasing the F1 score that considers both accuracy and completeness by 2.07% compared to the best existing method. Finally, our approach generates clearer point clouds that are 18% less redundant while with a higher accuracy before fusion","sentences":["Many practical systems for image-based surface reconstruction employ a stereo/multi-stereo paradigm, due to its ability to scale for large scenes and its ease of implementation for out-of-core operations.","In this process, multiple and abundant depth maps from stereo matching must be combined and fused into a single, consistent, and clean point cloud.","However, the noises and outliers caused by stereo matching and the heterogenous geometric errors of the poses present a challenge for existing fusion algorithms, since they mostly assume Gaussian errors and predict fused results based on data from local spatial neighborhoods, which may inherit uncertainties from multiple depths resulting in lowered accuracy.","In this paper, we propose a novel depth fusion paradigm, that instead of numerically fusing points from multiple depth maps, selects the best depth map per point, and combines them into a single and clean point cloud.","This paradigm, called select-and-combine (SAC), is achieved through modeling the point level fusion using local Markov Netlets, a micro-network over point across neighboring views for depth/view selection, followed by a Netlets collapse process for point combination.","The Markov Netlets are optimized such that they can inherently leverage spatial consistencies among depth maps of neighboring views, thus they can address errors beyond Gaussian ones.","Our experiment results show that our approach outperforms existing depth fusion approaches by increasing the F1 score that considers both accuracy and completeness by 2.07% compared to the best existing method.","Finally, our approach generates clearer point clouds that are 18% less redundant while with a higher accuracy before fusion"],"url":"http://arxiv.org/abs/2308.12138v1"}
{"created":"2023-08-23 13:44:14","title":"DarkDiff: Explainable web page similarity of TOR onion sites","abstract":"In large-scale data analysis, near-duplicates are often a problem. For example, with two near-duplicate phishing emails, a difference in the salutation (Mr versus Ms) is not essential, but whether it is bank A or B is important. The state-of-the-art in near-duplicate detection is a black box approach (MinHash), so one only knows that emails are near-duplicates, but not why. We present DarkDiff, which can efficiently detect near-duplicates while providing the reason why there is a near-duplicate. We have developed DarkDiff to detect near-duplicates of homepages on the Darkweb. DarkDiff works well on those pages because they resemble the clear web of the past.","sentences":["In large-scale data analysis, near-duplicates are often a problem.","For example, with two near-duplicate phishing emails, a difference in the salutation (Mr versus Ms) is not essential, but whether it is bank A or B is important.","The state-of-the-art in near-duplicate detection is a black box approach (MinHash), so one only knows that emails are near-duplicates, but not why.","We present DarkDiff, which can efficiently detect near-duplicates while providing the reason why there is a near-duplicate.","We have developed DarkDiff to detect near-duplicates of homepages on the Darkweb.","DarkDiff works well on those pages because they resemble the clear web of the past."],"url":"http://arxiv.org/abs/2308.12134v1"}
{"created":"2023-08-23 13:43:42","title":"Lite-HRNet Plus: Fast and Accurate Facial Landmark Detection","abstract":"Facial landmark detection is an essential technology for driver status tracking and has been in demand for real-time estimations. As a landmark coordinate prediction, heatmap-based methods are known to achieve a high accuracy, and Lite-HRNet can achieve a fast estimation. However, with Lite-HRNet, the problem of a heavy computational cost of the fusion block, which connects feature maps with different resolutions, has yet to be solved. In addition, the strong output module used in HRNetV2 is not applied to Lite-HRNet. Given these problems, we propose a novel architecture called Lite-HRNet Plus. Lite-HRNet Plus achieves two improvements: a novel fusion block based on a channel attention and a novel output module with less computational intensity using multi-resolution feature maps. Through experiments conducted on two facial landmark datasets, we confirmed that Lite-HRNet Plus further improved the accuracy in comparison with conventional methods, and achieved a state-of-the-art accuracy with a computational complexity with the range of 10M FLOPs.","sentences":["Facial landmark detection is an essential technology for driver status tracking and has been in demand for real-time estimations.","As a landmark coordinate prediction, heatmap-based methods are known to achieve a high accuracy, and Lite-HRNet can achieve a fast estimation.","However, with Lite-HRNet, the problem of a heavy computational cost of the fusion block, which connects feature maps with different resolutions, has yet to be solved.","In addition, the strong output module used in HRNetV2 is not applied to Lite-HRNet.","Given these problems, we propose a novel architecture called Lite-HRNet Plus.","Lite-HRNet Plus achieves two improvements: a novel fusion block based on a channel attention and a novel output module with less computational intensity using multi-resolution feature maps.","Through experiments conducted on two facial landmark datasets, we confirmed that Lite-HRNet Plus further improved the accuracy in comparison with conventional methods, and achieved a state-of-the-art accuracy with a computational complexity with the range of 10M FLOPs."],"url":"http://arxiv.org/abs/2308.12133v1"}
{"created":"2023-08-23 13:37:02","title":"Semantic Change Detection for the Romanian Language","abstract":"Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora. In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets. To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss. The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change.","sentences":["Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora.","In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets.","To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA).","Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss.","The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change."],"url":"http://arxiv.org/abs/2308.12131v1"}
{"created":"2023-08-23 13:35:36","title":"Resiliency Analysis of LLM generated models for Industrial Automation","abstract":"This paper proposes a study of the resilience and efficiency of automatically generated industrial automation and control systems using Large Language Models (LLMs). The approach involves modeling the system using percolation theory to estimate its resilience and formulating the design problem as an optimization problem subject to constraints. Techniques from stochastic optimization and regret analysis are used to find a near-optimal solution with provable regret bounds. The study aims to provide insights into the effectiveness and reliability of automatically generated systems in industrial automation and control, and to identify potential areas for improvement in their design and implementation.","sentences":["This paper proposes a study of the resilience and efficiency of automatically generated industrial automation and control systems using Large Language Models (LLMs).","The approach involves modeling the system using percolation theory to estimate its resilience and formulating the design problem as an optimization problem subject to constraints.","Techniques from stochastic optimization and regret analysis are used to find a near-optimal solution with provable regret bounds.","The study aims to provide insights into the effectiveness and reliability of automatically generated systems in industrial automation and control, and to identify potential areas for improvement in their design and implementation."],"url":"http://arxiv.org/abs/2308.12129v1"}
{"created":"2023-08-23 13:33:39","title":"Masking Strategies for Background Bias Removal in Computer Vision Models","abstract":"Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.","sentences":["Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds.","To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT).","We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background.","Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds.","The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance.","Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness."],"url":"http://arxiv.org/abs/2308.12127v1"}
{"created":"2023-08-23 13:16:31","title":"An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators","abstract":"Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a research-oriented 45 nm process.","sentences":["Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML.","To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms.","It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy.","In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters.","Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a research-oriented 45 nm process."],"url":"http://arxiv.org/abs/2308.12120v1"}
{"created":"2023-08-23 13:12:06","title":"Multi-UAV Deployment in Obstacle-Cluttered Environments with LOS Connectivity","abstract":"A reliable communication network is essential for multiple UAVs operating within obstacle-cluttered environments, where limited communication due to obstructions often occurs. A common solution is to deploy intermediate UAVs to relay information via a multi-hop network, which introduces two challenges: (i) how to design the structure of multi-hop networks; and (ii) how to maintain connectivity during collaborative motion. To this end, this work first proposes an efficient constrained search method based on the minimum-edge RRT$^\\star$ algorithm, to find a spanning-tree topology that requires a less number of UAVs for the deployment task. To achieve this deployment, a distributed model predictive control strategy is proposed for the online motion coordination. It explicitly incorporates not only the inter-UAV and UAV-obstacle distance constraints, but also the line-of-sight (LOS) connectivity constraint. These constraints are well-known to be nonlinear and often tackled by various approximations. In contrast, this work provides a theoretical guarantee that all agent trajectories are ensured to be collision-free with a team-wise LOS connectivity at all time. Numerous simulations are performed in 3D valley-like environments, while hardware experiments validate its dynamic adaptation when the deployment position changes online.","sentences":["A reliable communication network is essential for multiple UAVs operating within obstacle-cluttered environments, where limited communication due to obstructions often occurs.","A common solution is to deploy intermediate UAVs to relay information via a multi-hop network, which introduces two challenges: (i) how to design the structure of multi-hop networks; and (ii) how to maintain connectivity during collaborative motion.","To this end, this work first proposes an efficient constrained search method based on the minimum-edge RRT$^\\star$ algorithm, to find a spanning-tree topology that requires a less number of UAVs for the deployment task.","To achieve this deployment, a distributed model predictive control strategy is proposed for the online motion coordination.","It explicitly incorporates not only the inter-UAV and UAV-obstacle distance constraints, but also the line-of-sight (LOS) connectivity constraint.","These constraints are well-known to be nonlinear and often tackled by various approximations.","In contrast, this work provides a theoretical guarantee that all agent trajectories are ensured to be collision-free with a team-wise LOS connectivity at all time.","Numerous simulations are performed in 3D valley-like environments, while hardware experiments validate its dynamic adaptation when the deployment position changes online."],"url":"http://arxiv.org/abs/2308.12117v1"}
{"created":"2023-08-23 13:10:33","title":"The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures","abstract":"Segmenting cells and tracking their motion over time is a common task in biomedical applications. However, predicting accurate instance-wise segmentation and cell motions from microscopy imagery remains a challenging task. Using microstructured environments for analyzing single cells in a constant flow of media adds additional complexity. While large-scale labeled microscopy datasets are available, we are not aware of any large-scale dataset, including both cells and microstructures. In this paper, we introduce the trapped yeast cell (TYC) dataset, a novel dataset for understanding instance-level semantics and motions of cells in microstructures. We release $105$ dense annotated high-resolution brightfield microscopy images, including about $19$k instance masks. We also release $261$ curated video clips composed of $1293$ high-resolution microscopy images to facilitate unsupervised understanding of cell motions and morphology. TYC offers ten times more instance annotations than the previously largest dataset, including cells and microstructures. Our effort also exceeds previous attempts in terms of microstructure variability, resolution, complexity, and capturing device (microscopy) variability. We facilitate a unified comparison on our novel dataset by introducing a standardized evaluation strategy. TYC and evaluation code are publicly available under CC BY 4.0 license.","sentences":["Segmenting cells and tracking their motion over time is a common task in biomedical applications.","However, predicting accurate instance-wise segmentation and cell motions from microscopy imagery remains a challenging task.","Using microstructured environments for analyzing single cells in a constant flow of media adds additional complexity.","While large-scale labeled microscopy datasets are available, we are not aware of any large-scale dataset, including both cells and microstructures.","In this paper, we introduce the trapped yeast cell (TYC) dataset, a novel dataset for understanding instance-level semantics and motions of cells in microstructures.","We release $105$ dense annotated high-resolution brightfield microscopy images, including about $19$k instance masks.","We also release $261$ curated video clips composed of $1293$ high-resolution microscopy images to facilitate unsupervised understanding of cell motions and morphology.","TYC offers ten times more instance annotations than the previously largest dataset, including cells and microstructures.","Our effort also exceeds previous attempts in terms of microstructure variability, resolution, complexity, and capturing device (microscopy) variability.","We facilitate a unified comparison on our novel dataset by introducing a standardized evaluation strategy.","TYC and evaluation code are publicly available under CC BY 4.0 license."],"url":"http://arxiv.org/abs/2308.12116v1"}
{"created":"2023-08-23 13:09:12","title":"Theory vs. Practice in Modeling Edge Storage Systems","abstract":"Edge systems promise to bring data and computing closer to the users of time-critical applications. Specifically, edge storage systems are emerging as a new system paradigm, where users can retrieve data from small-scale servers inter-operating at the network's edge. The analysis, design, and optimization of such systems require a tractable model that will reflect their costs and bottlenecks. Alas, most existing mathematical models for edge systems focus on stateless tasks, network performance, or isolated nodes and are inapplicable for evaluating edge-based storage performance.   We analyze the capacity-region model - the most promising model proposed so far for edge storage systems. The model addresses the system's ability to serve a set of user demands. Our analysis reveals five inherent gaps between this model and reality, demonstrating the significant remaining challenges in modeling storage service at the edge.","sentences":["Edge systems promise to bring data and computing closer to the users of time-critical applications.","Specifically, edge storage systems are emerging as a new system paradigm, where users can retrieve data from small-scale servers inter-operating at the network's edge.","The analysis, design, and optimization of such systems require a tractable model that will reflect their costs and bottlenecks.","Alas, most existing mathematical models for edge systems focus on stateless tasks, network performance, or isolated nodes and are inapplicable for evaluating edge-based storage performance.   ","We analyze the capacity-region model - the most promising model proposed so far for edge storage systems.","The model addresses the system's ability to serve a set of user demands.","Our analysis reveals five inherent gaps between this model and reality, demonstrating the significant remaining challenges in modeling storage service at the edge."],"url":"http://arxiv.org/abs/2308.12115v1"}
{"created":"2023-08-23 13:09:03","title":"Less is More -- Towards parsimonious multi-task models using structured sparsity","abstract":"Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also investigate how changing the sparsification degree impacts both the performance of the model and the sparsity of groups.","sentences":["Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups.","This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model.","Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference.","We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN).","This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks.","We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ.","We also investigate how changing the sparsification degree impacts both the performance of the model and the sparsity of groups."],"url":"http://arxiv.org/abs/2308.12114v1"}
{"created":"2023-08-23 13:06:59","title":"Advancements in Point Cloud Data Augmentation for Deep Learning: A Survey","abstract":"Point cloud has a wide range of applications in areas such as autonomous driving, mapping, navigation, scene reconstruction, and medical imaging. Due to its great potentials in these applications, point cloud processing has gained great attention in the field of computer vision. Among various point cloud processing techniques, deep learning (DL) has become one of the mainstream and effective methods for tasks such as detection, segmentation and classification. To reduce overfitting during training DL models and improve model performance especially when the amount and/or diversity of training data are limited, augmentation is often crucial. Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods. Therefore, this article surveys and discusses these methods and categorizes them into a taxonomy framework. Through the comprehensive evaluation and comparison of the augmentation methods, this article identifies their potentials and limitations and suggests possible future research directions. This work helps researchers gain a holistic understanding of the current status of point cloud data augmentation and promotes its wider application and development.","sentences":["Point cloud has a wide range of applications in areas such as autonomous driving, mapping, navigation, scene reconstruction, and medical imaging.","Due to its great potentials in these applications, point cloud processing has gained great attention in the field of computer vision.","Among various point cloud processing techniques, deep learning (DL) has become one of the mainstream and effective methods for tasks such as detection, segmentation and classification.","To reduce overfitting during training DL models and improve model performance especially when the amount and/or diversity of training data are limited, augmentation is often crucial.","Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods.","Therefore, this article surveys and discusses these methods and categorizes them into a taxonomy framework.","Through the comprehensive evaluation and comparison of the augmentation methods, this article identifies their potentials and limitations and suggests possible future research directions.","This work helps researchers gain a holistic understanding of the current status of point cloud data augmentation and promotes its wider application and development."],"url":"http://arxiv.org/abs/2308.12113v1"}
{"created":"2023-08-23 13:02:52","title":"Generalized Continual Category Discovery","abstract":"Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of novel classes are present. In light of these limitations, we propose a method that incorporates both supervised and unsupervised signals and mitigates the forgetting through the use of centroid adaptation. Our method surpasses strong CL methods adopted for GCD techniques and presents a superior representation learning performance.","sentences":["Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge.","However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes.","Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption.","Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them.","We call this setting Generalized Continual Category Discovery (GCCD).","It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios.","With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of novel classes are present.","In light of these limitations, we propose a method that incorporates both supervised and unsupervised signals and mitigates the forgetting through the use of centroid adaptation.","Our method surpasses strong CL methods adopted for GCD techniques and presents a superior representation learning performance."],"url":"http://arxiv.org/abs/2308.12112v1"}
{"created":"2023-08-23 12:58:51","title":"Cross-Modality Proposal-guided Feature Mining for Unregistered RGB-Thermal Pedestrian Detection","abstract":"RGB-Thermal (RGB-T) pedestrian detection aims to locate the pedestrians in RGB-T image pairs to exploit the complementation between the two modalities for improving detection robustness in extreme conditions. Most existing algorithms assume that the RGB-T image pairs are well registered, while in the real world they are not aligned ideally due to parallax or different field-of-view of the cameras. The pedestrians in misaligned image pairs may locate at different positions in two images, which results in two challenges: 1) how to achieve inter-modality complementation using spatially misaligned RGB-T pedestrian patches, and 2) how to recognize the unpaired pedestrians at the boundary. To deal with these issues, we propose a new paradigm for unregistered RGB-T pedestrian detection, which predicts two separate pedestrian locations in the RGB and thermal images, respectively. Specifically, we propose a cross-modality proposal-guided feature mining (CPFM) mechanism to extract the two precise fusion features for representing the pedestrian in the two modalities, even if the RGB-T image pair is unaligned. It enables us to effectively exploit the complementation between the two modalities. With the CPFM mechanism, we build a two-stream dense detector; it predicts the two pedestrian locations in the two modalities based on the corresponding fusion feature mined by the CPFM mechanism. Besides, we design a data augmentation method, named Homography, to simulate the discrepancy in scales and views between images. We also investigate two non-maximum suppression (NMS) methods for post-processing. Favorable experimental results demonstrate the effectiveness and robustness of our method in dealing with unregistered pedestrians with different shifts.","sentences":["RGB-Thermal (RGB-T) pedestrian detection aims to locate the pedestrians in RGB-T image pairs to exploit the complementation between the two modalities for improving detection robustness in extreme conditions.","Most existing algorithms assume that the RGB-T image pairs are well registered, while in the real world they are not aligned ideally due to parallax or different field-of-view of the cameras.","The pedestrians in misaligned image pairs may locate at different positions in two images, which results in two challenges: 1) how to achieve inter-modality complementation using spatially misaligned RGB-T pedestrian patches, and 2) how to recognize the unpaired pedestrians at the boundary.","To deal with these issues, we propose a new paradigm for unregistered RGB-T pedestrian detection, which predicts two separate pedestrian locations in the RGB and thermal images, respectively.","Specifically, we propose a cross-modality proposal-guided feature mining (CPFM) mechanism to extract the two precise fusion features for representing the pedestrian in the two modalities, even if the RGB-T image pair is unaligned.","It enables us to effectively exploit the complementation between the two modalities.","With the CPFM mechanism, we build a two-stream dense detector; it predicts the two pedestrian locations in the two modalities based on the corresponding fusion feature mined by the CPFM mechanism.","Besides, we design a data augmentation method, named Homography, to simulate the discrepancy in scales and views between images.","We also investigate two non-maximum suppression (NMS) methods for post-processing.","Favorable experimental results demonstrate the effectiveness and robustness of our method in dealing with unregistered pedestrians with different shifts."],"url":"http://arxiv.org/abs/2308.12111v1"}
{"created":"2023-08-23 12:58:40","title":"Constrained Stein Variational Trajectory Optimization","abstract":"We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO succeeds in 20/20 trials vs 13/20 for the closest baseline. Our results demonstrate that generating diverse constraint-satisfying trajectories improves robustness to disturbances and initialization over baselines.","sentences":["We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel.","We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories.","Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints.","CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima.","By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization.","We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO succeeds in 20/20 trials vs 13/20 for the closest baseline.","Our results demonstrate that generating diverse constraint-satisfying trajectories improves robustness to disturbances and initialization over baselines."],"url":"http://arxiv.org/abs/2308.12110v1"}
{"created":"2023-08-23 12:53:50","title":"Optimal Linear Precoder Design for MIMO-OFDM Integrated Sensing and Communications Based on Bayesian Cram\u00e9r-Rao Bound","abstract":"In this paper, we investigate the fundamental limits of MIMO-OFDM integrated sensing and communications (ISAC) systems based on a Bayesian Cram\\'er-Rao bound (BCRB) analysis. We derive the BCRB for joint channel parameter estimation and data symbol detection, in which a performance trade-off between both functionalities is observed. We formulate the optimization problem for a linear precoder design and propose the stochastic Riemannian gradient descent (SRGD) approach to solve the non-convex problem. We analyze the optimality conditions and show that SRGD ensures convergence with high probability. The simulation results verify our analyses and also demonstrate a fast convergence speed. Finally, the performance trade-off is illustrated and investigated.","sentences":["In this paper, we investigate the fundamental limits of MIMO-OFDM integrated sensing and communications (ISAC) systems based on a Bayesian Cram\\'er-Rao bound (BCRB) analysis.","We derive the BCRB for joint channel parameter estimation and data symbol detection, in which a performance trade-off between both functionalities is observed.","We formulate the optimization problem for a linear precoder design and propose the stochastic Riemannian gradient descent (SRGD) approach to solve the non-convex problem.","We analyze the optimality conditions and show that SRGD ensures convergence with high probability.","The simulation results verify our analyses and also demonstrate a fast convergence speed.","Finally, the performance trade-off is illustrated and investigated."],"url":"http://arxiv.org/abs/2308.12106v1"}
{"created":"2023-08-23 12:36:57","title":"Instruction Position Matters in Sequence Generation with Large Language Models","abstract":"Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequence generation tasks (translation and summarization), without any additional data or annotation costs. Notably, our method significantly improves the zero-shot performance on conditional sequence generation, e.g., up to 9.7 BLEU points on WMT zero-shot translation tasks.","sentences":["Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning.","The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response.","Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences.","To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences.","Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities.","Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequence generation tasks (translation and summarization), without any additional data or annotation costs.","Notably, our method significantly improves the zero-shot performance on conditional sequence generation, e.g., up to 9.7 BLEU points on WMT zero-shot translation tasks."],"url":"http://arxiv.org/abs/2308.12097v1"}
{"created":"2023-08-23 12:28:18","title":"On Using Information Retrieval to Recommend Machine Learning Good Practices for Software Engineers","abstract":"Machine learning (ML) is nowadays widely used for different purposes and in several disciplines. From self-driving cars to automated medical diagnosis, machine learning models extensively support users' daily activities, and software engineering tasks are no exception. Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results. Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&A systems when looking for help and guidance when implementing ML systems. To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the user's context. As a first step in creating a recommender system for machine learning practices, we implemented Idaka. A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model. The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca. The platform has been designed to allow comparative studies of best practices retrieval tools. Idaka is publicly available at GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM.","sentences":["Machine learning (ML) is nowadays widely used for different purposes and in several disciplines.","From self-driving cars to automated medical diagnosis, machine learning models extensively support users' daily activities, and software engineering tasks are no exception.","Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results.","Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&A systems when looking for help and guidance when implementing ML systems.","To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the user's context.","As a first step in creating a recommender system for machine learning practices, we implemented Idaka.","A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model.","The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca.","The platform has been designed to allow comparative studies of best practices retrieval tools.","Idaka is publicly available at GitHub: https://bit.ly/idaka.","Video: https://youtu.be/cEb-AhIPxnM."],"url":"http://arxiv.org/abs/2308.12095v1"}
{"created":"2023-08-23 12:27:55","title":"Cached Operator Reordering: A Unified View for Fast GNN Training","abstract":"Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks in training large-scale GNN models.","sentences":["Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering.","However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks.","We address these challenges by providing a unified view of GNN computation, I/O, and memory.","By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies.","We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art.","Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x.","The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks in training large-scale GNN models."],"url":"http://arxiv.org/abs/2308.12093v1"}
{"created":"2023-08-23 12:20:06","title":"Trajectory Tracking Control of Dual-PAM Soft Actuator with Hysteresis Compensator","abstract":"Soft robotics is an emergent and swiftly evolving field. Pneumatic actuators are suitable for driving soft robots because of their superior performance. However, their control is not easy due to their hysteresis characteristics. In response to these challenges, we propose an adaptive control method to compensate hysteresis of a soft actuator. Employing a novel dual pneumatic artificial muscle (PAM) bending actuator, the innovative control strategy abates hysteresis effects by dynamically modulating gains within a traditional PID controller corresponding with the predicted motion of the reference trajectory. Through comparative experimental evaluation, we found that the new control method outperforms its conventional counterparts regarding tracking accuracy and response speed. Our work reveals a new direction for advancing control in soft actuators.","sentences":["Soft robotics is an emergent and swiftly evolving field.","Pneumatic actuators are suitable for driving soft robots because of their superior performance.","However, their control is not easy due to their hysteresis characteristics.","In response to these challenges, we propose an adaptive control method to compensate hysteresis of a soft actuator.","Employing a novel dual pneumatic artificial muscle (PAM) bending actuator, the innovative control strategy abates hysteresis effects by dynamically modulating gains within a traditional PID controller corresponding with the predicted motion of the reference trajectory.","Through comparative experimental evaluation, we found that the new control method outperforms its conventional counterparts regarding tracking accuracy and response speed.","Our work reveals a new direction for advancing control in soft actuators."],"url":"http://arxiv.org/abs/2308.12088v1"}
{"created":"2023-08-23 12:11:27","title":"Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments","abstract":"Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision-making tasks within cybersecurity.   Furthermore, we introduce a new network security environment named NetSecGame. The environment is designed to eventually support complex multi-agent scenarios within the network security domain. The proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.","sentences":["Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks.","Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios.","This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   ","We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments.","Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations.","In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process.","This design highlights the potential of LLMs to efficiently address complex decision-making tasks within cybersecurity.   ","Furthermore, we introduce a new network security environment named NetSecGame.","The environment is designed to eventually support complex multi-agent scenarios within the network security domain.","The proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios."],"url":"http://arxiv.org/abs/2308.12086v1"}
{"created":"2023-08-23 12:05:48","title":"Counterfactual Graph Augmentation for Consumer Unfairness Mitigation in Recommender Systems","abstract":"In recommendation literature, explainability and fairness are becoming two prominent perspectives to consider. However, prior works have mostly addressed them separately, for instance by explaining to consumers why a certain item was recommended or mitigating disparate impacts in recommendation utility. None of them has leveraged explainability techniques to inform unfairness mitigation. In this paper, we propose an approach that relies on counterfactual explanations to augment the set of user-item interactions, such that using them while inferring recommendations leads to fairer outcomes. Modeling user-item interactions as a bipartite graph, our approach augments the latter by identifying new user-item edges that not only can explain the original unfairness by design, but can also mitigate it. Experiments on two public data sets show that our approach effectively leads to a better trade-off between fairness and recommendation utility compared with state-of-the-art mitigation procedures. We further analyze the characteristics of added edges to highlight key unfairness patterns. Source code available at https://github.com/jackmedda/RS-BGExplainer/tree/cikm2023.","sentences":["In recommendation literature, explainability and fairness are becoming two prominent perspectives to consider.","However, prior works have mostly addressed them separately, for instance by explaining to consumers why a certain item was recommended or mitigating disparate impacts in recommendation utility.","None of them has leveraged explainability techniques to inform unfairness mitigation.","In this paper, we propose an approach that relies on counterfactual explanations to augment the set of user-item interactions, such that using them while inferring recommendations leads to fairer outcomes.","Modeling user-item interactions as a bipartite graph, our approach augments the latter by identifying new user-item edges that not only can explain the original unfairness by design, but can also mitigate it.","Experiments on two public data sets show that our approach effectively leads to a better trade-off between fairness and recommendation utility compared with state-of-the-art mitigation procedures.","We further analyze the characteristics of added edges to highlight key unfairness patterns.","Source code available at https://github.com/jackmedda/RS-BGExplainer/tree/cikm2023."],"url":"http://arxiv.org/abs/2308.12083v1"}
{"created":"2023-08-23 12:01:28","title":"Path-Constrained State Estimation for Rail Vehicles","abstract":"Globally rising demand for transportation by rail is pushing existing infrastructure to its capacity limits, necessitating the development of accurate, robust, and high-frequency positioning systems to ensure safe and efficient train operation. As individual sensor modalities cannot satisfy the strict requirements of robustness and safety, a combination thereof is required. We propose a path-constrained sensor fusion framework to integrate various modalities while leveraging the unique characteristics of the railway network. To reflect the constrained motion of rail vehicles along their tracks, the state is modeled in 1D along the track geometry. We further leverage the limited action space of a train by employing a novel multi-hypothesis tracking to account for multiple possible trajectories a vehicle can take through the railway network. We demonstrate the reliability and accuracy of our fusion framework on multiple tram datasets recorded in the city of Zurich, utilizing Visual-Inertial Odometry for local motion estimation and a standard GNSS for global localization. We evaluate our results using ground truth localizations recorded with a RTK-GNSS, and compare our method to standard baselines. A Root Mean Square Error of 4.78 m and a track selectivity score of up to 94.9 % have been achieved.","sentences":["Globally rising demand for transportation by rail is pushing existing infrastructure to its capacity limits, necessitating the development of accurate, robust, and high-frequency positioning systems to ensure safe and efficient train operation.","As individual sensor modalities cannot satisfy the strict requirements of robustness and safety, a combination thereof is required.","We propose a path-constrained sensor fusion framework to integrate various modalities while leveraging the unique characteristics of the railway network.","To reflect the constrained motion of rail vehicles along their tracks, the state is modeled in 1D along the track geometry.","We further leverage the limited action space of a train by employing a novel multi-hypothesis tracking to account for multiple possible trajectories a vehicle can take through the railway network.","We demonstrate the reliability and accuracy of our fusion framework on multiple tram datasets recorded in the city of Zurich, utilizing Visual-Inertial Odometry for local motion estimation and a standard GNSS for global localization.","We evaluate our results using ground truth localizations recorded with a RTK-GNSS, and compare our method to standard baselines.","A Root Mean Square Error of 4.78 m and a track selectivity score of up to 94.9 % have been achieved."],"url":"http://arxiv.org/abs/2308.12082v1"}
