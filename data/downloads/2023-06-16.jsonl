{"created":"2023-06-15","title":"CMMLU: Measuring massive multitask language understanding in Chinese","abstract":"As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models within the Chinese context.","sentences":["As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging.","This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities.","We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings.","The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%.","This highlights significant room for improvement in LLMs.","Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs.","CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models within the Chinese context."],"url":"http://arxiv.org/abs/2306.09212v1"}
{"created":"2023-06-15","title":"Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models","abstract":"We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree. We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS. Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images. We fine-tune an open-source large language model on this dataset. We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type. By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes through few-shot learning. Our analysis offers valuable insights into course prerequisites and curriculum design, highlighting language models' potential for learning and improving Mathematics and EECS education.","sentences":["We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree.","We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS.","Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images.","We fine-tune an open-source large language model on this dataset.","We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type.","By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes through few-shot learning.","Our analysis offers valuable insights into course prerequisites and curriculum design, highlighting language models' potential for learning and improving Mathematics and EECS education."],"url":"http://arxiv.org/abs/2306.08997v1"}
{"created":"2023-06-15","title":"Prompt Performance Prediction for Generative IR","abstract":"The ability to predict the performance of a query in Information Retrieval (IR) systems has been a longstanding challenge. In this paper, we introduce a novel task called \"Prompt Performance Prediction\" that aims to predict the performance of a query, referred to as a prompt, before obtaining the actual search results. The context of our task leverages a generative model as an IR engine to evaluate the prompts' performance on image retrieval tasks. We demonstrate the plausibility of our task by measuring the correlation coefficient between predicted and actual performance scores across three datasets containing pairs of prompts and generated images. Our results show promising performance prediction capabilities, suggesting potential applications for optimizing generative IR systems.","sentences":["The ability to predict the performance of a query in Information Retrieval (IR) systems has been a longstanding challenge.","In this paper, we introduce a novel task called \"Prompt Performance Prediction\" that aims to predict the performance of a query, referred to as a prompt, before obtaining the actual search results.","The context of our task leverages a generative model as an IR engine to evaluate the prompts' performance on image retrieval tasks.","We demonstrate the plausibility of our task by measuring the correlation coefficient between predicted and actual performance scores across three datasets containing pairs of prompts and generated images.","Our results show promising performance prediction capabilities, suggesting potential applications for optimizing generative IR systems."],"url":"http://arxiv.org/abs/2306.08915v1"}
{"created":"2023-06-15","title":"Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?","abstract":"The rise of Generative Artificial Intelligence systems (``AI systems'') has created unprecedented social engagement. AI code generation systems provide responses (output) to questions or requests by accessing the vast library of open-source code created by developers over decades. However, they do so by allegedly stealing the open-source code stored in virtual libraries, known as repositories. How all this happens and whether there is a solution short of years of litigation that can protect innovation is the focus of this article. We also peripherally touch upon the array of issues raised by the relationship between AI and copyright. Looking ahead, we propose the following: (a) immediate changes to the licenses for open-source code created by developers that will allow access and/or use of any open-source code to humans only; (b) we suggest revisions to the Massachusetts Institute of Technology (``MIT'') license so that AI systems procure appropriate licenses from open-source code developers, which we believe will harmonize standards and build social consensus for the benefit of all of humanity rather than profit-driven centers of innovation; (c) We call for urgent legislative action to protect the future of AI systems while also promoting innovation; and (d) we propose that there is a shift in the burden of proof to AI systems in obfuscation cases.","sentences":["The rise of Generative Artificial Intelligence systems (``AI systems'') has created unprecedented social engagement.","AI code generation systems provide responses (output) to questions or requests by accessing the vast library of open-source code created by developers over decades.","However, they do so by allegedly stealing the open-source code stored in virtual libraries, known as repositories.","How all this happens and whether there is a solution short of years of litigation that can protect innovation is the focus of this article.","We also peripherally touch upon the array of issues raised by the relationship between AI and copyright.","Looking ahead, we propose the following: (a) immediate changes to the licenses for open-source code created by developers that will allow access and/or use of any open-source code to humans only; (b) we suggest revisions to the Massachusetts Institute of Technology (``MIT'') license so that AI systems procure appropriate licenses from open-source code developers, which we believe will harmonize standards and build social consensus for the benefit of all of humanity rather than profit-driven centers of innovation; (c) We call for urgent legislative action to protect the future of AI systems while also promoting innovation; and (d) we propose that there is a shift in the burden of proof to AI systems in obfuscation cases."],"url":"http://arxiv.org/abs/2306.09267v1"}
{"created":"2023-06-15","title":"Can ChatGPT pass the Vietnamese National High School Graduation Examination?","abstract":"This research article highlights the potential of AI-powered chatbots in education and presents the results of using ChatGPT, a large language model, to complete the Vietnamese National High School Graduation Examination (VNHSGE). The study dataset included 30 essays in the literature test case and 1,700 multiple-choice questions designed for other subjects. The results showed that ChatGPT was able to pass the examination with an average score of 6-7, demonstrating the technology's potential to revolutionize the educational landscape. The analysis of ChatGPT performance revealed its proficiency in a range of subjects, including mathematics, English, physics, chemistry, biology, history, geography, civic education, and literature, which suggests its potential to provide effective support for learners. However, further research is needed to assess ChatGPT performance on more complex exam questions and its potential to support learners in different contexts. As technology continues to evolve and improve, we can expect to see the use of AI tools like ChatGPT become increasingly common in educational settings, ultimately enhancing the educational experience for both students and educators.","sentences":["This research article highlights the potential of AI-powered chatbots in education and presents the results of using ChatGPT, a large language model, to complete the Vietnamese National High School Graduation Examination (VNHSGE).","The study dataset included 30 essays in the literature test case and 1,700 multiple-choice questions designed for other subjects.","The results showed that ChatGPT was able to pass the examination with an average score of 6-7, demonstrating the technology's potential to revolutionize the educational landscape.","The analysis of ChatGPT performance revealed its proficiency in a range of subjects, including mathematics, English, physics, chemistry, biology, history, geography, civic education, and literature, which suggests its potential to provide effective support for learners.","However, further research is needed to assess ChatGPT performance on more complex exam questions and its potential to support learners in different contexts.","As technology continues to evolve and improve, we can expect to see the use of AI tools like ChatGPT become increasingly common in educational settings, ultimately enhancing the educational experience for both students and educators."],"url":"http://arxiv.org/abs/2306.09170v1"}
{"created":"2023-06-15","title":"Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation","abstract":"Zero-shot NL2SQL is crucial in achieving natural language to SQL that is adaptive to new environments (e.g., new databases, new linguistic phenomena or SQL structures) with zero annotated NL2SQL samples from such environments. Existing approaches either fine-tune pre-trained language models (PLMs) based on annotated data or use prompts to guide fixed large language models (LLMs) such as ChatGPT. PLMs can perform well in schema alignment but struggle to achieve complex reasoning, while LLMs is superior in complex reasoning tasks but cannot achieve precise schema alignment. In this paper, we propose a ZeroNL2SQL framework that combines the complementary advantages of PLMs and LLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an SQL sketch via schema alignment, then uses LLMs to fill the missing information via complex reasoning. Moreover, in order to better align the generated SQL queries with values in the given database instances, we design a predicate calibration method to guide the LLM in completing the SQL sketches based on the database instances and select the optimal SQL query via an execution-based strategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best zero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL outperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds LLM-based methods by 10% to 20% on execution accuracy.","sentences":["Zero-shot NL2SQL is crucial in achieving natural language to SQL that is adaptive to new environments (e.g., new databases, new linguistic phenomena or SQL structures) with zero annotated NL2SQL samples from such environments.","Existing approaches either fine-tune pre-trained language models (PLMs) based on annotated data or use prompts to guide fixed large language models (LLMs) such as ChatGPT.","PLMs can perform well in schema alignment but struggle to achieve complex reasoning, while LLMs is superior in complex reasoning tasks but cannot achieve precise schema alignment.","In this paper, we propose a ZeroNL2SQL framework that combines the complementary advantages of PLMs and LLMs for supporting zero-shot NL2SQL.","ZeroNL2SQL","first uses PLMs to generate an SQL sketch via schema alignment, then uses LLMs to fill the missing information via complex reasoning.","Moreover, in order to better align the generated SQL queries with values in the given database instances, we design a predicate calibration method to guide the LLM in completing the SQL sketches based on the database instances and select the optimal SQL query via an execution-based strategy.","Comprehensive experiments show that ZeroNL2SQL can achieve the best zero-shot NL2SQL performance on real-world benchmarks.","Specifically, ZeroNL2SQL outperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds LLM-based methods by 10% to 20% on execution accuracy."],"url":"http://arxiv.org/abs/2306.08891v1"}
{"created":"2023-06-15","title":"Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated Misinformation in the Medical Domain","abstract":"The pervasive influence of misinformation has far-reaching and detrimental effects on both individuals and society. The COVID-19 pandemic has witnessed an alarming surge in the dissemination of medical misinformation. However, existing datasets pertaining to misinformation predominantly focus on textual information, neglecting the inclusion of visual elements, and tend to center solely on COVID-19-related misinformation, overlooking misinformation surrounding other diseases. Furthermore, the potential of Large Language Models (LLMs), such as the ChatGPT developed in late 2022, in generating misinformation has been overlooked in previous works. To overcome these limitations, we present Med-MMHL, a novel multi-modal misinformation detection dataset in a general medical domain encompassing multiple diseases. Med-MMHL not only incorporates human-generated misinformation but also includes misinformation generated by LLMs like ChatGPT. Our dataset aims to facilitate comprehensive research and development of methodologies for detecting misinformation across diverse diseases and various scenarios, including human and LLM-generated misinformation detection at the sentence, document, and multi-modal levels. To access our dataset and code, visit our GitHub repository: \\url{https://github.com/styxsys0927/Med-MMHL}.","sentences":["The pervasive influence of misinformation has far-reaching and detrimental effects on both individuals and society.","The COVID-19 pandemic has witnessed an alarming surge in the dissemination of medical misinformation.","However, existing datasets pertaining to misinformation predominantly focus on textual information, neglecting the inclusion of visual elements, and tend to center solely on COVID-19-related misinformation, overlooking misinformation surrounding other diseases.","Furthermore, the potential of Large Language Models (LLMs), such as the ChatGPT developed in late 2022, in generating misinformation has been overlooked in previous works.","To overcome these limitations, we present Med-MMHL, a novel multi-modal misinformation detection dataset in a general medical domain encompassing multiple diseases.","Med-MMHL not only incorporates human-generated misinformation but also includes misinformation generated by LLMs like ChatGPT.","Our dataset aims to facilitate comprehensive research and development of methodologies for detecting misinformation across diverse diseases and various scenarios, including human and LLM-generated misinformation detection at the sentence, document, and multi-modal levels.","To access our dataset and code, visit our GitHub repository: \\url{https://github.com/styxsys0927/Med-MMHL}."],"url":"http://arxiv.org/abs/2306.08871v1"}
{"created":"2023-06-15","title":"Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection","abstract":"ChatGPT and other large language models (LLMs) have proven useful in crowdsourcing tasks, where they can effectively annotate machine learning training data. However, this means that they also have the potential for misuse, specifically to automatically answer surveys. LLMs can potentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys. In this paper, we propose a mechanism to detect LLM-generated responses to surveys. The mechanism uses \"prompt injection\", such as directions that can mislead LLMs into giving predictable responses. We evaluate our technique against a range of question scenarios, types, and positions, and find that it can reliably detect LLM-generated responses with more than 93% effectiveness. We also provide an open-source software to help survey designers use our technique to detect LLM responses. Our work is a step in ensuring that survey methodologies remain rigorous vis-a-vis LLMs.","sentences":["ChatGPT and other large language models (LLMs) have proven useful in crowdsourcing tasks, where they can effectively annotate machine learning training data.","However, this means that they also have the potential for misuse, specifically to automatically answer surveys.","LLMs can potentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys.","In this paper, we propose a mechanism to detect LLM-generated responses to surveys.","The mechanism uses \"prompt injection\", such as directions that can mislead LLMs into giving predictable responses.","We evaluate our technique against a range of question scenarios, types, and positions, and find that it can reliably detect LLM-generated responses with more than 93% effectiveness.","We also provide an open-source software to help survey designers use our technique to detect LLM responses.","Our work is a step in ensuring that survey methodologies remain rigorous vis-a-vis LLMs."],"url":"http://arxiv.org/abs/2306.08833v1"}
{"created":"2023-06-15","title":"Segment Any Point Cloud Sequences by Distilling Vision Foundation Models","abstract":"Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, eliminating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment stages, facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notably, Seal achieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing random initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU. Moreover, Seal demonstrates significant performance gains over existing methods across 20 different few-shot fine-tuning tasks on all eleven tested point cloud datasets.","sentences":["Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception.","In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences.","Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, eliminating the need for annotations in either 2D or 3D during pretraining.","ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment stages, facilitating cross-modal representation learning.","iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets.","Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal.","Notably, Seal achieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing random initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU. Moreover, Seal demonstrates significant performance gains over existing methods across 20 different few-shot fine-tuning tasks on all eleven tested point cloud datasets."],"url":"http://arxiv.org/abs/2306.09347v1"}
{"created":"2023-06-15","title":"Evaluating Data Attribution for Text-to-Image Models","abstract":"While large text-to-image models are able to synthesize \"novel\" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through \"customization\" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.","sentences":["While large text-to-image models are able to synthesize \"novel\" images, these images are necessarily a reflection of the training data.","The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one.","As an initial step toward this problem, we evaluate attribution through \"customization\" methods, which tune an existing large-scale model toward a given exemplar object or style.","Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction.","With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces.","Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem.","Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets.","Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images."],"url":"http://arxiv.org/abs/2306.09345v1"}
{"created":"2023-06-15","title":"DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data","abstract":"Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.","sentences":["Current perceptual similarity metrics operate at the level of pixels and patches.","These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content.","In this paper, we develop a perceptual metric that assesses images holistically.","Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways.","Critical to this dataset is that judgments are nearly automatic and shared by all observers.","To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions.","We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception.","We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout.","Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks.","Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks."],"url":"http://arxiv.org/abs/2306.09344v1"}
{"created":"2023-06-15","title":"SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts","abstract":"Lectures are a learning experience for both students and teachers. Students learn from teachers about the subject material, while teachers learn from students about how to refine their instruction. However, online student feedback is unstructured and abundant, making it challenging for teachers to learn and improve. We take a step towards tackling this challenge. First, we contribute a dataset for studying this problem: SIGHT is a large dataset of 288 math lecture transcripts and 15,784 comments collected from the Massachusetts Institute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we develop a rubric for categorizing feedback types using qualitative analysis. Qualitative analysis methods are powerful in uncovering domain-specific insights, however they are costly to apply to large data sources. To overcome this challenge, we propose a set of best practices for using large language models (LLMs) to cheaply classify the comments at scale. We observe a striking correlation between the model's and humans' annotation: Categories with consistent human annotations (>$0.9$ inter-rater reliability, IRR) also display higher human-model agreement (>$0.7$), while categories with less consistent human annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower human-model agreement ($0.3$-$0.5$). These techniques uncover useful student feedback from thousands of comments, costing around $\\$0.002$ per comment. We conclude by discussing exciting future directions on using online student feedback and improving automated annotation techniques for qualitative research.","sentences":["Lectures are a learning experience for both students and teachers.","Students learn from teachers about the subject material, while teachers learn from students about how to refine their instruction.","However, online student feedback is unstructured and abundant, making it challenging for teachers to learn and improve.","We take a step towards tackling this challenge.","First, we contribute a dataset for studying this problem: SIGHT is a large dataset of 288 math lecture transcripts and 15,784 comments collected from the Massachusetts Institute of Technology OpenCourseWare (MIT OCW) YouTube channel.","Second, we develop a rubric for categorizing feedback types using qualitative analysis.","Qualitative analysis methods are powerful in uncovering domain-specific insights, however they are costly to apply to large data sources.","To overcome this challenge, we propose a set of best practices for using large language models (LLMs) to cheaply classify the comments at scale.","We observe a striking correlation between the model's and humans' annotation: Categories with consistent human annotations (>$0.9$ inter-rater reliability, IRR) also display higher human-model agreement (>$0.7$), while categories with less consistent human annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower human-model agreement ($0.3$-$0.5$).","These techniques uncover useful student feedback from thousands of comments, costing around $\\$0.002$ per comment.","We conclude by discussing exciting future directions on using online student feedback and improving automated annotation techniques for qualitative research."],"url":"http://arxiv.org/abs/2306.09343v1"}
{"created":"2023-06-15","title":"PaReprop: Fast Parallelized Reversible Backpropagation","abstract":"The growing size of datasets and deep learning models has made faster and memory-efficient training crucial. Reversible transformers have recently been introduced as an exciting new method for extremely memory-efficient training, but they come with an additional computation overhead of activation re-computation in the backpropagation phase. We present PaReprop, a fast Parallelized Reversible Backpropagation algorithm that parallelizes the additional activation re-computation overhead in reversible training with the gradient computation itself in backpropagation phase. We demonstrate the effectiveness of the proposed PaReprop algorithm through extensive benchmarking across model families (ViT, MViT, Swin and RoBERTa), data modalities (Vision & NLP), model sizes (from small to giant), and training batch sizes. Our empirical results show that PaReprop achieves up to 20% higher training throughput than vanilla reversible training, largely mitigating the theoretical overhead of 25% lower throughput from activation recomputation in reversible training. Project page: https://tylerzhu.com/pareprop.","sentences":["The growing size of datasets and deep learning models has made faster and memory-efficient training crucial.","Reversible transformers have recently been introduced as an exciting new method for extremely memory-efficient training, but they come with an additional computation overhead of activation re-computation in the backpropagation phase.","We present PaReprop, a fast Parallelized Reversible Backpropagation algorithm that parallelizes the additional activation re-computation overhead in reversible training with the gradient computation itself in backpropagation phase.","We demonstrate the effectiveness of the proposed PaReprop algorithm through extensive benchmarking across model families (ViT, MViT, Swin and RoBERTa), data modalities (Vision & NLP), model sizes (from small to giant), and training batch sizes.","Our empirical results show that PaReprop achieves up to 20% higher training throughput than vanilla reversible training, largely mitigating the theoretical overhead of 25% lower throughput from activation recomputation in reversible training.","Project page: https://tylerzhu.com/pareprop."],"url":"http://arxiv.org/abs/2306.09342v1"}
{"created":"2023-06-15","title":"Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking","abstract":"In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30$\\times$ larger D3ST-XXL model by 5.0 points.","sentences":["In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services.","Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies.","We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space.","At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention.","We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets.","Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset.","Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30$\\times$ larger D3ST-XXL model by 5.0 points."],"url":"http://arxiv.org/abs/2306.09340v1"}
{"created":"2023-06-15","title":"Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis","abstract":"Recent text-to-image generative models can generate high-fidelity images from text inputs, but the quality of these generated images cannot be accurately evaluated by existing evaluation metrics. To address this issue, we introduce Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human preferences on images from a wide range of sources. HPD v2 comprises 798,090 human preference choices on 430,060 pairs of images, making it the largest dataset of its kind. The text prompts and images are deliberately collected to eliminate potential bias, which is a common issue in previous datasets. By fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a scoring model that can more accurately predict text-generated images' human preferences. Our experiments demonstrate that HPS v2 generalizes better than previous metrics across various image distributions and is responsive to algorithmic improvements of text-to-image generative models, making it a preferable evaluation metric for these models. We also investigate the design of the evaluation prompts for text-to-image generative models, to make the evaluation stable, fair and easy-to-use. Finally, we establish a benchmark for text-to-image generative models using HPS v2, which includes a set of recent text-to-image models from the academia, community and industry. The code and dataset is / will be available at https://github.com/tgxs002/HPSv2.","sentences":["Recent text-to-image generative models can generate high-fidelity images from text inputs, but the quality of these generated images cannot be accurately evaluated by existing evaluation metrics.","To address this issue, we introduce Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human preferences on images from a wide range of sources.","HPD v2 comprises 798,090 human preference choices on 430,060 pairs of images, making it the largest dataset of its kind.","The text prompts and images are deliberately collected to eliminate potential bias, which is a common issue in previous datasets.","By fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a scoring model that can more accurately predict text-generated images' human preferences.","Our experiments demonstrate that HPS v2 generalizes better than previous metrics across various image distributions and is responsive to algorithmic improvements of text-to-image generative models, making it a preferable evaluation metric for these models.","We also investigate the design of the evaluation prompts for text-to-image generative models, to make the evaluation stable, fair and easy-to-use.","Finally, we establish a benchmark for text-to-image generative models using HPS v2, which includes a set of recent text-to-image models from the academia, community and industry.","The code and dataset is / will be available at https://github.com/tgxs002/HPSv2."],"url":"http://arxiv.org/abs/2306.09341v1"}
{"created":"2023-06-15","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management","abstract":"Large language models have recently advanced the state of the art on many natural language processing benchmarks. The newest generation of models can be applied to a variety of tasks with little to no specialized training. This technology creates various opportunities for applications in the context of data management.   The tutorial will introduce participants to basic background on language models, discuss different methods to use language models, and give an overview and short demonstration of available libraries and APIs. Models for generating natural language will be considered as well as models, such as GPT-3 Codex, which complete program code or generate code from natural language instructions. Finally, the tutorial will discuss recent research in the database community that exploits language models in the context of traditional database systems or proposes novel system architectures that are based on them.   The tutorial is targeted at database researchers. No prior background on language models is required. The goal of the tutorial is to introduce database researchers to the latest generation of language models, and to their use cases in the domain of data management.","sentences":["Large language models have recently advanced the state of the art on many natural language processing benchmarks.","The newest generation of models can be applied to a variety of tasks with little to no specialized training.","This technology creates various opportunities for applications in the context of data management.   ","The tutorial will introduce participants to basic background on language models, discuss different methods to use language models, and give an overview and short demonstration of available libraries and APIs.","Models for generating natural language will be considered as well as models, such as GPT-3 Codex, which complete program code or generate code from natural language instructions.","Finally, the tutorial will discuss recent research in the database community that exploits language models in the context of traditional database systems or proposes novel system architectures that are based on them.   ","The tutorial is targeted at database researchers.","No prior background on language models is required.","The goal of the tutorial is to introduce database researchers to the latest generation of language models, and to their use cases in the domain of data management."],"url":"http://arxiv.org/abs/2306.09339v1"}
{"created":"2023-06-15","title":"Generative Proxemics: A Prior for 3D Social Interaction from Images","abstract":"Social interaction is a fundamental aspect of human behavior and communication. The way individuals position themselves in relation to others, also known as proxemics, conveys social cues and affects the dynamics of social interaction. We present a novel approach that learns a 3D proxemics prior of two people in close social interaction. Since collecting a large 3D dataset of interacting people is a challenge, we rely on 2D image collections where social interactions are abundant. We achieve this by reconstructing pseudo-ground truth 3D meshes of interacting people from images with an optimization approach using existing ground-truth contact maps. We then model the proxemics using a novel denoising diffusion model called BUDDI that learns the joint distribution of two people in close social interaction directly in the SMPL-X parameter space. Sampling from our generative proxemics model produces realistic 3D human interactions, which we validate through a user study. Additionally, we introduce a new optimization method that uses the diffusion prior to reconstruct two people in close proximity from a single image without any contact annotation. Our approach recovers more accurate and plausible 3D social interactions from noisy initial estimates and outperforms state-of-the-art methods. See our project site for code, data, and model: muelea.github.io/buddi.","sentences":["Social interaction is a fundamental aspect of human behavior and communication.","The way individuals position themselves in relation to others, also known as proxemics, conveys social cues and affects the dynamics of social interaction.","We present a novel approach that learns a 3D proxemics prior of two people in close social interaction.","Since collecting a large 3D dataset of interacting people is a challenge, we rely on 2D image collections where social interactions are abundant.","We achieve this by reconstructing pseudo-ground truth 3D meshes of interacting people from images with an optimization approach using existing ground-truth contact maps.","We then model the proxemics using a novel denoising diffusion model called BUDDI that learns the joint distribution of two people in close social interaction directly in the SMPL-X parameter space.","Sampling from our generative proxemics model produces realistic 3D human interactions, which we validate through a user study.","Additionally, we introduce a new optimization method that uses the diffusion prior to reconstruct two people in close proximity from a single image without any contact annotation.","Our approach recovers more accurate and plausible 3D social interactions from noisy initial estimates and outperforms state-of-the-art methods.","See our project site for code, data, and model: muelea.github.io/buddi."],"url":"http://arxiv.org/abs/2306.09337v1"}
{"created":"2023-06-15","title":"Seeing the Pose in the Pixels: Learning Pose-Aware Representations in Vision Transformers","abstract":"Human perception of surroundings is often guided by the various poses present within the environment. Many computer vision tasks, such as human action recognition and robot imitation learning, rely on pose-based entities like human skeletons or robotic arms. However, conventional Vision Transformer (ViT) models uniformly process all patches, neglecting valuable pose priors in input videos. We argue that incorporating poses into RGB data is advantageous for learning fine-grained and viewpoint-agnostic representations. Consequently, we introduce two strategies for learning pose-aware representations in ViTs. The first method, called Pose-aware Attention Block (PAAB), is a plug-and-play ViT block that performs localized attention on pose regions within videos. The second method, dubbed Pose-Aware Auxiliary Task (PAAT), presents an auxiliary pose prediction task optimized jointly with the primary ViT task. Although their functionalities differ, both methods succeed in learning pose-aware representations, enhancing performance in multiple diverse downstream tasks. Our experiments, conducted across seven datasets, reveal the efficacy of both pose-aware methods on three video analysis tasks, with PAAT holding a slight edge over PAAB. Both PAAT and PAAB surpass their respective backbone Transformers by up to 9.8% in real-world action recognition and 21.8% in multi-view robotic video alignment. Code is available at https://github.com/dominickrei/PoseAwareVT.","sentences":["Human perception of surroundings is often guided by the various poses present within the environment.","Many computer vision tasks, such as human action recognition and robot imitation learning, rely on pose-based entities like human skeletons or robotic arms.","However, conventional Vision Transformer (ViT) models uniformly process all patches, neglecting valuable pose priors in input videos.","We argue that incorporating poses into RGB data is advantageous for learning fine-grained and viewpoint-agnostic representations.","Consequently, we introduce two strategies for learning pose-aware representations in ViTs.","The first method, called Pose-aware Attention Block (PAAB), is a plug-and-play ViT block that performs localized attention on pose regions within videos.","The second method, dubbed Pose-Aware Auxiliary Task (PAAT), presents an auxiliary pose prediction task optimized jointly with the primary ViT task.","Although their functionalities differ, both methods succeed in learning pose-aware representations, enhancing performance in multiple diverse downstream tasks.","Our experiments, conducted across seven datasets, reveal the efficacy of both pose-aware methods on three video analysis tasks, with PAAT holding a slight edge over PAAB.","Both PAAT and PAAB surpass their respective backbone Transformers by up to 9.8% in real-world action recognition and 21.8% in multi-view robotic video alignment.","Code is available at https://github.com/dominickrei/PoseAwareVT."],"url":"http://arxiv.org/abs/2306.09331v1"}
{"created":"2023-06-15","title":"Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction","abstract":"Speaker diarization (SD) is typically used with an automatic speech recognition (ASR) system to ascribe speaker labels to recognized words. The conventional approach reconciles outputs from independently optimized ASR and SD systems, where the SD system typically uses only acoustic information to identify the speakers in the audio stream. This approach can lead to speaker errors especially around speaker turns and regions of speaker overlap. In this paper, we propose a novel second-pass speaker error correction system using lexical information, leveraging the power of modern language models (LMs). Our experiments across multiple telephony datasets show that our approach is both effective and robust. Training and tuning only on the Fisher dataset, this error correction approach leads to relative word-level diarization error rate (WDER) reductions of 15-30% on three telephony datasets: RT03-CTS, Callhome American English and held-out portions of Fisher.","sentences":["Speaker diarization (SD) is typically used with an automatic speech recognition (ASR) system to ascribe speaker labels to recognized words.","The conventional approach reconciles outputs from independently optimized ASR and SD systems, where the SD system typically uses only acoustic information to identify the speakers in the audio stream.","This approach can lead to speaker errors especially around speaker turns and regions of speaker overlap.","In this paper, we propose a novel second-pass speaker error correction system using lexical information, leveraging the power of modern language models (LMs).","Our experiments across multiple telephony datasets show that our approach is both effective and robust.","Training and tuning only on the Fisher dataset, this error correction approach leads to relative word-level diarization error rate (WDER) reductions of 15-30% on three telephony datasets: RT03-CTS, Callhome American English and held-out portions of Fisher."],"url":"http://arxiv.org/abs/2306.09313v1"}
{"created":"2023-06-15","title":"STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events","abstract":"While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker. This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events. Audio-visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence. We also introduce an audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events. Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events. STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system. Our benchmark results show that the audio-visual SELD system achieves lower localization error than the audio-only system. The data is available at https://zenodo.org/record/7880637.","sentences":["While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker.","This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events.","Audio-visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence.","We also introduce an audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events.","Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events.","STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system.","Our benchmark results show that the audio-visual SELD system achieves lower localization error than the audio-only system.","The data is available at https://zenodo.org/record/7880637."],"url":"http://arxiv.org/abs/2306.09126v1"}
{"created":"2023-06-15","title":"Tropical Logistic Regression Model on Space of Phylogenetic Trees","abstract":"Motivation:Classification of gene trees is an important task both in analysis of multi-locus phylogenetic data, and assessment of the convergence of Markov Chain Monte Carlo (MCMC) analyses used in Bayesian phylogenetic tree reconstruction. The logistic regression model is one of the most popular classification models in statistical learning, thanks to its computational speed and interpretability. However, it is not appropriate to directly apply the logistic regression model to a set of phylogenetic trees with the same set of leaf labels, as the space of phylogenetic trees is not Euclidean. Results: It is well-known in tropical geometry and phylogenetics that the space of phylogenetic trees is a tropical linear space in terms of the max-plus algebra. Therefore, in this paper, we propose an analogue approach of the logistic regression model in the setting of tropical geometry. In our proposed method, we consider two cases: where the numbers of the species trees are fixed as one and two, and we estimate the species tree(s) from a sample of gene trees distributed over the space of ultrametrics, which is a tropical linear space. we show that both models are statistically consistent and bounds on the generalization error of both models are derived. Finally, we conduct computational experiments on simulated data generated by the multi-species coalescent model and apply our model to African coelacanth genomes to infer the species tree.","sentences":["Motivation:Classification of gene trees is an important task both in analysis of multi-locus phylogenetic data, and assessment of the convergence of Markov Chain Monte Carlo (MCMC) analyses used in Bayesian phylogenetic tree reconstruction.","The logistic regression model is one of the most popular classification models in statistical learning, thanks to its computational speed and interpretability.","However, it is not appropriate to directly apply the logistic regression model to a set of phylogenetic trees with the same set of leaf labels, as the space of phylogenetic trees is not Euclidean.","Results: It is well-known in tropical geometry and phylogenetics that the space of phylogenetic trees is a tropical linear space in terms of the max-plus algebra.","Therefore, in this paper, we propose an analogue approach of the logistic regression model in the setting of tropical geometry.","In our proposed method, we consider two cases: where the numbers of the species trees are fixed as one and two, and we estimate the species tree(s) from a sample of gene trees distributed over the space of ultrametrics, which is a tropical linear space.","we show that both models are statistically consistent and bounds on the generalization error of both models are derived.","Finally, we conduct computational experiments on simulated data generated by the multi-species coalescent model and apply our model to African coelacanth genomes to infer the species tree."],"url":"http://arxiv.org/abs/2306.08796v1"}
{"created":"2023-06-15","title":"Quality and Efficiency of Manual Annotation: Pre-annotation Bias","abstract":"This paper presents an analysis of annotation using an automatic pre-annotation for a mid-level annotation complexity task -- dependency syntax annotation. It compares the annotation efforts made by annotators using a pre-annotated version (with a high-accuracy parser) and those made by fully manual annotation. The aim of the experiment is to judge the final annotation quality when pre-annotation is used. In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checks and another annotation on the same data available to the annotators, and their influence on annotation quality and efficiency. The experiment confirmed that the pre-annotation is an efficient tool for faster manual syntactic annotation which increases the consistency of the resulting annotation without reducing its quality.","sentences":["This paper presents an analysis of annotation using an automatic pre-annotation for a mid-level annotation complexity task -- dependency syntax annotation.","It compares the annotation efforts made by annotators using a pre-annotated version (with a high-accuracy parser) and those made by fully manual annotation.","The aim of the experiment is to judge the final annotation quality when pre-annotation is used.","In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checks and another annotation on the same data available to the annotators, and their influence on annotation quality and efficiency.","The experiment confirmed that the pre-annotation is an efficient tool for faster manual syntactic annotation which increases the consistency of the resulting annotation without reducing its quality."],"url":"http://arxiv.org/abs/2306.09307v1"}
{"created":"2023-06-15","title":"Datasets and Benchmarks for Offline Safe Reinforcement Learning","abstract":"This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases. Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations. We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving. We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions. Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area. Through extensive experiments with over 50000 CPU and 800 GPU hours of computations, we evaluate and compare the performance of these baseline algorithms on the collected datasets, offering insights into their strengths, limitations, and potential areas of improvement. Our benchmarking framework serves as a valuable resource for researchers and practitioners, facilitating the development of more robust and reliable offline safe RL solutions in safety-critical applications. The benchmark website is available at \\url{www.offline-saferl.org}.","sentences":["This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases.","Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations.","We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving.","We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions.","Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area.","Through extensive experiments with over 50000 CPU and 800 GPU hours of computations, we evaluate and compare the performance of these baseline algorithms on the collected datasets, offering insights into their strengths, limitations, and potential areas of improvement.","Our benchmarking framework serves as a valuable resource for researchers and practitioners, facilitating the development of more robust and reliable offline safe RL solutions in safety-critical applications.","The benchmark website is available at \\url{www.offline-saferl.org}."],"url":"http://arxiv.org/abs/2306.09303v1"}
{"created":"2023-06-15","title":"A9 Intersection Dataset: All You Need for Urban 3D Camera-LiDAR Roadside Perception","abstract":"Intelligent Transportation Systems (ITS) allow a drastic expansion of the visibility range and decrease occlusions for autonomous driving. To obtain accurate detections, detailed labeled sensor data for training is required. Unfortunately, high-quality 3D labels of LiDAR point clouds from the infrastructure perspective of an intersection are still rare. Therefore, we provide the A9 Intersection Dataset, which consists of labeled LiDAR point clouds and synchronized camera images. Here, we recorded the sensor output from two roadside cameras and LiDARs mounted on intersection gantry bridges. The point clouds were labeled in 3D by experienced annotators. Furthermore, we provide calibration data between all sensors, which allow the projection of the 3D labels into the camera images and an accurate data fusion. Our dataset consists of 4.8k images and point clouds with more than 57.4k manually labeled 3D boxes. With ten object classes, it has a high diversity of road users in complex driving maneuvers, such as left and right turns, overtaking, and U-turns. In experiments, we provided multiple baselines for the perception tasks. Overall, our dataset is a valuable contribution to the scientific community to perform complex 3D camera-LiDAR roadside perception tasks. Find data, code, and more information at https://a9-dataset.com.","sentences":["Intelligent Transportation Systems (ITS) allow a drastic expansion of the visibility range and decrease occlusions for autonomous driving.","To obtain accurate detections, detailed labeled sensor data for training is required.","Unfortunately, high-quality 3D labels of LiDAR point clouds from the infrastructure perspective of an intersection are still rare.","Therefore, we provide the A9 Intersection Dataset, which consists of labeled LiDAR point clouds and synchronized camera images.","Here, we recorded the sensor output from two roadside cameras and LiDARs mounted on intersection gantry bridges.","The point clouds were labeled in 3D by experienced annotators.","Furthermore, we provide calibration data between all sensors, which allow the projection of the 3D labels into the camera images and an accurate data fusion.","Our dataset consists of 4.8k images and point clouds with more than 57.4k manually labeled 3D boxes.","With ten object classes, it has a high diversity of road users in complex driving maneuvers, such as left and right turns, overtaking, and U-turns.","In experiments, we provided multiple baselines for the perception tasks.","Overall, our dataset is a valuable contribution to the scientific community to perform complex 3D camera-LiDAR roadside perception tasks.","Find data, code, and more information at https://a9-dataset.com."],"url":"http://arxiv.org/abs/2306.09266v1"}
{"created":"2023-06-15","title":"A Novel Approach to Encode Two-Way Epistatic Interactions Between Single Nucleotide Polymorphisms","abstract":"Modelling gene-gene epistatic interactions when computing genetic risk scores is not a well-explored subfield of genetics and could have potential to improve risk stratification in practice. Though applications of machine learning (ML) show promise as an avenue of improvement for current genetic risk assesments, they frequently suffer from the problem of two many features and to little data. We propose a method that when combined with ML allows information from individual genetic contributors to be preserved while incorporating information on their interactions in a single feature. This allows second-order analysis, while simultaneously increasing the number of input features to ML models as little as possible. We presented three methods that can be utilized to account for genetic interactions. We found that interaction methods that preserved information from the constituent SNPs performed significantly better than the simplest interaction method. Since the currently available ML methods are able to account for complex interactions, utilizing raw SNP genotypes alone is sufficient because the simplest model outperforms all the interaction methods Given that understanding and accounting for epistatic interactions is one of the most promising avenues for increasing explained variability in heritable disease, this work represents a first step toward an algorithmic interaction method that preserves the information in each component. This is relevant not only because of potential improvements in model quality, but also because explicit interaction terms allow a human readable interpretation of potential interaction pathways within the disease.","sentences":["Modelling gene-gene epistatic interactions when computing genetic risk scores is not a well-explored subfield of genetics and could have potential to improve risk stratification in practice.","Though applications of machine learning (ML) show promise as an avenue of improvement for current genetic risk assesments, they frequently suffer from the problem of two many features and to little data.","We propose a method that when combined with ML allows information from individual genetic contributors to be preserved while incorporating information on their interactions in a single feature.","This allows second-order analysis, while simultaneously increasing the number of input features to ML models as little as possible.","We presented three methods that can be utilized to account for genetic interactions.","We found that interaction methods that preserved information from the constituent SNPs performed significantly better than the simplest interaction method.","Since the currently available ML methods are able to account for complex interactions, utilizing raw SNP genotypes alone is sufficient because the simplest model outperforms all the interaction methods Given that understanding and accounting for epistatic interactions is one of the most promising avenues for increasing explained variability in heritable disease, this work represents a first step toward an algorithmic interaction method that preserves the information in each component.","This is relevant not only because of potential improvements in model quality, but also because explicit interaction terms allow a human readable interpretation of potential interaction pathways within the disease."],"url":"http://arxiv.org/abs/2306.09175v1"}
{"created":"2023-06-15","title":"Multi-Objective Optimization of Electrical Machines using a Hybrid Data-and Physics-Driven Approach","abstract":"Magneto-static finite element (FE) simulations make numerical optimization of electrical machines very time-consuming and computationally intensive during the design stage. In this paper, we present the application of a hybrid data-and physics-driven model for numerical optimization of permanent magnet synchronous machines (PMSM). Following the data-driven supervised training, deep neural network (DNN) will act as a meta-model to characterize the electromagnetic behavior of PMSM by predicting intermediate FE measures. These intermediate measures are then post-processed with various physical models to compute the required key performance indicators (KPIs), e.g., torque, shaft power, and material costs. We perform multi-objective optimization with both classical FE and a hybrid approach using a nature-inspired evolutionary algorithm. We show quantitatively that the hybrid approach maintains the quality of Pareto results better or close to conventional FE simulation-based optimization while being computationally very cheap.","sentences":["Magneto-static finite element (FE) simulations make numerical optimization of electrical machines very time-consuming and computationally intensive during the design stage.","In this paper, we present the application of a hybrid data-and physics-driven model for numerical optimization of permanent magnet synchronous machines (PMSM).","Following the data-driven supervised training, deep neural network (DNN) will act as a meta-model to characterize the electromagnetic behavior of PMSM by predicting intermediate FE measures.","These intermediate measures are then post-processed with various physical models to compute the required key performance indicators (KPIs), e.g., torque, shaft power, and material costs.","We perform multi-objective optimization with both classical FE and a hybrid approach using a nature-inspired evolutionary algorithm.","We show quantitatively that the hybrid approach maintains the quality of Pareto results better or close to conventional FE simulation-based optimization while being computationally very cheap."],"url":"http://arxiv.org/abs/2306.09096v1"}
{"created":"2023-06-15","title":"Learning by Analogy: Diverse Questions Generation in Math Word Problem","abstract":"Solving math word problem (MWP) with AI techniques has recently made great progress with the success of deep neural networks (DNN), but it is far from being solved. We argue that the ability of learning by analogy is essential for an MWP solver to better understand same problems which may typically be formulated in diverse ways. However most existing works exploit the shortcut learning to train MWP solvers simply based on samples with a single question. In lack of diverse questions, these methods merely learn shallow heuristics. In this paper, we make a first attempt to solve MWPs by generating diverse yet consistent questions/equations. Given a typical MWP including the scenario description, question, and equation (i.e., answer), we first generate multiple consistent equations via a group of heuristic rules. We then feed them to a question generator together with the scenario to obtain the corresponding diverse questions, forming a new MWP with a variety of questions and equations. Finally we engage a data filter to remove those unreasonable MWPs, keeping the high-quality augmented ones. To evaluate the ability of learning by analogy for an MWP solver, we generate a new MWP dataset (called DiverseMath23K) with diverse questions by extending the current benchmark Math23K. Extensive experimental results demonstrate that our proposed method can generate high-quality diverse questions with corresponding equations, further leading to performance improvement on Diverse-Math23K. The code and dataset is available at: https://github.com/zhouzihao501/DiverseMWP","sentences":["Solving math word problem (MWP) with AI techniques has recently made great progress with the success of deep neural networks (DNN), but it is far from being solved.","We argue that the ability of learning by analogy is essential for an MWP solver to better understand same problems which may typically be formulated in diverse ways.","However most existing works exploit the shortcut learning to train MWP solvers simply based on samples with a single question.","In lack of diverse questions, these methods merely learn shallow heuristics.","In this paper, we make a first attempt to solve MWPs by generating diverse yet consistent questions/equations.","Given a typical MWP including the scenario description, question, and equation (i.e., answer), we first generate multiple consistent equations via a group of heuristic rules.","We then feed them to a question generator together with the scenario to obtain the corresponding diverse questions, forming a new MWP with a variety of questions and equations.","Finally we engage a data filter to remove those unreasonable MWPs, keeping the high-quality augmented ones.","To evaluate the ability of learning by analogy for an MWP solver, we generate a new MWP dataset (called DiverseMath23K) with diverse questions by extending the current benchmark Math23K. Extensive experimental results demonstrate that our proposed method can generate high-quality diverse questions with corresponding equations, further leading to performance improvement on Diverse-Math23K. The code and dataset is available at: https://github.com/zhouzihao501/DiverseMWP"],"url":"http://arxiv.org/abs/2306.09064v1"}
{"created":"2023-06-15","title":"A Sensitive Test of Non-Gaussianity in Gravitational-wave Detector Data","abstract":"Methods for parameter estimation of gravitational-wave data assume that detector noise is stationary and Gaussian. Real data deviates from these assumptions, which will cause bias in the inferred parameters and incorrect estimates of the errors. We develop a sensitive test of non-Gaussianity for real gravitational-wave data which measures meaningful parameters that can be used to characterize these effects. As a test case, we investigate the quality of data cleaning performed on the binary black hole signal GW200129 which overlapped with the noise produced by the radio frequency modulation. We demonstrate that the data cleaned by LIGO-Virgo-KAGRA contains less noise than the original data, especially for frequencies below 50 Hz.","sentences":["Methods for parameter estimation of gravitational-wave data assume that detector noise is stationary and Gaussian.","Real data deviates from these assumptions, which will cause bias in the inferred parameters and incorrect estimates of the errors.","We develop a sensitive test of non-Gaussianity for real gravitational-wave data which measures meaningful parameters that can be used to characterize these effects.","As a test case, we investigate the quality of data cleaning performed on the binary black hole signal GW200129 which overlapped with the noise produced by the radio frequency modulation.","We demonstrate that the data cleaned by LIGO-Virgo-KAGRA contains less noise than the original data, especially for frequencies below 50 Hz."],"url":"http://arxiv.org/abs/2306.09019v1"}
{"created":"2023-06-15","title":"Overcoming the Limitations of Localization Uncertainty: Efficient & Exact Non-Linear Post-Processing and Calibration","abstract":"Robustly and accurately localizing objects in real-world environments can be challenging due to noisy data, hardware limitations, and the inherent randomness of physical systems. To account for these factors, existing works estimate the aleatoric uncertainty of object detectors by modeling their localization output as a Gaussian distribution $\\mathcal{N}(\\mu,\\,\\sigma^{2})\\,$, and training with loss attenuation. We identify three aspects that are unaddressed in the state of the art, but warrant further exploration: (1) the efficient and mathematically sound propagation of $\\mathcal{N}(\\mu,\\,\\sigma^{2})\\,$ through non-linear post-processing, (2) the calibration of the predicted uncertainty, and (3) its interpretation. We overcome these limitations by: (1) implementing loss attenuation in EfficientDet, and proposing two deterministic methods for the exact and fast propagation of the output distribution, (2) demonstrating on the KITTI and BDD100K datasets that the predicted uncertainty is miscalibrated, and adapting two calibration methods to the localization task, and (3) investigating the correlation between aleatoric uncertainty and task-relevant error sources. Our contributions are: (1) up to five times faster propagation while increasing localization performance by up to 1\\%, (2) up to fifteen times smaller expected calibration error, and (3) the predicted uncertainty is found to correlate with occlusion, object distance, detection accuracy, and image quality.","sentences":["Robustly and accurately localizing objects in real-world environments can be challenging due to noisy data, hardware limitations, and the inherent randomness of physical systems.","To account for these factors, existing works estimate the aleatoric uncertainty of object detectors by modeling their localization output as a Gaussian distribution $\\mathcal{N}(\\mu,\\,\\sigma^{2})\\,$, and training with loss attenuation.","We identify three aspects that are unaddressed in the state of the art, but warrant further exploration: (1) the efficient and mathematically sound propagation of $\\mathcal{N}(\\mu,\\,\\sigma^{2})\\,$ through non-linear post-processing, (2) the calibration of the predicted uncertainty, and (3) its interpretation.","We overcome these limitations by: (1) implementing loss attenuation in EfficientDet, and proposing two deterministic methods for the exact and fast propagation of the output distribution, (2) demonstrating on the KITTI and BDD100K datasets that the predicted uncertainty is miscalibrated, and adapting two calibration methods to the localization task, and (3) investigating the correlation between aleatoric uncertainty and task-relevant error sources.","Our contributions are: (1) up to five times faster propagation while increasing localization performance by up to 1\\%, (2) up to fifteen times smaller expected calibration error, and (3) the predicted uncertainty is found to correlate with occlusion, object distance, detection accuracy, and image quality."],"url":"http://arxiv.org/abs/2306.08981v1"}
{"created":"2023-06-15","title":"Enhancing Neural Rendering Methods with Image Augmentations","abstract":"Faithfully reconstructing 3D geometry and generating novel views of scenes are critical tasks in 3D computer vision. Despite the widespread use of image augmentations across computer vision applications, their potential remains underexplored when learning neural rendering methods (NRMs) for 3D scenes. This paper presents a comprehensive analysis of the use of image augmentations in NRMs, where we explore different augmentation strategies. We found that introducing image augmentations during training presents challenges such as geometric and photometric inconsistencies for learning NRMs from images. Specifically, geometric inconsistencies arise from alterations in shapes, positions, and orientations from the augmentations, disrupting spatial cues necessary for accurate 3D reconstruction. On the other hand, photometric inconsistencies arise from changes in pixel intensities introduced by the augmentations, affecting the ability to capture the underlying 3D structures of the scene. We alleviate these issues by focusing on color manipulations and introducing learnable appearance embeddings that allow NRMs to explain away photometric variations. Our experiments demonstrate the benefits of incorporating augmentations when learning NRMs, including improved photometric quality and surface reconstruction, as well as enhanced robustness against data quality issues, such as reduced training data and image degradations.","sentences":["Faithfully reconstructing 3D geometry and generating novel views of scenes are critical tasks in 3D computer vision.","Despite the widespread use of image augmentations across computer vision applications, their potential remains underexplored when learning neural rendering methods (NRMs) for 3D scenes.","This paper presents a comprehensive analysis of the use of image augmentations in NRMs, where we explore different augmentation strategies.","We found that introducing image augmentations during training presents challenges such as geometric and photometric inconsistencies for learning NRMs from images.","Specifically, geometric inconsistencies arise from alterations in shapes, positions, and orientations from the augmentations, disrupting spatial cues necessary for accurate 3D reconstruction.","On the other hand, photometric inconsistencies arise from changes in pixel intensities introduced by the augmentations, affecting the ability to capture the underlying 3D structures of the scene.","We alleviate these issues by focusing on color manipulations and introducing learnable appearance embeddings that allow NRMs to explain away photometric variations.","Our experiments demonstrate the benefits of incorporating augmentations when learning NRMs, including improved photometric quality and surface reconstruction, as well as enhanced robustness against data quality issues, such as reduced training data and image degradations."],"url":"http://arxiv.org/abs/2306.08904v1"}
{"created":"2023-06-15","title":"In Search of netUnicorn: A Data-Collection Platform to Develop Generalizable ML Models for Network Security Problems","abstract":"The remarkable success of the use of machine learning-based solutions for network security problems has been impeded by the developed ML models' inability to maintain efficacy when used in different network environments exhibiting different network behaviors. This issue is commonly referred to as the generalizability problem of ML models. The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem. Unfortunately, these methods are generally ill-suited or even counterproductive in the network security domain, where they often result in unrealistic or poor-quality datasets.   To address this issue, we propose an augmented ML pipeline that leverages explainable ML tools to guide the network data collection in an iterative fashion. To ensure the data's realism and quality, we require that the new datasets should be endogenously collected in this iterative process, thus advocating for a gradual removal of data-related problems to improve model generalizability. To realize this capability, we develop a data-collection platform, netUnicorn, that takes inspiration from the classic \"hourglass\" model and is implemented as its \"thin waist\" to simplify data collection for different learning problems from diverse network environments. The proposed system decouples data-collection intents from the deployment mechanisms and disaggregates these high-level intents into smaller reusable, self-contained tasks.   We demonstrate how netUnicorn simplifies collecting data for different learning problems from multiple network environments and how the proposed iterative data collection improves a model's generalizability.","sentences":["The remarkable success of the use of machine learning-based solutions for network security problems has been impeded by the developed ML models' inability to maintain efficacy when used in different network environments exhibiting different network behaviors.","This issue is commonly referred to as the generalizability problem of ML models.","The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem.","Unfortunately, these methods are generally ill-suited or even counterproductive in the network security domain, where they often result in unrealistic or poor-quality datasets.   ","To address this issue, we propose an augmented ML pipeline that leverages explainable ML tools to guide the network data collection in an iterative fashion.","To ensure the data's realism and quality, we require that the new datasets should be endogenously collected in this iterative process, thus advocating for a gradual removal of data-related problems to improve model generalizability.","To realize this capability, we develop a data-collection platform, netUnicorn, that takes inspiration from the classic \"hourglass\" model and is implemented as its \"thin waist\" to simplify data collection for different learning problems from diverse network environments.","The proposed system decouples data-collection intents from the deployment mechanisms and disaggregates these high-level intents into smaller reusable, self-contained tasks.   ","We demonstrate how netUnicorn simplifies collecting data for different learning problems from multiple network environments and how the proposed iterative data collection improves a model's generalizability."],"url":"http://arxiv.org/abs/2306.08853v1"}
{"created":"2023-06-15","title":"Your Room is not Private: Gradient Inversion Attack for Deep Q-Learning","abstract":"The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot access substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to decision-making algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the Deep Q-Learning algorithm, utilizing gradient inversion to reconstruct states, actions, and Q-values. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or transmitting the data to public servers. Nevertheless, these gradients contain sufficient information to potentially expose private data. To validate our approach, we conduct experiments on the AI2THOR simulator and evaluate our algorithm on active perception, a prevalent task in embodied AI. The experimental results convincingly demonstrate the effectiveness of our method in successfully recovering all information from the data across all 120 room layouts.","sentences":["The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models.","Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot access substantial personal information.","However, the issue of privacy leakage in embodied AI tasks, particularly in relation to decision-making algorithms, has not received adequate consideration in research.","This paper aims to address this gap by proposing an attack on the Deep Q-Learning algorithm, utilizing gradient inversion to reconstruct states, actions, and Q-values.","The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or transmitting the data to public servers.","Nevertheless, these gradients contain sufficient information to potentially expose private data.","To validate our approach, we conduct experiments on the AI2THOR simulator and evaluate our algorithm on active perception, a prevalent task in embodied AI.","The experimental results convincingly demonstrate the effectiveness of our method in successfully recovering all information from the data across all 120 room layouts."],"url":"http://arxiv.org/abs/2306.09273v1"}
