{"created":"2023-09-05 17:59:58","title":"GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction","abstract":"Neural implicit representations have recently demonstrated compelling results on dense Simultaneous Localization And Mapping (SLAM) but suffer from the accumulation of errors in camera tracking and distortion in the reconstruction. Purposely, we present GO-SLAM, a deep-learning-based dense visual SLAM framework globally optimizing poses and 3D reconstruction in real-time. Robust pose estimation is at its core, supported by efficient loop closing and online full bundle adjustment, which optimize per frame by utilizing the learned global geometry of the complete history of input frames. Simultaneously, we update the implicit and continuous surface representation on-the-fly to ensure global consistency of 3D reconstruction. Results on various synthetic and real-world datasets demonstrate that GO-SLAM outperforms state-of-the-art approaches at tracking robustness and reconstruction accuracy. Furthermore, GO-SLAM is versatile and can run with monocular, stereo, and RGB-D input.","sentences":["Neural implicit representations have recently demonstrated compelling results on dense Simultaneous Localization And Mapping (SLAM) but suffer from the accumulation of errors in camera tracking and distortion in the reconstruction.","Purposely, we present GO-SLAM, a deep-learning-based dense visual SLAM framework globally optimizing poses and 3D reconstruction in real-time.","Robust pose estimation is at its core, supported by efficient loop closing and online full bundle adjustment, which optimize per frame by utilizing the learned global geometry of the complete history of input frames.","Simultaneously, we update the implicit and continuous surface representation on-the-fly to ensure global consistency of 3D reconstruction.","Results on various synthetic and real-world datasets demonstrate that GO-SLAM outperforms state-of-the-art approaches at tracking robustness and reconstruction accuracy.","Furthermore, GO-SLAM is versatile and can run with monocular, stereo, and RGB-D input."],"url":"http://arxiv.org/abs/2309.02436v1"}
{"created":"2023-09-05 17:59:45","title":"Efficient RL via Disentangled Environment and Agent Representations","abstract":"Agents that are aware of the separation between themselves and their environments can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, such as its shape or mask, which is often inexpensive to obtain. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, Structured Environment-Agent Representations, outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots. Website at https://sear-rl.github.io/","sentences":["Agents that are aware of the separation between themselves and their environments can leverage this understanding to form effective representations of visual input.","We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, such as its shape or mask, which is often inexpensive to obtain.","This is incorporated into the RL objective using a simple auxiliary loss.","We show that our method, Structured Environment-Agent Representations, outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots.","Website at https://sear-rl.github.io/"],"url":"http://arxiv.org/abs/2309.02435v1"}
{"created":"2023-09-05 17:59:42","title":"ReliTalk: Relightable Talking Portrait Generation from a Single Video","abstract":"Recent years have witnessed great progress in creating vivid audio-driven portraits from monocular videos. However, how to seamlessly adapt the created video avatars to other scenarios with different backgrounds and lighting conditions remains unsolved. On the other hand, existing relighting studies mostly rely on dynamically lighted or multi-view data, which are too expensive for creating video portraits. To bridge this gap, we propose ReliTalk, a novel framework for relightable audio-driven talking portrait generation from monocular videos. Our key insight is to decompose the portrait's reflectance from implicitly learned audio-driven facial normals and images. Specifically, we involve 3D facial priors derived from audio features to predict delicate normal maps through implicit functions. These initially predicted normals then take a crucial part in reflectance decomposition by dynamically estimating the lighting condition of the given video. Moreover, the stereoscopic face representation is refined using the identity-consistent loss under simulated multiple lighting conditions, addressing the ill-posed problem caused by limited views available from a single monocular video. Extensive experiments validate the superiority of our proposed framework on both real and synthetic datasets. Our code is released in https://github.com/arthur-qiu/ReliTalk.","sentences":["Recent years have witnessed great progress in creating vivid audio-driven portraits from monocular videos.","However, how to seamlessly adapt the created video avatars to other scenarios with different backgrounds and lighting conditions remains unsolved.","On the other hand, existing relighting studies mostly rely on dynamically lighted or multi-view data, which are too expensive for creating video portraits.","To bridge this gap, we propose ReliTalk, a novel framework for relightable audio-driven talking portrait generation from monocular videos.","Our key insight is to decompose the portrait's reflectance from implicitly learned audio-driven facial normals and images.","Specifically, we involve 3D facial priors derived from audio features to predict delicate normal maps through implicit functions.","These initially predicted normals then take a crucial part in reflectance decomposition by dynamically estimating the lighting condition of the given video.","Moreover, the stereoscopic face representation is refined using the identity-consistent loss under simulated multiple lighting conditions, addressing the ill-posed problem caused by limited views available from a single monocular video.","Extensive experiments validate the superiority of our proposed framework on both real and synthetic datasets.","Our code is released in https://github.com/arthur-qiu/ReliTalk."],"url":"http://arxiv.org/abs/2309.02434v1"}
{"created":"2023-09-05 17:57:31","title":"Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach","abstract":"Estimating the transferability of publicly available pretrained models to a target task has assumed an important place for transfer learning tasks in recent years. Existing efforts propose metrics that allow a user to choose one model from a pool of pre-trained models without having to fine-tune each model individually and identify one explicitly. With the growth in the number of available pre-trained models and the popularity of model ensembles, it also becomes essential to study the transferability of multiple-source models for a given target task. The few existing efforts study transferability in such multi-source ensemble settings using just the outputs of the classification layer and neglect possible domain or task mismatch. Moreover, they overlook the most important factor while selecting the source models, viz., the cohesiveness factor between them, which can impact the performance and confidence in the prediction of the ensemble. To address these gaps, we propose a novel Optimal tranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate the transferability of an ensemble of models to a downstream task. OSBORN collectively accounts for image domain difference, task difference, and cohesiveness of models in the ensemble to provide reliable estimates of transferability. We gauge the performance of OSBORN on both image classification and semantic segmentation tasks. Our setup includes 28 source datasets, 11 target datasets, 5 model architectures, and 2 pre-training methods. We benchmark our method against current state-of-the-art metrics MS-LEEP and E-LEEP, and outperform them consistently using the proposed approach.","sentences":["Estimating the transferability of publicly available pretrained models to a target task has assumed an important place for transfer learning tasks in recent years.","Existing efforts propose metrics that allow a user to choose one model from a pool of pre-trained models without having to fine-tune each model individually and identify one explicitly.","With the growth in the number of available pre-trained models and the popularity of model ensembles, it also becomes essential to study the transferability of multiple-source models for a given target task.","The few existing efforts study transferability in such multi-source ensemble settings using just the outputs of the classification layer and neglect possible domain or task mismatch.","Moreover, they overlook the most important factor while selecting the source models, viz., the cohesiveness factor between them, which can impact the performance and confidence in the prediction of the ensemble.","To address these gaps, we propose a novel Optimal tranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate the transferability of an ensemble of models to a downstream task.","OSBORN collectively accounts for image domain difference, task difference, and cohesiveness of models in the ensemble to provide reliable estimates of transferability.","We gauge the performance of OSBORN on both image classification and semantic segmentation tasks.","Our setup includes 28 source datasets, 11 target datasets, 5 model architectures, and 2 pre-training methods.","We benchmark our method against current state-of-the-art metrics MS-LEEP and E-LEEP, and outperform them consistently using the proposed approach."],"url":"http://arxiv.org/abs/2309.02429v1"}
{"created":"2023-09-05 17:56:22","title":"Tensorization: Creating and Utilising Multidimensional Datasets for Multiway Analysis and Tensorised Deep Neural Networks -- Python Tutorial and Survey","abstract":"As the size and complexity of data continue to increase, the need for efficient and effective analysis methods becomes ever more crucial. Tensorization, the process of converting 2-dimensional datasets into multidimensional structures, has emerged as a promising approach for multiway analysis methods. This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches. A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python. Results indicate that multiway analysis is more expressive. Additionally, tensorization techniques aid in compressing deep learning models by reducing the number of required parameters while enhancing the expression of relationships across dimensions. A survey of the multi-away analysis methods and integration with various Deep Neural Networks models is presented using case studies in different domains.","sentences":["As the size and complexity of data continue to increase, the need for efficient and effective analysis methods becomes ever more crucial.","Tensorization, the process of converting 2-dimensional datasets into multidimensional structures, has emerged as a promising approach for multiway analysis methods.","This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches.","A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python.","Results indicate that multiway analysis is more expressive.","Additionally, tensorization techniques aid in compressing deep learning models by reducing the number of required parameters while enhancing the expression of relationships across dimensions.","A survey of the multi-away analysis methods and integration with various Deep Neural Networks models is presented using case studies in different domains."],"url":"http://arxiv.org/abs/2309.02428v1"}
{"created":"2023-09-05 17:56:20","title":"Cognitive Architectures for Language Agents","abstract":"Recent efforts have incorporated large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning. However, these efforts have largely been piecemeal, lacking a systematic framework for constructing a fully-fledged language agent. To address this challenge, we draw on the rich history of agent design in symbolic artificial intelligence to develop a blueprint for a new wave of cognitive language agents. We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems. We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework. Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future.","sentences":["Recent efforts have incorporated large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning.","However, these efforts have largely been piecemeal, lacking a systematic framework for constructing a fully-fledged language agent.","To address this challenge, we draw on the rich history of agent design in symbolic artificial intelligence to develop a blueprint for a new wave of cognitive language agents.","We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems.","We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework.","Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future."],"url":"http://arxiv.org/abs/2309.02427v1"}
{"created":"2023-09-05 17:53:10","title":"On the Minimax Regret in Online Ranking with Top-k Feedback","abstract":"In online ranking, a learning algorithm sequentially ranks a set of items and receives feedback on its ranking in the form of relevance scores. Since obtaining relevance scores typically involves human annotation, it is of great interest to consider a partial feedback setting where feedback is restricted to the top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed a framework to analyze online ranking algorithms with top $k$ feedback. A key element in their work was the use of techniques from partial monitoring. In this paper, we further investigate online ranking with top $k$ feedback and solve some open problems posed by Chaudhuri and Tewari [2017]. We provide a full characterization of minimax regret rates with the top $k$ feedback model for all $k$ and for the following ranking performance measures: Pairwise Loss, Discounted Cumulative Gain, and Precision@n. In addition, we give an efficient algorithm that achieves the minimax regret rate for Precision@n.","sentences":["In online ranking, a learning algorithm sequentially ranks a set of items and receives feedback on its ranking in the form of relevance scores.","Since obtaining relevance scores typically involves human annotation, it is of great interest to consider a partial feedback setting where feedback is restricted to the top-$k$ items in the rankings.","Chaudhuri and Tewari","[2017] developed a framework to analyze online ranking algorithms with top $k$ feedback.","A key element in their work was the use of techniques from partial monitoring.","In this paper, we further investigate online ranking with top $k$ feedback and solve some open problems posed by Chaudhuri and Tewari [2017].","We provide a full characterization of minimax regret rates with the top $k$ feedback model for all $k$ and for the following ranking performance measures: Pairwise Loss, Discounted Cumulative Gain, and Precision@n.","In addition, we give an efficient algorithm that achieves the minimax regret rate for Precision@n."],"url":"http://arxiv.org/abs/2309.02425v1"}
{"created":"2023-09-05 17:51:16","title":"EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding","abstract":"With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI), large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed. However, most current research is built on resources derived from third-person video action recognition. This inherent domain gap between first- and third-person action videos, which have not been adequately addressed before, makes current Ego-HOI suboptimal. This paper rethinks and proposes a new framework as an infrastructure to advance Ego-HOI recognition by Probing, Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets, balanced test sets and a new baseline, which are complete with a training-finetuning strategy. With our new framework, we not only achieve state-of-the-art performance on Ego-HOI benchmarks but also build several new and effective mechanisms and settings to advance further research. We believe our data and the findings will pave a new way for Ego-HOI understanding. Code and data are available at https://mvig-rhos.com/ego_pca","sentences":["With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI), large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed.","However, most current research is built on resources derived from third-person video action recognition.","This inherent domain gap between first- and third-person action videos, which have not been adequately addressed before, makes current Ego-HOI suboptimal.","This paper rethinks and proposes a new framework as an infrastructure to advance Ego-HOI recognition by Probing, Curation and Adaption (EgoPCA).","We contribute comprehensive pre-train sets, balanced test sets and a new baseline, which are complete with a training-finetuning strategy.","With our new framework, we not only achieve state-of-the-art performance on Ego-HOI benchmarks but also build several new and effective mechanisms and settings to advance further research.","We believe our data and the findings will pave a new way for Ego-HOI understanding.","Code and data are available at https://mvig-rhos.com/ego_pca"],"url":"http://arxiv.org/abs/2309.02423v1"}
{"created":"2023-09-05 17:50:36","title":"Doppelgangers: Learning to Disambiguate Images of Similar Structures","abstract":"We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building). Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results. We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs. To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels. We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues. Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions. See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/.","sentences":["We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building).","Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results.","We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs.","To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels.","We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues.","Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions.","See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/."],"url":"http://arxiv.org/abs/2309.02420v1"}
{"created":"2023-09-05 17:40:34","title":"Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices","abstract":"In this paper, we present Delta-LoRA, which is a novel parameter-efficient approach to fine-tune large language models (LLMs). In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $\\bA$ and $\\bB$, but also propagate the learning to the pre-trained weights $\\bW$ via updates utilizing the delta of the product of two low-rank matrices ($\\bA^{(t+1)}\\bB^{(t+1)} - \\bA^{(t)}\\bB^{(t)}$). Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks. Moreover, as the update of $\\bW$ does not need to compute the gradients of $\\bW$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA. Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods. We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA.","sentences":["In this paper, we present Delta-LoRA, which is a novel parameter-efficient approach to fine-tune large language models (LLMs).","In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $\\bA$ and $\\bB$, but also propagate the learning to the pre-trained weights $\\bW$ via updates utilizing the delta of the product of two low-rank matrices ($\\bA^{(t+1)}\\bB^{(t+1)} - \\bA^{(t)}\\bB^{(t)}$).","Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks.","Moreover, as the update of $\\bW$ does not need to compute the gradients of $\\bW$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA.","Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods.","We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA."],"url":"http://arxiv.org/abs/2309.02411v1"}
{"created":"2023-09-05 17:36:40","title":"Generating Realistic Images from In-the-wild Sounds","abstract":"Representing wild sounds as images is an important but challenging task due to the lack of paired datasets between sound and images and the significant differences in the characteristics of these two modalities. Previous studies have focused on generating images from sound in limited categories or music. In this paper, we propose a novel approach to generate images from in-the-wild sounds. First, we convert sound into text using audio captioning. Second, we propose audio attention and sentence attention to represent the rich characteristics of sound and visualize the sound. Lastly, we propose a direct sound optimization with CLIPscore and AudioCLIP and generate images with a diffusion-based model. In experiments, it shows that our model is able to generate high quality images from wild sounds and outperforms baselines in both quantitative and qualitative evaluations on wild audio datasets.","sentences":["Representing wild sounds as images is an important but challenging task due to the lack of paired datasets between sound and images and the significant differences in the characteristics of these two modalities.","Previous studies have focused on generating images from sound in limited categories or music.","In this paper, we propose a novel approach to generate images from in-the-wild sounds.","First, we convert sound into text using audio captioning.","Second, we propose audio attention and sentence attention to represent the rich characteristics of sound and visualize the sound.","Lastly, we propose a direct sound optimization with CLIPscore and AudioCLIP and generate images with a diffusion-based model.","In experiments, it shows that our model is able to generate high quality images from wild sounds and outperforms baselines in both quantitative and qualitative evaluations on wild audio datasets."],"url":"http://arxiv.org/abs/2309.02405v1"}
{"created":"2023-09-05 17:36:34","title":"Voice Morphing: Two Identities in One Voice","abstract":"In a biometric system, each biometric sample or template is typically associated with a single identity. However, recent research has demonstrated the possibility of generating \"morph\" biometric samples that can successfully match more than a single identity. Morph attacks are now recognized as a potential security threat to biometric systems. However, most morph attacks have been studied on biometric modalities operating in the image domain, such as face, fingerprint, and iris. In this preliminary work, we introduce Voice Identity Morphing (VIM) - a voice-based morph attack that can synthesize speech samples that impersonate the voice characteristics of a pair of individuals. Our experiments evaluate the vulnerabilities of two popular speaker recognition systems, ECAPA-TDNN and x-vector, to VIM, with a success rate (MMPMR) of over 80% at a false match rate of 1% on the Librispeech dataset.","sentences":["In a biometric system, each biometric sample or template is typically associated with a single identity.","However, recent research has demonstrated the possibility of generating \"morph\" biometric samples that can successfully match more than a single identity.","Morph attacks are now recognized as a potential security threat to biometric systems.","However, most morph attacks have been studied on biometric modalities operating in the image domain, such as face, fingerprint, and iris.","In this preliminary work, we introduce Voice Identity Morphing (VIM) - a voice-based morph attack that can synthesize speech samples that impersonate the voice characteristics of a pair of individuals.","Our experiments evaluate the vulnerabilities of two popular speaker recognition systems, ECAPA-TDNN and x-vector, to VIM, with a success rate (MMPMR) of over 80% at a false match rate of 1% on the Librispeech dataset."],"url":"http://arxiv.org/abs/2309.02404v1"}
{"created":"2023-09-05 17:33:59","title":"Substitution-based Semantic Change Detection using Contextual Embeddings","abstract":"Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors. Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation. We present a simplified approach to measuring semantic change using contextual embeddings, relying only on the most probable substitutes for masked terms. Not only is this approach directly interpretable, it is also far more efficient in terms of storage, achieves superior average performance across the most frequently cited datasets for this task, and allows for more nuanced investigation of change than is possible with static word vectors.","sentences":["Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors.","Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation.","We present a simplified approach to measuring semantic change using contextual embeddings, relying only on the most probable substitutes for masked terms.","Not only is this approach directly interpretable, it is also far more efficient in terms of storage, achieves superior average performance across the most frequently cited datasets for this task, and allows for more nuanced investigation of change than is possible with static word vectors."],"url":"http://arxiv.org/abs/2309.02403v1"}
{"created":"2023-09-05 17:31:10","title":"Breaking Barriers to Creative Expression: Co-Designing and Implementing an Accessible Text-to-Image Interface","abstract":"Text-to-image generation models have grown in popularity due to their ability to produce high-quality images from a text prompt. One use for this technology is to enable the creation of more accessible art creation software. In this paper, we document the development of an alternative user interface that reduces the typing effort needed to enter image prompts by providing suggestions from a large language model, developed through iterative design and testing within the project team. The results of this testing demonstrate how generative text models can support the accessibility of text-to-image models, enabling users with a range of abilities to create visual art.","sentences":["Text-to-image generation models have grown in popularity due to their ability to produce high-quality images from a text prompt.","One use for this technology is to enable the creation of more accessible art creation software.","In this paper, we document the development of an alternative user interface that reduces the typing effort needed to enter image prompts by providing suggestions from a large language model, developed through iterative design and testing within the project team.","The results of this testing demonstrate how generative text models can support the accessibility of text-to-image models, enabling users with a range of abilities to create visual art."],"url":"http://arxiv.org/abs/2309.02402v1"}
{"created":"2023-09-05 17:27:16","title":"Prototype-based Dataset Comparison","abstract":"Dataset summarisation is a fruitful approach to dataset inspection. However, when applied to a single dataset the discovery of visual concepts is restricted to those most prominent. We argue that a comparative approach can expand upon this paradigm to enable richer forms of dataset inspection that go beyond the most prominent concepts. To enable dataset comparison we present a module that learns concept-level prototypes across datasets. We leverage self-supervised learning to discover these prototypes without supervision, and we demonstrate the benefits of our approach in two case-studies. Our findings show that dataset comparison extends dataset inspection and we hope to encourage more works in this direction. Code and usage instructions available at https://github.com/Nanne/ProtoSim","sentences":["Dataset summarisation is a fruitful approach to dataset inspection.","However, when applied to a single dataset the discovery of visual concepts is restricted to those most prominent.","We argue that a comparative approach can expand upon this paradigm to enable richer forms of dataset inspection that go beyond the most prominent concepts.","To enable dataset comparison we present a module that learns concept-level prototypes across datasets.","We leverage self-supervised learning to discover these prototypes without supervision, and we demonstrate the benefits of our approach in two case-studies.","Our findings show that dataset comparison extends dataset inspection and we hope to encourage more works in this direction.","Code and usage instructions available at https://github.com/Nanne/ProtoSim"],"url":"http://arxiv.org/abs/2309.02401v1"}
{"created":"2023-09-05 17:13:47","title":"The Batik-plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations","abstract":"We present the Batik-plays-Mozart Corpus, a piano performance dataset combining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored B\\\"osendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can further be connected to the musicological annotations (harmony, cadences, phrases) on these scores that were recently published by Hentschel et al. (2021).   The result is a high-quality, high-precision corpus mapping scores and musical structure annotations to precise note-level professional performance information. As the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects. In the paper, we outline the curation process of the alignment and conduct two exploratory experiments to demonstrate its usefulness in analyzing expressive performance.","sentences":["We present the Batik-plays-Mozart Corpus, a piano performance dataset combining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level.","The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored B\\\"osendorfer grand piano, and are available both as MIDI files and audio recordings.","They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can further be connected to the musicological annotations (harmony, cadences, phrases) on these scores that were recently published by Hentschel et al. (2021).   ","The result is a high-quality, high-precision corpus mapping scores and musical structure annotations to precise note-level professional performance information.","As the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects.","In the paper, we outline the curation process of the alignment and conduct two exploratory experiments to demonstrate its usefulness in analyzing expressive performance."],"url":"http://arxiv.org/abs/2309.02399v1"}
{"created":"2023-09-05 17:09:38","title":"Black-Box Attacks against Signed Graph Analysis via Balance Poisoning","abstract":"Signed graphs are well-suited for modeling social networks as they capture both positive and negative relationships. Signed graph neural networks (SGNNs) are commonly employed to predict link signs (i.e., positive and negative) in such graphs due to their ability to handle the unique structure of signed graphs. However, real-world signed graphs are vulnerable to malicious attacks by manipulating edge relationships, and existing adversarial graph attack methods do not consider the specific structure of signed graphs. SGNNs often incorporate balance theory to effectively model the positive and negative links. Surprisingly, we find that the balance theory that they rely on can ironically be exploited as a black-box attack. In this paper, we propose a novel black-box attack called balance-attack that aims to decrease the balance degree of the signed graphs. We present an efficient heuristic algorithm to solve this NP-hard optimization problem. We conduct extensive experiments on five popular SGNN models and four real-world datasets to demonstrate the effectiveness and wide applicability of our proposed attack method. By addressing these challenges, our research contributes to a better understanding of the limitations and resilience of robust models when facing attacks on SGNNs. This work contributes to enhancing the security and reliability of signed graph analysis in social network modeling. Our PyTorch implementation of the attack is publicly available on GitHub: https://github.com/JialongZhou666/Balance-Attack.git.","sentences":["Signed graphs are well-suited for modeling social networks as they capture both positive and negative relationships.","Signed graph neural networks (SGNNs) are commonly employed to predict link signs (i.e., positive and negative) in such graphs due to their ability to handle the unique structure of signed graphs.","However, real-world signed graphs are vulnerable to malicious attacks by manipulating edge relationships, and existing adversarial graph attack methods do not consider the specific structure of signed graphs.","SGNNs often incorporate balance theory to effectively model the positive and negative links.","Surprisingly, we find that the balance theory that they rely on can ironically be exploited as a black-box attack.","In this paper, we propose a novel black-box attack called balance-attack that aims to decrease the balance degree of the signed graphs.","We present an efficient heuristic algorithm to solve this NP-hard optimization problem.","We conduct extensive experiments on five popular SGNN models and four real-world datasets to demonstrate the effectiveness and wide applicability of our proposed attack method.","By addressing these challenges, our research contributes to a better understanding of the limitations and resilience of robust models when facing attacks on SGNNs.","This work contributes to enhancing the security and reliability of signed graph analysis in social network modeling.","Our PyTorch implementation of the attack is publicly available on GitHub: https://github.com/JialongZhou666/Balance-Attack.git."],"url":"http://arxiv.org/abs/2309.02396v1"}
{"created":"2023-09-05 17:05:52","title":"Mind the Gap: The Difference Between Coverage and Mutation Score Can Guide Testing Efforts","abstract":"An \"adequate\" test suite should effectively find all inconsistencies between a system's requirements/specifications and its implementation. Practitioners frequently use code coverage to approximate adequacy, while academics argue that mutation score may better approximate true (oracular) adequacy coverage. High code coverage is increasingly attainable even on large systems via automatic test generation, including fuzzing. In light of all of these options for measuring and improving testing effort, how should a QA engineer spend their time? We propose a new framework for reasoning about the extent, limits, and nature of a given testing effort based on an idea we call the oracle gap, or the difference between source code coverage and mutation score for a given software element. We conduct (1) a large-scale observational study of the oracle gap across popular Maven projects, (2) a study that varies testing and oracle quality across several of those projects and (3) a small-scale observational study of highly critical, well-tested code across comparable blockchain projects. We show that the oracle gap surfaces important information about the extent and quality of a test effort beyond either adequacy metric alone. In particular, it provides a way for practitioners to identify source files where it is likely a weak oracle tests important code.","sentences":["An \"adequate\" test suite should effectively find all inconsistencies between a system's requirements/specifications and its implementation.","Practitioners frequently use code coverage to approximate adequacy, while academics argue that mutation score may better approximate true (oracular) adequacy coverage.","High code coverage is increasingly attainable even on large systems via automatic test generation, including fuzzing.","In light of all of these options for measuring and improving testing effort, how should a QA engineer spend their time?","We propose a new framework for reasoning about the extent, limits, and nature of a given testing effort based on an idea we call the oracle gap, or the difference between source code coverage and mutation score for a given software element.","We conduct (1) a large-scale observational study of the oracle gap across popular Maven projects, (2) a study that varies testing and oracle quality across several of those projects and (3) a small-scale observational study of highly critical, well-tested code across comparable blockchain projects.","We show that the oracle gap surfaces important information about the extent and quality of a test effort beyond either adequacy metric alone.","In particular, it provides a way for practitioners to identify source files where it is likely a weak oracle tests important code."],"url":"http://arxiv.org/abs/2309.02395v1"}
{"created":"2023-09-05 17:05:16","title":"Magnetic Navigation using Attitude-Invariant Magnetic Field Information for Loop Closure Detection","abstract":"Indoor magnetic fields are a combination of Earth's magnetic field and disruptions induced by ferromagnetic objects, such as steel structural components in buildings. As a result of these disruptions, pervasive in indoor spaces, magnetic field data is often omitted from navigation algorithms in indoor environments. This paper leverages the spatially-varying disruptions to Earth's magnetic field to extract positional information for use in indoor navigation algorithms. The algorithm uses a rate gyro and an array of four magnetometers to estimate the robot's pose. Additionally, the magnetometer array is used to compute attitude-invariant measurements associated with the magnetic field and its gradient. These measurements are used to detect loop closure points. Experimental results indicate that the proposed approach can estimate the pose of a ground robot in an indoor environment within meter accuracy.","sentences":["Indoor magnetic fields are a combination of Earth's magnetic field and disruptions induced by ferromagnetic objects, such as steel structural components in buildings.","As a result of these disruptions, pervasive in indoor spaces, magnetic field data is often omitted from navigation algorithms in indoor environments.","This paper leverages the spatially-varying disruptions to Earth's magnetic field to extract positional information for use in indoor navigation algorithms.","The algorithm uses a rate gyro and an array of four magnetometers to estimate the robot's pose.","Additionally, the magnetometer array is used to compute attitude-invariant measurements associated with the magnetic field and its gradient.","These measurements are used to detect loop closure points.","Experimental results indicate that the proposed approach can estimate the pose of a ground robot in an indoor environment within meter accuracy."],"url":"http://arxiv.org/abs/2309.02394v1"}
{"created":"2023-09-05 17:00:42","title":"Empirical Review of Smart Contract and DeFi Security: Vulnerability Detection and Automated Repair","abstract":"Decentralized Finance (DeFi) is emerging as a peer-to-peer financial ecosystem, enabling participants to trade products on a permissionless blockchain. Built on blockchain and smart contracts, the DeFi ecosystem has experienced explosive growth in recent years. Unfortunately, smart contracts hold a massive amount of value, making them an attractive target for attacks. So far, attacks against smart contracts and DeFi protocols have resulted in billions of dollars in financial losses, severely threatening the security of the entire DeFi ecosystem. Researchers have proposed various security tools for smart contracts and DeFi protocols as countermeasures. However, a comprehensive investigation of these efforts is still lacking, leaving a crucial gap in our understanding of how to enhance the security posture of the smart contract and DeFi landscape.   To fill the gap, this paper reviews the progress made in the field of smart contract and DeFi security from the perspective of both vulnerability detection and automated repair. First, we analyze the DeFi smart contract security issues and challenges. Specifically, we lucubrate various DeFi attack incidents and summarize the attacks into six categories. Then, we present an empirical study of 42 state-of-the-art techniques that can detect smart contract and DeFi vulnerabilities. In particular, we evaluate the effectiveness of traditional smart contract bug detection tools in analyzing complex DeFi protocols. Additionally, we investigate 8 existing automated repair tools for smart contracts and DeFi protocols, providing insight into their advantages and disadvantages. To make this work useful for as wide of an audience as possible, we also identify several open issues and challenges in the DeFi ecosystem that should be addressed in the future.","sentences":["Decentralized Finance (DeFi) is emerging as a peer-to-peer financial ecosystem, enabling participants to trade products on a permissionless blockchain.","Built on blockchain and smart contracts, the DeFi ecosystem has experienced explosive growth in recent years.","Unfortunately, smart contracts hold a massive amount of value, making them an attractive target for attacks.","So far, attacks against smart contracts and DeFi protocols have resulted in billions of dollars in financial losses, severely threatening the security of the entire DeFi ecosystem.","Researchers have proposed various security tools for smart contracts and DeFi protocols as countermeasures.","However, a comprehensive investigation of these efforts is still lacking, leaving a crucial gap in our understanding of how to enhance the security posture of the smart contract and DeFi landscape.   ","To fill the gap, this paper reviews the progress made in the field of smart contract and DeFi security from the perspective of both vulnerability detection and automated repair.","First, we analyze the DeFi smart contract security issues and challenges.","Specifically, we lucubrate various DeFi attack incidents and summarize the attacks into six categories.","Then, we present an empirical study of 42 state-of-the-art techniques that can detect smart contract and DeFi vulnerabilities.","In particular, we evaluate the effectiveness of traditional smart contract bug detection tools in analyzing complex DeFi protocols.","Additionally, we investigate 8 existing automated repair tools for smart contracts and DeFi protocols, providing insight into their advantages and disadvantages.","To make this work useful for as wide of an audience as possible, we also identify several open issues and challenges in the DeFi ecosystem that should be addressed in the future."],"url":"http://arxiv.org/abs/2309.02391v1"}
{"created":"2023-09-05 17:00:24","title":"Explaining grokking through circuit efficiency","abstract":"One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.","sentences":["One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation.","We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm.","We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient.","We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation.","Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy."],"url":"http://arxiv.org/abs/2309.02390v1"}
{"created":"2023-09-05 17:00:15","title":"Contextual Predictive Mutation Testing","abstract":"Mutation testing is a powerful technique for assessing and improving test suite quality that artificially introduces bugs and checks whether the test suites catch them. However, it is also computationally expensive and thus does not scale to large systems and projects. One promising recent approach to tackling this scalability problem uses machine learning to predict whether the tests will detect the synthetic bugs, without actually running those tests. However, existing predictive mutation testing approaches still misclassify 33% of detection outcomes on a randomly sampled set of mutant-test suite pairs. We introduce MutationBERT, an approach for predictive mutation testing that simultaneously encodes the source method mutation and test method, capturing key context in the input representation. Thanks to its higher precision, MutationBERT saves 33% of the time spent by a prior approach on checking/verifying live mutants. MutationBERT, also outperforms the state-of-the-art in both same project and cross project settings, with meaningful improvements in precision, recall, and F1 score. We validate our input representation, and aggregation approaches for lifting predictions from the test matrix level to the test suite level, finding similar improvements in performance. MutationBERT not only enhances the state-of-the-art in predictive mutation testing, but also presents practical benefits for real-world applications, both in saving developer time and finding hard to detect mutants.","sentences":["Mutation testing is a powerful technique for assessing and improving test suite quality that artificially introduces bugs and checks whether the test suites catch them.","However, it is also computationally expensive and thus does not scale to large systems and projects.","One promising recent approach to tackling this scalability problem uses machine learning to predict whether the tests will detect the synthetic bugs, without actually running those tests.","However, existing predictive mutation testing approaches still misclassify 33% of detection outcomes on a randomly sampled set of mutant-test suite pairs.","We introduce MutationBERT, an approach for predictive mutation testing that simultaneously encodes the source method mutation and test method, capturing key context in the input representation.","Thanks to its higher precision, MutationBERT saves 33% of the time spent by a prior approach on checking/verifying live mutants.","MutationBERT, also outperforms the state-of-the-art in both same project and cross project settings, with meaningful improvements in precision, recall, and F1 score.","We validate our input representation, and aggregation approaches for lifting predictions from the test matrix level to the test suite level, finding similar improvements in performance.","MutationBERT not only enhances the state-of-the-art in predictive mutation testing, but also presents practical benefits for real-world applications, both in saving developer time and finding hard to detect mutants."],"url":"http://arxiv.org/abs/2309.02389v1"}
{"created":"2023-09-05 16:53:27","title":"From Curves to Words and Back Again: Geometric Computation of Minimum-Area Homotopy","abstract":"Let $\\gamma$ be a generic closed curve in the plane. Samuel Blank, in his 1967 Ph.D. thesis, determined if $\\gamma$ is self-overlapping by geometrically constructing a combinatorial word from $\\gamma$. More recently, Zipei Nie, in an unpublished manuscript, computed the minimum homotopy area of $\\gamma$ by constructing a combinatorial word algebraically. We provide a unified framework for working with both words and determine the settings under which Blank's word and Nie's word are equivalent. Using this equivalence, we give a new geometric proof for the correctness of Nie's algorithm. Unlike previous work, our proof is constructive which allows us to naturally compute the actual homotopy that realizes the minimum area. Furthermore, we contribute to the theory of self-overlapping curves by providing the first polynomial-time algorithm to compute a self-overlapping decomposition of any closed curve $\\gamma$ with minimum area.","sentences":["Let $\\gamma$ be a generic closed curve in the plane.","Samuel Blank, in his 1967 Ph.D. thesis, determined if $\\gamma$ is self-overlapping by geometrically constructing a combinatorial word from $\\gamma$. More recently, Zipei Nie, in an unpublished manuscript, computed the minimum homotopy area of $\\gamma$ by constructing a combinatorial word algebraically.","We provide a unified framework for working with both words and determine the settings under which Blank's word and Nie's word are equivalent.","Using this equivalence, we give a new geometric proof for the correctness of Nie's algorithm.","Unlike previous work, our proof is constructive which allows us to naturally compute the actual homotopy that realizes the minimum area.","Furthermore, we contribute to the theory of self-overlapping curves by providing the first polynomial-time algorithm to compute a self-overlapping decomposition of any closed curve $\\gamma$ with minimum area."],"url":"http://arxiv.org/abs/2309.02383v1"}
{"created":"2023-09-05 16:35:41","title":"nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources","abstract":"State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community. To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance. With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community's demand for more user-friendly T5 (Encoder-Decoder) implementations. Our contributions, including configurations, codebase, software/hardware insights, and pre-trained models, are available to the public, aiming to strike a balance between research accessibility and resource constraints in NLP.","sentences":["State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community.","To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models.","Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance.","With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community's demand for more user-friendly T5 (Encoder-Decoder) implementations.","Our contributions, including configurations, codebase, software/hardware insights, and pre-trained models, are available to the public, aiming to strike a balance between research accessibility and resource constraints in NLP."],"url":"http://arxiv.org/abs/2309.02373v1"}
{"created":"2023-09-05 16:29:34","title":"Minimal modal logics, constructive modal logics and their relations","abstract":"We present a family of minimal modal logics (namely, modal logics based on minimal propositional logic) corresponding each to a different classical modal logic. The minimal modal logics are defined based on their classical counterparts in two distinct ways: (1) via embedding into fusions of classical modal logics through a natural extension of the G\\\"odel-Johansson translation of minimal logic into modal logic S4; (2) via extension to modal logics of the multi- vs. single-succedent correspondence of sequent calculi for classical and minimal logic. We show that, despite being mutually independent, the two methods turn out to be equivalent for a wide class of modal systems. Moreover, we compare the resulting minimal version of K with the constructive modal logic CK studied in the literature, displaying tight relations among the two systems. Based on these relations, we also define a constructive correspondent for each minimal system, thus obtaining a family of constructive modal logics which includes CK as well as other constructive modal logics studied in the literature.","sentences":["We present a family of minimal modal logics (namely, modal logics based on minimal propositional logic) corresponding each to a different classical modal logic.","The minimal modal logics are defined based on their classical counterparts in two distinct ways: (1) via embedding into fusions of classical modal logics through a natural extension of the G\\\"odel-Johansson translation of minimal logic into modal logic S4; (2) via extension to modal logics of the multi- vs. single-succedent correspondence of sequent calculi for classical and minimal logic.","We show that, despite being mutually independent, the two methods turn out to be equivalent for a wide class of modal systems.","Moreover, we compare the resulting minimal version of K with the constructive modal logic CK studied in the literature, displaying tight relations among the two systems.","Based on these relations, we also define a constructive correspondent for each minimal system, thus obtaining a family of constructive modal logics which includes CK as well as other constructive modal logics studied in the literature."],"url":"http://arxiv.org/abs/2309.02367v1"}
{"created":"2023-09-05 16:11:54","title":"STEP -- Towards Structured Scene-Text Spotting","abstract":"We introduce the structured scene-text spotting task, which requires a scene-text OCR system to spot text in the wild according to a query regular expression. Contrary to generic scene text OCR, structured scene-text spotting seeks to dynamically condition both scene text detection and recognition on user-provided regular expressions. To tackle this task, we propose the Structured TExt sPotter (STEP), a model that exploits the provided text structure to guide the OCR process. STEP is able to deal with regular expressions that contain spaces and it is not bound to detection at the word-level granularity. Our approach enables accurate zero-shot structured text spotting in a wide variety of real-world reading scenarios and is solely trained on publicly available data. To demonstrate the effectiveness of our approach, we introduce a new challenging test dataset that contains several types of out-of-vocabulary structured text, reflecting important reading applications of fields such as prices, dates, serial numbers, license plates etc. We demonstrate that STEP can provide specialised OCR performance on demand in all tested scenarios.","sentences":["We introduce the structured scene-text spotting task, which requires a scene-text OCR system to spot text in the wild according to a query regular expression.","Contrary to generic scene text OCR, structured scene-text spotting seeks to dynamically condition both scene text detection and recognition on user-provided regular expressions.","To tackle this task, we propose the Structured TExt sPotter (STEP), a model that exploits the provided text structure to guide the OCR process.","STEP is able to deal with regular expressions that contain spaces and it is not bound to detection at the word-level granularity.","Our approach enables accurate zero-shot structured text spotting in a wide variety of real-world reading scenarios and is solely trained on publicly available data.","To demonstrate the effectiveness of our approach, we introduce a new challenging test dataset that contains several types of out-of-vocabulary structured text, reflecting important reading applications of fields such as prices, dates, serial numbers, license plates etc.","We demonstrate that STEP can provide specialised OCR performance on demand in all tested scenarios."],"url":"http://arxiv.org/abs/2309.02356v1"}
{"created":"2023-09-05 16:11:37","title":"A Lightweight and Transferable Design for Robust LEGO Manipulation","abstract":"LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100\\% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the proposed solution enables sustainable robotic LEGO prototyping, in which the robot can repeatedly assemble and disassemble different prototypes.","sentences":["LEGO is a well-known platform for prototyping pixelized objects.","However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement.","This paper investigates safe and efficient robotic LEGO manipulation.","In particular, this paper reduces the complexity of the manipulation by hardware-software co-design.","An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks.","In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation.","Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100\\% success rate.","The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability.","In the end, we show that the proposed solution enables sustainable robotic LEGO prototyping, in which the robot can repeatedly assemble and disassemble different prototypes."],"url":"http://arxiv.org/abs/2309.02354v1"}
{"created":"2023-09-05 16:07:00","title":"Exact Inference for Continuous-Time Gaussian Process Dynamics","abstract":"Physical systems can often be described via a continuous-time dynamical system. In practice, the true system is often unknown and has to be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. Thus, we aim for a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. In order to make direct inference tractable, we propose to leverage multistep and Taylor integrators. We demonstrate how to derive flexible inference schemes for these types of integrators. Further, we derive tailored sampling schemes that allow to draw consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model. We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.","sentences":["Physical systems can often be described via a continuous-time dynamical system.","In practice, the true system is often unknown and has to be learned from measurement data.","Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions.","This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved.","Thus, we aim for a GP model of the true continuous-time dynamics.","Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy.","Many higher-order integrators require dynamics evaluations at intermediate time steps making exact GP inference intractable.","In previous work, this problem is often tackled by approximating the GP posterior with variational inference.","However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees.","In order to make direct inference tractable, we propose to leverage multistep and Taylor integrators.","We demonstrate how to derive flexible inference schemes for these types of integrators.","Further, we derive tailored sampling schemes that allow to draw consistent dynamics functions from the learned posterior.","This is crucial to sample consistent predictions from the dynamics model.","We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system."],"url":"http://arxiv.org/abs/2309.02351v1"}
{"created":"2023-09-05 15:57:23","title":"Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm","abstract":"In this paper, we introduce a novel approach for generating texture images of infinite resolutions using Generative Adversarial Networks (GANs) based on a patch-by-patch paradigm. Existing texture synthesis techniques often rely on generating a large-scale texture using a one-forward pass to the generating model, this limits the scalability and flexibility of the generated images. In contrast, the proposed approach trains GANs models on a single texture image to generate relatively small patches that are locally correlated and can be seamlessly concatenated to form a larger image while using a constant GPU memory footprint. Our method learns the local texture structure and is able to generate arbitrary-size textures, while also maintaining coherence and diversity. The proposed method relies on local padding in the generator to ensure consistency between patches and utilizes spatial stochastic modulation to allow for local variations and diversity within the large-scale image. Experimental results demonstrate superior scalability compared to existing approaches while maintaining visual coherence of generated textures.","sentences":["In this paper, we introduce a novel approach for generating texture images of infinite resolutions using Generative Adversarial Networks (GANs) based on a patch-by-patch paradigm.","Existing texture synthesis techniques often rely on generating a large-scale texture using a one-forward pass to the generating model, this limits the scalability and flexibility of the generated images.","In contrast, the proposed approach trains GANs models on a single texture image to generate relatively small patches that are locally correlated and can be seamlessly concatenated to form a larger image while using a constant GPU memory footprint.","Our method learns the local texture structure and is able to generate arbitrary-size textures, while also maintaining coherence and diversity.","The proposed method relies on local padding in the generator to ensure consistency between patches and utilizes spatial stochastic modulation to allow for local variations and diversity within the large-scale image.","Experimental results demonstrate superior scalability compared to existing approaches while maintaining visual coherence of generated textures."],"url":"http://arxiv.org/abs/2309.02340v1"}
{"created":"2023-09-05 15:54:09","title":"PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference","abstract":"Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. We demonstrate the effectiveness of this approach in three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and handwritten digit recognition using the MNIST dataset.","sentences":["Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference.","Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions.","Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs).","Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this.","In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block.","Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead.","We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements.","We demonstrate the effectiveness of this approach in three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and handwritten digit recognition using the MNIST dataset."],"url":"http://arxiv.org/abs/2309.02334v1"}
{"created":"2023-09-05 15:47:40","title":"Neurosymbolic Meta-Reinforcement Lookahead Learning Achieves Safe Self-Driving in Non-Stationary Environments","abstract":"In the area of learning-driven artificial intelligence advancement, the integration of machine learning (ML) into self-driving (SD) technology stands as an impressive engineering feat. Yet, in real-world applications outside the confines of controlled laboratory scenarios, the deployment of self-driving technology assumes a life-critical role, necessitating heightened attention from researchers towards both safety and efficiency. To illustrate, when a self-driving model encounters an unfamiliar environment in real-time execution, the focus must not solely revolve around enhancing its anticipated performance; equal consideration must be given to ensuring its execution or real-time adaptation maintains a requisite level of safety. This study introduces an algorithm for online meta-reinforcement learning, employing lookahead symbolic constraints based on \\emph{Neurosymbolic Meta-Reinforcement Lookahead Learning} (NUMERLA). NUMERLA proposes a lookahead updating mechanism that harmonizes the efficiency of online adaptations with the overarching goal of ensuring long-term safety. Experimental results demonstrate NUMERLA confers the self-driving agent with the capacity for real-time adaptability, leading to safe and self-adaptive driving under non-stationary urban human-vehicle interaction scenarios.","sentences":["In the area of learning-driven artificial intelligence advancement, the integration of machine learning (ML) into self-driving (SD) technology stands as an impressive engineering feat.","Yet, in real-world applications outside the confines of controlled laboratory scenarios, the deployment of self-driving technology assumes a life-critical role, necessitating heightened attention from researchers towards both safety and efficiency.","To illustrate, when a self-driving model encounters an unfamiliar environment in real-time execution, the focus must not solely revolve around enhancing its anticipated performance; equal consideration must be given to ensuring its execution or real-time adaptation maintains a requisite level of safety.","This study introduces an algorithm for online meta-reinforcement learning, employing lookahead symbolic constraints based on \\emph{Neurosymbolic Meta-Reinforcement Lookahead Learning} (NUMERLA).","NUMERLA proposes a lookahead updating mechanism that harmonizes the efficiency of online adaptations with the overarching goal of ensuring long-term safety.","Experimental results demonstrate NUMERLA confers the self-driving agent with the capacity for real-time adaptability, leading to safe and self-adaptive driving under non-stationary urban human-vehicle interaction scenarios."],"url":"http://arxiv.org/abs/2309.02328v1"}
{"created":"2023-09-05 15:44:46","title":"Revisiting File Context for Source Code Summarization","abstract":"Source code summarization is the task of writing natural language descriptions of source code. A typical use case is generating short summaries of subroutines for use in API documentation. The heart of almost all current research into code summarization is the encoder-decoder neural architecture, and the encoder input is almost always a single subroutine or other short code snippet. The problem with this setup is that the information needed to describe the code is often not present in the code itself -- that information often resides in other nearby code. In this paper, we revisit the idea of ``file context'' for code summarization. File context is the idea of encoding select information from other subroutines in the same file. We propose a novel modification of the Transformer architecture that is purpose-built to encode file context and demonstrate its improvement over several baselines. We find that file context helps on a subset of challenging examples where traditional approaches struggle.","sentences":["Source code summarization is the task of writing natural language descriptions of source code.","A typical use case is generating short summaries of subroutines for use in API documentation.","The heart of almost all current research into code summarization is the encoder-decoder neural architecture, and the encoder input is almost always a single subroutine or other short code snippet.","The problem with this setup is that the information needed to describe the code is often not present in the code itself -- that information often resides in other nearby code.","In this paper, we revisit the idea of ``file context'' for code summarization.","File context is the idea of encoding select information from other subroutines in the same file.","We propose a novel modification of the Transformer architecture that is purpose-built to encode file context and demonstrate its improvement over several baselines.","We find that file context helps on a subset of challenging examples where traditional approaches struggle."],"url":"http://arxiv.org/abs/2309.02326v1"}
{"created":"2023-09-05 15:41:26","title":"Fairness of Exposure in Dynamic Recommendation","abstract":"Exposure bias is a well-known issue in recommender systems where the exposure is not fairly distributed among items in the recommendation results. This is especially problematic when bias is amplified over time as a few items (e.g., popular ones) are repeatedly over-represented in recommendation lists and users' interactions with those items will amplify bias towards those items over time resulting in a feedback loop. This issue has been extensively studied in the literature in static recommendation environment where a single round of recommendation result is processed to improve the exposure fairness. However, less work has been done on addressing exposure bias in a dynamic recommendation setting where the system is operating over time, the recommendation model and the input data are dynamically updated with ongoing user feedback on recommended items at each round. In this paper, we study exposure bias in a dynamic recommendation setting. Our goal is to show that existing bias mitigation methods that are designed to operate in a static recommendation setting are unable to satisfy fairness of exposure for items in long run. In particular, we empirically study one of these methods and show that repeatedly applying this method fails to fairly distribute exposure among items in long run. To address this limitation, we show how this method can be adapted to effectively operate in a dynamic recommendation setting and achieve exposure fairness for items in long run. Experiments on a real-world dataset confirm that our solution is superior in achieving long-term exposure fairness for the items while maintaining the recommendation accuracy.","sentences":["Exposure bias is a well-known issue in recommender systems where the exposure is not fairly distributed among items in the recommendation results.","This is especially problematic when bias is amplified over time as a few items (e.g., popular ones) are repeatedly over-represented in recommendation lists and users' interactions with those items will amplify bias towards those items over time resulting in a feedback loop.","This issue has been extensively studied in the literature in static recommendation environment where a single round of recommendation result is processed to improve the exposure fairness.","However, less work has been done on addressing exposure bias in a dynamic recommendation setting where the system is operating over time, the recommendation model and the input data are dynamically updated with ongoing user feedback on recommended items at each round.","In this paper, we study exposure bias in a dynamic recommendation setting.","Our goal is to show that existing bias mitigation methods that are designed to operate in a static recommendation setting are unable to satisfy fairness of exposure for items in long run.","In particular, we empirically study one of these methods and show that repeatedly applying this method fails to fairly distribute exposure among items in long run.","To address this limitation, we show how this method can be adapted to effectively operate in a dynamic recommendation setting and achieve exposure fairness for items in long run.","Experiments on a real-world dataset confirm that our solution is superior in achieving long-term exposure fairness for the items while maintaining the recommendation accuracy."],"url":"http://arxiv.org/abs/2309.02322v1"}
{"created":"2023-09-05 15:34:37","title":"TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction","abstract":"Four-dimensional Digital Subtraction Angiography (4D DSA) plays a critical role in the diagnosis of many medical diseases, such as Arteriovenous Malformations (AVM) and Arteriovenous Fistulas (AVF). Despite its significant application value, the reconstruction of 4D DSA demands numerous views to effectively model the intricate vessels and radiocontrast flow, thereby implying a significant radiation dose. To address this high radiation issue, we propose a Time-aware Attenuation Voxel (TiAVox) approach for sparse-view 4D DSA reconstruction, which paves the way for high-quality 4D imaging. Additionally, 2D and 3D DSA imaging results can be generated from the reconstructed 4D DSA images. TiAVox introduces 4D attenuation voxel grids, which reflect attenuation properties from both spatial and temporal dimensions. It is optimized by minimizing discrepancies between the rendered images and sparse 2D DSA images. Without any neural network involved, TiAVox enjoys specific physical interpretability. The parameters of each learnable voxel represent the attenuation coefficients. We validated the TiAVox approach on both clinical and simulated datasets, achieving a 31.23 Peak Signal-to-Noise Ratio (PSNR) for novel view synthesis using only 30 views on the clinically sourced dataset, whereas traditional Feldkamp-Davis-Kress methods required 133 views. Similarly, with merely 10 views from the synthetic dataset, TiAVox yielded a PSNR of 34.32 for novel view synthesis and 41.40 for 3D reconstruction. We also executed ablation studies to corroborate the essential components of TiAVox. The code will be publically available.","sentences":["Four-dimensional Digital Subtraction Angiography (4D DSA) plays a critical role in the diagnosis of many medical diseases, such as Arteriovenous Malformations (AVM) and Arteriovenous Fistulas (AVF).","Despite its significant application value, the reconstruction of 4D DSA demands numerous views to effectively model the intricate vessels and radiocontrast flow, thereby implying a significant radiation dose.","To address this high radiation issue, we propose a Time-aware Attenuation Voxel (TiAVox) approach for sparse-view 4D DSA reconstruction, which paves the way for high-quality 4D imaging.","Additionally, 2D and 3D DSA imaging results can be generated from the reconstructed 4D DSA images.","TiAVox introduces 4D attenuation voxel grids, which reflect attenuation properties from both spatial and temporal dimensions.","It is optimized by minimizing discrepancies between the rendered images and sparse 2D DSA images.","Without any neural network involved, TiAVox enjoys specific physical interpretability.","The parameters of each learnable voxel represent the attenuation coefficients.","We validated the TiAVox approach on both clinical and simulated datasets, achieving a 31.23 Peak Signal-to-Noise Ratio (PSNR) for novel view synthesis using only 30 views on the clinically sourced dataset, whereas traditional Feldkamp-Davis-Kress methods required 133 views.","Similarly, with merely 10 views from the synthetic dataset, TiAVox yielded a PSNR of 34.32 for novel view synthesis and 41.40 for 3D reconstruction.","We also executed ablation studies to corroborate the essential components of TiAVox.","The code will be publically available."],"url":"http://arxiv.org/abs/2309.02318v1"}
{"created":"2023-09-05 15:34:22","title":"A study on the impact of pre-trained model on Just-In-Time defect prediction","abstract":"Previous researchers conducting Just-In-Time (JIT) defect prediction tasks have primarily focused on the performance of individual pre-trained models, without exploring the relationship between different pre-trained models as backbones. In this study, we build six models: RoBERTaJIT, CodeBERTJIT, BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trained model as its backbone. We systematically explore the differences and connections between these models. Specifically, we investigate the performance of the models when using Commit code and Commit message as inputs, as well as the relationship between training efficiency and model distribution among these six models. Additionally, we conduct an ablation experiment to explore the sensitivity of each model to inputs. Furthermore, we investigate how the models perform in zero-shot and few-shot scenarios. Our findings indicate that each model based on different backbones shows improvements, and when the backbone's pre-training model is similar, the training resources that need to be consumed are much more closer. We also observe that Commit code plays a significant role in defect detection, and different pre-trained models demonstrate better defect detection ability with a balanced dataset under few-shot scenarios. These results provide new insights for optimizing JIT defect prediction tasks using pre-trained models and highlight the factors that require more attention when constructing such models. Additionally, CodeGPTJIT and GPT2JIT achieved better performance than DeepJIT and CC2Vec on the two datasets respectively under 2000 training samples. These findings emphasize the effectiveness of transformer-based pre-trained models in JIT defect prediction tasks, especially in scenarios with limited training data.","sentences":["Previous researchers conducting Just-In-Time (JIT) defect prediction tasks have primarily focused on the performance of individual pre-trained models, without exploring the relationship between different pre-trained models as backbones.","In this study, we build six models: RoBERTaJIT, CodeBERTJIT, BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trained model as its backbone.","We systematically explore the differences and connections between these models.","Specifically, we investigate the performance of the models when using Commit code and Commit message as inputs, as well as the relationship between training efficiency and model distribution among these six models.","Additionally, we conduct an ablation experiment to explore the sensitivity of each model to inputs.","Furthermore, we investigate how the models perform in zero-shot and few-shot scenarios.","Our findings indicate that each model based on different backbones shows improvements, and when the backbone's pre-training model is similar, the training resources that need to be consumed are much more closer.","We also observe that Commit code plays a significant role in defect detection, and different pre-trained models demonstrate better defect detection ability with a balanced dataset under few-shot scenarios.","These results provide new insights for optimizing JIT defect prediction tasks using pre-trained models and highlight the factors that require more attention when constructing such models.","Additionally, CodeGPTJIT and GPT2JIT achieved better performance than DeepJIT and CC2Vec on the two datasets respectively under 2000 training samples.","These findings emphasize the effectiveness of transformer-based pre-trained models in JIT defect prediction tasks, especially in scenarios with limited training data."],"url":"http://arxiv.org/abs/2309.02317v1"}
{"created":"2023-09-05 15:27:22","title":"Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization","abstract":"Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data. This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation. Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. We experiment with two attention-based regularization techniques on a benchmark English dataset. Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.","sentences":["Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data.","This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language.","This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation.","Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives.","We experiment with two attention-based regularization techniques on a benchmark English dataset.","Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data.","This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce."],"url":"http://arxiv.org/abs/2309.02311v1"}
{"created":"2023-09-05 15:13:48","title":"Graph Self-Contrast Representation Learning","abstract":"Graph contrastive learning (GCL) has recently emerged as a promising approach for graph representation learning. Some existing methods adopt the 1-vs-K scheme to construct one positive and K negative samples for each graph, but it is difficult to set K. For those methods that do not use negative samples, it is often necessary to add additional strategies to avoid model collapse, which could only alleviate the problem to some extent. All these drawbacks will undoubtedly have an adverse impact on the generalizability and efficiency of the model. In this paper, to address these issues, we propose a novel graph self-contrast framework GraphSC, which only uses one positive and one negative sample, and chooses triplet loss as the objective. Specifically, self-contrast has two implications. First, GraphSC generates both positive and negative views of a graph sample from the graph itself via graph augmentation functions of various intensities, and use them for self-contrast. Second, GraphSC uses Hilbert-Schmidt Independence Criterion (HSIC) to factorize the representations into multiple factors and proposes a masked self-contrast mechanism to better separate positive and negative samples. Further, Since the triplet loss only optimizes the relative distance between the anchor and its positive/negative samples, it is difficult to ensure the absolute distance between the anchor and positive sample. Therefore, we explicitly reduced the absolute distance between the anchor and positive sample to accelerate convergence. Finally, we conduct extensive experiments to evaluate the performance of GraphSC against 19 other state-of-the-art methods in both unsupervised and transfer learning settings.","sentences":["Graph contrastive learning (GCL) has recently emerged as a promising approach for graph representation learning.","Some existing methods adopt the 1-vs-K scheme to construct one positive and K negative samples for each graph, but it is difficult to set K. For those methods that do not use negative samples, it is often necessary to add additional strategies to avoid model collapse, which could only alleviate the problem to some extent.","All these drawbacks will undoubtedly have an adverse impact on the generalizability and efficiency of the model.","In this paper, to address these issues, we propose a novel graph self-contrast framework GraphSC, which only uses one positive and one negative sample, and chooses triplet loss as the objective.","Specifically, self-contrast has two implications.","First, GraphSC generates both positive and negative views of a graph sample from the graph itself via graph augmentation functions of various intensities, and use them for self-contrast.","Second, GraphSC uses Hilbert-Schmidt Independence Criterion (HSIC) to factorize the representations into multiple factors and proposes a masked self-contrast mechanism to better separate positive and negative samples.","Further, Since the triplet loss only optimizes the relative distance between the anchor and its positive/negative samples, it is difficult to ensure the absolute distance between the anchor and positive sample.","Therefore, we explicitly reduced the absolute distance between the anchor and positive sample to accelerate convergence.","Finally, we conduct extensive experiments to evaluate the performance of GraphSC against 19 other state-of-the-art methods in both unsupervised and transfer learning settings."],"url":"http://arxiv.org/abs/2309.02304v1"}
{"created":"2023-09-05 15:06:37","title":"CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning","abstract":"Nowadays, the research on Large Vision-Language Models (LVLMs) has been significantly promoted thanks to the success of Large Language Models (LLM). Nevertheless, these Vision-Language Models (VLMs) are suffering from the drawback of hallucination -- due to insufficient understanding of vision and language modalities, VLMs may generate incorrect perception information when doing downstream applications, for example, captioning a non-existent entity. To address the hallucination phenomenon, on the one hand, we introduce a Contrastive Instruction Evaluation Method (CIEM), which is an automatic pipeline that leverages an annotated image-text dataset coupled with an LLM to generate factual/contrastive question-answer pairs for the evaluation of the hallucination of VLMs. On the other hand, based on CIEM, we further propose a new instruction tuning method called CIT (the abbreviation of Contrastive Instruction Tuning) to alleviate the hallucination of VLMs by automatically producing high-quality factual/contrastive question-answer pairs and corresponding justifications for model tuning. Through extensive experiments on CIEM and CIT, we pinpoint the hallucination issues commonly present in existing VLMs, the disability of the current instruction-tuning dataset to handle the hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM and public datasets.","sentences":["Nowadays, the research on Large Vision-Language Models (LVLMs) has been significantly promoted thanks to the success of Large Language Models (LLM).","Nevertheless, these Vision-Language Models (VLMs) are suffering from the drawback of hallucination -- due to insufficient understanding of vision and language modalities, VLMs may generate incorrect perception information when doing downstream applications, for example, captioning a non-existent entity.","To address the hallucination phenomenon, on the one hand, we introduce a Contrastive Instruction Evaluation Method (CIEM), which is an automatic pipeline that leverages an annotated image-text dataset coupled with an LLM to generate factual/contrastive question-answer pairs for the evaluation of the hallucination of VLMs.","On the other hand, based on CIEM, we further propose a new instruction tuning method called CIT (the abbreviation of Contrastive Instruction Tuning) to alleviate the hallucination of VLMs by automatically producing high-quality factual/contrastive question-answer pairs and corresponding justifications for model tuning.","Through extensive experiments on CIEM and CIT, we pinpoint the hallucination issues commonly present in existing VLMs, the disability of the current instruction-tuning dataset to handle the hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM and public datasets."],"url":"http://arxiv.org/abs/2309.02301v1"}
{"created":"2023-09-05 14:59:01","title":"Smoothening block rewards: How much should miners pay for mining pools?","abstract":"The rewards a blockchain miner earns vary with time. Most of the time is spent mining without receiving any rewards, and only occasionally the miner wins a block and earns a reward. Mining pools smoothen the stochastic flow of rewards, and in the ideal case, provide a steady flow of rewards over time. Smooth block rewards allow miners to choose an optimal mining power growth strategy that will result in a higher reward yield for a given investment. We quantify the economic advantage for a given miner of having smooth rewards, and use this to define a maximum percentage of rewards that a miner should be willing to pay for the mining pool services.","sentences":["The rewards a blockchain miner earns vary with time.","Most of the time is spent mining without receiving any rewards, and only occasionally the miner wins a block and earns a reward.","Mining pools smoothen the stochastic flow of rewards, and in the ideal case, provide a steady flow of rewards over time.","Smooth block rewards allow miners to choose an optimal mining power growth strategy that will result in a higher reward yield for a given investment.","We quantify the economic advantage for a given miner of having smooth rewards, and use this to define a maximum percentage of rewards that a miner should be willing to pay for the mining pool services."],"url":"http://arxiv.org/abs/2309.02297v1"}
{"created":"2023-09-05 14:52:38","title":"ATM: Action Temporality Modeling for Video Question Answering","abstract":"Despite significant progress in video question answering (VideoQA), existing methods fall short of questions that require causal/temporal reasoning across frames. This can be attributed to imprecise motion representations. We introduce Action Temporality Modeling (ATM) for temporality reasoning via three-fold uniqueness: (1) rethinking the optical flow and realizing that optical flow is effective in capturing the long horizon temporality reasoning; (2) training the visual-text embedding by contrastive learning in an action-centric manner, leading to better action representations in both vision and text modalities; and (3) preventing the model from answering the question given the shuffled video in the fine-tuning stage, to avoid spurious correlation between appearance and motion and hence ensure faithful temporality reasoning. In the experiments, we show that ATM outperforms previous approaches in terms of the accuracy on multiple VideoQAs and exhibits better true temporality reasoning ability.","sentences":["Despite significant progress in video question answering (VideoQA), existing methods fall short of questions that require causal/temporal reasoning across frames.","This can be attributed to imprecise motion representations.","We introduce Action Temporality Modeling (ATM) for temporality reasoning via three-fold uniqueness: (1) rethinking the optical flow and realizing that optical flow is effective in capturing the long horizon temporality reasoning; (2) training the visual-text embedding by contrastive learning in an action-centric manner, leading to better action representations in both vision and text modalities; and (3) preventing the model from answering the question given the shuffled video in the fine-tuning stage, to avoid spurious correlation between appearance and motion and hence ensure faithful temporality reasoning.","In the experiments, we show that ATM outperforms previous approaches in terms of the accuracy on multiple VideoQAs and exhibits better true temporality reasoning ability."],"url":"http://arxiv.org/abs/2309.02290v1"}
{"created":"2023-09-05 14:46:06","title":"Optimal Observation-Intervention Trade-Off in Optimisation Problems with Causal Structure","abstract":"We consider the problem of optimising an expensive-to-evaluate grey-box objective function, within a finite budget, where known side-information exists in the form of the causal structure between the design variables. Standard black-box optimisation ignores the causal structure, often making it inefficient and expensive. The few existing methods that consider the causal structure are myopic and do not fully accommodate the observation-intervention trade-off that emerges when estimating causal effects. In this paper, we show that the observation-intervention trade-off can be formulated as a non-myopic optimal stopping problem which permits an efficient solution. We give theoretical results detailing the structure of the optimal stopping times and demonstrate the generality of our approach by showing that it can be integrated with existing causal Bayesian optimisation algorithms. Experimental results show that our formulation can enhance existing algorithms on real and synthetic benchmarks.","sentences":["We consider the problem of optimising an expensive-to-evaluate grey-box objective function, within a finite budget, where known side-information exists in the form of the causal structure between the design variables.","Standard black-box optimisation ignores the causal structure, often making it inefficient and expensive.","The few existing methods that consider the causal structure are myopic and do not fully accommodate the observation-intervention trade-off that emerges when estimating causal effects.","In this paper, we show that the observation-intervention trade-off can be formulated as a non-myopic optimal stopping problem which permits an efficient solution.","We give theoretical results detailing the structure of the optimal stopping times and demonstrate the generality of our approach by showing that it can be integrated with existing causal Bayesian optimisation algorithms.","Experimental results show that our formulation can enhance existing algorithms on real and synthetic benchmarks."],"url":"http://arxiv.org/abs/2309.02287v1"}
{"created":"2023-09-05 14:45:54","title":"Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes","abstract":"Current scene graph datasets suffer from strong long-tail distributions of their predicate classes. Due to a very low number of some predicate classes in the test sets, no reliable metrics can be retrieved for the rarest classes. We construct a new panoptic scene graph dataset and a set of metrics that are designed as a benchmark for the predictive performance especially on rare predicate classes. To construct the new dataset, we propose a model-assisted annotation pipeline that efficiently finds rare predicate classes that are hidden in a large set of images like needles in a haystack.   Contrary to prior scene graph datasets, Haystack contains explicit negative annotations, i.e. annotations that a given relation does not have a certain predicate class. Negative annotations are helpful especially in the field of scene graph generation and open up a whole new set of possibilities to improve current scene graph generation models.   Haystack is 100% compatible with existing panoptic scene graph datasets and can easily be integrated with existing evaluation pipelines. Our dataset and code can be found here: https://lorjul.github.io/haystack/. It includes annotation files and simple to use scripts and utilities, to help with integrating our dataset in existing work.","sentences":["Current scene graph datasets suffer from strong long-tail distributions of their predicate classes.","Due to a very low number of some predicate classes in the test sets, no reliable metrics can be retrieved for the rarest classes.","We construct a new panoptic scene graph dataset and a set of metrics that are designed as a benchmark for the predictive performance especially on rare predicate classes.","To construct the new dataset, we propose a model-assisted annotation pipeline that efficiently finds rare predicate classes that are hidden in a large set of images like needles in a haystack.   ","Contrary to prior scene graph datasets, Haystack contains explicit negative annotations, i.e. annotations that a given relation does not have a certain predicate class.","Negative annotations are helpful especially in the field of scene graph generation and open up a whole new set of possibilities to improve current scene graph generation models.   ","Haystack is 100% compatible with existing panoptic scene graph datasets and can easily be integrated with existing evaluation pipelines.","Our dataset and code can be found here: https://lorjul.github.io/haystack/. It includes annotation files and simple to use scripts and utilities, to help with integrating our dataset in existing work."],"url":"http://arxiv.org/abs/2309.02286v1"}
{"created":"2023-09-05 14:43:10","title":"s-ID: Causal Effect Identification in a Sub-Population","abstract":"Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.","sentences":["Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population.","However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods.","We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population).","Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem.","To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population.","Given these conditions, we present a sound and complete algorithm for the s-ID problem."],"url":"http://arxiv.org/abs/2309.02281v1"}
{"created":"2023-09-05 14:37:59","title":"Computing Hive Plots: A Combinatorial Framework","abstract":"Hive plots are a graph visualization style placing vertices on a set of radial axes emanating from a common center and drawing edges as smooth curves connecting their respective endpoints. In previous work on hive plots, assignment to an axis and vertex positions on each axis were determined based on selected vertex attributes and the order of axes was prespecified. Here, we present a new framework focusing on combinatorial aspects of these drawings to extend the original hive plot idea and optimize visual properties such as the total edge length and the number of edge crossings in the resulting hive plots. Our framework comprises three steps: (1) partition the vertices into multiple groups, each corresponding to an axis of the hive plot; (2) optimize the cyclic axis order to bring more strongly connected groups near each other; (3) optimize the vertex ordering on each axis to minimize edge crossings. Each of the three steps is related to a well-studied, but NP-complete computational problem. We combine and adapt suitable algorithmic approaches, implement them as an instantiation of our framework and show in a case study how it can be applied in a practical setting. Furthermore, we conduct computational experiments to gain further insights regarding algorithmic choices of the framework. The code of the implementation and a prototype web application can be found on OSF.","sentences":["Hive plots are a graph visualization style placing vertices on a set of radial axes emanating from a common center and drawing edges as smooth curves connecting their respective endpoints.","In previous work on hive plots, assignment to an axis and vertex positions on each axis were determined based on selected vertex attributes and the order of axes was prespecified.","Here, we present a new framework focusing on combinatorial aspects of these drawings to extend the original hive plot idea and optimize visual properties such as the total edge length and the number of edge crossings in the resulting hive plots.","Our framework comprises three steps: (1) partition the vertices into multiple groups, each corresponding to an axis of the hive plot; (2) optimize the cyclic axis order to bring more strongly connected groups near each other; (3) optimize the vertex ordering on each axis to minimize edge crossings.","Each of the three steps is related to a well-studied, but NP-complete computational problem.","We combine and adapt suitable algorithmic approaches, implement them as an instantiation of our framework and show in a case study how it can be applied in a practical setting.","Furthermore, we conduct computational experiments to gain further insights regarding algorithmic choices of the framework.","The code of the implementation and a prototype web application can be found on OSF."],"url":"http://arxiv.org/abs/2309.02273v1"}
{"created":"2023-09-05 14:37:31","title":"Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette","abstract":"This paper introduces a novel graph-based filter method for automatic feature selection (abbreviated as GB-AFS) for multi-class classification tasks. The method determines the minimum combination of features required to sustain prediction performance while maintaining complementary discriminating abilities between different classes. It does not require any user-defined parameters such as the number of features to select. The methodology employs the Jeffries-Matusita (JM) distance in conjunction with t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space reflecting how effectively each feature can differentiate between each pair of classes. The minimum number of features is selected using our newly developed Mean Simplified Silhouette (abbreviated as MSS) index, designed to evaluate the clustering results for the feature selection task. Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. Moreover, the proposed algorithm maintained the accuracy achieved when utilizing all features, while using only $7\\%$ to $30\\%$ of the features. Consequently, this resulted in a reduction of the time needed for classifications, from $15\\%$ to $70\\%$.","sentences":["This paper introduces a novel graph-based filter method for automatic feature selection (abbreviated as GB-AFS) for multi-class classification tasks.","The method determines the minimum combination of features required to sustain prediction performance while maintaining complementary discriminating abilities between different classes.","It does not require any user-defined parameters such as the number of features to select.","The methodology employs the Jeffries-Matusita (JM) distance in conjunction with t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space reflecting how effectively each feature can differentiate between each pair of classes.","The minimum number of features is selected using our newly developed Mean Simplified Silhouette (abbreviated as MSS) index, designed to evaluate the clustering results for the feature selection task.","Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches.","Moreover, the proposed algorithm maintained the accuracy achieved when utilizing all features, while using only $7\\%$ to $30\\%$ of the features.","Consequently, this resulted in a reduction of the time needed for classifications, from $15\\%$ to $70\\%$."],"url":"http://arxiv.org/abs/2309.02272v1"}
{"created":"2023-09-05 14:33:56","title":"SAM-Deblur: Let Segment Anything Boost Image Deblurring","abstract":"Image deblurring is a critical task in the field of image restoration, aiming to eliminate blurring artifacts. However, the challenge of addressing non-uniform blurring leads to an ill-posed problem, which limits the generalization performance of existing deblurring models. To solve the problem, we propose a framework SAM-Deblur, integrating prior knowledge from the Segment Anything Model (SAM) into the deblurring task for the first time. In particular, SAM-Deblur is divided into three stages. First, We preprocess the blurred images, obtain image masks via SAM, and propose a mask dropout method for training to enhance model robustness. Then, to fully leverage the structural priors generated by SAM, we propose a Mask Average Pooling (MAP) unit specifically designed to average SAM-generated segmented areas, serving as a plug-and-play component which can be seamlessly integrated into existing deblurring networks. Finally, we feed the fused features generated by the MAP Unit into the deblurring model to obtain a sharp image. Experimental results on the RealBlurJ, ReloBlur, and REDS datasets reveal that incorporating our methods improves NAFNet's PSNR by 0.05, 0.96, and 7.03, respectively. Code will be available at \\href{https://github.com/HPLQAQ/SAM-Deblur}{SAM-Deblur}.","sentences":["Image deblurring is a critical task in the field of image restoration, aiming to eliminate blurring artifacts.","However, the challenge of addressing non-uniform blurring leads to an ill-posed problem, which limits the generalization performance of existing deblurring models.","To solve the problem, we propose a framework SAM-Deblur, integrating prior knowledge from the Segment Anything Model (SAM) into the deblurring task for the first time.","In particular, SAM-Deblur is divided into three stages.","First, We preprocess the blurred images, obtain image masks via SAM, and propose a mask dropout method for training to enhance model robustness.","Then, to fully leverage the structural priors generated by SAM, we propose a Mask Average Pooling (MAP) unit specifically designed to average SAM-generated segmented areas, serving as a plug-and-play component which can be seamlessly integrated into existing deblurring networks.","Finally, we feed the fused features generated by the MAP Unit into the deblurring model to obtain a sharp image.","Experimental results on the RealBlurJ, ReloBlur, and REDS datasets reveal that incorporating our methods improves NAFNet's PSNR by 0.05, 0.96, and 7.03, respectively.","Code will be available at \\href{https://github.com/HPLQAQ/SAM-Deblur}{SAM-Deblur}."],"url":"http://arxiv.org/abs/2309.02270v1"}
{"created":"2023-09-05 14:28:54","title":"Online hitting set of $d$-dimensional fat objects","abstract":"We consider an online version of the geometric minimum hitting set problem that can be described as a game between an adversary and an algorithm. For some integers $d$ and $N$, let $P$ be the set of points in $(0, N)^d$ with integral coordinates, and let $\\mathcal{O}$ be a family of subsets of $P$, called objects. Both $P$ and $\\mathcal{O}$ are known in advance by the algorithm and by the adversary. Then, the adversary gives some objects one by one, and the algorithm has to maintain a valid hitting set for these objects using points from $P$, with an immediate and irrevocable decision. We measure the performance of the algorithm by its competitive ratio, that is the ratio between the number of points used by the algorithm and the offline minimum hitting set for the sub-sequence of objects chosen by the adversary.   We present a simple deterministic online algorithm with competitive ratio $((4\\alpha+1)^{2d}\\log N)$ when objects correspond to a family of $\\alpha$-fat objects. Informally, $\\alpha$-fatness measures how cube-like is an object. We show that no algorithm can achieve a better ratio when $\\alpha$ and $d$ are fixed constants. In particular, our algorithm works for two-dimensional disks and $d$-cubes which answers two open questions from related previous papers in the special case where the set of points corresponds to all the points of integral coordinates with a fixed $d$-cube.","sentences":["We consider an online version of the geometric minimum hitting set problem that can be described as a game between an adversary and an algorithm.","For some integers $d$ and $N$, let $P$ be the set of points in $(0, N)^d$ with integral coordinates, and let $\\mathcal{O}$ be a family of subsets of $P$, called objects.","Both $P$ and $\\mathcal{O}$ are known in advance by the algorithm and by the adversary.","Then, the adversary gives some objects one by one, and the algorithm has to maintain a valid hitting set for these objects using points from $P$, with an immediate and irrevocable decision.","We measure the performance of the algorithm by its competitive ratio, that is the ratio between the number of points used by the algorithm and the offline minimum hitting set for the sub-sequence of objects chosen by the adversary.   ","We present a simple deterministic online algorithm with competitive ratio $((4\\alpha+1)^{2d}\\log N)$ when objects correspond to a family of $\\alpha$-fat objects.","Informally, $\\alpha$-fatness measures how cube-like is an object.","We show that no algorithm can achieve a better ratio when $\\alpha$ and $d$ are fixed constants.","In particular, our algorithm works for two-dimensional disks and $d$-cubes which answers two open questions from related previous papers in the special case where the set of points corresponds to all the points of integral coordinates with a fixed $d$-cube."],"url":"http://arxiv.org/abs/2309.02269v1"}
{"created":"2023-09-05 14:19:40","title":"Fairness Optimization of RSMA for Uplink Communication based on Intelligent Reflecting Surface","abstract":"In this paper, we propose a rate-splitting multiple access (RSMA) scheme for uplink wireless communication systems with intelligent reflecting surface (IRS) aided. In the considered model, IRS is adopted to overcome power attenuation caused by path loss. We construct a max-min fairness optimization problem to obtain the resource allocation, including the receive beamforming at the base station (BS) and phase-shift beamforming at IRS. We also introduce a successive group decoding (SGD) algorithm at the receiver, which trades off the fairness and complexity of decoding. In the simulation, the results show that the proposed scheme has superiority in improving the fairness of uplink communication.","sentences":["In this paper, we propose a rate-splitting multiple access (RSMA) scheme for uplink wireless communication systems with intelligent reflecting surface (IRS) aided.","In the considered model, IRS is adopted to overcome power attenuation caused by path loss.","We construct a max-min fairness optimization problem to obtain the resource allocation, including the receive beamforming at the base station (BS) and phase-shift beamforming at IRS.","We also introduce a successive group decoding (SGD) algorithm at the receiver, which trades off the fairness and complexity of decoding.","In the simulation, the results show that the proposed scheme has superiority in improving the fairness of uplink communication."],"url":"http://arxiv.org/abs/2309.02264v1"}
{"created":"2023-09-05 14:12:14","title":"Design of a New CIM-DCSK-Based Ambient Backscatter Communication System","abstract":"To improve the data rate in differential chaos shift keying (DCSK) based ambient backscatter communication (AmBC) system, we propose a new AmBC system based on code index modulation (CIM), referred to as CIM-DCSK-AmBC system. In the proposed system, the CIM-DCSK signal transmitted in the direct link is used as the radio frequency source of the backscatter link. The signal format in the backscatter link is designed to increase the data rate as well as eliminate the interference of the direct link signal. As such, the direct link signal and the backscatter link signal can be received and demodulated simultaneously. Moreover, we derive and validate the theoretical bit error rate (BER) expressions of the CIM-DCSK-AmBC system over multipath Rayleigh fading channels. Regarding the short reference DCSK-based AmBC (SR-DCSK-AmBC) system as a benchmark system, numerical results reveal that the CIM-DCSK-AmBC system can achieve better BER performance in the direct link and higher throughput in the backscatter link than the benchmark system.","sentences":["To improve the data rate in differential chaos shift keying (DCSK) based ambient backscatter communication (AmBC) system, we propose a new AmBC system based on code index modulation (CIM), referred to as CIM-DCSK-AmBC system.","In the proposed system, the CIM-DCSK signal transmitted in the direct link is used as the radio frequency source of the backscatter link.","The signal format in the backscatter link is designed to increase the data rate as well as eliminate the interference of the direct link signal.","As such, the direct link signal and the backscatter link signal can be received and demodulated simultaneously.","Moreover, we derive and validate the theoretical bit error rate (BER) expressions of the CIM-DCSK-AmBC system over multipath Rayleigh fading channels.","Regarding the short reference DCSK-based AmBC (SR-DCSK-AmBC) system as a benchmark system, numerical results reveal that the CIM-DCSK-AmBC system can achieve better BER performance in the direct link and higher throughput in the backscatter link than the benchmark system."],"url":"http://arxiv.org/abs/2309.02259v1"}
{"created":"2023-09-05 14:11:29","title":"On 3-Coloring Circle Graphs","abstract":"Given a graph $G$ with a fixed vertex order $\\prec$, one obtains a circle graph $H$ whose vertices are the edges of $G$ and where two such edges are adjacent if and only if their endpoints are pairwise distinct and alternate in $\\prec$. Therefore, the problem of determining whether $G$ has a $k$-page book embedding with spine order $\\prec$ is equivalent to deciding whether $H$ can be colored with $k$ colors. Finding a $k$-coloring for a circle graph is known to be NP-complete for $k \\geq 4$ and trivial for $k \\leq 2$. For $k = 3$, Unger (1992) claims an efficient algorithm that finds a 3-coloring in $O(n \\log n)$ time, if it exists. Given a circle graph $H$, Unger's algorithm (1) constructs a 3-\\textsc{Sat} formula $\\Phi$ that is satisfiable if and only if $H$ admits a 3-coloring and (2) solves $\\Phi$ by a backtracking strategy that relies on the structure imposed by the circle graph. However, the extended abstract misses several details and Unger refers to his PhD thesis (in German) for details. In this paper we argue that Unger's algorithm for 3-coloring circle graphs is not correct and that 3-coloring circle graphs should be considered as an open problem. We show that step (1) of Unger's algorithm is incorrect by exhibiting a circle graph whose formula $\\Phi$ is satisfiable but that is not 3-colorable. We further show that Unger's backtracking strategy for solving $\\Phi$ in step (2) may produce incorrect results and give empirical evidence that it exhibits a runtime behaviour that is not consistent with the claimed running time.","sentences":["Given a graph $G$ with a fixed vertex order $\\prec$, one obtains a circle graph $H$ whose vertices are the edges of $G$ and where two such edges are adjacent if and only if their endpoints are pairwise distinct and alternate in $\\prec$. Therefore, the problem of determining whether $G$ has a $k$-page book embedding with spine order $\\prec$ is equivalent to deciding whether $H$ can be colored with $k$ colors.","Finding a $k$-coloring for a circle graph is known to be NP-complete for $k \\geq 4$ and trivial for $k \\leq 2$.","For $k = 3$, Unger (1992) claims an efficient algorithm that finds a 3-coloring in $O(n \\log n)$ time, if it exists.","Given a circle graph $H$, Unger's algorithm (1) constructs a 3-\\textsc{Sat} formula $\\Phi$ that is satisfiable if and only if $H$ admits a 3-coloring and (2) solves $\\Phi$ by a backtracking strategy that relies on the structure imposed by the circle graph.","However, the extended abstract misses several details and Unger refers to his PhD thesis (in German) for details.","In this paper we argue that Unger's algorithm for 3-coloring circle graphs is not correct and that 3-coloring circle graphs should be considered as an open problem.","We show that step (1) of Unger's algorithm is incorrect by exhibiting a circle graph whose formula $\\Phi$ is satisfiable but that is not 3-colorable.","We further show that Unger's backtracking strategy for solving $\\Phi$ in step (2) may produce incorrect results and give empirical evidence that it exhibits a runtime behaviour that is not consistent with the claimed running time."],"url":"http://arxiv.org/abs/2309.02258v1"}
{"created":"2023-09-05 14:11:18","title":"Designing Interfaces for Human-Computer Communication: An On-Going Collection of Considerations","abstract":"While we do not always use words, communicating what we want to an AI is a conversation -- with ourselves as well as with it, a recurring loop with optional steps depending on the complexity of the situation and our request. Any given conversation of this type may include: (a) the human forming an intent, (b) the human expressing that intent as a command or utterance, (c) the AI performing one or more rounds of inference on that command to resolve ambiguities and/or requesting clarifications from the human, (d) the AI showing the inferred meaning of the command and/or its execution on current and future situations or data, (e) the human hopefully correctly recognizing whether the AI's interpretation actually aligns with their intent. In the process, they may (f) update their model of the AI's capabilities and characteristics, (g) update their model of the situations in which the AI is executing its interpretation of their intent, (h) confirm or refine their intent, and (i) revise their expression of their intent to the AI, where the loop repeats until the human is satisfied. With these critical cognitive and computational steps within this back-and-forth laid out as a framework, it is easier to anticipate where communication can fail, and design algorithms and interfaces that ameliorate those failure points.","sentences":["While we do not always use words, communicating what we want to an AI is a conversation -- with ourselves as well as with it, a recurring loop with optional steps depending on the complexity of the situation and our request.","Any given conversation of this type may include: (a) the human forming an intent, (b) the human expressing that intent as a command or utterance, (c) the AI performing one or more rounds of inference on that command to resolve ambiguities and/or requesting clarifications from the human, (d) the AI showing the inferred meaning of the command and/or its execution on current and future situations or data, (e) the human hopefully correctly recognizing whether the AI's interpretation actually aligns with their intent.","In the process, they may (f) update their model of the AI's capabilities and characteristics, (g) update their model of the situations in which the AI is executing its interpretation of their intent, (h) confirm or refine their intent, and (i) revise their expression of their intent to the AI, where the loop repeats until the human is satisfied.","With these critical cognitive and computational steps within this back-and-forth laid out as a framework, it is easier to anticipate where communication can fail, and design algorithms and interfaces that ameliorate those failure points."],"url":"http://arxiv.org/abs/2309.02257v1"}
{"created":"2023-09-05 14:08:36","title":"MAFIA: Protecting the Microarchitecture of Embedded Systems Against Fault Injection Attacks","abstract":"Fault injection attacks represent an effective threat to embedded systems. Recently, Laurent et al. have reported that fault injection attacks can leverage faults inside the microarchitecture. However, state-of-the-art counter-measures, hardwareonly or with hardware support, do not consider the integrity of microarchitecture control signals that are the target of these faults.   We present MAFIA, a microarchitecture protection against fault injection attacks. MAFIA ensures integrity of pipeline control signals through a signature-based mechanism, and ensures fine-grained control-flow integrity with a complete indirect branch support and code authenticity. We analyse the security properties of two different implementations with different security/overhead trade-offs: one with a CBC-MAC/Prince signature function, and another one with a CRC32. We present our implementation of MAFIA in a RISC-V processor, supported by a dedicated compiler toolchain based on LLVM/Clang. We report a hardware area overhead of 23.8 % and 6.5 % for the CBC-MAC/Prince and CRC32 respectively. The average code size and execution time overheads are 29.4 % and 18.4 % respectively for the CRC32 implementation and are 50 % and 39 % for the CBC-MAC/Prince.","sentences":["Fault injection attacks represent an effective threat to embedded systems.","Recently, Laurent et al. have reported that fault injection attacks can leverage faults inside the microarchitecture.","However, state-of-the-art counter-measures, hardwareonly or with hardware support, do not consider the integrity of microarchitecture control signals that are the target of these faults.   ","We present MAFIA, a microarchitecture protection against fault injection attacks.","MAFIA ensures integrity of pipeline control signals through a signature-based mechanism, and ensures fine-grained control-flow integrity with a complete indirect branch support and code authenticity.","We analyse the security properties of two different implementations with different security/overhead trade-offs: one with a CBC-MAC/Prince signature function, and another one with a CRC32.","We present our implementation of MAFIA in a RISC-V processor, supported by a dedicated compiler toolchain based on LLVM/Clang.","We report a hardware area overhead of 23.8 % and 6.5 % for the CBC-MAC/Prince and CRC32 respectively.","The average code size and execution time overheads are 29.4 % and 18.4 % respectively for the CRC32 implementation and are 50 % and 39 % for the CBC-MAC/Prince."],"url":"http://arxiv.org/abs/2309.02255v1"}
{"created":"2023-09-05 14:05:37","title":"MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance Powertrain Testing","abstract":"A clear need for automatic anomaly detection applied to automotive testing has emerged as more and more attention is paid to the data recorded and manual evaluation by humans reaches its capacity. Such real-world data is massive, diverse, multivariate and temporal in nature, therefore requiring modelling of the testee behaviour. We propose a variational autoencoder with multi-head attention (MA-VAE), which, when trained on unlabelled data, not only provides very few false positives but also manages to detect the majority of the anomalies presented. In addition to that, the approach offers a novel way to avoid the bypass phenomenon, an undesirable behaviour investigated in literature. Lastly, the approach also introduces a new method to remap individual windows to a continuous time series. The results are presented in the context of a real-world industrial data set and several experiments are undertaken to further investigate certain aspects of the proposed model. When configured properly, it is 9% of the time wrong when an anomaly is flagged and discovers 67% of the anomalies present. Also, MA-VAE has the potential to perform well with only a fraction of the training and validation subset, however, to extract it, a more sophisticated threshold estimation method is required.","sentences":["A clear need for automatic anomaly detection applied to automotive testing has emerged as more and more attention is paid to the data recorded and manual evaluation by humans reaches its capacity.","Such real-world data is massive, diverse, multivariate and temporal in nature, therefore requiring modelling of the testee behaviour.","We propose a variational autoencoder with multi-head attention (MA-VAE), which, when trained on unlabelled data, not only provides very few false positives but also manages to detect the majority of the anomalies presented.","In addition to that, the approach offers a novel way to avoid the bypass phenomenon, an undesirable behaviour investigated in literature.","Lastly, the approach also introduces a new method to remap individual windows to a continuous time series.","The results are presented in the context of a real-world industrial data set and several experiments are undertaken to further investigate certain aspects of the proposed model.","When configured properly, it is 9% of the time wrong when an anomaly is flagged and discovers 67% of the anomalies present.","Also, MA-VAE has the potential to perform well with only a fraction of the training and validation subset, however, to extract it, a more sophisticated threshold estimation method is required."],"url":"http://arxiv.org/abs/2309.02253v1"}
{"created":"2023-09-05 14:04:00","title":"STGIN: Spatial-Temporal Graph Interaction Network for Large-scale POI Recommendation","abstract":"In Location-Based Services, Point-Of-Interest(POI) recommendation plays a crucial role in both user experience and business opportunities. Graph neural networks have been proven effective in providing personalized POI recommendation services. However, there are still two critical challenges. First, existing graph models attempt to capture users' diversified interests through a unified graph, which limits their ability to express interests in various spatial-temporal contexts. Second, the efficiency limitations of graph construction and graph sampling in large-scale systems make it difficult to adapt quickly to new real-time interests. To tackle the above challenges, we propose a novel Spatial-Temporal Graph Interaction Network. Specifically, we construct subgraphs of spatial, temporal, spatial-temporal, and global views respectively to precisely characterize the user's interests in various contexts. In addition, we design an industry-friendly framework to track the user's latest interests. Extensive experiments on the real-world dataset show that our method outperforms state-of-the-art models. This work has been successfully deployed in a large e-commerce platform, delivering a 1.1% CTR and 6.3% RPM improvement.","sentences":["In Location-Based Services, Point-Of-Interest(POI) recommendation plays a crucial role in both user experience and business opportunities.","Graph neural networks have been proven effective in providing personalized POI recommendation services.","However, there are still two critical challenges.","First, existing graph models attempt to capture users' diversified interests through a unified graph, which limits their ability to express interests in various spatial-temporal contexts.","Second, the efficiency limitations of graph construction and graph sampling in large-scale systems make it difficult to adapt quickly to new real-time interests.","To tackle the above challenges, we propose a novel Spatial-Temporal Graph Interaction Network.","Specifically, we construct subgraphs of spatial, temporal, spatial-temporal, and global views respectively to precisely characterize the user's interests in various contexts.","In addition, we design an industry-friendly framework to track the user's latest interests.","Extensive experiments on the real-world dataset show that our method outperforms state-of-the-art models.","This work has been successfully deployed in a large e-commerce platform, delivering a 1.1% CTR and 6.3% RPM improvement."],"url":"http://arxiv.org/abs/2309.02251v1"}
{"created":"2023-09-05 13:59:50","title":"RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning","abstract":"In the domain of machine learning algorithms, the significance of the loss function is paramount, especially in supervised learning tasks. It serves as a fundamental pillar that profoundly influences the behavior and efficacy of supervised learning algorithms. Traditional loss functions, while widely used, often struggle to handle noisy and high-dimensional data, impede model interpretability, and lead to slow convergence during training. In this paper, we address the aforementioned constraints by proposing a novel robust, bounded, sparse, and smooth (RoBoSS) loss function for supervised learning. Further, we incorporate the RoBoSS loss function within the framework of support vector machine (SVM) and introduce a new robust algorithm named $\\mathcal{L}_{rbss}$-SVM. For the theoretical analysis, the classification-calibrated property and generalization ability are also presented. These investigations are crucial for gaining deeper insights into the performance of the RoBoSS loss function in the classification tasks and its potential to generalize well to unseen data. To empirically demonstrate the effectiveness of the proposed $\\mathcal{L}_{rbss}$-SVM, we evaluate it on $88$ real-world UCI and KEEL datasets from diverse domains. Additionally, to exemplify the effectiveness of the proposed $\\mathcal{L}_{rbss}$-SVM within the biomedical realm, we evaluated it on two medical datasets: the electroencephalogram (EEG) signal dataset and the breast cancer (BreaKHis) dataset. The numerical results substantiate the superiority of the proposed $\\mathcal{L}_{rbss}$-SVM model, both in terms of its remarkable generalization performance and its efficiency in training time.","sentences":["In the domain of machine learning algorithms, the significance of the loss function is paramount, especially in supervised learning tasks.","It serves as a fundamental pillar that profoundly influences the behavior and efficacy of supervised learning algorithms.","Traditional loss functions, while widely used, often struggle to handle noisy and high-dimensional data, impede model interpretability, and lead to slow convergence during training.","In this paper, we address the aforementioned constraints by proposing a novel robust, bounded, sparse, and smooth (RoBoSS) loss function for supervised learning.","Further, we incorporate the RoBoSS loss function within the framework of support vector machine (SVM) and introduce a new robust algorithm named $\\mathcal{L}_{rbss}$-SVM.","For the theoretical analysis, the classification-calibrated property and generalization ability are also presented.","These investigations are crucial for gaining deeper insights into the performance of the RoBoSS loss function in the classification tasks and its potential to generalize well to unseen data.","To empirically demonstrate the effectiveness of the proposed $\\mathcal{L}_{rbss}$-SVM, we evaluate it on $88$ real-world UCI and KEEL datasets from diverse domains.","Additionally, to exemplify the effectiveness of the proposed $\\mathcal{L}_{rbss}$-SVM within the biomedical realm, we evaluated it on two medical datasets: the electroencephalogram (EEG) signal dataset and the breast cancer (BreaKHis) dataset.","The numerical results substantiate the superiority of the proposed $\\mathcal{L}_{rbss}$-SVM model, both in terms of its remarkable generalization performance and its efficiency in training time."],"url":"http://arxiv.org/abs/2309.02250v1"}
{"created":"2023-09-05 13:58:59","title":"Encoding Seasonal Climate Predictions for Demand Forecasting with Modular Neural Network","abstract":"Current time-series forecasting problems use short-term weather attributes as exogenous inputs. However, in specific time-series forecasting solutions (e.g., demand prediction in the supply chain), seasonal climate predictions are crucial to improve its resilience. Representing mid to long-term seasonal climate forecasts is challenging as seasonal climate predictions are uncertain, and encoding spatio-temporal relationship of climate forecasts with demand is complex.   We propose a novel modeling framework that efficiently encodes seasonal climate predictions to provide robust and reliable time-series forecasting for supply chain functions. The encoding framework enables effective learning of latent representations -- be it uncertain seasonal climate prediction or other time-series data (e.g., buyer patterns) -- via a modular neural network architecture. Our extensive experiments indicate that learning such representations to model seasonal climate forecast results in an error reduction of approximately 13\\% to 17\\% across multiple real-world data sets compared to existing demand forecasting methods.","sentences":["Current time-series forecasting problems use short-term weather attributes as exogenous inputs.","However, in specific time-series forecasting solutions (e.g., demand prediction in the supply chain), seasonal climate predictions are crucial to improve its resilience.","Representing mid to long-term seasonal climate forecasts is challenging as seasonal climate predictions are uncertain, and encoding spatio-temporal relationship of climate forecasts with demand is complex.   ","We propose a novel modeling framework that efficiently encodes seasonal climate predictions to provide robust and reliable time-series forecasting for supply chain functions.","The encoding framework enables effective learning of latent representations -- be it uncertain seasonal climate prediction or other time-series data (e.g., buyer patterns) -- via a modular neural network architecture.","Our extensive experiments indicate that learning such representations to model seasonal climate forecast results in an error reduction of approximately 13\\% to 17\\% across multiple real-world data sets compared to existing demand forecasting methods."],"url":"http://arxiv.org/abs/2309.02248v1"}
{"created":"2023-09-05 13:56:31","title":"Second International Workshop on Adaptive Cyber Defense, 2023","abstract":"Recently, reinforcement and deep reinforcement learning (RL/DRL) have been applied to develop autonomous agents for cyber network operations(CyOps), where the agents are trained in a representative environment using RL and particularly DRL algorithms. The training environment must simulate CyOps with high fidelity, which the agent aims to learn and accomplish. A good simulator is hard to achieve due to the extreme complexity of the cyber environment. The trained agent must also be generalizable to network variations because operational cyber networks change constantly. The red agent case is taken to discuss these two issues in this work. We elaborate on their essential requirements and potential solution options, illustrated by some preliminary experimentations in a Cyber Gym for Intelligent Learning (CyGIL) testbed.","sentences":["Recently, reinforcement and deep reinforcement learning (RL/DRL) have been applied to develop autonomous agents for cyber network operations(CyOps), where the agents are trained in a representative environment using RL and particularly DRL algorithms.","The training environment must simulate CyOps with high fidelity, which the agent aims to learn and accomplish.","A good simulator is hard to achieve due to the extreme complexity of the cyber environment.","The trained agent must also be generalizable to network variations because operational cyber networks change constantly.","The red agent case is taken to discuss these two issues in this work.","We elaborate on their essential requirements and potential solution options, illustrated by some preliminary experimentations in a Cyber Gym for Intelligent Learning (CyGIL) testbed."],"url":"http://arxiv.org/abs/2309.02247v1"}
{"created":"2023-09-05 13:52:43","title":"Augmenting Chest X-ray Datasets with Non-Expert Annotations","abstract":"The advancement of machine learning algorithms in medical image analysis requires the expansion of training datasets. A popular and cost-effective approach is automated annotation extraction from free-text medical reports, primarily due to the high costs associated with expert clinicians annotating chest X-ray images. However, it has been shown that the resulting datasets are susceptible to biases and shortcuts. Another strategy to increase the size of a dataset is crowdsourcing, a widely adopted practice in general computer vision with some success in medical image analysis. In a similar vein to crowdsourcing, we enhance two publicly available chest X-ray datasets by incorporating non-expert annotations. However, instead of using diagnostic labels, we annotate shortcuts in the form of tubes. We collect 3.5k chest drain annotations for CXR14, and 1k annotations for 4 different tube types in PadChest. We train a chest drain detector with the non-expert annotations that generalizes well to expert labels. Moreover, we compare our annotations to those provided by experts and show \"moderate\" to \"almost perfect\" agreement. Finally, we present a pathology agreement study to raise awareness about ground truth annotations. We make our annotations and code available.","sentences":["The advancement of machine learning algorithms in medical image analysis requires the expansion of training datasets.","A popular and cost-effective approach is automated annotation extraction from free-text medical reports, primarily due to the high costs associated with expert clinicians annotating chest X-ray images.","However, it has been shown that the resulting datasets are susceptible to biases and shortcuts.","Another strategy to increase the size of a dataset is crowdsourcing, a widely adopted practice in general computer vision with some success in medical image analysis.","In a similar vein to crowdsourcing, we enhance two publicly available chest X-ray datasets by incorporating non-expert annotations.","However, instead of using diagnostic labels, we annotate shortcuts in the form of tubes.","We collect 3.5k chest drain annotations for CXR14, and 1k annotations for 4 different tube types in PadChest.","We train a chest drain detector with the non-expert annotations that generalizes well to expert labels.","Moreover, we compare our annotations to those provided by experts and show \"moderate\" to \"almost perfect\" agreement.","Finally, we present a pathology agreement study to raise awareness about ground truth annotations.","We make our annotations and code available."],"url":"http://arxiv.org/abs/2309.02244v1"}
{"created":"2023-09-05 13:49:29","title":"Self-Similarity-Based and Novelty-based loss for music structure analysis","abstract":"Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. In this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels. For this we jointly optimize -- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and -- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss. We also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA. Finally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.","sentences":["Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity.","In this paper we propose a supervised approach for the task of music boundary detection.","In our approach we simultaneously learn features and convolution kernels.","For this we jointly optimize -- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and -- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss.","We also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA.","Finally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI."],"url":"http://arxiv.org/abs/2309.02243v1"}
{"created":"2023-09-05 13:47:25","title":"Dialog Action-Aware Transformer for Dialog Policy Learning","abstract":"Recent works usually address Dialog policy learning DPL by training a reinforcement learning (RL) agent to determine the best dialog action. However, existing works on deep RL require a large volume of agent-user interactions to achieve acceptable performance. In this paper, we propose to make full use of the plain text knowledge from the pre-trained language model to accelerate the RL agent's learning speed. Specifically, we design a dialog action-aware transformer encoder (DaTrans), which integrates a new fine-tuning procedure named masked last action task to encourage DaTrans to be dialog-aware and distils action-specific features. Then, DaTrans is further optimized in an RL setting with ongoing interactions and evolves through exploration in the dialog action space toward maximizing long-term accumulated rewards. The effectiveness and efficiency of the proposed model are demonstrated with both simulator evaluation and human evaluation.","sentences":["Recent works usually address Dialog policy learning DPL by training a reinforcement learning (RL) agent to determine the best dialog action.","However, existing works on deep RL require a large volume of agent-user interactions to achieve acceptable performance.","In this paper, we propose to make full use of the plain text knowledge from the pre-trained language model to accelerate the RL agent's learning speed.","Specifically, we design a dialog action-aware transformer encoder (DaTrans), which integrates a new fine-tuning procedure named masked last action task to encourage DaTrans to be dialog-aware and distils action-specific features.","Then, DaTrans is further optimized in an RL setting with ongoing interactions and evolves through exploration in the dialog action space toward maximizing long-term accumulated rewards.","The effectiveness and efficiency of the proposed model are demonstrated with both simulator evaluation and human evaluation."],"url":"http://arxiv.org/abs/2309.02240v1"}
{"created":"2023-09-05 13:42:43","title":"Sample Size in Natural Language Processing within Healthcare Research","abstract":"Sample size calculation is an essential step in most data-based disciplines. Large enough samples ensure representativeness of the population and determine the precision of estimates. This is true for most quantitative studies, including those that employ machine learning methods, such as natural language processing, where free-text is used to generate predictions and classify instances of text. Within the healthcare domain, the lack of sufficient corpora of previously collected data can be a limiting factor when determining sample sizes for new studies. This paper tries to address the issue by making recommendations on sample sizes for text classification tasks in the healthcare domain.   Models trained on the MIMIC-III database of critical care records from Beth Israel Deaconess Medical Center were used to classify documents as having or not having Unspecified Essential Hypertension, the most common diagnosis code in the database. Simulations were performed using various classifiers on different sample sizes and class proportions. This was repeated for a comparatively less common diagnosis code within the database of diabetes mellitus without mention of complication.   Smaller sample sizes resulted in better results when using a K-nearest neighbours classifier, whereas larger sample sizes provided better results with support vector machines and BERT models. Overall, a sample size larger than 1000 was sufficient to provide decent performance metrics.   The simulations conducted within this study provide guidelines that can be used as recommendations for selecting appropriate sample sizes and class proportions, and for predicting expected performance, when building classifiers for textual healthcare data. The methodology used here can be modified for sample size estimates calculations with other datasets.","sentences":["Sample size calculation is an essential step in most data-based disciplines.","Large enough samples ensure representativeness of the population and determine the precision of estimates.","This is true for most quantitative studies, including those that employ machine learning methods, such as natural language processing, where free-text is used to generate predictions and classify instances of text.","Within the healthcare domain, the lack of sufficient corpora of previously collected data can be a limiting factor when determining sample sizes for new studies.","This paper tries to address the issue by making recommendations on sample sizes for text classification tasks in the healthcare domain.   ","Models trained on the MIMIC-III database of critical care records from Beth Israel Deaconess Medical Center were used to classify documents as having or not having Unspecified Essential Hypertension, the most common diagnosis code in the database.","Simulations were performed using various classifiers on different sample sizes and class proportions.","This was repeated for a comparatively less common diagnosis code within the database of diabetes mellitus without mention of complication.   ","Smaller sample sizes resulted in better results when using a K-nearest neighbours classifier, whereas larger sample sizes provided better results with support vector machines and BERT models.","Overall, a sample size larger than 1000 was sufficient to provide decent performance metrics.   ","The simulations conducted within this study provide guidelines that can be used as recommendations for selecting appropriate sample sizes and class proportions, and for predicting expected performance, when building classifiers for textual healthcare data.","The methodology used here can be modified for sample size estimates calculations with other datasets."],"url":"http://arxiv.org/abs/2309.02237v1"}
{"created":"2023-09-05 13:42:11","title":"Distributionally Robust Model-based Reinforcement Learning with Large State Spaces","abstract":"Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment. To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets. We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator). We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets. These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies. The proposed method can be further combined with other model-free distributionally robust reinforcement learning methods to obtain a near-optimal robust policy. Experimental results demonstrate the robustness of our algorithm to distributional shifts and its superior performance in terms of the number of samples needed.","sentences":["Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment.","To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets.","We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator).","We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets.","These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies.","The proposed method can be further combined with other model-free distributionally robust reinforcement learning methods to obtain a near-optimal robust policy.","Experimental results demonstrate the robustness of our algorithm to distributional shifts and its superior performance in terms of the number of samples needed."],"url":"http://arxiv.org/abs/2309.02236v1"}
{"created":"2023-09-05 13:40:48","title":"Experimental Evaluation of Air-to-Ground VHF Band Communication for UAV Relays","abstract":"Unmanned Aerial Vehicles (UAVs) are a disruptive technology that is transforming a range of industries. Because they operate in the sky, UAVs are able to take advantage of strong Line-of-Sight (LoS) channels for radio propagation, allowing them to communicate over much larger distances than equivalent hardware located at ground level. This has attracted the attention of organisations such as the Irish Defence Forces (DF), with whom we are developing a UAV-based radio relay system as part of the MISTRAL project. This relay system will support digital Very High Frequency (VHF) band communication between ground personnel, while they are deployed on missions. In this paper we report on the initial set of experimental measurements which were carried out to verify the feasibility of VHF signal relaying via UAV. In our experiments, a UAV carrying a lightweight Software-Defined Radio (SDR) receiver is positioned at a height of 500 meters above ground, while two 5W transmitters travel in vehicles on the ground. The SDR receiver measures the received signal power, while the Global Positioning System (GPS) coordinates of the vehicles are logged. This is combined to measure the signal pathloss over distance. Our results show that the signal is received successfully at distances of over 50 kilometers away. While the signals still appear to suffer from a degree of obstacle blockage and multipath effects, these communication ranges are a substantial improvement over the ground communication baseline, and validate the use of UAVs to support wide area emergency communication.","sentences":["Unmanned Aerial Vehicles (UAVs) are a disruptive technology that is transforming a range of industries.","Because they operate in the sky, UAVs are able to take advantage of strong Line-of-Sight (LoS) channels for radio propagation, allowing them to communicate over much larger distances than equivalent hardware located at ground level.","This has attracted the attention of organisations such as the Irish Defence Forces (DF), with whom we are developing a UAV-based radio relay system as part of the MISTRAL project.","This relay system will support digital Very High Frequency (VHF) band communication between ground personnel, while they are deployed on missions.","In this paper we report on the initial set of experimental measurements which were carried out to verify the feasibility of VHF signal relaying via UAV.","In our experiments, a UAV carrying a lightweight Software-Defined Radio (SDR) receiver is positioned at a height of 500 meters above ground, while two 5W transmitters travel in vehicles on the ground.","The SDR receiver measures the received signal power, while the Global Positioning System (GPS) coordinates of the vehicles are logged.","This is combined to measure the signal pathloss over distance.","Our results show that the signal is received successfully at distances of over 50 kilometers away.","While the signals still appear to suffer from a degree of obstacle blockage and multipath effects, these communication ranges are a substantial improvement over the ground communication baseline, and validate the use of UAVs to support wide area emergency communication."],"url":"http://arxiv.org/abs/2309.02235v1"}
