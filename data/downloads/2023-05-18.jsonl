{"created":"2023-05-17","title":"Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs","abstract":"Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation. Additionally, we experiment with different prompt engineering techniques to examine their impact on model performance. Our results show that LLMs significantly outperform existing KG completion models in relation labeling for e-commerce KGs and exhibit performance strong enough to replace human labeling.","sentences":["Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems.","However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor.","Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks.","In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data.","We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation.","Additionally, we experiment with different prompt engineering techniques to examine their impact on model performance.","Our results show that LLMs significantly outperform existing KG completion models in relation labeling for e-commerce KGs and exhibit performance strong enough to replace human labeling."],"url":"http://arxiv.org/abs/2305.09858v1"}
{"created":"2023-05-17","title":"BAD: BiAs Detection for Large Language Models in the context of candidate screening","abstract":"Application Tracking Systems (ATS) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently. Traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias. The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed. In this project, we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process.","sentences":["Application Tracking Systems (ATS) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently.","Traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias.","The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed.","In this project, we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process."],"url":"http://arxiv.org/abs/2305.10407v1"}
{"created":"2023-05-17","title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models","abstract":"In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. Extensive experiments indicate that CoS clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World.","sentences":["In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text.","We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation.","We found that current popular LLMs such as ChatGPT still lack abilities in complex planning.","This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs?","To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps.","CoS is easy to use and does not need additional training on LLMs.","Extensive experiments indicate that CoS clearly surpasses the performance of the Chain-of-Thought (CoT)","Prompting in all three planning tasks with even fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.","The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT.","CoS also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World."],"url":"http://arxiv.org/abs/2305.10276v1"}
{"created":"2023-05-17","title":"MemoryBank: Enhancing Large Language Models with Long-Term Memory","abstract":"Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a human-like memory mechanism. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models like ChatGLM. We exemplify application of MemoryBank through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialogs, SiliconFriend displays heightened empathy in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.","sentences":["Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems.","Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models.","This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling.","Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs.","MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions.","To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a human-like memory mechanism.","MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models like ChatGLM.","We exemplify application of MemoryBank through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario.","Further tuned with psychological dialogs, SiliconFriend displays heightened empathy in its interactions.","Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs.","In the latter, ChatGPT acts as users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics.","The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality."],"url":"http://arxiv.org/abs/2305.10250v1"}
{"created":"2023-05-17","title":"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model","abstract":"Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., only 51\\% of questions are answered correctly). While our knowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which not only passes the qualification but also surpasses the average score of humans (61). This research demonstrates the potential of knowledge-enhanced ChatGPT to serve as versatile medical assistants, capable of analyzing real-world medical problems in a more accessible, user-friendly, and adaptable manner.","sentences":["Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks.","Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare.","In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning.","By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT.","Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT.","Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., only 51\\% of questions are answered correctly).","While our knowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which not only passes the qualification but also surpasses the average score of humans (61).","This research demonstrates the potential of knowledge-enhanced ChatGPT to serve as versatile medical assistants, capable of analyzing real-world medical problems in a more accessible, user-friendly, and adaptable manner."],"url":"http://arxiv.org/abs/2305.10163v1"}
{"created":"2023-05-17","title":"Smaller Language Models are Better Black-box Machine-Generated Text Detectors","abstract":"With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45.","sentences":["With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures.","To this end, there have been a slew of methods proposed to detect machine-generated text.","Most of these methods need access to the logits of the target model or need the ability to sample from the target.","One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not.","We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models.","Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success.","For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45."],"url":"http://arxiv.org/abs/2305.09859v1"}
{"created":"2023-05-17","title":"FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention","abstract":"Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend features among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300$\\times$-2500$\\times$ speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available at https://github.com/mit-han-lab/fastcomposer.","sentences":["Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images.","However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment.","Moreover, existing methods struggle with multi-subject generation as they often blend features among subjects.","We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning.","FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes.","To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images.","Naively conditioning on subject embeddings results in subject overfitting.","FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation.","FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts.","It achieves 300$\\times$-2500$\\times$ speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects.","FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation.","Code, model, and dataset are available at https://github.com/mit-han-lab/fastcomposer."],"url":"http://arxiv.org/abs/2305.10431v1"}
{"created":"2023-05-17","title":"Rethinking the Open-Loop Evaluation of End-to-End Autonomous Driving in nuScenes","abstract":"Modern autonomous driving systems are typically divided into three main tasks: perception, prediction, and planning. The planning task involves predicting the trajectory of the ego vehicle based on inputs from both internal intention and the external environment, and manipulating the vehicle accordingly. Most existing works evaluate their performance on the nuScenes dataset using the L2 error and collision rate between the predicted trajectories and the ground truth. In this paper, we reevaluate these existing evaluation metrics and explore whether they accurately measure the superiority of different methods. Specifically, we design an MLP-based method that takes raw sensor data (e.g., past trajectory, velocity, etc.) as input and directly outputs the future trajectory of the ego vehicle, without using any perception or prediction information such as camera images or LiDAR. Surprisingly, such a simple method achieves state-of-the-art end-to-end planning performance on the nuScenes dataset, reducing the average L2 error by about 30%. We further conduct in-depth analysis and provide new insights into the factors that are critical for the success of the planning task on nuScenes dataset. Our observation also indicates that we need to rethink the current open-loop evaluation scheme of end-to-end autonomous driving in nuScenes. Codes are available at https://github.com/E2E-AD/AD-MLP.","sentences":["Modern autonomous driving systems are typically divided into three main tasks: perception, prediction, and planning.","The planning task involves predicting the trajectory of the ego vehicle based on inputs from both internal intention and the external environment, and manipulating the vehicle accordingly.","Most existing works evaluate their performance on the nuScenes dataset using the L2 error and collision rate between the predicted trajectories and the ground truth.","In this paper, we reevaluate these existing evaluation metrics and explore whether they accurately measure the superiority of different methods.","Specifically, we design an MLP-based method that takes raw sensor data (e.g., past trajectory, velocity, etc.) as input and directly outputs the future trajectory of the ego vehicle, without using any perception or prediction information such as camera images or LiDAR.","Surprisingly, such a simple method achieves state-of-the-art end-to-end planning performance on the nuScenes dataset, reducing the average L2 error by about 30%.","We further conduct in-depth analysis and provide new insights into the factors that are critical for the success of the planning task on nuScenes dataset.","Our observation also indicates that we need to rethink the current open-loop evaluation scheme of end-to-end autonomous driving in nuScenes.","Codes are available at https://github.com/E2E-AD/AD-MLP."],"url":"http://arxiv.org/abs/2305.10430v1"}
{"created":"2023-05-17","title":"DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining","abstract":"The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.","sentences":["The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance.","In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks.","We then resample a dataset with these domain weights and train a larger, full-sized model.","In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently.","On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain.","DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps.","On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks."],"url":"http://arxiv.org/abs/2305.10429v1"}
{"created":"2023-05-17","title":"ZeroFlow: Fast Zero Label Scene Flow via Distillation","abstract":"Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is over 1000$\\times$ faster than label-free state-of-the-art optimization-based methods on large-scale point clouds and over 1000$\\times$ cheaper to train on unlabeled data compared to the cost of human annotation of that data. To facilitate research reuse, we release our code, trained model weights, and high quality pseudo-labels for the Argoverse 2 and Waymo Open datasets.","sentences":["Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds.","State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection.","Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision.","To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model.","Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels.","Notably, at test-time ZeroFlow is over 1000$\\times$ faster than label-free state-of-the-art optimization-based methods on large-scale point clouds and over 1000$\\times$ cheaper to train on unlabeled data compared to the cost of human annotation of that data.","To facilitate research reuse, we release our code, trained model weights, and high quality pseudo-labels for the Argoverse 2 and Waymo Open datasets."],"url":"http://arxiv.org/abs/2305.10424v1"}
{"created":"2023-05-17","title":"Evolving Tsukamoto Neuro Fuzzy Model for Multiclass Covid 19 Classification with Chest X Ray Images","abstract":"Du e to rapid population growth and the need to use artificial intelligence to make quick decisions, developing a machine learning-based disease detection model and abnormality identification system has greatly improved the level of medical diagnosis Since COVID-19 has become one of the most severe diseases in the world, developing an automatic COVID-19 detection framework helps medical doctors in the diagnostic process of disease and provides correct and fast results. In this paper, we propose a machine lear ning based framework for the detection of Covid 19. The proposed model employs a Tsukamoto Neuro Fuzzy Inference network to identify and distinguish Covid 19 disease from normal and pneumonia cases. While the traditional training methods tune the parameters of the neuro-fuzzy model by gradient-based algorithms and recursive least square method, we use an evolutionary-based optimization, the Cat swarm algorithm to update the parameters. In addition, six texture features extracted from chest X-ray images are give n as input to the model. Finally, the proposed model is conducted on the chest X-ray dataset to detect Covid 19. The simulation results indicate that the proposed model achieves an accuracy of 98.51%, sensitivity of 98.35%, specificity of 98.08%, and F1 score of 98.17%.","sentences":["Du e to rapid population growth and the need to use artificial intelligence to make quick decisions, developing a machine learning-based disease detection model and abnormality identification system has greatly improved the level of medical diagnosis Since COVID-19 has become one of the most severe diseases in the world, developing an automatic COVID-19 detection framework helps medical doctors in the diagnostic process of disease and provides correct and fast results.","In this paper, we propose a machine lear ning based framework for the detection of Covid 19.","The proposed model employs a Tsukamoto Neuro Fuzzy Inference network to identify and distinguish Covid 19 disease from normal and pneumonia cases.","While the traditional training methods tune the parameters of the neuro-fuzzy model by gradient-based algorithms and recursive least square method, we use an evolutionary-based optimization, the Cat swarm algorithm to update the parameters.","In addition, six texture features extracted from chest X-ray images are give n as input to the model.","Finally, the proposed model is conducted on the chest X-ray dataset to detect Covid 19.","The simulation results indicate that the proposed model achieves an accuracy of 98.51%, sensitivity of 98.35%, specificity of 98.08%, and F1 score of 98.17%."],"url":"http://arxiv.org/abs/2305.10421v1"}
{"created":"2023-05-17","title":"CLIP-GCD: Simple Language Guided Generalized Category Discovery","abstract":"Generalized Category Discovery (GCD) requires a model to both classify known categories and cluster unknown categories in unlabeled data. Prior methods leveraged self-supervised pre-training combined with supervised fine-tuning on the labeled data, followed by simple clustering methods. In this paper, we posit that such methods are still prone to poor performance on out-of-distribution categories, and do not leverage a key ingredient: Semantic relationships between object categories. We therefore propose to leverage multi-modal (vision and language) models, in two complementary ways. First, we establish a strong baseline by replacing uni-modal features with CLIP, inspired by its zero-shot performance. Second, we propose a novel retrieval-based mechanism that leverages CLIP's aligned vision-language representations by mining text descriptions from a text corpus for the labeled and unlabeled set. We specifically use the alignment between CLIP's visual encoding of the image and textual encoding of the corpus to retrieve top-k relevant pieces of text and incorporate their embeddings to perform joint image+text semi-supervised clustering. We perform rigorous experimentation and ablations (including on where to retrieve from, how much to retrieve, and how to combine information), and validate our results on several datasets including out-of-distribution domains, demonstrating state-of-art results.","sentences":["Generalized Category Discovery (GCD) requires a model to both classify known categories and cluster unknown categories in unlabeled data.","Prior methods leveraged self-supervised pre-training combined with supervised fine-tuning on the labeled data, followed by simple clustering methods.","In this paper, we posit that such methods are still prone to poor performance on out-of-distribution categories, and do not leverage a key ingredient: Semantic relationships between object categories.","We therefore propose to leverage multi-modal (vision and language) models, in two complementary ways.","First, we establish a strong baseline by replacing uni-modal features with CLIP, inspired by its zero-shot performance.","Second, we propose a novel retrieval-based mechanism that leverages CLIP's aligned vision-language representations by mining text descriptions from a text corpus for the labeled and unlabeled set.","We specifically use the alignment between CLIP's visual encoding of the image and textual encoding of the corpus to retrieve top-k relevant pieces of text and incorporate their embeddings to perform joint image+text semi-supervised clustering.","We perform rigorous experimentation and ablations (including on where to retrieve from, how much to retrieve, and how to combine information), and validate our results on several datasets including out-of-distribution domains, demonstrating state-of-art results."],"url":"http://arxiv.org/abs/2305.10420v1"}
{"created":"2023-05-17","title":"Kitana: Efficient Data Augmentation Search for AutoML","abstract":"AutoML services provide a way for non-expert users to benefit from high-quality ML models without worrying about model design and deployment, in exchange for a charge per hour ($21.252 for VertexAI). However, existing AutoML services are model-centric, in that they are limited to extracting features and searching for models from initial training data-they are only as effective as the initial training data quality. With the increasing volume of tabular data available, there is a huge opportunity for data augmentation. For instance, vertical augmentation adds predictive features, while horizontal augmentation adds examples. This augmented training data yields potentially much better AutoML models at a lower cost. However, existing systems either forgo the augmentation opportunities that provide poor models, or apply expensive augmentation searching techniques that drain users' budgets.   Kitana is a data-centric AutoML system that also searches for new tabular datasets that can augment the tabular training data with new features and/or examples. Kitana manages a corpus of datasets, exposes an AutoML interface to users and searches for augmentation with datasets in the corpus to improve AutoML performance. To accelerate search, Kitana applies aggressive pre-computation to train a factorized proxy model and evaluate each candidate augmentation within 0.1s. Kitana also uses a cost model to limit the time spent on augmentation search, supports expressive data access controls, and performs request caching to benefit from past similar requests. Using a corpus of 518 open-source datasets, Kitana produces higher quality models than existing AutoML systems in orders of magnitude less time. Across different user requests, Kitana increases the model R2 from 0.16 to 0.66 while reducing the cost by >100x compared to the naive factorized learning and SOTA data augmentation search.","sentences":["AutoML services provide a way for non-expert users to benefit from high-quality ML models without worrying about model design and deployment, in exchange for a charge per hour ($21.252 for VertexAI).","However, existing AutoML services are model-centric, in that they are limited to extracting features and searching for models from initial training data-they are only as effective as the initial training data quality.","With the increasing volume of tabular data available, there is a huge opportunity for data augmentation.","For instance, vertical augmentation adds predictive features, while horizontal augmentation adds examples.","This augmented training data yields potentially much better AutoML models at a lower cost.","However, existing systems either forgo the augmentation opportunities that provide poor models, or apply expensive augmentation searching techniques that drain users' budgets.   ","Kitana is a data-centric AutoML system that also searches for new tabular datasets that can augment the tabular training data with new features and/or examples.","Kitana manages a corpus of datasets, exposes an AutoML interface to users and searches for augmentation with datasets in the corpus to improve AutoML performance.","To accelerate search, Kitana applies aggressive pre-computation to train a factorized proxy model and evaluate each candidate augmentation within 0.1s.","Kitana also uses a cost model to limit the time spent on augmentation search, supports expressive data access controls, and performs request caching to benefit from past similar requests.","Using a corpus of 518 open-source datasets, Kitana produces higher quality models than existing AutoML systems in orders of magnitude less time.","Across different user requests, Kitana increases the model R2 from 0.16 to 0.66 while reducing the cost by >100x compared to the naive factorized learning and SOTA data augmentation search."],"url":"http://arxiv.org/abs/2305.10419v1"}
{"created":"2023-05-17","title":"Towards Multi-Layered 3D Garments Animation","abstract":"Mimicking realistic dynamics in 3D garment animations is a challenging task due to the complex nature of multi-layered garments and the variety of outer forces involved. Existing approaches mostly focus on single-layered garments driven by only human bodies and struggle to handle general scenarios. In this paper, we propose a novel data-driven method, called LayersNet, to model garment-level animations as particle-wise interactions in a micro physics system. We improve simulation efficiency by representing garments as patch-level particles in a two-level structural hierarchy. Moreover, we introduce a novel Rotation Equivalent Transformation that leverages the rotation invariance and additivity of physics systems to better model outer forces. To verify the effectiveness of our approach and bridge the gap between experimental environments and real-world scenarios, we introduce a new challenging dataset, D-LAYERS, containing 700K frames of dynamics of 4,900 different combinations of multi-layered garments driven by both human bodies and randomly sampled wind. Our experiments show that LayersNet achieves superior performance both quantitatively and qualitatively. We will make the dataset and code publicly available at https://mmlab-ntu.github.io/project/layersnet/index.html .","sentences":["Mimicking realistic dynamics in 3D garment animations is a challenging task due to the complex nature of multi-layered garments and the variety of outer forces involved.","Existing approaches mostly focus on single-layered garments driven by only human bodies and struggle to handle general scenarios.","In this paper, we propose a novel data-driven method, called LayersNet, to model garment-level animations as particle-wise interactions in a micro physics system.","We improve simulation efficiency by representing garments as patch-level particles in a two-level structural hierarchy.","Moreover, we introduce a novel Rotation Equivalent Transformation that leverages the rotation invariance and additivity of physics systems to better model outer forces.","To verify the effectiveness of our approach and bridge the gap between experimental environments and real-world scenarios, we introduce a new challenging dataset, D-LAYERS, containing 700K frames of dynamics of 4,900 different combinations of multi-layered garments driven by both human bodies and randomly sampled wind.","Our experiments show that LayersNet achieves superior performance both quantitatively and qualitatively.","We will make the dataset and code publicly available at https://mmlab-ntu.github.io/project/layersnet/index.html ."],"url":"http://arxiv.org/abs/2305.10418v1"}
{"created":"2023-05-17","title":"Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families","abstract":"How can AI enhance creative coding experiences for families? This study explores the potential of large language models (LLMs) in helping families with creative coding using Scratch. Based on our previous user study involving a prototype AI assistant, we devised three evaluation scenarios to determine if LLMs could help families comprehend game code, debug programs, and generate new ideas for future projects. We utilized 22 Scratch projects for each scenario and generated responses from LLMs with and without practice tasks, resulting in 120 creative coding support scenario datasets. In addition, the authors independently evaluated their precision, pedagogical value, and age-appropriate language. Our findings show that LLMs achieved an overall success rate of more than 80\\% on the different tasks and evaluation criteria. This research offers valuable information on using LLMs for creative family coding and presents design guidelines for future AI-supported coding applications. Our evaluation framework, together with our labeled evaluation data, is publicly available.","sentences":["How can AI enhance creative coding experiences for families?","This study explores the potential of large language models (LLMs) in helping families with creative coding using Scratch.","Based on our previous user study involving a prototype AI assistant, we devised three evaluation scenarios to determine if LLMs could help families comprehend game code, debug programs, and generate new ideas for future projects.","We utilized 22 Scratch projects for each scenario and generated responses from LLMs with and without practice tasks, resulting in 120 creative coding support scenario datasets.","In addition, the authors independently evaluated their precision, pedagogical value, and age-appropriate language.","Our findings show that LLMs achieved an overall success rate of more than 80\\% on the different tasks and evaluation criteria.","This research offers valuable information on using LLMs for creative family coding and presents design guidelines for future AI-supported coding applications.","Our evaluation framework, together with our labeled evaluation data, is publicly available."],"url":"http://arxiv.org/abs/2305.10417v1"}
{"created":"2023-05-17","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","abstract":"In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which is crucial in efficiently interpreting medical images with vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction, we propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model. Secondly, we establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we pre-train our proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a large margin. Additionally, we propose a test set that has undergone manual verification, which is significantly more challenging, even the best models struggle to solve.","sentences":["In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which is crucial in efficiently interpreting medical images with vital clinic-relevant information.","Firstly, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction, we propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model.","Secondly, we establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases.","Thirdly, we pre-train our proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a large margin.","Additionally, we propose a test set that has undergone manual verification, which is significantly more challenging, even the best models struggle to solve."],"url":"http://arxiv.org/abs/2305.10415v1"}
{"created":"2023-05-17","title":"Unbounded Quantum Advantage in One-Way Strong Communication Complexity of a Distributed Clique Labelling Relation","abstract":"We investigate the one-way zero-error classical and quantum communication complexities for a class of relations induced by a distributed clique labelling problem. We consider two variants: 1) the receiver outputs an answer satisfying the relation - the traditional communication complexity of relations (CCR) and 2) the receiver has non-zero probabilities of outputting every valid answer satisfying the relation (equivalently, the relation can be fully reconstructed), that we denote the strong communication complexity of the relation (S-CCR). We prove that for the specific class of relations considered here when the players do not share any resources, there is no quantum advantage in the CCR task for any graph. On the other hand, we show that there exist, classes of graphs for which the separation between one-way classical and quantum communication in the S-CCR task grow with the order of the graph, specifically, the quantum complexity is $O(1)$ while the classical complexity is $\\Omega(\\log m)$. Secondly, we prove a lower bound (that is linear in the number of cliques) on the amount of shared randomness necessary to overcome the separation in the scenario of fixed restricted communication and connect this to the existence of Orthogonal Arrays. Finally, we highlight some applications of this task to semi-device-independent dimension witnessing as well as to the detection of Mutually Unbiased Bases.","sentences":["We investigate the one-way zero-error classical and quantum communication complexities for a class of relations induced by a distributed clique labelling problem.","We consider two variants: 1) the receiver outputs an answer satisfying the relation - the traditional communication complexity of relations (CCR) and 2) the receiver has non-zero probabilities of outputting every valid answer satisfying the relation (equivalently, the relation can be fully reconstructed), that we denote the strong communication complexity of the relation (S-CCR).","We prove that for the specific class of relations considered here when the players do not share any resources, there is no quantum advantage in the CCR task for any graph.","On the other hand, we show that there exist, classes of graphs for which the separation between one-way classical and quantum communication in the S-CCR task grow with the order of the graph, specifically, the quantum complexity is $O(1)$ while the classical complexity is $\\Omega(\\log m)$.","Secondly, we prove a lower bound (that is linear in the number of cliques) on the amount of shared randomness necessary to overcome the separation in the scenario of fixed restricted communication and connect this to the existence of Orthogonal Arrays.","Finally, we highlight some applications of this task to semi-device-independent dimension witnessing as well as to the detection of Mutually Unbiased Bases."],"url":"http://arxiv.org/abs/2305.10372v1"}
{"created":"2023-05-17","title":"LeTI: Learning to Generate from Textual Interactions","abstract":"Finetuning pre-trained language models (LMs) enhances the models' capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the generated program fails to solve the task. Prepended to this fine-tuning text, a binary reward token is used to differentiate correct and buggy solutions. On MBPP, a code generation dataset, LeTI substantially improves the performance of two base LMs of different scales. LeTI requires no ground-truth outputs for training and even outperforms a fine-tuned baseline that does. LeTI's strong performance generalizes to other datasets. Trained on MBPP, it achieves comparable or better performance than the base LMs on unseen problems in HumanEval. Furthermore, compared to binary feedback, we observe that textual feedback leads to improved generation quality and sample efficiency, achieving the same performance with fewer than half of the gradient steps. LeTI is equally applicable in natural language tasks when they can be formulated as code generation, which we empirically verified on event argument extraction.","sentences":["Finetuning pre-trained language models (LMs) enhances the models' capabilities.","Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback).","We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback.","Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions.","This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter.","LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the generated program fails to solve the task.","Prepended to this fine-tuning text, a binary reward token is used to differentiate correct and buggy solutions.","On MBPP, a code generation dataset, LeTI substantially improves the performance of two base LMs of different scales.","LeTI requires no ground-truth outputs for training and even outperforms a fine-tuned baseline that does.","LeTI's strong performance generalizes to other datasets.","Trained on MBPP, it achieves comparable or better performance than the base LMs on unseen problems in HumanEval.","Furthermore, compared to binary feedback, we observe that textual feedback leads to improved generation quality and sample efficiency, achieving the same performance with fewer than half of the gradient steps.","LeTI is equally applicable in natural language tasks when they can be formulated as code generation, which we empirically verified on event argument extraction."],"url":"http://arxiv.org/abs/2305.10314v1"}
{"created":"2023-05-17","title":"A robust multi-domain network for short-scanning amyloid PET reconstruction","abstract":"This paper presents a robust multi-domain network designed to restore low-quality amyloid PET images acquired in a short period of time. The proposed method is trained on pairs of PET images from short (2 minutes) and standard (20 minutes) scanning times, sourced from multiple domains. Learning relevant image features between these domains with a single network is challenging. Our key contribution is the introduction of a mapping label, which enables effective learning of specific representations between different domains. The network, trained with various mapping labels, can efficiently correct amyloid PET datasets in multiple training domains and unseen domains, such as those obtained with new radiotracers, acquisition protocols, or PET scanners. Internal, temporal, and external validations demonstrate the effectiveness of the proposed method. Notably, for external validation datasets from unseen domains, the proposed method achieved comparable or superior results relative to methods trained with these datasets, in terms of quantitative metrics such as normalized root mean-square error and structure similarity index measure. Two nuclear medicine physicians evaluated the amyloid status as positive or negative for the external validation datasets, with accuracies of 0.970 and 0.930 for readers 1 and 2, respectively.","sentences":["This paper presents a robust multi-domain network designed to restore low-quality amyloid PET images acquired in a short period of time.","The proposed method is trained on pairs of PET images from short (2 minutes) and standard (20 minutes) scanning times, sourced from multiple domains.","Learning relevant image features between these domains with a single network is challenging.","Our key contribution is the introduction of a mapping label, which enables effective learning of specific representations between different domains.","The network, trained with various mapping labels, can efficiently correct amyloid PET datasets in multiple training domains and unseen domains, such as those obtained with new radiotracers, acquisition protocols, or PET scanners.","Internal, temporal, and external validations demonstrate the effectiveness of the proposed method.","Notably, for external validation datasets from unseen domains, the proposed method achieved comparable or superior results relative to methods trained with these datasets, in terms of quantitative metrics such as normalized root mean-square error and structure similarity index measure.","Two nuclear medicine physicians evaluated the amyloid status as positive or negative for the external validation datasets, with accuracies of 0.970 and 0.930 for readers 1 and 2, respectively."],"url":"http://arxiv.org/abs/2305.09986v1"}
{"created":"2023-05-17","title":"ZeroFlow: Fast Zero Label Scene Flow via Distillation","abstract":"Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is over 1000$\\times$ faster than label-free state-of-the-art optimization-based methods on large-scale point clouds and over 1000$\\times$ cheaper to train on unlabeled data compared to the cost of human annotation of that data. To facilitate research reuse, we release our code, trained model weights, and high quality pseudo-labels for the Argoverse 2 and Waymo Open datasets.","sentences":["Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds.","State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection.","Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision.","To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model.","Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels.","Notably, at test-time ZeroFlow is over 1000$\\times$ faster than label-free state-of-the-art optimization-based methods on large-scale point clouds and over 1000$\\times$ cheaper to train on unlabeled data compared to the cost of human annotation of that data.","To facilitate research reuse, we release our code, trained model weights, and high quality pseudo-labels for the Argoverse 2 and Waymo Open datasets."],"url":"http://arxiv.org/abs/2305.10424v1"}
{"created":"2023-05-17","title":"Kitana: Efficient Data Augmentation Search for AutoML","abstract":"AutoML services provide a way for non-expert users to benefit from high-quality ML models without worrying about model design and deployment, in exchange for a charge per hour ($21.252 for VertexAI). However, existing AutoML services are model-centric, in that they are limited to extracting features and searching for models from initial training data-they are only as effective as the initial training data quality. With the increasing volume of tabular data available, there is a huge opportunity for data augmentation. For instance, vertical augmentation adds predictive features, while horizontal augmentation adds examples. This augmented training data yields potentially much better AutoML models at a lower cost. However, existing systems either forgo the augmentation opportunities that provide poor models, or apply expensive augmentation searching techniques that drain users' budgets.   Kitana is a data-centric AutoML system that also searches for new tabular datasets that can augment the tabular training data with new features and/or examples. Kitana manages a corpus of datasets, exposes an AutoML interface to users and searches for augmentation with datasets in the corpus to improve AutoML performance. To accelerate search, Kitana applies aggressive pre-computation to train a factorized proxy model and evaluate each candidate augmentation within 0.1s. Kitana also uses a cost model to limit the time spent on augmentation search, supports expressive data access controls, and performs request caching to benefit from past similar requests. Using a corpus of 518 open-source datasets, Kitana produces higher quality models than existing AutoML systems in orders of magnitude less time. Across different user requests, Kitana increases the model R2 from 0.16 to 0.66 while reducing the cost by >100x compared to the naive factorized learning and SOTA data augmentation search.","sentences":["AutoML services provide a way for non-expert users to benefit from high-quality ML models without worrying about model design and deployment, in exchange for a charge per hour ($21.252 for VertexAI).","However, existing AutoML services are model-centric, in that they are limited to extracting features and searching for models from initial training data-they are only as effective as the initial training data quality.","With the increasing volume of tabular data available, there is a huge opportunity for data augmentation.","For instance, vertical augmentation adds predictive features, while horizontal augmentation adds examples.","This augmented training data yields potentially much better AutoML models at a lower cost.","However, existing systems either forgo the augmentation opportunities that provide poor models, or apply expensive augmentation searching techniques that drain users' budgets.   ","Kitana is a data-centric AutoML system that also searches for new tabular datasets that can augment the tabular training data with new features and/or examples.","Kitana manages a corpus of datasets, exposes an AutoML interface to users and searches for augmentation with datasets in the corpus to improve AutoML performance.","To accelerate search, Kitana applies aggressive pre-computation to train a factorized proxy model and evaluate each candidate augmentation within 0.1s.","Kitana also uses a cost model to limit the time spent on augmentation search, supports expressive data access controls, and performs request caching to benefit from past similar requests.","Using a corpus of 518 open-source datasets, Kitana produces higher quality models than existing AutoML systems in orders of magnitude less time.","Across different user requests, Kitana increases the model R2 from 0.16 to 0.66 while reducing the cost by >100x compared to the naive factorized learning and SOTA data augmentation search."],"url":"http://arxiv.org/abs/2305.10419v1"}
{"created":"2023-05-17","title":"Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents","abstract":"Labeling data is essential for training text classifiers but is often difficult to accomplish accurately, especially for complex and abstract concepts. Seeking an improved method, this paper employs a novel approach using a generative language model (GPT-4) to produce labels and rationales for large-scale text analysis. We apply this approach to the task of discovering public value expressions in US AI patents. We collect a database comprising 154,934 patent documents using an advanced Boolean query submitted to InnovationQ+. The results are merged with full patent text from the USPTO, resulting in 5.4 million sentences. We design a framework for identifying and labeling public value expressions in these AI patent sentences. A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification. We evaluate the quality of the labels and rationales produced by GPT-4 using BLEU scores and topic modeling and find that they are accurate, diverse, and faithful. These rationales also serve as a chain-of-thought for the model, a transparent mechanism for human verification, and support for human annotators to overcome cognitive limitations. We conclude that GPT-4 achieved a high-level of recognition of public value theory from our framework, which it also uses to discover unseen public value expressions. We use the labels produced by GPT-4 to train BERT-based classifiers and predict sentences on the entire database, achieving high F1 scores for the 3-class (0.85) and 2-class classification (0.91) tasks. We discuss the implications of our approach for conducting large-scale text analyses with complex and abstract concepts and suggest that, with careful framework design and interactive human oversight, generative language models can offer significant advantages in quality and in reduced time and costs for producing labels and rationales.","sentences":["Labeling data is essential for training text classifiers but is often difficult to accomplish accurately, especially for complex and abstract concepts.","Seeking an improved method, this paper employs a novel approach using a generative language model (GPT-4) to produce labels and rationales for large-scale text analysis.","We apply this approach to the task of discovering public value expressions in US AI patents.","We collect a database comprising 154,934 patent documents using an advanced Boolean query submitted to InnovationQ+.","The results are merged with full patent text from the USPTO, resulting in 5.4 million sentences.","We design a framework for identifying and labeling public value expressions in these AI patent sentences.","A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification.","We evaluate the quality of the labels and rationales produced by GPT-4 using BLEU scores and topic modeling and find that they are accurate, diverse, and faithful.","These rationales also serve as a chain-of-thought for the model, a transparent mechanism for human verification, and support for human annotators to overcome cognitive limitations.","We conclude that GPT-4 achieved a high-level of recognition of public value theory from our framework, which it also uses to discover unseen public value expressions.","We use the labels produced by GPT-4 to train BERT-based classifiers and predict sentences on the entire database, achieving high F1 scores for the 3-class (0.85) and 2-class classification (0.91) tasks.","We discuss the implications of our approach for conducting large-scale text analyses with complex and abstract concepts and suggest that, with careful framework design and interactive human oversight, generative language models can offer significant advantages in quality and in reduced time and costs for producing labels and rationales."],"url":"http://arxiv.org/abs/2305.10383v1"}
{"created":"2023-05-17","title":"Confidence-Guided Semi-supervised Learning in Land Cover Classification","abstract":"Semi-supervised learning has been well developed to help reduce the cost of manual labelling by exploiting a large quantity of unlabelled data. Especially in the application of land cover classification, pixel-level manual labelling in large-scale imagery is labour-intensive and expensive. However, the existing semi-supervised learning methods pay limited attention to the quality of pseudo-labels whilst supervising the network. That is, nevertheless, one of the critical factors determining network performance. In order to fill this gap, we develop a confidence-guided semi-supervised learning (CGSSL) approach to make use of high-confidence pseudo labels and reduce the negative effect of low-confidence ones on training the land cover classification network. Meanwhile, the proposed semi-supervised learning approach uses multiple network architectures to increase pseudo-label diversity. The proposed semi-supervised learning approach significantly improves the performance of land cover classification compared to the classical semi-supervised learning methods in computer vision and even outperforms fully supervised learning with a complete set of labelled imagery of the benchmark Potsdam land cover data set.","sentences":["Semi-supervised learning has been well developed to help reduce the cost of manual labelling by exploiting a large quantity of unlabelled data.","Especially in the application of land cover classification, pixel-level manual labelling in large-scale imagery is labour-intensive and expensive.","However, the existing semi-supervised learning methods pay limited attention to the quality of pseudo-labels whilst supervising the network.","That is, nevertheless, one of the critical factors determining network performance.","In order to fill this gap, we develop a confidence-guided semi-supervised learning (CGSSL) approach to make use of high-confidence pseudo labels and reduce the negative effect of low-confidence ones on training the land cover classification network.","Meanwhile, the proposed semi-supervised learning approach uses multiple network architectures to increase pseudo-label diversity.","The proposed semi-supervised learning approach significantly improves the performance of land cover classification compared to the classical semi-supervised learning methods in computer vision and even outperforms fully supervised learning with a complete set of labelled imagery of the benchmark Potsdam land cover data set."],"url":"http://arxiv.org/abs/2305.10344v1"}
{"created":"2023-05-17","title":"Linear Query Approximation Algorithms for Non-monotone Submodular Maximization under Knapsack Constraint","abstract":"This work, for the first time, introduces two constant factor approximation algorithms with linear query complexity for non-monotone submodular maximization over a ground set of size $n$ subject to a knapsack constraint, $\\mathsf{DLA}$ and $\\mathsf{RLA}$. $\\mathsf{DLA}$ is a deterministic algorithm that provides an approximation factor of $6+\\epsilon$ while $\\mathsf{RLA}$ is a randomized algorithm with an approximation factor of $4+\\epsilon$. Both run in $O(n \\log(1/\\epsilon)/\\epsilon)$ query complexity. The key idea to obtain a constant approximation ratio with linear query lies in: (1) dividing the ground set into two appropriate subsets to find the near-optimal solution over these subsets with linear queries, and (2) combining a threshold greedy with properties of two disjoint sets or a random selection process to improve solution quality. In addition to the theoretical analysis, we have evaluated our proposed solutions with three applications: Revenue Maximization, Image Summarization, and Maximum Weighted Cut, showing that our algorithms not only return comparative results to state-of-the-art algorithms but also require significantly fewer queries.","sentences":["This work, for the first time, introduces two constant factor approximation algorithms with linear query complexity for non-monotone submodular maximization over a ground set of size $n$ subject to a knapsack constraint, $\\mathsf{DLA}$ and $\\mathsf{RLA}$. $\\mathsf{DLA}$ is a deterministic algorithm that provides an approximation factor of $6+\\epsilon$ while $\\mathsf{RLA}$ is a randomized algorithm with an approximation factor of $4+\\epsilon$. Both run in $O(n \\log(1/\\epsilon)/\\epsilon)$ query complexity.","The key idea to obtain a constant approximation ratio with linear query lies in: (1) dividing the ground set into two appropriate subsets to find the near-optimal solution over these subsets with linear queries, and (2) combining a threshold greedy with properties of two disjoint sets or a random selection process to improve solution quality.","In addition to the theoretical analysis, we have evaluated our proposed solutions with three applications: Revenue Maximization, Image Summarization, and Maximum Weighted Cut, showing that our algorithms not only return comparative results to state-of-the-art algorithms but also require significantly fewer queries."],"url":"http://arxiv.org/abs/2305.10292v1"}
{"created":"2023-05-17","title":"Large-Scale Package Manipulation via Learned Metrics of Pick Success","abstract":"Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.   This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.   The developed learned pick quality measure ranks various pick alternatives in real-time and prioritizes the most promising ones for execution. The pick success predictor aims to estimate from prior experience the success probability of a desired pick by the deployed industrial robotic arms in cluttered scenes containing deformable and rigid objects with partially known properties. It is a shallow machine learning model, which allows us to evaluate which features are most important for the prediction. An online pick ranker leverages the learned success predictor to prioritize the most promising picks for the robotic arm, which are then assessed for collision avoidance. This learned ranking process is demonstrated to overcome the limitations and outperform the performance of manually engineered and heuristic alternatives.   To the best of the authors' knowledge, this paper presents the first large-scale deployment of learned pick quality estimation methods in a real production system.","sentences":["Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations.","The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings.","Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.   ","This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data.","Specifically, the system was trained on over 394K picks.","It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.   ","The developed learned pick quality measure ranks various pick alternatives in real-time and prioritizes the most promising ones for execution.","The pick success predictor aims to estimate from prior experience the success probability of a desired pick by the deployed industrial robotic arms in cluttered scenes containing deformable and rigid objects with partially known properties.","It is a shallow machine learning model, which allows us to evaluate which features are most important for the prediction.","An online pick ranker leverages the learned success predictor to prioritize the most promising picks for the robotic arm, which are then assessed for collision avoidance.","This learned ranking process is demonstrated to overcome the limitations and outperform the performance of manually engineered and heuristic alternatives.   ","To the best of the authors' knowledge, this paper presents the first large-scale deployment of learned pick quality estimation methods in a real production system."],"url":"http://arxiv.org/abs/2305.10272v1"}
{"created":"2023-05-17","title":"NFI$_2$: Learning Noise-Free Illuminance-Interpolator for Unsupervised Low-Light Image Enhancement","abstract":"Low-light situations severely restrict the pursuit of aesthetic quality in consumer photography. Although many efforts are devoted to designing heuristics, it is generally mired in a shallow spiral of tedium, such as piling up complex network architectures and empirical strategies. How to delve into the essential physical principles of illumination compensation has been neglected. Following the way of simplifying the complexity, this paper innovatively proposes a simple and efficient Noise-Free Illumination Interpolator (NFI$_2$). According to the constraint principle of illuminance and reflectance within a limited dynamic range, as a prior knowledge in the recovery process, we construct a learnable illuminance interpolator and thereby compensating for non-uniform lighting. With the intention of adapting denoising without annotated data, we design a self-calibrated denoiser with the intrinsic image properties to acquire noise-free low-light images. Starting from the properties of natural image manifolds, a self-regularized recovery loss is introduced as a way to encourage more natural and realistic reflectance map. The model architecture and training losses, guided by prior knowledge, complement and benefit each other, forming a powerful unsupervised leaning framework. Comprehensive experiments demonstrate that the proposed algorithm produces competitive qualitative and quantitative results while maintaining favorable generalization capability in unknown real-world scenarios.","sentences":["Low-light situations severely restrict the pursuit of aesthetic quality in consumer photography.","Although many efforts are devoted to designing heuristics, it is generally mired in a shallow spiral of tedium, such as piling up complex network architectures and empirical strategies.","How to delve into the essential physical principles of illumination compensation has been neglected.","Following the way of simplifying the complexity, this paper innovatively proposes a simple and efficient Noise-Free Illumination Interpolator (NFI$_2$).","According to the constraint principle of illuminance and reflectance within a limited dynamic range, as a prior knowledge in the recovery process, we construct a learnable illuminance interpolator and thereby compensating for non-uniform lighting.","With the intention of adapting denoising without annotated data, we design a self-calibrated denoiser with the intrinsic image properties to acquire noise-free low-light images.","Starting from the properties of natural image manifolds, a self-regularized recovery loss is introduced as a way to encourage more natural and realistic reflectance map.","The model architecture and training losses, guided by prior knowledge, complement and benefit each other, forming a powerful unsupervised leaning framework.","Comprehensive experiments demonstrate that the proposed algorithm produces competitive qualitative and quantitative results while maintaining favorable generalization capability in unknown real-world scenarios."],"url":"http://arxiv.org/abs/2305.10223v1"}
{"created":"2023-05-17","title":"DesignTracking: Track and Replay BIM-based Design Process","abstract":"Among different phases of the life cycle of a building or facility, design is of the utmost importance to ensure safety, efficiency and sustainability of the building or facility. How to control and improve design quality and efficiency has been explored for years, and more studies emerged with the popularization of Building Information Modelling (BIM). However, most of them focused on the extraction of design behaviors, while paying less attention to how a design is formed. Therefore, this study proposes an approach to tracking and replaying the BIM-based design process by integrating data logging and 4D visualization techniques. First of all, potential design behaviors and procedures are analyzed and extracted by observing how a designer designs a BIM model. Meanwhile, the required data for logging design process is defined and a relevant method to collect these data is developed based on the APIs of BIM software. Then, strategies on how to visualize different design procedures are designed and implemented via 4D visualization. Finally, a prototype system is developed based on Autodesk Revit and validated through a case study. Result shows that the proposed approach enables intuitively and interactively review of the design process, and makes it easier to understand design behaviors and even identify potential pitfalls, thus improving the design efficiency and quality.","sentences":["Among different phases of the life cycle of a building or facility, design is of the utmost importance to ensure safety, efficiency and sustainability of the building or facility.","How to control and improve design quality and efficiency has been explored for years, and more studies emerged with the popularization of Building Information Modelling (BIM).","However, most of them focused on the extraction of design behaviors, while paying less attention to how a design is formed.","Therefore, this study proposes an approach to tracking and replaying the BIM-based design process by integrating data logging and 4D visualization techniques.","First of all, potential design behaviors and procedures are analyzed and extracted by observing how a designer designs a BIM model.","Meanwhile, the required data for logging design process is defined and a relevant method to collect these data is developed based on the APIs of BIM software.","Then, strategies on how to visualize different design procedures are designed and implemented via 4D visualization.","Finally, a prototype system is developed based on Autodesk Revit and validated through a case study.","Result shows that the proposed approach enables intuitively and interactively review of the design process, and makes it easier to understand design behaviors and even identify potential pitfalls, thus improving the design efficiency and quality."],"url":"http://arxiv.org/abs/2305.10205v1"}
{"created":"2023-05-17","title":"An improved Compton parameter map of thermal Sunyaev-Zeldovich effect from Planck PR4 data","abstract":"Taking advantage of the reduced levels of noise and systematics in the data of the latest Planck release (PR4, also known as NPIPE), we construct a new all-sky Compton-$y$ parameter map (hereafter, $y$-map) of the thermal Sunyaev-Zeldovich (SZ) effect from the Planck PR4 data. A tailored Needlet Internal Linear Combination (NILC) pipeline, first validated on detailed sky simulations, is applied to the nine single-frequency Planck PR4 sky maps, ranging from $30$ to $857$ GHz, to produce the PR4 $y$-map over 98% of the sky. Using map comparisons, angular power spectra and one-point statistics we show that the PR4 NILC $y$-map is of improved quality compared to that of the previous PR2 release. The new $y$-map shows reduced levels of large-scale striations associated with $1/f$ noise in the scan direction. Regions near the Galactic plane also show lower residual contamination by Galactic thermal dust emission. At small angular scales, the residual contamination by thermal noise and cosmic infrared background (CIB) emission is found to be reduced by around 7% and 34%, respectively, in the PR4 $y$-map. The PR4 NILC $y$-map is made publicly available for astrophysical and cosmological analyses of the thermal SZ effect.","sentences":["Taking advantage of the reduced levels of noise and systematics in the data of the latest Planck release (PR4, also known as NPIPE), we construct a new all-sky Compton-$y$ parameter map (hereafter, $y$-map) of the thermal Sunyaev-Zeldovich (SZ) effect from the Planck PR4 data.","A tailored Needlet Internal Linear Combination (NILC) pipeline, first validated on detailed sky simulations, is applied to the nine single-frequency Planck PR4 sky maps, ranging from $30$ to $857$ GHz, to produce the PR4 $y$-map over 98% of the sky.","Using map comparisons, angular power spectra and one-point statistics we show that the PR4 NILC $y$-map is of improved quality compared to that of the previous PR2 release.","The new $y$-map shows reduced levels of large-scale striations associated with $1/f$ noise in the scan direction.","Regions near the Galactic plane also show lower residual contamination by Galactic thermal dust emission.","At small angular scales, the residual contamination by thermal noise and cosmic infrared background (CIB) emission is found to be reduced by around 7% and 34%, respectively, in the PR4 $y$-map.","The PR4 NILC $y$-map is made publicly available for astrophysical and cosmological analyses of the thermal SZ effect."],"url":"http://arxiv.org/abs/2305.10193v1"}
{"created":"2023-05-17","title":"Controllable Mind Visual Diffusion Model","abstract":"Brain signal visualization has emerged as an active research area, serving as a critical interface between the human visual system and computer vision models. Although diffusion models have shown promise in analyzing functional magnetic resonance imaging (fMRI) data, including reconstructing high-quality images consistent with original visual stimuli, their accuracy in extracting semantic and silhouette information from brain signals remains limited. In this regard, we propose a novel approach, referred to as Controllable Mind Visual Diffusion Model (CMVDM). CMVDM extracts semantic and silhouette information from fMRI data using attribute alignment and assistant networks. Additionally, a residual block is incorporated to capture information beyond semantic and silhouette features. We then leverage a control model to fully exploit the extracted information for image synthesis, resulting in generated images that closely resemble the visual stimuli in terms of semantics and silhouette. Through extensive experimentation, we demonstrate that CMVDM outperforms existing state-of-the-art methods both qualitatively and quantitatively.","sentences":["Brain signal visualization has emerged as an active research area, serving as a critical interface between the human visual system and computer vision models.","Although diffusion models have shown promise in analyzing functional magnetic resonance imaging (fMRI) data, including reconstructing high-quality images consistent with original visual stimuli, their accuracy in extracting semantic and silhouette information from brain signals remains limited.","In this regard, we propose a novel approach, referred to as Controllable Mind Visual Diffusion Model (CMVDM).","CMVDM extracts semantic and silhouette information from fMRI data using attribute alignment and assistant networks.","Additionally, a residual block is incorporated to capture information beyond semantic and silhouette features.","We then leverage a control model to fully exploit the extracted information for image synthesis, resulting in generated images that closely resemble the visual stimuli in terms of semantics and silhouette.","Through extensive experimentation, we demonstrate that CMVDM outperforms existing state-of-the-art methods both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2305.10135v1"}
