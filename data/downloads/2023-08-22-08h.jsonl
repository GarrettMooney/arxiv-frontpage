{"created":"2023-08-21 17:59:54","title":"CamP: Camera Preconditioning for Neural Radiance Fields","abstract":"Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input -- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.","sentences":["Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes.","However, NeRFs require accurate camera parameters as input -- inaccurate camera parameters result in blurry renderings.","Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates.","Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings.","In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem.","We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization.","Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF.","Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models."],"url":"http://arxiv.org/abs/2308.10902v1"}
{"created":"2023-08-21 17:59:32","title":"Structured World Models from Human Videos","abstract":"We tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at https://human-world-model.github.io","sentences":["We tackle the problem of learning complex, general behaviors directly in the real world.","We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings.","Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data.","Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation.","Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos.","Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision.","We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction.","Videos can be found at https://human-world-model.github.io"],"url":"http://arxiv.org/abs/2308.10901v1"}
{"created":"2023-08-21 17:59:10","title":"TADA! Text to Animatable Digital Avatars","abstract":"We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent alignment between the geometry and the texture, particularly in the face region. To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model. Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process. We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character. Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures. TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language. The code will be public for research purposes.","sentences":["We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines.","Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent alignment between the geometry and the texture, particularly in the face region.","To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model.","Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text.","To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process.","We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character.","Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures.","TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language.","The code will be public for research purposes."],"url":"http://arxiv.org/abs/2308.10899v1"}
{"created":"2023-08-21 17:59:07","title":"Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation","abstract":"We study the problem of few-shot physically-aware articulated mesh generation. By observing an articulated object dataset containing only a few examples, we wish to learn a model that can generate diverse meshes with high visual fidelity and physical validity. Previous mesh generative models either have difficulties in depicting a diverse data space from only a few examples or fail to ensure physical validity of their samples. Regarding the above challenges, we propose two key innovations, including 1) a hierarchical mesh deformation-based generative model based upon the divide-and-conquer philosophy to alleviate the few-shot challenge by borrowing transferrable deformation patterns from large scale rigid meshes and 2) a physics-aware deformation correction scheme to encourage physically plausible generations. We conduct extensive experiments on 6 articulated categories to demonstrate the superiority of our method in generating articulated meshes with better diversity, higher visual fidelity, and better physical validity over previous methods in the few-shot setting. Further, we validate solid contributions of our two innovations in the ablation study. Project page with code is available at https://meowuu7.github.io/few-arti-obj-gen.","sentences":["We study the problem of few-shot physically-aware articulated mesh generation.","By observing an articulated object dataset containing only a few examples, we wish to learn a model that can generate diverse meshes with high visual fidelity and physical validity.","Previous mesh generative models either have difficulties in depicting a diverse data space from only a few examples or fail to ensure physical validity of their samples.","Regarding the above challenges, we propose two key innovations, including 1) a hierarchical mesh deformation-based generative model based upon the divide-and-conquer philosophy to alleviate the few-shot challenge by borrowing transferrable deformation patterns from large scale rigid meshes and 2) a physics-aware deformation correction scheme to encourage physically plausible generations.","We conduct extensive experiments on 6 articulated categories to demonstrate the superiority of our method in generating articulated meshes with better diversity, higher visual fidelity, and better physical validity over previous methods in the few-shot setting.","Further, we validate solid contributions of our two innovations in the ablation study.","Project page with code is available at https://meowuu7.github.io/few-arti-obj-gen."],"url":"http://arxiv.org/abs/2308.10898v1"}
{"created":"2023-08-21 17:59:02","title":"Can Language Models Learn to Listen?","abstract":"We present a framework for generating appropriate facial responses from a listener in dyadic social interactions based on the speaker's words. Given an input transcription of the speaker's words with their timestamps, our approach autoregressively predicts a response of a listener: a sequence of listener facial gestures, quantized using a VQ-VAE. Since gesture is a language component, we propose treating the quantized atomic motion elements as additional language token inputs to a transformer-based large language model. Initializing our transformer with the weights of a language model pre-trained only on text results in significantly higher quality listener responses than training a transformer from scratch. We show that our generated listener motion is fluent and reflective of language semantics through quantitative metrics and a qualitative user study. In our evaluation, we analyze the model's ability to utilize temporal and semantic aspects of spoken text. Project page: https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/","sentences":["We present a framework for generating appropriate facial responses from a listener in dyadic social interactions based on the speaker's words.","Given an input transcription of the speaker's words with their timestamps, our approach autoregressively predicts a response of a listener: a sequence of listener facial gestures, quantized using a VQ-VAE.","Since gesture is a language component, we propose treating the quantized atomic motion elements as additional language token inputs to a transformer-based large language model.","Initializing our transformer with the weights of a language model pre-trained only on text results in significantly higher quality listener responses than training a transformer from scratch.","We show that our generated listener motion is fluent and reflective of language semantics through quantitative metrics and a qualitative user study.","In our evaluation, we analyze the model's ability to utilize temporal and semantic aspects of spoken text.","Project page: https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/"],"url":"http://arxiv.org/abs/2308.10897v1"}
{"created":"2023-08-21 17:58:43","title":"Differentiable Shadow Mapping for Efficient Inverse Graphics","abstract":"We show how shadows can be efficiently generated in differentiable rendering of triangle meshes. Our central observation is that pre-filtered shadow mapping, a technique for approximating shadows based on rendering from the perspective of a light, can be combined with existing differentiable rasterizers to yield differentiable visibility information. We demonstrate at several inverse graphics problems that differentiable shadow maps are orders of magnitude faster than differentiable light transport simulation with similar accuracy -- while differentiable rasterization without shadows often fails to converge.","sentences":["We show how shadows can be efficiently generated in differentiable rendering of triangle meshes.","Our central observation is that pre-filtered shadow mapping, a technique for approximating shadows based on rendering from the perspective of a light, can be combined with existing differentiable rasterizers to yield differentiable visibility information.","We demonstrate at several inverse graphics problems that differentiable shadow maps are orders of magnitude faster than differentiable light transport simulation with similar accuracy -- while differentiable rasterization without shadows often fails to converge."],"url":"http://arxiv.org/abs/2308.10896v1"}
{"created":"2023-08-21 17:42:33","title":"Unlocking Accuracy and Fairness in Differentially Private Image Classification","abstract":"Privacy-preserving machine learning aims to train models on private data without leaking sensitive information. Differential privacy (DP) is considered the gold standard framework for privacy-preserving training, as it provides formal privacy guarantees. However, compared to their non-private counterparts, models trained with DP often have significantly reduced accuracy. Private classifiers are also believed to exhibit larger performance disparities across subpopulations, raising fairness concerns. The poor performance of classifiers trained with DP has prevented the widespread adoption of privacy preserving machine learning in industry. Here we show that pre-trained foundation models fine-tuned with DP can achieve similar accuracy to non-private classifiers, even in the presence of significant distribution shifts between pre-training data and downstream tasks. We achieve private accuracies within a few percent of the non-private state of the art across four datasets, including two medical imaging benchmarks. Furthermore, our private medical classifiers do not exhibit larger performance disparities across demographic groups than non-private models. This milestone to make DP training a practical and reliable technology has the potential to widely enable machine learning practitioners to train safely on sensitive datasets while protecting individuals' privacy.","sentences":["Privacy-preserving machine learning aims to train models on private data without leaking sensitive information.","Differential privacy (DP) is considered the gold standard framework for privacy-preserving training, as it provides formal privacy guarantees.","However, compared to their non-private counterparts, models trained with DP often have significantly reduced accuracy.","Private classifiers are also believed to exhibit larger performance disparities across subpopulations, raising fairness concerns.","The poor performance of classifiers trained with DP has prevented the widespread adoption of privacy preserving machine learning in industry.","Here we show that pre-trained foundation models fine-tuned with DP can achieve similar accuracy to non-private classifiers, even in the presence of significant distribution shifts between pre-training data and downstream tasks.","We achieve private accuracies within a few percent of the non-private state of the art across four datasets, including two medical imaging benchmarks.","Furthermore, our private medical classifiers do not exhibit larger performance disparities across demographic groups than non-private models.","This milestone to make DP training a practical and reliable technology has the potential to widely enable machine learning practitioners to train safely on sensitive datasets while protecting individuals' privacy."],"url":"http://arxiv.org/abs/2308.10888v1"}
{"created":"2023-08-21 17:30:38","title":"Quantum Symmetric Private Information Retrieval with Secure Storage and Eavesdroppers","abstract":"We consider both the classical and quantum variations of $X$-secure, $E$-eavesdropped and $T$-colluding symmetric private information retrieval (SPIR). This is the first work to study SPIR with $X$-security in classical or quantum variations. We first develop a scheme for classical $X$-secure, $E$-eavesdropped and $T$-colluding SPIR (XSETSPIR) based on a modified version of cross subspace alignment (CSA), which achieves a rate of $R= 1 - \\frac{X+\\max(T,E)}{N}$. The modified scheme achieves the same rate as the scheme used for $X$-secure PIR with the extra benefit of symmetric privacy. Next, we extend this scheme to its quantum counterpart based on the $N$-sum box abstraction. This is the first work to consider the presence of eavesdroppers in quantum private information retrieval (QPIR). In the quantum variation, the eavesdroppers have better access to information over the quantum channel compared to the classical channel due to the over-the-air decodability. To that end, we develop another scheme specialized to combat eavesdroppers over quantum channels. The scheme proposed for $X$-secure, $E$-eavesdropped and $T$-colluding quantum SPIR (XSETQSPIR) in this work maintains the super-dense coding gain from the shared entanglement between the databases, i.e., achieves a rate of $R_Q = \\min\\left\\{ 1, 2\\left(1-\\frac{X+\\max(T,E)}{N}\\right)\\right\\}$.","sentences":["We consider both the classical and quantum variations of $X$-secure, $E$-eavesdropped and $T$-colluding symmetric private information retrieval (SPIR).","This is the first work to study SPIR with $X$-security in classical or quantum variations.","We first develop a scheme for classical $X$-secure, $E$-eavesdropped and $T$-colluding SPIR (XSETSPIR) based on a modified version of cross subspace alignment (CSA), which achieves a rate of $R= 1 - \\frac{X+\\max(T,E)}{N}$.","The modified scheme achieves the same rate as the scheme used for $X$-secure PIR with the extra benefit of symmetric privacy.","Next, we extend this scheme to its quantum counterpart based on the $N$-sum box abstraction.","This is the first work to consider the presence of eavesdroppers in quantum private information retrieval (QPIR).","In the quantum variation, the eavesdroppers have better access to information over the quantum channel compared to the classical channel due to the over-the-air decodability.","To that end, we develop another scheme specialized to combat eavesdroppers over quantum channels.","The scheme proposed for $X$-secure, $E$-eavesdropped and $T$-colluding quantum SPIR (XSETQSPIR) in this work maintains the super-dense coding gain from the shared entanglement between the databases, i.e., achieves a rate of $R_Q = \\min\\left\\{ 1, 2\\left(1-\\frac{X+\\max(T,E)}{N}\\right)\\right\\}$."],"url":"http://arxiv.org/abs/2308.10883v1"}
{"created":"2023-08-21 17:30:16","title":"Giraffe: Adventures in Expanding Context Lengths in LLMs","abstract":"Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time. To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence. We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well -- in particular, a new truncation strategy for modifying the basis for the position encoding.   We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities in the truncated basis. To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results.","sentences":["Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time.","To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence.","We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well -- in particular, a new truncation strategy for modifying the basis for the position encoding.   ","We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs.","We release the three tasks publicly as datasets on HuggingFace.","We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time.","We also discover promising extrapolation capabilities in the truncated basis.","To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results."],"url":"http://arxiv.org/abs/2308.10882v1"}
{"created":"2023-08-21 17:21:23","title":"Analyzing Transformer Dynamics as Movement through Embedding Space","abstract":"Transformer language models exhibit intelligent behaviors such as understanding natural language, recognizing patterns, acquiring knowledge, reasoning, planning, reflecting and using tools. This paper explores how their underlying mechanics give rise to intelligent behaviors. We adopt a systems approach to analyze Transformers in detail and develop a mathematical framework that frames their dynamics as movement through embedding space. This novel perspective provides a principled way of thinking about the problem and reveals important insights related to the emergence of intelligence:   1. At its core the Transformer is a Embedding Space walker, mapping intelligent behavior to trajectories in this vector space.   2. At each step of the walk, it composes context into a single composite vector whose location in Embedding Space defines the next step.   3. No learning actually occurs during decoding; in-context learning and generalization are simply the result of different contexts composing into different vectors.   4. Ultimately the knowledge, intelligence and skills exhibited by the model are embodied in the organization of vectors in Embedding Space rather than in specific neurons or layers. These abilities are properties of this organization.   5. Attention's contribution boils down to the association-bias it lends to vector composition and which influences the aforementioned organization. However, more investigation is needed to ascertain its significance.   6. The entire model is composed from two principal operations: data independent filtering and data dependent aggregation. This generalization unifies Transformers with other sequence models and across modalities.   Building upon this foundation we formalize and test a semantic space theory which posits that embedding vectors represent semantic concepts and find some evidence of its validity.","sentences":["Transformer language models exhibit intelligent behaviors such as understanding natural language, recognizing patterns, acquiring knowledge, reasoning, planning, reflecting and using tools.","This paper explores how their underlying mechanics give rise to intelligent behaviors.","We adopt a systems approach to analyze Transformers in detail and develop a mathematical framework that frames their dynamics as movement through embedding space.","This novel perspective provides a principled way of thinking about the problem and reveals important insights related to the emergence of intelligence:   1.","At its core the Transformer is a Embedding Space walker, mapping intelligent behavior to trajectories in this vector space.   ","2.","At each step of the walk, it composes context into a single composite vector whose location in Embedding Space defines the next step.   ","3.","No learning actually occurs during decoding; in-context learning and generalization are simply the result of different contexts composing into different vectors.   ","4.","Ultimately the knowledge, intelligence and skills exhibited by the model are embodied in the organization of vectors in Embedding Space rather than in specific neurons or layers.","These abilities are properties of this organization.   ","5.","Attention's contribution boils down to the association-bias it lends to vector composition and which influences the aforementioned organization.","However, more investigation is needed to ascertain its significance.   ","6.","The entire model is composed from two principal operations: data independent filtering and data dependent aggregation.","This generalization unifies Transformers with other sequence models and across modalities.   ","Building upon this foundation we formalize and test a semantic space theory which posits that embedding vectors represent semantic concepts and find some evidence of its validity."],"url":"http://arxiv.org/abs/2308.10874v1"}
{"created":"2023-08-21 17:20:05","title":"SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation","abstract":"Large language Models (LLMs), though growing exceedingly powerful, comprises of orders of magnitude less neurons and synapses than the human brain. However, it requires significantly more power/energy to operate. In this work, we propose a novel bio-inspired spiking language model (LM) which aims to reduce the computational cost of conventional LMs by drawing motivation from the synaptic information flow in the brain. In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient. The steady-state convergence of the spiking neurons also allows us to design a spiking attention mechanism, which is critical in developing a scalable spiking LM. Moreover, the convergence of average spiking rate of neurons at equilibrium is utilized to develop a novel ANN-SNN knowledge distillation based technique wherein we use a pre-trained BERT model as \"teacher\" to train our \"student\" spiking architecture. While the primary architecture proposed in this paper is motivated by BERT, the technique can be potentially extended to different kinds of LLMs. Our work is the first one to demonstrate the performance of an operational spiking LM architecture on multiple different tasks in the GLUE benchmark.","sentences":["Large language Models (LLMs), though growing exceedingly powerful, comprises of orders of magnitude less neurons and synapses than the human brain.","However, it requires significantly more power/energy to operate.","In this work, we propose a novel bio-inspired spiking language model (LM) which aims to reduce the computational cost of conventional LMs by drawing motivation from the synaptic information flow in the brain.","In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient.","The steady-state convergence of the spiking neurons also allows us to design a spiking attention mechanism, which is critical in developing a scalable spiking LM.","Moreover, the convergence of average spiking rate of neurons at equilibrium is utilized to develop a novel ANN-SNN knowledge distillation based technique wherein we use a pre-trained BERT model as \"teacher\" to train our \"student\" spiking architecture.","While the primary architecture proposed in this paper is motivated by BERT, the technique can be potentially extended to different kinds of LLMs.","Our work is the first one to demonstrate the performance of an operational spiking LM architecture on multiple different tasks in the GLUE benchmark."],"url":"http://arxiv.org/abs/2308.10873v1"}
{"created":"2023-08-21 17:04:23","title":"Characterization of random walks on space of unordered trees using efficient metric simulation","abstract":"The simple random walk on $\\mathbb{Z}^p$ shows two drastically different behaviours depending on the value of $p$: it is recurrent when $p\\in\\{1,2\\}$ while it escapes (with a rate increasing with $p$) as soon as $p\\geq3$. This classical example illustrates that the asymptotic properties of a random walk provides some information on the structure of its state space. This paper aims to explore analogous questions on space made up of combinatorial objects with no algebraic structure. We take as a model for this problem the space of unordered unlabeled rooted trees endowed with Zhang edit distance. To this end, it defines the canonical unbiased random walk on the space of trees and provides an efficient algorithm to evaluate its escape rate. Compared to Zhang algorithm, it is incremental and computes the edit distance along the random walk approximately 100 times faster on trees of size $500$ on average. The escape rate of the random walk on trees is precisely estimated using intensive numerical simulations, out of reasonable reach without the incremental algorithm.","sentences":["The simple random walk on $\\mathbb{Z}^p$ shows two drastically different behaviours depending on the value of $p$: it is recurrent when $p\\in\\{1,2\\}$ while it escapes (with a rate increasing with $p$) as soon as $p\\geq3$. This classical example illustrates that the asymptotic properties of a random walk provides some information on the structure of its state space.","This paper aims to explore analogous questions on space made up of combinatorial objects with no algebraic structure.","We take as a model for this problem the space of unordered unlabeled rooted trees endowed with Zhang edit distance.","To this end, it defines the canonical unbiased random walk on the space of trees and provides an efficient algorithm to evaluate its escape rate.","Compared to Zhang algorithm, it is incremental and computes the edit distance along the random walk approximately 100 times faster on trees of size $500$ on average.","The escape rate of the random walk on trees is precisely estimated using intensive numerical simulations, out of reasonable reach without the incremental algorithm."],"url":"http://arxiv.org/abs/2308.10861v1"}
{"created":"2023-08-21 16:56:14","title":"Compliant Mechanism Synthesis Using Nonlinear Elastic Topology Optimization with Variable Boundary Conditions","abstract":"In topology optimization of compliant mechanisms, the specific placement of boundary conditions strongly affects the resulting material distribution and performance of the design. At the same time, the most effective locations of the loads and supports are often difficult to find manually. This substantially limits topology optimization's effectiveness for many mechanism design problems. We remove this limitation by developing a method which automatically determines optimal positioning of a prescribed input displacement and a set of supports simultaneously with an optimal material layout. Using nonlinear elastic physics, we synthesize a variety of compliant mechanisms with large output displacements, snap-through responses, and prescribed output paths, producing designs with significantly improved performance in every case tested. Compared to optimal designs generated using best-guess boundary conditions used in previous studies, the mechanisms presented in this paper see performance increases ranging from 23%-430%. The results show that nonlinear mechanism responses may be particularly sensitive to boundary condition locations and that effective placements can be difficult to find without an automated method.","sentences":["In topology optimization of compliant mechanisms, the specific placement of boundary conditions strongly affects the resulting material distribution and performance of the design.","At the same time, the most effective locations of the loads and supports are often difficult to find manually.","This substantially limits topology optimization's effectiveness for many mechanism design problems.","We remove this limitation by developing a method which automatically determines optimal positioning of a prescribed input displacement and a set of supports simultaneously with an optimal material layout.","Using nonlinear elastic physics, we synthesize a variety of compliant mechanisms with large output displacements, snap-through responses, and prescribed output paths, producing designs with significantly improved performance in every case tested.","Compared to optimal designs generated using best-guess boundary conditions used in previous studies, the mechanisms presented in this paper see performance increases ranging from 23%-430%.","The results show that nonlinear mechanism responses may be particularly sensitive to boundary condition locations and that effective placements can be difficult to find without an automated method."],"url":"http://arxiv.org/abs/2308.10858v1"}
{"created":"2023-08-21 16:50:59","title":"Majorana Demonstrator Data Release for AI/ML Applications","abstract":"The enclosed data release consists of a subset of the calibration data from the Majorana Demonstrator experiment. Each Majorana event is accompanied by raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, all shared in an HDF5 file format along with relevant metadata. This release is specifically designed to support the training and testing of Artificial Intelligence (AI) and Machine Learning (ML) algorithms upon our data. This document is structured as follows. Section I provides an overview of the dataset's content and format; Section II outlines the location of this dataset and the method for accessing it; Section III presents the NPML Machine Learning Challenge associated with this dataset; Section IV contains a disclaimer from the Majorana collaboration regarding the use of this dataset; Appendix A contains technical details of this data release. Please direct questions about the material provided within this release to liaobo77@ucsd.edu (A. Li).","sentences":["The enclosed data release consists of a subset of the calibration data from the Majorana Demonstrator experiment.","Each Majorana event is accompanied by raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, all shared in an HDF5 file format along with relevant metadata.","This release is specifically designed to support the training and testing of Artificial Intelligence (AI) and Machine Learning (ML) algorithms upon our data.","This document is structured as follows.","Section I provides an overview of the dataset's content and format; Section II outlines the location of this dataset and the method for accessing it; Section III presents the NPML Machine Learning Challenge associated with this dataset; Section IV contains a disclaimer from the Majorana collaboration regarding the use of this dataset; Appendix A contains technical details of this data release.","Please direct questions about the material provided within this release to liaobo77@ucsd.edu (A. Li)."],"url":"http://arxiv.org/abs/2308.10856v1"}
{"created":"2023-08-21 16:49:40","title":"LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles","abstract":"With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant.","sentences":["With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities.","But can they think out of the box?","Do they possess proficient lateral thinking abilities?","Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework.","In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving.","We find that nearly all LLMs struggle with employing lateral thinking during interactions.","For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human.","This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant."],"url":"http://arxiv.org/abs/2308.10855v1"}
{"created":"2023-08-21 16:48:55","title":"Uncertainty benchmarks for time-dependent transport problems","abstract":"Uncertainty quantification results are presented for a well known verification solution, the time dependent transport infinite plane pulse. The method of polynomial chaos expansions (PCE) is employed for quick and accurate calculation of the quantities of interest. Also, the method of uncollided solutions is used in this problem to treat part of the uncertainty calculation analytically.","sentences":["Uncertainty quantification results are presented for a well known verification solution, the time dependent transport infinite plane pulse.","The method of polynomial chaos expansions (PCE) is employed for quick and accurate calculation of the quantities of interest.","Also, the method of uncollided solutions is used in this problem to treat part of the uncertainty calculation analytically."],"url":"http://arxiv.org/abs/2308.10852v1"}
{"created":"2023-08-21 16:47:11","title":"AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents","abstract":"Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \\framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \\framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \\framework will soon be released at \\url{https://github.com/OpenBMB/AgentVerse}.","sentences":["Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks.","However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment.","Hence, inspired by human group dynamics, we propose a multi-agent framework \\framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system.","Our experiments demonstrate that \\framework framework can effectively deploy multi-agent groups that outperform a single agent.","Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment.","In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups.","Our codes for \\framework will soon be released at \\url{https://github.com/OpenBMB/AgentVerse}."],"url":"http://arxiv.org/abs/2308.10848v1"}
{"created":"2023-08-21 16:44:56","title":"Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility","abstract":"The scarcity of task-labeled time-series benchmarks in the financial domain hinders progress in continual learning. Addressing this deficit would foster innovation in this area. Therefore, we present COB, Crude Oil Benchmark datasets. COB includes 30 years of asset prices that exhibit significant distribution shifts and optimally generates corresponding task (i.e., regime) labels based on these distribution shifts for the three most important crude oils in the world. Our contributions include creating real-world benchmark datasets by transforming asset price data into volatility proxies, fitting models using expectation-maximization (EM), generating contextual task labels that align with real-world events, and providing these labels as well as the general algorithm to the public. We show that the inclusion of these task labels universally improves performance on four continual learning algorithms, some state-of-the-art, over multiple forecasting horizons. We hope these benchmarks accelerate research in handling distribution shifts in real-world data, especially due to the global importance of the assets considered. We've made the (1) raw price data, (2) task labels generated by our approach, (3) and code for our algorithm available at https://oilpricebenchmarks.github.io.","sentences":["The scarcity of task-labeled time-series benchmarks in the financial domain hinders progress in continual learning.","Addressing this deficit would foster innovation in this area.","Therefore, we present COB, Crude Oil Benchmark datasets.","COB includes 30 years of asset prices that exhibit significant distribution shifts and optimally generates corresponding task (i.e., regime) labels based on these distribution shifts for the three most important crude oils in the world.","Our contributions include creating real-world benchmark datasets by transforming asset price data into volatility proxies, fitting models using expectation-maximization (EM), generating contextual task labels that align with real-world events, and providing these labels as well as the general algorithm to the public.","We show that the inclusion of these task labels universally improves performance on four continual learning algorithms, some state-of-the-art, over multiple forecasting horizons.","We hope these benchmarks accelerate research in handling distribution shifts in real-world data, especially due to the global importance of the assets considered.","We've made the (1) raw price data, (2) task labels generated by our approach, (3) and code for our algorithm available at https://oilpricebenchmarks.github.io."],"url":"http://arxiv.org/abs/2308.10846v1"}
{"created":"2023-08-21 16:44:50","title":"Election Manipulation in Social Networks with Single-Peaked Agents","abstract":"Several elections run in the last years have been characterized by attempts to manipulate the result of the election through the diffusion of fake or malicious news over social networks. This problem has been recognized as a critical issue for the robustness of our democracy. Analyzing and understanding how such manipulations may occur is crucial to the design of effective countermeasures to these practices.   Many studies have observed that, in general, to design an optimal manipulation is usually a computationally hard task. Nevertheless, literature on bribery in voting and election manipulation has frequently observed that most hardness results melt down when one focuses on the setting of (nearly) single-peaked agents, i.e., when each voter has a preferred candidate (usually, the one closer to her own belief) and preferences of remaining candidates are inversely proportional to the distance between the candidate position and the voter's belief. Unfortunately, no such analysis has been done for election manipulations run in social networks.   In this work, we try to close this gap: specifically, we consider a setting for election manipulation that naturally raises (nearly) single-peaked preferences, and we evaluate the complexity of election manipulation problem in this setting: while most of the hardness and approximation results still hold, we will show that single-peaked preferences allow to design simple, efficient and effective heuristics for election manipulation.","sentences":["Several elections run in the last years have been characterized by attempts to manipulate the result of the election through the diffusion of fake or malicious news over social networks.","This problem has been recognized as a critical issue for the robustness of our democracy.","Analyzing and understanding how such manipulations may occur is crucial to the design of effective countermeasures to these practices.   ","Many studies have observed that, in general, to design an optimal manipulation is usually a computationally hard task.","Nevertheless, literature on bribery in voting and election manipulation has frequently observed that most hardness results melt down when one focuses on the setting of (nearly) single-peaked agents, i.e., when each voter has a preferred candidate (usually, the one closer to her own belief) and preferences of remaining candidates are inversely proportional to the distance between the candidate position and the voter's belief.","Unfortunately, no such analysis has been done for election manipulations run in social networks.   ","In this work, we try to close this gap: specifically, we consider a setting for election manipulation that naturally raises (nearly) single-peaked preferences, and we evaluate the complexity of election manipulation problem in this setting: while most of the hardness and approximation results still hold, we will show that single-peaked preferences allow to design simple, efficient and effective heuristics for election manipulation."],"url":"http://arxiv.org/abs/2308.10845v1"}
{"created":"2023-08-21 16:40:51","title":"Vision Transformer Pruning Via Matrix Decomposition","abstract":"This is a further development of Vision Transformer Pruning via matrix decomposition. The purpose of the Vision Transformer Pruning is to prune the dimension of the linear projection of the dataset by learning their associated importance score in order to reduce the storage, run-time memory, and computational demands. In this paper we further reduce dimension and complexity of the linear projection by implementing and comparing several matrix decomposition methods while preserving the generated important features. We end up selected the Singular Value Decomposition as the method to achieve our goal by comparing the original accuracy scores in the original Github repository and the accuracy scores of using those matrix decomposition methods, including Singular Value Decomposition, four versions of QR Decomposition, and LU factorization.","sentences":["This is a further development of Vision Transformer Pruning via matrix decomposition.","The purpose of the Vision Transformer Pruning is to prune the dimension of the linear projection of the dataset by learning their associated importance score in order to reduce the storage, run-time memory, and computational demands.","In this paper we further reduce dimension and complexity of the linear projection by implementing and comparing several matrix decomposition methods while preserving the generated important features.","We end up selected the Singular Value Decomposition as the method to achieve our goal by comparing the original accuracy scores in the original Github repository and the accuracy scores of using those matrix decomposition methods, including Singular Value Decomposition, four versions of QR Decomposition, and LU factorization."],"url":"http://arxiv.org/abs/2308.10839v1"}
{"created":"2023-08-21 16:40:27","title":"An impossibility result for Markov Chain Monte Carlo sampling from micro-canonical bipartite graph ensembles","abstract":"Markov Chain Monte Carlo (MCMC) algorithms are commonly used to sample from graph ensembles. Two graphs are neighbors in the state space if one can be obtained from the other with only a few modifications, e.g., edge rewirings. For many common ensembles, e.g., those preserving the degree sequences of bipartite graphs, rewiring operations involving two edges are sufficient to create a fully-connected state space, and they can be performed efficiently. We show that, for ensembles of bipartite graphs with fixed degree sequences and number of butterflies (k2,2 bi-cliques), there is no universal constant c such that a rewiring of at most c edges at every step is sufficient for any such ensemble to be fully connected. Our proof relies on an explicit construction of a family of pairs of graphs with the same degree sequences and number of butterflies, with each pair indexed by a natural c, and such that any sequence of rewiring operations transforming one graph into the other must include at least one rewiring operation involving at least c edges. Whether rewiring these many edges is sufficient to guarantee the full connectivity of the state space of any such ensemble remains an open question. Our result implies the impossibility of developing efficient, graph-agnostic, MCMC algorithms for these ensembles, as the necessity to rewire an impractically large number of edges may hinder taking a step on the state space.","sentences":["Markov Chain Monte Carlo (MCMC) algorithms are commonly used to sample from graph ensembles.","Two graphs are neighbors in the state space if one can be obtained from the other with only a few modifications, e.g., edge rewirings.","For many common ensembles, e.g., those preserving the degree sequences of bipartite graphs, rewiring operations involving two edges are sufficient to create a fully-connected state space, and they can be performed efficiently.","We show that, for ensembles of bipartite graphs with fixed degree sequences and number of butterflies (k2,2 bi-cliques), there is no universal constant c such that a rewiring of at most c edges at every step is sufficient for any such ensemble to be fully connected.","Our proof relies on an explicit construction of a family of pairs of graphs with the same degree sequences and number of butterflies, with each pair indexed by a natural c, and such that any sequence of rewiring operations transforming one graph into the other must include at least one rewiring operation involving at least c edges.","Whether rewiring these many edges is sufficient to guarantee the full connectivity of the state space of any such ensemble remains an open question.","Our result implies the impossibility of developing efficient, graph-agnostic, MCMC algorithms for these ensembles, as the necessity to rewire an impractically large number of edges may hinder taking a step on the state space."],"url":"http://arxiv.org/abs/2308.10838v1"}
{"created":"2023-08-21 16:39:11","title":"Leveraging Large Language Models for Pre-trained Recommender Systems","abstract":"Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM). However, effectively integrating LLM's commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem. In this paper, we propose RecSysLLM, a novel pre-trained recommendation model based on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data, training, and inference. This allows RecSysLLM to leverage LLMs' capabilities for recommendation tasks in an efficient, unified framework. We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models.","sentences":["Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM).","However, effectively integrating LLM's commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem.","In this paper, we propose RecSysLLM, a novel pre-trained recommendation model based on LLMs.","RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data, training, and inference.","This allows RecSysLLM to leverage LLMs' capabilities for recommendation tasks in an efficient, unified framework.","We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios.","RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models."],"url":"http://arxiv.org/abs/2308.10837v1"}
{"created":"2023-08-21 16:35:19","title":"Enhancing Recommender Systems with Large Language Model Reasoning Graphs","abstract":"Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement. The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information. Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs. LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs. We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models.","sentences":["Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles.","In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs.","These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way.","Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement.","The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information.","Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs.","LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs.","We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models."],"url":"http://arxiv.org/abs/2308.10835v1"}
{"created":"2023-08-21 16:32:11","title":"SRSS: A New Chaos-Based Single-Round Single S-Box Image Encryption Scheme for Highly Auto-Correlated Data","abstract":"With the advent of digital communication, securing digital images during transmission and storage has become a critical concern. The traditional s-box substitution methods often fail to effectively conceal the information within highly auto-correlated regions of an image. This paper addresses the security issues presented by three prevalent S-box substitution methods, i.e., single S-box, multiple S-boxes, and multiple rounds with multiple S-boxes, especially when handling images with highly auto-correlated pixels. To resolve the addressed security issues, this paper proposes a new scheme SRSS-the Single Round Single S-Box encryption scheme. SRSS uses a single S-box for substitution in just one round to break the pixel correlations and encrypt the plaintext image effectively. Additionally, this paper introduces a new Chaos-based Random Operation Selection System-CROSS, which nullifies the requirement for multiple S-boxes, thus reducing the encryption scheme's complexity. By randomly selecting the operation to be performed on each pixel, driven by a chaotic sequence, the proposed scheme effectively scrambles even high auto-correlation areas. When compared to the substitution methods mentioned above, the proposed encryption scheme exhibited exceptionally well in just a single round with a single S-box. The close-to-ideal statistical security analysis results, i.e., an entropy of 7.89 and a correlation coefficient of 0.007, validate the effectiveness of the proposed scheme. This research offers an innovative path forward for securing images in applications requiring low computational complexity and fast encryption and decryption speeds.","sentences":["With the advent of digital communication, securing digital images during transmission and storage has become a critical concern.","The traditional s-box substitution methods often fail to effectively conceal the information within highly auto-correlated regions of an image.","This paper addresses the security issues presented by three prevalent S-box substitution methods, i.e., single S-box, multiple S-boxes, and multiple rounds with multiple S-boxes, especially when handling images with highly auto-correlated pixels.","To resolve the addressed security issues, this paper proposes a new scheme SRSS-the Single Round Single S-Box encryption scheme.","SRSS uses a single S-box for substitution in just one round to break the pixel correlations and encrypt the plaintext image effectively.","Additionally, this paper introduces a new Chaos-based Random Operation Selection System-CROSS, which nullifies the requirement for multiple S-boxes, thus reducing the encryption scheme's complexity.","By randomly selecting the operation to be performed on each pixel, driven by a chaotic sequence, the proposed scheme effectively scrambles even high auto-correlation areas.","When compared to the substitution methods mentioned above, the proposed encryption scheme exhibited exceptionally well in just a single round with a single S-box.","The close-to-ideal statistical security analysis results, i.e., an entropy of 7.89 and a correlation coefficient of 0.007, validate the effectiveness of the proposed scheme.","This research offers an innovative path forward for securing images in applications requiring low computational complexity and fast encryption and decryption speeds."],"url":"http://arxiv.org/abs/2308.10834v1"}
{"created":"2023-08-21 16:27:31","title":"EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition","abstract":"Visual Place Recognition is a task that aims to predict the place of an image (called query) based solely on its visual features. This is typically done through image retrieval, where the query is matched to the most similar images from a large database of geotagged photos, using learned global descriptors. A major challenge in this task is recognizing places seen from different viewpoints. To overcome this limitation, we propose a new method, called EigenPlaces, to train our neural network on images from different point of views, which embeds viewpoint robustness into the learned global descriptors. The underlying idea is to cluster the training data so as to explicitly present the model with different views of the same points of interest. The selection of this points of interest is done without the need for extra supervision. We then present experiments on the most comprehensive set of datasets in literature, finding that EigenPlaces is able to outperform previous state of the art on the majority of datasets, while requiring 60\\% less GPU memory for training and using 50\\% smaller descriptors. The code and trained models for EigenPlaces are available at {\\small{\\url{https://github.com/gmberton/EigenPlaces}}}, while results with any other baseline can be computed with the codebase at {\\small{\\url{https://github.com/gmberton/auto_VPR}}}.","sentences":["Visual Place Recognition is a task that aims to predict the place of an image (called query) based solely on its visual features.","This is typically done through image retrieval, where the query is matched to the most similar images from a large database of geotagged photos, using learned global descriptors.","A major challenge in this task is recognizing places seen from different viewpoints.","To overcome this limitation, we propose a new method, called EigenPlaces, to train our neural network on images from different point of views, which embeds viewpoint robustness into the learned global descriptors.","The underlying idea is to cluster the training data so as to explicitly present the model with different views of the same points of interest.","The selection of this points of interest is done without the need for extra supervision.","We then present experiments on the most comprehensive set of datasets in literature, finding that EigenPlaces is able to outperform previous state of the art on the majority of datasets, while requiring 60\\% less GPU memory for training and using 50\\% smaller descriptors.","The code and trained models for EigenPlaces are available at {\\small{\\url{https://github.com/gmberton/EigenPlaces}}}, while results with any other baseline can be computed with the codebase at {\\small{\\url{https://github.com/gmberton/auto_VPR}}}."],"url":"http://arxiv.org/abs/2308.10832v1"}
{"created":"2023-08-21 16:24:15","title":"A Large-scale Benchmark for Log Parsing","abstract":"Log data is pivotal in activities like anomaly detection and failure diagnosis in the automated maintenance of software systems. Due to their unstructured format, log parsing is often required to transform them into a structured format for automated analysis. A variety of log parsers exist, making it vital to benchmark these tools to comprehend their features and performance. However, existing datasets for log parsing are limited in terms of scale and representativeness, posing challenges for studies that aim to evaluate or develop log parsers. This problem becomes more pronounced when these parsers are evaluated for production use. To address these issues, we introduce a new collection of large-scale annotated log datasets, named LogPub, which more accurately mirrors log data observed in real-world software systems. LogPub comprises 14 datasets, each averaging 3.6 million log lines. Utilizing LogPub, we re-evaluate 15 log parsers in a more rigorous and practical setting. We also propose a new evaluation metric to lessen the sensitivity of current metrics to imbalanced data distribution. Furthermore, we are the first to scrutinize the detailed performance of log parsers on logs that represent rare system events and offer comprehensive information for system troubleshooting. Parsing such logs accurately is vital yet challenging. We believe that our work could shed light on the design and evaluation of log parsers in more realistic settings, thereby facilitating their implementation in production systems.","sentences":["Log data is pivotal in activities like anomaly detection and failure diagnosis in the automated maintenance of software systems.","Due to their unstructured format, log parsing is often required to transform them into a structured format for automated analysis.","A variety of log parsers exist, making it vital to benchmark these tools to comprehend their features and performance.","However, existing datasets for log parsing are limited in terms of scale and representativeness, posing challenges for studies that aim to evaluate or develop log parsers.","This problem becomes more pronounced when these parsers are evaluated for production use.","To address these issues, we introduce a new collection of large-scale annotated log datasets, named LogPub, which more accurately mirrors log data observed in real-world software systems.","LogPub comprises 14 datasets, each averaging 3.6 million log lines.","Utilizing LogPub, we re-evaluate 15 log parsers in a more rigorous and practical setting.","We also propose a new evaluation metric to lessen the sensitivity of current metrics to imbalanced data distribution.","Furthermore, we are the first to scrutinize the detailed performance of log parsers on logs that represent rare system events and offer comprehensive information for system troubleshooting.","Parsing such logs accurately is vital yet challenging.","We believe that our work could shed light on the design and evaluation of log parsers in more realistic settings, thereby facilitating their implementation in production systems."],"url":"http://arxiv.org/abs/2308.10828v1"}
{"created":"2023-08-21 16:13:23","title":"Neural Networks Optimizations Against Concept and Data Drift in Malware Detection","abstract":"Despite the promising results of machine learning models in malware detection, they face the problem of concept drift due to malware constant evolution. This leads to a decline in performance over time, as the data distribution of the new files differs from the training one, requiring regular model update. In this work, we propose a model-agnostic protocol to improve a baseline neural network to handle with the drift problem. We show the importance of feature reduction and training with the most recent validation set possible, and propose a loss function named Drift-Resilient Binary Cross-Entropy, an improvement to the classical Binary Cross-Entropy more effective against drift. We train our model on the EMBER dataset (2018) and evaluate it on a dataset of recent malicious files, collected between 2020 and 2023. Our improved model shows promising results, detecting 15.2% more malware than a baseline model.","sentences":["Despite the promising results of machine learning models in malware detection, they face the problem of concept drift due to malware constant evolution.","This leads to a decline in performance over time, as the data distribution of the new files differs from the training one, requiring regular model update.","In this work, we propose a model-agnostic protocol to improve a baseline neural network to handle with the drift problem.","We show the importance of feature reduction and training with the most recent validation set possible, and propose a loss function named Drift-Resilient Binary Cross-Entropy, an improvement to the classical Binary Cross-Entropy more effective against drift.","We train our model on the EMBER dataset (2018) and evaluate it on a dataset of recent malicious files, collected between 2020 and 2023.","Our improved model shows promising results, detecting 15.2% more malware than a baseline model."],"url":"http://arxiv.org/abs/2308.10821v1"}
{"created":"2023-08-21 16:12:31","title":"Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction","abstract":"Hyperspectral Image (HSI) reconstruction has made gratifying progress with the deep unfolding framework by formulating the problem into a data module and a prior module. Nevertheless, existing methods still face the problem of insufficient matching with HSI data. The issues lie in three aspects: 1) fixed gradient descent step in the data module while the degradation of HSI is agnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3) stage interaction ignoring the differences in features at different stages. To address these issues, in this work, we propose a Pixel Adaptive Deep Unfolding Transformer (PADUT) for HSI reconstruction. In the data module, a pixel adaptive descent step is employed to focus on pixel-level agnostic degradation. In the prior module, we introduce the Non-local Spectral Transformer (NST) to emphasize the 3D characteristics of HSI for recovering. Moreover, inspired by the diverse expression of features in different stages and depths, the stage interaction is improved by the Fast Fourier Transform (FFT). Experimental results on both simulated and real scenes exhibit the superior performance of our method compared to state-of-the-art HSI reconstruction methods. The code is released at: https://github.com/MyuLi/PADUT.","sentences":["Hyperspectral Image (HSI) reconstruction has made gratifying progress with the deep unfolding framework by formulating the problem into a data module and a prior module.","Nevertheless, existing methods still face the problem of insufficient matching with HSI data.","The issues lie in three aspects: 1) fixed gradient descent step in the data module while the degradation of HSI is agnostic in the pixel-level.","2) inadequate prior module for 3D HSI cube.","3) stage interaction ignoring the differences in features at different stages.","To address these issues, in this work, we propose a Pixel Adaptive Deep Unfolding Transformer (PADUT) for HSI reconstruction.","In the data module, a pixel adaptive descent step is employed to focus on pixel-level agnostic degradation.","In the prior module, we introduce the Non-local Spectral Transformer (NST) to emphasize the 3D characteristics of HSI for recovering.","Moreover, inspired by the diverse expression of features in different stages and depths, the stage interaction is improved by the Fast Fourier Transform (FFT).","Experimental results on both simulated and real scenes exhibit the superior performance of our method compared to state-of-the-art HSI reconstruction methods.","The code is released at: https://github.com/MyuLi/PADUT."],"url":"http://arxiv.org/abs/2308.10820v1"}
{"created":"2023-08-21 16:03:35","title":"Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers","abstract":"Quantization scale and bit-width are the most important parameters when considering how to quantize a neural network. Prior work focuses on optimizing quantization scales in a global manner through gradient methods (gradient descent \\& Hessian analysis). Yet, when applying perturbations to quantization scales, we observe a very jagged, highly non-smooth test loss landscape. In fact, small perturbations in quantization scale can greatly affect accuracy, yielding a $0.5-0.8\\%$ accuracy boost in 4-bit quantized vision transformers (ViTs). In this regime, gradient methods break down, since they cannot reliably reach local minima. In our work, dubbed Evol-Q, we use evolutionary search to effectively traverse the non-smooth landscape. Additionally, we propose using an infoNCE loss, which not only helps combat overfitting on the small calibration dataset ($1,000$ images) but also makes traversing such a highly non-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully quantized ViT-Base by $10.30\\%$, $0.78\\%$, and $0.15\\%$ for $3$-bit, $4$-bit, and $8$-bit weight quantization levels. Extensive experiments on a variety of CNN and ViT architectures further demonstrate its robustness in extreme quantization scenarios. Our code is available at https://github.com/enyac-group/evol-q","sentences":["Quantization scale and bit-width are the most important parameters when considering how to quantize a neural network.","Prior work focuses on optimizing quantization scales in a global manner through gradient methods (gradient descent \\& Hessian analysis).","Yet, when applying perturbations to quantization scales, we observe a very jagged, highly non-smooth test loss landscape.","In fact, small perturbations in quantization scale can greatly affect accuracy, yielding a $0.5-0.8\\%$ accuracy boost in 4-bit quantized vision transformers (ViTs).","In this regime, gradient methods break down, since they cannot reliably reach local minima.","In our work, dubbed Evol-Q, we use evolutionary search to effectively traverse the non-smooth landscape.","Additionally, we propose using an infoNCE loss, which not only helps combat overfitting on the small calibration dataset ($1,000$ images) but also makes traversing such a highly non-smooth surface easier.","Evol-Q improves the top-1 accuracy of a fully quantized ViT-Base by $10.30\\%$, $0.78\\%$, and $0.15\\%$ for $3$-bit, $4$-bit, and $8$-bit weight quantization levels.","Extensive experiments on a variety of CNN and ViT architectures further demonstrate its robustness in extreme quantization scenarios.","Our code is available at https://github.com/enyac-group/evol-q"],"url":"http://arxiv.org/abs/2308.10814v1"}
{"created":"2023-08-21 15:59:36","title":"Tree Drawings with Columns","abstract":"Our goal is to visualize an additional data dimension of a tree with multifaceted data through superimposition on vertical strips, which we call columns. Specifically, we extend upward drawings of unordered rooted trees where vertices have assigned heights by mapping each vertex to a column. Under an orthogonal drawing style and with every subtree within a column drawn planar, we consider different natural variants concerning the arrangement of subtrees within a column. We show that minimizing the number of crossings in such a drawing can be achieved in fixed-parameter tractable (FPT) time in the maximum vertex degree $\\Delta$ for the most restrictive variant, while becoming NP-hard (even to approximate) already for a slightly relaxed variant. However, we provide an FPT algorithm in the number of crossings plus $\\Delta$, and an FPT-approximation algorithm in $\\Delta$ via a reduction to feedback arc set.","sentences":["Our goal is to visualize an additional data dimension of a tree with multifaceted data through superimposition on vertical strips, which we call columns.","Specifically, we extend upward drawings of unordered rooted trees where vertices have assigned heights by mapping each vertex to a column.","Under an orthogonal drawing style and with every subtree within a column drawn planar, we consider different natural variants concerning the arrangement of subtrees within a column.","We show that minimizing the number of crossings in such a drawing can be achieved in fixed-parameter tractable (FPT) time in the maximum vertex degree $\\Delta$ for the most restrictive variant, while becoming NP-hard (even to approximate) already for a slightly relaxed variant.","However, we provide an FPT algorithm in the number of crossings plus $\\Delta$, and an FPT-approximation algorithm in $\\Delta$ via a reduction to feedback arc set."],"url":"http://arxiv.org/abs/2308.10811v1"}
{"created":"2023-08-21 15:58:47","title":"Improving Continuous Sign Language Recognition with Cross-Lingual Signs","abstract":"This work dedicates to continuous sign language recognition (CSLR), which is a weakly supervised task dealing with the recognition of continuous signs from videos, without any prior knowledge about the temporal boundaries between consecutive signs. Data scarcity heavily impedes the progress of CSLR. Existing approaches typically train CSLR models on a monolingual corpus, which is orders of magnitude smaller than that of speech recognition. In this work, we explore the feasibility of utilizing multilingual sign language corpora to facilitate monolingual CSLR. Our work is built upon the observation of cross-lingual signs, which originate from different sign languages but have similar visual signals (e.g., hand shape and motion). The underlying idea of our approach is to identify the cross-lingual signs in one sign language and properly leverage them as auxiliary training data to improve the recognition capability of another. To achieve the goal, we first build two sign language dictionaries containing isolated signs that appear in two datasets. Then we identify the sign-to-sign mappings between two sign languages via a well-optimized isolated sign language recognition model. At last, we train a CSLR model on the combination of the target data with original labels and the auxiliary data with mapped labels. Experimentally, our approach achieves state-of-the-art performance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T.","sentences":["This work dedicates to continuous sign language recognition (CSLR), which is a weakly supervised task dealing with the recognition of continuous signs from videos, without any prior knowledge about the temporal boundaries between consecutive signs.","Data scarcity heavily impedes the progress of CSLR.","Existing approaches typically train CSLR models on a monolingual corpus, which is orders of magnitude smaller than that of speech recognition.","In this work, we explore the feasibility of utilizing multilingual sign language corpora to facilitate monolingual CSLR.","Our work is built upon the observation of cross-lingual signs, which originate from different sign languages but have similar visual signals (e.g., hand shape and motion).","The underlying idea of our approach is to identify the cross-lingual signs in one sign language and properly leverage them as auxiliary training data to improve the recognition capability of another.","To achieve the goal, we first build two sign language dictionaries containing isolated signs that appear in two datasets.","Then we identify the sign-to-sign mappings between two sign languages via a well-optimized isolated sign language recognition model.","At last, we train a CSLR model on the combination of the target data with original labels and the auxiliary data with mapped labels.","Experimentally, our approach achieves state-of-the-art performance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T."],"url":"http://arxiv.org/abs/2308.10809v1"}
{"created":"2023-08-21 15:57:57","title":"Graph Neural Bandits","abstract":"Contextual bandits algorithms aim to choose the optimal arm with the highest reward out of a set of candidates based on the contextual information. Various bandit algorithms have been applied to real-world applications due to their ability of tackling the exploitation-exploration dilemma. Motivated by online recommendation scenarios, in this paper, we propose a framework named Graph Neural Bandits (GNB) to leverage the collaborative nature among users empowered by graph neural networks (GNNs). Instead of estimating rigid user clusters as in existing works, we model the \"fine-grained\" collaborative effects through estimated user graphs in terms of exploitation and exploration respectively. Then, to refine the recommendation strategy, we utilize separate GNN-based models on estimated user graphs for exploitation and adaptive exploration. Theoretical analysis and experimental results on multiple real data sets in comparison with state-of-the-art baselines are provided to demonstrate the effectiveness of our proposed framework.","sentences":["Contextual bandits algorithms aim to choose the optimal arm with the highest reward out of a set of candidates based on the contextual information.","Various bandit algorithms have been applied to real-world applications due to their ability of tackling the exploitation-exploration dilemma.","Motivated by online recommendation scenarios, in this paper, we propose a framework named Graph Neural Bandits (GNB) to leverage the collaborative nature among users empowered by graph neural networks (GNNs).","Instead of estimating rigid user clusters as in existing works, we model the \"fine-grained\" collaborative effects through estimated user graphs in terms of exploitation and exploration respectively.","Then, to refine the recommendation strategy, we utilize separate GNN-based models on estimated user graphs for exploitation and adaptive exploration.","Theoretical analysis and experimental results on multiple real data sets in comparison with state-of-the-art baselines are provided to demonstrate the effectiveness of our proposed framework."],"url":"http://arxiv.org/abs/2308.10808v1"}
{"created":"2023-08-21 15:56:05","title":"DynED: Dynamic Ensemble Diversification in Data Stream Classification","abstract":"Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines.","sentences":["Ensemble methods are commonly used in classification due to their remarkable performance.","Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift.","A greater diversity of ensemble components is known to enhance prediction accuracy in such settings.","Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance.","This necessitates a method for selecting components that exhibit high performance and diversity.","We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble.","The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines."],"url":"http://arxiv.org/abs/2308.10807v1"}
{"created":"2023-08-21 15:53:38","title":"Differentiable Frank-Wolfe Optimization Layer","abstract":"Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks. The existing methods leverages the optimality conditions and implicit function theorem to obtain the Jacobian matrix of the output, which increases the computational cost and limits the application of differentiable optimization. In addition, some non-differentiable constraints lead to more challenges when using prior differentiable optimization layers. This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimization algorithm which can solve constrained optimization problems without projections and Hessian matrix computations, thus leading to a efficient way of dealing with large-scale problems. Theoretically, we establish a bound on the suboptimality gap of the DFWLayer in the context of l1-norm constraints. Experimental assessments demonstrate that the DFWLayer not only attains competitive accuracy in solutions and gradients but also consistently adheres to constraints. Moreover, it surpasses the baselines in both forward and backward computational speeds.","sentences":["Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks.","The existing methods leverages the optimality conditions and implicit function theorem to obtain the Jacobian matrix of the output, which increases the computational cost and limits the application of differentiable optimization.","In addition, some non-differentiable constraints lead to more challenges when using prior differentiable optimization layers.","This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimization algorithm which can solve constrained optimization problems without projections and Hessian matrix computations, thus leading to a efficient way of dealing with large-scale problems.","Theoretically, we establish a bound on the suboptimality gap of the DFWLayer in the context of l1-norm constraints.","Experimental assessments demonstrate that the DFWLayer not only attains competitive accuracy in solutions and gradients but also consistently adheres to constraints.","Moreover, it surpasses the baselines in both forward and backward computational speeds."],"url":"http://arxiv.org/abs/2308.10806v1"}
{"created":"2023-08-21 15:48:38","title":"LSCPM: communities in massive real-world Link Streams by Clique Percolation Method","abstract":"Community detection is a popular approach to understand the organization of interactions in static networks. For that purpose, the Clique Percolation Method (CPM), which involves the percolation of k-cliques, is a well-studied technique that offers several advantages. Besides, studying interactions that occur over time is useful in various contexts, which can be modeled by the link stream formalism. The Dynamic Clique Percolation Method (DCPM) has been proposed for extending CPM to temporal networks.   However, existing implementations are unable to handle massive datasets. We present a novel algorithm that adapts CPM to link streams, which has the advantage that it allows us to speed up the computation time with respect to the existing DCPM method. We evaluate it experimentally on real datasets and show that it scales to massive link streams. For example, it allows to obtain a complete set of communities in under twenty-five minutes for a dataset with thirty million links, what the state of the art fails to achieve even after a week of computation. We further show that our method provides communities similar to DCPM, but slightly more aggregated. We exhibit the relevance of the obtained communities in real world cases, and show that they provide information on the importance of vertices in the link streams.","sentences":["Community detection is a popular approach to understand the organization of interactions in static networks.","For that purpose, the Clique Percolation Method (CPM), which involves the percolation of k-cliques, is a well-studied technique that offers several advantages.","Besides, studying interactions that occur over time is useful in various contexts, which can be modeled by the link stream formalism.","The Dynamic Clique Percolation Method (DCPM) has been proposed for extending CPM to temporal networks.   ","However, existing implementations are unable to handle massive datasets.","We present a novel algorithm that adapts CPM to link streams, which has the advantage that it allows us to speed up the computation time with respect to the existing DCPM method.","We evaluate it experimentally on real datasets and show that it scales to massive link streams.","For example, it allows to obtain a complete set of communities in under twenty-five minutes for a dataset with thirty million links, what the state of the art fails to achieve even after a week of computation.","We further show that our method provides communities similar to DCPM, but slightly more aggregated.","We exhibit the relevance of the obtained communities in real world cases, and show that they provide information on the importance of vertices in the link streams."],"url":"http://arxiv.org/abs/2308.10801v1"}
{"created":"2023-08-21 15:47:37","title":"Artificial intelligence is ineffective and potentially harmful for fact checking","abstract":"Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent artificial intelligence (AI) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here we investigate the impact of fact checks generated by a popular AI model on belief in, and sharing intent of, political news in a preregistered randomized control experiment. Although the AI performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. However, the AI fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about. On the positive side, the AI increases sharing intents for correctly labeled true headlines. When participants are given the option to view AI fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false news. Our findings highlight an important source of potential harm stemming from AI applications and underscore the critical need for policies to prevent or mitigate such unintended consequences.","sentences":["Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online.","Recent artificial intelligence (AI) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear.","Here we investigate the impact of fact checks generated by a popular AI model on belief in, and sharing intent of, political news in a preregistered randomized control experiment.","Although the AI performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news.","However, the AI fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about.","On the positive side, the AI increases sharing intents for correctly labeled true headlines.","When participants are given the option to view AI fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false news.","Our findings highlight an important source of potential harm stemming from AI applications and underscore the critical need for policies to prevent or mitigate such unintended consequences."],"url":"http://arxiv.org/abs/2308.10800v1"}
{"created":"2023-08-21 15:42:56","title":"Stabilizing Unsupervised Environment Design with a Learned Adversary","abstract":"A key challenge in training generally-capable agents is the design of training tasks that facilitate broad generalization and robustness to environment variations. This challenge motivates the problem setting of Unsupervised Environment Design (UED), whereby a student agent trains on an adaptive distribution of tasks proposed by a teacher agent. A pioneering approach for UED is PAIRED, which uses reinforcement learning (RL) to train a teacher policy to design tasks from scratch, making it possible to directly generate tasks that are adapted to the agent's current capabilities. Despite its strong theoretical backing, PAIRED suffers from a variety of challenges that hinder its practical performance. Thus, state-of-the-art methods currently rely on curation and mutation rather than generation of new tasks. In this work, we investigate several key shortcomings of PAIRED and propose solutions for each shortcoming. As a result, we make it possible for PAIRED to match or exceed state-of-the-art methods, producing robust agents in several established challenging procedurally-generated environments, including a partially-observed maze navigation task and a continuous-control car racing environment. We believe this work motivates a renewed emphasis on UED methods based on learned models that directly generate challenging environments, potentially unlocking more open-ended RL training and, as a result, more general agents.","sentences":["A key challenge in training generally-capable agents is the design of training tasks that facilitate broad generalization and robustness to environment variations.","This challenge motivates the problem setting of Unsupervised Environment Design (UED), whereby a student agent trains on an adaptive distribution of tasks proposed by a teacher agent.","A pioneering approach for UED is PAIRED, which uses reinforcement learning (RL) to train a teacher policy to design tasks from scratch, making it possible to directly generate tasks that are adapted to the agent's current capabilities.","Despite its strong theoretical backing, PAIRED suffers from a variety of challenges that hinder its practical performance.","Thus, state-of-the-art methods currently rely on curation and mutation rather than generation of new tasks.","In this work, we investigate several key shortcomings of PAIRED and propose solutions for each shortcoming.","As a result, we make it possible for PAIRED to match or exceed state-of-the-art methods, producing robust agents in several established challenging procedurally-generated environments, including a partially-observed maze navigation task and a continuous-control car racing environment.","We believe this work motivates a renewed emphasis on UED methods based on learned models that directly generate challenging environments, potentially unlocking more open-ended RL training and, as a result, more general agents."],"url":"http://arxiv.org/abs/2308.10797v1"}
{"created":"2023-08-21 15:40:08","title":"Visualizing Historical Book Trade Data: An Iterative Design Study with Close Collaboration with Domain Experts","abstract":"The circulation of historical books has always been an area of interest for historians. However, the data used to represent the journey of a book across different places and times can be difficult for domain experts to digest due to buried geographical and chronological features within text-based presentations. This situation provides an opportunity for collaboration between visualization researchers and historians. This paper describes a design study where a variant of the Nine-Stage Framework was employed to develop a Visual Analytics (VA) tool called DanteExploreVis. This tool was designed to aid domain experts in exploring, explaining, and presenting book trade data from multiple perspectives. We discuss the design choices made and how each panel in the interface meets the domain requirements. We also present the results of a qualitative evaluation conducted with domain experts. The main contributions of this paper include: 1) the development of a VA tool to support domain experts in exploring, explaining, and presenting book trade data; 2) a comprehensive documentation of the iterative design, development, and evaluation process following the variant Nine-Stage Framework; 3) a summary of the insights gained and lessons learned from this design study in the context of the humanities field; and 4) reflections on how our approach could be applied in a more generalizable way.","sentences":["The circulation of historical books has always been an area of interest for historians.","However, the data used to represent the journey of a book across different places and times can be difficult for domain experts to digest due to buried geographical and chronological features within text-based presentations.","This situation provides an opportunity for collaboration between visualization researchers and historians.","This paper describes a design study where a variant of the Nine-Stage Framework was employed to develop a Visual Analytics (VA) tool called DanteExploreVis.","This tool was designed to aid domain experts in exploring, explaining, and presenting book trade data from multiple perspectives.","We discuss the design choices made and how each panel in the interface meets the domain requirements.","We also present the results of a qualitative evaluation conducted with domain experts.","The main contributions of this paper include: 1) the development of a VA tool to support domain experts in exploring, explaining, and presenting book trade data; 2) a comprehensive documentation of the iterative design, development, and evaluation process following the variant Nine-Stage Framework; 3) a summary of the insights gained and lessons learned from this design study in the context of the humanities field; and 4) reflections on how our approach could be applied in a more generalizable way."],"url":"http://arxiv.org/abs/2308.10795v1"}
{"created":"2023-08-21 15:39:41","title":"MGMAE: Motion Guided Masking for Video Masked Autoencoding","abstract":"Masked autoencoding has shown excellent performance on self-supervised video representation learning. Temporal redundancy has led to a high masking ratio and customized masking strategy in VideoMAE. In this paper, we aim to further improve the performance of video masked autoencoding by introducing a motion guided masking strategy. Our key insight is that motion is a general and unique prior in video, which should be taken into account during masked pre-training. Our motion guided masking explicitly incorporates motion information to build temporal consistent masking volume. Based on this masking volume, we can track the unmasked tokens in time and sample a set of temporal consistent cubes from videos. These temporal aligned unmasked tokens will further relieve the information leakage issue in time and encourage the MGMAE to learn more useful structure information. We implement our MGMAE with an online efficient optical flow estimator and backward masking map warping strategy. We perform experiments on the datasets of Something-Something V2 and Kinetics-400, demonstrating the superior performance of our MGMAE to the original VideoMAE. In addition, we provide the visualization analysis to illustrate that our MGMAE can sample temporal consistent cubes in a motion-adaptive manner for more effective video pre-training.","sentences":["Masked autoencoding has shown excellent performance on self-supervised video representation learning.","Temporal redundancy has led to a high masking ratio and customized masking strategy in VideoMAE.","In this paper, we aim to further improve the performance of video masked autoencoding by introducing a motion guided masking strategy.","Our key insight is that motion is a general and unique prior in video, which should be taken into account during masked pre-training.","Our motion guided masking explicitly incorporates motion information to build temporal consistent masking volume.","Based on this masking volume, we can track the unmasked tokens in time and sample a set of temporal consistent cubes from videos.","These temporal aligned unmasked tokens will further relieve the information leakage issue in time and encourage the MGMAE to learn more useful structure information.","We implement our MGMAE with an online efficient optical flow estimator and backward masking map warping strategy.","We perform experiments on the datasets of Something-Something V2 and Kinetics-400, demonstrating the superior performance of our MGMAE to the original VideoMAE.","In addition, we provide the visualization analysis to illustrate that our MGMAE can sample temporal consistent cubes in a motion-adaptive manner for more effective video pre-training."],"url":"http://arxiv.org/abs/2308.10794v1"}
{"created":"2023-08-21 15:35:59","title":"Simple Cycle Reservoirs are Universal","abstract":"Reservoir computation models form a subclass of recurrent neural networks with fixed non-trainable input and dynamic coupling weights. Only the static readout from the state space (reservoir) is trainable, thus avoiding the known problems with propagation of gradient information backwards through time. Reservoir models have been successfully applied in a variety of tasks and were shown to be universal approximators of time-invariant fading memory dynamic filters under various settings. Simple cycle reservoirs (SCR) have been suggested as severely restricted reservoir architecture, with equal weight ring connectivity of the reservoir units and input-to-reservoir weights of binary nature with the same absolute value. Such architectures are well suited for hardware implementations without performance degradation in many practical tasks. In this contribution, we rigorously study the expressive power of SCR in the complex domain and show that they are capable of universal approximation of any unrestricted linear reservoir system (with continuous readout) and hence any time-invariant fading memory filter over uniformly bounded input streams.","sentences":["Reservoir computation models form a subclass of recurrent neural networks with fixed non-trainable input and dynamic coupling weights.","Only the static readout from the state space (reservoir) is trainable, thus avoiding the known problems with propagation of gradient information backwards through time.","Reservoir models have been successfully applied in a variety of tasks and were shown to be universal approximators of time-invariant fading memory dynamic filters under various settings.","Simple cycle reservoirs (SCR) have been suggested as severely restricted reservoir architecture, with equal weight ring connectivity of the reservoir units and input-to-reservoir weights of binary nature with the same absolute value.","Such architectures are well suited for hardware implementations without performance degradation in many practical tasks.","In this contribution, we rigorously study the expressive power of SCR in the complex domain and show that they are capable of universal approximation of any unrestricted linear reservoir system (with continuous readout) and hence any time-invariant fading memory filter over uniformly bounded input streams."],"url":"http://arxiv.org/abs/2308.10793v1"}
{"created":"2023-08-21 15:35:16","title":"Instruction Tuning for Large Language Models: A Survey","abstract":"This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.","sentences":["This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs).","Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions.","In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc).","We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research."],"url":"http://arxiv.org/abs/2308.10792v1"}
{"created":"2023-08-21 15:27:02","title":"Effectiveness of Reconfigurable Intelligent Surfaces to Enhance Connectivity in UAV Networks","abstract":"Reconfigurable intelligent surfaces (RISs) are expected to make future 6G networks more connected and resilient against node failures, due to their ability to introduce controllable phase-shifts onto impinging electromagnetic waves and impose link redundancy. Meanwhile, unmanned aerial vehicles (UAVs) are prone to failure due to limited energy, random failures, or targeted failures, which causes network disintegration that results in information delivery loss. In this paper, we show that the integration between UAVs and RISs for improving network connectivity is crucial. We utilize RISs to provide path diversity and alternative connectivity options for information flow from user equipments (UEs) to less critical UAVs by adding more links to the network, thereby making the network more resilient and connected. To that end, we first define the criticality of UAV nodes, which reflects the importance of some nodes over other nodes. We then employ the algebraic connectivity metric, which is adjusted by the reflected links of the RISs and their criticality weights, to formulate the problem of maximizing the network connectivity. Such problem is a computationally expensive combinatorial optimization. To tackle this problem, we propose a relaxation method such that the discrete scheduling constraint of the problem is relaxed and becomes continuous. Leveraging this, we propose two efficient solutions, namely semi-definite programming (SDP) optimization and perturbation heuristic, which both solve the problem in polynomial time. For the perturbation heuristic, we derive the lower and upper bounds of the algebraic connectivity obtained by adding new links to the network. Finally, we corroborate the effectiveness of the proposed solutions through extensive simulation experiments.","sentences":["Reconfigurable intelligent surfaces (RISs) are expected to make future 6G networks more connected and resilient against node failures, due to their ability to introduce controllable phase-shifts onto impinging electromagnetic waves and impose link redundancy.","Meanwhile, unmanned aerial vehicles (UAVs) are prone to failure due to limited energy, random failures, or targeted failures, which causes network disintegration that results in information delivery loss.","In this paper, we show that the integration between UAVs and RISs for improving network connectivity is crucial.","We utilize RISs to provide path diversity and alternative connectivity options for information flow from user equipments (UEs) to less critical UAVs by adding more links to the network, thereby making the network more resilient and connected.","To that end, we first define the criticality of UAV nodes, which reflects the importance of some nodes over other nodes.","We then employ the algebraic connectivity metric, which is adjusted by the reflected links of the RISs and their criticality weights, to formulate the problem of maximizing the network connectivity.","Such problem is a computationally expensive combinatorial optimization.","To tackle this problem, we propose a relaxation method such that the discrete scheduling constraint of the problem is relaxed and becomes continuous.","Leveraging this, we propose two efficient solutions, namely semi-definite programming (SDP) optimization and perturbation heuristic, which both solve the problem in polynomial time.","For the perturbation heuristic, we derive the lower and upper bounds of the algebraic connectivity obtained by adding new links to the network.","Finally, we corroborate the effectiveness of the proposed solutions through extensive simulation experiments."],"url":"http://arxiv.org/abs/2308.10788v1"}
{"created":"2023-08-21 15:19:10","title":"Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis","abstract":"The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing, politics, customer service, and healthcare. While there have been significant advancements in sentiment analysis for widely spoken languages, low-resource languages, such as Bangla, remain largely under-researched due to resource constraints. Furthermore, the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low-resource languages. In this study, we present a sizeable manually annotated dataset encompassing 33,605 Bangla news tweets and Facebook comments. We also investigate zero- and few-shot in-context learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering a comparative analysis against fine-tuned models. Our findings suggest that monolingual transformer-based models consistently outperform other models, even in zero and few-shot scenarios. To foster continued exploration, we intend to make this dataset and our research tools publicly available to the broader research community. In the spirit of further research, we plan to make this dataset and our experimental resources publicly accessible to the wider research community.","sentences":["The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing, politics, customer service, and healthcare.","While there have been significant advancements in sentiment analysis for widely spoken languages, low-resource languages, such as Bangla, remain largely under-researched due to resource constraints.","Furthermore, the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low-resource languages.","In this study, we present a sizeable manually annotated dataset encompassing 33,605 Bangla news tweets and Facebook comments.","We also investigate zero- and few-shot in-context learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering a comparative analysis against fine-tuned models.","Our findings suggest that monolingual transformer-based models consistently outperform other models, even in zero and few-shot scenarios.","To foster continued exploration, we intend to make this dataset and our research tools publicly available to the broader research community.","In the spirit of further research, we plan to make this dataset and our experimental resources publicly accessible to the wider research community."],"url":"http://arxiv.org/abs/2308.10783v1"}
{"created":"2023-08-21 15:16:19","title":"Sparse Linear Concept Discovery Models","abstract":"The recent mass adoption of DNNs, even in safety-critical scenarios, has shifted the focus of the research community towards the creation of inherently intrepretable models. Concept Bottleneck Models (CBMs) constitute a popular approach where hidden layers are tied to human understandable concepts allowing for investigation and correction of the network's decisions. However, CBMs usually suffer from: (i) performance degradation and (ii) lower interpretability than intended due to the sheer amount of concepts contributing to each decision. In this work, we propose a simple yet highly intuitive interpretable framework based on Contrastive Language Image models and a single sparse linear layer. In stark contrast to related approaches, the sparsity in our framework is achieved via principled Bayesian arguments by inferring concept presence via a data-driven Bernoulli distribution. As we experimentally show, our framework not only outperforms recent CBM approaches accuracy-wise, but it also yields high per example concept sparsity, facilitating the individual investigation of the emerging concepts.","sentences":["The recent mass adoption of DNNs, even in safety-critical scenarios, has shifted the focus of the research community towards the creation of inherently intrepretable models.","Concept Bottleneck Models (CBMs) constitute a popular approach where hidden layers are tied to human understandable concepts allowing for investigation and correction of the network's decisions.","However, CBMs usually suffer from: (i) performance degradation and (ii) lower interpretability than intended due to the sheer amount of concepts contributing to each decision.","In this work, we propose a simple yet highly intuitive interpretable framework based on Contrastive Language Image models and a single sparse linear layer.","In stark contrast to related approaches, the sparsity in our framework is achieved via principled Bayesian arguments by inferring concept presence via a data-driven Bernoulli distribution.","As we experimentally show, our framework not only outperforms recent CBM approaches accuracy-wise, but it also yields high per example concept sparsity, facilitating the individual investigation of the emerging concepts."],"url":"http://arxiv.org/abs/2308.10782v1"}
{"created":"2023-08-21 15:14:49","title":"Mixed-Integer Projections for Automated Data Correction of EMRs Improve Predictions of Sepsis among Hospitalized Patients","abstract":"Machine learning (ML) models are increasingly pivotal in automating clinical decisions. Yet, a glaring oversight in prior research has been the lack of proper processing of Electronic Medical Record (EMR) data in the clinical context for errors and outliers. Addressing this oversight, we introduce an innovative projections-based method that seamlessly integrates clinical expertise as domain constraints, generating important meta-data that can be used in ML workflows. In particular, by using high-dimensional mixed-integer programs that capture physiological and biological constraints on patient vitals and lab values, we can harness the power of mathematical \"projections\" for the EMR data to correct patient data. Consequently, we measure the distance of corrected data from the constraints defining a healthy range of patient data, resulting in a unique predictive metric we term as \"trust-scores\". These scores provide insight into the patient's health status and significantly boost the performance of ML classifiers in real-life clinical settings. We validate the impact of our framework in the context of early detection of sepsis using ML. We show an AUROC of 0.865 and a precision of 0.922, that surpasses conventional ML models without such projections.","sentences":["Machine learning (ML) models are increasingly pivotal in automating clinical decisions.","Yet, a glaring oversight in prior research has been the lack of proper processing of Electronic Medical Record (EMR) data in the clinical context for errors and outliers.","Addressing this oversight, we introduce an innovative projections-based method that seamlessly integrates clinical expertise as domain constraints, generating important meta-data that can be used in ML workflows.","In particular, by using high-dimensional mixed-integer programs that capture physiological and biological constraints on patient vitals and lab values, we can harness the power of mathematical \"projections\" for the EMR data to correct patient data.","Consequently, we measure the distance of corrected data from the constraints defining a healthy range of patient data, resulting in a unique predictive metric we term as \"trust-scores\".","These scores provide insight into the patient's health status and significantly boost the performance of ML classifiers in real-life clinical settings.","We validate the impact of our framework in the context of early detection of sepsis using ML.","We show an AUROC of 0.865 and a precision of 0.922, that surpasses conventional ML models without such projections."],"url":"http://arxiv.org/abs/2308.10781v1"}
{"created":"2023-08-21 15:09:51","title":"Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs","abstract":"Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks. Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs. While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored. In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs. Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model. Moreover, we propose a robust training approach T-SHIELD to mitigate the impact of adversarial attacks. By using edge filtering and enforcing temporal smoothness to node embeddings, we enhance the robustness of the victim model. Our experimental study shows that T-SPEAR significantly degrades the victim model's performance on link prediction tasks, and even more, our attacks are transferable to other TGNNs, which differ from the victim model assumed by the attacker. Moreover, we demonstrate that T-SHIELD effectively filters out adversarial edges and exhibits robustness against adversarial attacks, surpassing the link prediction performance of the naive TGNN by up to 11.2% under T-SPEAR.","sentences":["Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks.","Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs.","While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored.","In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs.","Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model.","Moreover, we propose a robust training approach T-SHIELD to mitigate the impact of adversarial attacks.","By using edge filtering and enforcing temporal smoothness to node embeddings, we enhance the robustness of the victim model.","Our experimental study shows that T-SPEAR significantly degrades the victim model's performance on link prediction tasks, and even more, our attacks are transferable to other TGNNs, which differ from the victim model assumed by the attacker.","Moreover, we demonstrate that T-SHIELD effectively filters out adversarial edges and exhibits robustness against adversarial attacks, surpassing the link prediction performance of the naive TGNN by up to 11.2% under T-SPEAR."],"url":"http://arxiv.org/abs/2308.10779v1"}
{"created":"2023-08-21 15:09:19","title":"A Topology-aware Analysis of Graph Collaborative Filtering","abstract":"The successful integration of graph neural networks into recommender systems (RSs) has led to a novel paradigm in collaborative filtering (CF), graph collaborative filtering (graph CF). By representing user-item data as an undirected, bipartite graph, graph CF utilizes short- and long-range connections to extract collaborative signals that yield more accurate user preferences than traditional CF methods. Although the recent literature highlights the efficacy of various algorithmic strategies in graph CF, the impact of datasets and their topological features on recommendation performance is yet to be studied. To fill this gap, we propose a topology-aware analysis of graph CF. In this study, we (i) take some widely-adopted recommendation datasets and use them to generate a large set of synthetic sub-datasets through two state-of-the-art graph sampling methods, (ii) measure eleven of their classical and topological characteristics, and (iii) estimate the accuracy calculated on the generated sub-datasets considering four popular and recent graph-based RSs (i.e., LightGCN, DGCF, UltraGCN, and SVD-GCN). Finally, the investigation presents an explanatory framework that reveals the linear relationships between characteristics and accuracy measures. The results, statistically validated under different graph sampling settings, confirm the existence of solid dependencies between topological characteristics and accuracy in the graph-based recommendation, offering a new perspective on how to interpret graph CF.","sentences":["The successful integration of graph neural networks into recommender systems (RSs) has led to a novel paradigm in collaborative filtering (CF), graph collaborative filtering (graph CF).","By representing user-item data as an undirected, bipartite graph, graph CF utilizes short- and long-range connections to extract collaborative signals that yield more accurate user preferences than traditional CF methods.","Although the recent literature highlights the efficacy of various algorithmic strategies in graph CF, the impact of datasets and their topological features on recommendation performance is yet to be studied.","To fill this gap, we propose a topology-aware analysis of graph CF.","In this study, we (i) take some widely-adopted recommendation datasets and use them to generate a large set of synthetic sub-datasets through two state-of-the-art graph sampling methods, (ii) measure eleven of their classical and topological characteristics, and (iii) estimate the accuracy calculated on the generated sub-datasets considering four popular and recent graph-based RSs (i.e., LightGCN, DGCF, UltraGCN, and SVD-GCN).","Finally, the investigation presents an explanatory framework that reveals the linear relationships between characteristics and accuracy measures.","The results, statistically validated under different graph sampling settings, confirm the existence of solid dependencies between topological characteristics and accuracy in the graph-based recommendation, offering a new perspective on how to interpret graph CF."],"url":"http://arxiv.org/abs/2308.10778v1"}
{"created":"2023-08-21 15:06:02","title":"A Modular and Adaptive System for Business Email Compromise Detection","abstract":"The growing sophistication of Business Email Compromise (BEC) and spear phishing attacks poses significant challenges to organizations worldwide. The techniques featured in traditional spam and phishing detection are insufficient due to the tailored nature of modern BEC attacks as they often blend in with the regular benign traffic. Recent advances in machine learning, particularly in Natural Language Understanding (NLU), offer a promising avenue for combating such attacks but in a practical system, due to limitations such as data availability, operational costs, verdict explainability requirements or a need to robustly evolve the system, it is essential to combine multiple approaches together. We present CAPE, a comprehensive and efficient system for BEC detection that has been proven in a production environment for a period of over two years. Rather than being a single model, CAPE is a system that combines independent ML models and algorithms detecting BEC-related behaviors across various email modalities such as text, images, metadata and the email's communication context. This decomposition makes CAPE's verdicts naturally explainable. In the paper, we describe the design principles and constraints behind its architecture, as well as the challenges of model design, evaluation and adapting the system continuously through a Bayesian approach that combines limited data with domain knowledge. Furthermore, we elaborate on several specific behavioral detectors, such as those based on Transformer neural architectures.","sentences":["The growing sophistication of Business Email Compromise (BEC) and spear phishing attacks poses significant challenges to organizations worldwide.","The techniques featured in traditional spam and phishing detection are insufficient due to the tailored nature of modern BEC attacks as they often blend in with the regular benign traffic.","Recent advances in machine learning, particularly in Natural Language Understanding (NLU), offer a promising avenue for combating such attacks but in a practical system, due to limitations such as data availability, operational costs, verdict explainability requirements or a need to robustly evolve the system, it is essential to combine multiple approaches together.","We present CAPE, a comprehensive and efficient system for BEC detection that has been proven in a production environment for a period of over two years.","Rather than being a single model, CAPE is a system that combines independent ML models and algorithms detecting BEC-related behaviors across various email modalities such as text, images, metadata and the email's communication context.","This decomposition makes CAPE's verdicts naturally explainable.","In the paper, we describe the design principles and constraints behind its architecture, as well as the challenges of model design, evaluation and adapting the system continuously through a Bayesian approach that combines limited data with domain knowledge.","Furthermore, we elaborate on several specific behavioral detectors, such as those based on Transformer neural architectures."],"url":"http://arxiv.org/abs/2308.10776v1"}
{"created":"2023-08-21 15:00:42","title":"Toward Extending Concentric Tube Robot Kinematics for Large Clearance and Impulse Curvature","abstract":"Concentric Tube Robots (CTRs) have been proposed to operate within the unstructured environment for minimally invasive surgeries. In this letter, we consider the operation scenario where the tubes travel inside the channels with a large clearance or large curvature, such as aortas or industrial pipes. Accurate kinematic modeling of CTRs is required for the development of advanced control and sensing algorithms. To this end, we extended the conventional CTR kinematics model to a more general case with large tube-to-tube clearance and large centerline curvature. Numerical simulations and experimental validations are conducted to compare our model with respect to the conventional CTR kinematic model. In the physical experiments, our proposed model achieved a tip position error of 1.53 mm in the 2D planer case and 4.36 mm in 3D case, outperforming the state-of-the-art model by 71% and 66%, respectively.","sentences":["Concentric Tube Robots (CTRs) have been proposed to operate within the unstructured environment for minimally invasive surgeries.","In this letter, we consider the operation scenario where the tubes travel inside the channels with a large clearance or large curvature, such as aortas or industrial pipes.","Accurate kinematic modeling of CTRs is required for the development of advanced control and sensing algorithms.","To this end, we extended the conventional CTR kinematics model to a more general case with large tube-to-tube clearance and large centerline curvature.","Numerical simulations and experimental validations are conducted to compare our model with respect to the conventional CTR kinematic model.","In the physical experiments, our proposed model achieved a tip position error of 1.53 mm in the 2D planer case and 4.36 mm in 3D case, outperforming the state-of-the-art model by 71% and 66%, respectively."],"url":"http://arxiv.org/abs/2308.10770v1"}
{"created":"2023-08-21 14:56:51","title":"GBM-based Bregman Proximal Algorithms for Constrained Learning","abstract":"As the complexity of learning tasks surges, modern machine learning encounters a new constrained learning paradigm characterized by more intricate and data-driven function constraints. Prominent applications include Neyman-Pearson classification (NPC) and fairness classification, which entail specific risk constraints that render standard projection-based training algorithms unsuitable. Gradient boosting machines (GBMs) are among the most popular algorithms for supervised learning; however, they are generally limited to unconstrained settings. In this paper, we adapt the GBM for constrained learning tasks within the framework of Bregman proximal algorithms. We introduce a new Bregman primal-dual method with a global optimality guarantee when the learning objective and constraint functions are convex. In cases of nonconvex functions, we demonstrate how our algorithm remains effective under a Bregman proximal point framework. Distinct from existing constrained learning algorithms, ours possess a unique advantage in their ability to seamlessly integrate with publicly available GBM implementations such as XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017), exclusively relying on their public interfaces. We provide substantial experimental evidence to showcase the effectiveness of the Bregman algorithm framework. While our primary focus is on NPC and fairness ML, our framework holds significant potential for a broader range of constrained learning applications. The source code is currently freely available at https://github.com/zhenweilin/ConstrainedGBM}{https://github.com/zhenweilin/ConstrainedGBM.","sentences":["As the complexity of learning tasks surges, modern machine learning encounters a new constrained learning paradigm characterized by more intricate and data-driven function constraints.","Prominent applications include Neyman-Pearson classification (NPC) and fairness classification, which entail specific risk constraints that render standard projection-based training algorithms unsuitable.","Gradient boosting machines (GBMs) are among the most popular algorithms for supervised learning; however, they are generally limited to unconstrained settings.","In this paper, we adapt the GBM for constrained learning tasks within the framework of Bregman proximal algorithms.","We introduce a new Bregman primal-dual method with a global optimality guarantee when the learning objective and constraint functions are convex.","In cases of nonconvex functions, we demonstrate how our algorithm remains effective under a Bregman proximal point framework.","Distinct from existing constrained learning algorithms, ours possess a unique advantage in their ability to seamlessly integrate with publicly available GBM implementations such as XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017), exclusively relying on their public interfaces.","We provide substantial experimental evidence to showcase the effectiveness of the Bregman algorithm framework.","While our primary focus is on NPC and fairness ML, our framework holds significant potential for a broader range of constrained learning applications.","The source code is currently freely available at https://github.com/zhenweilin/ConstrainedGBM}{https://github.com/zhenweilin/ConstrainedGBM."],"url":"http://arxiv.org/abs/2308.10767v1"}
{"created":"2023-08-21 14:49:37","title":"CoNe: Contrast Your Neighbours for Supervised Image Classification","abstract":"Image classification is a longstanding problem in computer vision and machine learning research. Most recent works (e.g. SupCon , Triplet, and max-margin) mainly focus on grouping the intra-class samples aggressively and compactly, with the assumption that all intra-class samples should be pulled tightly towards their class centers. However, such an objective will be very hard to achieve since it ignores the intra-class variance in the dataset. (i.e. different instances from the same class can have significant differences). Thus, such a monotonous objective is not sufficient. To provide a more informative objective, we introduce Contrast Your Neighbours (CoNe) - a simple yet practical learning framework for supervised image classification. Specifically, in CoNe, each sample is not only supervised by its class center but also directly employs the features of its similar neighbors as anchors to generate more adaptive and refined targets. Moreover, to further boost the performance, we propose ``distributional consistency\" as a more informative regularization to enable similar instances to have a similar probability distribution. Extensive experimental results demonstrate that CoNe achieves state-of-the-art performance across different benchmark datasets, network architectures, and settings. Notably, even without a complicated training recipe, our CoNe achieves 80.8\\% Top-1 accuracy on ImageNet with ResNet-50, which surpasses the recent Timm training recipe (80.4\\%). Code and pre-trained models are available at \\href{https://github.com/mingkai-zheng/CoNe}{https://github.com/mingkai-zheng/CoNe}.","sentences":["Image classification is a longstanding problem in computer vision and machine learning research.","Most recent works (e.g. SupCon , Triplet, and max-margin) mainly focus on grouping the intra-class samples aggressively and compactly, with the assumption that all intra-class samples should be pulled tightly towards their class centers.","However, such an objective will be very hard to achieve since it ignores the intra-class variance in the dataset.","(i.e. different instances from the same class can have significant differences).","Thus, such a monotonous objective is not sufficient.","To provide a more informative objective, we introduce Contrast Your Neighbours (CoNe) - a simple yet practical learning framework for supervised image classification.","Specifically, in CoNe, each sample is not only supervised by its class center but also directly employs the features of its similar neighbors as anchors to generate more adaptive and refined targets.","Moreover, to further boost the performance, we propose ``distributional consistency\" as a more informative regularization to enable similar instances to have a similar probability distribution.","Extensive experimental results demonstrate that CoNe achieves state-of-the-art performance across different benchmark datasets, network architectures, and settings.","Notably, even without a complicated training recipe, our CoNe achieves 80.8\\% Top-1 accuracy on ImageNet with ResNet-50, which surpasses the recent Timm training recipe (80.4\\%).","Code and pre-trained models are available at \\href{https://github.com/mingkai-zheng/CoNe}{https://github.com/mingkai-zheng/CoNe}."],"url":"http://arxiv.org/abs/2308.10761v1"}
{"created":"2023-08-21 14:46:43","title":"EALink: An Efficient and Accurate Pre-trained Framework for Issue-Commit Link Recovery","abstract":"Issue-commit links, as a type of software traceability links, play a vital role in various software development and maintenance tasks. However, they are typically deficient, as developers often forget or fail to create tags when making commits. Existing studies have deployed deep learning techniques, including pretrained models, to improve automatic issue-commit link recovery.Despite their promising performance, we argue that previous approaches have four main problems, hindering them from recovering links in large software projects. To overcome these problems, we propose an efficient and accurate pre-trained framework called EALink for issue-commit link recovery. EALink requires much fewer model parameters than existing pre-trained methods, bringing efficient training and recovery. Moreover, we design various techniques to improve the recovery accuracy of EALink. We construct a large-scale dataset and conduct extensive experiments to demonstrate the power of EALink. Results show that EALink outperforms the state-of-the-art methods by a large margin (15.23%-408.65%) on various evaluation metrics. Meanwhile, its training and inference overhead is orders of magnitude lower than existing methods.","sentences":["Issue-commit links, as a type of software traceability links, play a vital role in various software development and maintenance tasks.","However, they are typically deficient, as developers often forget or fail to create tags when making commits.","Existing studies have deployed deep learning techniques, including pretrained models, to improve automatic issue-commit link recovery.","Despite their promising performance, we argue that previous approaches have four main problems, hindering them from recovering links in large software projects.","To overcome these problems, we propose an efficient and accurate pre-trained framework called EALink for issue-commit link recovery.","EALink requires much fewer model parameters than existing pre-trained methods, bringing efficient training and recovery.","Moreover, we design various techniques to improve the recovery accuracy of EALink.","We construct a large-scale dataset and conduct extensive experiments to demonstrate the power of EALink.","Results show that EALink outperforms the state-of-the-art methods by a large margin (15.23%-408.65%) on various evaluation metrics.","Meanwhile, its training and inference overhead is orders of magnitude lower than existing methods."],"url":"http://arxiv.org/abs/2308.10759v1"}
{"created":"2023-08-21 14:44:31","title":"DepreSym: A Depression Symptom Annotated Corpus and the Role of LLMs as Assessors of Psychological Markers","abstract":"Computational methods for depression detection aim to mine traces of depression from online publications posted by Internet users. However, solutions trained on existing collections exhibit limited generalisation and interpretability. To tackle these issues, recent studies have shown that identifying depressive symptoms can lead to more robust models. The eRisk initiative fosters research on this area and has recently proposed a new ranking task focused on developing search methods to find sentences related to depressive symptoms. This search challenge relies on the symptoms specified by the Beck Depression Inventory-II (BDI-II), a questionnaire widely used in clinical practice. Based on the participant systems' results, we present the DepreSym dataset, consisting of 21580 sentences annotated according to their relevance to the 21 BDI-II symptoms. The labelled sentences come from a pool of diverse ranking methods, and the final dataset serves as a valuable resource for advancing the development of models that incorporate depressive markers such as clinical symptoms. Due to the complex nature of this relevance annotation, we designed a robust assessment methodology carried out by three expert assessors (including an expert psychologist). Additionally, we explore here the feasibility of employing recent Large Language Models (ChatGPT and GPT4) as potential assessors in this complex task. We undertake a comprehensive examination of their performance, determine their main limitations and analyze their role as a complement or replacement for human annotators.","sentences":["Computational methods for depression detection aim to mine traces of depression from online publications posted by Internet users.","However, solutions trained on existing collections exhibit limited generalisation and interpretability.","To tackle these issues, recent studies have shown that identifying depressive symptoms can lead to more robust models.","The eRisk initiative fosters research on this area and has recently proposed a new ranking task focused on developing search methods to find sentences related to depressive symptoms.","This search challenge relies on the symptoms specified by the Beck Depression Inventory-II (BDI-II), a questionnaire widely used in clinical practice.","Based on the participant systems' results, we present the DepreSym dataset, consisting of 21580 sentences annotated according to their relevance to the 21 BDI-II symptoms.","The labelled sentences come from a pool of diverse ranking methods, and the final dataset serves as a valuable resource for advancing the development of models that incorporate depressive markers such as clinical symptoms.","Due to the complex nature of this relevance annotation, we designed a robust assessment methodology carried out by three expert assessors (including an expert psychologist).","Additionally, we explore here the feasibility of employing recent Large Language Models (ChatGPT and GPT4) as potential assessors in this complex task.","We undertake a comprehensive examination of their performance, determine their main limitations and analyze their role as a complement or replacement for human annotators."],"url":"http://arxiv.org/abs/2308.10758v1"}
{"created":"2023-08-21 14:43:42","title":"To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills","abstract":"Communicating shapes our social word. For a robot to be considered social and being consequently integrated in our social environment it is fundamental to understand some of the dynamics that rule human-human communication. In this work, we tackle the problem of Addressee Estimation, the ability to understand an utterance's addressee, by interpreting and exploiting non-verbal bodily cues from the speaker. We do so by implementing an hybrid deep learning model composed of convolutional layers and LSTM cells taking as input images portraying the face of the speaker and 2D vectors of the speaker's body posture. Our implementation choices were guided by the aim to develop a model that could be deployed on social robots and be efficient in ecological scenarios. We demonstrate that our model is able to solve the Addressee Estimation problem in terms of addressee localisation in space, from a robot ego-centric point of view.","sentences":["Communicating shapes our social word.","For a robot to be considered social and being consequently integrated in our social environment it is fundamental to understand some of the dynamics that rule human-human communication.","In this work, we tackle the problem of Addressee Estimation, the ability to understand an utterance's addressee, by interpreting and exploiting non-verbal bodily cues from the speaker.","We do so by implementing an hybrid deep learning model composed of convolutional layers and LSTM cells taking as input images portraying the face of the speaker and 2D vectors of the speaker's body posture.","Our implementation choices were guided by the aim to develop a model that could be deployed on social robots and be efficient in ecological scenarios.","We demonstrate that our model is able to solve the Addressee Estimation problem in terms of addressee localisation in space, from a robot ego-centric point of view."],"url":"http://arxiv.org/abs/2308.10757v1"}
{"created":"2023-08-21 14:40:48","title":"WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models","abstract":"The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.","sentences":["The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs).","These cutting-edge models owe their remarkable performance to high-quality data.","However, the details of the training data used in leading paradigms are often kept confidential.","This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community.","As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources.","The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB.","It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale.","All data can be accessed at https://opendatalab.org.cn/WanJuan1.0."],"url":"http://arxiv.org/abs/2308.10755v1"}
{"created":"2023-08-21 14:16:36","title":"Boosting Adversarial Attack with Similar Target","abstract":"Deep neural networks are vulnerable to adversarial examples, posing a threat to the models' applications and raising security concerns. An intriguing property of adversarial examples is their strong transferability. Several methods have been proposed to enhance transferability, including ensemble attacks which have demonstrated their efficacy. However, prior approaches simply average logits, probabilities, or losses for model ensembling, lacking a comprehensive analysis of how and why model ensembling significantly improves transferability. In this paper, we propose a similar targeted attack method named Similar Target~(ST). By promoting cosine similarity between the gradients of each model, our method regularizes the optimization direction to simultaneously attack all surrogate models. This strategy has been proven to enhance generalization ability. Experimental results on ImageNet validate the effectiveness of our approach in improving adversarial transferability. Our method outperforms state-of-the-art attackers on 18 discriminative classifiers and adversarially trained models.","sentences":["Deep neural networks are vulnerable to adversarial examples, posing a threat to the models' applications and raising security concerns.","An intriguing property of adversarial examples is their strong transferability.","Several methods have been proposed to enhance transferability, including ensemble attacks which have demonstrated their efficacy.","However, prior approaches simply average logits, probabilities, or losses for model ensembling, lacking a comprehensive analysis of how and why model ensembling significantly improves transferability.","In this paper, we propose a similar targeted attack method named Similar Target~(ST).","By promoting cosine similarity between the gradients of each model, our method regularizes the optimization direction to simultaneously attack all surrogate models.","This strategy has been proven to enhance generalization ability.","Experimental results on ImageNet validate the effectiveness of our approach in improving adversarial transferability.","Our method outperforms state-of-the-art attackers on 18 discriminative classifiers and adversarially trained models."],"url":"http://arxiv.org/abs/2308.10743v1"}
{"created":"2023-08-21 14:09:09","title":"On the Adversarial Robustness of Multi-Modal Foundation Models","abstract":"Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model.","sentences":["Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest.","Alignment of foundation models is used to prevent models from providing toxic or harmful output.","While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content.","In this paper we show that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information.","This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model."],"url":"http://arxiv.org/abs/2308.10741v1"}
{"created":"2023-08-21 14:08:42","title":"We Don't Need No Adam, All We Need Is EVE: On The Variance of Dual Learning Rate And Beyond","abstract":"In the rapidly advancing field of deep learning, optimising deep neural networks is paramount. This paper introduces a novel method, Enhanced Velocity Estimation (EVE), which innovatively applies different learning rates to distinct components of the gradients. By bifurcating the learning rate, EVE enables more nuanced control and faster convergence, addressing the challenges associated with traditional single learning rate approaches. Utilising a momentum term that adapts to the learning landscape, the method achieves a more efficient navigation of the complex loss surface, resulting in enhanced performance and stability. Extensive experiments demonstrate that EVE significantly outperforms existing optimisation techniques across various benchmark datasets and architectures.","sentences":["In the rapidly advancing field of deep learning, optimising deep neural networks is paramount.","This paper introduces a novel method, Enhanced Velocity Estimation (EVE), which innovatively applies different learning rates to distinct components of the gradients.","By bifurcating the learning rate, EVE enables more nuanced control and faster convergence, addressing the challenges associated with traditional single learning rate approaches.","Utilising a momentum term that adapts to the learning landscape, the method achieves a more efficient navigation of the complex loss surface, resulting in enhanced performance and stability.","Extensive experiments demonstrate that EVE significantly outperforms existing optimisation techniques across various benchmark datasets and architectures."],"url":"http://arxiv.org/abs/2308.10740v1"}
{"created":"2023-08-21 14:05:21","title":"UGSL: A Unified Framework for Benchmarking Graph Structure Learning","abstract":"Graph neural networks (GNNs) demonstrate outstanding performance in a broad range of applications. While the majority of GNN applications assume that a graph structure is given, some recent methods substantially expanded the applicability of GNNs by showing that they may be effective even when no graph structure is explicitly provided. The GNN parameters and a graph structure are jointly learned. Previous studies adopt different experimentation setups, making it difficult to compare their merits. In this paper, we propose a benchmarking strategy for graph structure learning using a unified framework. Our framework, called Unified Graph Structure Learning (UGSL), reformulates existing models into a single model. We implement a wide range of existing models in our framework and conduct extensive analyses of the effectiveness of different components in the framework. Our results provide a clear and concise understanding of the different methods in this area as well as their strengths and weaknesses. The benchmark code is available at https://github.com/google-research/google-research/tree/master/ugsl.","sentences":["Graph neural networks (GNNs) demonstrate outstanding performance in a broad range of applications.","While the majority of GNN applications assume that a graph structure is given, some recent methods substantially expanded the applicability of GNNs by showing that they may be effective even when no graph structure is explicitly provided.","The GNN parameters and a graph structure are jointly learned.","Previous studies adopt different experimentation setups, making it difficult to compare their merits.","In this paper, we propose a benchmarking strategy for graph structure learning using a unified framework.","Our framework, called Unified Graph Structure Learning (UGSL), reformulates existing models into a single model.","We implement a wide range of existing models in our framework and conduct extensive analyses of the effectiveness of different components in the framework.","Our results provide a clear and concise understanding of the different methods in this area as well as their strengths and weaknesses.","The benchmark code is available at https://github.com/google-research/google-research/tree/master/ugsl."],"url":"http://arxiv.org/abs/2308.10737v1"}
{"created":"2023-08-21 14:01:07","title":"Different Types of Isomorphisms of Drawings of Complete Multipartite Graphs","abstract":"Simple drawings are drawings of graphs in which any two edges intersect at most once (either at a common endpoint or a proper crossing), and no edge intersects itself. We analyze several characteristics of simple drawings of complete multipartite graphs: which pairs of edges cross, in which order they cross, and the cyclic order around vertices and crossings, respectively. We consider all possible combinations of how two drawings can share some characteristics and determine which other characteristics they imply and which they do not imply. Our main results are that for simple drawings of complete multipartite graphs, the orders in which edges cross determine all other considered characteristics. Further, if all partition classes have at least three vertices, then the pairs of edges that cross determine the rotation system and the rotation around the crossings determine the extended rotation system. We also show that most other implications -- including the ones that hold for complete graphs -- do not hold for complete multipartite graphs. Using this analysis, we establish which types of isomorphisms are meaningful for simple drawings of complete multipartite graphs.","sentences":["Simple drawings are drawings of graphs in which any two edges intersect at most once (either at a common endpoint or a proper crossing), and no edge intersects itself.","We analyze several characteristics of simple drawings of complete multipartite graphs: which pairs of edges cross, in which order they cross, and the cyclic order around vertices and crossings, respectively.","We consider all possible combinations of how two drawings can share some characteristics and determine which other characteristics they imply and which they do not imply.","Our main results are that for simple drawings of complete multipartite graphs, the orders in which edges cross determine all other considered characteristics.","Further, if all partition classes have at least three vertices, then the pairs of edges that cross determine the rotation system and the rotation around the crossings determine the extended rotation system.","We also show that most other implications -- including the ones that hold for complete graphs -- do not hold for complete multipartite graphs.","Using this analysis, we establish which types of isomorphisms are meaningful for simple drawings of complete multipartite graphs."],"url":"http://arxiv.org/abs/2308.10735v1"}
{"created":"2023-08-21 13:54:00","title":"Patch Is Not All You Need","abstract":"Vision Transformers have achieved great success in computer visions, delivering exceptional performance across various tasks. However, their inherent reliance on sequential input enforces the manual partitioning of images into patch sequences, which disrupts the image's inherent structural and semantic continuity. To handle this, we propose a novel Pattern Transformer (Patternformer) to adaptively convert images to pattern sequences for Transformer input. Specifically, we employ the Convolutional Neural Network to extract various patterns from the input image, with each channel representing a unique pattern that is fed into the succeeding Transformer as a visual token. By enabling the network to optimize these patterns, each pattern concentrates on its local region of interest, thereby preserving its intrinsic structural and semantic information. Only employing the vanilla ResNet and Transformer, we have accomplished state-of-the-art performance on CIFAR-10 and CIFAR-100, and have achieved competitive results on ImageNet.","sentences":["Vision Transformers have achieved great success in computer visions, delivering exceptional performance across various tasks.","However, their inherent reliance on sequential input enforces the manual partitioning of images into patch sequences, which disrupts the image's inherent structural and semantic continuity.","To handle this, we propose a novel Pattern Transformer (Patternformer) to adaptively convert images to pattern sequences for Transformer input.","Specifically, we employ the Convolutional Neural Network to extract various patterns from the input image, with each channel representing a unique pattern that is fed into the succeeding Transformer as a visual token.","By enabling the network to optimize these patterns, each pattern concentrates on its local region of interest, thereby preserving its intrinsic structural and semantic information.","Only employing the vanilla ResNet and Transformer, we have accomplished state-of-the-art performance on CIFAR-10 and CIFAR-100, and have achieved competitive results on ImageNet."],"url":"http://arxiv.org/abs/2308.10729v1"}
{"created":"2023-08-21 13:50:41","title":"Test-time augmentation-based active learning and self-training for label-efficient segmentation","abstract":"Deep learning techniques depend on large datasets whose annotation is time-consuming. To reduce annotation burden, the self-training (ST) and active-learning (AL) methods have been developed as well as methods that combine them in an iterative fashion. However, it remains unclear when each method is the most useful, and when it is advantageous to combine them. In this paper, we propose a new method that combines ST with AL using Test-Time Augmentations (TTA). First, TTA is performed on an initial teacher network. Then, cases for annotation are selected based on the lowest estimated Dice score. Cases with high estimated scores are used as soft pseudo-labels for ST. The selected annotated cases are trained with existing annotated cases and ST cases with border slices annotations. We demonstrate the method on MRI fetal body and placenta segmentation tasks with different data variability characteristics. Our results indicate that ST is highly effective for both tasks, boosting performance for in-distribution (ID) and out-of-distribution (OOD) data. However, while self-training improved the performance of single-sequence fetal body segmentation when combined with AL, it slightly deteriorated performance of multi-sequence placenta segmentation on ID data. AL was helpful for the high variability placenta data, but did not improve upon random selection for the single-sequence body data. For fetal body segmentation sequence transfer, combining AL with ST following ST iteration yielded a Dice of 0.961 with only 6 original scans and 2 new sequence scans. Results using only 15 high-variability placenta cases were similar to those using 50 cases. Code is available at: https://github.com/Bella31/TTA-quality-estimation-ST-AL","sentences":["Deep learning techniques depend on large datasets whose annotation is time-consuming.","To reduce annotation burden, the self-training (ST) and active-learning (AL) methods have been developed as well as methods that combine them in an iterative fashion.","However, it remains unclear when each method is the most useful, and when it is advantageous to combine them.","In this paper, we propose a new method that combines ST with AL using Test-Time Augmentations (TTA).","First, TTA is performed on an initial teacher network.","Then, cases for annotation are selected based on the lowest estimated Dice score.","Cases with high estimated scores are used as soft pseudo-labels for ST.","The selected annotated cases are trained with existing annotated cases and ST cases with border slices annotations.","We demonstrate the method on MRI fetal body and placenta segmentation tasks with different data variability characteristics.","Our results indicate that ST is highly effective for both tasks, boosting performance for in-distribution (ID) and out-of-distribution (OOD) data.","However, while self-training improved the performance of single-sequence fetal body segmentation when combined with AL, it slightly deteriorated performance of multi-sequence placenta segmentation on ID data.","AL was helpful for the high variability placenta data, but did not improve upon random selection for the single-sequence body data.","For fetal body segmentation sequence transfer, combining AL with ST following ST iteration yielded a Dice of 0.961 with only 6 original scans and 2 new sequence scans.","Results using only 15 high-variability placenta cases were similar to those using 50 cases.","Code is available at: https://github.com/Bella31/TTA-quality-estimation-ST-AL"],"url":"http://arxiv.org/abs/2308.10727v1"}
{"created":"2023-08-21 13:47:41","title":"Global visibility of publications through Digital Object Identifiers","abstract":"This brief research report analyzes the availability of Digital Object Identifiers (DOIs) worldwide, highlighting the dominance of large publishing houses and the need for unique persistent identifiers to increase the visibility of publications from developing countries. The study reveals that a considerable amount of publications from developing countries are excluded from the global flow of scientific information due to the absence of DOIs, emphasizing the need for alternative publishing models. The authors suggest that the availability of DOIs should receive more attention in scholarly communication and scientometrics, contributing to a necessary debate on DOIs relevant for librarians, publishers, and scientometricians.","sentences":["This brief research report analyzes the availability of Digital Object Identifiers (DOIs) worldwide, highlighting the dominance of large publishing houses and the need for unique persistent identifiers to increase the visibility of publications from developing countries.","The study reveals that a considerable amount of publications from developing countries are excluded from the global flow of scientific information due to the absence of DOIs, emphasizing the need for alternative publishing models.","The authors suggest that the availability of DOIs should receive more attention in scholarly communication and scientometrics, contributing to a necessary debate on DOIs relevant for librarians, publishers, and scientometricians."],"url":"http://arxiv.org/abs/2308.10724v1"}
{"created":"2023-08-21 13:47:13","title":"Clustered Linear Contextual Bandits with Knapsacks","abstract":"In this work, we study clustered contextual bandits where rewards and resource consumption are the outcomes of cluster-specific linear models. The arms are divided in clusters, with the cluster memberships being unknown to an algorithm. Pulling an arm in a time period results in a reward and in consumption for each one of multiple resources, and with the total consumption of any resource exceeding a constraint implying the termination of the algorithm. Thus, maximizing the total reward requires learning not only models about the reward and the resource consumption, but also cluster memberships. We provide an algorithm that achieves regret sublinear in the number of time periods, without requiring access to all of the arms. In particular, we show that it suffices to perform clustering only once to a randomly selected subset of the arms. To achieve this result, we provide a sophisticated combination of techniques from the literature of econometrics and of bandits with constraints.","sentences":["In this work, we study clustered contextual bandits where rewards and resource consumption are the outcomes of cluster-specific linear models.","The arms are divided in clusters, with the cluster memberships being unknown to an algorithm.","Pulling an arm in a time period results in a reward and in consumption for each one of multiple resources, and with the total consumption of any resource exceeding a constraint implying the termination of the algorithm.","Thus, maximizing the total reward requires learning not only models about the reward and the resource consumption, but also cluster memberships.","We provide an algorithm that achieves regret sublinear in the number of time periods, without requiring access to all of the arms.","In particular, we show that it suffices to perform clustering only once to a randomly selected subset of the arms.","To achieve this result, we provide a sophisticated combination of techniques from the literature of econometrics and of bandits with constraints."],"url":"http://arxiv.org/abs/2308.10722v1"}
{"created":"2023-08-21 13:45:44","title":"CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision Making","abstract":"Robust coordination skills enable agents to operate cohesively in shared environments, together towards a common goal and, ideally, individually without hindering each other's progress. To this end, this paper presents Coordinated QMIX (CoMIX), a novel training framework for decentralized agents that enables emergent coordination through flexible policies, allowing at the same time independent decision-making at individual level. CoMIX models selfish and collaborative behavior as incremental steps in each agent's decision process. This allows agents to dynamically adapt their behavior to different situations balancing independence and collaboration. Experiments using a variety of simulation environments demonstrate that CoMIX outperforms baselines on collaborative tasks. The results validate our incremental policy approach as effective technique for improving coordination in multi-agent systems.","sentences":["Robust coordination skills enable agents to operate cohesively in shared environments, together towards a common goal and, ideally, individually without hindering each other's progress.","To this end, this paper presents Coordinated QMIX (CoMIX), a novel training framework for decentralized agents that enables emergent coordination through flexible policies, allowing at the same time independent decision-making at individual level.","CoMIX models selfish and collaborative behavior as incremental steps in each agent's decision process.","This allows agents to dynamically adapt their behavior to different situations balancing independence and collaboration.","Experiments using a variety of simulation environments demonstrate that CoMIX outperforms baselines on collaborative tasks.","The results validate our incremental policy approach as effective technique for improving coordination in multi-agent systems."],"url":"http://arxiv.org/abs/2308.10721v1"}
{"created":"2023-08-21 13:39:04","title":"Backdooring Textual Inversion for Concept Censorship","abstract":"Recent years have witnessed success in AIGC (AI Generated Content). People can make use of a pre-trained diffusion model to generate images of high quality or freely modify existing pictures with only prompts in nature language. More excitingly, the emerging personalization techniques make it feasible to create specific-desired images with only a few images as references. However, this induces severe threats if such advanced techniques are misused by malicious users, such as spreading fake news or defaming individual reputations. Thus, it is necessary to regulate personalization models (i.e., concept censorship) for their development and advancement.   In this paper, we focus on the personalization technique dubbed Textual Inversion (TI), which is becoming prevailing for its lightweight nature and excellent performance. TI crafts the word embedding that contains detailed information about a specific object. Users can easily download the word embedding from public websites like Civitai and add it to their own stable diffusion model without fine-tuning for personalization. To achieve the concept censorship of a TI model, we propose leveraging the backdoor technique for good by injecting backdoors into the Textual Inversion embeddings. Briefly, we select some sensitive words as triggers during the training of TI, which will be censored for normal use. In the subsequent generation stage, if the triggers are combined with personalized embeddings as final prompts, the model will output a pre-defined target image rather than images including the desired malicious concept.   To demonstrate the effectiveness of our approach, we conduct extensive experiments on Stable Diffusion, a prevailing open-sourced text-to-image model. Our code, data, and results are available at https://concept-censorship.github.io.","sentences":["Recent years have witnessed success in AIGC (AI Generated Content).","People can make use of a pre-trained diffusion model to generate images of high quality or freely modify existing pictures with only prompts in nature language.","More excitingly, the emerging personalization techniques make it feasible to create specific-desired images with only a few images as references.","However, this induces severe threats if such advanced techniques are misused by malicious users, such as spreading fake news or defaming individual reputations.","Thus, it is necessary to regulate personalization models (i.e., concept censorship) for their development and advancement.   ","In this paper, we focus on the personalization technique dubbed Textual Inversion (TI), which is becoming prevailing for its lightweight nature and excellent performance.","TI crafts the word embedding that contains detailed information about a specific object.","Users can easily download the word embedding from public websites like Civitai and add it to their own stable diffusion model without fine-tuning for personalization.","To achieve the concept censorship of a TI model, we propose leveraging the backdoor technique for good by injecting backdoors into the Textual Inversion embeddings.","Briefly, we select some sensitive words as triggers during the training of TI, which will be censored for normal use.","In the subsequent generation stage, if the triggers are combined with personalized embeddings as final prompts, the model will output a pre-defined target image rather than images including the desired malicious concept.   ","To demonstrate the effectiveness of our approach, we conduct extensive experiments on Stable Diffusion, a prevailing open-sourced text-to-image model.","Our code, data, and results are available at https://concept-censorship.github.io."],"url":"http://arxiv.org/abs/2308.10718v1"}
{"created":"2023-08-21 13:38:10","title":"Rethinking Person Re-identification from a Projection-on-Prototypes Perspective","abstract":"Person Re-IDentification (Re-ID) as a retrieval task, has achieved tremendous development over the past decade. Existing state-of-the-art methods follow an analogous framework to first extract features from the input images and then categorize them with a classifier. However, since there is no identity overlap between training and testing sets, the classifier is often discarded during inference. Only the extracted features are used for person retrieval via distance metrics. In this paper, we rethink the role of the classifier in person Re-ID, and advocate a new perspective to conceive the classifier as a projection from image features to class prototypes. These prototypes are exactly the learned parameters of the classifier. In this light, we describe the identity of input images as similarities to all prototypes, which are then utilized as more discriminative features to perform person Re-ID. We thereby propose a new baseline ProNet, which innovatively reserves the function of the classifier at the inference stage. To facilitate the learning of class prototypes, both triplet loss and identity classification loss are applied to features that undergo the projection by the classifier. An improved version of ProNet++ is presented by further incorporating multi-granularity designs. Experiments on four benchmarks demonstrate that our proposed ProNet is simple yet effective, and significantly beats previous baselines. ProNet++ also achieves competitive or even better results than transformer-based competitors.","sentences":["Person Re-IDentification (Re-ID) as a retrieval task, has achieved tremendous development over the past decade.","Existing state-of-the-art methods follow an analogous framework to first extract features from the input images and then categorize them with a classifier.","However, since there is no identity overlap between training and testing sets, the classifier is often discarded during inference.","Only the extracted features are used for person retrieval via distance metrics.","In this paper, we rethink the role of the classifier in person Re-ID, and advocate a new perspective to conceive the classifier as a projection from image features to class prototypes.","These prototypes are exactly the learned parameters of the classifier.","In this light, we describe the identity of input images as similarities to all prototypes, which are then utilized as more discriminative features to perform person Re-ID.","We thereby propose a new baseline ProNet, which innovatively reserves the function of the classifier at the inference stage.","To facilitate the learning of class prototypes, both triplet loss and identity classification loss are applied to features that undergo the projection by the classifier.","An improved version of ProNet++ is presented by further incorporating multi-granularity designs.","Experiments on four benchmarks demonstrate that our proposed ProNet is simple yet effective, and significantly beats previous baselines.","ProNet++ also achieves competitive or even better results than transformer-based competitors."],"url":"http://arxiv.org/abs/2308.10717v1"}
{"created":"2023-08-21 13:38:09","title":"Color Prompting for Data-Free Continual Unsupervised Domain Adaptive Person Re-Identification","abstract":"Unsupervised domain adaptive person re-identification (Re-ID) methods alleviate the burden of data annotation through generating pseudo supervision messages. However, real-world Re-ID systems, with continuously accumulating data streams, simultaneously demand more robust adaptation and anti-forgetting capabilities. Methods based on image rehearsal addresses the forgetting issue with limited extra storage but carry the risk of privacy leakage. In this work, we propose a Color Prompting (CoP) method for data-free continual unsupervised domain adaptive person Re-ID. Specifically, we employ a light-weighted prompter network to fit the color distribution of the current task together with Re-ID training. Then for the incoming new tasks, the learned color distribution serves as color style transfer guidance to transfer the images into past styles. CoP achieves accurate color style recovery for past tasks with adequate data diversity, leading to superior anti-forgetting effects compared with image rehearsal methods. Moreover, CoP demonstrates strong generalization performance for fast adaptation into new domains, given only a small amount of unlabeled images. Extensive experiments demonstrate that after the continual training pipeline the proposed CoP achieves 6.7% and 8.1% average rank-1 improvements over the replay method on seen and unseen domains, respectively. The source code for this work is publicly available in https://github.com/vimar-gu/ColorPromptReID.","sentences":["Unsupervised domain adaptive person re","-identification (Re-ID) methods alleviate the burden of data annotation through generating pseudo supervision messages.","However, real-world Re-ID systems, with continuously accumulating data streams, simultaneously demand more robust adaptation and anti-forgetting capabilities.","Methods based on image rehearsal addresses the forgetting issue with limited extra storage but carry the risk of privacy leakage.","In this work, we propose a Color Prompting (CoP) method for data-free continual unsupervised domain adaptive person Re-ID.","Specifically, we employ a light-weighted prompter network to fit the color distribution of the current task together with Re-ID training.","Then for the incoming new tasks, the learned color distribution serves as color style transfer guidance to transfer the images into past styles.","CoP achieves accurate color style recovery for past tasks with adequate data diversity, leading to superior anti-forgetting effects compared with image rehearsal methods.","Moreover, CoP demonstrates strong generalization performance for fast adaptation into new domains, given only a small amount of unlabeled images.","Extensive experiments demonstrate that after the continual training pipeline the proposed CoP achieves 6.7% and 8.1% average rank-1 improvements over the replay method on seen and unseen domains, respectively.","The source code for this work is publicly available in https://github.com/vimar-gu/ColorPromptReID."],"url":"http://arxiv.org/abs/2308.10716v1"}
{"created":"2023-08-21 13:27:27","title":"CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach","abstract":"In the landscape of High-Performance Computing (HPC), the quest for efficient and scalable memory solutions remains paramount. The advent of Compute Express Link (CXL) introduces a promising avenue with its potential to function as a Persistent Memory (PMem) solution in the context of disaggregated HPC systems. This paper presents a comprehensive exploration of CXL memory's viability as a candidate for PMem, supported by physical experiments conducted on cutting-edge multi-NUMA nodes equipped with CXL-attached memory prototypes. Our study not only benchmarks the performance of CXL memory but also illustrates the seamless transition from traditional PMem programming models to CXL, reinforcing its practicality.   To substantiate our claims, we establish a tangible CXL prototype using an FPGA card embodying CXL 1.1/2.0 compliant endpoint designs (Intel FPGA CXL IP). Performance evaluations, executed through the STREAM and STREAM-PMem benchmarks, showcase CXL memory's ability to mirror PMem characteristics in App-Direct and Memory Mode while achieving impressive bandwidth metrics with Intel 4th generation Xeon (Sapphire Rapids) processors.   The results elucidate the feasibility of CXL memory as a persistent memory solution, outperforming previously established benchmarks. In contrast to published DCPMM results, our CXL-DDR4 memory module offers comparable bandwidth to local DDR4 memory configurations, albeit with a moderate decrease in performance. The modified STREAM-PMem application underscores the ease of transitioning programming models from PMem to CXL, thus underscoring the practicality of adopting CXL memory.","sentences":["In the landscape of High-Performance Computing (HPC), the quest for efficient and scalable memory solutions remains paramount.","The advent of Compute Express Link (CXL) introduces a promising avenue with its potential to function as a Persistent Memory (PMem) solution in the context of disaggregated HPC systems.","This paper presents a comprehensive exploration of CXL memory's viability as a candidate for PMem, supported by physical experiments conducted on cutting-edge multi-NUMA nodes equipped with CXL-attached memory prototypes.","Our study not only benchmarks the performance of CXL memory but also illustrates the seamless transition from traditional PMem programming models to CXL, reinforcing its practicality.   ","To substantiate our claims, we establish a tangible CXL prototype using an FPGA card embodying","CXL 1.1/2.0 compliant endpoint designs (Intel FPGA CXL IP).","Performance evaluations, executed through the STREAM and STREAM-PMem benchmarks, showcase CXL memory's ability to mirror PMem characteristics in App-Direct and Memory Mode while achieving impressive bandwidth metrics with Intel 4th generation Xeon (Sapphire Rapids) processors.   ","The results elucidate the feasibility of CXL memory as a persistent memory solution, outperforming previously established benchmarks.","In contrast to published DCPMM results, our CXL-DDR4 memory module offers comparable bandwidth to local DDR4 memory configurations, albeit with a moderate decrease in performance.","The modified STREAM-PMem application underscores the ease of transitioning programming models from PMem to CXL, thus underscoring the practicality of adopting CXL memory."],"url":"http://arxiv.org/abs/2308.10714v1"}
{"created":"2023-08-21 13:24:52","title":"Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization","abstract":"In recent years, bilevel approaches have become very popular to efficiently estimate high-dimensional hyperparameters of machine learning models. However, to date, binary parameters are handled by continuous relaxation and rounding strategies, which could lead to inconsistent solutions. In this context, we tackle the challenging optimization of mixed-binary hyperparameters by resorting to an equivalent continuous bilevel reformulation based on an appropriate penalty term. We propose an algorithmic framework that, under suitable assumptions, is guaranteed to provide mixed-binary solutions. Moreover, the generality of the method allows to safely use existing continuous bilevel solvers within the proposed framework. We evaluate the performance of our approach for a specific machine learning problem, i.e., the estimation of the group-sparsity structure in regression problems. Reported results clearly show that our method outperforms state-of-the-art approaches based on relaxation and rounding","sentences":["In recent years, bilevel approaches have become very popular to efficiently estimate high-dimensional hyperparameters of machine learning models.","However, to date, binary parameters are handled by continuous relaxation and rounding strategies, which could lead to inconsistent solutions.","In this context, we tackle the challenging optimization of mixed-binary hyperparameters by resorting to an equivalent continuous bilevel reformulation based on an appropriate penalty term.","We propose an algorithmic framework that, under suitable assumptions, is guaranteed to provide mixed-binary solutions.","Moreover, the generality of the method allows to safely use existing continuous bilevel solvers within the proposed framework.","We evaluate the performance of our approach for a specific machine learning problem, i.e., the estimation of the group-sparsity structure in regression problems.","Reported results clearly show that our method outperforms state-of-the-art approaches based on relaxation and rounding"],"url":"http://arxiv.org/abs/2308.10711v1"}
{"created":"2023-08-21 13:22:12","title":"Measuring the Effect of Causal Disentanglement on the Adversarial Robustness of Neural Network Models","abstract":"Causal Neural Network models have shown high levels of robustness to adversarial attacks as well as an increased capacity for generalisation tasks such as few-shot learning and rare-context classification compared to traditional Neural Networks. This robustness is argued to stem from the disentanglement of causal and confounder input signals. However, no quantitative study has yet measured the level of disentanglement achieved by these types of causal models or assessed how this relates to their adversarial robustness.   Existing causal disentanglement metrics are not applicable to deterministic models trained on real-world datasets. We, therefore, utilise metrics of content/style disentanglement from the field of Computer Vision to measure different aspects of the causal disentanglement for four state-of-the-art causal Neural Network models. By re-implementing these models with a common ResNet18 architecture we are able to fairly measure their adversarial robustness on three standard image classification benchmarking datasets under seven common white-box attacks. We find a strong association (r=0.820, p=0.001) between the degree to which models decorrelate causal and confounder signals and their adversarial robustness. Additionally, we find a moderate negative association between the pixel-level information content of the confounder signal and adversarial robustness (r=-0.597, p=0.040).","sentences":["Causal Neural Network models have shown high levels of robustness to adversarial attacks as well as an increased capacity for generalisation tasks such as few-shot learning and rare-context classification compared to traditional Neural Networks.","This robustness is argued to stem from the disentanglement of causal and confounder input signals.","However, no quantitative study has yet measured the level of disentanglement achieved by these types of causal models or assessed how this relates to their adversarial robustness.   ","Existing causal disentanglement metrics are not applicable to deterministic models trained on real-world datasets.","We, therefore, utilise metrics of content/style disentanglement from the field of Computer Vision to measure different aspects of the causal disentanglement for four state-of-the-art causal Neural Network models.","By re-implementing these models with a common ResNet18 architecture we are able to fairly measure their adversarial robustness on three standard image classification benchmarking datasets under seven common white-box attacks.","We find a strong association (r=0.820, p=0.001) between the degree to which models decorrelate causal and confounder signals and their adversarial robustness.","Additionally, we find a moderate negative association between the pixel-level information content of the confounder signal and adversarial robustness (r=-0.597, p=0.040)."],"url":"http://arxiv.org/abs/2308.10708v1"}
{"created":"2023-08-21 13:18:12","title":"Sampling From Autoencoders' Latent Space via Quantization And Probability Mass Function Concepts","abstract":"In this study, we focus on sampling from the latent space of generative models built upon autoencoders so as the reconstructed samples are lifelike images. To do to, we introduce a novel post-training sampling algorithm rooted in the concept of probability mass functions, coupled with a quantization process. Our proposed algorithm establishes a vicinity around each latent vector from the input data and then proceeds to draw samples from these defined neighborhoods. This strategic approach ensures that the sampled latent vectors predominantly inhabit high-probability regions, which, in turn, can be effectively transformed into authentic real-world images. A noteworthy point of comparison for our sampling algorithm is the sampling technique based on Gaussian mixture models (GMM), owing to its inherent capability to represent clusters. Remarkably, we manage to improve the time complexity from the previous $\\mathcal{O}(n\\times d \\times k \\times i)$ associated with GMM sampling to a much more streamlined $\\mathcal{O}(n\\times d)$, thereby resulting in substantial speedup during runtime. Moreover, our experimental results, gauged through the Fr\\'echet inception distance (FID) for image generation, underscore the superior performance of our sampling algorithm across a diverse range of models and datasets. On the MNIST benchmark dataset, our approach outperforms GMM sampling by yielding a noteworthy improvement of up to $0.89$ in FID value. Furthermore, when it comes to generating images of faces and ocular images, our approach showcases substantial enhancements with FID improvements of $1.69$ and $0.87$ respectively, as compared to GMM sampling, as evidenced on the CelebA and MOBIUS datasets. Lastly, we substantiate our methodology's efficacy in estimating latent space distributions in contrast to GMM sampling, particularly through the lens of the Wasserstein distance.","sentences":["In this study, we focus on sampling from the latent space of generative models built upon autoencoders so as the reconstructed samples are lifelike images.","To do to, we introduce a novel post-training sampling algorithm rooted in the concept of probability mass functions, coupled with a quantization process.","Our proposed algorithm establishes a vicinity around each latent vector from the input data and then proceeds to draw samples from these defined neighborhoods.","This strategic approach ensures that the sampled latent vectors predominantly inhabit high-probability regions, which, in turn, can be effectively transformed into authentic real-world images.","A noteworthy point of comparison for our sampling algorithm is the sampling technique based on Gaussian mixture models (GMM), owing to its inherent capability to represent clusters.","Remarkably, we manage to improve the time complexity from the previous $\\mathcal{O}(n\\times d","\\times k \\times i)$ associated with GMM sampling to a much more streamlined $\\mathcal{O}(n\\times d)$, thereby resulting in substantial speedup during runtime.","Moreover, our experimental results, gauged through the Fr\\'echet inception distance (FID) for image generation, underscore the superior performance of our sampling algorithm across a diverse range of models and datasets.","On the MNIST benchmark dataset, our approach outperforms GMM sampling by yielding a noteworthy improvement of up to $0.89$ in FID value.","Furthermore, when it comes to generating images of faces and ocular images, our approach showcases substantial enhancements with FID improvements of $1.69$ and $0.87$ respectively, as compared to GMM sampling, as evidenced on the CelebA and MOBIUS datasets.","Lastly, we substantiate our methodology's efficacy in estimating latent space distributions in contrast to GMM sampling, particularly through the lens of the Wasserstein distance."],"url":"http://arxiv.org/abs/2308.10704v1"}
{"created":"2023-08-21 13:13:56","title":"Bayesian Optimal Experimental Design for Constitutive Model Calibration","abstract":"Computational simulation is increasingly relied upon for high-consequence engineering decisions, and a foundational element to solid mechanics simulations, such as finite element analysis (FEA), is a credible constitutive or material model. Calibration of these complex models is an essential step; however, the selection, calibration and validation of material models is often a discrete, multi-stage process that is decoupled from material characterization activities, which means the data collected does not always align with the data that is needed. To address this issue, an integrated workflow for delivering an enhanced characterization and calibration procedure (Interlaced Characterization and Calibration (ICC)) is introduced. This framework leverages Bayesian optimal experimental design (BOED) to select the optimal load path for a cruciform specimen in order to collect the most informative data for model calibration. The critical first piece of algorithm development is to demonstrate the active experimental design for a fast model with simulated data. For this demonstration, a material point simulator that models a plane stress elastoplastic material subject to bi-axial loading was chosen. The ICC framework is demonstrated on two exemplar problems in which BOED is used to determine which load step to take, e.g., in which direction to increment the strain, at each iteration of the characterization and calibration cycle. Calibration results from data obtained by adaptively selecting the load path within the ICC algorithm are compared to results from data generated under two naive static load paths that were chosen a priori based on human intuition. In these exemplar problems, data generated in an adaptive setting resulted in calibrated model parameters with reduced measures of uncertainty compared to the static settings.","sentences":["Computational simulation is increasingly relied upon for high-consequence engineering decisions, and a foundational element to solid mechanics simulations, such as finite element analysis (FEA), is a credible constitutive or material model.","Calibration of these complex models is an essential step; however, the selection, calibration and validation of material models is often a discrete, multi-stage process that is decoupled from material characterization activities, which means the data collected does not always align with the data that is needed.","To address this issue, an integrated workflow for delivering an enhanced characterization and calibration procedure (Interlaced Characterization and Calibration (ICC)) is introduced.","This framework leverages Bayesian optimal experimental design (BOED) to select the optimal load path for a cruciform specimen in order to collect the most informative data for model calibration.","The critical first piece of algorithm development is to demonstrate the active experimental design for a fast model with simulated data.","For this demonstration, a material point simulator that models a plane stress elastoplastic material subject to bi-axial loading was chosen.","The ICC framework is demonstrated on two exemplar problems in which BOED is used to determine which load step to take, e.g., in which direction to increment the strain, at each iteration of the characterization and calibration cycle.","Calibration results from data obtained by adaptively selecting the load path within the ICC algorithm are compared to results from data generated under two naive static load paths that were chosen a priori based on human intuition.","In these exemplar problems, data generated in an adaptive setting resulted in calibrated model parameters with reduced measures of uncertainty compared to the static settings."],"url":"http://arxiv.org/abs/2308.10702v1"}
{"created":"2023-08-21 13:09:31","title":"Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach","abstract":"Online decision making plays a crucial role in numerous real-world applications. In many scenarios, the decision is made based on performing a sequence of tests on the incoming data points. However, performing all tests can be expensive and is not always possible. In this paper, we provide a novel formulation of the online decision making problem based on combinatorial multi-armed bandits and take the cost of performing tests into account. Based on this formulation, we provide a new framework for cost-efficient online decision making which can utilize posterior sampling or BayesUCB for exploration. We provide a rigorous theoretical analysis for our framework and present various experimental results that demonstrate its applicability to real-world problems.","sentences":["Online decision making plays a crucial role in numerous real-world applications.","In many scenarios, the decision is made based on performing a sequence of tests on the incoming data points.","However, performing all tests can be expensive and is not always possible.","In this paper, we provide a novel formulation of the online decision making problem based on combinatorial multi-armed bandits and take the cost of performing tests into account.","Based on this formulation, we provide a new framework for cost-efficient online decision making which can utilize posterior sampling or BayesUCB for exploration.","We provide a rigorous theoretical analysis for our framework and present various experimental results that demonstrate its applicability to real-world problems."],"url":"http://arxiv.org/abs/2308.10699v1"}
{"created":"2023-08-21 13:04:45","title":"SCC5G: A PQC-based Architecture for Highly Secure Critical Communication over Cellular Network in Zero-Trust Environment","abstract":"5G made a significant jump in cellular network security by offering enhanced subscriber identity protection and a user-network mutual authentication implementation. However, it still does not fully follow the zero-trust (ZT) requirements, as users need to trust the network, 5G network is not necessarily authenticated in each communication instance, and there is no mutual authentication between end users. When critical communications need to use commercial networks, but the environment is ZT, specific security architecture is needed to provide security services that do not rely on any 5G network trusted authority. In this paper, we propose SCC5G Secure Critical-mission Communication over a 5G network in ZT setting. SCC5G is a post-quantum cryptography (PQC) security solution that loads an embedded hardware root of authentication (HRA), such as physically unclonable functions (PUF), into the users' devices, to achieve tamper-resistant and unclonability features for authentication and key agreement. We evaluate the performance of the proposed architecture through an exhaustive simulation of a 5G network in an ns-3 network simulator. Results verify the scalability and efficiency of SCC5G by showing that it poses only a few kilobytes of traffic overhead and adds only an order of $O(0.1)$ second of latency under the normal traffic load.","sentences":["5G made a significant jump in cellular network security by offering enhanced subscriber identity protection and a user-network mutual authentication implementation.","However, it still does not fully follow the zero-trust (ZT) requirements, as users need to trust the network, 5G network is not necessarily authenticated in each communication instance, and there is no mutual authentication between end users.","When critical communications need to use commercial networks, but the environment is ZT, specific security architecture is needed to provide security services that do not rely on any 5G network trusted authority.","In this paper, we propose SCC5G Secure Critical-mission Communication over a 5G network in ZT setting.","SCC5G is a post-quantum cryptography (PQC) security solution that loads an embedded hardware root of authentication (HRA), such as physically unclonable functions (PUF), into the users' devices, to achieve tamper-resistant and unclonability features for authentication and key agreement.","We evaluate the performance of the proposed architecture through an exhaustive simulation of a 5G network in an ns-3 network simulator.","Results verify the scalability and efficiency of SCC5G by showing that it poses only a few kilobytes of traffic overhead and adds only an order of $O(0.1)$ second of latency under the normal traffic load."],"url":"http://arxiv.org/abs/2308.10696v1"}
{"created":"2023-08-21 13:03:25","title":"Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction","abstract":"We tackle the problem of estimating a Manhattan frame, i.e. three orthogonal vanishing points, and the unknown focal length of the camera, leveraging a prior vertical direction. The direction can come from an Inertial Measurement Unit that is a standard component of recent consumer devices, e.g., smartphones. We provide an exhaustive analysis of minimal line configurations and derive two new 2-line solvers, one of which does not suffer from singularities affecting existing solvers. Additionally, we design a new non-minimal method, running on an arbitrary number of lines, to boost the performance in local optimization. Combining all solvers in a hybrid robust estimator, our method achieves increased accuracy even with a rough prior. Experiments on synthetic and real-world datasets demonstrate the superior accuracy of our method compared to the state of the art, while having comparable runtimes. We further demonstrate the applicability of our solvers for relative rotation estimation. The code is available at https://github.com/cvg/VP-Estimation-with-Prior-Gravity.","sentences":["We tackle the problem of estimating a Manhattan frame, i.e. three orthogonal vanishing points, and the unknown focal length of the camera, leveraging a prior vertical direction.","The direction can come from an Inertial Measurement Unit that is a standard component of recent consumer devices, e.g., smartphones.","We provide an exhaustive analysis of minimal line configurations and derive two new 2-line solvers, one of which does not suffer from singularities affecting existing solvers.","Additionally, we design a new non-minimal method, running on an arbitrary number of lines, to boost the performance in local optimization.","Combining all solvers in a hybrid robust estimator, our method achieves increased accuracy even with a rough prior.","Experiments on synthetic and real-world datasets demonstrate the superior accuracy of our method compared to the state of the art, while having comparable runtimes.","We further demonstrate the applicability of our solvers for relative rotation estimation.","The code is available at https://github.com/cvg/VP-Estimation-with-Prior-Gravity."],"url":"http://arxiv.org/abs/2308.10694v1"}
{"created":"2023-08-21 12:59:48","title":"Exploring Fine-Grained Representation and Recomposition for Cloth-Changing Person Re-Identification","abstract":"Cloth-changing person Re-IDentification (Re-ID) is a particularly challenging task, suffering from two limitations of inferior identity-relevant features and limited training samples. Existing methods mainly leverage auxiliary information to facilitate discriminative feature learning, including soft-biometrics features of shapes and gaits, and additional labels of clothing. However, these information may be unavailable in real-world applications. In this paper, we propose a novel FIne-grained Representation and Recomposition (FIRe$^{2}$) framework to tackle both limitations without any auxiliary information. Specifically, we first design a Fine-grained Feature Mining (FFM) module to separately cluster images of each person. Images with similar so-called fine-grained attributes (e.g., clothes and viewpoints) are encouraged to cluster together. An attribute-aware classification loss is introduced to perform fine-grained learning based on cluster labels, which are not shared among different people, promoting the model to learn identity-relevant features. Furthermore, by taking full advantage of the clustered fine-grained attributes, we present a Fine-grained Attribute Recomposition (FAR) module to recompose image features with different attributes in the latent space. It can significantly enhance representations for robust feature learning. Extensive experiments demonstrate that FIRe$^{2}$ can achieve state-of-the-art performance on five widely-used cloth-changing person Re-ID benchmarks.","sentences":["Cloth-changing person Re-IDentification (Re-ID) is a particularly challenging task, suffering from two limitations of inferior identity-relevant features and limited training samples.","Existing methods mainly leverage auxiliary information to facilitate discriminative feature learning, including soft-biometrics features of shapes and gaits, and additional labels of clothing.","However, these information may be unavailable in real-world applications.","In this paper, we propose a novel FIne-grained Representation and Recomposition (FIRe$^{2}$) framework to tackle both limitations without any auxiliary information.","Specifically, we first design a Fine-grained Feature Mining (FFM) module to separately cluster images of each person.","Images with similar so-called fine-grained attributes (e.g., clothes and viewpoints) are encouraged to cluster together.","An attribute-aware classification loss is introduced to perform fine-grained learning based on cluster labels, which are not shared among different people, promoting the model to learn identity-relevant features.","Furthermore, by taking full advantage of the clustered fine-grained attributes, we present a Fine-grained Attribute Recomposition (FAR) module to recompose image features with different attributes in the latent space.","It can significantly enhance representations for robust feature learning.","Extensive experiments demonstrate that FIRe$^{2}$ can achieve state-of-the-art performance on five widely-used cloth-changing person Re-ID benchmarks."],"url":"http://arxiv.org/abs/2308.10692v1"}
{"created":"2023-08-21 12:57:49","title":"Dexterous Soft Hands Linearize Feedback-Control for In-Hand Manipulation","abstract":"This paper presents a feedback-control framework for in-hand manipulation (IHM) with dexterous soft hands that enables the acquisition of manipulation skills in the real-world within minutes. We choose the deformation state of the soft hand as the control variable. To control for a desired deformation state, we use coarsely approximated Jacobians of the actuation-deformation dynamics. These Jacobian are obtained via exploratory actions. This is enabled by the self-stabilizing properties of compliant hands, which allow us to use linear feedback control in the presence of complex contact dynamics. To evaluate the effectiveness of our approach, we show the generalization capabilities for a learned manipulation skill to variations in object size by 100 %, 360 degree changes in palm inclination and to disabling up to 50 % of the involved actuators. In addition, complex manipulations can be obtained by sequencing such feedback-skills.","sentences":["This paper presents a feedback-control framework for in-hand manipulation (IHM) with dexterous soft hands that enables the acquisition of manipulation skills in the real-world within minutes.","We choose the deformation state of the soft hand as the control variable.","To control for a desired deformation state, we use coarsely approximated Jacobians of the actuation-deformation dynamics.","These Jacobian are obtained via exploratory actions.","This is enabled by the self-stabilizing properties of compliant hands, which allow us to use linear feedback control in the presence of complex contact dynamics.","To evaluate the effectiveness of our approach, we show the generalization capabilities for a learned manipulation skill to variations in object size by 100 %, 360 degree changes in palm inclination and to disabling up to 50 % of the involved actuators.","In addition, complex manipulations can be obtained by sequencing such feedback-skills."],"url":"http://arxiv.org/abs/2308.10691v1"}
{"created":"2023-08-21 12:54:46","title":"Towards a knowledge leakage Mitigation framework for mobile Devices in knowledge-intensive Organizations","abstract":"The use of mobile devices in knowledge-intensive organizations while effective and cost-efficient also pose a challenging management problem. Often employees whether deliberately or inadvertently are the cause of knowledge leakage in organizations and the use of mobile devices further exacerbates it. This problem is the result of overly focusing on technical controls while neglecting human factors. Knowledge leakage is a multidimensional problem, and in this paper, we highlight the different dimensions that constitute it. In this study, our contributions are threefold. First, we study knowledge leakage risk (KLR) within the context of mobile devices in knowledge-intensive organizations in Australia. Second, we present a conceptual framework to explain and categorize the mitigation strategies to combat KLR through the use of mobile devices grounded in the literature. And third, we apply the framework to the findings from interviews with security and knowledge managers. Keywords: Knowledge Leakage, Knowledge Risk, Knowledge intensive, Mobile device.","sentences":["The use of mobile devices in knowledge-intensive organizations while effective and cost-efficient also pose a challenging management problem.","Often employees whether deliberately or inadvertently are the cause of knowledge leakage in organizations and the use of mobile devices further exacerbates it.","This problem is the result of overly focusing on technical controls while neglecting human factors.","Knowledge leakage is a multidimensional problem, and in this paper, we highlight the different dimensions that constitute it.","In this study, our contributions are threefold.","First, we study knowledge leakage risk (KLR) within the context of mobile devices in knowledge-intensive organizations in Australia.","Second, we present a conceptual framework to explain and categorize the mitigation strategies to combat KLR through the use of mobile devices grounded in the literature.","And third, we apply the framework to the findings from interviews with security and knowledge managers.","Keywords: Knowledge Leakage, Knowledge Risk, Knowledge intensive, Mobile device."],"url":"http://arxiv.org/abs/2308.10689v1"}
{"created":"2023-08-21 12:47:30","title":"Normative conditional reasoning as a fragment of HOL","abstract":"We report some results regarding the mechanization of normative (preference-based) conditional reasoning. Our focus is on Aqvist's system E for conditional obligation (and its extensions). Our mechanization is achieved via a shallow semantical embedding in Isabelle/HOL. We consider two possible uses of the framework. The first one is as a tool for meta-reasoning about the considered logic. We employ it for the automated verification of deontic correspondences (broadly conceived) and related matters, analogous to what has been previously achieved for the modal logic cube. The second use is as a tool for assessing ethical arguments. We provide a computer encoding of a well-known paradox in population ethics, Parfit's repugnant conclusion. Whether the presented encoding increases or decreases the attractiveness and persuasiveness of the repugnant conclusion is a question we would like to pass on to philosophy and ethics.","sentences":["We report some results regarding the mechanization of normative (preference-based) conditional reasoning.","Our focus is on Aqvist's system E for conditional obligation (and its extensions).","Our mechanization is achieved via a shallow semantical embedding in Isabelle/HOL.","We consider two possible uses of the framework.","The first one is as a tool for meta-reasoning about the considered logic.","We employ it for the automated verification of deontic correspondences (broadly conceived) and related matters, analogous to what has been previously achieved for the modal logic cube.","The second use is as a tool for assessing ethical arguments.","We provide a computer encoding of a well-known paradox in population ethics, Parfit's repugnant conclusion.","Whether the presented encoding increases or decreases the attractiveness and persuasiveness of the repugnant conclusion is a question we would like to pass on to philosophy and ethics."],"url":"http://arxiv.org/abs/2308.10686v1"}
{"created":"2023-08-21 12:38:53","title":"Contrastive Graph Prompt-tuning for Cross-domain Recommendation","abstract":"Recommender systems are frequently challenged by the data sparsity problem. One approach to mitigate this issue is through cross-domain recommendation techniques. In a cross-domain context, sharing knowledge between domains can enhance the effectiveness in the target domain. Recent cross-domain methods have employed a pre-training approach, but we argue that these methods often result in suboptimal fine-tuning, especially with large neural models. Modern language models utilize prompts for efficient model tuning. Such prompts act as a tunable latent vector, allowing for the freezing of the main model parameters. In our research, we introduce the Personalised Graph Prompt-based Recommendation (PGPRec) framework. This leverages the advantages of prompt-tuning. Within this framework, we formulate personalized graph prompts item-wise, rooted in items that a user has previously engaged with. Specifically, we employ Contrastive Learning (CL) to produce pre-trained embeddings that offer greater generalizability in the pre-training phase, ensuring robust training during the tuning phase. Our evaluation of PGPRec in cross-domain scenarios involves comprehensive testing with the top-k recommendation tasks and a cold-start analysis. Our empirical findings, based on four Amazon Review datasets, reveal that the PGPRec framework can decrease the tuned parameters by as much as 74%, maintaining competitive performance. Remarkably, there's an 11.41% enhancement in performance against the leading baseline in cold-start situations.","sentences":["Recommender systems are frequently challenged by the data sparsity problem.","One approach to mitigate this issue is through cross-domain recommendation techniques.","In a cross-domain context, sharing knowledge between domains can enhance the effectiveness in the target domain.","Recent cross-domain methods have employed a pre-training approach, but we argue that these methods often result in suboptimal fine-tuning, especially with large neural models.","Modern language models utilize prompts for efficient model tuning.","Such prompts act as a tunable latent vector, allowing for the freezing of the main model parameters.","In our research, we introduce the Personalised Graph Prompt-based Recommendation (PGPRec) framework.","This leverages the advantages of prompt-tuning.","Within this framework, we formulate personalized graph prompts item-wise, rooted in items that a user has previously engaged with.","Specifically, we employ Contrastive Learning (CL) to produce pre-trained embeddings that offer greater generalizability in the pre-training phase, ensuring robust training during the tuning phase.","Our evaluation of PGPRec in cross-domain scenarios involves comprehensive testing with the top-k recommendation tasks and a cold-start analysis.","Our empirical findings, based on four Amazon Review datasets, reveal that the PGPRec framework can decrease the tuned parameters by as much as 74%, maintaining competitive performance.","Remarkably, there's an 11.41% enhancement in performance against the leading baseline in cold-start situations."],"url":"http://arxiv.org/abs/2308.10685v1"}
{"created":"2023-08-21 12:37:42","title":"Systematic Offensive Stereotyping (SOS) Bias in Language Models","abstract":"Research has shown that language models (LMs) are socially biased. However, toxicity and offensive stereotyping bias in LMs are understudied. In this paper, we investigate the systematic offensive stereotype (SOS) bias in LMs. We propose a method to measure it. Then, we validate the SOS bias and investigate the effectiveness of debias methods from the literature on removing it. Finally, we investigate the impact of the SOS bias in LMs on their performance and their fairness on the task of hate speech detection. Our results suggest that all the inspected LMs are SOS biased. The results suggest that the SOS bias in LMs is reflective of the hate experienced online by the inspected marginalized groups. The results indicate that removing the SOS bias in LMs, using a popular debias method from the literature, leads to worse SOS bias scores. Finally, Our results show no strong evidence that the SOS bias in LMs is impactful on their performance on hate speech detection. On the other hand, there is evidence that the SOS bias in LMs is impactful on their fairness.","sentences":["Research has shown that language models (LMs) are socially biased.","However, toxicity and offensive stereotyping bias in LMs are understudied.","In this paper, we investigate the systematic offensive stereotype (SOS) bias in LMs.","We propose a method to measure it.","Then, we validate the SOS bias and investigate the effectiveness of debias methods from the literature on removing it.","Finally, we investigate the impact of the SOS bias in LMs on their performance and their fairness on the task of hate speech detection.","Our results suggest that all the inspected LMs are SOS biased.","The results suggest that the SOS bias in LMs is reflective of the hate experienced online by the inspected marginalized groups.","The results indicate that removing the SOS bias in LMs, using a popular debias method from the literature, leads to worse SOS bias scores.","Finally, Our results show no strong evidence that the SOS bias in LMs is impactful on their performance on hate speech detection.","On the other hand, there is evidence that the SOS bias in LMs is impactful on their fairness."],"url":"http://arxiv.org/abs/2308.10684v1"}
{"created":"2023-08-21 12:33:35","title":"LibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices","abstract":"We present LibriWASN, a data set whose design follows closely the LibriCSS meeting recognition data set, with the marked difference that the data is recorded with devices that are randomly positioned on a meeting table and whose sampling clocks are not synchronized. Nine different devices, five smartphones with a single recording channel and four microphone arrays, are used to record a total of 29 channels. Other than that, the data set follows closely the LibriCSS design: the same LibriSpeech sentences are played back from eight loudspeakers arranged around a meeting table and the data is organized in subsets with different percentages of speech overlap. LibriWASN is meant as a test set for clock synchronization algorithms, meeting separation, diarization and transcription systems on ad-hoc wireless acoustic sensor networks. Due to its similarity to LibriCSS, meeting transcription systems developed for the former can readily be tested on LibriWASN. The data set is recorded in two different rooms and is complemented with ground-truth diarization information of who speaks when.","sentences":["We present LibriWASN, a data set whose design follows closely the LibriCSS meeting recognition data set, with the marked difference that the data is recorded with devices that are randomly positioned on a meeting table and whose sampling clocks are not synchronized.","Nine different devices, five smartphones with a single recording channel and four microphone arrays, are used to record a total of 29 channels.","Other than that, the data set follows closely the LibriCSS design: the same LibriSpeech sentences are played back from eight loudspeakers arranged around a meeting table and the data is organized in subsets with different percentages of speech overlap.","LibriWASN is meant as a test set for clock synchronization algorithms, meeting separation, diarization and transcription systems on ad-hoc wireless acoustic sensor networks.","Due to its similarity to LibriCSS, meeting transcription systems developed for the former can readily be tested on LibriWASN.","The data set is recorded in two different rooms and is complemented with ground-truth diarization information of who speaks when."],"url":"http://arxiv.org/abs/2308.10682v1"}
{"created":"2023-08-21 12:27:18","title":"Co-Speech Gesture Detection through Multi-phase Sequence Labeling","abstract":"Gestures are integral components of face-to-face communication. They unfold over time, often following predictable movement phases of preparation, stroke, and retraction. Yet, the prevalent approach to automatic gesture detection treats the problem as binary classification, classifying a segment as either containing a gesture or not, thus failing to capture its inherently sequential and contextual nature. To address this, we introduce a novel framework that reframes the task as a multi-phase sequence labeling problem rather than binary classification. Our model processes sequences of skeletal movements over time windows, uses Transformer encoders to learn contextual embeddings, and leverages Conditional Random Fields to perform sequence labeling. We evaluate our proposal on a large dataset of diverse co-speech gestures in task-oriented face-to-face dialogues. The results consistently demonstrate that our method significantly outperforms strong baseline models in detecting gesture strokes. Furthermore, applying Transformer encoders to learn contextual embeddings from movement sequences substantially improves gesture unit detection. These results highlight our framework's capacity to capture the fine-grained dynamics of co-speech gesture phases, paving the way for more nuanced and accurate gesture detection and analysis.","sentences":["Gestures are integral components of face-to-face communication.","They unfold over time, often following predictable movement phases of preparation, stroke, and retraction.","Yet, the prevalent approach to automatic gesture detection treats the problem as binary classification, classifying a segment as either containing a gesture or not, thus failing to capture its inherently sequential and contextual nature.","To address this, we introduce a novel framework that reframes the task as a multi-phase sequence labeling problem rather than binary classification.","Our model processes sequences of skeletal movements over time windows, uses Transformer encoders to learn contextual embeddings, and leverages Conditional Random Fields to perform sequence labeling.","We evaluate our proposal on a large dataset of diverse co-speech gestures in task-oriented face-to-face dialogues.","The results consistently demonstrate that our method significantly outperforms strong baseline models in detecting gesture strokes.","Furthermore, applying Transformer encoders to learn contextual embeddings from movement sequences substantially improves gesture unit detection.","These results highlight our framework's capacity to capture the fine-grained dynamics of co-speech gesture phases, paving the way for more nuanced and accurate gesture detection and analysis."],"url":"http://arxiv.org/abs/2308.10680v1"}
{"created":"2023-08-21 12:24:20","title":"Visual Crowd Analysis: Open Research Problems","abstract":"Over the last decade, there has been a remarkable surge in interest in automated crowd monitoring within the computer vision community. Modern deep-learning approaches have made it possible to develop fully-automated vision-based crowd-monitoring applications. However, despite the magnitude of the issue at hand, the significant technological advancements, and the consistent interest of the research community, there are still numerous challenges that need to be overcome. In this article, we delve into six major areas of visual crowd analysis, emphasizing the key developments in each of these areas. We outline the crucial unresolved issues that must be tackled in future works, in order to ensure that the field of automated crowd monitoring continues to progress and thrive. Several surveys related to this topic have been conducted in the past. Nonetheless, this article thoroughly examines and presents a more intuitive categorization of works, while also depicting the latest breakthroughs within the field, incorporating more recent studies carried out within the last few years in a concise manner. By carefully choosing prominent works with significant contributions in terms of novelty or performance gains, this paper presents a more comprehensive exposition of advancements in the current state-of-the-art.","sentences":["Over the last decade, there has been a remarkable surge in interest in automated crowd monitoring within the computer vision community.","Modern deep-learning approaches have made it possible to develop fully-automated vision-based crowd-monitoring applications.","However, despite the magnitude of the issue at hand, the significant technological advancements, and the consistent interest of the research community, there are still numerous challenges that need to be overcome.","In this article, we delve into six major areas of visual crowd analysis, emphasizing the key developments in each of these areas.","We outline the crucial unresolved issues that must be tackled in future works, in order to ensure that the field of automated crowd monitoring continues to progress and thrive.","Several surveys related to this topic have been conducted in the past.","Nonetheless, this article thoroughly examines and presents a more intuitive categorization of works, while also depicting the latest breakthroughs within the field, incorporating more recent studies carried out within the last few years in a concise manner.","By carefully choosing prominent works with significant contributions in terms of novelty or performance gains, this paper presents a more comprehensive exposition of advancements in the current state-of-the-art."],"url":"http://arxiv.org/abs/2308.10677v1"}
