{"created":"2023-06-06","title":"Can large language models democratize access to dual-use biotechnology?","abstract":"Large language models (LLMs) such as those embedded in 'chatbots' are accelerating and democratizing research by providing comprehensible information and expertise from many different fields. However, these models may also confer easy access to dual-use technologies capable of inflicting great harm. To evaluate this risk, the 'Safeguarding the Future' course at MIT tasked non-scientist students with investigating whether LLM chatbots could be prompted to assist non-experts in causing a pandemic. In one hour, the chatbots suggested four potential pandemic pathogens, explained how they can be generated from synthetic DNA using reverse genetics, supplied the names of DNA synthesis companies unlikely to screen orders, identified detailed protocols and how to troubleshoot them, and recommended that anyone lacking the skills to perform reverse genetics engage a core facility or contract research organization. Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training. Promising nonproliferation measures include pre-release evaluations of LLMs by third parties, curating training datasets to remove harmful concepts, and verifiably screening all DNA generated by synthesis providers or used by contract research organizations and robotic cloud laboratories to engineer organisms or viruses.","sentences":["Large language models (LLMs) such as those embedded in 'chatbots' are accelerating and democratizing research by providing comprehensible information and expertise from many different fields.","However, these models may also confer easy access to dual-use technologies capable of inflicting great harm.","To evaluate this risk, the 'Safeguarding the Future' course at MIT tasked non-scientist students with investigating whether LLM chatbots could be prompted to assist non-experts in causing a pandemic.","In one hour, the chatbots suggested four potential pandemic pathogens, explained how they can be generated from synthetic DNA using reverse genetics, supplied the names of DNA synthesis companies unlikely to screen orders, identified detailed protocols and how to troubleshoot them, and recommended that anyone lacking the skills to perform reverse genetics engage a core facility or contract research organization.","Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training.","Promising nonproliferation measures include pre-release evaluations of LLMs by third parties, curating training datasets to remove harmful concepts, and verifiably screening all DNA generated by synthesis providers or used by contract research organizations and robotic cloud laboratories to engineer organisms or viruses."],"url":"http://arxiv.org/abs/2306.03809v1"}
{"created":"2023-06-06","title":"Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models","abstract":"Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid theoretical foundation for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt \"Let's think step by step\", Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and fundamental theoretical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs.","sentences":["Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions.","It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis.","Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning.","However, an unresolved problem arises from the fact that current approaches lack a solid theoretical foundation for determining optimal prompts.","To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space.","Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts.","Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks.","Notably, without the help of the CoT method and the prompt \"Let's think step by step\", Prompt Space shows superior performance over the few-shot method.","Overall, our approach provides a robust and fundamental theoretical framework for selecting simple and effective prompts.","This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs."],"url":"http://arxiv.org/abs/2306.03799v1"}
{"created":"2023-06-06","title":"An Approach to Solving the Abstraction and Reasoning Corpus (ARC) Challenge","abstract":"We utilise the power of Large Language Models (LLMs), in particular GPT4, to be prompt engineered into performing an arbitrary task. Here, we give the model some human priors via text, along with some typical procedures for solving the ARC tasks, and ask it to generate the i) broad description of the input-output relation, ii) detailed steps of the input-output mapping, iii) use the detailed steps to perform manipulation on the test input and derive the test output. The current GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (those with small grids of 8x8 and below). With tweaks to the prompt to make it more specific for the use case, it can solve more. We posit that when scaled to a multi-agent system with usage of past memory and equipped with an image interpretation tool via Visual Question Answering, we may actually be able to solve the majority of the ARC challenge","sentences":["We utilise the power of Large Language Models (LLMs), in particular GPT4, to be prompt engineered into performing an arbitrary task.","Here, we give the model some human priors via text, along with some typical procedures for solving the ARC tasks, and ask it to generate the i) broad description of the input-output relation, ii) detailed steps of the input-output mapping, iii) use the detailed steps to perform manipulation on the test input and derive the test output.","The current GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (those with small grids of 8x8 and below).","With tweaks to the prompt to make it more specific for the use case, it can solve more.","We posit that when scaled to a multi-agent system with usage of past memory and equipped with an image interpretation tool via Visual Question Answering, we may actually be able to solve the majority of the ARC challenge"],"url":"http://arxiv.org/abs/2306.03553v1"}
{"created":"2023-06-06","title":"Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models","abstract":"This paper explores how AI-owners can develop safeguards for AI-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. It delves into the current state of ethical awareness on Large Language Models (LLMs). By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. A comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation. Drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling LLM-generated content. Finally, potential conflicts of interest between dataset curation at upstream and ethical benchmarking downstream are highlighted to underscore the need for a broader evaluation beyond mere output. This study prompts a nuanced conversation around ethical implications in this rapidly evolving field of content generation.","sentences":["This paper explores how AI-owners can develop safeguards for AI-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries.","It delves into the current state of ethical awareness on Large Language Models (LLMs).","By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified.","A comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices.","The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation.","Drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling LLM-generated content.","Finally, potential conflicts of interest between dataset curation at upstream and ethical benchmarking downstream are highlighted to underscore the need for a broader evaluation beyond mere output.","This study prompts a nuanced conversation around ethical implications in this rapidly evolving field of content generation."],"url":"http://arxiv.org/abs/2306.03503v1"}
{"created":"2023-06-06","title":"I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models","abstract":"Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention. The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack. We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal. Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. The small manually-labeled dataset is used to train a refusal classifier, which achieves an accuracy of 92%. Second, we use this refusal classifier to bootstrap a larger (n=10,000) dataset adapted from the Quora Insincere Questions dataset. With this machine-labeled data, we train a prompt classifier to predict whether ChatGPT will refuse a given question, without seeing ChatGPT's response. This prompt classifier achieves 76% accuracy on a test set of manually labeled questions (n=1,009). We examine our classifiers and the prompt n-grams that are most predictive of either compliance or refusal. Datasets and code are available at https://github.com/maxwellreuter/chatgpt-refusals.","sentences":["Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention.","The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias.","Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content.","Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse.","In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack.","We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal.","Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal.","The small manually-labeled dataset is used to train a refusal classifier, which achieves an accuracy of 92%.","Second, we use this refusal classifier to bootstrap a larger (n=10,000) dataset adapted from the Quora Insincere Questions dataset.","With this machine-labeled data, we train a prompt classifier to predict whether ChatGPT will refuse a given question, without seeing ChatGPT's response.","This prompt classifier achieves 76% accuracy on a test set of manually labeled questions (n=1,009).","We examine our classifiers and the prompt n-grams that are most predictive of either compliance or refusal.","Datasets and code are available at https://github.com/maxwellreuter/chatgpt-refusals."],"url":"http://arxiv.org/abs/2306.03423v1"}
{"created":"2023-06-06","title":"Properties of Gamma-Ray Bursts Associated with Supernovae and Kilonovae","abstract":"We systematically compare the temporal and spectral properties of 53 Supernova (SN)-associated and 15 Kilonova (KN)-associated Gamma-Ray Bursts (GRBs). We find that the spectral parameters of both types GRBs are identically and lognormally distributed, consistent with those normal GRBs. The bolometric luminosities of SN/GRBs and KN/GRBs have a triple form with the corresponding break luminosities of SN/GRBs are roughly two orders of magnitude larger than those of KN/GRBs. We build the power-law relations between the spectral lag and the luminosity of prompt $\\gamma$-rays with indices of $-1.43\\pm0.33$ for SN/GRBs and $-2.17\\pm0.57$ for KN/GRBs in the laboratory frame, which are respectively coincident with the rest-frame values. We verify that both SN/GRBs and KN/GRBs comply with their own Amati relations that match those of long and short GRBs, respectively. Analyzing X-ray afterglows with good plateau segments, we build the power-law relations between the X-ray luminosity and the plateau time with an index of $-1.12\\pm0.17$ for KN/GRBs and $-1.08\\pm0.22$ for SN/GRBs, which can be well explained by the relativistic shock driven by an energy injection. The plots of luminosity-lag, Amati relation and luminosity-time show heavy overlap between the two types of GRBs, implying that they might share the same radiation mechanism despite originating from different progenitors or central engines.","sentences":["We systematically compare the temporal and spectral properties of 53 Supernova (SN)-associated and 15 Kilonova (KN)-associated Gamma-Ray Bursts (GRBs).","We find that the spectral parameters of both types GRBs are identically and lognormally distributed, consistent with those normal GRBs.","The bolometric luminosities of SN/GRBs and KN/GRBs have a triple form with the corresponding break luminosities of SN/GRBs are roughly two orders of magnitude larger than those of KN/GRBs.","We build the power-law relations between the spectral lag and the luminosity of prompt $\\gamma$-rays with indices of $-1.43\\pm0.33$ for SN/GRBs and $-2.17\\pm0.57$ for KN/GRBs in the laboratory frame, which are respectively coincident with the rest-frame values.","We verify that both SN/GRBs and KN/GRBs comply with their own Amati relations that match those of long and short GRBs, respectively.","Analyzing X-ray afterglows with good plateau segments, we build the power-law relations between the X-ray luminosity and the plateau time with an index of $-1.12\\pm0.17$ for KN/GRBs and $-1.08\\pm0.22$ for SN/GRBs, which can be well explained by the relativistic shock driven by an energy injection.","The plots of luminosity-lag, Amati relation and luminosity-time show heavy overlap between the two types of GRBs, implying that they might share the same radiation mechanism despite originating from different progenitors or central engines."],"url":"http://arxiv.org/abs/2306.03365v1"}
{"created":"2023-06-06","title":"Impact of Large Language Models on Generating Software Specifications","abstract":"Software specifications are essential for ensuring the reliability of software systems. Existing specification extraction approaches, however, suffer from limited generalizability and require manual efforts. We study the effectiveness of Large Language Models (LLMs) in generating software specifications from software documentation, utilizing Few-Shot Learning (FSL) to enable LLMs to generalize from a small number of examples. We compare the performance of LLMs with FSL to that of state-of-the-art specification extraction techniques and study the impact of prompt construction strategies on LLM performance. In addition, we conduct a comprehensive analysis of their symptoms and root causes of the failures to understand the pros and cons of LLMs and existing approaches. We also compare 11 LLMs' performance, cost, and response time for generating software specifications. Our findings include that (1) the best performing LLM outperforms existing approaches by 9.1--13.7% with a few similar examples, (2) the two dominant root causes combined (ineffective prompts and missing domain knowledge) result in 57--60% of LLM failures, and (3) most of the 11 LLMs achieve better or comparable performance compared to traditional techniques. Our study offers valuable insights for future research to improve specification generation.","sentences":["Software specifications are essential for ensuring the reliability of software systems.","Existing specification extraction approaches, however, suffer from limited generalizability and require manual efforts.","We study the effectiveness of Large Language Models (LLMs) in generating software specifications from software documentation, utilizing Few-Shot Learning (FSL) to enable LLMs to generalize from a small number of examples.","We compare the performance of LLMs with FSL to that of state-of-the-art specification extraction techniques and study the impact of prompt construction strategies on LLM performance.","In addition, we conduct a comprehensive analysis of their symptoms and root causes of the failures to understand the pros and cons of LLMs and existing approaches.","We also compare 11 LLMs' performance, cost, and response time for generating software specifications.","Our findings include that (1) the best performing LLM outperforms existing approaches by 9.1--13.7% with a few similar examples, (2) the two dominant root causes combined (ineffective prompts and missing domain knowledge) result in 57--60% of LLM failures, and (3) most of the 11 LLMs achieve better or comparable performance compared to traditional techniques.","Our study offers valuable insights for future research to improve specification generation."],"url":"http://arxiv.org/abs/2306.03324v1"}
{"created":"2023-06-06","title":"Deductive Verification of Chain-of-Thought Reasoning","abstract":"Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.","sentences":["Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks.","While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks.","Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification.","However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT.","In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises.","To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format.","Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps.","It also empowers language models to carry out reasoning self-verification in a step-by-step manner.","By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps.","Along this process, we also improve the answer correctness on complex reasoning tasks.","Code will be released at https://github.com/lz1oceani/verify_cot."],"url":"http://arxiv.org/abs/2306.03872v1"}
{"created":"2023-06-06","title":"I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models","abstract":"Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention. The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack. We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal. Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. The small manually-labeled dataset is used to train a refusal classifier, which achieves an accuracy of 92%. Second, we use this refusal classifier to bootstrap a larger (n=10,000) dataset adapted from the Quora Insincere Questions dataset. With this machine-labeled data, we train a prompt classifier to predict whether ChatGPT will refuse a given question, without seeing ChatGPT's response. This prompt classifier achieves 76% accuracy on a test set of manually labeled questions (n=1,009). We examine our classifiers and the prompt n-grams that are most predictive of either compliance or refusal. Datasets and code are available at https://github.com/maxwellreuter/chatgpt-refusals.","sentences":["Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention.","The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias.","Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content.","Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse.","In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack.","We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal.","Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal.","The small manually-labeled dataset is used to train a refusal classifier, which achieves an accuracy of 92%.","Second, we use this refusal classifier to bootstrap a larger (n=10,000) dataset adapted from the Quora Insincere Questions dataset.","With this machine-labeled data, we train a prompt classifier to predict whether ChatGPT will refuse a given question, without seeing ChatGPT's response.","This prompt classifier achieves 76% accuracy on a test set of manually labeled questions (n=1,009).","We examine our classifiers and the prompt n-grams that are most predictive of either compliance or refusal.","Datasets and code are available at https://github.com/maxwellreuter/chatgpt-refusals."],"url":"http://arxiv.org/abs/2306.03423v1"}
{"created":"2023-06-06","title":"SAM3D: Segment Anything in 3D Scenes","abstract":"In this work, we propose SAM3D, a novel framework that is able to predict masks in 3D point clouds by leveraging the Segment-Anything Model (SAM) in RGB images without further training or finetuning. For a point cloud of a 3D scene with posed RGB images, we first predict segmentation masks of RGB images with SAM, and then project the 2D masks into the 3D points. Later, we merge the 3D masks iteratively with a bottom-up merging approach. At each step, we merge the point cloud masks of two adjacent frames with the bidirectional merging approach. In this way, the 3D masks predicted from different frames are gradually merged into the 3D masks of the whole 3D scene. Finally, we can optionally ensemble the result from our SAM3D with the over-segmentation results based on the geometric information of the 3D scenes. Our approach is experimented with ScanNet dataset and qualitative results demonstrate that our SAM3D achieves reasonable and fine-grained 3D segmentation results without any training or finetuning of SAM.","sentences":["In this work, we propose SAM3D, a novel framework that is able to predict masks in 3D point clouds by leveraging the Segment-Anything Model (SAM) in RGB images without further training or finetuning.","For a point cloud of a 3D scene with posed RGB images, we first predict segmentation masks of RGB images with SAM, and then project the 2D masks into the 3D points.","Later, we merge the 3D masks iteratively with a bottom-up merging approach.","At each step, we merge the point cloud masks of two adjacent frames with the bidirectional merging approach.","In this way, the 3D masks predicted from different frames are gradually merged into the 3D masks of the whole 3D scene.","Finally, we can optionally ensemble the result from our SAM3D with the over-segmentation results based on the geometric information of the 3D scenes.","Our approach is experimented with ScanNet dataset and qualitative results demonstrate that our SAM3D achieves reasonable and fine-grained 3D segmentation results without any training or finetuning of SAM."],"url":"http://arxiv.org/abs/2306.03908v1"}
{"created":"2023-06-06","title":"CL-UZH at SemEval-2023 Task 10: Sexism Detection through Incremental Fine-Tuning and Multi-Task Learning with Label Descriptions","abstract":"The widespread popularity of social media has led to an increase in hateful, abusive, and sexist language, motivating methods for the automatic detection of such phenomena. The goal of the SemEval shared task \\textit{Towards Explainable Detection of Online Sexism} (EDOS 2023) is to detect sexism in English social media posts (subtask A), and to categorize such posts into four coarse-grained sexism categories (subtask B), and eleven fine-grained subcategories (subtask C). In this paper, we present our submitted systems for all three subtasks, based on a multi-task model that has been fine-tuned on a range of related tasks and datasets before being fine-tuned on the specific EDOS subtasks. We implement multi-task learning by formulating each task as binary pairwise text classification, where the dataset and label descriptions are given along with the input text. The results show clear improvements over a fine-tuned DeBERTa-V3 serving as a baseline leading to $F_1$-scores of 85.9\\% in subtask A (rank 13/84), 64.8\\% in subtask B (rank 19/69), and 44.9\\% in subtask C (26/63).","sentences":["The widespread popularity of social media has led to an increase in hateful, abusive, and sexist language, motivating methods for the automatic detection of such phenomena.","The goal of the SemEval shared task \\textit{Towards","Explainable Detection of Online Sexism} (EDOS 2023) is to detect sexism in English social media posts (subtask A), and to categorize such posts into four coarse-grained sexism categories (subtask B), and eleven fine-grained subcategories (subtask C).","In this paper, we present our submitted systems for all three subtasks, based on a multi-task model that has been fine-tuned on a range of related tasks and datasets before being fine-tuned on the specific EDOS subtasks.","We implement multi-task learning by formulating each task as binary pairwise text classification, where the dataset and label descriptions are given along with the input text.","The results show clear improvements over a fine-tuned DeBERTa-V3 serving as a baseline leading to $F_1$-scores of 85.9\\% in subtask A (rank 13/84), 64.8\\% in subtask B (rank 19/69), and 44.9\\% in subtask C (26/63)."],"url":"http://arxiv.org/abs/2306.03907v1"}
{"created":"2023-06-06","title":"ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory","abstract":"Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .","sentences":["Large language models (LLMs) with memory are computationally universal.","However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains.","Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning.","In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning.","Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases.","We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning.","The project website is available at https://chatdatabase.github.io/ ."],"url":"http://arxiv.org/abs/2306.03901v1"}
{"created":"2023-06-06","title":"Towards Label-free Scene Understanding by Vision Foundation Models","abstract":"Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%, respectively. And for nuScenes dataset, our performance is 26.8% with an improvement of 6%. Code will be released (https://github.com/runnanchen/Label-Free-Scene-Understanding).","sentences":["Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks.","However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored.","In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data.","The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain.","To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously.","In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation.","Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments.","Our 2D and 3D network achieves label-free semantic segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%, respectively.","And for nuScenes dataset, our performance is 26.8% with an improvement of 6%.","Code will be released (https://github.com/runnanchen/Label-Free-Scene-Understanding)."],"url":"http://arxiv.org/abs/2306.03899v1"}
{"created":"2023-06-06","title":"Systematic performance of the ASKAP Fast Radio Burst search algorithm","abstract":"Detecting fast radio bursts (FRBs) requires software pipelines to search for dispersed single pulses of emission in radio telescope data. In order to enable an unbiased estimation of the underlying FRB population, it is important to understand the algorithm efficiency with respect to the search parameter space and thus the survey completeness. The Fast Real-time Engine for Dedispersing Amplitudes (FREDDA) search pipeline is a single pulse detection pipeline designed to identify radio pulses over a large range of dispersion measures (DM) with low latency. It is used on the Australian Square Kilometre Array Pathfinder (ASKAP) for the Commensal Real-time ASKAP Fast Transients (CRAFT) project . We utilise simulated single pulses in the low- and high-frequency observation bands of ASKAP to analyse the performance of the pipeline and infer the underlying FRB population. The simulation explores the Signal-to-Noise Ratio (S/N) recovery as a function of DM and the temporal duration of FRB pulses in comparison to injected values. The effects of intra-channel broadening caused by dispersion are also carefully studied in this work using control datasets. Our results show that for Gaussian-like single pulses, $> 85 \\%$ of the injected signal is recovered by pipelines such as FREDDA at DM < 3000 $\\mathrm{pc\\ cm^{-3}}$ using standard boxcar filters compared to an ideal incoherent dedispersion match filter. Further calculations with sensitivity implies at least $\\sim 10\\%$ of FRBs in a Euclidean universe at target sensitivity will be missed by FREDDA and HEIMDALL, another common pipeline, in ideal radio environments at 1.1 GHz.","sentences":["Detecting fast radio bursts (FRBs) requires software pipelines to search for dispersed single pulses of emission in radio telescope data.","In order to enable an unbiased estimation of the underlying FRB population, it is important to understand the algorithm efficiency with respect to the search parameter space and thus the survey completeness.","The Fast Real-time Engine for Dedispersing Amplitudes (FREDDA) search pipeline is a single pulse detection pipeline designed to identify radio pulses over a large range of dispersion measures (DM) with low latency.","It is used on the Australian Square Kilometre Array Pathfinder (ASKAP) for the Commensal Real-time ASKAP Fast Transients (CRAFT) project .","We utilise simulated single pulses in the low- and high-frequency observation bands of ASKAP to analyse the performance of the pipeline and infer the underlying FRB population.","The simulation explores the Signal-to-Noise Ratio (S/N) recovery as a function of DM and the temporal duration of FRB pulses in comparison to injected values.","The effects of intra-channel broadening caused by dispersion are also carefully studied in this work using control datasets.","Our results show that for Gaussian-like single pulses, $> 85 \\%$ of the injected signal is recovered by pipelines such as FREDDA at DM < 3000 $\\mathrm{pc\\ cm^{-3}}$ using standard boxcar filters compared to an ideal incoherent dedispersion match filter.","Further calculations with sensitivity implies at least $\\sim 10\\%$ of FRBs in a Euclidean universe at target sensitivity will be missed by FREDDA and HEIMDALL, another common pipeline, in ideal radio environments at 1.1 GHz."],"url":"http://arxiv.org/abs/2306.03886v1"}
{"created":"2023-06-06","title":"Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation","abstract":"Recent advances in denoising diffusion probabilistic models have shown great success in image synthesis tasks. While there are already works exploring the potential of this powerful tool in image semantic segmentation, its application in weakly supervised semantic segmentation (WSSS) remains relatively under-explored. Observing that conditional diffusion models (CDM) is capable of generating images subject to specific distributions, in this work, we utilize category-aware semantic information underlied in CDM to get the prediction mask of the target object with only image-level annotations. More specifically, we locate the desired class by approximating the derivative of the output of CDM w.r.t the input condition. Our method is different from previous diffusion model methods with guidance from an external classifier, which accumulates noises in the background during the reconstruction process. Our method outperforms state-of-the-art CAM and diffusion model methods on two public medical image segmentation datasets, which demonstrates that CDM is a promising tool in WSSS. Also, experiment shows our method is more time-efficient than existing diffusion model methods, making it practical for wider applications.","sentences":["Recent advances in denoising diffusion probabilistic models have shown great success in image synthesis tasks.","While there are already works exploring the potential of this powerful tool in image semantic segmentation, its application in weakly supervised semantic segmentation (WSSS) remains relatively under-explored.","Observing that conditional diffusion models (CDM) is capable of generating images subject to specific distributions, in this work, we utilize category-aware semantic information underlied in CDM to get the prediction mask of the target object with only image-level annotations.","More specifically, we locate the desired class by approximating the derivative of the output of CDM w.r.t","the input condition.","Our method is different from previous diffusion model methods with guidance from an external classifier, which accumulates noises in the background during the reconstruction process.","Our method outperforms state-of-the-art CAM and diffusion model methods on two public medical image segmentation datasets, which demonstrates that CDM is a promising tool in WSSS.","Also, experiment shows our method is more time-efficient than existing diffusion model methods, making it practical for wider applications."],"url":"http://arxiv.org/abs/2306.03878v1"}
{"created":"2023-06-06","title":"From Key Points to Key Point Hierarchy: Structured and Expressive Opinion Summarization","abstract":"Key Point Analysis (KPA) has been recently proposed for deriving fine-grained insights from collections of textual comments. KPA extracts the main points in the data as a list of concise sentences or phrases, termed key points, and quantifies their prevalence. While key points are more expressive than word clouds and key phrases, making sense of a long, flat list of key points, which often express related ideas in varying levels of granularity, may still be challenging. To address this limitation of KPA, we introduce the task of organizing a given set of key points into a hierarchy, according to their specificity. Such hierarchies may be viewed as a novel type of Textual Entailment Graph. We develop ThinkP, a high quality benchmark dataset of key point hierarchies for business and product reviews, obtained by consolidating multiple annotations. We compare different methods for predicting pairwise relations between key points, and for inferring a hierarchy from these pairwise predictions. In particular, for the task of computing pairwise key point relations, we achieve significant gains over existing strong baselines by applying directional distributional similarity methods to a novel distributional representation of key points, and further boost performance via weak supervision.","sentences":["Key Point Analysis (KPA) has been recently proposed for deriving fine-grained insights from collections of textual comments.","KPA extracts the main points in the data as a list of concise sentences or phrases, termed key points, and quantifies their prevalence.","While key points are more expressive than word clouds and key phrases, making sense of a long, flat list of key points, which often express related ideas in varying levels of granularity, may still be challenging.","To address this limitation of KPA, we introduce the task of organizing a given set of key points into a hierarchy, according to their specificity.","Such hierarchies may be viewed as a novel type of Textual Entailment Graph.","We develop ThinkP, a high quality benchmark dataset of key point hierarchies for business and product reviews, obtained by consolidating multiple annotations.","We compare different methods for predicting pairwise relations between key points, and for inferring a hierarchy from these pairwise predictions.","In particular, for the task of computing pairwise key point relations, we achieve significant gains over existing strong baselines by applying directional distributional similarity methods to a novel distributional representation of key points, and further boost performance via weak supervision."],"url":"http://arxiv.org/abs/2306.03853v1"}
{"created":"2023-06-06","title":"Multi-Label ECG Classification using Temporal Convolutional Neural Network","abstract":"Automated analysis of 12-lead electrocardiogram (ECG) plays a crucial role in the early screening and management of cardiovascular diseases (CVDs). In practice, it is common to see multiple co-occurring cardiac disorders, i.e., multi-label or multimorbidity in patients with CVDs, which increases the risk for mortality. Most current research focuses on the single-label ECG classification, i.e., each ECG record corresponds to one cardiac disorder, ignoring ECG records with multi-label phenomenon. In this paper, we propose an ensemble of attention-based temporal convolutional neural network (ATCNN) models for the multi-label classification of 12-lead ECG records. Specifically, a set of ATCNN-based single-lead binary classifiers are trained one for each cardiac disorder, and the predictions from these classifiers with simple thresholding generate the final multi-label decisions. The ATCNN contains a stack of TCNN layers followed by temporal and spatial attention layers. The TCNN layers operate at different dilation rates to represent the multi-scaled pathological ECG features dynamics, and attention layers emphasize/reduce the diagnostically relevant/redundant 12-lead ECG information. The proposed framework is evaluated on the PTBXL-2020 dataset and achieved an average F1-score of 76.51%. Moreover, the spatial and temporal attention weights visualization provides the optimal ECG-lead subset selection for each disease and model interpretability results, respectively. The improved performance and interpretability of the proposed approach demonstrate its ability to screen multimorbidity patients and help clinicians initiate timely treatment.","sentences":["Automated analysis of 12-lead electrocardiogram (ECG) plays a crucial role in the early screening and management of cardiovascular diseases (CVDs).","In practice, it is common to see multiple co-occurring cardiac disorders, i.e., multi-label or multimorbidity in patients with CVDs, which increases the risk for mortality.","Most current research focuses on the single-label ECG classification, i.e., each ECG record corresponds to one cardiac disorder, ignoring ECG records with multi-label phenomenon.","In this paper, we propose an ensemble of attention-based temporal convolutional neural network (ATCNN) models for the multi-label classification of 12-lead ECG records.","Specifically, a set of ATCNN-based single-lead binary classifiers are trained one for each cardiac disorder, and the predictions from these classifiers with simple thresholding generate the final multi-label decisions.","The ATCNN contains a stack of TCNN layers followed by temporal and spatial attention layers.","The TCNN layers operate at different dilation rates to represent the multi-scaled pathological ECG features dynamics, and attention layers emphasize/reduce the diagnostically relevant/redundant 12-lead ECG information.","The proposed framework is evaluated on the PTBXL-2020 dataset and achieved an average F1-score of 76.51%.","Moreover, the spatial and temporal attention weights visualization provides the optimal ECG-lead subset selection for each disease and model interpretability results, respectively.","The improved performance and interpretability of the proposed approach demonstrate its ability to screen multimorbidity patients and help clinicians initiate timely treatment."],"url":"http://arxiv.org/abs/2306.03844v1"}
{"created":"2023-06-06","title":"Atrial Septal Defect Detection in Children Based on Ultrasound Video Using Multiple Instances Learning","abstract":"Purpose: Congenital heart defect (CHD) is the most common birth defect. Thoracic echocardiography (TTE) can provide sufficient cardiac structure information, evaluate hemodynamics and cardiac function, and is an effective method for atrial septal defect (ASD) examination. This paper aims to study a deep learning method based on cardiac ultrasound video to assist in ASD diagnosis. Materials and methods: We select two standard views of the atrial septum (subAS) and low parasternal four-compartment view (LPS4C) as the two views to identify ASD. We enlist data from 300 children patients as part of a double-blind experiment for five-fold cross-validation to verify the performance of our model. In addition, data from 30 children patients (15 positives and 15 negatives) are collected for clinician testing and compared to our model test results (these 30 samples do not participate in model training). We propose an echocardiography video-based atrial septal defect diagnosis system. In our model, we present a block random selection, maximal agreement decision and frame sampling strategy for training and testing respectively, resNet18 and r3D networks are used to extract the frame features and aggregate them to build a rich video-level representation. Results: We validate our model using our private dataset by five-cross validation. For ASD detection, we achieve 89.33 AUC, 84.95 accuracy, 85.70 sensitivity, 81.51 specificity and 81.99 F1 score. Conclusion: The proposed model is multiple instances learning-based deep learning model for video atrial septal defect detection which effectively improves ASD detection accuracy when compared to the performances of previous networks and clinical doctors.","sentences":["Purpose:","Congenital heart defect (CHD) is the most common birth defect.","Thoracic echocardiography (TTE) can provide sufficient cardiac structure information, evaluate hemodynamics and cardiac function, and is an effective method for atrial septal defect (ASD) examination.","This paper aims to study a deep learning method based on cardiac ultrasound video to assist in ASD diagnosis.","Materials and methods: We select two standard views of the atrial septum (subAS) and low parasternal four-compartment view (LPS4C) as the two views to identify ASD.","We enlist data from 300 children patients as part of a double-blind experiment for five-fold cross-validation to verify the performance of our model.","In addition, data from 30 children patients (15 positives and 15 negatives) are collected for clinician testing and compared to our model test results (these 30 samples do not participate in model training).","We propose an echocardiography video-based atrial septal defect diagnosis system.","In our model, we present a block random selection, maximal agreement decision and frame sampling strategy for training and testing respectively, resNet18 and r3D networks are used to extract the frame features and aggregate them to build a rich video-level representation.","Results: We validate our model using our private dataset by five-cross validation.","For ASD detection, we achieve 89.33 AUC, 84.95 accuracy, 85.70 sensitivity, 81.51 specificity and 81.99 F1 score.","Conclusion: The proposed model is multiple instances learning-based deep learning model for video atrial septal defect detection which effectively improves ASD detection accuracy when compared to the performances of previous networks and clinical doctors."],"url":"http://arxiv.org/abs/2306.03835v1"}
{"created":"2023-06-06","title":"MTS2Graph: Interpretable Multivariate Time Series Classification with Temporal Evolving Graphs","abstract":"Conventional time series classification approaches based on bags of patterns or shapelets face significant challenges in dealing with a vast amount of feature candidates from high-dimensional multivariate data. In contrast, deep neural networks can learn low-dimensional features efficiently, and in particular, Convolutional Neural Networks (CNN) have shown promising results in classifying Multivariate Time Series (MTS) data. A key factor in the success of deep neural networks is this astonishing expressive power. However, this power comes at the cost of complex, black-boxed models, conflicting with the goals of building reliable and human-understandable models. An essential criterion in understanding such predictive deep models involves quantifying the contribution of time-varying input variables to the classification. Hence, in this work, we introduce a new framework for interpreting multivariate time series data by extracting and clustering the input representative patterns that highly activate CNN neurons. This way, we identify each signal's role and dependencies, considering all possible combinations of signals in the MTS input. Then, we construct a graph that captures the temporal relationship between the extracted patterns for each layer. An effective graph merging strategy finds the connection of each node to the previous layer's nodes. Finally, a graph embedding algorithm generates new representations of the created interpretable time-series features. To evaluate the performance of our proposed framework, we run extensive experiments on eight datasets of the UCR/UEA archive, along with HAR and PAM datasets. The experiments indicate the benefit of our time-aware graph-based representation in MTS classification while enriching them with more interpretability.","sentences":["Conventional time series classification approaches based on bags of patterns or shapelets face significant challenges in dealing with a vast amount of feature candidates from high-dimensional multivariate data.","In contrast, deep neural networks can learn low-dimensional features efficiently, and in particular, Convolutional Neural Networks (CNN) have shown promising results in classifying Multivariate Time Series (MTS) data.","A key factor in the success of deep neural networks is this astonishing expressive power.","However, this power comes at the cost of complex, black-boxed models, conflicting with the goals of building reliable and human-understandable models.","An essential criterion in understanding such predictive deep models involves quantifying the contribution of time-varying input variables to the classification.","Hence, in this work, we introduce a new framework for interpreting multivariate time series data by extracting and clustering the input representative patterns that highly activate CNN neurons.","This way, we identify each signal's role and dependencies, considering all possible combinations of signals in the MTS input.","Then, we construct a graph that captures the temporal relationship between the extracted patterns for each layer.","An effective graph merging strategy finds the connection of each node to the previous layer's nodes.","Finally, a graph embedding algorithm generates new representations of the created interpretable time-series features.","To evaluate the performance of our proposed framework, we run extensive experiments on eight datasets of the UCR/UEA archive, along with HAR and PAM datasets.","The experiments indicate the benefit of our time-aware graph-based representation in MTS classification while enriching them with more interpretability."],"url":"http://arxiv.org/abs/2306.03834v1"}
{"created":"2023-06-06","title":"Parameter Estimation in Electrical Distribution Systems with limited Measurements using Regression Methods","abstract":"This paper presents novel methods for parameter identification in electrical grids with small numbers of spatially distributed measuring devices, which is an issue for distribution system operators managing aged and not properly mapped underground Low Voltage (LV) grids, especially in Germany. For this purpose, the total impedance of individual branches of the overall system is estimated by measuring currents and voltages at a subset of all system nodes over time. It is shown that, under common assumptions for electrical distsribution systems, an estimate of the total impedance can be made using readily computable proxies. Different regression methods are then used and compared to estimate the total impedance of the respective branches, with varying weights of the input data. The results on realistic LV feeders with different branch lengths and number of unmeasured segments are discussed and multiple influencing factors are investigated through simulations. It is shown that estimates of the total impedances can be obtained with acceptable quality under realistic assumptions.","sentences":["This paper presents novel methods for parameter identification in electrical grids with small numbers of spatially distributed measuring devices, which is an issue for distribution system operators managing aged and not properly mapped underground Low Voltage (LV) grids, especially in Germany.","For this purpose, the total impedance of individual branches of the overall system is estimated by measuring currents and voltages at a subset of all system nodes over time.","It is shown that, under common assumptions for electrical distsribution systems, an estimate of the total impedance can be made using readily computable proxies.","Different regression methods are then used and compared to estimate the total impedance of the respective branches, with varying weights of the input data.","The results on realistic LV feeders with different branch lengths and number of unmeasured segments are discussed and multiple influencing factors are investigated through simulations.","It is shown that estimates of the total impedances can be obtained with acceptable quality under realistic assumptions."],"url":"http://arxiv.org/abs/2306.03859v1"}
{"created":"2023-06-06","title":"From Key Points to Key Point Hierarchy: Structured and Expressive Opinion Summarization","abstract":"Key Point Analysis (KPA) has been recently proposed for deriving fine-grained insights from collections of textual comments. KPA extracts the main points in the data as a list of concise sentences or phrases, termed key points, and quantifies their prevalence. While key points are more expressive than word clouds and key phrases, making sense of a long, flat list of key points, which often express related ideas in varying levels of granularity, may still be challenging. To address this limitation of KPA, we introduce the task of organizing a given set of key points into a hierarchy, according to their specificity. Such hierarchies may be viewed as a novel type of Textual Entailment Graph. We develop ThinkP, a high quality benchmark dataset of key point hierarchies for business and product reviews, obtained by consolidating multiple annotations. We compare different methods for predicting pairwise relations between key points, and for inferring a hierarchy from these pairwise predictions. In particular, for the task of computing pairwise key point relations, we achieve significant gains over existing strong baselines by applying directional distributional similarity methods to a novel distributional representation of key points, and further boost performance via weak supervision.","sentences":["Key Point Analysis (KPA) has been recently proposed for deriving fine-grained insights from collections of textual comments.","KPA extracts the main points in the data as a list of concise sentences or phrases, termed key points, and quantifies their prevalence.","While key points are more expressive than word clouds and key phrases, making sense of a long, flat list of key points, which often express related ideas in varying levels of granularity, may still be challenging.","To address this limitation of KPA, we introduce the task of organizing a given set of key points into a hierarchy, according to their specificity.","Such hierarchies may be viewed as a novel type of Textual Entailment Graph.","We develop ThinkP, a high quality benchmark dataset of key point hierarchies for business and product reviews, obtained by consolidating multiple annotations.","We compare different methods for predicting pairwise relations between key points, and for inferring a hierarchy from these pairwise predictions.","In particular, for the task of computing pairwise key point relations, we achieve significant gains over existing strong baselines by applying directional distributional similarity methods to a novel distributional representation of key points, and further boost performance via weak supervision."],"url":"http://arxiv.org/abs/2306.03853v1"}
{"created":"2023-06-06","title":"Apertif 1.4 GHz continuum observations of the Bo\u00f6tes field and their combined view with LOFAR","abstract":"We present a new image of a 26.5 square degree region in the Bo\\\"otes constellation obtained at 1.4 GHz using the Aperture Tile in Focus (Apertif) system on the Westerbork Synthesis Radio Telescope. We use a newly developed processing pipeline which includes direction-dependent self-calibration which provides a significant improvement of the quality of the images compared to those released as part of the Apertif first data release. For the Bo\\\"otes region, we mosaic 187 Apertif images and extract a source catalog. The mosaic image has an angular resolution of 27${\\times}$11.5 arcseconds and a median background noise of 40 ${\\mu}$Jy/beam. The catalog has 8994 sources and is complete down to the 0.3 mJy level. We combine the Apertif image with LOFAR images of the Bo\\\"otes field at 54 and 150 MHz to study spectral properties of the sources. We find a spectral flattening towards low flux density sources. Using the spectral index limits from Apertif non-detections we derive that up to 9 percent of the sources have ultra-steep spectra with a slope steeper than -1.2. Steepening of the spectral index with increasing redshift is also seen in the data showing a different dependency for the low-frequency spectral index and the high frequency one. This can be explained by a population of sources having concave radio spectra with a turnover frequency around the LOFAR band. Additionally, we discuss cases of individual extended sources with an interesting resolved spectral structure. With the improved pipeline, we aim to continue processing data from the Apertif wide-area surveys and release the improved 1.4 GHz images of several famous fields.","sentences":["We present a new image of a 26.5 square degree region in the Bo\\\"otes constellation obtained at 1.4 GHz using the Aperture Tile in Focus (Apertif) system on the Westerbork Synthesis Radio Telescope.","We use a newly developed processing pipeline which includes direction-dependent self-calibration which provides a significant improvement of the quality of the images compared to those released as part of the Apertif first data release.","For the Bo\\\"otes region, we mosaic 187 Apertif images and extract a source catalog.","The mosaic image has an angular resolution of 27${\\times}$11.5 arcseconds and a median background noise of 40 ${\\mu}$Jy/beam.","The catalog has 8994 sources and is complete down to the 0.3 mJy level.","We combine the Apertif image with LOFAR images of the Bo\\\"otes field at 54 and 150 MHz to study spectral properties of the sources.","We find a spectral flattening towards low flux density sources.","Using the spectral index limits from Apertif non-detections we derive that up to 9 percent of the sources have ultra-steep spectra with a slope steeper than -1.2.","Steepening of the spectral index with increasing redshift is also seen in the data showing a different dependency for the low-frequency spectral index and the high frequency one.","This can be explained by a population of sources having concave radio spectra with a turnover frequency around the LOFAR band.","Additionally, we discuss cases of individual extended sources with an interesting resolved spectral structure.","With the improved pipeline, we aim to continue processing data from the Apertif wide-area surveys and release the improved 1.4 GHz images of several famous fields."],"url":"http://arxiv.org/abs/2306.03710v1"}
{"created":"2023-06-06","title":"PQM: A Point Quality Evaluation Metric for Dense Maps","abstract":"LiDAR-based mapping/reconstruction are important for various applications, but evaluating the quality of the dense maps they produce is challenging. The current methods have limitations, including the inability to capture completeness, structural information, and local variations in error. In this paper, we propose a novel point quality evaluation metric (PQM) that consists of four sub-metrics to provide a more comprehensive evaluation of point cloud quality. The completeness sub-metric evaluates the proportion of missing data, the artifact score sub-metric recognizes and characterizes artifacts, the accuracy sub-metric measures registration accuracy, and the resolution sub-metric quantifies point cloud density. Through an ablation study using a prototype dataset, we demonstrate the effectiveness of each of the sub-metrics and compare them to popular point cloud distance measures. Using three LiDAR SLAM systems to generate maps, we evaluate their output map quality and demonstrate the metrics robustness to noise and artifacts. Our implementation of PQM, datasets and detailed documentation on how to integrate with your custom dense mapping pipeline can be found at github.com/droneslab/pqm","sentences":["LiDAR-based mapping/reconstruction are important for various applications, but evaluating the quality of the dense maps they produce is challenging.","The current methods have limitations, including the inability to capture completeness, structural information, and local variations in error.","In this paper, we propose a novel point quality evaluation metric (PQM) that consists of four sub-metrics to provide a more comprehensive evaluation of point cloud quality.","The completeness sub-metric evaluates the proportion of missing data, the artifact score sub-metric recognizes and characterizes artifacts, the accuracy sub-metric measures registration accuracy, and the resolution sub-metric quantifies point cloud density.","Through an ablation study using a prototype dataset, we demonstrate the effectiveness of each of the sub-metrics and compare them to popular point cloud distance measures.","Using three LiDAR SLAM systems to generate maps, we evaluate their output map quality and demonstrate the metrics robustness to noise and artifacts.","Our implementation of PQM, datasets and detailed documentation on how to integrate with your custom dense mapping pipeline can be found at github.com/droneslab/pqm"],"url":"http://arxiv.org/abs/2306.03660v1"}
{"created":"2023-06-06","title":"Efficient Centrality Maximization with Rademacher Averages","abstract":"The identification of the set of k most central nodes of a graph, or centrality maximization, is a key task in network analysis, with various applications ranging from finding communities in social and biological networks to understanding which seed nodes are important to diffuse information in a graph. As the exact computation of centrality measures does not scale to modern-sized networks, the most practical solution is to resort to rigorous, but efficiently computable, randomized approximations. In this work we present CentRA, the first algorithm based on progressive sampling to compute high-quality approximations of the set of k most central nodes. CentRA is based on a novel approach to efficiently estimate Monte Carlo Rademacher Averages, a powerful tool from statistical learning theory to compute sharp data-dependent approximation bounds. Then, we study the sample complexity of centrality maximization using the VC-dimension, a key concept from statistical learning theory. We show that the number of random samples required to compute high-quality approximations scales with finer characteristics of the graph, such as its vertex diameter, or of the centrality of interest, significantly improving looser bounds derived from standard techniques. We apply CentRA to analyze large real-world networks, showing that it significantly outperforms the state-of-the-art approximation algorithm in terms of number of samples, running times, and accuracy.","sentences":["The identification of the set of k most central nodes of a graph, or centrality maximization, is a key task in network analysis, with various applications ranging from finding communities in social and biological networks to understanding which seed nodes are important to diffuse information in a graph.","As the exact computation of centrality measures does not scale to modern-sized networks, the most practical solution is to resort to rigorous, but efficiently computable, randomized approximations.","In this work we present CentRA, the first algorithm based on progressive sampling to compute high-quality approximations of the set of k most central nodes.","CentRA is based on a novel approach to efficiently estimate Monte Carlo Rademacher Averages, a powerful tool from statistical learning theory to compute sharp data-dependent approximation bounds.","Then, we study the sample complexity of centrality maximization using the VC-dimension, a key concept from statistical learning theory.","We show that the number of random samples required to compute high-quality approximations scales with finer characteristics of the graph, such as its vertex diameter, or of the centrality of interest, significantly improving looser bounds derived from standard techniques.","We apply CentRA to analyze large real-world networks, showing that it significantly outperforms the state-of-the-art approximation algorithm in terms of number of samples, running times, and accuracy."],"url":"http://arxiv.org/abs/2306.03651v1"}
{"created":"2023-06-06","title":"Scalable Concept Extraction in Industry 4.0","abstract":"The industry 4.0 is leveraging digital technologies and machine learning techniques to connect and optimize manufacturing processes. Central to this idea is the ability to transform raw data into human understandable knowledge for reliable data-driven decision-making. Convolutional Neural Networks (CNNs) have been instrumental in processing image data, yet, their ``black box'' nature complicates the understanding of their prediction process. In this context, recent advances in the field of eXplainable Artificial Intelligence (XAI) have proposed the extraction and localization of concepts, or which visual cues intervene on the prediction process of CNNs. This paper tackles the application of concept extraction (CE) methods to industry 4.0 scenarios. To this end, we modify a recently developed technique, ``Extracting Concepts with Local Aggregated Descriptors'' (ECLAD), improving its scalability. Specifically, we propose a novel procedure for calculating concept importance, utilizing a wrapper function designed for CNNs. This process is aimed at decreasing the number of times each image needs to be evaluated. Subsequently, we demonstrate the potential of CE methods, by applying them in three industrial use cases. We selected three representative use cases in the context of quality control for material design (tailored textiles), manufacturing (carbon fiber reinforcement), and maintenance (photovoltaic module inspection). In these examples, CE was able to successfully extract and locate concepts directly related to each task. This is, the visual cues related to each concept, coincided with what human experts would use to perform the task themselves, even when the visual cues were entangled between multiple classes. Through empirical results, we show that CE can be applied for understanding CNNs in an industrial context, giving useful insights that can relate to domain knowledge.","sentences":["The industry 4.0 is leveraging digital technologies and machine learning techniques to connect and optimize manufacturing processes.","Central to this idea is the ability to transform raw data into human understandable knowledge for reliable data-driven decision-making.","Convolutional Neural Networks (CNNs) have been instrumental in processing image data, yet, their ``black box'' nature complicates the understanding of their prediction process.","In this context, recent advances in the field of eXplainable Artificial Intelligence (XAI) have proposed the extraction and localization of concepts, or which visual cues intervene on the prediction process of CNNs.","This paper tackles the application of concept extraction (CE) methods to industry 4.0 scenarios.","To this end, we modify a recently developed technique, ``Extracting Concepts with Local Aggregated Descriptors'' (ECLAD), improving its scalability.","Specifically, we propose a novel procedure for calculating concept importance, utilizing a wrapper function designed for CNNs.","This process is aimed at decreasing the number of times each image needs to be evaluated.","Subsequently, we demonstrate the potential of CE methods, by applying them in three industrial use cases.","We selected three representative use cases in the context of quality control for material design (tailored textiles), manufacturing (carbon fiber reinforcement), and maintenance (photovoltaic module inspection).","In these examples, CE was able to successfully extract and locate concepts directly related to each task.","This is, the visual cues related to each concept, coincided with what human experts would use to perform the task themselves, even when the visual cues were entangled between multiple classes.","Through empirical results, we show that CE can be applied for understanding CNNs in an industrial context, giving useful insights that can relate to domain knowledge."],"url":"http://arxiv.org/abs/2306.03551v1"}
{"created":"2023-06-06","title":"Adversarial Attacks and Defenses for Semantic Communication in Vehicular Metaverses","abstract":"For vehicular metaverses, one of the ultimate user-centric goals is to optimize the immersive experience and Quality of Service (QoS) for users on board. Semantic Communication (SemCom) has been introduced as a revolutionary paradigm that significantly eases communication resource pressure for vehicular metaverse applications to achieve this goal. SemCom enables high-quality and ultra-efficient vehicular communication, even with explosively increasing data traffic among vehicles. In this article, we propose a hierarchical SemCom-enabled vehicular metaverses framework consisting of the global metaverse, local metaverses, SemCom module, and resource pool. The global and local metaverses are brand-new concepts from the metaverse's distribution standpoint. Considering the QoS of users, this article explores the potential security vulnerabilities of the proposed framework. To that purpose, this study highlights a specific security risk to the framework's SemCom module and offers a viable defense solution, so encouraging community researchers to focus more on vehicular metaverse security. Finally, we provide an overview of the open issues of secure SemCom in the vehicular metaverses, notably pointing out potential future research directions.","sentences":["For vehicular metaverses, one of the ultimate user-centric goals is to optimize the immersive experience and Quality of Service (QoS) for users on board.","Semantic Communication (SemCom) has been introduced as a revolutionary paradigm that significantly eases communication resource pressure for vehicular metaverse applications to achieve this goal.","SemCom enables high-quality and ultra-efficient vehicular communication, even with explosively increasing data traffic among vehicles.","In this article, we propose a hierarchical SemCom-enabled vehicular metaverses framework consisting of the global metaverse, local metaverses, SemCom module, and resource pool.","The global and local metaverses are brand-new concepts from the metaverse's distribution standpoint.","Considering the QoS of users, this article explores the potential security vulnerabilities of the proposed framework.","To that purpose, this study highlights a specific security risk to the framework's SemCom module and offers a viable defense solution, so encouraging community researchers to focus more on vehicular metaverse security.","Finally, we provide an overview of the open issues of secure SemCom in the vehicular metaverses, notably pointing out potential future research directions."],"url":"http://arxiv.org/abs/2306.03528v1"}
{"created":"2023-06-06","title":"Recognize Anything: A Strong Image Tagging Model","abstract":"We present the Recognize Anything Model (RAM): a strong foundation model for image tagging. RAM can recognize any common category with high accuracy. RAM introduces a new paradigm for image tagging, leveraging large-scale image-text pairs for training instead of manual annotations. The development of RAM comprises four key steps. Firstly, annotation-free image tags are obtained at scale through automatic text semantic parsing. Subsequently, a preliminary model is trained for automatic annotation by unifying the caption and tagging tasks, supervised by the original texts and parsed tags, respectively. Thirdly, a data engine is employed to generate additional annotations and clean incorrect ones. Lastly, the model is retrained with the processed data and fine-tuned using a smaller but higher-quality dataset. We evaluate the tagging capabilities of RAM on numerous benchmarks and observe impressive zero-shot performance, significantly outperforming CLIP and BLIP. Remarkably, RAM even surpasses the fully supervised manners and exhibits competitive performance with the Google API. We are releasing the RAM at \\url{https://recognize-anything.github.io/} to foster the advancements of large models in computer vision.","sentences":["We present the Recognize Anything Model (RAM): a strong foundation model for image tagging.","RAM can recognize any common category with high accuracy.","RAM introduces a new paradigm for image tagging, leveraging large-scale image-text pairs for training instead of manual annotations.","The development of RAM comprises four key steps.","Firstly, annotation-free image tags are obtained at scale through automatic text semantic parsing.","Subsequently, a preliminary model is trained for automatic annotation by unifying the caption and tagging tasks, supervised by the original texts and parsed tags, respectively.","Thirdly, a data engine is employed to generate additional annotations and clean incorrect ones.","Lastly, the model is retrained with the processed data and fine-tuned using a smaller but higher-quality dataset.","We evaluate the tagging capabilities of RAM on numerous benchmarks and observe impressive zero-shot performance, significantly outperforming CLIP and BLIP.","Remarkably, RAM even surpasses the fully supervised manners and exhibits competitive performance with the Google API.","We are releasing the RAM at \\url{https://recognize-anything.github.io/} to foster the advancements of large models in computer vision."],"url":"http://arxiv.org/abs/2306.03514v1"}
{"created":"2023-06-06","title":"\"A Little is Enough\": Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation","abstract":"Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality Estimation based Filtering approach to extract high-quality parallel data from the pseudo-parallel corpus. To the best of our knowledge, this is a novel adaptation of the QE framework to extract quality parallel corpus from the pseudo-parallel corpus. By training with this filtered corpus, we observe an improvement in the Machine Translation (MT) system's performance by up to 1.8 BLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language pairs, over the baseline model. The baseline model is the one that is trained on the whole pseudo-parallel corpus. Our Few-shot QE model transfer learned from the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali training instances, shows an improvement of up to 0.6 BLEU points for Hindi-Bengali language pair, compared to the baseline model. This demonstrates the promise of transfer learning in the setting under discussion. QE systems typically require in the order of (7K-25K) of training data. Our Hindi-Bengali QE is trained on only 500 instances of training that is 1/40th of the normal requirement and achieves comparable performance. All the scripts and datasets utilized in this study will be publicly available.","sentences":["Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available.","The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus.","We propose a Quality Estimation based Filtering approach to extract high-quality parallel data from the pseudo-parallel corpus.","To the best of our knowledge, this is a novel adaptation of the QE framework to extract quality parallel corpus from the pseudo-parallel corpus.","By training with this filtered corpus, we observe an improvement in the Machine Translation (MT) system's performance by up to 1.8 BLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language pairs, over the baseline model.","The baseline model is the one that is trained on the whole pseudo-parallel corpus.","Our Few-shot QE model transfer learned from the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali training instances, shows an improvement of up to 0.6 BLEU points for Hindi-Bengali language pair, compared to the baseline model.","This demonstrates the promise of transfer learning in the setting under discussion.","QE systems typically require in the order of (7K-25K) of training data.","Our Hindi-Bengali QE is trained on only 500 instances of training that is 1/40th of the normal requirement and achieves comparable performance.","All the scripts and datasets utilized in this study will be publicly available."],"url":"http://arxiv.org/abs/2306.03507v1"}
{"created":"2023-06-06","title":"Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis","abstract":"We are interested in a novel task, namely low-resource text-to-talking avatar. Given only a few-minute-long talking person video with the audio track as the training data and arbitrary texts as the driving input, we aim to synthesize high-quality talking portrait videos corresponding to the input text. This task has broad application prospects in the digital human industry but has not been technically achieved yet due to two challenges: (1) It is challenging to mimic the timbre from out-of-domain audio for a traditional multi-speaker Text-to-Speech system. (2) It is hard to render high-fidelity and lip-synchronized talking avatars with limited training data. In this paper, we introduce Adaptive Text-to-Talking Avatar (Ada-TTA), which (1) designs a generic zero-shot multi-speaker TTS model that well disentangles the text content, timbre, and prosody; and (2) embraces recent advances in neural rendering to achieve realistic audio-driven talking face video generation. With these designs, our method overcomes the aforementioned two challenges and achieves to generate identity-preserving speech and realistic talking person video. Experiments demonstrate that our method could synthesize realistic, identity-preserving, and audio-visual synchronized talking avatar videos.","sentences":["We are interested in a novel task, namely low-resource text-to-talking avatar.","Given only a few-minute-long talking person video with the audio track as the training data and arbitrary texts as the driving input, we aim to synthesize high-quality talking portrait videos corresponding to the input text.","This task has broad application prospects in the digital human industry but has not been technically achieved yet due to two challenges: (1) It is challenging to mimic the timbre from out-of-domain audio for a traditional multi-speaker Text-to-Speech system.","(2) It is hard to render high-fidelity and lip-synchronized talking avatars with limited training data.","In this paper, we introduce Adaptive Text-to-Talking Avatar (Ada-TTA), which (1) designs a generic zero-shot multi-speaker TTS model that well disentangles the text content, timbre, and prosody; and (2) embraces recent advances in neural rendering to achieve realistic audio-driven talking face video generation.","With these designs, our method overcomes the aforementioned two challenges and achieves to generate identity-preserving speech and realistic talking person video.","Experiments demonstrate that our method could synthesize realistic, identity-preserving, and audio-visual synchronized talking avatar videos."],"url":"http://arxiv.org/abs/2306.03504v1"}
{"created":"2023-06-06","title":"Characterization of stellar companion from high-contrast long-slit spectroscopy data: The EXtraction Of SPEctrum of COmpanion (EXOSPECO) algorithm","abstract":"High-contrast long-slit spectrographs can be used to characterize exoplanets. High-contrast long-slit spectroscopic data are however corrupted by stellar leakages which largely dominate other signals and make the process of extracting the companion spectrum very challenging. This paper presents a complete method to calibrate the spectrograph and extract the signal of interest.   The proposed method is based on a flexible direct model of the high-contrast long-slit spectroscopic data. This model explicitly accounts for the instrumental response and for the contributions of both the star and the companion. The contributions of these two components and the calibration parameters are jointly estimated by solving a regularized inverse problem. This problem having no closed-form solution, we propose an alternating minimization strategy to effectively find the solution.   We have tested our method on empirical long-slit spectroscopic data and by injecting synthetic companion signals in these data. The proposed initialization and the alternating strategy effectively avoid the self-subtraction bias, even for companions observed very close to the coronagraphic mask. Careful modeling and calibration of the angular and spectral dispersion laws of the instrument clearly reduce the contamination by the stellar leakages. In practice, the outputs of the method are mostly driven by a single hyper-parameter which tunes the level of regularization of the companion SED.","sentences":["High-contrast long-slit spectrographs can be used to characterize exoplanets.","High-contrast long-slit spectroscopic data are however corrupted by stellar leakages which largely dominate other signals and make the process of extracting the companion spectrum very challenging.","This paper presents a complete method to calibrate the spectrograph and extract the signal of interest.   ","The proposed method is based on a flexible direct model of the high-contrast long-slit spectroscopic data.","This model explicitly accounts for the instrumental response and for the contributions of both the star and the companion.","The contributions of these two components and the calibration parameters are jointly estimated by solving a regularized inverse problem.","This problem having no closed-form solution, we propose an alternating minimization strategy to effectively find the solution.   ","We have tested our method on empirical long-slit spectroscopic data and by injecting synthetic companion signals in these data.","The proposed initialization and the alternating strategy effectively avoid the self-subtraction bias, even for companions observed very close to the coronagraphic mask.","Careful modeling and calibration of the angular and spectral dispersion laws of the instrument clearly reduce the contamination by the stellar leakages.","In practice, the outputs of the method are mostly driven by a single hyper-parameter which tunes the level of regularization of the companion SED."],"url":"http://arxiv.org/abs/2306.03467v1"}
