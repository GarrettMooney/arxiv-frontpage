{"created":"2023-12-12 18:59:40","title":"SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration","abstract":"Recent techniques for real-time view synthesis have rapidly advanced in fidelity and speed, and modern methods are capable of rendering near-photorealistic scenes at interactive frame rates. At the same time, a tension has arisen between explicit scene representations amenable to rasterization and neural fields built on ray marching, with state-of-the-art instances of the latter surpassing the former in quality while being prohibitively expensive for real-time applications. In this work, we introduce SMERF, a view synthesis approach that achieves state-of-the-art accuracy among real-time methods on large scenes with footprints up to 300 m$^2$ at a volumetric resolution of 3.5 mm$^3$. Our method is built upon two primary contributions: a hierarchical model partitioning scheme, which increases model capacity while constraining compute and memory consumption, and a distillation training strategy that simultaneously yields high fidelity and internal consistency. Our approach enables full six degrees of freedom (6DOF) navigation within a web browser and renders in real-time on commodity smartphones and laptops. Extensive experiments show that our method exceeds the current state-of-the-art in real-time novel view synthesis by 0.78 dB on standard benchmarks and 1.78 dB on large scenes, renders frames three orders of magnitude faster than state-of-the-art radiance field models, and achieves real-time performance across a wide variety of commodity devices, including smartphones. We encourage the reader to explore these models in person at our project website: https://smerf-3d.github.io.","sentences":["Recent techniques for real-time view synthesis have rapidly advanced in fidelity and speed, and modern methods are capable of rendering near-photorealistic scenes at interactive frame rates.","At the same time, a tension has arisen between explicit scene representations amenable to rasterization and neural fields built on ray marching, with state-of-the-art instances of the latter surpassing the former in quality while being prohibitively expensive for real-time applications.","In this work, we introduce SMERF, a view synthesis approach that achieves state-of-the-art accuracy among real-time methods on large scenes with footprints up to 300 m$^2$ at a volumetric resolution of 3.5 mm$^3$. Our method is built upon two primary contributions: a hierarchical model partitioning scheme, which increases model capacity while constraining compute and memory consumption, and a distillation training strategy that simultaneously yields high fidelity and internal consistency.","Our approach enables full six degrees of freedom (6DOF) navigation within a web browser and renders in real-time on commodity smartphones and laptops.","Extensive experiments show that our method exceeds the current state-of-the-art in real-time novel view synthesis by 0.78 dB on standard benchmarks and 1.78 dB on large scenes, renders frames three orders of magnitude faster than state-of-the-art radiance field models, and achieves real-time performance across a wide variety of commodity devices, including smartphones.","We encourage the reader to explore these models in person at our project website: https://smerf-3d.github.io."],"url":"http://arxiv.org/abs/2312.07541v1"}
{"created":"2023-12-12 18:59:30","title":"diff History for Long-Context Language Agents","abstract":"Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, leads to prohibitively large textual prompts. As a result, prior work in LM agents is limited to restricted domains with either small observation size or minimal needs for interaction history. In this paper, we introduce a simple and highly effective solution to these issues. We exploit the fact that consecutive text observations have high similarity and propose to compress them via the Unix diff command. We demonstrate our approach in NetHack, a complex rogue-like video game, that requires long-horizon reasoning for decision-making and is far from solved, particularly for neural agents. Diff history offers an average of 4x increase in the length of the text-based interaction history available to the LM. This observational compression along with the benefits of abstraction yields a 7x improvement in game score on held-out environment instances over state-of-the-art baselines. It also outperforms prior agents that use visual observations by over 40%.","sentences":["Language Models (LMs) offer an exciting solution for general-purpose embodied control.","However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, leads to prohibitively large textual prompts.","As a result, prior work in LM agents is limited to restricted domains with either small observation size or minimal needs for interaction history.","In this paper, we introduce a simple and highly effective solution to these issues.","We exploit the fact that consecutive text observations have high similarity and propose to compress them via the Unix diff command.","We demonstrate our approach in NetHack, a complex rogue-like video game, that requires long-horizon reasoning for decision-making and is far from solved, particularly for neural agents.","Diff history offers an average of 4x increase in the length of the text-based interaction history available to the LM.","This observational compression along with the benefits of abstraction yields a 7x improvement in game score on held-out environment instances over state-of-the-art baselines.","It also outperforms prior agents that use visual observations by over 40%."],"url":"http://arxiv.org/abs/2312.07540v1"}
{"created":"2023-12-12 18:59:25","title":"HeadArtist: Text-conditioned 3D Head Generation with Self Score Distillation","abstract":"This work presents HeadArtist for 3D head generation from text descriptions. With a landmark-guided ControlNet serving as the generative prior, we come up with an efficient pipeline that optimizes a parameterized 3D head model under the supervision of the prior distillation itself. We call such a process self score distillation (SSD). In detail, given a sampled camera pose, we first render an image and its corresponding landmarks from the head model, and add some particular level of noise onto the image. The noisy image, landmarks, and text condition are then fed into the frozen ControlNet twice for noise prediction. Two different classifier-free guidance (CFG) weights are applied during these two predictions, and the prediction difference offers a direction on how the rendered image can better match the text of interest. Experimental results suggest that our approach delivers high-quality 3D head sculptures with adequate geometry and photorealistic appearance, significantly outperforming state-ofthe-art methods. We also show that the same pipeline well supports editing the generated heads, including both geometry deformation and appearance change.","sentences":["This work presents HeadArtist for 3D head generation from text descriptions.","With a landmark-guided ControlNet serving as the generative prior, we come up with an efficient pipeline that optimizes a parameterized 3D head model under the supervision of the prior distillation itself.","We call such a process self score distillation (SSD).","In detail, given a sampled camera pose, we first render an image and its corresponding landmarks from the head model, and add some particular level of noise onto the image.","The noisy image, landmarks, and text condition are then fed into the frozen ControlNet twice for noise prediction.","Two different classifier-free guidance (CFG) weights are applied during these two predictions, and the prediction difference offers a direction on how the rendered image can better match the text of interest.","Experimental results suggest that our approach delivers high-quality 3D head sculptures with adequate geometry and photorealistic appearance, significantly outperforming state-ofthe-art methods.","We also show that the same pipeline well supports editing the generated heads, including both geometry deformation and appearance change."],"url":"http://arxiv.org/abs/2312.07539v1"}
{"created":"2023-12-12 18:59:21","title":"Anatomically Constrained Implicit Face Models","abstract":"Coordinate based implicit neural representations have gained rapid popularity in recent years as they have been successfully used in image, geometry and scene modeling tasks. In this work, we present a novel use case for such implicit representations in the context of learning anatomically constrained face models. Actor specific anatomically constrained face models are the state of the art in both facial performance capture and performance retargeting. Despite their practical success, these anatomical models are slow to evaluate and often require extensive data capture to be built. We propose the anatomical implicit face model; an ensemble of implicit neural networks that jointly learn to model the facial anatomy and the skin surface with high-fidelity, and can readily be used as a drop in replacement to conventional blendshape models. Given an arbitrary set of skin surface meshes of an actor and only a neutral shape with estimated skull and jaw bones, our method can recover a dense anatomical substructure which constrains every point on the facial surface. We demonstrate the usefulness of our approach in several tasks ranging from shape fitting, shape editing, and performance retargeting.","sentences":["Coordinate based implicit neural representations have gained rapid popularity in recent years as they have been successfully used in image, geometry and scene modeling tasks.","In this work, we present a novel use case for such implicit representations in the context of learning anatomically constrained face models.","Actor specific anatomically constrained face models are the state of the art in both facial performance capture and performance retargeting.","Despite their practical success, these anatomical models are slow to evaluate and often require extensive data capture to be built.","We propose the anatomical implicit face model; an ensemble of implicit neural networks that jointly learn to model the facial anatomy and the skin surface with high-fidelity, and can readily be used as a drop in replacement to conventional blendshape models.","Given an arbitrary set of skin surface meshes of an actor and only a neutral shape with estimated skull and jaw bones, our method can recover a dense anatomical substructure which constrains every point on the facial surface.","We demonstrate the usefulness of our approach in several tasks ranging from shape fitting, shape editing, and performance retargeting."],"url":"http://arxiv.org/abs/2312.07538v1"}
{"created":"2023-12-12 18:59:16","title":"FreeInit: Bridging Initialization Gap in Video Diffusion Models","abstract":"Though diffusion-based video generation has witnessed rapid progress, the inference results of existing models still exhibit unsatisfactory temporal consistency and unnatural dynamics. In this paper, we delve deep into the noise initialization of video diffusion models, and discover an implicit training-inference gap that attributes to the unsatisfactory inference quality. Our key findings are: 1) the spatial-temporal frequency distribution of the initial latent at inference is intrinsically different from that for training, and 2) the denoising process is significantly influenced by the low-frequency components of the initial noise. Motivated by these observations, we propose a concise yet effective inference sampling strategy, FreeInit, which significantly improves temporal consistency of videos generated by diffusion models. Through iteratively refining the spatial-temporal low-frequency components of the initial latent during inference, FreeInit is able to compensate the initialization gap between training and inference, thus effectively improving the subject appearance and temporal consistency of generation results. Extensive experiments demonstrate that FreeInit consistently enhances the generation results of various text-to-video generation models without additional training.","sentences":["Though diffusion-based video generation has witnessed rapid progress, the inference results of existing models still exhibit unsatisfactory temporal consistency and unnatural dynamics.","In this paper, we delve deep into the noise initialization of video diffusion models, and discover an implicit training-inference gap that attributes to the unsatisfactory inference quality.","Our key findings are: 1) the spatial-temporal frequency distribution of the initial latent at inference is intrinsically different from that for training, and 2) the denoising process is significantly influenced by the low-frequency components of the initial noise.","Motivated by these observations, we propose a concise yet effective inference sampling strategy, FreeInit, which significantly improves temporal consistency of videos generated by diffusion models.","Through iteratively refining the spatial-temporal low-frequency components of the initial latent during inference, FreeInit is able to compensate the initialization gap between training and inference, thus effectively improving the subject appearance and temporal consistency of generation results.","Extensive experiments demonstrate that FreeInit consistently enhances the generation results of various text-to-video generation models without additional training."],"url":"http://arxiv.org/abs/2312.07537v1"}
{"created":"2023-12-12 18:59:14","title":"FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition","abstract":"Recent approaches such as ControlNet offer users fine-grained spatial control over text-to-image (T2I) diffusion models. However, auxiliary modules have to be trained for each type of spatial condition, model architecture, and checkpoint, putting them at odds with the diverse intents and preferences a human designer would like to convey to the AI models during the content creation process. In this work, we present FreeControl, a training-free approach for controllable T2I generation that supports multiple conditions, architectures, and checkpoints simultaneously. FreeControl designs structure guidance to facilitate the structure alignment with a guidance image, and appearance guidance to enable the appearance sharing between images generated using the same seed. Extensive qualitative and quantitative experiments demonstrate the superior performance of FreeControl across a variety of pre-trained T2I models. In particular, FreeControl facilitates convenient training-free control over many different architectures and checkpoints, allows the challenging input conditions on which most of the existing training-free methods fail, and achieves competitive synthesis quality with training-based approaches.","sentences":["Recent approaches such as ControlNet offer users fine-grained spatial control over text-to-image (T2I) diffusion models.","However, auxiliary modules have to be trained for each type of spatial condition, model architecture, and checkpoint, putting them at odds with the diverse intents and preferences a human designer would like to convey to the AI models during the content creation process.","In this work, we present FreeControl, a training-free approach for controllable T2I generation that supports multiple conditions, architectures, and checkpoints simultaneously.","FreeControl designs structure guidance to facilitate the structure alignment with a guidance image, and appearance guidance to enable the appearance sharing between images generated using the same seed.","Extensive qualitative and quantitative experiments demonstrate the superior performance of FreeControl across a variety of pre-trained T2I models.","In particular, FreeControl facilitates convenient training-free control over many different architectures and checkpoints, allows the challenging input conditions on which most of the existing training-free methods fail, and achieves competitive synthesis quality with training-based approaches."],"url":"http://arxiv.org/abs/2312.07536v1"}
{"created":"2023-12-12 18:59:06","title":"Improved Frequency Estimation Algorithms with and without Predictions","abstract":"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al. (2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. without the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","sentences":["Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis.","Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input.","The work of Hsu et al. (2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on.","In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream.","We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al.","without the use of any predictions.","Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art.","Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches."],"url":"http://arxiv.org/abs/2312.07535v1"}
{"created":"2023-12-12 18:58:18","title":"VILA: On Pre-training for Visual Language Models","abstract":"Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge.","sentences":["Visual language models (VLMs) rapidly progressed with the recent success of large language models.","There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities.","In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons.","We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy.","With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles.","Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge."],"url":"http://arxiv.org/abs/2312.07533v1"}
{"created":"2023-12-12 18:58:02","title":"Interfacing Foundation Models' Embeddings","abstract":"We present FIND, a generalized interface for aligning foundation models' embeddings. As shown in teaser figure, a lightweight transformer interface without tuning any foundation model weights is enough for a unified image (segmentation) and dataset-level (retrieval) understanding. The proposed interface has the following favorable attributes: (1) Generalizable. It applies to various tasks spanning retrieval, segmentation, \\textit{etc.}, under the same architecture and weights. (2) Prototypable. Different tasks are able to be implemented through prototyping attention masks and embedding types. (3) Extendable. The proposed interface is adaptive to new tasks, and new models. (4) Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space. In light of the interleaved embedding space, we introduce the FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleave segmentation and retrieval. Our approach achieves state-of-the-art performance on FIND-Bench and competitive performance on standard retrieval and segmentation settings. The training, evaluation, and demo code as well as the dataset have been released at https://github.com/UX-Decoder/FIND.","sentences":["We present FIND, a generalized interface for aligning foundation models' embeddings.","As shown in teaser figure, a lightweight transformer interface without tuning any foundation model weights is enough for a unified image (segmentation) and dataset-level (retrieval) understanding.","The proposed interface has the following favorable attributes: (1) Generalizable.","It applies to various tasks spanning retrieval, segmentation, \\textit{etc.}, under the same architecture and weights.","(2) Prototypable.","Different tasks are able to be implemented through prototyping attention masks and embedding types.","(3) Extendable.","The proposed interface is adaptive to new tasks, and new models.","(4) Interleavable.","With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space.","In light of the interleaved embedding space, we introduce the FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleave segmentation and retrieval.","Our approach achieves state-of-the-art performance on FIND-Bench and competitive performance on standard retrieval and segmentation settings.","The training, evaluation, and demo code as well as the dataset have been released at https://github.com/UX-Decoder/FIND."],"url":"http://arxiv.org/abs/2312.07532v1"}
{"created":"2023-12-12 18:57:46","title":"WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion","abstract":"The estimation of 3D human motion from video has progressed rapidly but current methods still have several key limitations. First, most methods estimate the human in camera coordinates. Second, prior work on estimating humans in global coordinates often assumes a flat ground plane and produces foot sliding. Third, the most accurate methods rely on computationally expensive optimization pipelines, limiting their use to offline applications. Finally, existing video-based methods are surprisingly less accurate than single-frame methods. We address these limitations with WHAM (World-grounded Humans with Accurate Motion), which accurately and efficiently reconstructs 3D human motion in a global coordinate system from video. WHAM learns to lift 2D keypoint sequences to 3D using motion capture data and fuses this with video features, integrating motion context and visual information. WHAM exploits camera angular velocity estimated from a SLAM method together with human motion to estimate the body's global trajectory. We combine this with a contact-aware trajectory refinement method that lets WHAM capture human motion in diverse conditions, such as climbing stairs. WHAM outperforms all existing 3D human motion recovery methods across multiple in-the-wild benchmarks. Code will be available for research purposes at http://wham.is.tue.mpg.de/","sentences":["The estimation of 3D human motion from video has progressed rapidly but current methods still have several key limitations.","First, most methods estimate the human in camera coordinates.","Second, prior work on estimating humans in global coordinates often assumes a flat ground plane and produces foot sliding.","Third, the most accurate methods rely on computationally expensive optimization pipelines, limiting their use to offline applications.","Finally, existing video-based methods are surprisingly less accurate than single-frame methods.","We address these limitations with WHAM (World-grounded Humans with Accurate Motion), which accurately and efficiently reconstructs 3D human motion in a global coordinate system from video.","WHAM learns to lift 2D keypoint sequences to 3D using motion capture data and fuses this with video features, integrating motion context and visual information.","WHAM exploits camera angular velocity estimated from a SLAM method together with human motion to estimate the body's global trajectory.","We combine this with a contact-aware trajectory refinement method that lets WHAM capture human motion in diverse conditions, such as climbing stairs.","WHAM outperforms all existing 3D human motion recovery methods across multiple in-the-wild benchmarks.","Code will be available for research purposes at http://wham.is.tue.mpg.de/"],"url":"http://arxiv.org/abs/2312.07531v1"}
{"created":"2023-12-12 18:57:25","title":"Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance","abstract":"Weakly supervised 3D object detection aims to learn a 3D detector with lower annotation cost, e.g., 2D labels. Unlike prior work which still relies on few accurate 3D annotations, we propose a framework to study how to leverage constraints between 2D and 3D domains without requiring any 3D labels. Specifically, we employ visual data from three perspectives to establish connections between 2D and 3D domains. First, we design a feature-level constraint to align LiDAR and image features based on object-aware regions. Second, the output-level constraint is developed to enforce the overlap between 2D and projected 3D box estimations. Finally, the training-level constraint is utilized by producing accurate and consistent 3D pseudo-labels that align with the visual data. We conduct extensive experiments on the KITTI dataset to validate the effectiveness of the proposed three constraints. Without using any 3D labels, our method achieves favorable performance against state-of-the-art approaches and is competitive with the method that uses 500-frame 3D annotations. Code and models will be made publicly available at https://github.com/kuanchihhuang/VG-W3D.","sentences":["Weakly supervised 3D object detection aims to learn a 3D detector with lower annotation cost, e.g., 2D labels.","Unlike prior work which still relies on few accurate 3D annotations, we propose a framework to study how to leverage constraints between 2D and 3D domains without requiring any 3D labels.","Specifically, we employ visual data from three perspectives to establish connections between 2D and 3D domains.","First, we design a feature-level constraint to align LiDAR and image features based on object-aware regions.","Second, the output-level constraint is developed to enforce the overlap between 2D and projected 3D box estimations.","Finally, the training-level constraint is utilized by producing accurate and consistent 3D pseudo-labels that align with the visual data.","We conduct extensive experiments on the KITTI dataset to validate the effectiveness of the proposed three constraints.","Without using any 3D labels, our method achieves favorable performance against state-of-the-art approaches and is competitive with the method that uses 500-frame 3D annotations.","Code and models will be made publicly available at https://github.com/kuanchihhuang/VG-W3D."],"url":"http://arxiv.org/abs/2312.07530v1"}
{"created":"2023-12-12 18:56:14","title":"Topological Obstructions and How to Avoid Them","abstract":"Incorporating geometric inductive biases into models can aid interpretability and generalization, but encoding to a specific geometric structure can be challenging due to the imposed topological constraints. In this paper, we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces. We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number. We then discuss how normalizing flows can potentially circumvent these obstructions by defining multimodal variational distributions. Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces and empirically evaluate our model on 2 domains. We observe improved stability during training and a higher chance of converging to a homeomorphic encoder.","sentences":["Incorporating geometric inductive biases into models can aid interpretability and generalization, but encoding to a specific geometric structure can be challenging due to the imposed topological constraints.","In this paper, we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces.","We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number.","We then discuss how normalizing flows can potentially circumvent these obstructions by defining multimodal variational distributions.","Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces and empirically evaluate our model on 2 domains.","We observe improved stability during training and a higher chance of converging to a homeomorphic encoder."],"url":"http://arxiv.org/abs/2312.07529v1"}
{"created":"2023-12-12 18:55:43","title":"BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability","abstract":"While there are numerous benchmarks comparing the performance of modern language models (LMs), end-task evaluations often conflate notions of *factual accuracy* (\"truth\") and *reasoning ability* (\"rationality\", or \"honesty\" in the sense of correctly reporting implications of beliefs). Our goal is a dataset that clearly distinguishes these two notions. Our approach is to leverage and extend a collection of human-annotated *entailment trees*, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the \"content effect\"). The resulting dataset, called BaRDa, contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319 false statements. Testing on four GPT-series models, GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of 74.1/80.6/82.6/87.1 and reasoning accuracy scores of 63.1/78.0/71.8/79.2. This shows the clear progression of models towards improved factual accuracy and entailment reasoning, and the dataset provides a new benchmark that more cleanly separates and quantifies these two notions.","sentences":["While there are numerous benchmarks comparing the performance of modern language models (LMs), end-task evaluations often conflate notions of *factual accuracy* (\"truth\") and *reasoning ability* (\"rationality\", or \"honesty\" in the sense of correctly reporting implications of beliefs).","Our goal is a dataset that clearly distinguishes these two notions.","Our approach is to leverage and extend a collection of human-annotated *entailment trees*, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the \"content effect\").","The resulting dataset, called BaRDa, contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319 false statements.","Testing on four GPT-series models, GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of 74.1/80.6/82.6/87.1 and reasoning accuracy scores of 63.1/78.0/71.8/79.2.","This shows the clear progression of models towards improved factual accuracy and entailment reasoning, and the dataset provides a new benchmark that more cleanly separates and quantifies these two notions."],"url":"http://arxiv.org/abs/2312.07527v1"}
{"created":"2023-12-12 18:55:29","title":"RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation","abstract":"Real-time multi-person pose estimation presents significant challenges in balancing speed and precision. While two-stage top-down methods slow down as the number of people in the image increases, existing one-stage methods often fail to simultaneously deliver high accuracy and real-time performance. This paper introduces RTMO, a one-stage pose estimation framework that seamlessly integrates coordinate classification by representing keypoints using dual 1-D heatmaps within the YOLO architecture, achieving accuracy comparable to top-down methods while maintaining high speed. We propose a dynamic coordinate classifier and a tailored loss function for heatmap learning, specifically designed to address the incompatibilities between coordinate classification and dense prediction models. RTMO outperforms state-of-the-art one-stage pose estimators, achieving 1.1% higher AP on COCO while operating about 9 times faster with the same backbone. Our largest model, RTMO-l, attains 74.8% AP on COCO val2017 and 141 FPS on a single V100 GPU, demonstrating its efficiency and accuracy. The code and models are available at https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmo.","sentences":["Real-time multi-person pose estimation presents significant challenges in balancing speed and precision.","While two-stage top-down methods slow down as the number of people in the image increases, existing one-stage methods often fail to simultaneously deliver high accuracy and real-time performance.","This paper introduces RTMO, a one-stage pose estimation framework that seamlessly integrates coordinate classification by representing keypoints using dual 1-D heatmaps within the YOLO architecture, achieving accuracy comparable to top-down methods while maintaining high speed.","We propose a dynamic coordinate classifier and a tailored loss function for heatmap learning, specifically designed to address the incompatibilities between coordinate classification and dense prediction models.","RTMO outperforms state-of-the-art one-stage pose estimators, achieving 1.1% higher AP on COCO while operating about 9 times faster with the same backbone.","Our largest model, RTMO-l, attains 74.8% AP on COCO val2017 and 141 FPS on a single V100 GPU, demonstrating its efficiency and accuracy.","The code and models are available at https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmo."],"url":"http://arxiv.org/abs/2312.07526v1"}
{"created":"2023-12-12 18:53:36","title":"Self-Healing Distributed Swarm Formation Control Using Image Moments","abstract":"Human-swarm interaction is facilitated by a low-dimensional encoding of the swarm formation, independent of the (possibly large) number of robots. We propose using image moments to encode two-dimensional formations of robots. Each robot knows the desired formation moments, and simultaneously estimates the current moments of the entire swarm while controlling its motion to better achieve the desired group moments. The estimator is a distributed optimization, requiring no centralized processing, and self-healing, meaning that the process is robust to initialization errors, packet drops, and robots being added to or removed from the swarm. Our experimental results with a swarm of 50 robots, suffering nearly 50% packet loss, show that distributed estimation and control of image moments effectively achieves desired swarm formations.","sentences":["Human-swarm interaction is facilitated by a low-dimensional encoding of the swarm formation, independent of the (possibly large) number of robots.","We propose using image moments to encode two-dimensional formations of robots.","Each robot knows the desired formation moments, and simultaneously estimates the current moments of the entire swarm while controlling its motion to better achieve the desired group moments.","The estimator is a distributed optimization, requiring no centralized processing, and self-healing, meaning that the process is robust to initialization errors, packet drops, and robots being added to or removed from the swarm.","Our experimental results with a swarm of 50 robots, suffering nearly 50% packet loss, show that distributed estimation and control of image moments effectively achieves desired swarm formations."],"url":"http://arxiv.org/abs/2312.07523v1"}
{"created":"2023-12-12 18:48:25","title":"Search Optimization with Query Likelihood Boosting and Two-Level Approximate Search for Edge Devices","abstract":"We present a novel search optimization solution for approximate nearest neighbor (ANN) search on resource-constrained edge devices. Traditional ANN approaches fall short in meeting the specific demands of real-world scenarios, e.g., skewed query likelihood distribution and search on large-scale indices with a low latency and small footprint. To address these limitations, we introduce two key components: a Query Likelihood Boosted Tree (QLBT) to optimize average search latency for frequently used small datasets, and a two-level approximate search algorithm to enable efficient retrieval with large datasets on edge devices. We perform thorough evaluation on simulated and real data and demonstrate QLBT can significantly reduce latency by 15% on real data and our two-level search algorithm successfully achieve deployable accuracy and latency on a 10 million dataset for edge devices. In addition, we provide a comprehensive protocol for configuring and optimizing on-device search algorithm through extensive empirical studies.","sentences":["We present a novel search optimization solution for approximate nearest neighbor (ANN) search on resource-constrained edge devices.","Traditional ANN approaches fall short in meeting the specific demands of real-world scenarios, e.g., skewed query likelihood distribution and search on large-scale indices with a low latency and small footprint.","To address these limitations, we introduce two key components: a Query Likelihood Boosted Tree (QLBT) to optimize average search latency for frequently used small datasets, and a two-level approximate search algorithm to enable efficient retrieval with large datasets on edge devices.","We perform thorough evaluation on simulated and real data and demonstrate QLBT can significantly reduce latency by 15% on real data and our two-level search algorithm successfully achieve deployable accuracy and latency on a 10 million dataset for edge devices.","In addition, we provide a comprehensive protocol for configuring and optimizing on-device search algorithm through extensive empirical studies."],"url":"http://arxiv.org/abs/2312.07517v1"}
{"created":"2023-12-12 18:46:30","title":"Integrated and Lightweight Design of Electro-hydraulic Ankle Prosthesis","abstract":"For lower limb amputees, an active ankle joint prosthesis can provide basic mobility functions. This study focuses on an ankle joint prosthesis system based on the principle of electric-hydraulic actuation. By analyzing the characteristics of human gait cycles and the mechanics of ankle joint movement, a lightweight and integrated ankle joint prosthesis is designed, considering the requirements for normal ankle joint kinematics and dynamics. The components of the prosthesis are optimized through simulation and iterative improvements, while ensuring tight integration within minimal space. The design and simulation verification of the integrated lightweight prosthesis components are achieved. This research addresses the contradiction between the high output capability and the constraints on volume and weight in prosthetic devices.","sentences":["For lower limb amputees, an active ankle joint prosthesis can provide basic mobility functions.","This study focuses on an ankle joint prosthesis system based on the principle of electric-hydraulic actuation.","By analyzing the characteristics of human gait cycles and the mechanics of ankle joint movement, a lightweight and integrated ankle joint prosthesis is designed, considering the requirements for normal ankle joint kinematics and dynamics.","The components of the prosthesis are optimized through simulation and iterative improvements, while ensuring tight integration within minimal space.","The design and simulation verification of the integrated lightweight prosthesis components are achieved.","This research addresses the contradiction between the high output capability and the constraints on volume and weight in prosthetic devices."],"url":"http://arxiv.org/abs/2312.07514v1"}
{"created":"2023-12-12 18:44:19","title":"A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems","abstract":"Recent advances in computational modelling of atomic systems, spanning molecules, proteins, and materials, represent them as geometric graphs with atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric attributes transform according to the inherent physical symmetries of 3D atomic systems, including rotations and translations in Euclidean space, as well as node permutations. In recent years, Geometric Graph Neural Networks have emerged as the preferred machine learning architecture powering applications ranging from protein structure prediction to molecular simulations and material generation. Their specificity lies in the inductive biases they leverage -- such as physical symmetries and chemical properties -- to learn informative representations of these geometric graphs. In this opinionated paper, we provide a comprehensive and self-contained overview of the field of Geometric GNNs for 3D atomic systems. We cover fundamental background material and introduce a pedagogical taxonomy of Geometric GNN architectures:(1) invariant networks, (2) equivariant networks in Cartesian basis, (3) equivariant networks in spherical basis, and (4) unconstrained networks. Additionally, we outline key datasets and application areas and suggest future research directions. The objective of this work is to present a structured perspective on the field, making it accessible to newcomers and aiding practitioners in gaining an intuition for its mathematical abstractions.","sentences":["Recent advances in computational modelling of atomic systems, spanning molecules, proteins, and materials, represent them as geometric graphs with atoms embedded as nodes in 3D Euclidean space.","In these graphs, the geometric attributes transform according to the inherent physical symmetries of 3D atomic systems, including rotations and translations in Euclidean space, as well as node permutations.","In recent years, Geometric Graph Neural Networks have emerged as the preferred machine learning architecture powering applications ranging from protein structure prediction to molecular simulations and material generation.","Their specificity lies in the inductive biases they leverage -- such as physical symmetries and chemical properties -- to learn informative representations of these geometric graphs.","In this opinionated paper, we provide a comprehensive and self-contained overview of the field of Geometric GNNs for 3D atomic systems.","We cover fundamental background material and introduce a pedagogical taxonomy of Geometric GNN architectures:(1) invariant networks, (2) equivariant networks in Cartesian basis, (3) equivariant networks in spherical basis, and (4) unconstrained networks.","Additionally, we outline key datasets and application areas and suggest future research directions.","The objective of this work is to present a structured perspective on the field, making it accessible to newcomers and aiding practitioners in gaining an intuition for its mathematical abstractions."],"url":"http://arxiv.org/abs/2312.07511v1"}
{"created":"2023-12-12 18:43:05","title":"PEEKABOO: Interactive Video Generation via Masked-Diffusion","abstract":"Recently there has been a lot of progress in text-to-video generation, with state-of-the-art models being capable of generating high quality, realistic videos. However, these models lack the capability for users to interactively control and generate videos, which can potentially unlock new areas of application. As a first step towards this goal, we tackle the problem of endowing diffusion-based video generation models with interactive spatio-temporal control over their output. To this end, we take inspiration from the recent advances in segmentation literature to propose a novel spatio-temporal masked attention module - Peekaboo. This module is a training-free, no-inference-overhead addition to off-the-shelf video generation models which enables spatio-temporal control. We also propose an evaluation benchmark for the interactive video generation task. Through extensive qualitative and quantitative evaluation, we establish that Peekaboo enables control video generation and even obtains a gain of upto 3.8x in mIoU over baseline models.","sentences":["Recently there has been a lot of progress in text-to-video generation, with state-of-the-art models being capable of generating high quality, realistic videos.","However, these models lack the capability for users to interactively control and generate videos, which can potentially unlock new areas of application.","As a first step towards this goal, we tackle the problem of endowing diffusion-based video generation models with interactive spatio-temporal control over their output.","To this end, we take inspiration from the recent advances in segmentation literature to propose a novel spatio-temporal masked attention module - Peekaboo.","This module is a training-free, no-inference-overhead addition to off-the-shelf video generation models which enables spatio-temporal control.","We also propose an evaluation benchmark for the interactive video generation task.","Through extensive qualitative and quantitative evaluation, we establish that Peekaboo enables control video generation and even obtains a gain of upto 3.8x in mIoU over baseline models."],"url":"http://arxiv.org/abs/2312.07509v1"}
{"created":"2023-12-12 18:41:30","title":"NAC-TCN: Temporal Convolutional Networks with Causal Dilated Neighborhood Attention for Emotion Understanding","abstract":"In the task of emotion recognition from videos, a key improvement has been to focus on emotions over time rather than a single frame. There are many architectures to address this task such as GRUs, LSTMs, Self-Attention, Transformers, and Temporal Convolutional Networks (TCNs). However, these methods suffer from high memory usage, large amounts of operations, or poor gradients. We propose a method known as Neighborhood Attention with Convolutions TCN (NAC-TCN) which incorporates the benefits of attention and Temporal Convolutional Networks while ensuring that causal relationships are understood which results in a reduction in computation and memory cost. We accomplish this by introducing a causal version of Dilated Neighborhood Attention while incorporating it with convolutions. Our model achieves comparable, better, or state-of-the-art performance over TCNs, TCAN, LSTMs, and GRUs while requiring fewer parameters on standard emotion recognition datasets. We publish our code online for easy reproducibility and use in other projects.","sentences":["In the task of emotion recognition from videos, a key improvement has been to focus on emotions over time rather than a single frame.","There are many architectures to address this task such as GRUs, LSTMs, Self-Attention, Transformers, and Temporal Convolutional Networks (TCNs).","However, these methods suffer from high memory usage, large amounts of operations, or poor gradients.","We propose a method known as Neighborhood Attention with Convolutions TCN (NAC-TCN) which incorporates the benefits of attention and Temporal Convolutional Networks while ensuring that causal relationships are understood which results in a reduction in computation and memory cost.","We accomplish this by introducing a causal version of Dilated Neighborhood Attention while incorporating it with convolutions.","Our model achieves comparable, better, or state-of-the-art performance over TCNs, TCAN, LSTMs, and GRUs while requiring fewer parameters on standard emotion recognition datasets.","We publish our code online for easy reproducibility and use in other projects."],"url":"http://arxiv.org/abs/2312.07507v1"}
{"created":"2023-12-12 18:39:52","title":"COLMAP-Free 3D Gaussian Splatting","abstract":"While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing. We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes. Our project page is https://oasisyang.github.io/colmap-free-3dgs","sentences":["While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses.","To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses.","However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time.","On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations.","This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing.","We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses.","Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes.","Our project page is https://oasisyang.github.io/colmap-free-3dgs"],"url":"http://arxiv.org/abs/2312.07504v1"}
{"created":"2023-12-12 18:34:56","title":"Multi-Branch Network for Imagery Emotion Prediction","abstract":"For a long time, images have proved perfect at both storing and conveying rich semantics, especially human emotions. A lot of research has been conducted to provide machines with the ability to recognize emotions in photos of people. Previous methods mostly focus on facial expressions but fail to consider the scene context, meanwhile scene context plays an important role in predicting emotions, leading to more accurate results. In addition, Valence-Arousal-Dominance (VAD) values offer a more precise quantitative understanding of continuous emotions, yet there has been less emphasis on predicting them compared to discrete emotional categories. In this paper, we present a novel Multi-Branch Network (MBN), which utilizes various source information, including faces, bodies, and scene contexts to predict both discrete and continuous emotions in an image. Experimental results on EMOTIC dataset, which contains large-scale images of people in unconstrained situations labeled with 26 discrete categories of emotions and VAD values, show that our proposed method significantly outperforms state-of-the-art methods with 28.4% in mAP and 0.93 in MAE. The results highlight the importance of utilizing multiple contextual information in emotion prediction and illustrate the potential of our proposed method in a wide range of applications, such as effective computing, human-computer interaction, and social robotics. Source code: https://github.com/BaoNinh2808/Multi-Branch-Network-for-Imagery-Emotion-Prediction","sentences":["For a long time, images have proved perfect at both storing and conveying rich semantics, especially human emotions.","A lot of research has been conducted to provide machines with the ability to recognize emotions in photos of people.","Previous methods mostly focus on facial expressions but fail to consider the scene context, meanwhile scene context plays an important role in predicting emotions, leading to more accurate results.","In addition, Valence-Arousal-Dominance (VAD) values offer a more precise quantitative understanding of continuous emotions, yet there has been less emphasis on predicting them compared to discrete emotional categories.","In this paper, we present a novel Multi-Branch Network (MBN), which utilizes various source information, including faces, bodies, and scene contexts to predict both discrete and continuous emotions in an image.","Experimental results on EMOTIC dataset, which contains large-scale images of people in unconstrained situations labeled with 26 discrete categories of emotions and VAD values, show that our proposed method significantly outperforms state-of-the-art methods with 28.4% in mAP and 0.93 in MAE.","The results highlight the importance of utilizing multiple contextual information in emotion prediction and illustrate the potential of our proposed method in a wide range of applications, such as effective computing, human-computer interaction, and social robotics.","Source code: https://github.com/BaoNinh2808/Multi-Branch-Network-for-Imagery-Emotion-Prediction"],"url":"http://arxiv.org/abs/2312.07500v1"}
{"created":"2023-12-12 18:28:59","title":"Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly Detection","abstract":"This work studies the recently proposed challenging and practical Multi-class Unsupervised Anomaly Detection (MUAD) task, which only requires normal images for training while simultaneously testing both normal/anomaly images for multiple classes. Existing reconstruction-based methods typically adopt pyramid networks as encoders/decoders to obtain multi-resolution features, accompanied by elaborate sub-modules with heavier handcraft engineering designs for more precise localization. In contrast, a plain Vision Transformer (ViT) with simple architecture has been shown effective in multiple domains, which is simpler, more effective, and elegant. Following this spirit, this paper explores plain ViT architecture for MUAD. Specifically, we abstract a Meta-AD concept by inducing current reconstruction-based methods. Then, we instantiate a novel and elegant plain ViT-based symmetric ViTAD structure, effectively designed step by step from three macro and four micro perspectives. In addition, this paper reveals several interesting findings for further exploration. Finally, we propose a comprehensive and fair evaluation benchmark on eight metrics for the MUAD task. Based on a naive training recipe, ViTAD achieves state-of-the-art (SoTA) results and efficiency on the MVTec AD and VisA datasets without bells and whistles, obtaining 85.4 mAD that surpasses SoTA UniAD by +3.0, and only requiring 1.1 hours and 2.3G GPU memory to complete model training by a single V100 GPU. Source code, models, and more results are available at https://zhangzjn.github.io/projects/ViTAD.","sentences":["This work studies the recently proposed challenging and practical Multi-class Unsupervised Anomaly Detection (MUAD) task, which only requires normal images for training while simultaneously testing both normal/anomaly images for multiple classes.","Existing reconstruction-based methods typically adopt pyramid networks as encoders/decoders to obtain multi-resolution features, accompanied by elaborate sub-modules with heavier handcraft engineering designs for more precise localization.","In contrast, a plain Vision Transformer (ViT) with simple architecture has been shown effective in multiple domains, which is simpler, more effective, and elegant.","Following this spirit, this paper explores plain ViT architecture for MUAD.","Specifically, we abstract a Meta-AD concept by inducing current reconstruction-based methods.","Then, we instantiate a novel and elegant plain ViT-based symmetric ViTAD structure, effectively designed step by step from three macro and four micro perspectives.","In addition, this paper reveals several interesting findings for further exploration.","Finally, we propose a comprehensive and fair evaluation benchmark on eight metrics for the MUAD task.","Based on a naive training recipe, ViTAD achieves state-of-the-art (SoTA) results and efficiency on the MVTec AD and VisA datasets without bells and whistles, obtaining 85.4 mAD that surpasses SoTA UniAD by +3.0, and only requiring 1.1 hours and 2.3G GPU memory to complete model training by a single V100 GPU.","Source code, models, and more results are available at https://zhangzjn.github.io/projects/ViTAD."],"url":"http://arxiv.org/abs/2312.07495v1"}
{"created":"2023-12-12 18:27:44","title":"SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models","abstract":"Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender. In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models. We start with a comprehensive list of 93 stigmas documented in social science literature and curate a question-answering (QA) dataset which involves simple social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness. We present results for SocialStigmaQA with two widely used open source generative language models and we demonstrate that the output generated by these models considerably amplifies existing social bias against stigmatized groups. Specifically, we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles. We discover that the deliberate design of the templates in our benchmark (e.g., by adding biasing text to the prompt or varying the answer that indicates bias) impact the model tendencies to generate socially biased output. Additionally, we report on patterns in the generated chain-of-thought output, finding a variety of problems from subtle bias to evidence of a lack of reasoning.   Warning: This paper contains examples of text which is toxic, biased, and harmful.","sentences":["Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender.","In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models.","We start with a comprehensive list of 93 stigmas documented in social science literature and curate a question-answering (QA) dataset which involves simple social situations.","Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness.","We present results for SocialStigmaQA with two widely used open source generative language models and we demonstrate that the output generated by these models considerably amplifies existing social bias against stigmatized groups.","Specifically, we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles.","We discover that the deliberate design of the templates in our benchmark (e.g., by adding biasing text to the prompt or varying the answer that indicates bias) impact the model tendencies to generate socially biased output.","Additionally, we report on patterns in the generated chain-of-thought output, finding a variety of problems from subtle bias to evidence of a lack of reasoning.   ","Warning:","This paper contains examples of text which is toxic, biased, and harmful."],"url":"http://arxiv.org/abs/2312.07492v1"}
{"created":"2023-12-12 18:26:38","title":"On Robot Acceptance and Trust: A Review and Unanswered Questions","abstract":"This position paper briefly considers the current benefits and shortcomings surrounding robot trust and acceptance, focusing on robots with interactive capabilities. The paper concludes with currently unanswered questions and may serve as a jumping-off point for discussion.","sentences":["This position paper briefly considers the current benefits and shortcomings surrounding robot trust and acceptance, focusing on robots with interactive capabilities.","The paper concludes with currently unanswered questions and may serve as a jumping-off point for discussion."],"url":"http://arxiv.org/abs/2312.07491v1"}
{"created":"2023-12-12 18:24:44","title":"NearbyPatchCL: Leveraging Nearby Patches for Self-Supervised Patch-Level Multi-Class Classification in Whole-Slide Images","abstract":"Whole-slide image (WSI) analysis plays a crucial role in cancer diagnosis and treatment. In addressing the demands of this critical task, self-supervised learning (SSL) methods have emerged as a valuable resource, leveraging their efficiency in circumventing the need for a large number of annotations, which can be both costly and time-consuming to deploy supervised methods. Nevertheless, patch-wise representation may exhibit instability in performance, primarily due to class imbalances stemming from patch selection within WSIs. In this paper, we introduce Nearby Patch Contrastive Learning (NearbyPatchCL), a novel self-supervised learning method that leverages nearby patches as positive samples and a decoupled contrastive loss for robust representation learning. Our method demonstrates a tangible enhancement in performance for downstream tasks involving patch-level multi-class classification. Additionally, we curate a new dataset derived from WSIs sourced from the Canine Cutaneous Cancer Histology, thus establishing a benchmark for the rigorous evaluation of patch-level multi-class classification methodologies. Intensive experiments show that our method significantly outperforms the supervised baseline and state-of-the-art SSL methods with top-1 classification accuracy of 87.56%. Our method also achieves comparable results while utilizing a mere 1% of labeled data, a stark contrast to the 100% labeled data requirement of other approaches. Source code: https://github.com/nvtien457/NearbyPatchCL","sentences":["Whole-slide image (WSI) analysis plays a crucial role in cancer diagnosis and treatment.","In addressing the demands of this critical task, self-supervised learning (SSL) methods have emerged as a valuable resource, leveraging their efficiency in circumventing the need for a large number of annotations, which can be both costly and time-consuming to deploy supervised methods.","Nevertheless, patch-wise representation may exhibit instability in performance, primarily due to class imbalances stemming from patch selection within WSIs.","In this paper, we introduce Nearby Patch Contrastive Learning (NearbyPatchCL), a novel self-supervised learning method that leverages nearby patches as positive samples and a decoupled contrastive loss for robust representation learning.","Our method demonstrates a tangible enhancement in performance for downstream tasks involving patch-level multi-class classification.","Additionally, we curate a new dataset derived from WSIs sourced from the Canine Cutaneous Cancer Histology, thus establishing a benchmark for the rigorous evaluation of patch-level multi-class classification methodologies.","Intensive experiments show that our method significantly outperforms the supervised baseline and state-of-the-art SSL methods with top-1 classification accuracy of 87.56%.","Our method also achieves comparable results while utilizing a mere 1% of labeled data, a stark contrast to the 100% labeled data requirement of other approaches.","Source code: https://github.com/nvtien457/NearbyPatchCL"],"url":"http://arxiv.org/abs/2312.07489v1"}
{"created":"2023-12-12 18:24:15","title":"LMDrive: Closed-Loop End-to-End Driving with Large Language Models","abstract":"Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unforeseen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impressive reasoning capabilities that approach \"Artificial General Intelligence\". On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g. sensor data and navigation waypoints), restricting the vehicle's ability to understand language information and interact with humans. To this end, this paper introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LMDrive uniquely processes and integrates multi-modal sensor data with natural language instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate further research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes can be found at https://github.com/opendilab/LMDrive","sentences":["Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unforeseen events and challenging urban scenarios.","On the one hand, large language models (LLM) have shown impressive reasoning capabilities that approach \"Artificial General Intelligence\".","On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g. sensor data and navigation waypoints), restricting the vehicle's ability to understand language information and interact with humans.","To this end, this paper introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous driving framework.","LMDrive uniquely processes and integrates multi-modal sensor data with natural language instructions, enabling interaction with humans and navigation software in realistic instructional settings.","To facilitate further research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios.","Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness.","To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving.","Codes can be found at https://github.com/opendilab/LMDrive"],"url":"http://arxiv.org/abs/2312.07488v1"}
{"created":"2023-12-12 18:21:36","title":"MinD-3D: Reconstruct High-quality 3D objects in Human Brain","abstract":"In this paper, we introduce Recon3DMind, a groundbreaking task focused on reconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI) signals. This represents a major step forward in cognitive neuroscience and computer vision. To support this task, we present the fMRI-Shape dataset, utilizing 360-degree view videos of 3D objects for comprehensive fMRI signal capture. Containing 55 categories of common objects from daily life, this dataset will bolster future research endeavors. We also propose MinD-3D, a novel and effective three-stage framework that decodes and reconstructs the brain's 3D visual information from fMRI signals. This method starts by extracting and aggregating features from fMRI frames using a neuro-fusion encoder, then employs a feature bridge diffusion model to generate corresponding visual features, and ultimately recovers the 3D object through a generative transformer decoder. Our experiments demonstrate that this method effectively extracts features that are valid and highly correlated with visual regions of interest (ROIs) in fMRI signals. Notably, it not only reconstructs 3D objects with high semantic relevance and spatial similarity but also significantly deepens our understanding of the human brain's 3D visual processing capabilities. Project page at: https://jianxgao.github.io/MinD-3D.","sentences":["In this paper, we introduce Recon3DMind, a groundbreaking task focused on reconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI) signals.","This represents a major step forward in cognitive neuroscience and computer vision.","To support this task, we present the fMRI-Shape dataset, utilizing 360-degree view videos of 3D objects for comprehensive fMRI signal capture.","Containing 55 categories of common objects from daily life, this dataset will bolster future research endeavors.","We also propose MinD-3D, a novel and effective three-stage framework that decodes and reconstructs the brain's 3D visual information from fMRI signals.","This method starts by extracting and aggregating features from fMRI frames using a neuro-fusion encoder, then employs a feature bridge diffusion model to generate corresponding visual features, and ultimately recovers the 3D object through a generative transformer decoder.","Our experiments demonstrate that this method effectively extracts features that are valid and highly correlated with visual regions of interest (ROIs) in fMRI signals.","Notably, it not only reconstructs 3D objects with high semantic relevance and spatial similarity but also significantly deepens our understanding of the human brain's 3D visual processing capabilities.","Project page at: https://jianxgao.github.io/MinD-3D."],"url":"http://arxiv.org/abs/2312.07485v1"}
{"created":"2023-12-12 18:11:15","title":"Classification of retail products: From probabilistic ranking to neural networks","abstract":"Food retailing is now on an accelerated path to a success penetration into the digital market by new ways of value creation at all stages of the consumer decision process. One of the most important imperatives in this path is the availability of quality data to feed all the process in digital transformation. But the quality of data is not so obvious if we consider the variety of products and suppliers in the grocery market. Within this context of digital transformation of grocery industry, \\textit{Midiadia} is Spanish data provider company that works on converting data from the retailers' products into knowledge with attributes and insights from the product labels, that is, maintaining quality data in a dynamic market with a high dispersion of products. Currently, they manually categorize products (groceries) according to the information extracted directly (text processing) from the product labelling and packaging. This paper introduces a solution to automatically categorize the constantly changing product catalogue into a 3-level food taxonomy. Our proposal studies three different approaches: a score-based ranking method, traditional machine learning algorithms, and deep neural networks. Thus, we provide four different classifiers that support a more efficient and less error-prone maintenance of groceries catalogues, the main asset of the company. Finally, we have compared the performance of these three alternatives, concluding that traditional machine learning algorithms perform better, but closely followed by the score-based approach.","sentences":["Food retailing is now on an accelerated path to a success penetration into the digital market by new ways of value creation at all stages of the consumer decision process.","One of the most important imperatives in this path is the availability of quality data to feed all the process in digital transformation.","But the quality of data is not so obvious if we consider the variety of products and suppliers in the grocery market.","Within this context of digital transformation of grocery industry, \\textit{Midiadia} is Spanish data provider company that works on converting data from the retailers' products into knowledge with attributes and insights from the product labels, that is, maintaining quality data in a dynamic market with a high dispersion of products.","Currently, they manually categorize products (groceries) according to the information extracted directly (text processing) from the product labelling and packaging.","This paper introduces a solution to automatically categorize the constantly changing product catalogue into a 3-level food taxonomy.","Our proposal studies three different approaches: a score-based ranking method, traditional machine learning algorithms, and deep neural networks.","Thus, we provide four different classifiers that support a more efficient and less error-prone maintenance of groceries catalogues, the main asset of the company.","Finally, we have compared the performance of these three alternatives, concluding that traditional machine learning algorithms perform better, but closely followed by the score-based approach."],"url":"http://arxiv.org/abs/2312.07482v1"}
{"created":"2023-12-12 18:07:57","title":"Double-Flow GAN model for the reconstruction of perceived faces from brain activities","abstract":"Face plays an important role in human's visual perception, and reconstructing perceived faces from brain activities is challenging because of its difficulty in extracting high-level features and maintaining consistency of multiple face attributes, such as expression, identity, gender, etc. In this study, we proposed a novel reconstruction framework, which we called Double-Flow GAN, that can enhance the capability of discriminator and handle imbalances in images from certain domains that are too easy for generators. We also designed a pretraining process that uses features extracted from images as conditions for making it possible to pretrain the conditional reconstruction model from fMRI in a larger pure image dataset. Moreover, we developed a simple pretrained model to perform fMRI alignment to alleviate the problem of cross-subject reconstruction due to the variations of brain structure among different subjects. We conducted experiments by using our proposed method and state-of-the-art reconstruction models. Our results demonstrated that our method showed significant reconstruction performance, outperformed the previous reconstruction models, and exhibited a good generation ability.","sentences":["Face plays an important role in human's visual perception, and reconstructing perceived faces from brain activities is challenging because of its difficulty in extracting high-level features and maintaining consistency of multiple face attributes, such as expression, identity, gender, etc.","In this study, we proposed a novel reconstruction framework, which we called Double-Flow GAN, that can enhance the capability of discriminator and handle imbalances in images from certain domains that are too easy for generators.","We also designed a pretraining process that uses features extracted from images as conditions for making it possible to pretrain the conditional reconstruction model from fMRI in a larger pure image dataset.","Moreover, we developed a simple pretrained model to perform fMRI alignment to alleviate the problem of cross-subject reconstruction due to the variations of brain structure among different subjects.","We conducted experiments by using our proposed method and state-of-the-art reconstruction models.","Our results demonstrated that our method showed significant reconstruction performance, outperformed the previous reconstruction models, and exhibited a good generation ability."],"url":"http://arxiv.org/abs/2312.07478v1"}
{"created":"2023-12-12 18:05:46","title":"Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection","abstract":"In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task's essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a novel perspective, providing a deeper insight into the demonstration selection strategy for ICL.","sentences":["In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations.","Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence.","Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship.","Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task's essence and eliminate potential spurious correlations through the inter-demonstration comparison.","Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios.","In summary, this study explores the ICL mechanisms from a novel perspective, providing a deeper insight into the demonstration selection strategy for ICL."],"url":"http://arxiv.org/abs/2312.07476v1"}
{"created":"2023-12-12 17:55:45","title":"MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception","abstract":"It is a long-lasting goal to design an embodied system that can solve long-horizon open-world tasks in human-like ways. However, existing approaches usually struggle with compound difficulties caused by the logic-aware decomposition and context-aware execution of these tasks. To this end, we introduce MP5, an open-ended multimodal embodied system built upon the challenging Minecraft simulator, which can decompose feasible sub-objectives, design sophisticated situation-aware plans, and perform embodied action control, with frequent communication with a goal-conditioned active perception scheme. Specifically, MP5 is developed on top of recent advances in Multimodal Large Language Models (MLLMs), and the system is modulated into functional modules that can be scheduled and collaborated to ultimately solve pre-defined context- and process-dependent tasks. Extensive experiments prove that MP5 can achieve a 22% success rate on difficult process-dependent tasks and a 91% success rate on tasks that heavily depend on the context. Moreover, MP5 exhibits a remarkable ability to address many open-ended tasks that are entirely novel.","sentences":["It is a long-lasting goal to design an embodied system that can solve long-horizon open-world tasks in human-like ways.","However, existing approaches usually struggle with compound difficulties caused by the logic-aware decomposition and context-aware execution of these tasks.","To this end, we introduce MP5, an open-ended multimodal embodied system built upon the challenging Minecraft simulator, which can decompose feasible sub-objectives, design sophisticated situation-aware plans, and perform embodied action control, with frequent communication with a goal-conditioned active perception scheme.","Specifically, MP5 is developed on top of recent advances in Multimodal Large Language Models (MLLMs), and the system is modulated into functional modules that can be scheduled and collaborated to ultimately solve pre-defined context- and process-dependent tasks.","Extensive experiments prove that MP5 can achieve a 22% success rate on difficult process-dependent tasks and a 91% success rate on tasks that heavily depend on the context.","Moreover, MP5 exhibits a remarkable ability to address many open-ended tasks that are entirely novel."],"url":"http://arxiv.org/abs/2312.07472v1"}
{"created":"2023-12-12 17:47:13","title":"Efficient Object Detection in Autonomous Driving using Spiking Neural Networks: Performance, Energy Consumption Analysis, and Insights into Open-set Object Discovery","abstract":"Besides performance, efficiency is a key design driver of technologies supporting vehicular perception. Indeed, a well-balanced trade-off between performance and energy consumption is crucial for the sustainability of autonomous vehicles. In this context, the diversity of real-world contexts in which autonomous vehicles can operate motivates the need for empowering perception models with the capability to detect, characterize and identify newly appearing objects by themselves. In this manuscript we elaborate on this threefold conundrum (performance, efficiency and open-world learning) for object detection modeling tasks over image data collected from vehicular scenarios. Specifically, we show that well-performing and efficient models can be realized by virtue of Spiking Neural Networks (SNNs), reaching competitive levels of detection performance when compared to their non-spiking counterparts at dramatic energy consumption savings (up to 85%) and a slightly improved robustness against image noise. Our experiments herein offered also expose qualitatively the complexity of detecting new objects based on the preliminary results of a simple approach to discriminate potential object proposals in the captured image.","sentences":["Besides performance, efficiency is a key design driver of technologies supporting vehicular perception.","Indeed, a well-balanced trade-off between performance and energy consumption is crucial for the sustainability of autonomous vehicles.","In this context, the diversity of real-world contexts in which autonomous vehicles can operate motivates the need for empowering perception models with the capability to detect, characterize and identify newly appearing objects by themselves.","In this manuscript we elaborate on this threefold conundrum (performance, efficiency and open-world learning) for object detection modeling tasks over image data collected from vehicular scenarios.","Specifically, we show that well-performing and efficient models can be realized by virtue of Spiking Neural Networks (SNNs), reaching competitive levels of detection performance when compared to their non-spiking counterparts at dramatic energy consumption savings (up to 85%) and a slightly improved robustness against image noise.","Our experiments herein offered also expose qualitatively the complexity of detecting new objects based on the preliminary results of a simple approach to discriminate potential object proposals in the captured image."],"url":"http://arxiv.org/abs/2312.07466v1"}
{"created":"2023-12-12 17:36:59","title":"Codesign of Humanoid Robots for Ergonomy Collaboration with Multiple Humans via Genetic Algorithms and Nonlinear Optimization","abstract":"Ergonomics is a key factor to consider when designing control architectures for effective physical collaborations between humans and humanoid robots. In contrast, ergonomic indexes are often overlooked in the robot design phase, which leads to suboptimal performance in physical human-robot interaction tasks. This paper proposes a novel methodology for optimizing the design of humanoid robots with respect to ergonomic indicators associated with the interaction of multiple agents. Our approach leverages a dynamic and kinematic parameterization of the robot link and motor specifications to seek for optimal robot designs using a bilevel optimization approach. Specifically, a genetic algorithm first generates robot designs by selecting the link and motor characteristics. Then, we use nonlinear optimization to evaluate interaction ergonomy indexes during collaborative payload lifting with different humans and weights. To assess the effectiveness of our approach, we compare the optimal design obtained using bilevel optimization against the design obtained using nonlinear optimization. Our results show that the proposed approach significantly improves ergonomics in terms of energy expenditure calculated in two reference scenarios involving static and dynamic robot motions. We plan to apply our methodology to drive the design of the ergoCub2 robot, a humanoid intended for optimal physical collaboration with humans in diverse environments","sentences":["Ergonomics is a key factor to consider when designing control architectures for effective physical collaborations between humans and humanoid robots.","In contrast, ergonomic indexes are often overlooked in the robot design phase, which leads to suboptimal performance in physical human-robot interaction tasks.","This paper proposes a novel methodology for optimizing the design of humanoid robots with respect to ergonomic indicators associated with the interaction of multiple agents.","Our approach leverages a dynamic and kinematic parameterization of the robot link and motor specifications to seek for optimal robot designs using a bilevel optimization approach.","Specifically, a genetic algorithm first generates robot designs by selecting the link and motor characteristics.","Then, we use nonlinear optimization to evaluate interaction ergonomy indexes during collaborative payload lifting with different humans and weights.","To assess the effectiveness of our approach, we compare the optimal design obtained using bilevel optimization against the design obtained using nonlinear optimization.","Our results show that the proposed approach significantly improves ergonomics in terms of energy expenditure calculated in two reference scenarios involving static and dynamic robot motions.","We plan to apply our methodology to drive the design of the ergoCub2 robot, a humanoid intended for optimal physical collaboration with humans in diverse environments"],"url":"http://arxiv.org/abs/2312.07459v1"}
{"created":"2023-12-12 17:34:42","title":"Dynamics Harmonic Analysis of Robotic Systems: Application in Data-Driven Koopman Modelling","abstract":"We introduce the use of harmonic analysis to decompose the state space of symmetric robotic systems into orthogonal isotypic subspaces. These are lower-dimensional spaces that capture distinct, symmetric, and synergistic motions. For linear dynamics, we characterize how this decomposition leads to a subdivision of the dynamics into independent linear systems on each subspace, a property we term dynamics harmonic analysis (DHA). To exploit this property, we use Koopman operator theory to propose an equivariant deep-learning architecture that leverages the properties of DHA to learn a global linear model of system dynamics. Our architecture, validated on synthetic systems and the dynamics of locomotion of a quadrupedal robot, demonstrates enhanced generalization, sample efficiency, and interpretability, with less trainable parameters and computational costs.","sentences":["We introduce the use of harmonic analysis to decompose the state space of symmetric robotic systems into orthogonal isotypic subspaces.","These are lower-dimensional spaces that capture distinct, symmetric, and synergistic motions.","For linear dynamics, we characterize how this decomposition leads to a subdivision of the dynamics into independent linear systems on each subspace, a property we term dynamics harmonic analysis (DHA).","To exploit this property, we use Koopman operator theory to propose an equivariant deep-learning architecture that leverages the properties of DHA to learn a global linear model of system dynamics.","Our architecture, validated on synthetic systems and the dynamics of locomotion of a quadrupedal robot, demonstrates enhanced generalization, sample efficiency, and interpretability, with less trainable parameters and computational costs."],"url":"http://arxiv.org/abs/2312.07457v1"}
{"created":"2023-12-12 17:31:32","title":"\"You Might Like It\": How People Respond to Small Talk in Human-Robot Collaboration","abstract":"In this work, we investigate people's engagement and attitudes towards a non-anthropomorphic robot manipulator that initiates small talk with the user during a collaborative assembly task, and explore how the presence of negative team feedback may affect team dynamics and blame attribution. Through an exploratory study with 20 participants, we found that 18 individuals interacted socially with the robot, nine of which initiated questions back to the robot. We report the frequency and length of users' responses in task-oriented and non-task-oriented dialogue, and further elaborate on people's reactions to the negative system feedback and robot-initiated small talk. We discuss the potential for integrating small talk in non-social robots, and propose three design guidelines to enhance human-robot small talk interactions.","sentences":["In this work, we investigate people's engagement and attitudes towards a non-anthropomorphic robot manipulator that initiates small talk with the user during a collaborative assembly task, and explore how the presence of negative team feedback may affect team dynamics and blame attribution.","Through an exploratory study with 20 participants, we found that 18 individuals interacted socially with the robot, nine of which initiated questions back to the robot.","We report the frequency and length of users' responses in task-oriented and non-task-oriented dialogue, and further elaborate on people's reactions to the negative system feedback and robot-initiated small talk.","We discuss the potential for integrating small talk in non-social robots, and propose three design guidelines to enhance human-robot small talk interactions."],"url":"http://arxiv.org/abs/2312.07454v1"}
{"created":"2023-12-12 17:23:38","title":"Daily Assistive View Control Learning of Low-Cost Low-Rigidity Robot via Large-Scale Vision-Language Model","abstract":"In this study, we develop a simple daily assistive robot that controls its own vision according to linguistic instructions. The robot performs several daily tasks such as recording a user's face, hands, or screen, and remotely capturing images of desired locations. To construct such a robot, we combine a pre-trained large-scale vision-language model with a low-cost low-rigidity robot arm. The correlation between the robot's physical and visual information is learned probabilistically using a neural network, and changes in the probability distribution based on changes in time and environment are considered by parametric bias, which is a learnable network input variable. We demonstrate the effectiveness of this learning method by open-vocabulary view control experiments with an actual robot arm, MyCobot.","sentences":["In this study, we develop a simple daily assistive robot that controls its own vision according to linguistic instructions.","The robot performs several daily tasks such as recording a user's face, hands, or screen, and remotely capturing images of desired locations.","To construct such a robot, we combine a pre-trained large-scale vision-language model with a low-cost low-rigidity robot arm.","The correlation between the robot's physical and visual information is learned probabilistically using a neural network, and changes in the probability distribution based on changes in time and environment are considered by parametric bias, which is a learnable network input variable.","We demonstrate the effectiveness of this learning method by open-vocabulary view control experiments with an actual robot arm, MyCobot."],"url":"http://arxiv.org/abs/2312.07451v1"}
{"created":"2023-12-12 17:06:39","title":"BIRB: A Generalization Benchmark for Information Retrieval in Bioacoustics","abstract":"The ability for a machine learning model to cope with differences in training and deployment conditions--e.g. in the presence of distribution shift or the generalization to new classes altogether--is crucial for real-world use cases. However, most empirical work in this area has focused on the image domain with artificial benchmarks constructed to measure individual aspects of generalization. We present BIRB, a complex benchmark centered on the retrieval of bird vocalizations from passively-recorded datasets given focal recordings from a large citizen science corpus available for training. We propose a baseline system for this collection of tasks using representation learning and a nearest-centroid search. Our thorough empirical evaluation and analysis surfaces open research directions, suggesting that BIRB fills the need for a more realistic and complex benchmark to drive progress on robustness to distribution shifts and generalization of ML models.","sentences":["The ability for a machine learning model to cope with differences in training and deployment conditions--e.g. in the presence of distribution shift or the generalization to new classes altogether--is crucial for real-world use cases.","However, most empirical work in this area has focused on the image domain with artificial benchmarks constructed to measure individual aspects of generalization.","We present BIRB, a complex benchmark centered on the retrieval of bird vocalizations from passively-recorded datasets given focal recordings from a large citizen science corpus available for training.","We propose a baseline system for this collection of tasks using representation learning and a nearest-centroid search.","Our thorough empirical evaluation and analysis surfaces open research directions, suggesting that BIRB fills the need for a more realistic and complex benchmark to drive progress on robustness to distribution shifts and generalization of ML models."],"url":"http://arxiv.org/abs/2312.07439v1"}
{"created":"2023-12-12 17:04:26","title":"Medical Image Classification Using Transfer Learning and Chaos Game Optimization on the Internet of Medical Things","abstract":"The Internet of Medical Things (IoMT) has dramatically benefited medical professionals that patients and physicians can access from all regions. Although the automatic detection and prediction of diseases such as melanoma and leukemia is still being researched and studied in IoMT, existing approaches are not able to achieve a high degree of efficiency. Thus, with a new approach that provides better results, patients would access the adequate treatments earlier and the death rate would be reduced. Therefore, this paper introduces an IoMT proposal for medical images classification that may be used anywhere, i.e. it is an ubiquitous approach. It was design in two stages: first, we employ a Transfer Learning (TL)-based method for feature extraction, which is carried out using MobileNetV3; second, we use the Chaos Game Optimization (CGO) for feature selection, with the aim of excluding unnecessary features and improving the performance, which is key in IoMT. Our methodology was evaluated using ISIC-2016, PH2, and Blood-Cell datasets. The experimental results indicated that the proposed approach obtained an accuracy of 88.39% on ISIC-2016, 97.52% on PH2, and 88.79% on Blood-cell. Moreover, our approach had successful performances for the metrics employed compared to other existing methods.","sentences":["The Internet of Medical Things (IoMT) has dramatically benefited medical professionals that patients and physicians can access from all regions.","Although the automatic detection and prediction of diseases such as melanoma and leukemia is still being researched and studied in IoMT, existing approaches are not able to achieve a high degree of efficiency.","Thus, with a new approach that provides better results, patients would access the adequate treatments earlier and the death rate would be reduced.","Therefore, this paper introduces an IoMT proposal for medical images classification that may be used anywhere, i.e. it is an ubiquitous approach.","It was design in two stages: first, we employ a Transfer Learning (TL)-based method for feature extraction, which is carried out using MobileNetV3; second, we use the Chaos Game Optimization (CGO) for feature selection, with the aim of excluding unnecessary features and improving the performance, which is key in IoMT.","Our methodology was evaluated using ISIC-2016, PH2, and Blood-Cell datasets.","The experimental results indicated that the proposed approach obtained an accuracy of 88.39% on ISIC-2016, 97.52% on PH2, and 88.79% on Blood-cell.","Moreover, our approach had successful performances for the metrics employed compared to other existing methods."],"url":"http://arxiv.org/abs/2312.07437v1"}
{"created":"2023-12-12 17:00:46","title":"Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval","abstract":"Video moment retrieval is a challenging task requiring fine-grained interactions between video and text modalities. Recent work in image-text pretraining has demonstrated that most existing pretrained models suffer from information asymmetry due to the difference in length between visual and textual sequences. We question whether the same problem also exists in the video-text domain with an auxiliary need to preserve both spatial and temporal information. Thus, we evaluate a recently proposed solution involving the addition of an asymmetric co-attention network for video grounding tasks. Additionally, we incorporate momentum contrastive loss for robust, discriminative representation learning in both modalities. We note that the integration of these supplementary modules yields better performance compared to state-of-the-art models on the TACoS dataset and comparable results on ActivityNet Captions, all while utilizing significantly fewer parameters with respect to baseline.","sentences":["Video moment retrieval is a challenging task requiring fine-grained interactions between video and text modalities.","Recent work in image-text pretraining has demonstrated that most existing pretrained models suffer from information asymmetry due to the difference in length between visual and textual sequences.","We question whether the same problem also exists in the video-text domain with an auxiliary need to preserve both spatial and temporal information.","Thus, we evaluate a recently proposed solution involving the addition of an asymmetric co-attention network for video grounding tasks.","Additionally, we incorporate momentum contrastive loss for robust, discriminative representation learning in both modalities.","We note that the integration of these supplementary modules yields better performance compared to state-of-the-art models on the TACoS dataset and comparable results on ActivityNet Captions, all while utilizing significantly fewer parameters with respect to baseline."],"url":"http://arxiv.org/abs/2312.07435v1"}
{"created":"2023-12-12 17:00:13","title":"Multi-Modal Conformal Prediction Regions by Optimizing Convex Shape Templates","abstract":"Conformal prediction is a statistical tool for producing prediction regions for machine learning models that are valid with high probability. A key component of conformal prediction algorithms is a non-conformity score function that quantifies how different a model's prediction is from the unknown ground truth value. Essentially, these functions determine the shape and the size of the conformal prediction regions. However, little work has gone into finding non-conformity score functions that produce prediction regions that are multi-modal and practical, i.e., that can efficiently be used in engineering applications. We propose a method that optimizes parameterized shape template functions over calibration data, which results in non-conformity score functions that produce prediction regions with minimum volume. Our approach results in prediction regions that are multi-modal, so they can properly capture residuals of distributions that have multiple modes, and practical, so each region is convex and can be easily incorporated into downstream tasks, such as a motion planner using conformal prediction regions. Our method applies to general supervised learning tasks, while we illustrate its use in time-series prediction. We provide a toolbox and present illustrative case studies of F16 fighter jets and autonomous vehicles, showing an up to $68\\%$ reduction in prediction region area.","sentences":["Conformal prediction is a statistical tool for producing prediction regions for machine learning models that are valid with high probability.","A key component of conformal prediction algorithms is a non-conformity score function that quantifies how different a model's prediction is from the unknown ground truth value.","Essentially, these functions determine the shape and the size of the conformal prediction regions.","However, little work has gone into finding non-conformity score functions that produce prediction regions that are multi-modal and practical, i.e., that can efficiently be used in engineering applications.","We propose a method that optimizes parameterized shape template functions over calibration data, which results in non-conformity score functions that produce prediction regions with minimum volume.","Our approach results in prediction regions that are multi-modal, so they can properly capture residuals of distributions that have multiple modes, and practical, so each region is convex and can be easily incorporated into downstream tasks, such as a motion planner using conformal prediction regions.","Our method applies to general supervised learning tasks, while we illustrate its use in time-series prediction.","We provide a toolbox and present illustrative case studies of F16 fighter jets and autonomous vehicles, showing an up to $68\\%$ reduction in prediction region area."],"url":"http://arxiv.org/abs/2312.07434v1"}
{"created":"2023-12-12 16:57:01","title":"Algorithms and Complexity for Congested Assignments","abstract":"We study the congested assignment problem as introduced by Bogomolnaia and Moulin (2023). We show that deciding whether a competitive assignment exists can be done in polynomial time, while deciding whether an envy-free assignment exists is NP-complete.","sentences":["We study the congested assignment problem as introduced by Bogomolnaia and Moulin (2023).","We show that deciding whether a competitive assignment exists can be done in polynomial time, while deciding whether an envy-free assignment exists is NP-complete."],"url":"http://arxiv.org/abs/2312.07431v1"}
{"created":"2023-12-12 16:53:18","title":"Ensemble Federated Learning: an approach for collaborative pneumonia diagnosis","abstract":"Federated learning is a very convenient approach for scenarios where (i) the exchange of data implies privacy concerns and/or (ii) a quick reaction is needed. In smart healthcare systems, both aspects are usually required. In this paper, we work on the first scenario, where preserving privacy is key and, consequently, building a unique and massive medical image data set by fusing different data sets from different medical institutions or research centers (computation nodes) is not an option. We propose an ensemble federated learning (EFL) approach that is based on the following characteristics: First, each computation node works with a different data set (but of the same type). They work locally and apply an ensemble approach combining eight well-known CNN models (densenet169, mobilenetv2, xception, inceptionv3, vgg16, resnet50, densenet121, and resnet152v2) on Chest X-ray images. Second, the best two local models are used to create a local ensemble model that is shared with a central node. Third, the ensemble models are aggregated to obtain a global model, which is shared with the computation nodes to continue with a new iteration. This procedure continues until there are no changes in the best local models. We have performed different experiments to compare our approach with centralized ones (with or without an ensemble approach)\\color{black}. The results conclude that our proposal outperforms these ones in Chest X-ray images (achieving an accuracy of 96.63\\%) and offers very competitive results compared to other proposals in the literature.","sentences":["Federated learning is a very convenient approach for scenarios where (i) the exchange of data implies privacy concerns and/or (ii) a quick reaction is needed.","In smart healthcare systems, both aspects are usually required.","In this paper, we work on the first scenario, where preserving privacy is key and, consequently, building a unique and massive medical image data set by fusing different data sets from different medical institutions or research centers (computation nodes) is not an option.","We propose an ensemble federated learning (EFL) approach that is based on the following characteristics:","First, each computation node works with a different data set (but of the same type).","They work locally and apply an ensemble approach combining eight well-known CNN models (densenet169, mobilenetv2, xception, inceptionv3, vgg16, resnet50, densenet121, and resnet152v2) on Chest X-ray images.","Second, the best two local models are used to create a local ensemble model that is shared with a central node.","Third, the ensemble models are aggregated to obtain a global model, which is shared with the computation nodes to continue with a new iteration.","This procedure continues until there are no changes in the best local models.","We have performed different experiments to compare our approach with centralized ones (with or without an ensemble approach)\\color{black}.","The results conclude that our proposal outperforms these ones in Chest X-ray images (achieving an accuracy of 96.63\\%) and offers very competitive results compared to other proposals in the literature."],"url":"http://arxiv.org/abs/2312.07428v1"}
{"created":"2023-12-12 16:48:53","title":"Deep Internal Learning: Deep Learning from a Single Input","abstract":"Deep learning in general focuses on training a neural network from large labeled datasets. Yet, in many cases there is value in training a network just from the input at hand. This may involve training a network from scratch using a single input or adapting an already trained network to a provided input example at inference time. This survey paper aims at covering deep internal-learning techniques that have been proposed in the past few years for these two important directions. While our main focus will be on image processing problems, most of the approaches that we survey are derived for general signals (vectors with recurring patterns that can be distinguished from noise) and are therefore applicable to other modalities. We believe that the topic of internal-learning is very important in many signal and image processing problems where training data is scarce and diversity is large on the one hand, and on the other, there is a lot of structure in the data that can be exploited.","sentences":["Deep learning in general focuses on training a neural network from large labeled datasets.","Yet, in many cases there is value in training a network just from the input at hand.","This may involve training a network from scratch using a single input or adapting an already trained network to a provided input example at inference time.","This survey paper aims at covering deep internal-learning techniques that have been proposed in the past few years for these two important directions.","While our main focus will be on image processing problems, most of the approaches that we survey are derived for general signals (vectors with recurring patterns that can be distinguished from noise) and are therefore applicable to other modalities.","We believe that the topic of internal-learning is very important in many signal and image processing problems where training data is scarce and diversity is large on the one hand, and on the other, there is a lot of structure in the data that can be exploited."],"url":"http://arxiv.org/abs/2312.07425v1"}
{"created":"2023-12-12 16:48:07","title":"How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation","abstract":"In machine learning, generalization against distribution shifts -- where deployment conditions diverge from the training scenarios -- is crucial, particularly in fields like climate modeling, biomedicine, and autonomous driving. The emergence of foundation models, distinguished by their extensive pretraining and task versatility, has led to an increased interest in their adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced publicly accessible multimodal foundation model, with extensive applications across various domains, including anomaly detection, video understanding, image generation, and medical diagnosis. However, its robustness against data distributions remains largely underexplored. Addressing this gap, this study rigorously evaluates GPT-4V's adaptability and generalization capabilities in dynamic environments, benchmarking against prominent models like CLIP and LLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse datasets spanning natural, medical, and molecular domains. We further investigate its adaptability to controlled data perturbations and examine the efficacy of in-context learning as a tool to enhance its adaptation. Our findings delineate GPT-4V's capability boundaries in distribution shifts, shedding light on its strengths and limitations across various scenarios. Importantly, this investigation contributes to our understanding of how AI foundation models generalize to distribution shifts, offering pivotal insights into their adaptability and robustness. Code is publicly available at https://github.com/jameszhou-gl/gpt-4v-distribution-shift.","sentences":["In machine learning, generalization against distribution shifts -- where deployment conditions diverge from the training scenarios -- is crucial, particularly in fields like climate modeling, biomedicine, and autonomous driving.","The emergence of foundation models, distinguished by their extensive pretraining and task versatility, has led to an increased interest in their adaptability to distribution shifts.","GPT-4V(ision) acts as the most advanced publicly accessible multimodal foundation model, with extensive applications across various domains, including anomaly detection, video understanding, image generation, and medical diagnosis.","However, its robustness against data distributions remains largely underexplored.","Addressing this gap, this study rigorously evaluates GPT-4V's adaptability and generalization capabilities in dynamic environments, benchmarking against prominent models like CLIP and LLaVA.","We delve into GPT-4V's zero-shot generalization across 13 diverse datasets spanning natural, medical, and molecular domains.","We further investigate its adaptability to controlled data perturbations and examine the efficacy of in-context learning as a tool to enhance its adaptation.","Our findings delineate GPT-4V's capability boundaries in distribution shifts, shedding light on its strengths and limitations across various scenarios.","Importantly, this investigation contributes to our understanding of how AI foundation models generalize to distribution shifts, offering pivotal insights into their adaptability and robustness.","Code is publicly available at https://github.com/jameszhou-gl/gpt-4v-distribution-shift."],"url":"http://arxiv.org/abs/2312.07424v1"}
{"created":"2023-12-12 16:45:52","title":"Holoported Characters: Real-time Free-viewpoint Rendering of Humans from Sparse RGB Cameras","abstract":"We present the first approach to render highly realistic free-viewpoint videos of a human actor in general apparel, from sparse multi-view recording to display, in real-time at an unprecedented 4K resolution. At inference, our method only requires four camera views of the moving actor and the respective 3D skeletal pose. It handles actors in wide clothing, and reproduces even fine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and hand gestures. At training time, our learning-based approach expects dense multi-view video and a rigged static surface scan of the actor. Our method comprises three main stages. Stage 1 is a skeleton-driven neural approach for high-quality capture of the detailed dynamic mesh geometry. Stage 2 is a novel solution to create a view-dependent texture using four test-time camera views as input. Finally, stage 3 comprises a new image-based refinement network rendering the final 4K image given the output from the previous stages. Our approach establishes a new benchmark for real-time rendering resolution and quality using sparse input camera views, unlocking possibilities for immersive telepresence.","sentences":["We present the first approach to render highly realistic free-viewpoint videos of a human actor in general apparel, from sparse multi-view recording to display, in real-time at an unprecedented 4K resolution.","At inference, our method only requires four camera views of the moving actor and the respective 3D skeletal pose.","It handles actors in wide clothing, and reproduces even fine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and hand gestures.","At training time, our learning-based approach expects dense multi-view video and a rigged static surface scan of the actor.","Our method comprises three main stages.","Stage 1 is a skeleton-driven neural approach for high-quality capture of the detailed dynamic mesh geometry.","Stage 2 is a novel solution to create a view-dependent texture using four test-time camera views as input.","Finally, stage 3 comprises a new image-based refinement network rendering the final 4K image given the output from the previous stages.","Our approach establishes a new benchmark for real-time rendering resolution and quality using sparse input camera views, unlocking possibilities for immersive telepresence."],"url":"http://arxiv.org/abs/2312.07423v1"}
{"created":"2023-12-12 16:44:47","title":"FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs","abstract":"Training large language models (LLMs) is a costly endeavour in terms of time and computational resources. The large amount of training data used during the unsupervised pre-training phase makes it difficult to verify all data and, unfortunately, undesirable data may be ingested during training. Re-training from scratch is impractical and has led to the creation of the 'unlearning' discipline where models are modified to \"unlearn\" undesirable information without retraining. However, any modification can alter the behaviour of LLMs, especially on key dimensions such as fairness. This is the first work that examines this interplay between unlearning and fairness for LLMs. In particular, we focus on a popular unlearning framework known as SISA [Bourtoule et al., 2021], which creates an ensemble of models trained on disjoint shards. We evaluate the performance-fairness trade-off for SISA, and empirically demsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we propose post-processing bias mitigation techniques for ensemble models produced by SISA. We adapt the post-processing fairness improvement technique from [Hardt et al., 2016] to design three methods that can handle model ensembles, and prove that one of the methods is an optimal fair predictor for ensemble of models. Through experimental results, we demonstrate the efficacy of our post-processing framework called 'FairSISA'.","sentences":["Training large language models (LLMs) is a costly endeavour in terms of time and computational resources.","The large amount of training data used during the unsupervised pre-training phase makes it difficult to verify all data and, unfortunately, undesirable data may be ingested during training.","Re-training from scratch is impractical and has led to the creation of the 'unlearning' discipline where models are modified to \"unlearn\" undesirable information without retraining.","However, any modification can alter the behaviour of LLMs, especially on key dimensions such as fairness.","This is the first work that examines this interplay between unlearning and fairness for LLMs.","In particular, we focus on a popular unlearning framework known as SISA [Bourtoule et al., 2021], which creates an ensemble of models trained on disjoint shards.","We evaluate the performance-fairness trade-off for SISA, and empirically demsontrate that SISA can indeed reduce fairness in LLMs.","To remedy this, we propose post-processing bias mitigation techniques for ensemble models produced by SISA.","We adapt the post-processing fairness improvement technique from [Hardt et al., 2016] to design three methods that can handle model ensembles, and prove that one of the methods is an optimal fair predictor for ensemble of models.","Through experimental results, we demonstrate the efficacy of our post-processing framework called 'FairSISA'."],"url":"http://arxiv.org/abs/2312.07420v1"}
{"created":"2023-12-12 16:41:29","title":"Towards Faster k-Nearest-Neighbor Machine Translation","abstract":"Recent works have proven the effectiveness of k-nearest-neighbor machine translation(a.k.a kNN-MT) approaches to produce remarkable improvement in cross-domain translations. However, these models suffer from heavy retrieve overhead on the entire datastore when decoding each token. We observe that during the decoding phase, about 67% to 84% of tokens are unvaried after searching over the corpus datastore, which means most of the tokens cause futile retrievals and introduce unnecessary computational costs by initiating k-nearest-neighbor searches. We consider this phenomenon is explainable in linguistics and propose a simple yet effective multi-layer perceptron (MLP) network to predict whether a token should be translated jointly by the neural machine translation model and probabilities produced by the kNN or just by the neural model. The results show that our method succeeds in reducing redundant retrieval operations and significantly reduces the overhead of kNN retrievals by up to 53% at the expense of a slight decline in translation quality. Moreover, our method could work together with all existing kNN-MT systems.","sentences":["Recent works have proven the effectiveness of k-nearest-neighbor machine translation(a.k.a","kNN-MT) approaches to produce remarkable improvement in cross-domain translations.","However, these models suffer from heavy retrieve overhead on the entire datastore when decoding each token.","We observe that during the decoding phase, about 67% to 84% of tokens are unvaried after searching over the corpus datastore, which means most of the tokens cause futile retrievals and introduce unnecessary computational costs by initiating k-nearest-neighbor searches.","We consider this phenomenon is explainable in linguistics and propose a simple yet effective multi-layer perceptron (MLP) network to predict whether a token should be translated jointly by the neural machine translation model and probabilities produced by the kNN or just by the neural model.","The results show that our method succeeds in reducing redundant retrieval operations and significantly reduces the overhead of kNN retrievals by up to 53% at the expense of a slight decline in translation quality.","Moreover, our method could work together with all existing kNN-MT systems."],"url":"http://arxiv.org/abs/2312.07419v1"}
{"created":"2023-12-12 16:39:12","title":"Attention Based Encoder Decoder Model for Video Captioning in Nepali (2023)","abstract":"Video captioning in Nepali, a language written in the Devanagari script, presents a unique challenge due to the lack of existing academic work in this domain. This work develops a novel encoder-decoder paradigm for Nepali video captioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models are used in the model to produce related textual descriptions based on features retrieved from video frames using CNNs. Using Google Translate and manual post-editing, a Nepali video captioning dataset is generated from the Microsoft Research Video Description Corpus (MSVD) dataset created using Google Translate, and manual post-editing work. The efficacy of the model for Devanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE measures, which are used to assess its performance.","sentences":["Video captioning in Nepali, a language written in the Devanagari script, presents a unique challenge due to the lack of existing academic work in this domain.","This work develops a novel encoder-decoder paradigm for Nepali video captioning to tackle this difficulty.","LSTM and GRU sequence-to-sequence models are used in the model to produce related textual descriptions based on features retrieved from video frames using CNNs.","Using Google Translate and manual post-editing, a Nepali video captioning dataset is generated from the Microsoft Research Video Description Corpus (MSVD) dataset created using Google Translate, and manual post-editing work.","The efficacy of the model for Devanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE measures, which are used to assess its performance."],"url":"http://arxiv.org/abs/2312.07418v1"}
{"created":"2023-12-12 16:34:32","title":"QSMVM: QoS-aware and social-aware multimetric routing protocol for video-streaming services over MANETs","abstract":"A mobile ad hoc network (MANET) is a set of autonomous mobile devices connected by wireless links in a distributed manner and without a fixed infrastructure. Real-time multimedia services, such as video-streaming over MANETs, offers very promising applications, e.g. two members of a group of tourists who want to share a video transmitted through the MANET they form; a video-streaming service deployed over a MANET where users watch a film; among other examples. On the other hand, social web technologies, where people actively interact online with others through social networks, are leading to a socialization of networks. Information of interaction among users is being used to provide socially-enhanced software. To achieve this, we need to know the strength of the relationship between a given user and each user they interact with. This strength of the relationship can be measured through a concept called tie strength (TS), first introduced by Mark Granovetter in 1973. In this article, we modify our previous proposal named multipath multimedia dynamic source routing (MMDSR) protocol to include a social metric TS in the decisions taken by the forwarding algorithm. We find a trade-off between the quality of service (QoS) and the trust level between users who form the forwarding path in the MANET. Our goal is to increase the trust metric while the QoS is not affected significantly.","sentences":["A mobile ad hoc network (MANET) is a set of autonomous mobile devices connected by wireless links in a distributed manner and without a fixed infrastructure.","Real-time multimedia services, such as video-streaming over MANETs, offers very promising applications, e.g. two members of a group of tourists who want to share a video transmitted through the MANET they form; a video-streaming service deployed over a MANET where users watch a film; among other examples.","On the other hand, social web technologies, where people actively interact online with others through social networks, are leading to a socialization of networks.","Information of interaction among users is being used to provide socially-enhanced software.","To achieve this, we need to know the strength of the relationship between a given user and each user they interact with.","This strength of the relationship can be measured through a concept called tie strength (TS), first introduced by Mark Granovetter in 1973.","In this article, we modify our previous proposal named multipath multimedia dynamic source routing (MMDSR) protocol to include a social metric TS in the decisions taken by the forwarding algorithm.","We find a trade-off between the quality of service (QoS) and the trust level between users who form the forwarding path in the MANET.","Our goal is to increase the trust metric while the QoS is not affected significantly."],"url":"http://arxiv.org/abs/2312.07414v1"}
{"created":"2023-12-12 16:34:19","title":"AI capabilities can be significantly improved without expensive retraining","abstract":"State-of-the-art AI systems can be significantly improved without expensive retraining via \"post-training enhancements\"-techniques applied after initial training like fine-tuning the system to use a web browser. We review recent post-training enhancements, categorizing them into five types: tool-use, prompting methods, scaffolding, solution selection, and data generation. Different enhancements improve performance on different tasks, making it hard to compare their significance. So we translate improvements from different enhancements into a common currency, the compute-equivalent gain: how much additional training compute would be needed to improve performance by the same amount as the enhancement. Our non-experimental work shows that post-training enhancements have significant benefits: most surveyed enhancements improve benchmark performance by more than a 5x increase in training compute, some by more than 20x. Post-training enhancements are relatively cheap to develop: fine-tuning costs are typically <1% of the original training cost. Governing the development of capable post-training enhancements may be challenging because frontier models could be enhanced by a wide range of actors.","sentences":["State-of-the-art AI systems can be significantly improved without expensive retraining via \"post-training enhancements\"-techniques applied after initial training like fine-tuning the system to use a web browser.","We review recent post-training enhancements, categorizing them into five types: tool-use, prompting methods, scaffolding, solution selection, and data generation.","Different enhancements improve performance on different tasks, making it hard to compare their significance.","So we translate improvements from different enhancements into a common currency, the compute-equivalent gain: how much additional training compute would be needed to improve performance by the same amount as the enhancement.","Our non-experimental work shows that post-training enhancements have significant benefits: most surveyed enhancements improve benchmark performance by more than a 5x increase in training compute, some by more than 20x.","Post-training enhancements are relatively cheap to develop: fine-tuning costs are typically <1% of the original training cost.","Governing the development of capable post-training enhancements may be challenging because frontier models could be enhanced by a wide range of actors."],"url":"http://arxiv.org/abs/2312.07413v1"}
{"created":"2023-12-12 16:28:08","title":"DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing","abstract":"Diffusion models have achieved remarkable image generation quality surpassing previous generative models. However, a notable limitation of diffusion models, in comparison to GANs, is their difficulty in smoothly interpolating between two image samples, due to their highly unstructured latent space. Such a smooth interpolation is intriguing as it naturally serves as a solution for the image morphing task with many applications. In this work, we present DiffMorpher, the first approach enabling smooth and natural image interpolation using diffusion models. Our key idea is to capture the semantics of the two images by fitting two LoRAs to them respectively, and interpolate between both the LoRA parameters and the latent noises to ensure a smooth semantic transition, where correspondence automatically emerges without the need for annotation. In addition, we propose an attention interpolation and injection technique and a new sampling schedule to further enhance the smoothness between consecutive images. Extensive experiments demonstrate that DiffMorpher achieves starkly better image morphing effects than previous methods across a variety of object categories, bridging a critical functional gap that distinguished diffusion models from GANs.","sentences":["Diffusion models have achieved remarkable image generation quality surpassing previous generative models.","However, a notable limitation of diffusion models, in comparison to GANs, is their difficulty in smoothly interpolating between two image samples, due to their highly unstructured latent space.","Such a smooth interpolation is intriguing as it naturally serves as a solution for the image morphing task with many applications.","In this work, we present DiffMorpher, the first approach enabling smooth and natural image interpolation using diffusion models.","Our key idea is to capture the semantics of the two images by fitting two LoRAs to them respectively, and interpolate between both the LoRA parameters and the latent noises to ensure a smooth semantic transition, where correspondence automatically emerges without the need for annotation.","In addition, we propose an attention interpolation and injection technique and a new sampling schedule to further enhance the smoothness between consecutive images.","Extensive experiments demonstrate that DiffMorpher achieves starkly better image morphing effects than previous methods across a variety of object categories, bridging a critical functional gap that distinguished diffusion models from GANs."],"url":"http://arxiv.org/abs/2312.07409v1"}
{"created":"2023-12-12 16:27:35","title":"Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Models","abstract":"Vision-Language Large Models (VLMs) have become primary backbone of AI, due to the impressive performance. However, their expensive computation costs, i.e., throughput and delay, impede potentials in real-world scenarios. To achieve acceleration for VLMs, most existing methods focus on the model perspective: pruning, distillation, quantification, but completely overlook the data-perspective redundancy. To fill the overlook, this paper pioneers the severity of data redundancy, and designs one plug-and-play Turbo module guided by information degree to prune inefficient tokens from visual or textual data. In pursuit of efficiency-performance trade-offs, information degree takes two key factors into consideration: mutual redundancy and semantic value. Concretely, the former evaluates the data duplication between sequential tokens; while the latter evaluates each token by its contribution to the overall semantics. As a result, tokens with high information degree carry less redundancy and stronger semantics. For VLMs' calculation, Turbo works as a user-friendly plug-in that sorts data referring to information degree, utilizing only top-level ones to save costs. Its advantages are multifaceted, e.g., being generally compatible to various VLMs across understanding and generation, simple use without retraining and trivial engineering efforts. On multiple public VLMs benchmarks, we conduct extensive experiments to reveal the gratifying acceleration of Turbo, under negligible performance drop.","sentences":["Vision-Language Large Models (VLMs) have become primary backbone of AI, due to the impressive performance.","However, their expensive computation costs, i.e., throughput and delay, impede potentials in real-world scenarios.","To achieve acceleration for VLMs, most existing methods focus on the model perspective: pruning, distillation, quantification, but completely overlook the data-perspective redundancy.","To fill the overlook, this paper pioneers the severity of data redundancy, and designs one plug-and-play Turbo module guided by information degree to prune inefficient tokens from visual or textual data.","In pursuit of efficiency-performance trade-offs, information degree takes two key factors into consideration: mutual redundancy and semantic value.","Concretely, the former evaluates the data duplication between sequential tokens; while the latter evaluates each token by its contribution to the overall semantics.","As a result, tokens with high information degree carry less redundancy and stronger semantics.","For VLMs' calculation, Turbo works as a user-friendly plug-in that sorts data referring to information degree, utilizing only top-level ones to save costs.","Its advantages are multifaceted, e.g., being generally compatible to various VLMs across understanding and generation, simple use without retraining and trivial engineering efforts.","On multiple public VLMs benchmarks, we conduct extensive experiments to reveal the gratifying acceleration of Turbo, under negligible performance drop."],"url":"http://arxiv.org/abs/2312.07408v1"}
{"created":"2023-12-12 16:25:05","title":"ICL Markup: Structuring In-Context Learning using Soft-Token Tags","abstract":"Large pretrained language models (LLMs) can be rapidly adapted to a wide variety of tasks via a text-to-text approach, where the instruction and input are fed to the model in natural language. Combined with in-context learning (ICL), this paradigm is impressively flexible and powerful. However, it also burdens users with an overwhelming number of choices, many of them arbitrary. Inspired by markup languages like HTML, we contribute a method of using soft-token tags to compose prompt templates. This approach reduces arbitrary decisions and streamlines the application of ICL. Our method is a form of meta-learning for ICL; it learns these tags in advance during a parameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently be used in templates for ICL on new, unseen tasks without any additional fine-tuning. Our experiments with this approach yield promising initial results, improving LLM performance on important enterprise applications such as few-shot and open-world intent detection, as well as text classification in news and legal domains.","sentences":["Large pretrained language models (LLMs) can be rapidly adapted to a wide variety of tasks via a text-to-text approach, where the instruction and input are fed to the model in natural language.","Combined with in-context learning (ICL), this paradigm is impressively flexible and powerful.","However, it also burdens users with an overwhelming number of choices, many of them arbitrary.","Inspired by markup languages like HTML, we contribute a method of using soft-token tags to compose prompt templates.","This approach reduces arbitrary decisions and streamlines the application of ICL.","Our method is a form of meta-learning for ICL; it learns these tags in advance during a parameter-efficient fine-tuning ``warm-up'' process.","The tags can subsequently be used in templates for ICL on new, unseen tasks without any additional fine-tuning.","Our experiments with this approach yield promising initial results, improving LLM performance on important enterprise applications such as few-shot and open-world intent detection, as well as text classification in news and legal domains."],"url":"http://arxiv.org/abs/2312.07405v1"}
{"created":"2023-12-12 16:17:15","title":"On Diverse Preferences for Large Language Model Alignment","abstract":"The alignment of large language models (LLMs) with human values is crucial for the development of artificial general intelligence (AGI). One promising approach to achieve this alignment is reinforcement learning from human feedback, which employs a reward model (RM) learned from human preference datasets to guide LLMs in generating text that aligns with human preferences. Through intensive experiments and analysis of reward distribution, this paper finds that preference datasets are diverse from each other, even though they are all proposed to align human preference. Hence, mixing diverse human preference datasets to increase data size for enhancing reward modeling could fail. To address the issue and capture the shared human values from diverse preferences, a new training policy called MORE is introduced, which minimizes preference bias by adaptively adjusting the preference objective across diverse preferences. Experiments with the Pythia-1.4B model and five mixed preference datasets show that MORE achieves superior reward accuracy and lower calibration error, highlighting its ability to leverage diverse human preference data.","sentences":["The alignment of large language models (LLMs) with human values is crucial for the development of artificial general intelligence (AGI).","One promising approach to achieve this alignment is reinforcement learning from human feedback, which employs a reward model (RM) learned from human preference datasets to guide LLMs in generating text that aligns with human preferences.","Through intensive experiments and analysis of reward distribution, this paper finds that preference datasets are diverse from each other, even though they are all proposed to align human preference.","Hence, mixing diverse human preference datasets to increase data size for enhancing reward modeling could fail.","To address the issue and capture the shared human values from diverse preferences, a new training policy called MORE is introduced, which minimizes preference bias by adaptively adjusting the preference objective across diverse preferences.","Experiments with the Pythia-1.4B model and five mixed preference datasets show that MORE achieves superior reward accuracy and lower calibration error, highlighting its ability to leverage diverse human preference data."],"url":"http://arxiv.org/abs/2312.07401v1"}
{"created":"2023-12-12 16:14:45","title":"Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales","abstract":"Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a ``reasoning-aware'' diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various settings. We further propose a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, facilitating and benefiting future research in this area.","sentences":["Machine reasoning has made great progress in recent years owing to large language models (LLMs).","In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians.","In this work, we present a ``reasoning-aware'' diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales.","Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT).","We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various settings.","We further propose a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, facilitating and benefiting future research in this area."],"url":"http://arxiv.org/abs/2312.07399v1"}
{"created":"2023-12-12 16:14:43","title":"LLMEval: A Preliminary Study on How to Evaluate Large Language Models","abstract":"Recently, the evaluation of Large Language Models has emerged as a popular area of research. The three crucial questions for LLM evaluation are ``what, where, and how to evaluate''. However, the existing research mainly focuses on the first two questions, which are basically what tasks to give the LLM during testing and what kind of knowledge it should deal with. As for the third question, which is about what standards to use, the types of evaluators, how to score, and how to rank, there hasn't been much discussion. In this paper, we analyze evaluation methods by comparing various criteria with both manual and automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and GPT-4, with different scoring methods and ranking systems. We propose a new dataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186 individuals participated, leading to the generation of 243,337 manual annotations and 57,511 automatic evaluation results. We perform comparisons and analyses of different settings and conduct 10 conclusions that can provide some insights for evaluating LLM in the future. The dataset and the results are publicly available at https://github.com/llmeval .","sentences":["Recently, the evaluation of Large Language Models has emerged as a popular area of research.","The three crucial questions for LLM evaluation are ``what, where, and how to evaluate''.","However, the existing research mainly focuses on the first two questions, which are basically what tasks to give the LLM during testing and what kind of knowledge it should deal with.","As for the third question, which is about what standards to use, the types of evaluators, how to score, and how to rank, there hasn't been much discussion.","In this paper, we analyze evaluation methods by comparing various criteria with both manual and automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and GPT-4, with different scoring methods and ranking systems.","We propose a new dataset, LLMEval and conduct evaluations on 20 LLMs.","A total of 2,186 individuals participated, leading to the generation of 243,337 manual annotations and 57,511 automatic evaluation results.","We perform comparisons and analyses of different settings and conduct 10 conclusions that can provide some insights for evaluating LLM in the future.","The dataset and the results are publicly available at https://github.com/llmeval ."],"url":"http://arxiv.org/abs/2312.07398v1"}
{"created":"2023-12-12 16:10:19","title":"A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames","abstract":"Understanding long, real-world videos requires modeling of long-range visual dependencies. To this end, we explore video-first architectures, building on the common paradigm of transferring large-scale, image--text models to video via shallow temporal fusion. However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to poor video--language alignment in standard video datasets, and (2) higher memory consumption, bottlenecking the number of frames that can be processed. To mitigate the memory bottleneck, we systematically analyze the memory/accuracy trade-off of various efficient methods: factorized attention, parameter-efficient image-to-video adaptation, input masking, and multi-resolution patchification. Surprisingly, simply masking large portions of the video (up to 75%) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our simple approach for training long video-to-text models, which scales to 1B parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segment-based information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema).","sentences":["Understanding long, real-world videos requires modeling of long-range visual dependencies.","To this end, we explore video-first architectures, building on the common paradigm of transferring large-scale, image--text models to video via shallow temporal fusion.","However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to poor video--language alignment in standard video datasets, and (2) higher memory consumption, bottlenecking the number of frames that can be processed.","To mitigate the memory bottleneck, we systematically analyze the memory/accuracy trade-off of various efficient methods: factorized attention, parameter-efficient image-to-video adaptation, input masking, and multi-resolution patchification.","Surprisingly, simply masking large portions of the video (up to 75%) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS.","Our simple approach for training long video-to-text models, which scales to 1B parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segment-based information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema)."],"url":"http://arxiv.org/abs/2312.07395v1"}
{"created":"2023-12-12 16:05:55","title":"ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning","abstract":"While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention, its algorithmic robustness, particularly against adversarial perturbations, remains unexplored. Unfortunately, the attacks and robust representation training methods specifically designed for traditional RL are not so effective when applied to GCRL. To address this challenge, we propose the \\textit{Semi-Contrastive Representation} attack, a novel approach inspired by the adversarial contrastive attack. Unlike existing attacks in RL, it only necessitates information from the policy function and can be seamlessly implemented during deployment. Furthermore, to mitigate the vulnerability of existing GCRL algorithms, we introduce \\textit{Adversarial Representation Tactics}. This strategy combines \\textit{Semi-Contrastive Adversarial Augmentation} with \\textit{Sensitivity-Aware Regularizer}. It improves the adversarial robustness of the underlying agent against various types of perturbations. Extensive experiments validate the superior performance of our attack and defence mechanism across multiple state-of-the-art GCRL algorithms. Our tool {\\bf ReRoGCRL} is available at \\url{https://github.com/TrustAI/ReRoGCRL}.","sentences":["While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention, its algorithmic robustness, particularly against adversarial perturbations, remains unexplored.","Unfortunately, the attacks and robust representation training methods specifically designed for traditional RL are not so effective when applied to GCRL.","To address this challenge, we propose the \\textit{Semi-Contrastive Representation} attack, a novel approach inspired by the adversarial contrastive attack.","Unlike existing attacks in RL, it only necessitates information from the policy function and can be seamlessly implemented during deployment.","Furthermore, to mitigate the vulnerability of existing GCRL algorithms, we introduce \\textit{Adversarial Representation Tactics}.","This strategy combines \\textit{Semi-Contrastive Adversarial Augmentation} with \\textit{Sensitivity-Aware Regularizer}.","It improves the adversarial robustness of the underlying agent against various types of perturbations.","Extensive experiments validate the superior performance of our attack and defence mechanism across multiple state-of-the-art GCRL algorithms.","Our tool {\\bf ReRoGCRL} is available at \\url{https://github.com/TrustAI/ReRoGCRL}."],"url":"http://arxiv.org/abs/2312.07392v1"}
{"created":"2023-12-12 16:05:12","title":"Eroding Trust In Aerial Imagery: Comprehensive Analysis and Evaluation Of Adversarial Attacks In Geospatial Systems","abstract":"In critical operations where aerial imagery plays an essential role, the integrity and trustworthiness of data are paramount. The emergence of adversarial attacks, particularly those that exploit control over labels or employ physically feasible trojans, threatens to erode that trust, making the analysis and mitigation of these attacks a matter of urgency. We demonstrate how adversarial attacks can degrade confidence in geospatial systems, specifically focusing on scenarios where the attacker's control over labels is restricted and the use of realistic threat vectors. Proposing and evaluating several innovative attack methodologies, including those tailored to overhead images, we empirically show their threat to remote sensing systems using high-quality SpaceNet datasets. Our experimentation reflects the unique challenges posed by aerial imagery, and these preliminary results not only reveal the potential risks but also highlight the non-trivial nature of the problem compared to recent works.","sentences":["In critical operations where aerial imagery plays an essential role, the integrity and trustworthiness of data are paramount.","The emergence of adversarial attacks, particularly those that exploit control over labels or employ physically feasible trojans, threatens to erode that trust, making the analysis and mitigation of these attacks a matter of urgency.","We demonstrate how adversarial attacks can degrade confidence in geospatial systems, specifically focusing on scenarios where the attacker's control over labels is restricted and the use of realistic threat vectors.","Proposing and evaluating several innovative attack methodologies, including those tailored to overhead images, we empirically show their threat to remote sensing systems using high-quality SpaceNet datasets.","Our experimentation reflects the unique challenges posed by aerial imagery, and these preliminary results not only reveal the potential risks but also highlight the non-trivial nature of the problem compared to recent works."],"url":"http://arxiv.org/abs/2312.07389v1"}
{"created":"2023-12-12 16:03:38","title":"Transformation rules for the decentralization of a blockchain-extended global process model","abstract":"Blockchains and distributed ledger technology offer promising capabilities for supporting collaborative business processes across organizations. Typically, approaches in this field fall into two categories: either executing the entire process model on the blockchain or using the blockchain primarily to enforce or monitor the exchange of messages between participants. Our work proposes a novel approach that sits between these two methods.   We introduce a centralized process model extended with blockchain annotations, detailing the tasks of each participating organization and the extent to which blockchain technology is needed to secure task execution. This model also includes all critical data objects and specifies how their handling should be protected by the blockchain.   This technical report outlines a systematic three-step method for automatically decentralizing this comprehensive model into individual local process models for each organization, coupled with a separate process model for the blockchain. This decentralized structure effectively replicates the original global process model.   Our transformation approach is rule-based, focusing on creating a platform-inde-pendent model first, then a platform-specific model. Subsequently, we project the platform-specific model to obtain one model for the blockchain and one model for each participating organization.","sentences":["Blockchains and distributed ledger technology offer promising capabilities for supporting collaborative business processes across organizations.","Typically, approaches in this field fall into two categories: either executing the entire process model on the blockchain or using the blockchain primarily to enforce or monitor the exchange of messages between participants.","Our work proposes a novel approach that sits between these two methods.   ","We introduce a centralized process model extended with blockchain annotations, detailing the tasks of each participating organization and the extent to which blockchain technology is needed to secure task execution.","This model also includes all critical data objects and specifies how their handling should be protected by the blockchain.   ","This technical report outlines a systematic three-step method for automatically decentralizing this comprehensive model into individual local process models for each organization, coupled with a separate process model for the blockchain.","This decentralized structure effectively replicates the original global process model.   ","Our transformation approach is rule-based, focusing on creating a platform-inde-pendent model first, then a platform-specific model.","Subsequently, we project the platform-specific model to obtain one model for the blockchain and one model for each participating organization."],"url":"http://arxiv.org/abs/2312.07388v1"}
{"created":"2023-12-12 16:00:55","title":"Unsupervised Temporal Action Localization via Self-paced Incremental Learning","abstract":"Recently, temporal action localization (TAL) has garnered significant interest in information retrieval community. However, existing supervised/weakly supervised methods are heavily dependent on extensive labeled temporal boundaries and action categories, which is labor-intensive and time-consuming. Although some unsupervised methods have utilized the ``iteratively clustering and localization'' paradigm for TAL, they still suffer from two pivotal impediments: 1) unsatisfactory video clustering confidence, and 2) unreliable video pseudolabels for model training. To address these limitations, we present a novel self-paced incremental learning model to enhance clustering and localization training simultaneously, thereby facilitating more effective unsupervised TAL. Concretely, we improve the clustering confidence through exploring the contextual feature-robust visual information. Thereafter, we design two (constant- and variable- speed) incremental instance learning strategies for easy-to-hard model training, thus ensuring the reliability of these video pseudolabels and further improving overall localization performance. Extensive experiments on two public datasets have substantiated the superiority of our model over several state-of-the-art competitors.","sentences":["Recently, temporal action localization (TAL) has garnered significant interest in information retrieval community.","However, existing supervised/weakly supervised methods are heavily dependent on extensive labeled temporal boundaries and action categories, which is labor-intensive and time-consuming.","Although some unsupervised methods have utilized the ``iteratively clustering and localization'' paradigm for TAL, they still suffer from two pivotal impediments: 1) unsatisfactory video clustering confidence, and 2) unreliable video pseudolabels for model training.","To address these limitations, we present a novel self-paced incremental learning model to enhance clustering and localization training simultaneously, thereby facilitating more effective unsupervised TAL.","Concretely, we improve the clustering confidence through exploring the contextual feature-robust visual information.","Thereafter, we design two (constant- and variable- speed) incremental instance learning strategies for easy-to-hard model training, thus ensuring the reliability of these video pseudolabels and further improving overall localization performance.","Extensive experiments on two public datasets have substantiated the superiority of our model over several state-of-the-art competitors."],"url":"http://arxiv.org/abs/2312.07384v1"}
{"created":"2023-12-12 16:00:55","title":"GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance","abstract":"Although existing speech-driven talking face generation methods achieve significant progress, they are far from real-world application due to the avatar-specific training demand and unstable lip movements. To address the above issues, we propose the GSmoothFace, a novel two-stage generalized talking face generation model guided by a fine-grained 3d face model, which can synthesize smooth lip dynamics while preserving the speaker's identity. Our proposed GSmoothFace model mainly consists of the Audio to Expression Prediction (A2EP) module and the Target Adaptive Face Translation (TAFT) module. Specifically, we first develop the A2EP module to predict expression parameters synchronized with the driven speech. It uses a transformer to capture the long-term audio context and learns the parameters from the fine-grained 3D facial vertices, resulting in accurate and smooth lip-synchronization performance. Afterward, the well-designed TAFT module, empowered by Morphology Augmented Face Blending (MAFB), takes the predicted expression parameters and target video as inputs to modify the facial region of the target video without distorting the background content. The TAFT effectively exploits the identity appearance and background context in the target video, which makes it possible to generalize to different speakers without retraining. Both quantitative and qualitative experiments confirm the superiority of our method in terms of realism, lip synchronization, and visual quality. See the project page for code, data, and request pre-trained models: https://zhanghm1995.github.io/GSmoothFace.","sentences":["Although existing speech-driven talking face generation methods achieve significant progress, they are far from real-world application due to the avatar-specific training demand and unstable lip movements.","To address the above issues, we propose the GSmoothFace, a novel two-stage generalized talking face generation model guided by a fine-grained 3d face model, which can synthesize smooth lip dynamics while preserving the speaker's identity.","Our proposed GSmoothFace model mainly consists of the Audio to Expression Prediction (A2EP) module and the Target Adaptive Face Translation (TAFT) module.","Specifically, we first develop the A2EP module to predict expression parameters synchronized with the driven speech.","It uses a transformer to capture the long-term audio context and learns the parameters from the fine-grained 3D facial vertices, resulting in accurate and smooth lip-synchronization performance.","Afterward, the well-designed TAFT module, empowered by Morphology Augmented Face Blending (MAFB), takes the predicted expression parameters and target video as inputs to modify the facial region of the target video without distorting the background content.","The TAFT effectively exploits the identity appearance and background context in the target video, which makes it possible to generalize to different speakers without retraining.","Both quantitative and qualitative experiments confirm the superiority of our method in terms of realism, lip synchronization, and visual quality.","See the project page for code, data, and request pre-trained models: https://zhanghm1995.github.io/GSmoothFace."],"url":"http://arxiv.org/abs/2312.07385v1"}
{"created":"2023-12-12 15:58:22","title":"Delay analysis of the IEEE 802.11bd EDCA with repetitions","abstract":"We analyse the performance of the IEEE 802.11bd MAC protocol, with Enhanced Distributed Channel Access (EDCA) and repeated transmissions, in terms of the MAC access delay of packets pertaining to safety-related events. We outline Markov chain models for the contention mechanism of priority-based access categories, and derive the associated steady-state probabilities. Using these probabilities, we characterise the delay experienced by the packet in the MAC layer. Further, we characterise the reliability of the protocol in terms of the likelihood that a packet is delivered within a critical time interval. Numerical computations are conducted to understand the impact of various system parameters on the MAC access delay. The analysis indicates that the MAC access delay depends on various system parameters, some of which are influenced by the traffic scenario and nature of safety-critical events. Motivated by this, we used our analysis to study the delay and reliability of the 802.11bd MAC protocol specific to the context of platooning of connected vehicles subject to interruptions by human-driven motorised two wheelers. We observe that while the delay performance of the protocol is as per the QoS requirements of the standard, the protocol may not be reliable for this specific application. Our study suggests that it is desirable to co-design vehicular communication protocols with prevalent safety-related traffic applications.","sentences":["We analyse the performance of the IEEE 802.11bd MAC protocol, with Enhanced Distributed Channel Access (EDCA) and repeated transmissions, in terms of the MAC access delay of packets pertaining to safety-related events.","We outline Markov chain models for the contention mechanism of priority-based access categories, and derive the associated steady-state probabilities.","Using these probabilities, we characterise the delay experienced by the packet in the MAC layer.","Further, we characterise the reliability of the protocol in terms of the likelihood that a packet is delivered within a critical time interval.","Numerical computations are conducted to understand the impact of various system parameters on the MAC access delay.","The analysis indicates that the MAC access delay depends on various system parameters, some of which are influenced by the traffic scenario and nature of safety-critical events.","Motivated by this, we used our analysis to study the delay and reliability of the 802.11bd MAC protocol specific to the context of platooning of connected vehicles subject to interruptions by human-driven motorised two wheelers.","We observe that while the delay performance of the protocol is as per the QoS requirements of the standard, the protocol may not be reliable for this specific application.","Our study suggests that it is desirable to co-design vehicular communication protocols with prevalent safety-related traffic applications."],"url":"http://arxiv.org/abs/2312.07383v1"}
{"created":"2023-12-12 15:57:03","title":"ScribblePrompt: Fast and Flexible Interactive Segmentation for Any Medical Image","abstract":"Semantic medical image segmentation is a crucial part of both scientific research and clinical care. With enough labelled data, deep learning models can be trained to accurately automate specific medical image segmentation tasks. However, manually segmenting images to create training data is highly labor intensive. In this paper, we present ScribblePrompt, an interactive segmentation framework for medical imaging that enables human annotators to segment unseen structures using scribbles, clicks, and bounding boxes. Scribbles are an intuitive and effective form of user interaction for complex tasks, however most existing methods focus on click-based interactions. We introduce algorithms for simulating realistic scribbles that enable training models that are amenable to multiple types of interaction. To achieve generalization to new tasks, we train on a diverse collection of 65 open-access biomedical datasets -- using both real and synthetic labels. We test ScribblePrompt on multiple network architectures and unseen datasets, and demonstrate that it can be used in real-time on a single CPU. We evaluate ScribblePrompt using manually-collected scribbles, simulated interactions, and a user study. ScribblePrompt outperforms existing methods in all our evaluations. In the user study, ScribblePrompt reduced annotation time by 28% while improving Dice by 15% compared to existing methods. We showcase ScribblePrompt in an online demo and provide code at https://scribbleprompt.csail.mit.edu","sentences":["Semantic medical image segmentation is a crucial part of both scientific research and clinical care.","With enough labelled data, deep learning models can be trained to accurately automate specific medical image segmentation tasks.","However, manually segmenting images to create training data is highly labor intensive.","In this paper, we present ScribblePrompt, an interactive segmentation framework for medical imaging that enables human annotators to segment unseen structures using scribbles, clicks, and bounding boxes.","Scribbles are an intuitive and effective form of user interaction for complex tasks, however most existing methods focus on click-based interactions.","We introduce algorithms for simulating realistic scribbles that enable training models that are amenable to multiple types of interaction.","To achieve generalization to new tasks, we train on a diverse collection of 65 open-access biomedical datasets -- using both real and synthetic labels.","We test ScribblePrompt on multiple network architectures and unseen datasets, and demonstrate that it can be used in real-time on a single CPU.","We evaluate ScribblePrompt using manually-collected scribbles, simulated interactions, and a user study.","ScribblePrompt outperforms existing methods in all our evaluations.","In the user study, ScribblePrompt reduced annotation time by 28% while improving Dice by 15% compared to existing methods.","We showcase ScribblePrompt in an online demo and provide code at https://scribbleprompt.csail.mit.edu"],"url":"http://arxiv.org/abs/2312.07381v1"}
{"created":"2023-12-12 15:48:12","title":"X4D-SceneFormer: Enhanced Scene Understanding on 4D Point Cloud Videos through Cross-modal Knowledge Transfer","abstract":"The field of 4D point cloud understanding is rapidly developing with the goal of analyzing dynamic 3D point cloud sequences. However, it remains a challenging task due to the sparsity and lack of texture in point clouds. Moreover, the irregularity of point cloud poses a difficulty in aligning temporal information within video sequences. To address these issues, we propose a novel cross-modal knowledge transfer framework, called X4D-SceneFormer. This framework enhances 4D-Scene understanding by transferring texture priors from RGB sequences using a Transformer architecture with temporal relationship mining. Specifically, the framework is designed with a dual-branch architecture, consisting of an 4D point cloud transformer and a Gradient-aware Image Transformer (GIT). During training, we employ multiple knowledge transfer techniques, including temporal consistency losses and masked self-attention, to strengthen the knowledge transfer between modalities. This leads to enhanced performance during inference using single-modal 4D point cloud inputs. Extensive experiments demonstrate the superior performance of our framework on various 4D point cloud video understanding tasks, including action recognition, action segmentation and semantic segmentation. The results achieve 1st places, i.e., 85.3% (+7.9%) accuracy and 47.3% (+5.0%) mIoU for 4D action segmentation and semantic segmentation, on the HOI4D challenge\\footnote{\\url{http://www.hoi4d.top/}.}, outperforming previous state-of-the-art by a large margin. We release the code at https://github.com/jinglinglingling/X4D","sentences":["The field of 4D point cloud understanding is rapidly developing with the goal of analyzing dynamic 3D point cloud sequences.","However, it remains a challenging task due to the sparsity and lack of texture in point clouds.","Moreover, the irregularity of point cloud poses a difficulty in aligning temporal information within video sequences.","To address these issues, we propose a novel cross-modal knowledge transfer framework, called X4D-SceneFormer.","This framework enhances 4D-Scene understanding by transferring texture priors from RGB sequences using a Transformer architecture with temporal relationship mining.","Specifically, the framework is designed with a dual-branch architecture, consisting of an 4D point cloud transformer and a Gradient-aware Image Transformer (GIT).","During training, we employ multiple knowledge transfer techniques, including temporal consistency losses and masked self-attention, to strengthen the knowledge transfer between modalities.","This leads to enhanced performance during inference using single-modal 4D point cloud inputs.","Extensive experiments demonstrate the superior performance of our framework on various 4D point cloud video understanding tasks, including action recognition, action segmentation and semantic segmentation.","The results achieve 1st places, i.e., 85.3% (+7.9%) accuracy and 47.3% (+5.0%)","mIoU for 4D action segmentation and semantic segmentation, on the HOI4D challenge\\footnote{\\url{http://www.hoi4d.top/}.","}, outperforming previous state-of-the-art by a large margin.","We release the code at https://github.com/jinglinglingling/X4D"],"url":"http://arxiv.org/abs/2312.07378v1"}
{"created":"2023-12-12 15:43:36","title":"Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects","abstract":"Camouflaged object detection (COD) approaches heavily rely on pixel-level annotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse annotations like scribbles or points to reduce annotation effort, but this can lead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable segmentation ability with sparse prompts like points. However, manual prompt is not always feasible, as it may not be accessible in real-world application. Additionally, it only provides localization information instead of semantic one, which can intrinsically cause ambiguity in interpreting the targets. In this work, we aim to eliminate the need for manual prompt. The key idea is to employ Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts using the semantic information given by a generic text prompt.To that end, we introduce a test-time adaptation per-instance mechanism called Generalizable SAM (GenSAM) to automatically enerate and optimize visual prompts the generic task prompt for WSCOD. In particular, CCTP maps a single generic text prompt onto image-specific consensus foreground and background heatmaps using vision-language models, acquiring reliable visual prompts. Moreover, to test-time adapt the visual prompts, we further propose Progressive Mask Generation (PMG) to iteratively reweight the input image, guiding the model to focus on the targets in a coarse-to-fine manner. Crucially, all network parameters are fixed, avoiding the need for additional training. Experiments demonstrate the superiority of GenSAM. Experiments on three benchmarks demonstrate that GenSAM outperforms point supervision approaches and achieves comparable results to scribble supervision ones, solely relying on general task descriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.","sentences":["Camouflaged object detection (COD) approaches heavily rely on pixel-level annotated datasets.","Weakly-supervised COD (WSCOD) approaches use sparse annotations like scribbles or points to reduce annotation effort, but this can lead to decreased accuracy.","The Segment Anything Model (SAM) shows remarkable segmentation ability with sparse prompts like points.","However, manual prompt is not always feasible, as it may not be accessible in real-world application.","Additionally, it only provides localization information instead of semantic one, which can intrinsically cause ambiguity in interpreting the targets.","In this work, we aim to eliminate the need for manual prompt.","The key idea is to employ Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts using the semantic information given by a generic text prompt.","To that end, we introduce a test-time adaptation per-instance mechanism called Generalizable SAM (GenSAM) to automatically enerate and optimize visual prompts the generic task prompt for WSCOD.","In particular, CCTP maps a single generic text prompt onto image-specific consensus foreground and background heatmaps using vision-language models, acquiring reliable visual prompts.","Moreover, to test-time adapt the visual prompts, we further propose Progressive Mask Generation (PMG) to iteratively reweight the input image, guiding the model to focus on the targets in a coarse-to-fine manner.","Crucially, all network parameters are fixed, avoiding the need for additional training.","Experiments demonstrate the superiority of GenSAM.","Experiments on three benchmarks demonstrate that GenSAM outperforms point supervision approaches and achieves comparable results to scribble supervision ones, solely relying on general task descriptions as prompts.","our codes is in: https://lwpyh.github.io/GenSAM/."],"url":"http://arxiv.org/abs/2312.07374v1"}
{"created":"2023-12-12 15:40:38","title":"Privacy-Aware Energy Consumption Modeling of Connected Battery Electric Vehicles using Federated Learning","abstract":"Battery Electric Vehicles (BEVs) are increasingly significant in modern cities due to their potential to reduce air pollution. Precise and real-time estimation of energy consumption for them is imperative for effective itinerary planning and optimizing vehicle systems, which can reduce driving range anxiety and decrease energy costs. As public awareness of data privacy increases, adopting approaches that safeguard data privacy in the context of BEV energy consumption modeling is crucial. Federated Learning (FL) is a promising solution mitigating the risk of exposing sensitive information to third parties by allowing local data to remain on devices and only sharing model updates with a central server. Our work investigates the potential of using FL methods, such as FedAvg, and FedPer, to improve BEV energy consumption prediction while maintaining user privacy. We conducted experiments using data from 10 BEVs under simulated real-world driving conditions. Our results demonstrate that the FedAvg-LSTM model achieved a reduction of up to 67.84\\% in the MAE value of the prediction results. Furthermore, we explored various real-world scenarios and discussed how FL methods can be employed in those cases. Our findings show that FL methods can effectively improve the performance of BEV energy consumption prediction while maintaining user privacy.","sentences":["Battery Electric Vehicles (BEVs) are increasingly significant in modern cities due to their potential to reduce air pollution.","Precise and real-time estimation of energy consumption for them is imperative for effective itinerary planning and optimizing vehicle systems, which can reduce driving range anxiety and decrease energy costs.","As public awareness of data privacy increases, adopting approaches that safeguard data privacy in the context of BEV energy consumption modeling is crucial.","Federated Learning (FL) is a promising solution mitigating the risk of exposing sensitive information to third parties by allowing local data to remain on devices and only sharing model updates with a central server.","Our work investigates the potential of using FL methods, such as FedAvg, and FedPer, to improve BEV energy consumption prediction while maintaining user privacy.","We conducted experiments using data from 10 BEVs under simulated real-world driving conditions.","Our results demonstrate that the FedAvg-LSTM model achieved a reduction of up to 67.84\\% in the MAE value of the prediction results.","Furthermore, we explored various real-world scenarios and discussed how FL methods can be employed in those cases.","Our findings show that FL methods can effectively improve the performance of BEV energy consumption prediction while maintaining user privacy."],"url":"http://arxiv.org/abs/2312.07371v1"}
{"created":"2023-12-12 15:40:22","title":"Adversarial Semi-Supervised Domain Adaptation for Semantic Segmentation: A New Role for Labeled Target Samples","abstract":"Adversarial learning baselines for domain adaptation (DA) approaches in the context of semantic segmentation are under explored in semi-supervised framework. These baselines involve solely the available labeled target samples in the supervision loss. In this work, we propose to enhance their usefulness on both semantic segmentation and the single domain classifier neural networks. We design new training objective losses for cases when labeled target data behave as source samples or as real target samples. The underlying rationale is that considering the set of labeled target samples as part of source domain helps reducing the domain discrepancy and, hence, improves the contribution of the adversarial loss. To support our approach, we consider a complementary method that mixes source and labeled target data, then applies the same adaptation process. We further propose an unsupervised selection procedure using entropy to optimize the choice of labeled target samples for adaptation. We illustrate our findings through extensive experiments on the benchmarks GTA5, SYNTHIA, and Cityscapes. The empirical evaluation highlights competitive performance of our proposed approach.","sentences":["Adversarial learning baselines for domain adaptation (DA) approaches in the context of semantic segmentation are under explored in semi-supervised framework.","These baselines involve solely the available labeled target samples in the supervision loss.","In this work, we propose to enhance their usefulness on both semantic segmentation and the single domain classifier neural networks.","We design new training objective losses for cases when labeled target data behave as source samples or as real target samples.","The underlying rationale is that considering the set of labeled target samples as part of source domain helps reducing the domain discrepancy and, hence, improves the contribution of the adversarial loss.","To support our approach, we consider a complementary method that mixes source and labeled target data, then applies the same adaptation process.","We further propose an unsupervised selection procedure using entropy to optimize the choice of labeled target samples for adaptation.","We illustrate our findings through extensive experiments on the benchmarks GTA5, SYNTHIA, and Cityscapes.","The empirical evaluation highlights competitive performance of our proposed approach."],"url":"http://arxiv.org/abs/2312.07370v1"}
{"created":"2023-12-12 15:36:59","title":"Sequential Planning in Large Partially Observable Environments guided by LLMs","abstract":"Sequential planning in large state space and action space quickly becomes intractable due to combinatorial explosion of the search space. Heuristic methods, like monte-carlo tree search, though effective for large state space, but struggle if action space is large. Pure reinforcement learning methods, relying only on reward signals, needs prohibitively large interactions with the environment to device a viable plan. If the state space, observations and actions can be represented in natural language then Large Language models (LLM) can be used to generate action plans. Recently several such goal-directed agents like Reflexion, CLIN, SayCan were able to surpass the performance of other state-of-the-art methods with minimum or no task specific training. But they still struggle with exploration and get stuck in local optima. Their planning capabilities are limited by the limited reasoning capability of the foundational LLMs on text data. We propose a hybrid agent \"neoplanner\", that synergizes both state space search with queries to foundational LLM to get the best action plan. The reward signals are quantitatively used to drive the search. A balance of exploration and exploitation is maintained by maximizing upper confidence bounds of values of states. In places where random exploration is needed, the LLM is queried to generate an action plan. Learnings from each trial are stored as entity relationships in text format. Those are used in future queries to the LLM for continual improvement. Experiments in the Scienceworld environment reveals a 124% improvement from the current best method in terms of average reward gained across multiple tasks.","sentences":["Sequential planning in large state space and action space quickly becomes intractable due to combinatorial explosion of the search space.","Heuristic methods, like monte-carlo tree search, though effective for large state space, but struggle if action space is large.","Pure reinforcement learning methods, relying only on reward signals, needs prohibitively large interactions with the environment to device a viable plan.","If the state space, observations and actions can be represented in natural language then Large Language models (LLM) can be used to generate action plans.","Recently several such goal-directed agents like Reflexion, CLIN, SayCan were able to surpass the performance of other state-of-the-art methods with minimum or no task specific training.","But they still struggle with exploration and get stuck in local optima.","Their planning capabilities are limited by the limited reasoning capability of the foundational LLMs on text data.","We propose a hybrid agent \"neoplanner\", that synergizes both state space search with queries to foundational LLM to get the best action plan.","The reward signals are quantitatively used to drive the search.","A balance of exploration and exploitation is maintained by maximizing upper confidence bounds of values of states.","In places where random exploration is needed, the LLM is queried to generate an action plan.","Learnings from each trial are stored as entity relationships in text format.","Those are used in future queries to the LLM for continual improvement.","Experiments in the Scienceworld environment reveals a 124% improvement from the current best method in terms of average reward gained across multiple tasks."],"url":"http://arxiv.org/abs/2312.07368v1"}
{"created":"2023-12-12 15:33:08","title":"Collapse-Oriented Adversarial Training with Triplet Decoupling for Robust Image Retrieval","abstract":"Adversarial training has achieved substantial performance in defending image retrieval systems against adversarial examples. However, existing studies still suffer from two major limitations: model collapse and weak adversary. This paper addresses these two limitations by proposing collapse-oriented (COLO) adversarial training with triplet decoupling (TRIDE). Specifically, COLO prevents model collapse by temporally orienting the perturbation update direction with a new collapse metric, while TRIDE yields a strong adversary by spatially decoupling the update targets of perturbation into the anchor and the two candidates of a triplet. Experimental results demonstrate that our COLO-TRIDE outperforms the current state of the art by 7% on average over 10 robustness metrics and across 3 popular datasets. In addition, we identify the fairness limitations of commonly used robustness metrics in image retrieval and propose a new metric for more meaningful robustness evaluation. Codes will be made publicly available on GitHub.","sentences":["Adversarial training has achieved substantial performance in defending image retrieval systems against adversarial examples.","However, existing studies still suffer from two major limitations: model collapse and weak adversary.","This paper addresses these two limitations by proposing collapse-oriented (COLO) adversarial training with triplet decoupling (TRIDE).","Specifically, COLO prevents model collapse by temporally orienting the perturbation update direction with a new collapse metric, while TRIDE yields a strong adversary by spatially decoupling the update targets of perturbation into the anchor and the two candidates of a triplet.","Experimental results demonstrate that our COLO-TRIDE outperforms the current state of the art by 7% on average over 10 robustness metrics and across 3 popular datasets.","In addition, we identify the fairness limitations of commonly used robustness metrics in image retrieval and propose a new metric for more meaningful robustness evaluation.","Codes will be made publicly available on GitHub."],"url":"http://arxiv.org/abs/2312.07364v1"}
{"created":"2023-12-12 15:31:37","title":"Intelligible Protocol Learning for Resource Allocation in 6G O-RAN Slicing","abstract":"An adaptive standardized protocol is essential for addressing inter-slice resource contention and conflict in network slicing. Traditional protocol standardization is a cumbersome task that yields hardcoded predefined protocols, resulting in increased costs and delayed rollout. Going beyond these limitations, this paper proposes a novel multi-agent deep reinforcement learning (MADRL) communication framework called standalone explainable protocol (STEP) for future sixth-generation (6G) open radio access network (O-RAN) slicing. As new conditions arise and affect network operation, resource orchestration agents adapt their communication messages to promote the emergence of a protocol on-the-fly, which enables the mitigation of conflict and resource contention between network slices. STEP weaves together the notion of information bottleneck (IB) theory with deep Q-network (DQN) learning concepts. By incorporating a stochastic bottleneck layer -- inspired by variational autoencoders (VAEs) -- STEP imposes an information-theoretic constraint for emergent inter-agent communication. This ensures that agents exchange concise and meaningful information, preventing resource waste and enhancing the overall system performance. The learned protocols enhance interpretability, laying a robust foundation for standardizing next-generation 6G networks. By considering an O-RAN compliant network slicing resource allocation problem, a conflict resolution protocol is developed. In particular, the results demonstrate that, on average, STEP reduces inter-slice conflicts by up to 6.06x compared to a predefined protocol method. Furthermore, in comparison with an MADRL baseline, STEP achieves 1.4x and 3.5x lower resource underutilization and latency, respectively.","sentences":["An adaptive standardized protocol is essential for addressing inter-slice resource contention and conflict in network slicing.","Traditional protocol standardization is a cumbersome task that yields hardcoded predefined protocols, resulting in increased costs and delayed rollout.","Going beyond these limitations, this paper proposes a novel multi-agent deep reinforcement learning (MADRL) communication framework called standalone explainable protocol (STEP) for future sixth-generation (6G) open radio access network (O-RAN) slicing.","As new conditions arise and affect network operation, resource orchestration agents adapt their communication messages to promote the emergence of a protocol on-the-fly, which enables the mitigation of conflict and resource contention between network slices.","STEP weaves together the notion of information bottleneck (IB) theory with deep Q-network (DQN) learning concepts.","By incorporating a stochastic bottleneck layer -- inspired by variational autoencoders (VAEs) -- STEP imposes an information-theoretic constraint for emergent inter-agent communication.","This ensures that agents exchange concise and meaningful information, preventing resource waste and enhancing the overall system performance.","The learned protocols enhance interpretability, laying a robust foundation for standardizing next-generation 6G networks.","By considering an O-RAN compliant network slicing resource allocation problem, a conflict resolution protocol is developed.","In particular, the results demonstrate that, on average, STEP reduces inter-slice conflicts by up to 6.06x compared to a predefined protocol method.","Furthermore, in comparison with an MADRL baseline, STEP achieves 1.4x and 3.5x lower resource underutilization and latency, respectively."],"url":"http://arxiv.org/abs/2312.07362v1"}
{"created":"2023-12-12 15:30:24","title":"Boosting Latent Diffusion with Flow Matching","abstract":"Recently, there has been tremendous progress in visual synthesis and the underlying generative models. Here, diffusion models (DMs) stand out particularly, but lately, flow matching (FM) has also garnered considerable interest. While DMs excel in providing diverse images, they suffer from long training and slow generation. With latent diffusion, these issues are only partially alleviated. Conversely, FM offers faster training and inference but exhibits less diversity in synthesis. We demonstrate that introducing FM between the Diffusion model and the convolutional decoder offers high-resolution image synthesis with reduced computational cost and model size. Diffusion can then efficiently provide the necessary generation diversity. FM compensates for the lower resolution, mapping the small latent space to a high-dimensional one. Subsequently, the convolutional decoder of the LDM maps these latents to high-resolution images. By combining the diversity of DMs, the efficiency of FMs, and the effectiveness of convolutional decoders, we achieve state-of-the-art high-resolution image synthesis at $1024^2$ with minimal computational cost. Importantly, our approach is orthogonal to recent approximation and speed-up strategies for the underlying DMs, making it easily integrable into various DM frameworks.","sentences":["Recently, there has been tremendous progress in visual synthesis and the underlying generative models.","Here, diffusion models (DMs) stand out particularly, but lately, flow matching (FM) has also garnered considerable interest.","While DMs excel in providing diverse images, they suffer from long training and slow generation.","With latent diffusion, these issues are only partially alleviated.","Conversely, FM offers faster training and inference but exhibits less diversity in synthesis.","We demonstrate that introducing FM between the Diffusion model and the convolutional decoder offers high-resolution image synthesis with reduced computational cost and model size.","Diffusion can then efficiently provide the necessary generation diversity.","FM compensates for the lower resolution, mapping the small latent space to a high-dimensional one.","Subsequently, the convolutional decoder of the LDM maps these latents to high-resolution images.","By combining the diversity of DMs, the efficiency of FMs, and the effectiveness of convolutional decoders, we achieve state-of-the-art high-resolution image synthesis at $1024^2$ with minimal computational cost.","Importantly, our approach is orthogonal to recent approximation and speed-up strategies for the underlying DMs, making it easily integrable into various DM frameworks."],"url":"http://arxiv.org/abs/2312.07360v1"}
{"created":"2023-12-12 15:26:06","title":"Automatic coral reef fish identification and 3D measurement in the wild","abstract":"In this paper we present a pipeline using stereo images in order to automatically identify, track in 3D fish, and measure fish population.","sentences":["In this paper we present a pipeline using stereo images in order to automatically identify, track in 3D fish, and measure fish population."],"url":"http://arxiv.org/abs/2312.07357v1"}
{"created":"2023-12-12 15:23:38","title":"MRCN: Enhanced Coherence Mechanism for Near Memory Processing Architectures","abstract":"In Near Memory Processing (NMP), processing elements(PEs) are placed near the 3D memory, reducing unnecessary data transfers between the CPU and the memory. However, as the CPUs and the PEs of the NMP use a shared memory space, maintaining coherency between them is a challenge. Most current literature relies on maintaining coherence for fine-grained or coarse-grained instruction granularities for the offloaded code blocks. We understand that for most NMP-offloaded instructions, the coherence conflict is low, and waiting for the coherence transaction hinders the performance. We construct an analytical model for an existing coherence strategy called CONDA, which is within 4% accuracy. This model indicates the key parameters responsible - the granularity of offloaded code, probability of conflicts, transaction times, and commit time. This paper identifies the prospective optimizations using the analytical model for CONDA. It proposes a new coherence scheme called MRCN: Monitored Rollback Coherence for NMP. MRCN addresses the coherence issue while eliminating unnecessary re-executions with limited hardware overhead. The MRCN is evaluated on synthetic as well as Rodinia benchmarks. The analytical results are within 4% accuracy of the simulation results. The MRCN shows improvement of upto 25% over CONDA strategy for the same benchmark under different execution conditions.","sentences":["In Near Memory Processing (NMP), processing elements(PEs) are placed near the 3D memory, reducing unnecessary data transfers between the CPU and the memory.","However, as the CPUs and the PEs of the NMP use a shared memory space, maintaining coherency between them is a challenge.","Most current literature relies on maintaining coherence for fine-grained or coarse-grained instruction granularities for the offloaded code blocks.","We understand that for most NMP-offloaded instructions, the coherence conflict is low, and waiting for the coherence transaction hinders the performance.","We construct an analytical model for an existing coherence strategy called CONDA, which is within 4% accuracy.","This model indicates the key parameters responsible - the granularity of offloaded code, probability of conflicts, transaction times, and commit time.","This paper identifies the prospective optimizations using the analytical model for CONDA.","It proposes a new coherence scheme called MRCN:","Monitored Rollback Coherence for NMP.","MRCN addresses the coherence issue while eliminating unnecessary re-executions with limited hardware overhead.","The MRCN is evaluated on synthetic as well as Rodinia benchmarks.","The analytical results are within 4% accuracy of the simulation results.","The MRCN shows improvement of upto 25% over CONDA strategy for the same benchmark under different execution conditions."],"url":"http://arxiv.org/abs/2312.07355v1"}
{"created":"2023-12-12 15:21:57","title":"CLIP in Medical Imaging: A Comprehensive Survey","abstract":"Contrastive Language-Image Pre-training (CLIP), a straightforward yet effective pre-training paradigm, successfully introduces semantic-rich text supervision to vision models and has demonstrated promising results in various tasks due to its generalizability and interpretability. It has recently gained increasing interest in the medical imaging domain, either as a powerful pre-training paradigm for medical vision language alignment or a pre-trained key component for various clinical tasks. With the aim of facilitating a deeper understanding of this promising direction, this survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications. Our survey (1) starts with a brief introduction to the fundamentals of CLIP methodology. (2) Then, we investigate the adaptation of CLIP pre-training in the medical domain, focusing on how to optimize CLIP given characteristics of medical images and reports. (3) Furthermore, we explore the practical utilization of CLIP pre-trained models in various tasks, including classification, dense prediction, and cross-modal tasks. (4) Finally, we discuss existing limitations of CLIP in the context of medical imaging and propose forward-looking directions to address the demands of medical imaging domain. We expect that this comprehensive survey will provide researchers in the field of medical image analysis with a holistic understanding of the CLIP paradigm and its potential implications. The project page is available at https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging, which will be regularly updated.","sentences":["Contrastive Language-Image Pre-training (CLIP), a straightforward yet effective pre-training paradigm, successfully introduces semantic-rich text supervision to vision models and has demonstrated promising results in various tasks due to its generalizability and interpretability.","It has recently gained increasing interest in the medical imaging domain, either as a powerful pre-training paradigm for medical vision language alignment or a pre-trained key component for various clinical tasks.","With the aim of facilitating a deeper understanding of this promising direction, this survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications.","Our survey (1) starts with a brief introduction to the fundamentals of CLIP methodology.","(2) Then, we investigate the adaptation of CLIP pre-training in the medical domain, focusing on how to optimize CLIP given characteristics of medical images and reports.","(3) Furthermore, we explore the practical utilization of CLIP pre-trained models in various tasks, including classification, dense prediction, and cross-modal tasks.","(4) Finally, we discuss existing limitations of CLIP in the context of medical imaging and propose forward-looking directions to address the demands of medical imaging domain.","We expect that this comprehensive survey will provide researchers in the field of medical image analysis with a holistic understanding of the CLIP paradigm and its potential implications.","The project page is available at https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging, which will be regularly updated."],"url":"http://arxiv.org/abs/2312.07353v1"}
{"created":"2023-12-12 15:18:15","title":"CholecTrack20: A Dataset for Multi-Class Multiple Tool Tracking in Laparoscopic Surgery","abstract":"Tool tracking in surgical videos is vital in computer-assisted intervention for tasks like surgeon skill assessment, safety zone estimation, and human-machine collaboration during minimally invasive procedures. The lack of large-scale datasets hampers Artificial Intelligence implementation in this domain. Current datasets exhibit overly generic tracking formalization, often lacking surgical context: a deficiency that becomes evident when tools move out of the camera's scope, resulting in rigid trajectories that hinder realistic surgical representation. This paper addresses the need for a more precise and adaptable tracking formalization tailored to the intricacies of endoscopic procedures by introducing CholecTrack20, an extensive dataset meticulously annotated for multi-class multi-tool tracking across three perspectives representing the various ways of considering the temporal duration of a tool trajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within the camera's scope. The dataset comprises 20 laparoscopic videos with over 35,000 frames and 65,000 annotated tool instances with details on spatial location, category, identity, operator, phase, and surgical visual conditions. This detailed dataset caters to the evolving assistive requirements within a procedure.","sentences":["Tool tracking in surgical videos is vital in computer-assisted intervention for tasks like surgeon skill assessment, safety zone estimation, and human-machine collaboration during minimally invasive procedures.","The lack of large-scale datasets hampers Artificial Intelligence implementation in this domain.","Current datasets exhibit overly generic tracking formalization, often lacking surgical context: a deficiency that becomes evident when tools move out of the camera's scope, resulting in rigid trajectories that hinder realistic surgical representation.","This paper addresses the need for a more precise and adaptable tracking formalization tailored to the intricacies of endoscopic procedures by introducing CholecTrack20, an extensive dataset meticulously annotated for multi-class multi-tool tracking across three perspectives representing the various ways of considering the temporal duration of a tool trajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within the camera's scope.","The dataset comprises 20 laparoscopic videos with over 35,000 frames and 65,000 annotated tool instances with details on spatial location, category, identity, operator, phase, and surgical visual conditions.","This detailed dataset caters to the evolving assistive requirements within a procedure."],"url":"http://arxiv.org/abs/2312.07352v1"}
{"created":"2023-12-12 15:11:00","title":"A discontinuous Galerkin / cohesive zone model approach for the computational modeling of fracture in geometrically exact slender beams","abstract":"Slender beams are often employed as constituents in engineering materials and structures. Prior experiments on lattices of slender beams have highlighted their complex failure response, where the interplay between buckling and fracture plays a critical role. In this paper, we introduce a novel computational approach for modeling fracture in slender beams subjected to large deformations. We adopt a state-of-the-art geometrically exact Kirchhoff beam formulation to describe the finite deformations of beams in three-dimensions. We develop a discontinuous Galerkin finite element discretization of the beam governing equations, incorporating discontinuities in the position and tangent degrees of freedom at the inter-element boundaries of the finite elements. Before fracture initiation, we enforce compatibility of nodal positions and tangents weakly, via the exchange of variationally-consistent forces and moments at the interfaces between adjacent elements. At the onset of fracture, these forces and moments transition to cohesive laws modeling interface failure. We conduct a series of numerical tests to verify our computational framework against a set of benchmarks and we demonstrate its ability to capture the tensile and bending fracture modes in beams exhibiting large deformations. Finally, we present the validation of our framework against fracture experiments of dry spaghetti rods subjected to sudden relaxation of curvature.","sentences":["Slender beams are often employed as constituents in engineering materials and structures.","Prior experiments on lattices of slender beams have highlighted their complex failure response, where the interplay between buckling and fracture plays a critical role.","In this paper, we introduce a novel computational approach for modeling fracture in slender beams subjected to large deformations.","We adopt a state-of-the-art geometrically exact Kirchhoff beam formulation to describe the finite deformations of beams in three-dimensions.","We develop a discontinuous Galerkin finite element discretization of the beam governing equations, incorporating discontinuities in the position and tangent degrees of freedom at the inter-element boundaries of the finite elements.","Before fracture initiation, we enforce compatibility of nodal positions and tangents weakly, via the exchange of variationally-consistent forces and moments at the interfaces between adjacent elements.","At the onset of fracture, these forces and moments transition to cohesive laws modeling interface failure.","We conduct a series of numerical tests to verify our computational framework against a set of benchmarks and we demonstrate its ability to capture the tensile and bending fracture modes in beams exhibiting large deformations.","Finally, we present the validation of our framework against fracture experiments of dry spaghetti rods subjected to sudden relaxation of curvature."],"url":"http://arxiv.org/abs/2312.07349v1"}
{"created":"2023-12-12 15:10:03","title":"\"It doesn't tell me anything about how my data is used'': User Perceptions of Data Collection Purposes","abstract":"Data collection purposes and their descriptions are presented on almost all privacy notices under the GDPR, yet there is a lack of research focusing on how effective they are at informing users about data practices. We fill this gap by investigating users' perceptions of data collection purposes and their descriptions, a crucial aspect of informed consent. We conducted 23 semi-structured interviews with European users to investigate user perceptions of six common purposes (Strictly Necessary, Statistics and Analytics, Performance and Functionality, Marketing and Advertising, Personalized Advertising, and Personalized Content) and identified elements of an effective purpose name and description.   We found that most purpose descriptions do not contain the information users wish to know, and that participants preferred some purpose names over others due to their perceived transparency or ease of understanding. Based on these findings, we suggest how the framing of purposes can be improved toward meaningful informed consent.","sentences":["Data collection purposes and their descriptions are presented on almost all privacy notices under the GDPR, yet there is a lack of research focusing on how effective they are at informing users about data practices.","We fill this gap by investigating users' perceptions of data collection purposes and their descriptions, a crucial aspect of informed consent.","We conducted 23 semi-structured interviews with European users to investigate user perceptions of six common purposes (Strictly Necessary, Statistics and Analytics, Performance and Functionality, Marketing and Advertising, Personalized Advertising, and Personalized Content) and identified elements of an effective purpose name and description.   ","We found that most purpose descriptions do not contain the information users wish to know, and that participants preferred some purpose names over others due to their perceived transparency or ease of understanding.","Based on these findings, we suggest how the framing of purposes can be improved toward meaningful informed consent."],"url":"http://arxiv.org/abs/2312.07348v1"}
{"created":"2023-12-12 15:06:44","title":"Can ChatGPT Play the Role of a Teaching Assistant in an Introductory Programming Course?","abstract":"The emergence of Large language models (LLMs) is expected to have a major impact on education. This paper explores the potential of using ChatGPT, an LLM, as a virtual Teaching Assistant (TA) in an Introductory Programming Course. We evaluate ChatGPT's capabilities by comparing its performance with that of human TAs in some TA functions. The TA functions which we focus on include (1) solving programming assignments, (2) grading student code submissions, and (3) providing feedback to undergraduate students in an introductory programming course. Firstly, we investigate how closely ChatGPT's solutions align with those submitted by students. This analysis goes beyond code correctness and also considers code quality. Secondly, we assess ChatGPT's proficiency in grading student code submissions using a given grading rubric and compare its performance with the grades assigned by human TAs. Thirdly, we analyze the quality and relevance of the feedback provided by ChatGPT. This evaluation considers how well ChatGPT addresses mistakes and offers suggestions for improvement in student solutions from both code correctness and code quality perspectives. We conclude with a discussion on the implications of integrating ChatGPT into computing education for automated grading, personalized learning experiences, and instructional support.","sentences":["The emergence of Large language models (LLMs) is expected to have a major impact on education.","This paper explores the potential of using ChatGPT, an LLM, as a virtual Teaching Assistant (TA) in an Introductory Programming Course.","We evaluate ChatGPT's capabilities by comparing its performance with that of human TAs in some TA functions.","The TA functions which we focus on include (1) solving programming assignments, (2) grading student code submissions, and (3) providing feedback to undergraduate students in an introductory programming course.","Firstly, we investigate how closely ChatGPT's solutions align with those submitted by students.","This analysis goes beyond code correctness and also considers code quality.","Secondly, we assess ChatGPT's proficiency in grading student code submissions using a given grading rubric and compare its performance with the grades assigned by human TAs.","Thirdly, we analyze the quality and relevance of the feedback provided by ChatGPT.","This evaluation considers how well ChatGPT addresses mistakes and offers suggestions for improvement in student solutions from both code correctness and code quality perspectives.","We conclude with a discussion on the implications of integrating ChatGPT into computing education for automated grading, personalized learning experiences, and instructional support."],"url":"http://arxiv.org/abs/2312.07343v1"}
{"created":"2023-12-12 15:04:44","title":"Expand-and-Quantize: Unsupervised Semantic Segmentation Using High-Dimensional Space and Product Quantization","abstract":"Unsupervised semantic segmentation (USS) aims to discover and recognize meaningful categories without any labels. For a successful USS, two key abilities are required: 1) information compression and 2) clustering capability. Previous methods have relied on feature dimension reduction for information compression, however, this approach may hinder the process of clustering. In this paper, we propose a novel USS framework called Expand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines the benefits of high-dimensional spaces for better clustering and product quantization for effective information compression. Our extensive experiments demonstrate that EQUSS achieves state-of-the-art results on three standard benchmarks. In addition, we analyze the entropy of USS features, which is the first step towards understanding USS from the perspective of information theory.","sentences":["Unsupervised semantic segmentation (USS) aims to discover and recognize meaningful categories without any labels.","For a successful USS, two key abilities are required: 1) information compression and 2) clustering capability.","Previous methods have relied on feature dimension reduction for information compression, however, this approach may hinder the process of clustering.","In this paper, we propose a novel USS framework called Expand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines the benefits of high-dimensional spaces for better clustering and product quantization for effective information compression.","Our extensive experiments demonstrate that EQUSS achieves state-of-the-art results on three standard benchmarks.","In addition, we analyze the entropy of USS features, which is the first step towards understanding USS from the perspective of information theory."],"url":"http://arxiv.org/abs/2312.07342v1"}
{"created":"2023-12-12 15:01:17","title":"MuscleVAE: Model-Based Controllers of Muscle-Actuated Characters","abstract":"In this paper, we present a simulation and control framework for generating biomechanically plausible motion for muscle-actuated characters. We incorporate a fatigue dynamics model, the 3CC-r model, into the widely-adopted Hill-type muscle model to simulate the development and recovery of fatigue in muscles, which creates a natural evolution of motion style caused by the accumulation of fatigue from prolonged activities. To address the challenging problem of controlling a musculoskeletal system with high degrees of freedom, we propose a novel muscle-space control strategy based on PD control. Our simulation and control framework facilitates the training of a generative model for muscle-based motion control, which we refer to as MuscleVAE. By leveraging the variational autoencoders (VAEs), MuscleVAE is capable of learning a rich and flexible latent representation of skills from a large unstructured motion dataset, encoding not only motion features but also muscle control and fatigue properties. We demonstrate that the MuscleVAE model can be efficiently trained using a model-based approach, resulting in the production of high-fidelity motions and enabling a variety of downstream tasks.","sentences":["In this paper, we present a simulation and control framework for generating biomechanically plausible motion for muscle-actuated characters.","We incorporate a fatigue dynamics model, the 3CC-r model, into the widely-adopted Hill-type muscle model to simulate the development and recovery of fatigue in muscles, which creates a natural evolution of motion style caused by the accumulation of fatigue from prolonged activities.","To address the challenging problem of controlling a musculoskeletal system with high degrees of freedom, we propose a novel muscle-space control strategy based on PD control.","Our simulation and control framework facilitates the training of a generative model for muscle-based motion control, which we refer to as MuscleVAE.","By leveraging the variational autoencoders (VAEs), MuscleVAE is capable of learning a rich and flexible latent representation of skills from a large unstructured motion dataset, encoding not only motion features but also muscle control and fatigue properties.","We demonstrate that the MuscleVAE model can be efficiently trained using a model-based approach, resulting in the production of high-fidelity motions and enabling a variety of downstream tasks."],"url":"http://arxiv.org/abs/2312.07340v1"}
{"created":"2023-12-12 14:58:08","title":"Self-supervised Adaptive Pre-training of Multilingual Speech Models for Language and Dialect Identification","abstract":"Pre-trained Transformer-based speech models have shown striking performance when fine-tuned on various downstream tasks such as automatic speech recognition and spoken language identification (SLID). However, the problem of domain mismatch remains a challenge in this area, where the domain of the pre-training data might differ from that of the downstream labeled data used for fine-tuning. In multilingual tasks such as SLID, the pre-trained speech model may not support all the languages in the downstream task. To address this challenge, we propose self-supervised adaptive pre-training (SAPT) to adapt the pre-trained model to the target domain and languages of the downstream task. We apply SAPT to the XLSR-128 model and investigate the effectiveness of this approach for the SLID task. First, we demonstrate that SAPT improves XLSR performance on the FLEURS benchmark with substantial gains up to 40.1% for under-represented languages. Second, we apply SAPT on four different datasets in a few-shot learning setting, showing that our approach improves the sample efficiency of XLSR during fine-tuning. Our experiments provide strong empirical evidence that continual adaptation via self-supervision improves downstream performance for multilingual speech models.","sentences":["Pre-trained Transformer-based speech models have shown striking performance when fine-tuned on various downstream tasks such as automatic speech recognition and spoken language identification (SLID).","However, the problem of domain mismatch remains a challenge in this area, where the domain of the pre-training data might differ from that of the downstream labeled data used for fine-tuning.","In multilingual tasks such as SLID, the pre-trained speech model may not support all the languages in the downstream task.","To address this challenge, we propose self-supervised adaptive pre-training (SAPT) to adapt the pre-trained model to the target domain and languages of the downstream task.","We apply SAPT to the XLSR-128 model and investigate the effectiveness of this approach for the SLID task.","First, we demonstrate that SAPT improves XLSR performance on the FLEURS benchmark with substantial gains up to 40.1% for under-represented languages.","Second, we apply SAPT on four different datasets in a few-shot learning setting, showing that our approach improves the sample efficiency of XLSR during fine-tuning.","Our experiments provide strong empirical evidence that continual adaptation via self-supervision improves downstream performance for multilingual speech models."],"url":"http://arxiv.org/abs/2312.07338v1"}
{"created":"2023-12-12 14:55:49","title":"RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose Estimation in Degenerated Environments","abstract":"The typical point cloud sampling methods used in state estimation for mobile robots preserve a high level of point redundancy. The point redundancy slows down the estimation pipeline and can make real-time estimation drift in geometrically symmetrical and structureless environments. We propose a novel point cloud sampling method that is capable of lowering the effects of geometrical degeneracies by minimizing redundancy within the cloud. The proposed method is an alternative to the commonly used sparsification methods that normalize the density of points to comply with the constraints on the real-time capabilities of a robot. In contrast to density normalization, our method builds on the fact that linear and planar surfaces contain a high level of redundancy propagated into iterative estimation pipelines. We define the concept of gradient flow quantifying the surface underlying a point. We also show that maximizing the entropy of the gradient flow minimizes point redundancy for robot ego-motion estimation. We integrate the proposed method into the point-based KISS-ICP and feature-based LOAM odometry pipelines and evaluate it experimentally on KITTI, Hilti-Oxford, and custom datasets from multirotor UAVs. The experiments show that the proposed sampling technique outperforms state-of-the-art methods in well-conditioned as well as in geometrically-degenerated settings, in both accuracy and speed.","sentences":["The typical point cloud sampling methods used in state estimation for mobile robots preserve a high level of point redundancy.","The point redundancy slows down the estimation pipeline and can make real-time estimation drift in geometrically symmetrical and structureless environments.","We propose a novel point cloud sampling method that is capable of lowering the effects of geometrical degeneracies by minimizing redundancy within the cloud.","The proposed method is an alternative to the commonly used sparsification methods that normalize the density of points to comply with the constraints on the real-time capabilities of a robot.","In contrast to density normalization, our method builds on the fact that linear and planar surfaces contain a high level of redundancy propagated into iterative estimation pipelines.","We define the concept of gradient flow quantifying the surface underlying a point.","We also show that maximizing the entropy of the gradient flow minimizes point redundancy for robot ego-motion estimation.","We integrate the proposed method into the point-based KISS-ICP and feature-based LOAM odometry pipelines and evaluate it experimentally on KITTI, Hilti-Oxford, and custom datasets from multirotor UAVs.","The experiments show that the proposed sampling technique outperforms state-of-the-art methods in well-conditioned as well as in geometrically-degenerated settings, in both accuracy and speed."],"url":"http://arxiv.org/abs/2312.07337v1"}
{"created":"2023-12-12 14:53:18","title":"Momentum Particle Maximum Likelihood","abstract":"Maximum likelihood estimation (MLE) of latent variable models is often recast as an optimization problem over the extended space of parameters and probability distributions. For example, the Expectation Maximization (EM) algorithm can be interpreted as coordinate descent applied to a suitable free energy functional over this space. Recently, this perspective has been combined with insights from optimal transport and Wasserstein gradient flows to develop particle-based algorithms applicable to wider classes of models than standard EM.   Drawing inspiration from prior works which interpret `momentum-enriched' optimisation algorithms as discretizations of ordinary differential equations, we propose an analogous dynamical systems-inspired approach to minimizing the free energy functional over the extended space of parameters and probability distributions. The result is a dynamic system that blends elements of Nesterov's Accelerated Gradient method, the underdamped Langevin diffusion, and particle methods.   Under suitable assumptions, we establish quantitative convergence of the proposed system to the unique minimiser of the functional in continuous time. We then propose a numerical discretization of this system which enables its application to parameter estimation in latent variable models. Through numerical experiments, we demonstrate that the resulting algorithm converges faster than existing methods and compares favourably with other (approximate) MLE algorithms.","sentences":["Maximum likelihood estimation (MLE) of latent variable models is often recast as an optimization problem over the extended space of parameters and probability distributions.","For example, the Expectation Maximization (EM) algorithm can be interpreted as coordinate descent applied to a suitable free energy functional over this space.","Recently, this perspective has been combined with insights from optimal transport and Wasserstein gradient flows to develop particle-based algorithms applicable to wider classes of models than standard EM.   ","Drawing inspiration from prior works which interpret `momentum-enriched' optimisation algorithms as discretizations of ordinary differential equations, we propose an analogous dynamical systems-inspired approach to minimizing the free energy functional over the extended space of parameters and probability distributions.","The result is a dynamic system that blends elements of Nesterov's Accelerated Gradient method, the underdamped Langevin diffusion, and particle methods.   ","Under suitable assumptions, we establish quantitative convergence of the proposed system to the unique minimiser of the functional in continuous time.","We then propose a numerical discretization of this system which enables its application to parameter estimation in latent variable models.","Through numerical experiments, we demonstrate that the resulting algorithm converges faster than existing methods and compares favourably with other (approximate) MLE algorithms."],"url":"http://arxiv.org/abs/2312.07335v1"}
