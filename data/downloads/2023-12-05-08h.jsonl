{"created":"2023-12-04 18:59:59","title":"PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness","abstract":"We propose the task of Panoptic Scene Completion (PSC) which extends the recently popular Semantic Scene Completion (SSC) task with instance-level information to produce a richer understanding of the 3D scene. Our PSC proposal utilizes a hybrid mask-based technique on the non-empty voxels from sparse multi-scale completions. Whereas the SSC literature overlooks uncertainty which is critical for robotics applications, we instead propose an efficient ensembling to estimate both voxel-wise and instance-wise uncertainties along PSC. This is achieved by building on a multi-input multi-output (MIMO) strategy, while improving performance and yielding better uncertainty for little additional compute. Additionally, we introduce a technique to aggregate permutation-invariant mask predictions. Our experiments demonstrate that our method surpasses all baselines in both Panoptic Scene Completion and uncertainty estimation on three large-scale autonomous driving datasets. Our code and data are available at https://astra-vision.github.io/PaSCo .","sentences":["We propose the task of Panoptic Scene Completion (PSC) which extends the recently popular Semantic Scene Completion (SSC) task with instance-level information to produce a richer understanding of the 3D scene.","Our PSC proposal utilizes a hybrid mask-based technique on the non-empty voxels from sparse multi-scale completions.","Whereas the SSC literature overlooks uncertainty which is critical for robotics applications, we instead propose an efficient ensembling to estimate both voxel-wise and instance-wise uncertainties along PSC.","This is achieved by building on a multi-input multi-output (MIMO) strategy, while improving performance and yielding better uncertainty for little additional compute.","Additionally, we introduce a technique to aggregate permutation-invariant mask predictions.","Our experiments demonstrate that our method surpasses all baselines in both Panoptic Scene Completion and uncertainty estimation on three large-scale autonomous driving datasets.","Our code and data are available at https://astra-vision.github.io/PaSCo ."],"url":"http://arxiv.org/abs/2312.02158v1"}
{"created":"2023-12-04 18:59:58","title":"Mesh-Guided Neural Implicit Field Editing","abstract":"Neural implicit fields have emerged as a powerful 3D representation for reconstructing and rendering photo-realistic views, yet they possess limited editability. Conversely, explicit 3D representations, such as polygonal meshes, offer ease of editing but may not be as suitable for rendering high-quality novel views. To harness the strengths of both representations, we propose a new approach that employs a mesh as a guiding mechanism in editing the neural radiance field. We first introduce a differentiable method using marching tetrahedra for polygonal mesh extraction from the neural implicit field and then design a differentiable color extractor to assign colors obtained from the volume renderings to this extracted mesh. This differentiable colored mesh allows gradient back-propagation from the explicit mesh to the implicit fields, empowering users to easily manipulate the geometry and color of neural implicit fields. To enhance user control from coarse-grained to fine-grained levels, we introduce an octree-based structure into its optimization. This structure prioritizes the edited regions and the surface part, making our method achieve fine-grained edits to the neural implicit field and accommodate various user modifications, including object additions, component removals, specific area deformations, and adjustments to local and global colors. Through extensive experiments involving diverse scenes and editing operations, we have demonstrated the capabilities and effectiveness of our method. Our project page is: \\url{https://cassiepython.github.io/MNeuEdit/}","sentences":["Neural implicit fields have emerged as a powerful 3D representation for reconstructing and rendering photo-realistic views, yet they possess limited editability.","Conversely, explicit 3D representations, such as polygonal meshes, offer ease of editing but may not be as suitable for rendering high-quality novel views.","To harness the strengths of both representations, we propose a new approach that employs a mesh as a guiding mechanism in editing the neural radiance field.","We first introduce a differentiable method using marching tetrahedra for polygonal mesh extraction from the neural implicit field and then design a differentiable color extractor to assign colors obtained from the volume renderings to this extracted mesh.","This differentiable colored mesh allows gradient back-propagation from the explicit mesh to the implicit fields, empowering users to easily manipulate the geometry and color of neural implicit fields.","To enhance user control from coarse-grained to fine-grained levels, we introduce an octree-based structure into its optimization.","This structure prioritizes the edited regions and the surface part, making our method achieve fine-grained edits to the neural implicit field and accommodate various user modifications, including object additions, component removals, specific area deformations, and adjustments to local and global colors.","Through extensive experiments involving diverse scenes and editing operations, we have demonstrated the capabilities and effectiveness of our method.","Our project page is: \\url{https://cassiepython.github.io/MNeuEdit/}"],"url":"http://arxiv.org/abs/2312.02157v1"}
{"created":"2023-12-04 18:59:55","title":"GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis","abstract":"We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.","sentences":["We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner.","The proposed method enables 2K-resolution rendering under a sparse-view camera setting.","Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization.","To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space.","The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed."],"url":"http://arxiv.org/abs/2312.02155v1"}
{"created":"2023-12-04 18:59:55","title":"Latent Feature-Guided Diffusion Models for Shadow Removal","abstract":"Recovering textures under shadows has remained a challenging problem due to the difficulty of inferring shadow-free scenes from shadow images. In this paper, we propose the use of diffusion models as they offer a promising approach to gradually refine the details of shadow regions during the diffusion process. Our method improves this process by conditioning on a learned latent feature space that inherits the characteristics of shadow-free images, thus avoiding the limitation of conventional methods that condition on degraded images only. Additionally, we propose to alleviate potential local optima during training by fusing noise features with the diffusion network. We demonstrate the effectiveness of our approach which outperforms the previous best method by 13% in terms of RMSE on the AISTD dataset. Further, we explore instance-level shadow removal, where our model outperforms the previous best method by 82% in terms of RMSE on the DESOBA dataset.","sentences":["Recovering textures under shadows has remained a challenging problem due to the difficulty of inferring shadow-free scenes from shadow images.","In this paper, we propose the use of diffusion models as they offer a promising approach to gradually refine the details of shadow regions during the diffusion process.","Our method improves this process by conditioning on a learned latent feature space that inherits the characteristics of shadow-free images, thus avoiding the limitation of conventional methods that condition on degraded images only.","Additionally, we propose to alleviate potential local optima during training by fusing noise features with the diffusion network.","We demonstrate the effectiveness of our approach which outperforms the previous best method by 13% in terms of RMSE on the AISTD dataset.","Further, we explore instance-level shadow removal, where our model outperforms the previous best method by 82% in terms of RMSE on the DESOBA dataset."],"url":"http://arxiv.org/abs/2312.02156v1"}
{"created":"2023-12-04 18:59:50","title":"Aligning and Prompting Everything All at Once for Universal Visual Perception","abstract":"Vision foundation models have been explored recently to build general-purpose vision systems. However, predominant paradigms, driven by casting instance-level tasks as an object-word alignment, bring heavy cross-modality interaction, which is not effective in prompting object detection and visual grounding. Another line of work that focuses on pixel-level tasks often encounters a large annotation gap of things and stuff, and suffers from mutual interference between foreground-object and background-class segmentation. In stark contrast to the prevailing methods, we present APE, a universal visual perception model for aligning and prompting everything all at once in an image to perform diverse tasks, i.e., detection, segmentation, and grounding, as an instance-level sentence-object matching paradigm. Specifically, APE advances the convergence of detection and grounding by reformulating language-guided grounding as open-vocabulary detection, which efficiently scales up model prompting to thousands of category vocabularies and region descriptions while maintaining the effectiveness of cross-modality fusion. To bridge the granularity gap of different pixel-level tasks, APE equalizes semantic and panoptic segmentation to proxy instance learning by considering any isolated regions as individual instances. APE aligns vision and language representation on broad data with natural and challenging characteristics all at once without task-specific fine-tuning. The extensive experiments on over 160 datasets demonstrate that, with only one-suit of weights, APE outperforms (or is on par with) the state-of-the-art models, proving that an effective yet universal perception for anything aligning and prompting is indeed feasible. Codes and trained models are released at https://github.com/shenyunhang/APE.","sentences":["Vision foundation models have been explored recently to build general-purpose vision systems.","However, predominant paradigms, driven by casting instance-level tasks as an object-word alignment, bring heavy cross-modality interaction, which is not effective in prompting object detection and visual grounding.","Another line of work that focuses on pixel-level tasks often encounters a large annotation gap of things and stuff, and suffers from mutual interference between foreground-object and background-class segmentation.","In stark contrast to the prevailing methods, we present APE, a universal visual perception model for aligning and prompting everything all at once in an image to perform diverse tasks, i.e., detection, segmentation, and grounding, as an instance-level sentence-object matching paradigm.","Specifically, APE advances the convergence of detection and grounding by reformulating language-guided grounding as open-vocabulary detection, which efficiently scales up model prompting to thousands of category vocabularies and region descriptions while maintaining the effectiveness of cross-modality fusion.","To bridge the granularity gap of different pixel-level tasks, APE equalizes semantic and panoptic segmentation to proxy instance learning by considering any isolated regions as individual instances.","APE aligns vision and language representation on broad data with natural and challenging characteristics all at once without task-specific fine-tuning.","The extensive experiments on over 160 datasets demonstrate that, with only one-suit of weights, APE outperforms (or is on par with) the state-of-the-art models, proving that an effective yet universal perception for anything aligning and prompting is indeed feasible.","Codes and trained models are released at https://github.com/shenyunhang/APE."],"url":"http://arxiv.org/abs/2312.02153v1"}
{"created":"2023-12-04 18:59:44","title":"Steerers: A framework for rotation equivariant keypoint descriptors","abstract":"Image keypoint descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction. However, descriptions output by learned descriptors are typically not robust to camera rotation. While they can be made more robust by, e.g., data augmentation, this degrades performance on upright images. Another approach is test-time augmentation, which incurs a significant increase in runtime. We instead learn a linear transform in description space that encodes rotations of the input image. We call this linear transform a steerer since it allows us to transform the descriptions as if the image was rotated. From representation theory we know all possible steerers for the rotation group. Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize a descriptor given a fixed steerer. We perform experiments in all of these three settings and obtain state-of-the-art results on the rotation invariant image matching benchmarks AIMS and Roto-360. We publish code and model weights at github.com/georg-bn/rotation-steerers.","sentences":["Image keypoint descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction.","However, descriptions output by learned descriptors are typically not robust to camera rotation.","While they can be made more robust by, e.g., data augmentation, this degrades performance on upright images.","Another approach is test-time augmentation, which incurs a significant increase in runtime.","We instead learn a linear transform in description space that encodes rotations of the input image.","We call this linear transform a steerer since it allows us to transform the descriptions as if the image was rotated.","From representation theory we know all possible steerers for the rotation group.","Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize a descriptor given a fixed steerer.","We perform experiments in all of these three settings and obtain state-of-the-art results on the rotation invariant image matching benchmarks AIMS and Roto-360.","We publish code and model weights at github.com/georg-bn/rotation-steerers."],"url":"http://arxiv.org/abs/2312.02152v1"}
{"created":"2023-12-04 18:59:36","title":"Guarding Barlow Twins Against Overfitting with Mixed Samples","abstract":"Self-supervised Learning (SSL) aims to learn transferable feature representations for downstream applications without relying on labeled data. The Barlow Twins algorithm, renowned for its widespread adoption and straightforward implementation compared to its counterparts like contrastive learning methods, minimizes feature redundancy while maximizing invariance to common corruptions. Optimizing for the above objective forces the network to learn useful representations, while avoiding noisy or constant features, resulting in improved downstream task performance with limited adaptation. Despite Barlow Twins' proven effectiveness in pre-training, the underlying SSL objective can inadvertently cause feature overfitting due to the lack of strong interaction between the samples unlike the contrastive learning approaches. From our experiments, we observe that optimizing for the Barlow Twins objective doesn't necessarily guarantee sustained improvements in representation quality beyond a certain pre-training phase, and can potentially degrade downstream performance on some datasets. To address this challenge, we introduce Mixed Barlow Twins, which aims to improve sample interaction during Barlow Twins training via linearly interpolated samples. This results in an additional regularization term to the original Barlow Twins objective, assuming linear interpolation in the input space translates to linearly interpolated features in the feature space. Pre-training with this regularization effectively mitigates feature overfitting and further enhances the downstream performance on CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets. The code and checkpoints are available at: https://github.com/wgcban/mix-bt.git","sentences":["Self-supervised Learning (SSL) aims to learn transferable feature representations for downstream applications without relying on labeled data.","The Barlow Twins algorithm, renowned for its widespread adoption and straightforward implementation compared to its counterparts like contrastive learning methods, minimizes feature redundancy while maximizing invariance to common corruptions.","Optimizing for the above objective forces the network to learn useful representations, while avoiding noisy or constant features, resulting in improved downstream task performance with limited adaptation.","Despite Barlow Twins' proven effectiveness in pre-training, the underlying SSL objective can inadvertently cause feature overfitting due to the lack of strong interaction between the samples unlike the contrastive learning approaches.","From our experiments, we observe that optimizing for the Barlow Twins objective doesn't necessarily guarantee sustained improvements in representation quality beyond a certain pre-training phase, and can potentially degrade downstream performance on some datasets.","To address this challenge, we introduce Mixed Barlow Twins, which aims to improve sample interaction during Barlow Twins training via linearly interpolated samples.","This results in an additional regularization term to the original Barlow Twins objective, assuming linear interpolation in the input space translates to linearly interpolated features in the feature space.","Pre-training with this regularization effectively mitigates feature overfitting and further enhances the downstream performance on CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets.","The code and checkpoints are available at: https://github.com/wgcban/mix-bt.git"],"url":"http://arxiv.org/abs/2312.02151v1"}
{"created":"2023-12-04 18:59:32","title":"Readout Guidance: Learning Control from Diffusion Features","abstract":"We present Readout Guidance, a method for controlling text-to-image diffusion models with learned signals. Readout Guidance uses readout heads, lightweight networks trained to extract signals from the features of a pre-trained, frozen diffusion model at every timestep. These readouts can encode single-image properties, such as pose, depth, and edges; or higher-order properties that relate multiple images, such as correspondence and appearance similarity. Furthermore, by comparing the readout estimates to a user-defined target, and back-propagating the gradient through the readout head, these estimates can be used to guide the sampling process. Compared to prior methods for conditional generation, Readout Guidance requires significantly fewer added parameters and training samples, and offers a convenient and simple recipe for reproducing different forms of conditional control under a single framework, with a single architecture and sampling procedure. We showcase these benefits in the applications of drag-based manipulation, identity-consistent generation, and spatially aligned control. Project page: https://readout-guidance.github.io.","sentences":["We present Readout Guidance, a method for controlling text-to-image diffusion models with learned signals.","Readout Guidance uses readout heads, lightweight networks trained to extract signals from the features of a pre-trained, frozen diffusion model at every timestep.","These readouts can encode single-image properties, such as pose, depth, and edges; or higher-order properties that relate multiple images, such as correspondence and appearance similarity.","Furthermore, by comparing the readout estimates to a user-defined target, and back-propagating the gradient through the readout head, these estimates can be used to guide the sampling process.","Compared to prior methods for conditional generation, Readout Guidance requires significantly fewer added parameters and training samples, and offers a convenient and simple recipe for reproducing different forms of conditional control under a single framework, with a single architecture and sampling procedure.","We showcase these benefits in the applications of drag-based manipulation, identity-consistent generation, and spatially aligned control.","Project page: https://readout-guidance.github.io."],"url":"http://arxiv.org/abs/2312.02150v1"}
{"created":"2023-12-04 18:59:25","title":"Generative Powers of Ten","abstract":"We present a method that uses a text-to-image model to generate consistent content across multiple image scales, enabling extreme semantic zooms into a scene, e.g., ranging from a wide-angle landscape view of a forest to a macro shot of an insect sitting on one of the tree branches. We achieve this through a joint multi-scale diffusion sampling approach that encourages consistency across different scales while preserving the integrity of each individual sampling process. Since each generated scale is guided by a different text prompt, our method enables deeper levels of zoom than traditional super-resolution methods that may struggle to create new contextual structure at vastly different scales. We compare our method qualitatively with alternative techniques in image super-resolution and outpainting, and show that our method is most effective at generating consistent multi-scale content.","sentences":["We present a method that uses a text-to-image model to generate consistent content across multiple image scales, enabling extreme semantic zooms into a scene, e.g., ranging from a wide-angle landscape view of a forest to a macro shot of an insect sitting on one of the tree branches.","We achieve this through a joint multi-scale diffusion sampling approach that encourages consistency across different scales while preserving the integrity of each individual sampling process.","Since each generated scale is guided by a different text prompt, our method enables deeper levels of zoom than traditional super-resolution methods that may struggle to create new contextual structure at vastly different scales.","We compare our method qualitatively with alternative techniques in image super-resolution and outpainting, and show that our method is most effective at generating consistent multi-scale content."],"url":"http://arxiv.org/abs/2312.02149v1"}
{"created":"2023-12-04 18:59:20","title":"Rejuvenating image-GPT as Strong Visual Representation Learners","abstract":"This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement of D-iGPT is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT achieves 89.5\\% top-1 accuracy with a vanilla ViT-Large model. This model also shows strong generalization on the downstream task and robustness on out-of-distribution samples. Code is avaiable at \\href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}.","sentences":["This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict next pixels for visual representation learning.","Two simple yet essential changes are made.","First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content.","Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens.","This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP.","We introduce this novel approach as D-iGPT.","Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement of D-iGPT is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT achieves 89.5\\% top-1 accuracy with a vanilla ViT-Large model.","This model also shows strong generalization on the downstream task and robustness on out-of-distribution samples.","Code is avaiable at \\href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}."],"url":"http://arxiv.org/abs/2312.02147v1"}
{"created":"2023-12-04 18:59:19","title":"Learning Polynomial Problems with $SL(2,\\mathbb{R})$ Equivariance","abstract":"Optimizing and certifying the positivity of polynomials are fundamental primitives across mathematics and engineering applications, from dynamical systems to operations research. However, solving these problems in practice requires large semidefinite programs, with poor scaling in dimension and degree. In this work, we demonstrate for the first time that neural networks can effectively solve such problems in a data-driven fashion, achieving tenfold speedups while retaining high accuracy. Moreover, we observe that these polynomial learning problems are equivariant to the non-compact group $SL(2,\\mathbb{R})$, which consists of area-preserving linear transformations. We therefore adapt our learning pipelines to accommodate this structure, including data augmentation, a new $SL(2,\\mathbb{R})$-equivariant architecture, and an architecture equivariant with respect to its maximal compact subgroup, $SO(2, \\mathbb{R})$. Surprisingly, the most successful approaches in practice do not enforce equivariance to the entire group, which we prove arises from an unusual lack of architecture universality for $SL(2,\\mathbb{R})$ in particular. A consequence of this result, which is of independent interest, is that there exists an equivariant function for which there is no sequence of equivariant polynomials multiplied by arbitrary invariants that approximates the original function. This is a rare example of a symmetric problem where data augmentation outperforms a fully equivariant architecture, and provides interesting lessons in both theory and practice for other problems with non-compact symmetries.","sentences":["Optimizing and certifying the positivity of polynomials are fundamental primitives across mathematics and engineering applications, from dynamical systems to operations research.","However, solving these problems in practice requires large semidefinite programs, with poor scaling in dimension and degree.","In this work, we demonstrate for the first time that neural networks can effectively solve such problems in a data-driven fashion, achieving tenfold speedups while retaining high accuracy.","Moreover, we observe that these polynomial learning problems are equivariant to the non-compact group $SL(2,\\mathbb{R})$, which consists of area-preserving linear transformations.","We therefore adapt our learning pipelines to accommodate this structure, including data augmentation, a new $SL(2,\\mathbb{R})$-equivariant architecture, and an architecture equivariant with respect to its maximal compact subgroup, $SO(2, \\mathbb{R})$. Surprisingly, the most successful approaches in practice do not enforce equivariance to the entire group, which we prove arises from an unusual lack of architecture universality for $SL(2,\\mathbb{R})$ in particular.","A consequence of this result, which is of independent interest, is that there exists an equivariant function for which there is no sequence of equivariant polynomials multiplied by arbitrary invariants that approximates the original function.","This is a rare example of a symmetric problem where data augmentation outperforms a fully equivariant architecture, and provides interesting lessons in both theory and practice for other problems with non-compact symmetries."],"url":"http://arxiv.org/abs/2312.02146v1"}
{"created":"2023-12-04 18:59:13","title":"Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation","abstract":"Monocular depth estimation is a fundamental computer vision task. Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough. The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures. Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains. This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation. We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge. The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data. It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases. Project page: https://marigoldmonodepth.github.io.","sentences":["Monocular depth estimation is a fundamental computer vision task.","Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough.","The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures.","Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains.","This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation.","We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge.","The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data.","It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases.","Project page: https://marigoldmonodepth.github.io."],"url":"http://arxiv.org/abs/2312.02145v1"}
{"created":"2023-12-04 18:59:02","title":"Optimizing Camera Configurations for Multi-View Pedestrian Detection","abstract":"Jointly considering multiple camera views (multi-view) is very effective for pedestrian detection under occlusion. For such multi-view systems, it is critical to have well-designed camera configurations, including camera locations, directions, and fields-of-view (FoVs). Usually, these configurations are crafted based on human experience or heuristics. In this work, we present a novel solution that features a transformer-based camera configuration generator. Using reinforcement learning, this generator autonomously explores vast combinations within the action space and searches for configurations that give the highest detection accuracy according to the training dataset. The generator learns advanced techniques like maximizing coverage, minimizing occlusion, and promoting collaboration. Across multiple simulation scenarios, the configurations generated by our transformer-based model consistently outperform random search, heuristic-based methods, and configurations designed by human experts, shedding light on future camera layout optimization.","sentences":["Jointly considering multiple camera views (multi-view) is very effective for pedestrian detection under occlusion.","For such multi-view systems, it is critical to have well-designed camera configurations, including camera locations, directions, and fields-of-view (FoVs).","Usually, these configurations are crafted based on human experience or heuristics.","In this work, we present a novel solution that features a transformer-based camera configuration generator.","Using reinforcement learning, this generator autonomously explores vast combinations within the action space and searches for configurations that give the highest detection accuracy according to the training dataset.","The generator learns advanced techniques like maximizing coverage, minimizing occlusion, and promoting collaboration.","Across multiple simulation scenarios, the configurations generated by our transformer-based model consistently outperform random search, heuristic-based methods, and configurations designed by human experts, shedding light on future camera layout optimization."],"url":"http://arxiv.org/abs/2312.02144v1"}
{"created":"2023-12-04 18:58:57","title":"Competition-Level Problems Are Effective Evaluators of LLMs","abstract":"Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future.","sentences":["Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently.","This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills.","We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered.","Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems.","We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges.","Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future."],"url":"http://arxiv.org/abs/2312.02143v1"}
{"created":"2023-12-04 18:58:40","title":"Object Recognition as Next Token Prediction","abstract":"We present an approach to pose object recognition as next token prediction. The idea is to apply a language decoder that auto-regressively predicts the text tokens from image embeddings to form labels. To ground this prediction process in auto-regression, we customize a non-causal attention mask for the decoder, incorporating two key features: modeling tokens from different labels to be independent, and treating image tokens as a prefix. This masking mechanism inspires an efficient method - one-shot sampling - to simultaneously sample tokens of multiple labels in parallel and rank generated labels by their probabilities during inference. To further enhance the efficiency, we propose a simple strategy to construct a compact decoder by simply discarding the intermediate blocks of a pretrained language model. This approach yields a decoder that matches the full model's performance while being notably more efficient. The code is available at https://github.com/kaiyuyue/nxtp","sentences":["We present an approach to pose object recognition as next token prediction.","The idea is to apply a language decoder that auto-regressively predicts the text tokens from image embeddings to form labels.","To ground this prediction process in auto-regression, we customize a non-causal attention mask for the decoder, incorporating two key features: modeling tokens from different labels to be independent, and treating image tokens as a prefix.","This masking mechanism inspires an efficient method - one-shot sampling - to simultaneously sample tokens of multiple labels in parallel and rank generated labels by their probabilities during inference.","To further enhance the efficiency, we propose a simple strategy to construct a compact decoder by simply discarding the intermediate blocks of a pretrained language model.","This approach yields a decoder that matches the full model's performance while being notably more efficient.","The code is available at https://github.com/kaiyuyue/nxtp"],"url":"http://arxiv.org/abs/2312.02142v1"}
{"created":"2023-12-04 18:58:20","title":"iMatching: Imperative Correspondence Learning","abstract":"Learning feature correspondence is a foundational task in computer vision, holding immense importance for downstream applications such as visual odometry and 3D reconstruction. Despite recent progress in data-driven models, feature correspondence learning is still limited by the lack of accurate per-pixel correspondence labels. To overcome this difficulty, we introduce a new self-supervised scheme, imperative learning (IL), for training feature correspondence. It enables correspondence learning on arbitrary uninterrupted videos without any camera pose or depth labels, heralding a new era for self-supervised correspondence learning. Specifically, we formulated the problem of correspondence learning as a bilevel optimization, which takes the reprojection error from bundle adjustment as a supervisory signal for the model. To avoid large memory and computation overhead, we leverage the stationary point to effectively back-propagate the implicit gradients through bundle adjustment. Through extensive experiments, we demonstrate superior performance on tasks including feature matching and pose estimation, in which we obtained an average of 30% accuracy gain over the state-of-the-art matching models.","sentences":["Learning feature correspondence is a foundational task in computer vision, holding immense importance for downstream applications such as visual odometry and 3D reconstruction.","Despite recent progress in data-driven models, feature correspondence learning is still limited by the lack of accurate per-pixel correspondence labels.","To overcome this difficulty, we introduce a new self-supervised scheme, imperative learning (IL), for training feature correspondence.","It enables correspondence learning on arbitrary uninterrupted videos without any camera pose or depth labels, heralding a new era for self-supervised correspondence learning.","Specifically, we formulated the problem of correspondence learning as a bilevel optimization, which takes the reprojection error from bundle adjustment as a supervisory signal for the model.","To avoid large memory and computation overhead, we leverage the stationary point to effectively back-propagate the implicit gradients through bundle adjustment.","Through extensive experiments, we demonstrate superior performance on tasks including feature matching and pose estimation, in which we obtained an average of 30% accuracy gain over the state-of-the-art matching models."],"url":"http://arxiv.org/abs/2312.02141v1"}
{"created":"2023-12-04 18:57:01","title":"DiffiT: Diffusion Vision Transformers for Image Generation","abstract":"Diffusion models with their powerful expressivity and high sample quality have enabled many new applications and use-cases in various domains. For sample generation, these models rely on a denoising neural network that generates images by iterative denoising. Yet, the role of denoising network architecture is not well-studied with most efforts relying on convolutional residual U-Nets. In this paper, we study the effectiveness of vision transformers in diffusion-based generative learning. Specifically, we propose a new model, denoted as Diffusion Vision Transformers (DiffiT), which consists of a hybrid hierarchical architecture with a U-shaped encoder and decoder. We introduce a novel time-dependent self-attention module that allows attention layers to adapt their behavior at different stages of the denoising process in an efficient manner. We also introduce latent DiffiT which consists of transformer model with the proposed self-attention layers, for high-resolution image generation. Our results show that DiffiT is surprisingly effective in generating high-fidelity images, and it achieves state-of-the-art (SOTA) benchmarks on a variety of class-conditional and unconditional synthesis tasks. In the latent space, DiffiT achieves a new SOTA FID score of 1.73 on ImageNet-256 dataset. Repository: https://github.com/NVlabs/DiffiT","sentences":["Diffusion models with their powerful expressivity and high sample quality have enabled many new applications and use-cases in various domains.","For sample generation, these models rely on a denoising neural network that generates images by iterative denoising.","Yet, the role of denoising network architecture is not well-studied with most efforts relying on convolutional residual U-Nets.","In this paper, we study the effectiveness of vision transformers in diffusion-based generative learning.","Specifically, we propose a new model, denoted as Diffusion Vision Transformers (DiffiT), which consists of a hybrid hierarchical architecture with a U-shaped encoder and decoder.","We introduce a novel time-dependent self-attention module that allows attention layers to adapt their behavior at different stages of the denoising process in an efficient manner.","We also introduce latent DiffiT which consists of transformer model with the proposed self-attention layers, for high-resolution image generation.","Our results show that DiffiT is surprisingly effective in generating high-fidelity images, and it achieves state-of-the-art (SOTA) benchmarks on a variety of class-conditional and unconditional synthesis tasks.","In the latent space, DiffiT achieves a new SOTA FID score of 1.73 on ImageNet-256 dataset.","Repository: https://github.com/NVlabs/DiffiT"],"url":"http://arxiv.org/abs/2312.02139v1"}
{"created":"2023-12-04 18:56:22","title":"MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians","abstract":"Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality. However, this challenging problem requires accurate modeling of the contact between hands and objects. To capture grasps, existing methods use skeletons, meshes, or parametric models that can cause misalignments resulting in inaccurate contacts. We present MANUS, a method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians. We build a novel articulated 3D Gaussians representation that extends 3D Gaussian splatting for high-fidelity representation of articulating hands. Since our representation uses Gaussian primitives, it enables us to efficiently and accurately estimate contacts between the hand and the object. For the most accurate results, our method requires tens of camera views that current datasets do not provide. We therefore build MANUS-Grasps, a new dataset that contains hand-object grasps viewed from 53 cameras across 30+ scenes, 3 subjects, and comprising over 7M frames. In addition to extensive qualitative results, we also show that our method outperforms others on a quantitative contact evaluation method that uses paint transfer from the object to the hand.","sentences":["Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality.","However, this challenging problem requires accurate modeling of the contact between hands and objects.","To capture grasps, existing methods use skeletons, meshes, or parametric models that can cause misalignments resulting in inaccurate contacts.","We present MANUS, a method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians.","We build a novel articulated 3D Gaussians representation that extends 3D Gaussian splatting for high-fidelity representation of articulating hands.","Since our representation uses Gaussian primitives, it enables us to efficiently and accurately estimate contacts between the hand and the object.","For the most accurate results, our method requires tens of camera views that current datasets do not provide.","We therefore build MANUS-Grasps, a new dataset that contains hand-object grasps viewed from 53 cameras across 30+ scenes, 3 subjects, and comprising over 7M frames.","In addition to extensive qualitative results, we also show that our method outperforms others on a quantitative contact evaluation method that uses paint transfer from the object to the hand."],"url":"http://arxiv.org/abs/2312.02137v1"}
{"created":"2023-12-04 18:56:10","title":"BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation","abstract":"Generating large-scale 3D scenes cannot simply apply existing 3D object synthesis technique since 3D scenes usually hold complex spatial configurations and consist of a number of objects at varying scales. We thus propose a practical and efficient 3D representation that incorporates an equivariant radiance field with the guidance of a bird's-eye view (BEV) map. Concretely, objects of synthesized 3D scenes could be easily manipulated through steering the corresponding BEV maps. Moreover, by adequately incorporating positional encoding and low-pass filters into the generator, the representation becomes equivariant to the given BEV map. Such equivariance allows us to produce large-scale, even infinite-scale, 3D scenes via synthesizing local scenes and then stitching them with smooth consistency. Extensive experiments on 3D scene datasets demonstrate the effectiveness of our approach. Our project website is at https://zqh0253.github.io/BerfScene/.","sentences":["Generating large-scale 3D scenes cannot simply apply existing 3D object synthesis technique since 3D scenes usually hold complex spatial configurations and consist of a number of objects at varying scales.","We thus propose a practical and efficient 3D representation that incorporates an equivariant radiance field with the guidance of a bird's-eye view (BEV) map.","Concretely, objects of synthesized 3D scenes could be easily manipulated through steering the corresponding BEV maps.","Moreover, by adequately incorporating positional encoding and low-pass filters into the generator, the representation becomes equivariant to the given BEV map.","Such equivariance allows us to produce large-scale, even infinite-scale, 3D scenes via synthesizing local scenes and then stitching them with smooth consistency.","Extensive experiments on 3D scene datasets demonstrate the effectiveness of our approach.","Our project website is at https://zqh0253.github.io/BerfScene/."],"url":"http://arxiv.org/abs/2312.02136v1"}
{"created":"2023-12-04 18:55:48","title":"Fast View Synthesis of Casual Videos","abstract":"Novel view synthesis from an in-the-wild video is difficult due to challenges like scene dynamics and lack of parallax. While existing methods have shown promising results with implicit neural radiance fields, they are slow to train and render. This paper revisits explicit video representations to synthesize high-quality novel views from a monocular video efficiently. We treat static and dynamic video content separately. Specifically, we build a global static scene model using an extended plane-based scene representation to synthesize temporally coherent novel video. Our plane-based scene representation is augmented with spherical harmonics and displacement maps to capture view-dependent effects and model non-planar complex surface geometry. We opt to represent the dynamic content as per-frame point clouds for efficiency. While such representations are inconsistency-prone, minor temporal inconsistencies are perceptually masked due to motion. We develop a method to quickly estimate such a hybrid video representation and render novel views in real time. Our experiments show that our method can render high-quality novel views from an in-the-wild video with comparable quality to state-of-the-art methods while being 100x faster in training and enabling real-time rendering.","sentences":["Novel view synthesis from an in-the-wild video is difficult due to challenges like scene dynamics and lack of parallax.","While existing methods have shown promising results with implicit neural radiance fields, they are slow to train and render.","This paper revisits explicit video representations to synthesize high-quality novel views from a monocular video efficiently.","We treat static and dynamic video content separately.","Specifically, we build a global static scene model using an extended plane-based scene representation to synthesize temporally coherent novel video.","Our plane-based scene representation is augmented with spherical harmonics and displacement maps to capture view-dependent effects and model non-planar complex surface geometry.","We opt to represent the dynamic content as per-frame point clouds for efficiency.","While such representations are inconsistency-prone, minor temporal inconsistencies are perceptually masked due to motion.","We develop a method to quickly estimate such a hybrid video representation and render novel views in real time.","Our experiments show that our method can render high-quality novel views from an in-the-wild video with comparable quality to state-of-the-art methods while being 100x faster in training and enabling real-time rendering."],"url":"http://arxiv.org/abs/2312.02135v1"}
{"created":"2023-12-04 18:55:45","title":"GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians","abstract":"We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.","sentences":["We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video.","We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles.","Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations.","Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping.","Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings.","The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency."],"url":"http://arxiv.org/abs/2312.02134v1"}
{"created":"2023-12-04 18:55:35","title":"Style Aligned Image Generation via Shared Attention","abstract":"Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.","sentences":["Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts.","However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style.","In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images.","By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models.","This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation.","Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs."],"url":"http://arxiv.org/abs/2312.02133v1"}
{"created":"2023-12-04 18:54:34","title":"Hot PATE: Private Aggregation of Distributions for Diverse Task","abstract":"The Private Aggregation of Teacher Ensembles (PATE) framework~\\cite{PapernotAEGT:ICLR2017} is a versatile approach to privacy-preserving machine learning. In PATE, teacher models are trained on distinct portions of sensitive data, and their predictions are privately aggregated to label new training examples for a student model.   Until now, PATE has primarily been explored with classification-like tasks, where each example possesses a ground-truth label, and knowledge is transferred to the student by labeling public examples. Generative AI models, however, excel in open ended \\emph{diverse} tasks with multiple valid responses and scenarios that may not align with traditional labeled examples. Furthermore, the knowledge of models is often encapsulated in the response distribution itself and may be transferred from teachers to student in a more fluid way. We propose \\emph{hot PATE}, tailored for the diverse setting. In hot PATE, each teacher model produces a response distribution and the aggregation method must preserve both privacy and diversity of responses. We demonstrate, analytically and empirically, that hot PATE achieves privacy-utility tradeoffs that are comparable to, and in diverse settings, significantly surpass, the baseline ``cold'' PATE.","sentences":["The Private Aggregation of Teacher Ensembles (PATE) framework~\\cite{PapernotAEGT:","ICLR2017} is a versatile approach to privacy-preserving machine learning.","In PATE, teacher models are trained on distinct portions of sensitive data, and their predictions are privately aggregated to label new training examples for a student model.   ","Until now, PATE has primarily been explored with classification-like tasks, where each example possesses a ground-truth label, and knowledge is transferred to the student by labeling public examples.","Generative AI models, however, excel in open ended \\emph{diverse} tasks with multiple valid responses and scenarios that may not align with traditional labeled examples.","Furthermore, the knowledge of models is often encapsulated in the response distribution itself and may be transferred from teachers to student in a more fluid way.","We propose \\emph{hot PATE}, tailored for the diverse setting.","In hot PATE, each teacher model produces a response distribution and the aggregation method must preserve both privacy and diversity of responses.","We demonstrate, analytically and empirically, that hot PATE achieves privacy-utility tradeoffs that are comparable to, and in diverse settings, significantly surpass, the baseline ``cold'' PATE."],"url":"http://arxiv.org/abs/2312.02132v1"}
{"created":"2023-12-04 18:53:42","title":"Can we truly transfer an actor's genuine happiness to avatars? An investigation into virtual, real, posed and spontaneous faces","abstract":"A look is worth a thousand words is a popular phrase. And why is a simple look enough to portray our feelings about something or someone? Behind this question are the theoretical foundations of the field of psychology regarding social cognition and the studies of psychologist Paul Ekman. Facial expressions, as a form of non-verbal communication, are the primary way to transmit emotions between human beings. The set of movements and expressions of facial muscles that convey some emotional state of the individual to their observers are targets of studies in many areas. Our research aims to evaluate Ekman's action units in datasets of real human faces, posed and spontaneous, and virtual human faces resulting from transferring real faces into Computer Graphics faces. In addition, we also conducted a case study with specific movie characters, such as SheHulk and Genius. We intend to find differences and similarities in facial expressions between real and CG datasets, posed and spontaneous faces, and also to consider the actors' genders in the videos. This investigation can help several areas of knowledge, whether using real or virtual human beings, in education, health, entertainment, games, security, and even legal matters. Our results indicate that AU intensities are greater for posed than spontaneous datasets, regardless of gender. Furthermore, there is a smoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when a real face is transformed into CG.","sentences":["A look is worth a thousand words is a popular phrase.","And why is a simple look enough to portray our feelings about something or someone?","Behind this question are the theoretical foundations of the field of psychology regarding social cognition and the studies of psychologist Paul Ekman.","Facial expressions, as a form of non-verbal communication, are the primary way to transmit emotions between human beings.","The set of movements and expressions of facial muscles that convey some emotional state of the individual to their observers are targets of studies in many areas.","Our research aims to evaluate Ekman's action units in datasets of real human faces, posed and spontaneous, and virtual human faces resulting from transferring real faces into Computer Graphics faces.","In addition, we also conducted a case study with specific movie characters, such as SheHulk and Genius.","We intend to find differences and similarities in facial expressions between real and CG datasets, posed and spontaneous faces, and also to consider the actors' genders in the videos.","This investigation can help several areas of knowledge, whether using real or virtual human beings, in education, health, entertainment, games, security, and even legal matters.","Our results indicate that AU intensities are greater for posed than spontaneous datasets, regardless of gender.","Furthermore, there is a smoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when a real face is transformed into CG."],"url":"http://arxiv.org/abs/2312.02128v1"}
{"created":"2023-12-04 18:53:24","title":"SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM","abstract":"Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments show that SplaTAM achieves up to 2X state-of-the-art performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map.","sentences":["Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding.","Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras.","In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera.","Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians.","We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering.","Extensive experiments show that SplaTAM achieves up to 2X state-of-the-art performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map."],"url":"http://arxiv.org/abs/2312.02126v1"}
{"created":"2023-12-04 18:52:26","title":"TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques","abstract":"Recent advances in language models (LMs), have demonstrated significant efficacy in tasks related to the arts and humanities. While LMs have exhibited exceptional performance across a wide range of natural language processing tasks, there are notable challenges associated with their utilization on small datasets and their ability to replicate more creative human capacities. In this study, we aim to address these challenges by training a Persian classical poetry generation model using a transformer architecture on a specialized dataset with no pretraining. Additionally, we propose a novel decoding method to enhance coherence and meaningfulness in the generated poetry, effectively managing the tradeoff between diversity and quality. Furthermore, the results of our training approach and the proposed decoding method are evaluated through comprehensive set of automatic and human evaluations and showed its superior capability to generate coherent and meaningful poetry in compare to other decoding methods and an existing Persian large language model (LLM).","sentences":["Recent advances in language models (LMs), have demonstrated significant efficacy in tasks related to the arts and humanities.","While LMs have exhibited exceptional performance across a wide range of natural language processing tasks, there are notable challenges associated with their utilization on small datasets and their ability to replicate more creative human capacities.","In this study, we aim to address these challenges by training a Persian classical poetry generation model using a transformer architecture on a specialized dataset with no pretraining.","Additionally, we propose a novel decoding method to enhance coherence and meaningfulness in the generated poetry, effectively managing the tradeoff between diversity and quality.","Furthermore, the results of our training approach and the proposed decoding method are evaluated through comprehensive set of automatic and human evaluations and showed its superior capability to generate coherent and meaningful poetry in compare to other decoding methods and an existing Persian large language model (LLM)."],"url":"http://arxiv.org/abs/2312.02125v1"}
{"created":"2023-12-04 18:51:44","title":"VerA: Versatile Anonymization Fit for Clinical Facial Images","abstract":"The escalating legislative demand for data privacy in facial image dissemination has underscored the significance of image anonymization. Recent advancements in the field surpass traditional pixelation or blur methods, yet they predominantly address regular single images. This leaves clinical image anonymization -- a necessity for illustrating medical interventions -- largely unaddressed. We present VerA, a versatile facial image anonymization that is fit for clinical facial images where: (1) certain semantic areas must be preserved to show medical intervention results, and (2) anonymizing image pairs is crucial for showing before-and-after results. VerA outperforms or is on par with state-of-the-art methods in de-identification and photorealism for regular images. In addition, we validate our results on paired anonymization, and on the anonymization of both single and paired clinical images with extensive quantitative and qualitative evaluation.","sentences":["The escalating legislative demand for data privacy in facial image dissemination has underscored the significance of image anonymization.","Recent advancements in the field surpass traditional pixelation or blur methods, yet they predominantly address regular single images.","This leaves clinical image anonymization -- a necessity for illustrating medical interventions -- largely unaddressed.","We present VerA, a versatile facial image anonymization that is fit for clinical facial images where: (1) certain semantic areas must be preserved to show medical intervention results, and (2) anonymizing image pairs is crucial for showing before-and-after results.","VerA outperforms or is on par with state-of-the-art methods in de-identification and photorealism for regular images.","In addition, we validate our results on paired anonymization, and on the anonymization of both single and paired clinical images with extensive quantitative and qualitative evaluation."],"url":"http://arxiv.org/abs/2312.02124v1"}
{"created":"2023-12-04 18:50:41","title":"Mathematical Supplement for the $\\texttt{gsplat}$ Library","abstract":"This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al. It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting. To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization at github.com/nerfstudio-project/gsplat .","sentences":["This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al.","It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting.","To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization at github.com/nerfstudio-project/gsplat ."],"url":"http://arxiv.org/abs/2312.02121v1"}
{"created":"2023-12-04 18:50:35","title":"Magicoder: Source Code Is All You Need","abstract":"We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references.","sentences":["We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters.","Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code.","Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data.","The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion.","Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).","Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references."],"url":"http://arxiv.org/abs/2312.02120v1"}
{"created":"2023-12-04 18:49:23","title":"Tree of Attacks: Jailbreaking Black-Box LLMs Automatically","abstract":"While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thoughts reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80% of the prompts using only a small number of queries. This significantly improves upon the previous state-of-the-art black-box method for generating jailbreaks.","sentences":["While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks.","In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM.","TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thoughts reasoning until one of the generated prompts jailbreaks the target.","Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks.","Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target.","In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80% of the prompts using only a small number of queries.","This significantly improves upon the previous state-of-the-art black-box method for generating jailbreaks."],"url":"http://arxiv.org/abs/2312.02119v1"}
{"created":"2023-12-04 18:49:06","title":"When it Rains, it Pours: Modeling Media Storms and the News Ecosystem","abstract":"Most events in the world receive at most brief coverage by the news media. Occasionally, however, an event will trigger a media storm, with voluminous and widespread coverage lasting for weeks instead of days. In this work, we develop and apply a pairwise article similarity model, allowing us to identify story clusters in corpora covering local and national online news, and thereby create a comprehensive corpus of media storms over a nearly two year period. Using this corpus, we investigate media storms at a new level of granularity, allowing us to validate claims about storm evolution and topical distribution, and provide empirical support for previously hypothesized patterns of influence of storms on media coverage and intermedia agenda setting.","sentences":["Most events in the world receive at most brief coverage by the news media.","Occasionally, however, an event will trigger a media storm, with voluminous and widespread coverage lasting for weeks instead of days.","In this work, we develop and apply a pairwise article similarity model, allowing us to identify story clusters in corpora covering local and national online news, and thereby create a comprehensive corpus of media storms over a nearly two year period.","Using this corpus, we investigate media storms at a new level of granularity, allowing us to validate claims about storm evolution and topical distribution, and provide empirical support for previously hypothesized patterns of influence of storms on media coverage and intermedia agenda setting."],"url":"http://arxiv.org/abs/2312.02118v1"}
{"created":"2023-12-04 18:48:02","title":"GIVT: Generative Infinite-Vocabulary Transformers","abstract":"We introduce generative infinite-vocabulary transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a VAE. When applying GIVT to class-conditional image generation with iterative masked modeling, we show competitive results with MaskGIT, while our approach outperforms both VQ-GAN and MaskGIT when using it for causal modeling. Finally, we obtain competitive results outside of image generation when applying our approach to panoptic segmentation and depth estimation with a VAE-based variant of the UViM framework.","sentences":["We introduce generative infinite-vocabulary transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary.","To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model.","Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a VAE.","When applying GIVT to class-conditional image generation with iterative masked modeling, we show competitive results with MaskGIT, while our approach outperforms both VQ-GAN and MaskGIT when using it for causal modeling.","Finally, we obtain competitive results outside of image generation when applying our approach to panoptic segmentation and depth estimation with a VAE-based variant of the UViM framework."],"url":"http://arxiv.org/abs/2312.02116v1"}
{"created":"2023-12-04 18:47:13","title":"Transitions of Solutions and Their Efficiency","abstract":"We broaden the basis of non-cooperative game theory by considering miscoordination on a solution concept. For any solution concept, we extend the solution set of a strategic-form game to a transition set. This set contains profiles where various agents simultaneously follow different solutions, e.g.~different Nash equilibria. This models the fact that in practice, complicated agents are rarely perfectly coordinated on the same equilibrium. We define two efficiency measures, called the price of transition anarchy and stability, and bound them. We also refine the notion of transition to the notion of limited transition, where only a limited number of solutions is simultaneously played, and to stable transitions, which allow for only minor lack of coordination. We compare the above mentioned efficiency measures and bound the efficiency of transitions in important cases, including the important cases of constant-sum and potential games, which span the set of finite games with the same number of strategies for each agent. We also prove tight efficiency bounds for routing games and coordination games on graphs. Finally, we study algorithms to find the transition degree required to make a given profile a transition, or to render all the profiles transitions. We conclude that for the sake of efficiency, it is crucial to avoid uncoordinated transitions, besides certain cases, such as constant-sum games, identical utility games, some types of routing games, limited transitions in potential games, and stable transitions in coordination games.","sentences":["We broaden the basis of non-cooperative game theory by considering miscoordination on a solution concept.","For any solution concept, we extend the solution set of a strategic-form game to a transition set.","This set contains profiles where various agents simultaneously follow different solutions, e.g.~different Nash equilibria.","This models the fact that in practice, complicated agents are rarely perfectly coordinated on the same equilibrium.","We define two efficiency measures, called the price of transition anarchy and stability, and bound them.","We also refine the notion of transition to the notion of limited transition, where only a limited number of solutions is simultaneously played, and to stable transitions, which allow for only minor lack of coordination.","We compare the above mentioned efficiency measures and bound the efficiency of transitions in important cases, including the important cases of constant-sum and potential games, which span the set of finite games with the same number of strategies for each agent.","We also prove tight efficiency bounds for routing games and coordination games on graphs.","Finally, we study algorithms to find the transition degree required to make a given profile a transition, or to render all the profiles transitions.","We conclude that for the sake of efficiency, it is crucial to avoid uncoordinated transitions, besides certain cases, such as constant-sum games, identical utility games, some types of routing games, limited transitions in potential games, and stable transitions in coordination games."],"url":"http://arxiv.org/abs/2312.02114v1"}
{"created":"2023-12-04 18:46:42","title":"A Framework for Self-Intersecting Surfaces (SOS): Symmetric Optimisation for Stability","abstract":"In this paper, we give a stable and efficient method for fixing self-intersections and non-manifold parts in a given embedded simplicial complex. In addition, we show how symmetric properties can be used for further optimisation. We prove an initialisation criterion for computation of the outer hull of an embedded simplicial complex. To regularise the outer hull of the retriangulated surface, we present a method to remedy non-manifold edges and points. We also give a modification of the outer hull algorithm to determine chambers of complexes which gives rise to many new insights. All of these methods have applications in many areas, for example in 3D-printing, artistic realisations of 3D models or fixing errors introduced by scanning equipment applied for tomography. Implementations of the proposed algorithms are given in the computer algebra system GAP4. For verification of our methods, we use a data-set of highly self-intersecting symmetric icosahedra.","sentences":["In this paper, we give a stable and efficient method for fixing self-intersections and non-manifold parts in a given embedded simplicial complex.","In addition, we show how symmetric properties can be used for further optimisation.","We prove an initialisation criterion for computation of the outer hull of an embedded simplicial complex.","To regularise the outer hull of the retriangulated surface, we present a method to remedy non-manifold edges and points.","We also give a modification of the outer hull algorithm to determine chambers of complexes which gives rise to many new insights.","All of these methods have applications in many areas, for example in 3D-printing, artistic realisations of 3D models or fixing errors introduced by scanning equipment applied for tomography.","Implementations of the proposed algorithms are given in the computer algebra system GAP4.","For verification of our methods, we use a data-set of highly self-intersecting symmetric icosahedra."],"url":"http://arxiv.org/abs/2312.02113v1"}
{"created":"2023-12-04 18:45:04","title":"Distributed Optimization with Feasible Set Privacy","abstract":"We consider the setup of a constrained optimization problem with two agents $E_1$ and $E_2$ who jointly wish to learn the optimal solution set while keeping their feasible sets $\\mathcal{P}_1$ and $\\mathcal{P}_2$ private from each other. The objective function $f$ is globally known and each feasible set is a collection of points from a global alphabet. We adopt a sequential symmetric private information retrieval (SPIR) framework where one of the agents (say $E_1$) privately checks in $\\mathcal{P}_2$, the presence of candidate solutions of the problem constrained to $\\mathcal{P}_1$ only, while learning no further information on $\\mathcal{P}_2$ than the solution alone. Further, we extract an information theoretically private threshold PSI (ThPSI) protocol from our scheme and characterize its download cost. We show that, compared to privately acquiring the feasible set $\\mathcal{P}_1\\cap \\mathcal{P}_2$ using an SPIR-based private set intersection (PSI) protocol, and finding the optimum, our scheme is better as it incurs less information leakage and less download cost than the former. Over all possible uniform mappings of $f$ to a fixed range of values, our scheme outperforms the former with a high probability.","sentences":["We consider the setup of a constrained optimization problem with two agents $E_1$ and $E_2$ who jointly wish to learn the optimal solution set while keeping their feasible sets $\\mathcal{P}_1$ and $\\mathcal{P}_2$ private from each other.","The objective function $f$ is globally known and each feasible set is a collection of points from a global alphabet.","We adopt a sequential symmetric private information retrieval (SPIR) framework where one of the agents (say $E_1$) privately checks in $\\mathcal{P}_2$, the presence of candidate solutions of the problem constrained to $\\mathcal{P}_1$ only, while learning no further information on $\\mathcal{P}_2$ than the solution alone.","Further, we extract an information theoretically private threshold PSI (ThPSI) protocol from our scheme and characterize its download cost.","We show that, compared to privately acquiring the feasible set $\\mathcal{P}_1\\cap \\mathcal{P}_2$ using an SPIR-based private set intersection (PSI) protocol, and finding the optimum, our scheme is better as it incurs less information leakage and less download cost than the former.","Over all possible uniform mappings of $f$ to a fixed range of values, our scheme outperforms the former with a high probability."],"url":"http://arxiv.org/abs/2312.02112v1"}
{"created":"2023-12-04 18:43:45","title":"TriDeNT: Triple Deep Network Training for Privileged Knowledge Distillation in Histopathology","abstract":"Computational pathology models rarely utilise data that will not be available for inference. This means most models cannot learn from highly informative data such as additional immunohistochemical (IHC) stains and spatial transcriptomics. We present TriDeNT, a novel self-supervised method for utilising privileged data that is not available during inference to improve performance. We demonstrate the efficacy of this method for a range of different paired data including immunohistochemistry, spatial transcriptomics and expert nuclei annotations. In all settings, TriDeNT outperforms other state-of-the-art methods in downstream tasks, with observed improvements of up to 101%. Furthermore, we provide qualitative and quantitative measurements of the features learned by these models and how they differ from baselines. TriDeNT offers a novel method to distil knowledge from scarce or costly data during training, to create significantly better models for routine inputs.","sentences":["Computational pathology models rarely utilise data that will not be available for inference.","This means most models cannot learn from highly informative data such as additional immunohistochemical (IHC) stains and spatial transcriptomics.","We present TriDeNT, a novel self-supervised method for utilising privileged data that is not available during inference to improve performance.","We demonstrate the efficacy of this method for a range of different paired data including immunohistochemistry, spatial transcriptomics and expert nuclei annotations.","In all settings, TriDeNT outperforms other state-of-the-art methods in downstream tasks, with observed improvements of up to 101%.","Furthermore, we provide qualitative and quantitative measurements of the features learned by these models and how they differ from baselines.","TriDeNT offers a novel method to distil knowledge from scarce or costly data during training, to create significantly better models for routine inputs."],"url":"http://arxiv.org/abs/2312.02111v1"}
{"created":"2023-12-04 18:39:00","title":"ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation","abstract":"This work introduces ArtAdapter, a transformative text-to-image (T2I) style transfer framework that transcends traditional limitations of color, brushstrokes, and object shape, capturing high-level style elements such as composition and distinctive artistic expression. The integration of a multi-level style encoder with our proposed explicit adaptation mechanism enables ArtAdapte to achieve unprecedented fidelity in style transfer, ensuring close alignment with textual descriptions. Additionally, the incorporation of an Auxiliary Content Adapter (ACA) effectively separates content from style, alleviating the borrowing of content from style references. Moreover, our novel fast finetuning approach could further enhance zero-shot style representation while mitigating the risk of overfitting. Comprehensive evaluations confirm that ArtAdapter surpasses current state-of-the-art methods.","sentences":["This work introduces ArtAdapter, a transformative text-to-image (T2I) style transfer framework that transcends traditional limitations of color, brushstrokes, and object shape, capturing high-level style elements such as composition and distinctive artistic expression.","The integration of a multi-level style encoder with our proposed explicit adaptation mechanism enables ArtAdapte to achieve unprecedented fidelity in style transfer, ensuring close alignment with textual descriptions.","Additionally, the incorporation of an Auxiliary Content Adapter (ACA) effectively separates content from style, alleviating the borrowing of content from style references.","Moreover, our novel fast finetuning approach could further enhance zero-shot style representation while mitigating the risk of overfitting.","Comprehensive evaluations confirm that ArtAdapter surpasses current state-of-the-art methods."],"url":"http://arxiv.org/abs/2312.02109v1"}
{"created":"2023-12-04 18:32:55","title":"Authoring Worked Examples for Java Programming with Human-AI Collaboration","abstract":"Worked examples (solutions to typical programming problems presented as a source code in a certain language and are used to explain the topics from a programming class) are among the most popular types of learning content in programming classes. Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code. However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class. In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming. We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary. We also present a study that assesses the quality of explanations created with this approach.","sentences":["Worked examples (solutions to typical programming problems presented as a source code in a certain language and are used to explain the topics from a programming class) are among the most popular types of learning content in programming classes.","Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code.","However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class.","In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming.","We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary.","We also present a study that assesses the quality of explanations created with this approach."],"url":"http://arxiv.org/abs/2312.02105v1"}
{"created":"2023-12-04 18:29:03","title":"Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object Detection","abstract":"Open-vocabulary object detection (OVOD) has recently gained significant attention as a crucial step toward achieving human-like visual intelligence. Existing OVOD methods extend target vocabulary from pre-defined categories to open-world by transferring knowledge of arbitrary concepts from vision-language pre-training models to the detectors. While previous methods have shown remarkable successes, they suffer from indirect supervision or limited transferable concepts. In this paper, we propose a simple yet effective method to directly learn region-text alignment for arbitrary concepts. Specifically, the proposed method aims to learn arbitrary image-to-text mapping for pseudo-labeling of arbitrary concepts, named Pseudo-Labeling for Arbitrary Concepts (PLAC). The proposed method shows competitive performance on the standard OVOD benchmark for noun concepts and a large improvement on referring expression comprehension benchmark for arbitrary concepts.","sentences":["Open-vocabulary object detection (OVOD) has recently gained significant attention as a crucial step toward achieving human-like visual intelligence.","Existing OVOD methods extend target vocabulary from pre-defined categories to open-world by transferring knowledge of arbitrary concepts from vision-language pre-training models to the detectors.","While previous methods have shown remarkable successes, they suffer from indirect supervision or limited transferable concepts.","In this paper, we propose a simple yet effective method to directly learn region-text alignment for arbitrary concepts.","Specifically, the proposed method aims to learn arbitrary image-to-text mapping for pseudo-labeling of arbitrary concepts, named Pseudo-Labeling for Arbitrary Concepts (PLAC).","The proposed method shows competitive performance on the standard OVOD benchmark for noun concepts and a large improvement on referring expression comprehension benchmark for arbitrary concepts."],"url":"http://arxiv.org/abs/2312.02103v1"}
{"created":"2023-12-04 18:26:31","title":"Mitigating Data Injection Attacks on Federated Learning","abstract":"Federated learning is a technique that allows multiple entities to collaboratively train models using their data without compromising data privacy. However, despite its advantages, federated learning can be susceptible to false data injection attacks. In these scenarios, a malicious entity with control over specific agents in the network can manipulate the learning process, leading to a suboptimal model. Consequently, addressing these data injection attacks presents a significant research challenge in federated learning systems. In this paper, we propose a novel technique to detect and mitigate data injection attacks on federated learning systems. Our mitigation method is a local scheme, performed during a single instance of training by the coordinating node, allowing the mitigation during the convergence of the algorithm. Whenever an agent is suspected to be an attacker, its data will be ignored for a certain period, this decision will often be re-evaluated. We prove that with probability 1, after a finite time, all attackers will be ignored while the probability of ignoring a trustful agent becomes 0, provided that there is a majority of truthful agents. Simulations show that when the coordinating node detects and isolates all the attackers, the model recovers and converges to the truthful model.","sentences":["Federated learning is a technique that allows multiple entities to collaboratively train models using their data without compromising data privacy.","However, despite its advantages, federated learning can be susceptible to false data injection attacks.","In these scenarios, a malicious entity with control over specific agents in the network can manipulate the learning process, leading to a suboptimal model.","Consequently, addressing these data injection attacks presents a significant research challenge in federated learning systems.","In this paper, we propose a novel technique to detect and mitigate data injection attacks on federated learning systems.","Our mitigation method is a local scheme, performed during a single instance of training by the coordinating node, allowing the mitigation during the convergence of the algorithm.","Whenever an agent is suspected to be an attacker, its data will be ignored for a certain period, this decision will often be re-evaluated.","We prove that with probability 1, after a finite time, all attackers will be ignored while the probability of ignoring a trustful agent becomes 0, provided that there is a majority of truthful agents.","Simulations show that when the coordinating node detects and isolates all the attackers, the model recovers and converges to the truthful model."],"url":"http://arxiv.org/abs/2312.02102v1"}
{"created":"2023-12-04 18:16:27","title":"Inapproximability of Maximum Diameter Clustering for Few Clusters","abstract":"We consider the k-diameter clustering problem, where the goal is to partition a set of points in a metric space into $k$ clusters, minimizing the maximum distance between any two points in the same cluster. In general metrics, k-diameter is known to be NP-hard, while it has a $2$-approximation algorithm (Gonzalez'85). Complementing this algorithm, it is known that k-diameter is NP-hard to approximate within a factor better than $2$ in the $\\ell_1$ and $\\ell_\\infty$ metrics, and within a factor of $1.969$ in the $\\ell_2$ metric (Feder-Greene'88).   When $k\\geq 3$ is fixed, k-diameter remains NP-hard to approximate within a factor better than $2$ in the $\\ell_\\infty$ metric (Megiddo'90). However, its approximability in this setting has not previously been studied in the $\\ell_1$ and $\\ell_2$ metrics, though a $1.415$-approximation algorithm in the $\\ell_2$ metric follows from a known result (Badoiu et al.'02). In this paper, we address the remaining gap by showing new hardness of approximation results that hold even when $k=3$. Specifically, we prove that 3-diameter is NP-hard to approximate within a factor better than $1.5$ in the $\\ell_1$ metric, and within a factor of $1.304$ in the $\\ell_2$ metric.","sentences":["We consider the k-diameter clustering problem, where the goal is to partition a set of points in a metric space into $k$ clusters, minimizing the maximum distance between any two points in the same cluster.","In general metrics, k-diameter is known to be NP-hard, while it has a $2$-approximation algorithm (Gonzalez'85).","Complementing this algorithm, it is known that k-diameter is NP-hard to approximate within a factor better than $2$ in the $\\ell_1$ and $\\ell_\\infty$ metrics, and within a factor of $1.969$ in the $\\ell_2$ metric (Feder-Greene'88).   ","When $k\\geq 3$ is fixed, k-diameter remains NP-hard to approximate within a factor better than $2$ in the $\\ell_\\infty$ metric (Megiddo'90).","However, its approximability in this setting has not previously been studied in the $\\ell_1$ and $\\ell_2$ metrics, though a $1.415$-approximation algorithm in the $\\ell_2$ metric follows from a known result (Badoiu et al.'02).","In this paper, we address the remaining gap by showing new hardness of approximation results that hold even when $k=3$. Specifically, we prove that 3-diameter is NP-hard to approximate within a factor better than $1.5$ in the $\\ell_1$ metric, and within a factor of $1.304$ in the $\\ell_2$ metric."],"url":"http://arxiv.org/abs/2312.02097v1"}
{"created":"2023-12-04 18:13:58","title":"Single-sample versus case-control sampling scheme for Positive Unlabeled data: the story of two scenarios","abstract":"In the paper we argue that performance of the classifiers based on Empirical Risk Minimization (ERM) for positive unlabeled data, which are designed for case-control sampling scheme may significantly deteriorate when applied to a single-sample scenario. We reveal why their behavior depends, in all but very specific cases, on the scenario. Also, we introduce a single-sample case analogue of the popular non-negative risk classifier designed for case-control data and compare its performance with the original proposal. We show that the significant differences occur between them, especiall when half or more positive of observations are labeled. The opposite case when ERM minimizer designed for the case-control case is applied for single-sample data is also considered and similar conclusions are drawn. Taking into account difference of scenarios requires a sole, but crucial, change in the definition of the Empirical Risk.","sentences":["In the paper we argue that performance of the classifiers based on Empirical Risk Minimization (ERM) for positive unlabeled data, which are designed for case-control sampling scheme may significantly deteriorate when applied to a single-sample scenario.","We reveal why their behavior depends, in all but very specific cases, on the scenario.","Also, we introduce a single-sample case analogue of the popular non-negative risk classifier designed for case-control data and compare its performance with the original proposal.","We show that the significant differences occur between them, especiall when half or more positive of observations are labeled.","The opposite case when ERM minimizer designed for the case-control case is applied for single-sample data is also considered and similar conclusions are drawn.","Taking into account difference of scenarios requires a sole, but crucial, change in the definition of the Empirical Risk."],"url":"http://arxiv.org/abs/2312.02095v1"}
{"created":"2023-12-04 18:10:20","title":"Cultural Differences in Students' Privacy Concerns in Learning Analytics across Germany, South Korea, Spain, Sweden, and the United States","abstract":"Applications of learning analytics (LA) can raise concerns from students about their privacy in higher education contexts. Developing effective privacy-enhancing practices requires a systematic understanding of students' privacy concerns and how they vary across national and cultural dimensions. We conducted a survey study with established instruments to measure privacy concerns and cultural values for university students in five countries (Germany, South Korea, Spain, Sweden, and the United States; N = 762). The results show that students generally trusted institutions with their data and disclosed information as they perceived the risks to be manageable even though they felt somewhat limited in their ability to control their privacy. Across the five countries, German and Swedish students stood out as the most trusting and least concerned, especially compared to US students who reported greater perceived risk and less control. Students in South Korea and Spain responded similarly on all five privacy dimensions (perceived privacy risk, perceived privacy control, privacy concerns, trusting beliefs, and non-self-disclosure behavior), despite their significant cultural differences. Culture measured at the individual level affected the antecedents and outcomes of privacy concerns more than country-level culture. Perceived privacy risk and privacy control increase with power distance. Trusting beliefs increase with a desire for uncertainty avoidance and lower masculinity. Non-self-disclosure behaviors rise with power distance and masculinity, and decrease with more uncertainty avoidance. Thus, cultural values related to trust in institutions, social equality and risk-taking should be considered when developing privacy-enhancing practices and policies in higher education.","sentences":["Applications of learning analytics (LA) can raise concerns from students about their privacy in higher education contexts.","Developing effective privacy-enhancing practices requires a systematic understanding of students' privacy concerns and how they vary across national and cultural dimensions.","We conducted a survey study with established instruments to measure privacy concerns and cultural values for university students in five countries (Germany, South Korea, Spain, Sweden, and the United States; N = 762).","The results show that students generally trusted institutions with their data and disclosed information as they perceived the risks to be manageable even though they felt somewhat limited in their ability to control their privacy.","Across the five countries, German and Swedish students stood out as the most trusting and least concerned, especially compared to US students who reported greater perceived risk and less control.","Students in South Korea and Spain responded similarly on all five privacy dimensions (perceived privacy risk, perceived privacy control, privacy concerns, trusting beliefs, and non-self-disclosure behavior), despite their significant cultural differences.","Culture measured at the individual level affected the antecedents and outcomes of privacy concerns more than country-level culture.","Perceived privacy risk and privacy control increase with power distance.","Trusting beliefs increase with a desire for uncertainty avoidance and lower masculinity.","Non-self-disclosure behaviors rise with power distance and masculinity, and decrease with more uncertainty avoidance.","Thus, cultural values related to trust in institutions, social equality and risk-taking should be considered when developing privacy-enhancing practices and policies in higher education."],"url":"http://arxiv.org/abs/2312.02093v1"}
{"created":"2023-12-04 18:06:41","title":"Physics simulation capabilities of LLMs","abstract":"[Abridged abstract] Large Language Models (LLMs) can solve some undergraduate-level to graduate-level physics textbook problems and are proficient at coding. Combining these two capabilities could one day enable AI systems to simulate and predict the physical world.   We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to research-level computational physics problems. We condition LLM generation on the use of well-documented and widely-used packages to elicit coding capabilities in the physics and astrophysics domains. We contribute $\\sim 50$ original and challenging problems in celestial mechanics (with REBOUND), stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear dynamics (with SciPy). Since our problems do not admit unique solutions, we evaluate LLM performance on several soft metrics: counts of lines that contain different types of errors (coding, physics, necessity and sufficiency) as well as a more \"educational\" Pass-Fail metric focused on capturing the salient physical ingredients of the problem at hand.   As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems, although about 40\\% of the solutions could plausibly get a passing grade. About $70-90 \\%$ of the code lines produced are necessary, sufficient and correct (coding \\& physics). Physics and coding errors are the most common, with some unnecessary or insufficient lines. We observe significant variations across problem class and difficulty. We identify several failure modes of GPT4 in the computational physics domain.   Our reconnaissance work provides a snapshot of current computational capabilities in classical physics and points to obvious improvement targets if AI systems are ever to reach a basic level of autonomy in physics simulation capabilities.","sentences":["[Abridged abstract] Large Language Models (LLMs) can solve some undergraduate-level to graduate-level physics textbook problems and are proficient at coding.","Combining these two capabilities could one day enable AI systems to simulate and predict the physical world.   ","We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to research-level computational physics problems.","We condition LLM generation on the use of well-documented and widely-used packages to elicit coding capabilities in the physics and astrophysics domains.","We contribute $\\sim 50$ original and challenging problems in celestial mechanics (with REBOUND), stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear dynamics (with SciPy).","Since our problems do not admit unique solutions, we evaluate LLM performance on several soft metrics: counts of lines that contain different types of errors (coding, physics, necessity and sufficiency) as well as a more \"educational\" Pass-Fail metric focused on capturing the salient physical ingredients of the problem at hand.   ","As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems, although about 40\\% of the solutions could plausibly get a passing grade.","About $70-90 \\%$ of the code lines produced are necessary, sufficient and correct (coding \\& physics).","Physics and coding errors are the most common, with some unnecessary or insufficient lines.","We observe significant variations across problem class and difficulty.","We identify several failure modes of GPT4 in the computational physics domain.   ","Our reconnaissance work provides a snapshot of current computational capabilities in classical physics and points to obvious improvement targets if AI systems are ever to reach a basic level of autonomy in physics simulation capabilities."],"url":"http://arxiv.org/abs/2312.02091v1"}
{"created":"2023-12-04 18:02:18","title":"Sequential Sweeps and High Dimensional Expansion","abstract":"It is well known that the spectral gap of the down-up walk over an $n$-partite simplicial complex (also known as Glauber dynamics) cannot be better than $O(1/n)$ due to natural obstructions such as coboundaries. We study an alternative random walk over partite simplicial complexes known as the sequential sweep or the systematic scan Glauber dynamics: Whereas the down-up walk at each step selects a random coordinate and updates it based on the remaining coordinates, the sequential sweep goes through each of the coordinates one by one in a deterministic order and applies the same update operation. It is natural, thus, to compare $n$-steps of the down-up walk with a single step of the sequential sweep. Interestingly, while the spectral gap of the $n$-th power of the down-up walk is still bounded from above by a constant, under a strong enough local spectral assumption (in the sense of Gur, Lifschitz, Liu, STOC 2022) we can show that the spectral gap of this walk can be arbitrarily close to 1. We also study other isoperimetric inequalities for these walks, and show that under the assumptions of local entropy contraction (related to the considerations of Gur, Lifschitz, Liu), these walks satisfy an entropy contraction inequality.","sentences":["It is well known that the spectral gap of the down-up walk over an $n$-partite simplicial complex (also known as Glauber dynamics) cannot be better than $O(1/n)$ due to natural obstructions such as coboundaries.","We study an alternative random walk over partite simplicial complexes known as the sequential sweep or the systematic scan Glauber dynamics: Whereas the down-up walk at each step selects a random coordinate and updates it based on the remaining coordinates, the sequential sweep goes through each of the coordinates one by one in a deterministic order and applies the same update operation.","It is natural, thus, to compare $n$-steps of the down-up walk with a single step of the sequential sweep.","Interestingly, while the spectral gap of the $n$-th power of the down-up walk is still bounded from above by a constant, under a strong enough local spectral assumption (in the sense of Gur, Lifschitz, Liu, STOC 2022) we can show that the spectral gap of this walk can be arbitrarily close to 1.","We also study other isoperimetric inequalities for these walks, and show that under the assumptions of local entropy contraction (related to the considerations of Gur, Lifschitz, Liu), these walks satisfy an entropy contraction inequality."],"url":"http://arxiv.org/abs/2312.02089v1"}
{"created":"2023-12-04 17:58:06","title":"VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence","abstract":"Current diffusion-based video editing primarily focuses on structure-preserved editing by utilizing various dense correspondences to ensure temporal consistency and motion alignment. However, these approaches are often ineffective when the target edit involves a shape change. To embark on video editing with shape change, we explore customized video subject swapping in this work, where we aim to replace the main subject in a source video with a target subject having a distinct identity and potentially different shape. In contrast to previous methods that rely on dense correspondences, we introduce the VideoSwap framework that exploits semantic point correspondences, inspired by our observation that only a small number of semantic points are necessary to align the subject's motion trajectory and modify its shape. We also introduce various user-point interactions (\\eg, removing points and dragging points) to address various semantic point correspondence. Extensive experiments demonstrate state-of-the-art video subject swapping results across a variety of real-world videos.","sentences":["Current diffusion-based video editing primarily focuses on structure-preserved editing by utilizing various dense correspondences to ensure temporal consistency and motion alignment.","However, these approaches are often ineffective when the target edit involves a shape change.","To embark on video editing with shape change, we explore customized video subject swapping in this work, where we aim to replace the main subject in a source video with a target subject having a distinct identity and potentially different shape.","In contrast to previous methods that rely on dense correspondences, we introduce the VideoSwap framework that exploits semantic point correspondences, inspired by our observation that only a small number of semantic points are necessary to align the subject's motion trajectory and modify its shape.","We also introduce various user-point interactions (\\eg, removing points and dragging points) to address various semantic point correspondence.","Extensive experiments demonstrate state-of-the-art video subject swapping results across a variety of real-world videos."],"url":"http://arxiv.org/abs/2312.02087v1"}
{"created":"2023-12-04 17:47:27","title":"Fixed-point methods for long-term power control and beamforming design in large-scale MIMO","abstract":"This study presents novel applications of fixed-point methods to solve previously open joint power control and beamforming design problems in modern large-scale MIMO systems, e.g., based on the cell-free massive MIMO and XL-MIMO concepts. In particular, motivated by the need for scalable system architectures, we revisit the classical sum power minimization and max-min fair design criteria by considering long-term power control and beamforming design based on channel statistics and possibly limited channel state information (CSI) sharing across distributed processing units. This approach is believed to mitigate the severe scalability issues of competing short-term optimal algorithms in the literature, which must be executed for every channel realization by a central controller endowed with global CSI, hence imposing very demanding requirements in terms of computation and interconnection capabilities. The obtained optimal algorithms are then illustrated and compared against existing short-term and long-term approaches via numerical simulations in a cell-free massive MIMO setup.","sentences":["This study presents novel applications of fixed-point methods to solve previously open joint power control and beamforming design problems in modern large-scale MIMO systems, e.g., based on the cell-free massive MIMO and XL-MIMO concepts.","In particular, motivated by the need for scalable system architectures, we revisit the classical sum power minimization and max-min fair design criteria by considering long-term power control and beamforming design based on channel statistics and possibly limited channel state information (CSI) sharing across distributed processing units.","This approach is believed to mitigate the severe scalability issues of competing short-term optimal algorithms in the literature, which must be executed for every channel realization by a central controller endowed with global CSI, hence imposing very demanding requirements in terms of computation and interconnection capabilities.","The obtained optimal algorithms are then illustrated and compared against existing short-term and long-term approaches via numerical simulations in a cell-free massive MIMO setup."],"url":"http://arxiv.org/abs/2312.02080v1"}
{"created":"2023-12-04 17:46:57","title":"Deep Set Neural Networks for forecasting asynchronous bioprocess timeseries","abstract":"Cultivation experiments often produce sparse and irregular time series. Classical approaches based on mechanistic models, like Maximum Likelihood fitting or Monte-Carlo Markov chain sampling, can easily account for sparsity and time-grid irregularities, but most statistical and Machine Learning tools are not designed for handling sparse data out-of-the-box. Among popular approaches there are various schemes for filling missing values (imputation) and interpolation into a regular grid (alignment). However, such methods transfer the biases of the interpolation or imputation models to the target model. We show that Deep Set Neural Networks equipped with triplet encoding of the input data can successfully handle bio-process data without any need for imputation or alignment procedures. The method is agnostic to the particular nature of the time series and can be adapted for any task, for example, online monitoring, predictive control, design of experiments, etc. In this work, we focus on forecasting. We argue that such an approach is especially suitable for typical cultivation processes, demonstrate the performance of the method on several forecasting tasks using data generated from macrokinetic growth models under realistic conditions, and compare the method to a conventional fitting procedure and methods based on imputation and alignment.","sentences":["Cultivation experiments often produce sparse and irregular time series.","Classical approaches based on mechanistic models, like Maximum Likelihood fitting or Monte-Carlo Markov chain sampling, can easily account for sparsity and time-grid irregularities, but most statistical and Machine Learning tools are not designed for handling sparse data out-of-the-box.","Among popular approaches there are various schemes for filling missing values (imputation) and interpolation into a regular grid (alignment).","However, such methods transfer the biases of the interpolation or imputation models to the target model.","We show that Deep Set Neural Networks equipped with triplet encoding of the input data can successfully handle bio-process data without any need for imputation or alignment procedures.","The method is agnostic to the particular nature of the time series and can be adapted for any task, for example, online monitoring, predictive control, design of experiments, etc.","In this work, we focus on forecasting.","We argue that such an approach is especially suitable for typical cultivation processes, demonstrate the performance of the method on several forecasting tasks using data generated from macrokinetic growth models under realistic conditions, and compare the method to a conventional fitting procedure and methods based on imputation and alignment."],"url":"http://arxiv.org/abs/2312.02079v1"}
{"created":"2023-12-04 17:41:52","title":"Integrating AI into CCTV Systems: A Comprehensive Evaluation of Smart Video Surveillance in Community Space","abstract":"This article presents an AI-enabled Smart Video Surveillance (SVS) designed to enhance safety in community spaces such as educational and recreational areas, and small businesses. The proposed system innovatively integrates with existing CCTV and wired camera networks, simplifying its adoption across various community cases to leverage recent AI advancements. Our SVS system, focusing on privacy, uses metadata instead of pixel data for activity recognition, aligning with ethical standards. It features cloud-based infrastructure and a mobile app for real-time, privacy-conscious alerts in communities.   This article notably pioneers a comprehensive real-world evaluation of the SVS system, covering AI-driven visual processing, statistical analysis, database management, cloud communication, and user notifications. It's also the first to assess an end-to-end anomaly detection system's performance, vital for identifying potential public safety incidents.   For our evaluation, we implemented the system in a community college, serving as an ideal model to exemplify the proposed system's capabilities. Our findings in this setting demonstrate the system's robustness, with throughput, latency, and scalability effectively managing 16 CCTV cameras. The system maintained a consistent 16.5 frames per second (FPS) over a 21-hour operation. The average end-to-end latency for detecting behavioral anomalies and alerting users was 26.76 seconds.","sentences":["This article presents an AI-enabled Smart Video Surveillance (SVS) designed to enhance safety in community spaces such as educational and recreational areas, and small businesses.","The proposed system innovatively integrates with existing CCTV and wired camera networks, simplifying its adoption across various community cases to leverage recent AI advancements.","Our SVS system, focusing on privacy, uses metadata instead of pixel data for activity recognition, aligning with ethical standards.","It features cloud-based infrastructure and a mobile app for real-time, privacy-conscious alerts in communities.   ","This article notably pioneers a comprehensive real-world evaluation of the SVS system, covering AI-driven visual processing, statistical analysis, database management, cloud communication, and user notifications.","It's also the first to assess an end-to-end anomaly detection system's performance, vital for identifying potential public safety incidents.   ","For our evaluation, we implemented the system in a community college, serving as an ideal model to exemplify the proposed system's capabilities.","Our findings in this setting demonstrate the system's robustness, with throughput, latency, and scalability effectively managing 16 CCTV cameras.","The system maintained a consistent 16.5 frames per second (FPS) over a 21-hour operation.","The average end-to-end latency for detecting behavioral anomalies and alerting users was 26.76 seconds."],"url":"http://arxiv.org/abs/2312.02078v1"}
{"created":"2023-12-04 17:37:41","title":"Federated Learning is Better with Non-Homomorphic Encryption","abstract":"Traditional AI methodologies necessitate centralized data collection, which becomes impractical when facing problems with network communication, data privacy, or storage capacity. Federated Learning (FL) offers a paradigm that empowers distributed AI model training without collecting raw data. There are different choices for providing privacy during FL training. One of the popular methodologies is employing Homomorphic Encryption (HE) - a breakthrough in privacy-preserving computation from Cryptography. However, these methods have a price in the form of extra computation and memory footprint. To resolve these issues, we propose an innovative framework that synergizes permutation-based compressors with Classical Cryptography, even though employing Classical Cryptography was assumed to be impossible in the past in the context of FL. Our framework offers a way to replace HE with cheaper Classical Cryptography primitives which provides security for the training process. It fosters asynchronous communication and provides flexible deployment options in various communication topologies.","sentences":["Traditional AI methodologies necessitate centralized data collection, which becomes impractical when facing problems with network communication, data privacy, or storage capacity.","Federated Learning (FL) offers a paradigm that empowers distributed AI model training without collecting raw data.","There are different choices for providing privacy during FL training.","One of the popular methodologies is employing Homomorphic Encryption (HE) - a breakthrough in privacy-preserving computation from Cryptography.","However, these methods have a price in the form of extra computation and memory footprint.","To resolve these issues, we propose an innovative framework that synergizes permutation-based compressors with Classical Cryptography, even though employing Classical Cryptography was assumed to be impossible in the past in the context of FL.","Our framework offers a way to replace HE with cheaper Classical Cryptography primitives which provides security for the training process.","It fosters asynchronous communication and provides flexible deployment options in various communication topologies."],"url":"http://arxiv.org/abs/2312.02074v1"}
{"created":"2023-12-04 17:35:42","title":"A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia","abstract":"Large language models (LLMs) have demonstrated impressive capabilities in storing and recalling factual knowledge, but also in adapting to novel in-context information. Yet, the mechanisms underlying their in-context grounding remain unknown, especially in situations where in-context information contradicts factual knowledge embedded in the parameters. This is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify the outdated parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the parametric knowledge clashes with the in-context information. We benchmark various LLMs with Fakepedia and discover that GPT-4-turbo has a strong preference for its parametric knowledge. Mistral-7B, on the contrary, is the model that most robustly chooses the grounded answer. Then, we conduct causal mediation analysis on LLM components when answering Fakepedia queries. We demonstrate that inspection of the computational graph alone can predict LLM grounding with 92.8% accuracy, especially because few MLPs in the Transformer can predict non-grounded behavior. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.","sentences":["Large language models (LLMs) have demonstrated impressive capabilities in storing and recalling factual knowledge, but also in adapting to novel in-context information.","Yet, the mechanisms underlying their in-context grounding remain unknown, especially in situations where in-context information contradicts factual knowledge embedded in the parameters.","This is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify the outdated parametric knowledge.","In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the parametric knowledge clashes with the in-context information.","We benchmark various LLMs with Fakepedia and discover that GPT-4-turbo has a strong preference for its parametric knowledge.","Mistral-7B, on the contrary, is the model that most robustly chooses the grounded answer.","Then, we conduct causal mediation analysis on LLM components when answering Fakepedia queries.","We demonstrate that inspection of the computational graph alone can predict LLM grounding with 92.8% accuracy, especially because few MLPs in the Transformer can predict non-grounded behavior.","Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs."],"url":"http://arxiv.org/abs/2312.02073v1"}
{"created":"2023-12-04 17:30:09","title":"Evaluating the Claims of \"SAT Requires Exhaustive Search\"","abstract":"In this paper, we take a closer look at the claims made by Xu and Zhou in their paper \"SAT Requires Exhaustive Search\" [XZ23], which claims to provide a lower bound on the complexity of the so-called Model RB. Xu and Zhou conclude that their result implies a separation between P and NP, since the lower bound purportedly proves that the Strong Exponential Time Hypothesis (SETH) is true. In examining Xu and Zhou's arguments, we find a flaw in their main theorems. The authors assume that an algorithm for Model RB must have a certain structure that can leverage downward self-reducibility, and argue that such an algorithm cannot run in polynomial time. We argue that this structure is not guaranteed to exist and thus their paper neither proves SETH to be true nor proves P $\\neq$ NP.","sentences":["In this paper, we take a closer look at the claims made by Xu and Zhou in their paper \"SAT Requires Exhaustive Search\"","[XZ23], which claims to provide a lower bound on the complexity of the so-called Model RB.","Xu and Zhou conclude that their result implies a separation between P and NP, since the lower bound purportedly proves that the Strong Exponential Time Hypothesis (SETH) is true.","In examining Xu and Zhou's arguments, we find a flaw in their main theorems.","The authors assume that an algorithm for Model RB must have a certain structure that can leverage downward self-reducibility, and argue that such an algorithm cannot run in polynomial time.","We argue that this structure is not guaranteed to exist and thus their paper neither proves SETH to be true nor proves P $\\neq$ NP."],"url":"http://arxiv.org/abs/2312.02071v1"}
{"created":"2023-12-04 17:28:35","title":"GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians","abstract":"We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.","sentences":["We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint.","The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model.","This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters.","We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation.","During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion.","We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios.","For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin."],"url":"http://arxiv.org/abs/2312.02069v1"}
{"created":"2023-12-04 17:19:53","title":"Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?","abstract":"Large language models (LLMs) offer a range of new possibilities, including adapting the text to different audiences and their reading needs. But how well do they adapt? We evaluate the readability of answers generated by four state-of-the-art LLMs (commercial and open-source) to science questions when prompted to target different age groups and education levels. To assess the adaptability of LLMs to diverse audiences, we compare the readability scores of the generated responses against the recommended comprehension level of each age and education group. We find large variations in the readability of the answers by different LLMs. Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible. They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels. Overall, current LLMs have set readability ranges and do not adapt well to different audiences, even when prompted. That limits their potential for educational purposes.","sentences":["Large language models (LLMs) offer a range of new possibilities, including adapting the text to different audiences and their reading needs.","But how well do they adapt?","We evaluate the readability of answers generated by four state-of-the-art LLMs (commercial and open-source) to science questions when prompted to target different age groups and education levels.","To assess the adaptability of LLMs to diverse audiences, we compare the readability scores of the generated responses against the recommended comprehension level of each age and education group.","We find large variations in the readability of the answers by different LLMs.","Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible.","They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels.","Overall, current LLMs have set readability ranges and do not adapt well to different audiences, even when prompted.","That limits their potential for educational purposes."],"url":"http://arxiv.org/abs/2312.02065v1"}
{"created":"2023-12-04 17:18:24","title":"Right-sizing compute resource allocations for bioinformatics tools with Total Perspective Vortex","abstract":"In biomedical research, computational methods have become indispensable and their use is increasing, making the efficient allocation of computing resources paramount. Practitioners routinely allocate resources far in excess of what is required for batch processing jobs, leading to not just inflated wait times and costs, but also unnecessary carbon emissions. This is not without reason however, as accurately determining resource needs is complex, affected by the nature of tools, data size, and analysis parameters, especially on popular servers that handle numerous jobs. The Galaxy platform, a web-based hub for biomedical analysis used globally by scientists, exemplifies this challenge. Serving nearly half a million registered users and managing around 2 million monthly jobs, Galaxy's growth outpaces the resources at its disposal. This is necessitating smarter resource utilization. To address this, we have developed a tool named Total Perspective Vortex (TPV) - a software package that right-sizes resource allocations for each job. TPV is able to dynamically set resource requirements for individual jobs and perform meta-scheduling across heterogeneous resources. It also includes a first-ever community-curated database of default resource requirements for nearly 1,000 popular bioinformatics tools. Deployments in Galaxy Australia and Europe demonstrate its effectiveness with meta-scheduling user jobs and an improved experience for systems administrators managing Galaxy servers.","sentences":["In biomedical research, computational methods have become indispensable and their use is increasing, making the efficient allocation of computing resources paramount.","Practitioners routinely allocate resources far in excess of what is required for batch processing jobs, leading to not just inflated wait times and costs, but also unnecessary carbon emissions.","This is not without reason however, as accurately determining resource needs is complex, affected by the nature of tools, data size, and analysis parameters, especially on popular servers that handle numerous jobs.","The Galaxy platform, a web-based hub for biomedical analysis used globally by scientists, exemplifies this challenge.","Serving nearly half a million registered users and managing around 2 million monthly jobs, Galaxy's growth outpaces the resources at its disposal.","This is necessitating smarter resource utilization.","To address this, we have developed a tool named Total Perspective Vortex (TPV) - a software package that right-sizes resource allocations for each job.","TPV is able to dynamically set resource requirements for individual jobs and perform meta-scheduling across heterogeneous resources.","It also includes a first-ever community-curated database of default resource requirements for nearly 1,000 popular bioinformatics tools.","Deployments in Galaxy Australia and Europe demonstrate its effectiveness with meta-scheduling user jobs and an improved experience for systems administrators managing Galaxy servers."],"url":"http://arxiv.org/abs/2312.02060v1"}
{"created":"2023-12-04 17:15:12","title":"An improved bound on sums of square roots via the subspace theorem","abstract":"The sum of square roots is as follows: Given $x_1,\\dots,x_n \\in \\mathbb{Z}$ and $a_1,\\dots,a_n \\in \\mathbb{N}$ decide whether $ E=\\sum_{i=1}^n x_i \\sqrt{a_i} \\geq 0$. It is a prominent open problem (Problem 33 of the Open Problems Project), whether this can be decided in polynomial time. The state-of-the-art methods rely on separation bounds, which are lower bounds on the minimum nonzero absolute value of $E$. The current best bound shows that $|E| \\geq \\left(n \\cdot \\max_i (|x_i| \\cdot \\sqrt{a_i})\\right)^{-2^n} $, which is doubly exponentially small.   We provide a new bound of the form $|E| \\geq \\gamma \\cdot (n \\cdot \\max_i|x_i|)^{-2n}$ where $\\gamma $ is a constant depending on $a_1,\\dots,a_n$. This is singly exponential in $n$ for fixed $a_1,\\dots,a_n$. The constant $\\gamma$ is not explicit and stems from the subspace theorem, a deep result in the geometry of numbers.","sentences":["The sum of square roots is as follows: Given $x_1,\\dots,x_n \\in \\mathbb{Z}$ and $a_1,\\dots,a_n","\\in \\mathbb{N}$ decide whether $ E=\\sum_{i=1}^n x_i \\sqrt{a_i} \\geq 0$.","It is a prominent open problem (Problem 33 of the Open Problems Project), whether this can be decided in polynomial time.","The state-of-the-art methods rely on separation bounds, which are lower bounds on the minimum nonzero absolute value of $E$. The current best bound shows that $|E| \\geq \\left(n","\\cdot \\max_i (|x_i| \\cdot \\sqrt{a_i})\\right)^{-2^n} $, which is doubly exponentially small.   ","We provide a new bound of the form $|E| \\geq \\gamma \\cdot (n \\cdot \\max_i|x_i|)^{-2n}$ where $\\gamma $ is a constant depending on $a_1,\\dots,a_n$. This is singly exponential in $n$ for fixed $a_1,\\dots,a_n$. The constant $\\gamma$ is not explicit and stems from the subspace theorem, a deep result in the geometry of numbers."],"url":"http://arxiv.org/abs/2312.02057v1"}
{"created":"2023-12-04 17:14:06","title":"Transaction Ordering Auctions","abstract":"We study equilibrium investment into bidding and latency reduction for different sequencing policies. For a batch auction design, we observe that bidders shade bids according to the likelihood that competing bidders land in the current batch. Moreover, in equilibrium, in the ex-ante investment stage before the auction, bidders invest into latency until they make zero profit in expectation.   We compare the batch auction design to continuous time bidding policies (time boost) and observe that (depending on the choice of parameters) they obtain similar revenue and welfare guarantees.","sentences":["We study equilibrium investment into bidding and latency reduction for different sequencing policies.","For a batch auction design, we observe that bidders shade bids according to the likelihood that competing bidders land in the current batch.","Moreover, in equilibrium, in the ex-ante investment stage before the auction, bidders invest into latency until they make zero profit in expectation.   ","We compare the batch auction design to continuous time bidding policies (time boost) and observe that (depending on the choice of parameters) they obtain similar revenue and welfare guarantees."],"url":"http://arxiv.org/abs/2312.02055v1"}
{"created":"2023-12-04 17:13:17","title":"From High to Low: Simulating Nondeterminism and State with State","abstract":"Some effects are considered to be higher-level than others. High-level effects provide expressive and succinct abstraction of programming concepts, while low-level effects allow more fine-grained control over program execution and resources. Yet, often it is desirable to write programs using the convenient abstraction offered by high-level effects, and meanwhile still benefit from the optimisations enabled by low-level effects. One solution is to translate high-level effects to low-level ones.   This paper studies how algebraic effects and handlers allow us to simulate high-level effects in terms of low-level effects. In particular, we focus on the interaction between state and nondeterminism known as the local state, as provided by Prolog. We map this high-level semantics in successive steps onto a low-level composite state effect, similar to that managed by Prolog's Warren Abstract Machine. We first give a translation from the high-level local-state semantics to the low-level global-state semantics, by explicitly restoring state updates on backtracking. Next, we eliminate nondeterminsm altogether in favor of a lower-level state containing a choicepoint stack. Then we avoid copying the state by restricting ourselves to incremental, reversible state updates. We show how these updates can be stored on a trail stack with another state effect. We prove the correctness of all our steps using program calculation where the fusion laws of effect handlers play a central role.","sentences":["Some effects are considered to be higher-level than others.","High-level effects provide expressive and succinct abstraction of programming concepts, while low-level effects allow more fine-grained control over program execution and resources.","Yet, often it is desirable to write programs using the convenient abstraction offered by high-level effects, and meanwhile still benefit from the optimisations enabled by low-level effects.","One solution is to translate high-level effects to low-level ones.   ","This paper studies how algebraic effects and handlers allow us to simulate high-level effects in terms of low-level effects.","In particular, we focus on the interaction between state and nondeterminism known as the local state, as provided by Prolog.","We map this high-level semantics in successive steps onto a low-level composite state effect, similar to that managed by Prolog's Warren Abstract Machine.","We first give a translation from the high-level local-state semantics to the low-level global-state semantics, by explicitly restoring state updates on backtracking.","Next, we eliminate nondeterminsm altogether in favor of a lower-level state containing a choicepoint stack.","Then we avoid copying the state by restricting ourselves to incremental, reversible state updates.","We show how these updates can be stored on a trail stack with another state effect.","We prove the correctness of all our steps using program calculation where the fusion laws of effect handlers play a central role."],"url":"http://arxiv.org/abs/2312.02054v1"}
{"created":"2023-12-04 17:10:25","title":"DUCK: Distance-based Unlearning via Centroid Kinematics","abstract":"Machine Unlearning is rising as a new field, driven by the pressing necessity of ensuring privacy in modern artificial intelligence models. This technique primarily aims to eradicate any residual influence of a specific subset of data from the knowledge acquired by a neural model during its training. This work introduces a novel unlearning algorithm, denoted as Distance-based Unlearning via Centroid Kinematics (DUCK), which employs metric learning to guide the removal of samples matching the nearest incorrect centroid in the embedding space. Evaluation of the algorithm's performance is conducted across various benchmark datasets in two distinct scenarios, class removal, and homogeneous sampling removal, obtaining state-of-the-art performance. We introduce a novel metric, called Adaptive Unlearning Score (AUS), encompassing not only the efficacy of the unlearning process in forgetting target data but also quantifying the performance loss relative to the original model. Moreover, we propose a novel membership inference attack to assess the algorithm's capacity to erase previously acquired knowledge, designed to be adaptable to future methodologies.","sentences":["Machine Unlearning is rising as a new field, driven by the pressing necessity of ensuring privacy in modern artificial intelligence models.","This technique primarily aims to eradicate any residual influence of a specific subset of data from the knowledge acquired by a neural model during its training.","This work introduces a novel unlearning algorithm, denoted as Distance-based Unlearning via Centroid Kinematics (DUCK), which employs metric learning to guide the removal of samples matching the nearest incorrect centroid in the embedding space.","Evaluation of the algorithm's performance is conducted across various benchmark datasets in two distinct scenarios, class removal, and homogeneous sampling removal, obtaining state-of-the-art performance.","We introduce a novel metric, called Adaptive Unlearning Score (AUS), encompassing not only the efficacy of the unlearning process in forgetting target data but also quantifying the performance loss relative to the original model.","Moreover, we propose a novel membership inference attack to assess the algorithm's capacity to erase previously acquired knowledge, designed to be adaptable to future methodologies."],"url":"http://arxiv.org/abs/2312.02052v1"}
{"created":"2023-12-04 17:09:52","title":"TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding","abstract":"This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long-form video comprehension tasks and satisfy realistic user requirements.","sentences":["This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding.","Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations.","Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance.","Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities.","For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long-form video comprehension tasks and satisfy realistic user requirements."],"url":"http://arxiv.org/abs/2312.02051v1"}
{"created":"2023-12-04 17:02:59","title":"Isomorphism for Tournaments of Small Twin Width","abstract":"We prove that isomorphism of tournaments of twin width at most $k$ can be decided in time $k^{O(\\log k)}n^{O(1)}$. This implies that the isomorphism problem for classes of tournaments of bounded or moderately growing twin width is in polynomial time. By comparison, there are classes of undirected graphs of bounded twin width that are isomorphism complete, that is, the isomorphism problem for the classes is as hard as the general graph isomorphism problem. Twin width is a graph parameter that has been introduced only recently (Bonnet et al., FOCS 2020), but has received a lot of attention in structural graph theory since then. On directed graphs, it is functionally smaller than clique width. We prove that on tournaments (but not on general directed graphs) it is also functionally smaller than directed tree width (and thus, the same also holds for cut width and directed path width). Hence, our result implies that tournament isomorphism testing is also fixed-parameter tractable when parameterized by any of these parameters.   Our isomorphism algorithm heavily employs group-theoretic techniques. This seems to be necessary: as a second main result, we show that the combinatorial Weisfeiler-Leman algorithm does not decide isomorphism of tournaments of twin width at most 35 if its dimension is $o(\\sqrt n)$. (Throughout this abstract, $n$ is the order of the input graphs.)","sentences":["We prove that isomorphism of tournaments of twin width at most $k$ can be decided in time $k^{O(\\log k)}n^{O(1)}$.","This implies that the isomorphism problem for classes of tournaments of bounded or moderately growing twin width is in polynomial time.","By comparison, there are classes of undirected graphs of bounded twin width that are isomorphism complete, that is, the isomorphism problem for the classes is as hard as the general graph isomorphism problem.","Twin width is a graph parameter that has been introduced only recently (Bonnet et al., FOCS 2020), but has received a lot of attention in structural graph theory since then.","On directed graphs, it is functionally smaller than clique width.","We prove that on tournaments (but not on general directed graphs)","it is also functionally smaller than directed tree width (and thus, the same also holds for cut width and directed path width).","Hence, our result implies that tournament isomorphism testing is also fixed-parameter tractable when parameterized by any of these parameters.   ","Our isomorphism algorithm heavily employs group-theoretic techniques.","This seems to be necessary: as a second main result, we show that the combinatorial Weisfeiler-Leman algorithm does not decide isomorphism of tournaments of twin width at most 35 if its dimension is $o(\\sqrt n)$. (Throughout this abstract, $n$ is the order of the input graphs.)"],"url":"http://arxiv.org/abs/2312.02048v1"}
{"created":"2023-12-04 16:59:24","title":"Kirchhoff Meets Johnson: In Pursuit of Unconditionally Secure Communication","abstract":"Noise: an enemy to be dealt with and a major factor limiting communication system performance. However, what if there is gold in that garbage? In conventional engineering, our focus is primarily on eliminating, suppressing, combating, or even ignoring noise and its detrimental impacts. Conversely, could we exploit it similarly to biology, which utilizes noise-alike carrier signals to convey information? In this context, the utilization of noise, or noise-alike signals in general, has been put forward as a means to realize unconditionally secure communication systems in the future. In this tutorial article, we begin by tracing the origins of thermal noise-based communication and highlighting one of its significant applications for ensuring unconditionally secure networks: the Kirchhoff-law-Johnson-noise (KLJN) secure key exchange scheme. We then delve into the inherent challenges tied to secure communication and discuss the imperative need for physics-based key distribution schemes in pursuit of unconditional security. Concurrently, we provide a concise overview of quantum key distribution (QKD) schemes and draw comparisons with their KLJN-based counterparts. Finally, extending beyond wired communication loops, we explore the transmission of noise signals over-the-air and evaluate their potential for stealth and secure wireless communication systems.","sentences":["Noise: an enemy to be dealt with and a major factor limiting communication system performance.","However, what if there is gold in that garbage?","In conventional engineering, our focus is primarily on eliminating, suppressing, combating, or even ignoring noise and its detrimental impacts.","Conversely, could we exploit it similarly to biology, which utilizes noise-alike carrier signals to convey information?","In this context, the utilization of noise, or noise-alike signals in general, has been put forward as a means to realize unconditionally secure communication systems in the future.","In this tutorial article, we begin by tracing the origins of thermal noise-based communication and highlighting one of its significant applications for ensuring unconditionally secure networks: the Kirchhoff-law-Johnson-noise (KLJN) secure key exchange scheme.","We then delve into the inherent challenges tied to secure communication and discuss the imperative need for physics-based key distribution schemes in pursuit of unconditional security.","Concurrently, we provide a concise overview of quantum key distribution (QKD) schemes and draw comparisons with their KLJN-based counterparts.","Finally, extending beyond wired communication loops, we explore the transmission of noise signals over-the-air and evaluate their potential for stealth and secure wireless communication systems."],"url":"http://arxiv.org/abs/2312.02042v1"}
{"created":"2023-12-04 16:54:40","title":"GFS: Graph-based Feature Synthesis for Prediction over Relational Databases","abstract":"Relational databases are extensively utilized in a variety of modern information system applications, and they always carry valuable data patterns. There are a huge number of data mining or machine learning tasks conducted on relational databases. However, it is worth noting that there are limited machine learning models specifically designed for relational databases, as most models are primarily tailored for single table settings. Consequently, the prevalent approach for training machine learning models on data stored in relational databases involves performing feature engineering to merge the data from multiple tables into a single table and subsequently applying single table models. This approach not only requires significant effort in feature engineering but also destroys the inherent relational structure present in the data. To address these challenges, we propose a novel framework called Graph-based Feature Synthesis (GFS). GFS formulates the relational database as a heterogeneous graph, thereby preserving the relational structure within the data. By leveraging the inductive bias from single table models, GFS effectively captures the intricate relationships inherent in each table. Additionally, the whole framework eliminates the need for manual feature engineering. In the extensive experiment over four real-world multi-table relational databases, GFS outperforms previous methods designed for relational databases, demonstrating its superior performance.","sentences":["Relational databases are extensively utilized in a variety of modern information system applications, and they always carry valuable data patterns.","There are a huge number of data mining or machine learning tasks conducted on relational databases.","However, it is worth noting that there are limited machine learning models specifically designed for relational databases, as most models are primarily tailored for single table settings.","Consequently, the prevalent approach for training machine learning models on data stored in relational databases involves performing feature engineering to merge the data from multiple tables into a single table and subsequently applying single table models.","This approach not only requires significant effort in feature engineering but also destroys the inherent relational structure present in the data.","To address these challenges, we propose a novel framework called Graph-based Feature Synthesis (GFS).","GFS formulates the relational database as a heterogeneous graph, thereby preserving the relational structure within the data.","By leveraging the inductive bias from single table models, GFS effectively captures the intricate relationships inherent in each table.","Additionally, the whole framework eliminates the need for manual feature engineering.","In the extensive experiment over four real-world multi-table relational databases, GFS outperforms previous methods designed for relational databases, demonstrating its superior performance."],"url":"http://arxiv.org/abs/2312.02037v1"}
{"created":"2023-12-04 16:53:11","title":"Trust, distrust, and appropriate reliance in (X)AI: a survey of empirical evaluation of user trust","abstract":"A current concern in the field of Artificial Intelligence (AI) is to ensure the trustworthiness of AI systems. The development of explainability methods is one prominent way to address this, which has often resulted in the assumption that the use of explainability will lead to an increase in the trust of users and wider society. However, the dynamics between explainability and trust are not well established and empirical investigations of their relation remain mixed or inconclusive. In this paper we provide a detailed description of the concepts of user trust and distrust in AI and their relation to appropriate reliance. For that we draw from the fields of machine learning, human-computer interaction, and the social sciences. Furthermore, we have created a survey of existing empirical studies that investigate the effects of AI systems and XAI methods on user (dis)trust. With clarifying the concepts and summarizing the empirical investigations, we aim to provide researchers, who examine user trust in AI, with an improved starting point for developing user studies to measure and evaluate the user's attitude towards and reliance on AI systems.","sentences":["A current concern in the field of Artificial Intelligence (AI) is to ensure the trustworthiness of AI systems.","The development of explainability methods is one prominent way to address this, which has often resulted in the assumption that the use of explainability will lead to an increase in the trust of users and wider society.","However, the dynamics between explainability and trust are not well established and empirical investigations of their relation remain mixed or inconclusive.","In this paper we provide a detailed description of the concepts of user trust and distrust in AI and their relation to appropriate reliance.","For that we draw from the fields of machine learning, human-computer interaction, and the social sciences.","Furthermore, we have created a survey of existing empirical studies that investigate the effects of AI systems and XAI methods on user (dis)trust.","With clarifying the concepts and summarizing the empirical investigations, we aim to provide researchers, who examine user trust in AI, with an improved starting point for developing user studies to measure and evaluate the user's attitude towards and reliance on AI systems."],"url":"http://arxiv.org/abs/2312.02034v1"}
{"created":"2023-12-04 16:51:23","title":"Implicit Learning of Scene Geometry from Poses for Global Localization","abstract":"Global visual localization estimates the absolute pose of a camera using a single image, in a previously mapped area. Obtaining the pose from a single image enables many robotics and augmented/virtual reality applications. Inspired by latest advances in deep learning, many existing approaches directly learn and regress 6 DoF pose from an input image. However, these methods do not fully utilize the underlying scene geometry for pose regression. The challenge in monocular relocalization is the minimal availability of supervised training data, which is just the corresponding 6 DoF poses of the images. In this paper, we propose to utilize these minimal available labels (.i.e, poses) to learn the underlying 3D geometry of the scene and use the geometry to estimate the 6 DoF camera pose. We present a learning method that uses these pose labels and rigid alignment to learn two 3D geometric representations (\\textit{X, Y, Z coordinates}) of the scene, one in camera coordinate frame and the other in global coordinate frame. Given a single image, it estimates these two 3D scene representations, which are then aligned to estimate a pose that matches the pose label. This formulation allows for the active inclusion of additional learning constraints to minimize 3D alignment errors between the two 3D scene representations, and 2D re-projection errors between the 3D global scene representation and 2D image pixels, resulting in improved localization accuracy. During inference, our model estimates the 3D scene geometry in camera and global frames and aligns them rigidly to obtain pose in real-time. We evaluate our work on three common visual localization datasets, conduct ablation studies, and show that our method exceeds state-of-the-art regression methods' pose accuracy on all datasets.","sentences":["Global visual localization estimates the absolute pose of a camera using a single image, in a previously mapped area.","Obtaining the pose from a single image enables many robotics and augmented/virtual reality applications.","Inspired by latest advances in deep learning, many existing approaches directly learn and regress 6 DoF pose from an input image.","However, these methods do not fully utilize the underlying scene geometry for pose regression.","The challenge in monocular relocalization is the minimal availability of supervised training data, which is just the corresponding 6 DoF poses of the images.","In this paper, we propose to utilize these minimal available labels (.i.e, poses) to learn the underlying 3D geometry of the scene and use the geometry to estimate the 6 DoF camera pose.","We present a learning method that uses these pose labels and rigid alignment to learn two 3D geometric representations (\\textit{X, Y, Z coordinates}) of the scene, one in camera coordinate frame and the other in global coordinate frame.","Given a single image, it estimates these two 3D scene representations, which are then aligned to estimate a pose that matches the pose label.","This formulation allows for the active inclusion of additional learning constraints to minimize 3D alignment errors between the two 3D scene representations, and 2D re-projection errors between the 3D global scene representation and 2D image pixels, resulting in improved localization accuracy.","During inference, our model estimates the 3D scene geometry in camera and global frames and aligns them rigidly to obtain pose in real-time.","We evaluate our work on three common visual localization datasets, conduct ablation studies, and show that our method exceeds state-of-the-art regression methods' pose accuracy on all datasets."],"url":"http://arxiv.org/abs/2312.02029v1"}
{"created":"2023-12-04 16:47:50","title":"Consistency of Relations over Monoids","abstract":"The interplay between local consistency and global consistency has been the object of study in several different areas, including probability theory, relational databases, and quantum information. For relational databases, Beeri, Fagin, Maier, and Yannakakis showed that a database schema is acyclic if and only if it has the local-to-global consistency property for relations, which means that every collection of pairwise consistent relations over the schema is globally consistent. More recently, the same result has been shown under bag semantics. In this paper, we carry out a systematic study of local vs.\\ global consistency for relations over positive commutative monoids, which is a common generalization of ordinary relations and bags. Let $\\mathbb K$ be an arbitrary positive commutative monoid. We begin by showing that acyclicity of the schema is a necessary condition for the local-to-global consistency property for $\\mathbb K$-relations to hold. Unlike the case of ordinary relations and bags, however, we show that acyclicity is not always sufficient. After this, we characterize the positive commutative monoids for which acyclicity is both necessary and sufficient for the local-to-global consistency property to hold; this characterization involves a combinatorial property of monoids, which we call the \\emph{transportation property}. We then identify several different classes of monoids that possess the transportation property. As our final contribution, we introduce a modified notion of local consistency of $\\mathbb{K}$-relations, which we call \\emph{pairwise consistency up to the free cover}. We prove that, for all positive commutative monoids $\\mathbb{K}$, even those without the transportation property, acyclicity is both necessary and sufficient for every family of $\\mathbb{K}$-relations that is pairwise consistent up to the free cover to be globally consistent.","sentences":["The interplay between local consistency and global consistency has been the object of study in several different areas, including probability theory, relational databases, and quantum information.","For relational databases, Beeri, Fagin, Maier, and Yannakakis showed that a database schema is acyclic if and only if it has the local-to-global consistency property for relations, which means that every collection of pairwise consistent relations over the schema is globally consistent.","More recently, the same result has been shown under bag semantics.","In this paper, we carry out a systematic study of local vs.\\ global consistency for relations over positive commutative monoids, which is a common generalization of ordinary relations and bags.","Let $\\mathbb K$ be an arbitrary positive commutative monoid.","We begin by showing that acyclicity of the schema is a necessary condition for the local-to-global consistency property for $\\mathbb K$-relations to hold.","Unlike the case of ordinary relations and bags, however, we show that acyclicity is not always sufficient.","After this, we characterize the positive commutative monoids for which acyclicity is both necessary and sufficient for the local-to-global consistency property to hold; this characterization involves a combinatorial property of monoids, which we call the \\emph{transportation property}.","We then identify several different classes of monoids that possess the transportation property.","As our final contribution, we introduce a modified notion of local consistency of $\\mathbb{K}$-relations, which we call \\emph{pairwise consistency up to the free cover}.","We prove that, for all positive commutative monoids $\\mathbb{K}$, even those without the transportation property, acyclicity is both necessary and sufficient for every family of $\\mathbb{K}$-relations that is pairwise consistent up to the free cover to be globally consistent."],"url":"http://arxiv.org/abs/2312.02023v1"}
{"created":"2023-12-04 16:46:38","title":"VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation","abstract":"Domain generalization (DG) remains a significant challenge for perception based on deep neural networks (DNN), where domain shifts occur due to lighting, weather, or geolocation changes. In this work, we propose VLTSeg to enhance domain generalization in semantic segmentation, where the network is solely trained on the source domain and evaluated on unseen target domains. Our method leverages the inherent semantic robustness of vision-language models. First, by substituting traditional vision-only backbones with pre-trained encoders from CLIP and EVA-CLIP as transfer learning setting we find that in the field of DG, vision-language pre-training significantly outperforms supervised and self-supervised vision pre-training. We thus propose a new vision-language approach for domain generalized segmentation, which improves the domain generalization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset. We further show the superior generalization capabilities of vision-language segmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC benchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test set at the time of writing. Additionally, our approach shows strong in-domain generalization capabilities indicated by 86.1% mIoU on the Cityscapes test set, resulting in a shared first place with the previous SOTA on the current leaderboard at the time of submission.","sentences":["Domain generalization (DG) remains a significant challenge for perception based on deep neural networks (DNN), where domain shifts occur due to lighting, weather, or geolocation changes.","In this work, we propose VLTSeg to enhance domain generalization in semantic segmentation, where the network is solely trained on the source domain and evaluated on unseen target domains.","Our method leverages the inherent semantic robustness of vision-language models.","First, by substituting traditional vision-only backbones with pre-trained encoders from CLIP and EVA-CLIP as transfer learning setting we find that in the field of DG, vision-language pre-training significantly outperforms supervised and self-supervised vision pre-training.","We thus propose a new vision-language approach for domain generalized segmentation, which improves the domain generalization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset.","We further show the superior generalization capabilities of vision-language segmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC benchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test set at the time of writing.","Additionally, our approach shows strong in-domain generalization capabilities indicated by 86.1% mIoU on the Cityscapes test set, resulting in a shared first place with the previous SOTA on the current leaderboard at the time of submission."],"url":"http://arxiv.org/abs/2312.02021v1"}
{"created":"2023-12-04 16:43:36","title":"Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models","abstract":"Unlike most reinforcement learning agents which require an unrealistic amount of environment interactions to learn a new behaviour, humans excel at learning quickly by merely observing and imitating others. This ability highly depends on the fact that humans have a model of their own embodiment that allows them to infer the most likely actions that led to the observed behaviour. In this paper, we propose Action Inference by Maximising Evidence (AIME) to replicate this behaviour using world models. AIME consists of two distinct phases. In the first phase, the agent learns a world model from its past experience to understand its own body by maximising the ELBO. While in the second phase, the agent is given some observation-only demonstrations of an expert performing a novel task and tries to imitate the expert's behaviour. AIME achieves this by defining a policy as an inference model and maximising the evidence of the demonstration under the policy and world model. Our method is \"zero-shot\" in the sense that it does not require further training for the world model or online interactions with the environment after given the demonstration. We empirically validate the zero-shot imitation performance of our method on the Walker and Cheetah embodiment of the DeepMind Control Suite and find it outperforms the state-of-the-art baselines. Code is available at: https://github.com/argmax-ai/aime.","sentences":["Unlike most reinforcement learning agents which require an unrealistic amount of environment interactions to learn a new behaviour, humans excel at learning quickly by merely observing and imitating others.","This ability highly depends on the fact that humans have a model of their own embodiment that allows them to infer the most likely actions that led to the observed behaviour.","In this paper, we propose Action Inference by Maximising Evidence (AIME) to replicate this behaviour using world models.","AIME consists of two distinct phases.","In the first phase, the agent learns a world model from its past experience to understand its own body by maximising the ELBO.","While in the second phase, the agent is given some observation-only demonstrations of an expert performing a novel task and tries to imitate the expert's behaviour.","AIME achieves this by defining a policy as an inference model and maximising the evidence of the demonstration under the policy and world model.","Our method is \"zero-shot\" in the sense that it does not require further training for the world model or online interactions with the environment after given the demonstration.","We empirically validate the zero-shot imitation performance of our method on the Walker and Cheetah embodiment of the DeepMind Control Suite and find it outperforms the state-of-the-art baselines.","Code is available at: https://github.com/argmax-ai/aime."],"url":"http://arxiv.org/abs/2312.02019v1"}
{"created":"2023-12-04 16:38:16","title":"ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence Colonoscopy Reconstruction","abstract":"Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer. However, accurate long-sequence colonoscopy reconstruction faces three major challenges: (1) dissimilarity among segments of the colon due to its meandering and convoluted shape; (2) co-existence of simple and intricately folded geometry structures; (3) sparse viewpoints due to constrained camera trajectories. To tackle these challenges, we introduce a new reconstruction framework based on neural radiance field (NeRF), named ColonNeRF, which leverages neural rendering for novel view synthesis of long-sequence colonoscopy. Specifically, to reconstruct the entire colon in a piecewise manner, our ColonNeRF introduces a region division and integration module, effectively reducing shape dissimilarity and ensuring geometric consistency in each segment. To learn both the simple and complex geometry in a unified framework, our ColonNeRF incorporates a multi-level fusion module that progressively models the colon regions from easy to hard. Additionally, to overcome the challenges from sparse views, we devise a DensiNet module for densifying camera poses under the guidance of semantic consistency. We conduct extensive experiments on both synthetic and real-world datasets to evaluate our ColonNeRF. Quantitatively, our ColonNeRF outperforms existing methods on two benchmarks over four evaluation metrics. Notably, our LPIPS-ALEX scores exhibit a substantial increase of about 67%-85% on the SimCol-to-3D dataset. Qualitatively, our reconstruction visualizations show much clearer textures and more accurate geometric details. These sufficiently demonstrate our superior performance over the state-of-the-art methods.","sentences":["Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.","However, accurate long-sequence colonoscopy reconstruction faces three major challenges: (1) dissimilarity among segments of the colon due to its meandering and convoluted shape; (2) co-existence of simple and intricately folded geometry structures; (3) sparse viewpoints due to constrained camera trajectories.","To tackle these challenges, we introduce a new reconstruction framework based on neural radiance field (NeRF), named ColonNeRF, which leverages neural rendering for novel view synthesis of long-sequence colonoscopy.","Specifically, to reconstruct the entire colon in a piecewise manner, our ColonNeRF introduces a region division and integration module, effectively reducing shape dissimilarity and ensuring geometric consistency in each segment.","To learn both the simple and complex geometry in a unified framework, our ColonNeRF incorporates a multi-level fusion module that progressively models the colon regions from easy to hard.","Additionally, to overcome the challenges from sparse views, we devise a DensiNet module for densifying camera poses under the guidance of semantic consistency.","We conduct extensive experiments on both synthetic and real-world datasets to evaluate our ColonNeRF.","Quantitatively, our ColonNeRF outperforms existing methods on two benchmarks over four evaluation metrics.","Notably, our LPIPS-ALEX scores exhibit a substantial increase of about 67%-85% on the SimCol-to-3D dataset.","Qualitatively, our reconstruction visualizations show much clearer textures and more accurate geometric details.","These sufficiently demonstrate our superior performance over the state-of-the-art methods."],"url":"http://arxiv.org/abs/2312.02015v1"}
{"created":"2023-12-04 16:36:29","title":"Optimal Data Generation in Multi-Dimensional Parameter Spaces, using Bayesian Optimization","abstract":"Acquiring a substantial number of data points for training accurate machine learning (ML) models is a big challenge in scientific fields where data collection is resource-intensive. Here, we propose a novel approach for constructing a minimal yet highly informative database for training ML models in complex multi-dimensional parameter spaces. To achieve this, we mimic the underlying relation between the output and input parameters using Gaussian process regression (GPR). Using a set of known data, GPR provides predictive means and standard deviation for the unknown data. Given the predicted standard deviation by GPR, we select data points using Bayesian optimization to obtain an efficient database for training ML models. We compare the performance of ML models trained on databases obtained through this method, with databases obtained using traditional approaches. Our results demonstrate that the ML models trained on the database obtained using Bayesian optimization approach consistently outperform the other two databases, achieving high accuracy with a significantly smaller number of data points. Our work contributes to the resource-efficient collection of data in high-dimensional complex parameter spaces, to achieve high precision machine learning predictions.","sentences":["Acquiring a substantial number of data points for training accurate machine learning (ML) models is a big challenge in scientific fields where data collection is resource-intensive.","Here, we propose a novel approach for constructing a minimal yet highly informative database for training ML models in complex multi-dimensional parameter spaces.","To achieve this, we mimic the underlying relation between the output and input parameters using Gaussian process regression (GPR).","Using a set of known data, GPR provides predictive means and standard deviation for the unknown data.","Given the predicted standard deviation by GPR, we select data points using Bayesian optimization to obtain an efficient database for training ML models.","We compare the performance of ML models trained on databases obtained through this method, with databases obtained using traditional approaches.","Our results demonstrate that the ML models trained on the database obtained using Bayesian optimization approach consistently outperform the other two databases, achieving high accuracy with a significantly smaller number of data points.","Our work contributes to the resource-efficient collection of data in high-dimensional complex parameter spaces, to achieve high precision machine learning predictions."],"url":"http://arxiv.org/abs/2312.02012v1"}
{"created":"2023-12-04 16:34:31","title":"What is the disinformation problem? Reviewing the dominant paradigm and motivating an alternative sociopolitical view","abstract":"Disinformation research has proliferated in reaction to widespread false, problematic beliefs purported to explain major social phenomena. Yet while the effects of disinformation are well-known, there is less consensus about its causes; the research spans several disciplines, each focusing on different pieces. This article contributes to this growing field by reviewing prevalent U.S. disinformation discourse (academic writing, media, and corporate and government narrative) and outlining the dominant understanding, or paradigm, of the disinformation problem by analyzing cross-disciplinary discourse about the content, individual, group, and institutional layers of the problem. The result is an individualistic explanation largely blaming social media, malicious individuals or nations, and irrational people. Yet this understanding has shortcomings: notably, that its limited, individualistic views of truth and rationality obscures the influence of oppressive ideologies and media or domestic actors in creating flawed worldviews and spreading disinformation. The article then concludes by putting forth an alternative, sociopolitical paradigm that allows subjective models of the world to govern rationality and information processing -- largely informed by social and group identity -- which are being formed and catered to by institutional actors (corporations, media, political parties, and the government) to maintain or gain legitimacy for their actions.","sentences":["Disinformation research has proliferated in reaction to widespread false, problematic beliefs purported to explain major social phenomena.","Yet while the effects of disinformation are well-known, there is less consensus about its causes; the research spans several disciplines, each focusing on different pieces.","This article contributes to this growing field by reviewing prevalent U.S. disinformation discourse (academic writing, media, and corporate and government narrative) and outlining the dominant understanding, or paradigm, of the disinformation problem by analyzing cross-disciplinary discourse about the content, individual, group, and institutional layers of the problem.","The result is an individualistic explanation largely blaming social media, malicious individuals or nations, and irrational people.","Yet this understanding has shortcomings: notably, that its limited, individualistic views of truth and rationality obscures the influence of oppressive ideologies and media or domestic actors in creating flawed worldviews and spreading disinformation.","The article then concludes by putting forth an alternative, sociopolitical paradigm that allows subjective models of the world to govern rationality and information processing -- largely informed by social and group identity -- which are being formed and catered to by institutional actors (corporations, media, political parties, and the government) to maintain or gain legitimacy for their actions."],"url":"http://arxiv.org/abs/2312.02011v1"}
{"created":"2023-12-04 16:32:51","title":"Towards Learning a Generalist Model for Embodied Navigation","abstract":"Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries. Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios. Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation. Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based instruction. The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks. This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide range of capabilities required by embodied navigation. We conduct extensive experiments to evaluate the performance and generalizability of our model. The experimental results demonstrate that our unified model achieves state-of-the-art performance on CVDN, SOON, and ScanQA. Specifically, it surpasses the previous stats-of-the-art method by a significant margin of 29% in goal progress on CVDN. Moreover, our model also demonstrates strong generalizability and presents impressive results on unseen tasks, e.g., embodied question answering and 3D captioning.","sentences":["Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries.","Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios.","Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation.","Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM.","It adapts LLMs to embodied navigation by introducing schema-based instruction.","The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks.","This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide range of capabilities required by embodied navigation.","We conduct extensive experiments to evaluate the performance and generalizability of our model.","The experimental results demonstrate that our unified model achieves state-of-the-art performance on CVDN, SOON, and ScanQA.","Specifically, it surpasses the previous stats-of-the-art method by a significant margin of 29% in goal progress on CVDN.","Moreover, our model also demonstrates strong generalizability and presents impressive results on unseen tasks, e.g., embodied question answering and 3D captioning."],"url":"http://arxiv.org/abs/2312.02010v1"}
{"created":"2023-12-04 16:30:19","title":"Multi-Agent Behavior Retrieval","abstract":"This paper aims to enable multi-agent systems to effectively utilize past memories to adapt to novel collaborative tasks in a data-efficient fashion. We propose the Multi-Agent Coordination Skill Database, a repository for storing a collection of coordinated behaviors associated with the key vector distinctive to them. Our Transformer-based skill encoder effectively captures spatio-temporal interactions that contribute to coordination and provide a skill representation unique to each coordinated behavior. By leveraging a small number of demonstrations of the target task, the database allows us to train the policy using a dataset augmented with the retrieved demonstrations. Experimental evaluations clearly demonstrate that our method achieves a significantly higher success rate in push manipulation tasks compared to baseline methods like few-shot imitation learning. Furthermore, we validate the effectiveness of our retrieve-and-learn framework in a real environment using a team of wheeled robots.","sentences":["This paper aims to enable multi-agent systems to effectively utilize past memories to adapt to novel collaborative tasks in a data-efficient fashion.","We propose the Multi-Agent Coordination Skill Database, a repository for storing a collection of coordinated behaviors associated with the key vector distinctive to them.","Our Transformer-based skill encoder effectively captures spatio-temporal interactions that contribute to coordination and provide a skill representation unique to each coordinated behavior.","By leveraging a small number of demonstrations of the target task, the database allows us to train the policy using a dataset augmented with the retrieved demonstrations.","Experimental evaluations clearly demonstrate that our method achieves a significantly higher success rate in push manipulation tasks compared to baseline methods like few-shot imitation learning.","Furthermore, we validate the effectiveness of our retrieve-and-learn framework in a real environment using a team of wheeled robots."],"url":"http://arxiv.org/abs/2312.02008v1"}
{"created":"2023-12-04 16:25:18","title":"A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly","abstract":"Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes findings into \"The Good\" (beneficial LLM applications), \"The Bad\" (offensive applications), and \"The Ugly\" (vulnerabilities and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code and data security, outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.","sentences":["Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation.","They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation).","In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks.","This paper explores the intersection of LLMs with security and privacy.","Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs.","Through a comprehensive literature review, the paper categorizes findings into \"The Good\" (beneficial LLM applications), \"The Bad\" (offensive applications), and \"The Ugly\" (vulnerabilities and their defenses).","We have some interesting findings.","For example, LLMs have proven to enhance code and data security, outperforming traditional methods.","However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities.","We have identified areas that require further research efforts.","For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality.","Safe instruction tuning, a recent development, requires more exploration.","We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity."],"url":"http://arxiv.org/abs/2312.02003v1"}
{"created":"2023-12-04 16:22:06","title":"Language-only Efficient Training of Zero-shot Composed Image Retrieval","abstract":"Composed image retrieval (CIR) task takes a composed query of image and text, aiming to search relative images for both conditions. Conventional CIR approaches need a training dataset composed of triplets of query image, query text, and target image, which is very expensive to collect. Several recent works have worked on the zero-shot (ZS) CIR paradigm to tackle the issue without using pre-collected triplets. However, the existing ZS-CIR methods show limited backbone scalability and generalizability due to the lack of diversity of the input texts during training. We propose a novel CIR framework, only using language for its training. Our LinCIR (Language-only training for CIR) can be trained only with text datasets by a novel self-supervision named self-masking projection (SMP). We project the text latent embedding to the token embedding space and construct a new text by replacing the keyword tokens of the original text. Then, we let the new and original texts have the same latent embedding vector. With this simple strategy, LinCIR is surprisingly efficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in 48 minutes and shows the best ZS-CIR performances on four different CIR benchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised method on FashionIQ. Code is available at https://github.com/navervision/lincir","sentences":["Composed image retrieval (CIR) task takes a composed query of image and text, aiming to search relative images for both conditions.","Conventional CIR approaches need a training dataset composed of triplets of query image, query text, and target image, which is very expensive to collect.","Several recent works have worked on the zero-shot (ZS) CIR paradigm to tackle the issue without using pre-collected triplets.","However, the existing ZS-CIR methods show limited backbone scalability and generalizability due to the lack of diversity of the input texts during training.","We propose a novel CIR framework, only using language for its training.","Our LinCIR (Language-only training for CIR) can be trained only with text datasets by a novel self-supervision named self-masking projection (SMP).","We project the text latent embedding to the token embedding space and construct a new text by replacing the keyword tokens of the original text.","Then, we let the new and original texts have the same latent embedding vector.","With this simple strategy, LinCIR is surprisingly efficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in 48 minutes and shows the best ZS-CIR performances on four different CIR benchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised method on FashionIQ.","Code is available at https://github.com/navervision/lincir"],"url":"http://arxiv.org/abs/2312.01998v1"}
{"created":"2023-12-04 16:14:43","title":"A Generative Self-Supervised Framework using Functional Connectivity in fMRI Data","abstract":"Deep neural networks trained on Functional Connectivity (FC) networks extracted from functional Magnetic Resonance Imaging (fMRI) data have gained popularity due to the increasing availability of data and advances in model architectures, including Graph Neural Network (GNN). Recent research on the application of GNN to FC suggests that exploiting the time-varying properties of the FC could significantly improve the accuracy and interpretability of the model prediction. However, the high cost of acquiring high-quality fMRI data and corresponding phenotypic labels poses a hurdle to their application in real-world settings, such that a model na\\\"ively trained in a supervised fashion can suffer from insufficient performance or a lack of generalization on a small number of data. In addition, most Self-Supervised Learning (SSL) approaches for GNNs to date adopt a contrastive strategy, which tends to lose appropriate semantic information when the graph structure is perturbed or does not leverage both spatial and temporal information simultaneously. In light of these challenges, we propose a generative SSL approach that is tailored to effectively harness spatio-temporal information within dynamic FC. Our empirical results, experimented with large-scale (>50,000) fMRI datasets, demonstrate that our approach learns valuable representations and enables the construction of accurate and robust models when fine-tuned for downstream tasks.","sentences":["Deep neural networks trained on Functional Connectivity (FC) networks extracted from functional Magnetic Resonance Imaging (fMRI) data have gained popularity due to the increasing availability of data and advances in model architectures, including Graph Neural Network (GNN).","Recent research on the application of GNN to FC suggests that exploiting the time-varying properties of the FC could significantly improve the accuracy and interpretability of the model prediction.","However, the high cost of acquiring high-quality fMRI data and corresponding phenotypic labels poses a hurdle to their application in real-world settings, such that a model na\\\"ively trained in a supervised fashion can suffer from insufficient performance or a lack of generalization on a small number of data.","In addition, most Self-Supervised Learning (SSL) approaches for GNNs to date adopt a contrastive strategy, which tends to lose appropriate semantic information when the graph structure is perturbed or does not leverage both spatial and temporal information simultaneously.","In light of these challenges, we propose a generative SSL approach that is tailored to effectively harness spatio-temporal information within dynamic FC.","Our empirical results, experimented with large-scale (>50,000) fMRI datasets, demonstrate that our approach learns valuable representations and enables the construction of accurate and robust models when fine-tuned for downstream tasks."],"url":"http://arxiv.org/abs/2312.01994v1"}
{"created":"2023-12-04 16:10:34","title":"Information Modified K-Nearest Neighbor","abstract":"In this research paper, we introduce a novel classification method aimed at improving the performance of the K-Nearest Neighbors (KNN) algorithm. Our approach leverages Mutual Information (MI) to enhance the significance of weights and draw inspiration from Shapley values, a concept originating from cooperative game theory, to refine value allocation. The fundamental concept underlying KNN is the classification of samples based on the majority thorough their k-nearest neighbors. While both the distances and labels of these neighbors are crucial, traditional KNN assigns equal weight to all samples and prevance considering the varying importance of each neighbor based on their distances and labels.   In the proposed method, known as Information-Modified KNN (IMKNN), we address this issue by introducing a straightforward algorithm. To evaluate the effectiveness of our approach, it is compared with 7 contemporary variants of KNN, as well as the traditional KNN. Each of these variants exhibits its unique advantages and limitations. We conduct experiments on 12 widely-used datasets, assessing the methods' performance in terms of accuracy, precision and recall.   Our study demonstrates that IMKNN consistently outperforms other methods across different datasets and criteria by highlighting its superior performance in various classification tasks. These findings underscore the potential of IMKNN as a valuable tool for enhancing the capabilities of the KNN algorithm in diverse applications.","sentences":["In this research paper, we introduce a novel classification method aimed at improving the performance of the K-Nearest Neighbors (KNN) algorithm.","Our approach leverages Mutual Information (MI) to enhance the significance of weights and draw inspiration from Shapley values, a concept originating from cooperative game theory, to refine value allocation.","The fundamental concept underlying KNN is the classification of samples based on the majority thorough their k-nearest neighbors.","While both the distances and labels of these neighbors are crucial, traditional KNN assigns equal weight to all samples and prevance considering the varying importance of each neighbor based on their distances and labels.   ","In the proposed method, known as Information-Modified KNN (IMKNN), we address this issue by introducing a straightforward algorithm.","To evaluate the effectiveness of our approach, it is compared with 7 contemporary variants of KNN, as well as the traditional KNN.","Each of these variants exhibits its unique advantages and limitations.","We conduct experiments on 12 widely-used datasets, assessing the methods' performance in terms of accuracy, precision and recall.   ","Our study demonstrates that IMKNN consistently outperforms other methods across different datasets and criteria by highlighting its superior performance in various classification tasks.","These findings underscore the potential of IMKNN as a valuable tool for enhancing the capabilities of the KNN algorithm in diverse applications."],"url":"http://arxiv.org/abs/2312.01991v1"}
{"created":"2023-12-04 16:05:27","title":"Geranos: a Novel Tilted-Rotors Aerial Robot for the Transportation of Poles","abstract":"In challenging terrains, constructing structures such as antennas and cable-car masts often requires the use of helicopters to transport loads via ropes. The swinging of the load, exacerbated by wind, impairs positioning accuracy, therefore necessitating precise manual placement by ground crews. This increases costs and risk of injuries. Challenging this paradigm, we present Geranos: a specialized multirotor Unmanned Aerial Vehicle (UAV) designed to enhance aerial transportation and assembly. Geranos demonstrates exceptional prowess in accurately positioning vertical poles, achieving this through an innovative integration of load transport and precision. Its unique ring design mitigates the impact of high pole inertia, while a lightweight two-part grasping mechanism ensures secure load attachment without active force. With four primary propellers countering gravity and four auxiliary ones enhancing lateral precision, Geranos achieves comprehensive position and attitude control around hovering. Our experimental demonstration mimicking antenna/cable-car mast installations showcases Geranos ability in stacking poles (3 kg, 2 m long) with remarkable sub-5 cm placement accuracy, without the need of human manual intervention.","sentences":["In challenging terrains, constructing structures such as antennas and cable-car masts often requires the use of helicopters to transport loads via ropes.","The swinging of the load, exacerbated by wind, impairs positioning accuracy, therefore necessitating precise manual placement by ground crews.","This increases costs and risk of injuries.","Challenging this paradigm, we present Geranos: a specialized multirotor Unmanned Aerial Vehicle (UAV) designed to enhance aerial transportation and assembly.","Geranos demonstrates exceptional prowess in accurately positioning vertical poles, achieving this through an innovative integration of load transport and precision.","Its unique ring design mitigates the impact of high pole inertia, while a lightweight two-part grasping mechanism ensures secure load attachment without active force.","With four primary propellers countering gravity and four auxiliary ones enhancing lateral precision, Geranos achieves comprehensive position and attitude control around hovering.","Our experimental demonstration mimicking antenna/cable-car mast installations showcases Geranos ability in stacking poles (3 kg, 2 m long) with remarkable sub-5 cm placement accuracy, without the need of human manual intervention."],"url":"http://arxiv.org/abs/2312.01988v1"}
{"created":"2023-12-04 16:04:41","title":"Bootstrapping SparseFormers from Vision Foundation Models","abstract":"The recently proposed SparseFormer architecture provides an alternative approach to visual understanding by utilizing a significantly lower number of visual tokens via adjusting RoIs, greatly reducing computational costs while still achieving promising performance. However, training SparseFormers from scratch is still expensive, and scaling up the number of parameters can be challenging. In this paper, we propose to bootstrap SparseFormers from ViT-based vision foundation models in a simple and efficient way. Since the majority of SparseFormer blocks are the standard transformer ones, we can inherit weights from large-scale pre-trained vision transformers and freeze them as much as possible. Therefore, we only need to train the SparseFormer-specific lightweight focusing transformer to adjust token RoIs and fine-tune a few early pre-trained blocks to align the final token representation. In such a way, we can bootstrap SparseFormer architectures from various large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or CLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and without labels or captions within just a few hours. As a result, the bootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9% accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from CLIPs also demonstrates notable zero-shot performance with highly reduced computational cost without seeing any caption during the bootstrapping procedure. In addition, CLIP-bootstrapped SparseFormers, which align the output space with language without seeing a word, can serve as efficient vision encoders in multimodal large language models. Code will be publicly available at https://github.com/showlab/sparseformer","sentences":["The recently proposed SparseFormer architecture provides an alternative approach to visual understanding by utilizing a significantly lower number of visual tokens via adjusting RoIs, greatly reducing computational costs while still achieving promising performance.","However, training SparseFormers from scratch is still expensive, and scaling up the number of parameters can be challenging.","In this paper, we propose to bootstrap SparseFormers from ViT-based vision foundation models in a simple and efficient way.","Since the majority of SparseFormer blocks are the standard transformer ones, we can inherit weights from large-scale pre-trained vision transformers and freeze them as much as possible.","Therefore, we only need to train the SparseFormer-specific lightweight focusing transformer to adjust token RoIs and fine-tune a few early pre-trained blocks to align the final token representation.","In such a way, we can bootstrap SparseFormer architectures from various large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or CLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and without labels or captions within just a few hours.","As a result, the bootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9% accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from CLIPs also demonstrates notable zero-shot performance with highly reduced computational cost without seeing any caption during the bootstrapping procedure.","In addition, CLIP-bootstrapped SparseFormers, which align the output space with language without seeing a word, can serve as efficient vision encoders in multimodal large language models.","Code will be publicly available at https://github.com/showlab/sparseformer"],"url":"http://arxiv.org/abs/2312.01987v1"}
{"created":"2023-12-04 15:59:27","title":"UniGS: Unified Representation for Image Generation and Segmentation","abstract":"This paper introduces a novel unified representation of diffusion models for image generation and segmentation. Specifically, we use a colormap to represent entity-level masks, addressing the challenge of varying entity numbers while aligning the representation closely with the image RGB domain. Two novel modules, including the location-aware color palette and progressive dichotomy module, are proposed to support our mask representation. On the one hand, a location-aware palette guarantees the colors' consistency to entities' locations. On the other hand, the progressive dichotomy module can efficiently decode the synthesized colormap to high-quality entity-level masks in a depth-first binary search without knowing the cluster numbers. To tackle the issue of lacking large-scale segmentation training data, we employ an inpainting pipeline and then improve the flexibility of diffusion models across various tasks, including inpainting, image synthesis, referring segmentation, and entity segmentation. Comprehensive experiments validate the efficiency of our approach, demonstrating comparable segmentation mask quality to state-of-the-art and adaptability to multiple tasks. The code will be released at \\href{https://github.com/qqlu/Entity}{https://github.com/qqlu/Entity}.","sentences":["This paper introduces a novel unified representation of diffusion models for image generation and segmentation.","Specifically, we use a colormap to represent entity-level masks, addressing the challenge of varying entity numbers while aligning the representation closely with the image RGB domain.","Two novel modules, including the location-aware color palette and progressive dichotomy module, are proposed to support our mask representation.","On the one hand, a location-aware palette guarantees the colors' consistency to entities' locations.","On the other hand, the progressive dichotomy module can efficiently decode the synthesized colormap to high-quality entity-level masks in a depth-first binary search without knowing the cluster numbers.","To tackle the issue of lacking large-scale segmentation training data, we employ an inpainting pipeline and then improve the flexibility of diffusion models across various tasks, including inpainting, image synthesis, referring segmentation, and entity segmentation.","Comprehensive experiments validate the efficiency of our approach, demonstrating comparable segmentation mask quality to state-of-the-art and adaptability to multiple tasks.","The code will be released at \\href{https://github.com/qqlu/Entity}{https://github.com/qqlu/Entity}."],"url":"http://arxiv.org/abs/2312.01985v1"}
{"created":"2023-12-04 15:50:16","title":"Unveiling Competition Dynamics in Mobile App Markets through User Reviews","abstract":"User reviews published in mobile app repositories are essential for understanding user satisfaction and engagement within a specific market segment. Manual analysis of these reviews is impractical due to the large volume of available data, while automatic analysis poses several challenges, including data synthesis and effective reporting. These challenges complicate the task for app providers in identifying hidden patterns and significant events related to app acceptance, especially in assessing the influence of competitor apps. Furthermore, review-based analysis is mostly limited to a single app or a single app provider, excluding potential market and competition analysis. Following a case-study research method in the microblogging app market, we introduce an automatic, novel approach to support mobile app market analysis processes through quantitative metrics and event detection techniques based on newly published user reviews. Significant events are proactively identified and summarized by comparing metric deviations with historical baseline indicators within the lifecycle of a mobile app. Results from our case study show empirical evidence of the detection of relevant events within the selected market segment, including software- or release-based events, contextual events and the emergence of new competitors.","sentences":["User reviews published in mobile app repositories are essential for understanding user satisfaction and engagement within a specific market segment.","Manual analysis of these reviews is impractical due to the large volume of available data, while automatic analysis poses several challenges, including data synthesis and effective reporting.","These challenges complicate the task for app providers in identifying hidden patterns and significant events related to app acceptance, especially in assessing the influence of competitor apps.","Furthermore, review-based analysis is mostly limited to a single app or a single app provider, excluding potential market and competition analysis.","Following a case-study research method in the microblogging app market, we introduce an automatic, novel approach to support mobile app market analysis processes through quantitative metrics and event detection techniques based on newly published user reviews.","Significant events are proactively identified and summarized by comparing metric deviations with historical baseline indicators within the lifecycle of a mobile app.","Results from our case study show empirical evidence of the detection of relevant events within the selected market segment, including software- or release-based events, contextual events and the emergence of new competitors."],"url":"http://arxiv.org/abs/2312.01981v1"}
{"created":"2023-12-04 15:41:41","title":"Computing Repairs Under Functional and Inclusion Dependencies via Argumentation","abstract":"We discover a connection between finding subset-maximal repairs for sets of functional and inclusion dependencies, and computing extensions within argumentation frameworks (AFs). We study the complexity of the existence of a repair and deciding whether a given tuple belongs to some (or every) repair, by simulating the instances of these problems via AFs. We prove that subset-maximal repairs under functional dependencies correspond to the naive extensions, which also coincide with the preferred and stable extensions in the resulting AFs. For inclusion dependencies, one needs a pre-processing step on the resulting AFs in order for the extensions to coincide. Allowing both types of dependencies breaks this relationship between extensions, and only preferred semantics captures the repairs. Finally, we establish that the complexities of the above decision problems are NP-complete and Pi_2^P-complete, when both functional and inclusion dependencies are allowed.","sentences":["We discover a connection between finding subset-maximal repairs for sets of functional and inclusion dependencies, and computing extensions within argumentation frameworks (AFs).","We study the complexity of the existence of a repair and deciding whether a given tuple belongs to some (or every) repair, by simulating the instances of these problems via AFs.","We prove that subset-maximal repairs under functional dependencies correspond to the naive extensions, which also coincide with the preferred and stable extensions in the resulting AFs.","For inclusion dependencies, one needs a pre-processing step on the resulting AFs in order for the extensions to coincide.","Allowing both types of dependencies breaks this relationship between extensions, and only preferred semantics captures the repairs.","Finally, we establish that the complexities of the above decision problems are NP-complete and Pi_2^P-complete, when both functional and inclusion dependencies are allowed."],"url":"http://arxiv.org/abs/2312.01973v1"}
{"created":"2023-12-04 15:33:00","title":"CaRL: Cascade Reinforcement Learning with State Space Splitting for O-RAN based Traffic Steering","abstract":"The Open Radio Access Network (O-RAN) architecture empowers intelligent and automated optimization of the RAN through applications deployed on the RAN Intelligent Controller (RIC) platform, enabling capabilities beyond what is achievable with traditional RAN solutions. Within this paradigm, Traffic Steering (TS) emerges as a pivotal RIC application that focuses on optimizing cell-level mobility settings in near-real-time, aiming to significantly improve network spectral efficiency. In this paper, we design a novel TS algorithm based on a Cascade Reinforcement Learning (CaRL) framework. We propose state space factorization and policy decomposition to reduce the need for large models and well-labeled datasets. For each sub-state space, an RL sub-policy will be trained to learn an optimized mapping onto the action space. To apply CaRL on new network regions, we propose a knowledge transfer approach to initialize a new sub-policy based on knowledge learned by the trained policies. To evaluate CaRL, we build a data-driven and scalable RIC digital twin (DT) that is modeled using important real-world data, including network configuration, user geo-distribution, and traffic demand, among others, from a tier-1 mobile operator in the US. We evaluate CaRL on two DT scenarios representing two network clusters in two different cities and compare its performance with the business-as-usual (BAU) policy and other competing optimization approaches using heuristic and Q-table algorithms. Benchmarking results show that CaRL performs the best and improves the average cluster-aggregated downlink throughput over the BAU policy by 24% and 18% in these two scenarios, respectively.","sentences":["The Open Radio Access Network (O-RAN) architecture empowers intelligent and automated optimization of the RAN through applications deployed on the RAN Intelligent Controller (RIC) platform, enabling capabilities beyond what is achievable with traditional RAN solutions.","Within this paradigm, Traffic Steering (TS) emerges as a pivotal RIC application that focuses on optimizing cell-level mobility settings in near-real-time, aiming to significantly improve network spectral efficiency.","In this paper, we design a novel TS algorithm based on a Cascade Reinforcement Learning (CaRL) framework.","We propose state space factorization and policy decomposition to reduce the need for large models and well-labeled datasets.","For each sub-state space, an RL sub-policy will be trained to learn an optimized mapping onto the action space.","To apply CaRL on new network regions, we propose a knowledge transfer approach to initialize a new sub-policy based on knowledge learned by the trained policies.","To evaluate CaRL, we build a data-driven and scalable RIC digital twin (DT) that is modeled using important real-world data, including network configuration, user geo-distribution, and traffic demand, among others, from a tier-1 mobile operator in the US.","We evaluate CaRL on two DT scenarios representing two network clusters in two different cities and compare its performance with the business-as-usual (BAU) policy and other competing optimization approaches using heuristic and Q-table algorithms.","Benchmarking results show that CaRL performs the best and improves the average cluster-aggregated downlink throughput over the BAU policy by 24% and 18% in these two scenarios, respectively."],"url":"http://arxiv.org/abs/2312.01970v1"}
{"created":"2023-12-04 15:26:46","title":"Augmenting Channel Charting with Classical Wireless Source Localization Techniques","abstract":"Channel Charting aims to construct a map of the radio environment by leveraging similarity relationships found in high-dimensional channel state information. Although resulting channel charts usually accurately represent local neighborhood relationships, even under conditions with strong multipath propagation, they often fall short in capturing global geometric features. On the other hand, classical model-based localization methods, such as triangulation and multilateration, can easily localize signal sources in the global coordinate frame. However, these methods rely heavily on the assumption of line-of-sight channels and distributed antenna deployments. Based on measured data, we compare classical source localization techniques to channel charts with respect to localization performance. We suggest and evaluate methods to enhance Channel Charting with model-based localization approaches: One approach involves using information derived from classical localization methods to map channel chart locations to physical positions after conventional training of the forward charting function. Foremost, though, we suggest to incorporate information from model-based approaches during the training of the forward charting function in what we call \"augmented Channel Charting\". We demonstrate that Channel Charting can outperform classical localization methods on the considered dataset.","sentences":["Channel Charting aims to construct a map of the radio environment by leveraging similarity relationships found in high-dimensional channel state information.","Although resulting channel charts usually accurately represent local neighborhood relationships, even under conditions with strong multipath propagation, they often fall short in capturing global geometric features.","On the other hand, classical model-based localization methods, such as triangulation and multilateration, can easily localize signal sources in the global coordinate frame.","However, these methods rely heavily on the assumption of line-of-sight channels and distributed antenna deployments.","Based on measured data, we compare classical source localization techniques to channel charts with respect to localization performance.","We suggest and evaluate methods to enhance Channel Charting with model-based localization approaches: One approach involves using information derived from classical localization methods to map channel chart locations to physical positions after conventional training of the forward charting function.","Foremost, though, we suggest to incorporate information from model-based approaches during the training of the forward charting function in what we call \"augmented Channel Charting\".","We demonstrate that Channel Charting can outperform classical localization methods on the considered dataset."],"url":"http://arxiv.org/abs/2312.01968v1"}
{"created":"2023-12-04 15:23:49","title":"Semantics-aware Motion Retargeting with Vision-Language Models","abstract":"Capturing and preserving motion semantics is essential to motion retargeting between animation characters. However, most of the previous works neglect the semantic information or rely on human-designed joint-level representations. Here, we present a novel Semantics-aware Motion reTargeting (SMT) method with the advantage of vision-language models to extract and maintain meaningful motion semantics. We utilize a differentiable module to render 3D motions. Then the high-level motion semantics are incorporated into the motion retargeting process by feeding the vision-language model with the rendered images and aligning the extracted semantic embeddings. To ensure the preservation of fine-grained motion details and high-level semantics, we adopt a two-stage pipeline consisting of skeleton-aware pre-training and fine-tuning with semantics and geometry constraints. Experimental results show the effectiveness of the proposed method in producing high-quality motion retargeting results while accurately preserving motion semantics. Project page can be found at https://sites.google.com/view/smtnet.","sentences":["Capturing and preserving motion semantics is essential to motion retargeting between animation characters.","However, most of the previous works neglect the semantic information or rely on human-designed joint-level representations.","Here, we present a novel Semantics-aware Motion reTargeting (SMT) method with the advantage of vision-language models to extract and maintain meaningful motion semantics.","We utilize a differentiable module to render 3D motions.","Then the high-level motion semantics are incorporated into the motion retargeting process by feeding the vision-language model with the rendered images and aligning the extracted semantic embeddings.","To ensure the preservation of fine-grained motion details and high-level semantics, we adopt a two-stage pipeline consisting of skeleton-aware pre-training and fine-tuning with semantics and geometry constraints.","Experimental results show the effectiveness of the proposed method in producing high-quality motion retargeting results while accurately preserving motion semantics.","Project page can be found at https://sites.google.com/view/smtnet."],"url":"http://arxiv.org/abs/2312.01964v1"}
{"created":"2023-12-04 15:16:42","title":"Learning-Based Approaches to Predictive Monitoring with Conformal Statistical Guarantees","abstract":"This tutorial focuses on efficient methods to predictive monitoring (PM), the problem of detecting at runtime future violations of a given requirement from the current state of a system. While performing model checking at runtime would offer a precise solution to the PM problem, it is generally computationally expensive. To address this scalability issue, several lightweight approaches based on machine learning have recently been proposed. These approaches work by learning an approximate yet efficient surrogate (deep learning) model of the expensive model checker. A key challenge remains to ensure reliable predictions, especially in safety-critical applications. We review our recent work on predictive monitoring, one of the first to propose learning-based approximations for CPS verification of temporal logic specifications and the first in this context to apply conformal prediction (CP) for rigorous uncertainty quantification. These CP-based uncertainty estimators offer statistical guarantees regarding the generalization error of the learning model, and they can be used to determine unreliable predictions that should be rejected. In this tutorial, we present a general and comprehensive framework summarizing our approach to the predictive monitoring of CPSs, examining in detail several variants determined by three main dimensions: system dynamics (deterministic, non-deterministic, stochastic), state observability, and semantics of requirements' satisfaction (Boolean or quantitative).","sentences":["This tutorial focuses on efficient methods to predictive monitoring (PM), the problem of detecting at runtime future violations of a given requirement from the current state of a system.","While performing model checking at runtime would offer a precise solution to the PM problem, it is generally computationally expensive.","To address this scalability issue, several lightweight approaches based on machine learning have recently been proposed.","These approaches work by learning an approximate yet efficient surrogate (deep learning) model of the expensive model checker.","A key challenge remains to ensure reliable predictions, especially in safety-critical applications.","We review our recent work on predictive monitoring, one of the first to propose learning-based approximations for CPS verification of temporal logic specifications and the first in this context to apply conformal prediction (CP) for rigorous uncertainty quantification.","These CP-based uncertainty estimators offer statistical guarantees regarding the generalization error of the learning model, and they can be used to determine unreliable predictions that should be rejected.","In this tutorial, we present a general and comprehensive framework summarizing our approach to the predictive monitoring of CPSs, examining in detail several variants determined by three main dimensions: system dynamics (deterministic, non-deterministic, stochastic), state observability, and semantics of requirements' satisfaction (Boolean or quantitative)."],"url":"http://arxiv.org/abs/2312.01959v1"}
{"created":"2023-12-04 15:16:34","title":"Mechanical Comparison of Arrangement Strategies for Topological Interlocking Assemblies","abstract":"Topological Interlocking assemblies are arrangements of blocks kinematically constrained by a fixed frame, such that all rigid body motions of each block are constrained only by its permanent contact with other blocks and the frame. In the literature several blocks are introduced that can be arranged into different interlocking assemblies. In this study we investigate the influence of arrangement on the overall structural behaviour of the resulting interlocking assemblies. This is performed using the Versatile Block, as it can be arranged in three different doubly periodic ways given by wallpaper symmetries. Our focus lies on the load transfer mechanisms from the assembly onto the frame. For fast a priori evaluation of the assemblies we introduce a combinatorial model called Interlocking Flows. To investigate our assemblies from a mechanical point of view we conduct several finite element studies. These reveal a strong influence of arrangement on the structural behaviour, for instance, an impact on both the point and amount of maximum deflection. The results of the finite element analysis are in very good agreement with the predictions of the Interlocking Flow model. Our source code, data and examples are available under https://doi.org/10.5281/zenodo.10246034.","sentences":["Topological Interlocking assemblies are arrangements of blocks kinematically constrained by a fixed frame, such that all rigid body motions of each block are constrained only by its permanent contact with other blocks and the frame.","In the literature several blocks are introduced that can be arranged into different interlocking assemblies.","In this study we investigate the influence of arrangement on the overall structural behaviour of the resulting interlocking assemblies.","This is performed using the Versatile Block, as it can be arranged in three different doubly periodic ways given by wallpaper symmetries.","Our focus lies on the load transfer mechanisms from the assembly onto the frame.","For fast a priori evaluation of the assemblies we introduce a combinatorial model called Interlocking Flows.","To investigate our assemblies from a mechanical point of view we conduct several finite element studies.","These reveal a strong influence of arrangement on the structural behaviour, for instance, an impact on both the point and amount of maximum deflection.","The results of the finite element analysis are in very good agreement with the predictions of the Interlocking Flow model.","Our source code, data and examples are available under https://doi.org/10.5281/zenodo.10246034."],"url":"http://arxiv.org/abs/2312.01958v1"}
{"created":"2023-12-04 15:16:12","title":"Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective","abstract":"This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \\url{https://github.com/vicgalle/distilled-self-critique}.","sentences":["This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model.","Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs.","Code released at \\url{https://github.com/vicgalle/distilled-self-critique}."],"url":"http://arxiv.org/abs/2312.01957v1"}
