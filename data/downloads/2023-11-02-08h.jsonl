{"created":"2023-11-01 17:55:09","title":"End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation","abstract":"Conventional speech-to-text translation (ST) systems are trained on single-speaker utterances, and they may not generalize to real-life scenarios where the audio contains conversations by multiple speakers. In this paper, we tackle single-channel multi-speaker conversational ST with an end-to-end and multi-task training model, named Speaker-Turn Aware Conversational Speech Translation, that combines automatic speech recognition, speech translation and speaker turn detection using special tokens in a serialized labeling format. We run experiments on the Fisher-CALLHOME corpus, which we adapted by merging the two single-speaker channels into one multi-speaker channel, thus representing the more realistic and challenging scenario with multi-speaker turns and cross-talk. Experimental results across single- and multi-speaker conditions and against conventional ST systems, show that our model outperforms the reference systems on the multi-speaker condition, while attaining comparable performance on the single-speaker condition. We release scripts for data processing and model training.","sentences":["Conventional speech-to-text translation (ST) systems are trained on single-speaker utterances, and they may not generalize to real-life scenarios where the audio contains conversations by multiple speakers.","In this paper, we tackle single-channel multi-speaker conversational ST with an end-to-end and multi-task training model, named Speaker-Turn Aware Conversational Speech Translation, that combines automatic speech recognition, speech translation and speaker turn detection using special tokens in a serialized labeling format.","We run experiments on the Fisher-CALLHOME corpus, which we adapted by merging the two single-speaker channels into one multi-speaker channel, thus representing the more realistic and challenging scenario with multi-speaker turns and cross-talk.","Experimental results across single- and multi-speaker conditions and against conventional ST systems, show that our model outperforms the reference systems on the multi-speaker condition, while attaining comparable performance on the single-speaker condition.","We release scripts for data processing and model training."],"url":"http://arxiv.org/abs/2311.00697v1"}
{"created":"2023-11-01 17:54:49","title":"Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA","abstract":"Population aging is a global challenge, leading to increased demand for healthcare and social services for the elderly. Home Health Care (HHC) emerges as a vital solution, specifically designed to serve this population segment. Given the surging demand for HHC, it's essential to coordinate and regulate caregiver allocation efficiently. This is crucial for both budget-optimized planning and ensuring the delivery of high-quality care. This research addresses a key question faced by home health agencies (HHAs): \"How can caregiver allocation be optimized, especially when caregivers prefer flexibility in their visiting sequences?\". While earlier studies proposed rigid visiting sequences, our study introduces a decision support framework that allocates caregivers through a hybrid method that considers the flexibility in visiting sequences and aims to reduce travel mileage, increase the number of visits per planning period, and maintain the continuity of care - a critical metric for patient satisfaction. Utilizing data from an HHA in Tennessee, United States, our approach led to an impressive reduction in average travel mileage (up to 42% depending on discipline) without imposing restrictions on caregivers. Furthermore, the proposed framework is used for caregivers' supply analysis to provide valuable insights into caregiver resource management.","sentences":["Population aging is a global challenge, leading to increased demand for healthcare and social services for the elderly.","Home Health Care (HHC) emerges as a vital solution, specifically designed to serve this population segment.","Given the surging demand for HHC, it's essential to coordinate and regulate caregiver allocation efficiently.","This is crucial for both budget-optimized planning and ensuring the delivery of high-quality care.","This research addresses a key question faced by home health agencies (HHAs): \"How can caregiver allocation be optimized, especially when caregivers prefer flexibility in their visiting sequences?\".","While earlier studies proposed rigid visiting sequences, our study introduces a decision support framework that allocates caregivers through a hybrid method that considers the flexibility in visiting sequences and aims to reduce travel mileage, increase the number of visits per planning period, and maintain the continuity of care - a critical metric for patient satisfaction.","Utilizing data from an HHA in Tennessee, United States, our approach led to an impressive reduction in average travel mileage (up to 42% depending on discipline) without imposing restrictions on caregivers.","Furthermore, the proposed framework is used for caregivers' supply analysis to provide valuable insights into caregiver resource management."],"url":"http://arxiv.org/abs/2311.00696v1"}
{"created":"2023-11-01 17:52:15","title":"Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving","abstract":"Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additionally, we propose an effective and efficient tournament-based approach to select among these explored solution groups to reach the final answer. Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging problems in the MATH dataset. Code will be released at https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.","sentences":["Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems.","Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains.","However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space.","In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning.","This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction.","The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal.","Additionally, we propose an effective and efficient tournament-based approach to select among these explored solution groups to reach the final answer.","Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging problems in the MATH dataset.","Code will be released at https://github.com/lz1oceani/LLM-As-Hierarchical-Policy."],"url":"http://arxiv.org/abs/2311.00694v1"}
{"created":"2023-11-01 17:51:43","title":"On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval","abstract":"Visually-rich document entity retrieval (VDER), which extracts key information (e.g. date, address) from document images like invoices and receipts, has become an important topic in industrial NLP applications. The emergence of new document types at a constant pace, each with its unique entity types, presents a unique challenge: many documents contain unseen entity types that occur only a couple of times. Addressing this challenge requires models to have the ability of learning entities in a few-shot manner. However, prior works for Few-shot VDER mainly address the problem at the document level with a predefined global entity space, which doesn't account for the entity-level few-shot scenario: target entity types are locally personalized by each task and entity occurrences vary significantly among documents. To address this unexplored scenario, this paper studies a novel entity-level few-shot VDER task. The challenges lie in the uniqueness of the label space for each task and the increased complexity of out-of-distribution (OOD) contents. To tackle this novel task, we present a task-aware meta-learning based framework, with a central focus on achieving effective task personalization that distinguishes between in-task and out-of-task distribution. Specifically, we adopt a hierarchical decoder (HC) and employ contrastive learning (ContrastProtoNet) to achieve this goal. Furthermore, we introduce a new dataset, FewVEX, to boost future research in the field of entity-level few-shot VDER. Experimental results demonstrate our approaches significantly improve the robustness of popular meta-learning baselines.","sentences":["Visually-rich document entity retrieval (VDER), which extracts key information (e.g. date, address) from document images like invoices and receipts, has become an important topic in industrial NLP applications.","The emergence of new document types at a constant pace, each with its unique entity types, presents a unique challenge: many documents contain unseen entity types that occur only a couple of times.","Addressing this challenge requires models to have the ability of learning entities in a few-shot manner.","However, prior works for Few-shot VDER mainly address the problem at the document level with a predefined global entity space, which doesn't account for the entity-level few-shot scenario: target entity types are locally personalized by each task and entity occurrences vary significantly among documents.","To address this unexplored scenario, this paper studies a novel entity-level few-shot VDER task.","The challenges lie in the uniqueness of the label space for each task and the increased complexity of out-of-distribution (OOD) contents.","To tackle this novel task, we present a task-aware meta-learning based framework, with a central focus on achieving effective task personalization that distinguishes between in-task and out-of-task distribution.","Specifically, we adopt a hierarchical decoder (HC) and employ contrastive learning (ContrastProtoNet) to achieve this goal.","Furthermore, we introduce a new dataset, FewVEX, to boost future research in the field of entity-level few-shot VDER.","Experimental results demonstrate our approaches significantly improve the robustness of popular meta-learning baselines."],"url":"http://arxiv.org/abs/2311.00693v1"}
{"created":"2023-11-01 17:46:07","title":"Software Repositories and Machine Learning Research in Cyber Security","abstract":"In today's rapidly evolving technological landscape and advanced software development, the rise in cyber security attacks has become a pressing concern. The integration of robust cyber security defenses has become essential across all phases of software development. It holds particular significance in identifying critical cyber security vulnerabilities at the initial stages of the software development life cycle, notably during the requirement phase. Through the utilization of cyber security repositories like The Common Attack Pattern Enumeration and Classification (CAPEC) from MITRE and the Common Vulnerabilities and Exposures (CVE) databases, attempts have been made to leverage topic modeling and machine learning for the detection of these early-stage vulnerabilities in the software requirements process. Past research themes have returned successful outcomes in attempting to automate vulnerability identification for software developers, employing a mixture of unsupervised machine learning methodologies such as LDA and topic modeling. Looking ahead, in our pursuit to improve automation and establish connections between software requirements and vulnerabilities, our strategy entails adopting a variety of supervised machine learning techniques. This array encompasses Support Vector Machines (SVM), Na\\\"ive Bayes, random forest, neural networking and eventually transitioning into deep learning for our investigation. In the face of the escalating complexity of cyber security, the question of whether machine learning can enhance the identification of vulnerabilities in diverse software development scenarios is a paramount consideration, offering crucial assistance to software developers in developing secure software.","sentences":["In today's rapidly evolving technological landscape and advanced software development, the rise in cyber security attacks has become a pressing concern.","The integration of robust cyber security defenses has become essential across all phases of software development.","It holds particular significance in identifying critical cyber security vulnerabilities at the initial stages of the software development life cycle, notably during the requirement phase.","Through the utilization of cyber security repositories like The Common Attack Pattern Enumeration and Classification (CAPEC) from MITRE and the Common Vulnerabilities and Exposures (CVE) databases, attempts have been made to leverage topic modeling and machine learning for the detection of these early-stage vulnerabilities in the software requirements process.","Past research themes have returned successful outcomes in attempting to automate vulnerability identification for software developers, employing a mixture of unsupervised machine learning methodologies such as LDA and topic modeling.","Looking ahead, in our pursuit to improve automation and establish connections between software requirements and vulnerabilities, our strategy entails adopting a variety of supervised machine learning techniques.","This array encompasses Support Vector Machines (SVM), Na\\\"ive Bayes, random forest, neural networking and eventually transitioning into deep learning for our investigation.","In the face of the escalating complexity of cyber security, the question of whether machine learning can enhance the identification of vulnerabilities in diverse software development scenarios is a paramount consideration, offering crucial assistance to software developers in developing secure software."],"url":"http://arxiv.org/abs/2311.00691v1"}
{"created":"2023-11-01 17:45:52","title":"What User Behaviors Make the Differences During the Process of Visual Analytics?","abstract":"The understanding of visual analytics process can benefit visualization researchers from multiple aspects, including improving visual designs and developing advanced interaction functions. However, the log files of user behaviors are still hard to analyze due to the complexity of sensemaking and our lack of knowledge on the related user behaviors. This work presents a study on a comprehensive data collection of user behaviors, and our analysis approach with time-series classification methods. We have chosen a classical visualization application, Covid-19 data analysis, with common analysis tasks covering geo-spatial, time-series and multi-attributes. Our user study collects user behaviors on a diverse set of visualization tasks with two comparable systems, desktop and immersive visualizations. We summarize the classification results with three time-series machine learning algorithms at two scales, and explore the influences of behavior features. Our results reveal that user behaviors can be distinguished during the process of visual analytics and there is a potentially strong association between the physical behaviors of users and the visualization tasks they perform. We also demonstrate the usage of our models by interpreting open sessions of visual analytics, which provides an automatic way to study sensemaking without tedious manual annotations.","sentences":["The understanding of visual analytics process can benefit visualization researchers from multiple aspects, including improving visual designs and developing advanced interaction functions.","However, the log files of user behaviors are still hard to analyze due to the complexity of sensemaking and our lack of knowledge on the related user behaviors.","This work presents a study on a comprehensive data collection of user behaviors, and our analysis approach with time-series classification methods.","We have chosen a classical visualization application, Covid-19 data analysis, with common analysis tasks covering geo-spatial, time-series and multi-attributes.","Our user study collects user behaviors on a diverse set of visualization tasks with two comparable systems, desktop and immersive visualizations.","We summarize the classification results with three time-series machine learning algorithms at two scales, and explore the influences of behavior features.","Our results reveal that user behaviors can be distinguished during the process of visual analytics and there is a potentially strong association between the physical behaviors of users and the visualization tasks they perform.","We also demonstrate the usage of our models by interpreting open sessions of visual analytics, which provides an automatic way to study sensemaking without tedious manual annotations."],"url":"http://arxiv.org/abs/2311.00690v1"}
{"created":"2023-11-01 17:45:22","title":"Collaboration in Immersive Environments: Challenges and Solutions","abstract":"Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in all engineering fields in order to avoid the use of physical prototypes, to train in high-risk situations, and to interpret real or simulated results. In order to complete a shared task or assign tasks to the agents in such immersive environments, collaboration or Shared Cooperative Activities are a necessity. Collaboration in immersive environments is an emerging field of research that aims to study and enhance the ways in which people interact and work together in Virtual and Augmented Reality settings. Collaboration in immersive environments is a complex process that involves different factors such as communication, coordination, and social presence. This paper provides an overview of the current state of research on collaboration in immersive environments. It discusses the different types of immersive environments, including VR and AR, and the different forms of collaboration that can occur in these environments. The paper also highlights the challenges and limitations of collaboration in immersive environments, such as the lack of physical cues, cost and usability and the need for further research in this area. Overall, collaboration in immersive environments is a promising field with a wide range of potential applications, from education to industry, and it can benefit both individuals and groups by enhancing their ability to work together effectively.","sentences":["Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in all engineering fields in order to avoid the use of physical prototypes, to train in high-risk situations, and to interpret real or simulated results.","In order to complete a shared task or assign tasks to the agents in such immersive environments, collaboration or Shared Cooperative Activities are a necessity.","Collaboration in immersive environments is an emerging field of research that aims to study and enhance the ways in which people interact and work together in Virtual and Augmented Reality settings.","Collaboration in immersive environments is a complex process that involves different factors such as communication, coordination, and social presence.","This paper provides an overview of the current state of research on collaboration in immersive environments.","It discusses the different types of immersive environments, including VR and AR, and the different forms of collaboration that can occur in these environments.","The paper also highlights the challenges and limitations of collaboration in immersive environments, such as the lack of physical cues, cost and usability and the need for further research in this area.","Overall, collaboration in immersive environments is a promising field with a wide range of potential applications, from education to industry, and it can benefit both individuals and groups by enhancing their ability to work together effectively."],"url":"http://arxiv.org/abs/2311.00689v1"}
{"created":"2023-11-01 17:44:50","title":"Improving Interpersonal Communication by Simulating Audiences with Language Models","abstract":"How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal communication. For each scenario, we collect a dataset of human evaluations across candidates and baselines, and showcase that our framework's chosen candidate is preferred over popular generation mechanisms including Chain-of-Thought. We also find that audience simulations achieve reasonably high agreement with human raters across 5 of the 8 scenarios. Finally, we demonstrate the generality of our framework by applying it to real-world scenarios described by users on web forums. Through evaluations and demonstrations, we show that EGS enhances the effectiveness and outcomes of goal-oriented communication across a variety of situations, thus opening up new possibilities for the application of large language models in revolutionizing communication and decision-making processes.","sentences":["How do we communicate with others to achieve our goals?","We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received.","However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging.","In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better.","We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve.","EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use.","We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal communication.","For each scenario, we collect a dataset of human evaluations across candidates and baselines, and showcase that our framework's chosen candidate is preferred over popular generation mechanisms including Chain-of-Thought.","We also find that audience simulations achieve reasonably high agreement with human raters across 5 of the 8 scenarios.","Finally, we demonstrate the generality of our framework by applying it to real-world scenarios described by users on web forums.","Through evaluations and demonstrations, we show that EGS enhances the effectiveness and outcomes of goal-oriented communication across a variety of situations, thus opening up new possibilities for the application of large language models in revolutionizing communication and decision-making processes."],"url":"http://arxiv.org/abs/2311.00687v1"}
{"created":"2023-11-01 17:44:35","title":"Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task","abstract":"This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries. We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting. In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures. Our work reveals that combining these approaches using a \"small\", open source model (orca_mini_v3_7B) yields competitive results.","sentences":["This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries.","We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting.","In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures.","Our work reveals that combining these approaches using a \"small\", open source model (orca_mini_v3_7B) yields competitive results."],"url":"http://arxiv.org/abs/2311.00686v1"}
{"created":"2023-11-01 17:43:35","title":"Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation","abstract":"An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\\footnote{\\url{https://github.com/chijames/Attention-Alignment-Transformer-Length-Extrapolation}}","sentences":["An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning.","Such long-context utilization capability highly relies on a flexible positional embedding design.","Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns.","However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution.","To alleviate the issue, we propose two attention alignment strategies via temperature scaling.","Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\\footnote{\\url{https://github.com/chijames/Attention-Alignment-Transformer-Length-Extrapolation}}"],"url":"http://arxiv.org/abs/2311.00684v1"}
{"created":"2023-11-01 17:42:45","title":"Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs","abstract":"In recent years, Large Language Models (LLMs) have gained immense attention due to their notable emergent capabilities, surpassing those seen in earlier language models. A particularly intriguing application of LLMs is their role as evaluators for texts produced by various generative models.   In this study, we delve into the potential of LLMs as reliable assessors of factual consistency in summaries generated by text-generation models. Initially, we introduce an innovative approach for factuality assessment using LLMs. This entails employing a singular LLM for the entirety of the question-answering-based factuality scoring process. Following this, we examine the efficacy of various LLMs in direct factuality scoring, benchmarking them against traditional measures and human annotations.   Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across two factuality subcategories. These consistent findings across various factual error categories suggest a fundamental limitation in the current LLMs' capability to accurately gauge factuality.   This version presents the information more concisely while maintaining the main points and findings of the original text.","sentences":["In recent years, Large Language Models (LLMs) have gained immense attention due to their notable emergent capabilities, surpassing those seen in earlier language models.","A particularly intriguing application of LLMs is their role as evaluators for texts produced by various generative models.   ","In this study, we delve into the potential of LLMs as reliable assessors of factual consistency in summaries generated by text-generation models.","Initially, we introduce an innovative approach for factuality assessment using LLMs.","This entails employing a singular LLM for the entirety of the question-answering-based factuality scoring process.","Following this, we examine the efficacy of various LLMs in direct factuality scoring, benchmarking them against traditional measures and human annotations.   ","Contrary to initial expectations, our results indicate a lack of significant correlations between factuality metrics and human evaluations, specifically for GPT-4 and PaLM-2.","Notable correlations were only observed with GPT-3.5 across two factuality subcategories.","These consistent findings across various factual error categories suggest a fundamental limitation in the current LLMs' capability to accurately gauge factuality.   ","This version presents the information more concisely while maintaining the main points and findings of the original text."],"url":"http://arxiv.org/abs/2311.00681v1"}
{"created":"2023-11-01 17:34:58","title":"Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games","abstract":"Algorithms based on regret matching, specifically regret matching$^+$ (RM$^+$), and its variants are the most popular approaches for solving large-scale two-player zero-sum games in practice. Unlike algorithms such as optimistic gradient descent ascent, which have strong last-iterate and ergodic convergence properties for zero-sum games, virtually nothing is known about the last-iterate properties of regret-matching algorithms. Given the importance of last-iterate convergence for numerical optimization reasons and relevance as modeling real-word learning in games, in this paper, we study the last-iterate convergence properties of various popular variants of RM$^+$. First, we show numerically that several practical variants such as simultaneous RM$^+$, alternating RM$^+$, and simultaneous predictive RM$^+$, all lack last-iterate convergence guarantees even on a simple $3\\times 3$ game. We then prove that recent variants of these algorithms based on a smoothing technique do enjoy last-iterate convergence: we prove that extragradient RM$^{+}$ and smooth Predictive RM$^+$ enjoy asymptotic last-iterate convergence (without a rate) and $1/\\sqrt{t}$ best-iterate convergence. Finally, we introduce restarted variants of these algorithms, and show that they enjoy linear-rate last-iterate convergence.","sentences":["Algorithms based on regret matching, specifically regret matching$^+$ (RM$^+$), and its variants are the most popular approaches for solving large-scale two-player zero-sum games in practice.","Unlike algorithms such as optimistic gradient descent ascent, which have strong last-iterate and ergodic convergence properties for zero-sum games, virtually nothing is known about the last-iterate properties of regret-matching algorithms.","Given the importance of last-iterate convergence for numerical optimization reasons and relevance as modeling real-word learning in games, in this paper, we study the last-iterate convergence properties of various popular variants of RM$^+$. First, we show numerically that several practical variants such as simultaneous RM$^+$, alternating RM$^+$, and simultaneous predictive RM$^+$, all lack last-iterate convergence guarantees even on a simple $3\\times 3$ game.","We then prove that recent variants of these algorithms based on a smoothing technique do enjoy last-iterate convergence: we prove that extragradient RM$^{+}$ and smooth Predictive RM$^+$ enjoy asymptotic last-iterate convergence (without a rate) and $1/\\sqrt{t}$ best-iterate convergence.","Finally, we introduce restarted variants of these algorithms, and show that they enjoy linear-rate last-iterate convergence."],"url":"http://arxiv.org/abs/2311.00676v1"}
{"created":"2023-11-01 17:21:09","title":"Emotion Detection for Misinformation: A Review","abstract":"With the advent of social media, an increasing number of netizens are sharing and reading posts and news online. However, the huge volumes of misinformation (e.g., fake news and rumors) that flood the internet can adversely affect people's lives, and have resulted in the emergence of rumor and fake news detection as a hot research topic. The emotions and sentiments of netizens, as expressed in social media posts and news, constitute important factors that can help to distinguish fake news from genuine news and to understand the spread of rumors. This article comprehensively reviews emotion-based methods for misinformation detection. We begin by explaining the strong links between emotions and misinformation. We subsequently provide a detailed analysis of a range of misinformation detection methods that employ a variety of emotion, sentiment and stance-based features, and describe their strengths and weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based misinformation detection based on large language models and suggest future research directions, including data collection (multi-platform, multilingual), annotation, benchmark, multimodality, and interpretability.","sentences":["With the advent of social media, an increasing number of netizens are sharing and reading posts and news online.","However, the huge volumes of misinformation (e.g., fake news and rumors) that flood the internet can adversely affect people's lives, and have resulted in the emergence of rumor and fake news detection as a hot research topic.","The emotions and sentiments of netizens, as expressed in social media posts and news, constitute important factors that can help to distinguish fake news from genuine news and to understand the spread of rumors.","This article comprehensively reviews emotion-based methods for misinformation detection.","We begin by explaining the strong links between emotions and misinformation.","We subsequently provide a detailed analysis of a range of misinformation detection methods that employ a variety of emotion, sentiment and stance-based features, and describe their strengths and weaknesses.","Finally, we discuss a number of ongoing challenges in emotion-based misinformation detection based on large language models and suggest future research directions, including data collection (multi-platform, multilingual), annotation, benchmark, multimodality, and interpretability."],"url":"http://arxiv.org/abs/2311.00671v1"}
{"created":"2023-11-01 17:17:14","title":"ProcSim: Proxy-based Confidence for Robust Similarity Learning","abstract":"Deep Metric Learning (DML) methods aim at learning an embedding space in which distances are closely related to the inherent semantic similarity of the inputs. Previous studies have shown that popular benchmark datasets often contain numerous wrong labels, and DML methods are susceptible to them. Intending to study the effect of realistic noise, we create an ontology of the classes in a dataset and use it to simulate semantically coherent labeling mistakes. To train robust DML models, we propose ProcSim, a simple framework that assigns a confidence score to each sample using the normalized distance to its class representative. The experimental results show that the proposed method achieves state-of-the-art performance on the DML benchmark datasets injected with uniform and the proposed semantically coherent noise.","sentences":["Deep Metric Learning (DML) methods aim at learning an embedding space in which distances are closely related to the inherent semantic similarity of the inputs.","Previous studies have shown that popular benchmark datasets often contain numerous wrong labels, and DML methods are susceptible to them.","Intending to study the effect of realistic noise, we create an ontology of the classes in a dataset and use it to simulate semantically coherent labeling mistakes.","To train robust DML models, we propose ProcSim, a simple framework that assigns a confidence score to each sample using the normalized distance to its class representative.","The experimental results show that the proposed method achieves state-of-the-art performance on the DML benchmark datasets injected with uniform and the proposed semantically coherent noise."],"url":"http://arxiv.org/abs/2311.00668v1"}
{"created":"2023-11-01 17:12:00","title":"Latent Space Translation via Semantic Alignment","abstract":"While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Notably, we show how it is possible to zero-shot stitch text encoders and vision decoders, or vice-versa, yielding surprisingly good classification performance in this multimodal setting.","sentences":["While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible.","Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought.","An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions.","Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training.","We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction).","Notably, we show how it is possible to zero-shot stitch text encoders and vision decoders, or vice-versa, yielding surprisingly good classification performance in this multimodal setting."],"url":"http://arxiv.org/abs/2311.00664v1"}
{"created":"2023-11-01 17:08:26","title":"TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain","abstract":"Rain generation algorithms have the potential to improve the generalization of deraining methods and scene understanding in rainy conditions. However, in practice, they produce artifacts and distortions and struggle to control the amount of rain generated due to a lack of proper constraints. In this paper, we propose an unpaired image-to-image translation framework for generating realistic rainy images. We first introduce a Triangular Probability Similarity (TPS) constraint to guide the generated images toward clear and rainy images in the discriminator manifold, thereby minimizing artifacts and distortions during rain generation. Unlike conventional contrastive learning approaches, which indiscriminately push negative samples away from the anchors, we propose a Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing force of negative samples based on the semantic similarity between the clear and the rainy images and the feature similarity between the anchor and the negative samples. Experiments demonstrate realistic rain generation with minimal artifacts and distortions, which benefits image deraining and object detection in rain. Furthermore, the method can be used to generate realistic snowy and night images, underscoring its potential for broader applicability. Code is available at https://github.com/ShenZheng2000/TPSeNCE.","sentences":["Rain generation algorithms have the potential to improve the generalization of deraining methods and scene understanding in rainy conditions.","However, in practice, they produce artifacts and distortions and struggle to control the amount of rain generated due to a lack of proper constraints.","In this paper, we propose an unpaired image-to-image translation framework for generating realistic rainy images.","We first introduce a Triangular Probability Similarity (TPS) constraint to guide the generated images toward clear and rainy images in the discriminator manifold, thereby minimizing artifacts and distortions during rain generation.","Unlike conventional contrastive learning approaches, which indiscriminately push negative samples away from the anchors, we propose a Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing force of negative samples based on the semantic similarity between the clear and the rainy images and the feature similarity between the anchor and the negative samples.","Experiments demonstrate realistic rain generation with minimal artifacts and distortions, which benefits image deraining and object detection in rain.","Furthermore, the method can be used to generate realistic snowy and night images, underscoring its potential for broader applicability.","Code is available at https://github.com/ShenZheng2000/TPSeNCE."],"url":"http://arxiv.org/abs/2311.00660v1"}
{"created":"2023-11-01 17:02:49","title":"Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew","abstract":"Pre-trained language models (PLMs) have shown remarkable successes in acquiring a wide range of linguistic knowledge, relying solely on self-supervised training on text streams. Nevertheless, the effectiveness of this language-agnostic approach has been frequently questioned for its sub-optimal performance when applied to morphologically-rich languages (MRLs). We investigate the hypothesis that incorporating explicit morphological knowledge in the pre-training phase can improve the performance of PLMs for MRLs. We propose various morphologically driven tokenization methods enabling the model to leverage morphological cues beyond raw text. We pre-train multiple language models utilizing the different methods and evaluate them on Hebrew, a language with complex and highly ambiguous morphology. Our experiments show that morphologically driven tokenization demonstrates improved results compared to a standard language-agnostic tokenization, on a benchmark of both semantic and morphologic tasks. These findings suggest that incorporating morphological knowledge holds the potential for further improving PLMs for morphologically rich languages.","sentences":["Pre-trained language models (PLMs) have shown remarkable successes in acquiring a wide range of linguistic knowledge, relying solely on self-supervised training on text streams.","Nevertheless, the effectiveness of this language-agnostic approach has been frequently questioned for its sub-optimal performance when applied to morphologically-rich languages (MRLs).","We investigate the hypothesis that incorporating explicit morphological knowledge in the pre-training phase can improve the performance of PLMs for MRLs.","We propose various morphologically driven tokenization methods enabling the model to leverage morphological cues beyond raw text.","We pre-train multiple language models utilizing the different methods and evaluate them on Hebrew, a language with complex and highly ambiguous morphology.","Our experiments show that morphologically driven tokenization demonstrates improved results compared to a standard language-agnostic tokenization, on a benchmark of both semantic and morphologic tasks.","These findings suggest that incorporating morphological knowledge holds the potential for further improving PLMs for morphologically rich languages."],"url":"http://arxiv.org/abs/2311.00658v1"}
{"created":"2023-11-01 16:58:26","title":"Integrating measures of replicability into literature search: Challenges and opportunities","abstract":"Challenges to reproducibility and replicability have gained widespread attention over the past decade, driven by a number of large replication projects with lukewarm success rates. A nascent work has emerged developing algorithms to estimate, or predict, the replicability of published findings. The current study explores ways in which AI-enabled signals of confidence in research might be integrated into literature search. We interview 17 PhD researchers about their current processes for literature search and ask them to provide feedback on a prototype replicability estimation tool. Our findings suggest that information about replicability can support researchers throughout literature review and research design processes. However, explainability and interpretability of system outputs is critical, and potential drawbacks of AI-enabled confidence assessment need to be further studied before such tools could be widely accepted and deployed. We discuss implications for the design of technological tools to support scholarly activities and advance reproducibility and replicability.","sentences":["Challenges to reproducibility and replicability have gained widespread attention over the past decade, driven by a number of large replication projects with lukewarm success rates.","A nascent work has emerged developing algorithms to estimate, or predict, the replicability of published findings.","The current study explores ways in which AI-enabled signals of confidence in research might be integrated into literature search.","We interview 17 PhD researchers about their current processes for literature search and ask them to provide feedback on a prototype replicability estimation tool.","Our findings suggest that information about replicability can support researchers throughout literature review and research design processes.","However, explainability and interpretability of system outputs is critical, and potential drawbacks of AI-enabled confidence assessment need to be further studied before such tools could be widely accepted and deployed.","We discuss implications for the design of technological tools to support scholarly activities and advance reproducibility and replicability."],"url":"http://arxiv.org/abs/2311.00653v1"}
{"created":"2023-11-01 16:56:44","title":"Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning","abstract":"Recent works have proven that intricate cooperative behaviors can emerge in agents trained using meta reinforcement learning on open ended task distributions using self-play. While the results are impressive, we argue that self-play and other centralized training techniques do not accurately reflect how general collective exploration strategies emerge in the natural world: through decentralized training and over an open-ended distribution of tasks. In this work we therefore investigate the emergence of collective exploration strategies, where several agents meta-learn independent recurrent policies on an open ended distribution of tasks. To this end we introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees. We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel objects at test time. Additionally, despite never being forced to cooperate during training the agents learn collective exploration strategies which allow them to solve novel tasks never encountered during training. We further find that the agents learned collective exploration strategies extend to an open ended task setting, allowing them to solve task trees of twice the depth compared to the ones seen during training. Our open source code as well as videos of the agents can be found on our companion website.","sentences":["Recent works have proven that intricate cooperative behaviors can emerge in agents trained using meta reinforcement learning on open ended task distributions using self-play.","While the results are impressive, we argue that self-play and other centralized training techniques do not accurately reflect how general collective exploration strategies emerge in the natural world: through decentralized training and over an open-ended distribution of tasks.","In this work we therefore investigate the emergence of collective exploration strategies, where several agents meta-learn independent recurrent policies on an open ended distribution of tasks.","To this end we introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees.","We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel objects at test time.","Additionally, despite never being forced to cooperate during training the agents learn collective exploration strategies which allow them to solve novel tasks never encountered during training.","We further find that the agents learned collective exploration strategies extend to an open ended task setting, allowing them to solve task trees of twice the depth compared to the ones seen during training.","Our open source code as well as videos of the agents can be found on our companion website."],"url":"http://arxiv.org/abs/2311.00651v1"}
{"created":"2023-11-01 16:46:53","title":"Understanding the Issues and Causes in WebAssembly Application Development: A Mining-based Study","abstract":"WebAssembly (Wasm) is a binary instruction format designed for secure and efficient execution within sandboxed environments - predominantly web apps and browsers - to facilitate performance, security, and flexibility of web programming languages. In recent years, Wasm has gained significant attention from academic research community and industrial development projects to engineer high-performance web applications. Despite the offered benefits, developers encounter a multitude of issues rooted in Wasm (e.g., faults, errors, failures) and are often unaware of their root-causes that impact the development of web applications. Wasm developers require knowledge, documented as empirically rooted guidelines, patterns, documents etc., that help them to understand, analyse, and resolve the issues that currently lacks in existing research and practice. To this end, we conducted an empirical study that mines and documents practitioners' knowledge expressed as 385 issues from 12 open-source Wasm projects deployed on GitHub and 354 question-answer posts via Stack Overflow. Our study led to the first-of-its-kind taxonomies of issues faced by developers and their underlying causes in Wasm-based applications. Issues faced by developers arise from 'Infrastructure, Integration and Compatibility Aspects' (28.16%), 'Language Features and Documentation Errors' (18.00%), along with 'Code Implementation and Build failures' (13.83%). The results indicate that 'Syntactic and Semantic Errors' (25.77%), 'Configuration and Compatibility Constraints' (20.1%), and 'Operational Limitations' (12.98%) are the principal causes of these issues. The study provides a taxonomical classification of issues and their causes, offering empirically derived guidelines, that can inform researchers and developers to systematically design, develop, and refactor Wasm-based applications.","sentences":["WebAssembly (Wasm) is a binary instruction format designed for secure and efficient execution within sandboxed environments - predominantly web apps and browsers - to facilitate performance, security, and flexibility of web programming languages.","In recent years, Wasm has gained significant attention from academic research community and industrial development projects to engineer high-performance web applications.","Despite the offered benefits, developers encounter a multitude of issues rooted in Wasm (e.g., faults, errors, failures) and are often unaware of their root-causes that impact the development of web applications.","Wasm developers require knowledge, documented as empirically rooted guidelines, patterns, documents etc., that help them to understand, analyse, and resolve the issues that currently lacks in existing research and practice.","To this end, we conducted an empirical study that mines and documents practitioners' knowledge expressed as 385 issues from 12 open-source Wasm projects deployed on GitHub and 354 question-answer posts via Stack Overflow.","Our study led to the first-of-its-kind taxonomies of issues faced by developers and their underlying causes in Wasm-based applications.","Issues faced by developers arise from 'Infrastructure, Integration and Compatibility Aspects' (28.16%), 'Language Features and Documentation Errors' (18.00%), along with 'Code Implementation and Build failures' (13.83%).","The results indicate that 'Syntactic and Semantic Errors' (25.77%), 'Configuration and Compatibility Constraints' (20.1%), and 'Operational Limitations' (12.98%) are the principal causes of these issues.","The study provides a taxonomical classification of issues and their causes, offering empirically derived guidelines, that can inform researchers and developers to systematically design, develop, and refactor Wasm-based applications."],"url":"http://arxiv.org/abs/2311.00646v1"}
{"created":"2023-11-01 16:42:10","title":"Near-Optimal $k$-Clustering in the Sliding Window Model","abstract":"Clustering is an important technique for identifying structural information in large-scale data analysis, where the underlying dataset may be too large to store. In many applications, recent data can provide more accurate information and thus older data past a certain time is expired. The sliding window model captures these desired properties and thus there has been substantial interest in clustering in the sliding window model.   In this paper, we give the first algorithm that achieves near-optimal $(1+\\varepsilon)$-approximation to $(k,z)$-clustering in the sliding window model, where $z$ is the exponent of the distance function in the cost. Our algorithm uses $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ words of space when the points are from $[\\Delta]^d$, thus significantly improving on works by Braverman et. al. (SODA 2016), Borassi et. al. (NeurIPS 2021), and Epasto et. al. (SODA 2022).   Along the way, we develop a data structure for clustering called an online coreset, which outputs a coreset not only for the end of a stream, but also for all prefixes of the stream. Our online coreset samples $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ points from the stream. We then show that any online coreset requires $\\Omega\\left(\\frac{k}{\\varepsilon^2}\\log n\\right)$ samples, which shows a separation from the problem of constructing an offline coreset, i.e., constructing online coresets is strictly harder. Our results also extend to general metrics on $[\\Delta]^d$ and are near-optimal in light of a $\\Omega\\left(\\frac{k}{\\varepsilon^{2+z}}\\right)$ lower bound for the size of an offline coreset.","sentences":["Clustering is an important technique for identifying structural information in large-scale data analysis, where the underlying dataset may be too large to store.","In many applications, recent data can provide more accurate information and thus older data past a certain time is expired.","The sliding window model captures these desired properties and thus there has been substantial interest in clustering in the sliding window model.   ","In this paper, we give the first algorithm that achieves near-optimal $(1+\\varepsilon)$-approximation to $(k,z)$-clustering in the sliding window model, where $z$ is the exponent of the distance function in the cost.","Our algorithm uses $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ words of space when the points are from $[\\Delta]^d$, thus significantly improving on works by Braverman et. al. (SODA 2016), Borassi et.","al. (NeurIPS 2021), and Epasto et.","al. (SODA 2022).   ","Along the way, we develop a data structure for clustering called an online coreset, which outputs a coreset not only for the end of a stream, but also for all prefixes of the stream.","Our online coreset samples $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ points from the stream.","We then show that any online coreset requires $\\Omega\\left(\\frac{k}{\\varepsilon^2}\\log n\\right)$ samples, which shows a separation from the problem of constructing an offline coreset, i.e., constructing online coresets is strictly harder.","Our results also extend to general metrics on $[\\Delta]^d$ and are near-optimal in light of a $\\Omega\\left(\\frac{k}{\\varepsilon^{2+z}}\\right)$ lower bound for the size of an offline coreset."],"url":"http://arxiv.org/abs/2311.00642v1"}
{"created":"2023-11-01 16:38:27","title":"FAIRLABEL: Correcting Bias in Labels","abstract":"There are several algorithms for measuring fairness of ML models. A fundamental assumption in these approaches is that the ground truth is fair or unbiased. In real-world datasets, however, the ground truth often contains data that is a result of historical and societal biases and discrimination. Models trained on these datasets will inherit and propagate the biases to the model outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across groups while maintaining high accuracy in predictions. We propose metrics to measure the quality of bias correction and validate FAIRLABEL on synthetic datasets and show that the label correction is correct 86.7% of the time vs. 71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such as UCI Adult, German Credit Risk, and Compas datasets and show that the Disparate Impact Ratio increases by as much as 54.2%.","sentences":["There are several algorithms for measuring fairness of ML models.","A fundamental assumption in these approaches is that the ground truth is fair or unbiased.","In real-world datasets, however, the ground truth often contains data that is a result of historical and societal biases and discrimination.","Models trained on these datasets will inherit and propagate the biases to the model outputs.","We propose FAIRLABEL, an algorithm which detects and corrects biases in labels.","The goal of FAIRLABELis to reduce the Disparate Impact (DI) across groups while maintaining high accuracy in predictions.","We propose metrics to measure the quality of bias correction and validate FAIRLABEL on synthetic datasets and show that the label correction is correct 86.7% of the time vs. 71.9% for a baseline model.","We also apply FAIRLABEL on benchmark datasets such as UCI Adult, German Credit Risk, and Compas datasets and show that the Disparate Impact Ratio increases by as much as 54.2%."],"url":"http://arxiv.org/abs/2311.00638v1"}
{"created":"2023-11-01 16:37:00","title":"Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures","abstract":"The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- $\\textit{expand}$ and $\\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer. However, both variations are able to reach a fixed validation metric target in $50$-$75\\%$ of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. This highlights the potential of applying K-FAC to modern neural network architectures.","sentences":["The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs.","However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers.","In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- $\\textit{expand}$ and $\\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting.","Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet.","Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer.","However, both variations are able to reach a fixed validation metric target in $50$-$75\\%$ of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time.","This highlights the potential of applying K-FAC to modern neural network architectures."],"url":"http://arxiv.org/abs/2311.00636v1"}
{"created":"2023-11-01 16:36:19","title":"GATSY: Graph Attention Network for Music Artist Similarity","abstract":"The artist similarity quest has become a crucial subject in social and scientific contexts. Modern research solutions facilitate music discovery according to user tastes. However, defining similarity among artists may involve several aspects, even related to a subjective perspective, and it often affects a recommendation. This paper presents GATSY, a recommendation system built upon graph attention networks and driven by a clusterized embedding of artists. The proposed framework takes advantage of a graph topology of the input data to achieve outstanding performance results without relying heavily on hand-crafted features. This flexibility allows us to introduce fictitious artists in a music dataset, create bridges to previously unrelated artists, and get recommendations conditioned by possibly heterogeneous sources. Experimental results prove the effectiveness of the proposed method with respect to state-of-the-art solutions.","sentences":["The artist similarity quest has become a crucial subject in social and scientific contexts.","Modern research solutions facilitate music discovery according to user tastes.","However, defining similarity among artists may involve several aspects, even related to a subjective perspective, and it often affects a recommendation.","This paper presents GATSY, a recommendation system built upon graph attention networks and driven by a clusterized embedding of artists.","The proposed framework takes advantage of a graph topology of the input data to achieve outstanding performance results without relying heavily on hand-crafted features.","This flexibility allows us to introduce fictitious artists in a music dataset, create bridges to previously unrelated artists, and get recommendations conditioned by possibly heterogeneous sources.","Experimental results prove the effectiveness of the proposed method with respect to state-of-the-art solutions."],"url":"http://arxiv.org/abs/2311.00635v1"}
{"created":"2023-11-01 16:33:37","title":"A Bi-level Framework for Traffic Accident Duration Prediction: Leveraging Weather and Road Condition Data within a Practical Optimum Pipeline","abstract":"Due to the stochastic nature of events, predicting the duration of a traffic incident presents a formidable challenge. Accurate duration estimation can result in substantial advantages for commuters in selecting optimal routes and for traffic management personnel in addressing non-recurring congestion issues. In this study, we gathered accident duration, road conditions, and meteorological data from a database of traffic accidents to check the feasibility of a traffic accident duration pipeline without accident contextual information data like accident severity and textual description. Multiple machine learning models were employed to predict whether an accident's impact on road traffic would be of a short-term or long-term nature, and then utilizing a bimodal approach the precise duration of the incident's effect was determined. Our binary classification random forest model distinguished between short-term and long-term effects with an 83% accuracy rate, while the LightGBM regression model outperformed other machine learning regression models with Mean Average Error (MAE) values of 26.15 and 13.3 and RMSE values of 32.91 and 28.91 for short and long-term accident duration prediction, respectively. Using the optimal classification and regression model identified in the preceding section, we then construct an end-to-end pipeline to incorporate the entire process. The results of both separate and combined approaches were comparable with previous works, which shows the applicability of only using static features for predicting traffic accident duration. The SHAP value analysis identified weather conditions, wind chill and wind speed as the most influential factors in determining the duration of an accident.","sentences":["Due to the stochastic nature of events, predicting the duration of a traffic incident presents a formidable challenge.","Accurate duration estimation can result in substantial advantages for commuters in selecting optimal routes and for traffic management personnel in addressing non-recurring congestion issues.","In this study, we gathered accident duration, road conditions, and meteorological data from a database of traffic accidents to check the feasibility of a traffic accident duration pipeline without accident contextual information data like accident severity and textual description.","Multiple machine learning models were employed to predict whether an accident's impact on road traffic would be of a short-term or long-term nature, and then utilizing a bimodal approach the precise duration of the incident's effect was determined.","Our binary classification random forest model distinguished between short-term and long-term effects with an 83% accuracy rate, while the LightGBM regression model outperformed other machine learning regression models with Mean Average Error (MAE) values of 26.15 and 13.3 and RMSE values of 32.91 and 28.91 for short and long-term accident duration prediction, respectively.","Using the optimal classification and regression model identified in the preceding section, we then construct an end-to-end pipeline to incorporate the entire process.","The results of both separate and combined approaches were comparable with previous works, which shows the applicability of only using static features for predicting traffic accident duration.","The SHAP value analysis identified weather conditions, wind chill and wind speed as the most influential factors in determining the duration of an accident."],"url":"http://arxiv.org/abs/2311.00634v1"}
{"created":"2023-11-01 16:28:38","title":"Formal Translation from Reversing Petri Nets to Coloured Petri Nets","abstract":"Reversible computation is an emerging computing paradigm that allows any sequence of operations to be executed in reverse order at any point during computation. Its appeal lies in its potential for lowpower computation and its relevance to a wide array of applications such as chemical reactions, quantum computation, robotics, and distributed systems. Reversing Petri nets are a recently-proposed extension of Petri nets that implements the three main forms of reversibility, namely, backtracking, causal reversing, and out-of-causal-order reversing. Their distinguishing feature is the use of named tokens that can be combined together to form bonds. Named tokens along with a history function, constitute the means of remembering past behaviour, thus, enabling reversal. In recent work, we have proposed a structural translation from a subclass of RPNs to the model of Coloured Petri Nets (CPNs), an extension of traditional Petri nets where tokens carry data values. In this paper, we extend the translation to handle RPNs with token multiplicity under the individual-token interpretation, a model which allows multiple tokens of the same type to exist in a system. To support the three types of reversibility, tokens are associated with their causal history and, while tokens of the same type are equally eligible to fire a transition when going forward, when going backwards they are able to reverse only the transitions they have previously fired. The new translation, in addition to lifting the restriction on token uniqueness, presents a refined approach for transforming RPNs to CPNs through a unifying approach that allows instantiating each of the three types of reversibility. The paper also reports on a tool that implements this translation, paving the way for automated translations and analysis of reversible systems using CPN Tools.","sentences":["Reversible computation is an emerging computing paradigm that allows any sequence of operations to be executed in reverse order at any point during computation.","Its appeal lies in its potential for lowpower computation and its relevance to a wide array of applications such as chemical reactions, quantum computation, robotics, and distributed systems.","Reversing Petri nets are a recently-proposed extension of Petri nets that implements the three main forms of reversibility, namely, backtracking, causal reversing, and out-of-causal-order reversing.","Their distinguishing feature is the use of named tokens that can be combined together to form bonds.","Named tokens along with a history function, constitute the means of remembering past behaviour, thus, enabling reversal.","In recent work, we have proposed a structural translation from a subclass of RPNs to the model of Coloured Petri Nets (CPNs), an extension of traditional Petri nets where tokens carry data values.","In this paper, we extend the translation to handle RPNs with token multiplicity under the individual-token interpretation, a model which allows multiple tokens of the same type to exist in a system.","To support the three types of reversibility, tokens are associated with their causal history and, while tokens of the same type are equally eligible to fire a transition when going forward, when going backwards they are able to reverse only the transitions they have previously fired.","The new translation, in addition to lifting the restriction on token uniqueness, presents a refined approach for transforming RPNs to CPNs through a unifying approach that allows instantiating each of the three types of reversibility.","The paper also reports on a tool that implements this translation, paving the way for automated translations and analysis of reversible systems using CPN Tools."],"url":"http://arxiv.org/abs/2311.00629v1"}
{"created":"2023-11-01 16:23:59","title":"nvblox: GPU-Accelerated Incremental Signed Distance Field Mapping","abstract":"Dense, volumetric maps are essential for safe robot navigation through cluttered spaces, as well as interaction with the environment. For latency and robustness, it is best if these can be computed on-board on computationally-constrained hardware from camera or LiDAR-based sensors. Previous works leave a gap between CPU-based systems for robotic mapping, which due to computation constraints limit map resolution or scale, and GPU-based reconstruction systems which omit features that are critical to robotic path planning. We introduce a library, nvblox, that aims to fill this gap, by GPU-accelerating robotic volumetric mapping, and which is optimized for embedded GPUs. nvblox delivers a significant performance improvement over the state of the art, achieving up to a 177x speed-up in surface reconstruction, and up to a 31x improvement in distance field computation, and is available open-source.","sentences":["Dense, volumetric maps are essential for safe robot navigation through cluttered spaces, as well as interaction with the environment.","For latency and robustness, it is best if these can be computed on-board on computationally-constrained hardware from camera or LiDAR-based sensors.","Previous works leave a gap between CPU-based systems for robotic mapping, which due to computation constraints limit map resolution or scale, and GPU-based reconstruction systems which omit features that are critical to robotic path planning.","We introduce a library, nvblox, that aims to fill this gap, by GPU-accelerating robotic volumetric mapping, and which is optimized for embedded GPUs.","nvblox delivers a significant performance improvement over the state of the art, achieving up to a 177x speed-up in surface reconstruction, and up to a 31x improvement in distance field computation, and is available open-source."],"url":"http://arxiv.org/abs/2311.00626v1"}
{"created":"2023-11-01 16:14:34","title":"Loss Modeling for Multi-Annotator Datasets","abstract":"Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.","sentences":["Accounting for the opinions of all annotators of a dataset is critical for fairness.","However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue.","Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time.","To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction.","We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations.","Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting.","Lastly, we show that this method remains robust to additional label noise that is applied to subjective data."],"url":"http://arxiv.org/abs/2311.00619v1"}
{"created":"2023-11-01 16:12:40","title":"De-Diffusion Makes Text a Strong Cross-Modal Interface","abstract":"We demonstrate text as a strong cross-modal interface. Rather than relying on deep embeddings to connect image and language as the interface representation, our approach represents an image as text, from which we enjoy the interpretability and flexibility inherent to natural language. We employ an autoencoder that uses a pre-trained text-to-image diffusion model for decoding. The encoder is trained to transform an input image into text, which is then fed into the fixed text-to-image diffusion decoder to reconstruct the original input -- a process we term De-Diffusion. Experiments validate both the precision and comprehensiveness of De-Diffusion text representing images, such that it can be readily ingested by off-the-shelf text-to-image tools and LLMs for diverse multi-modal tasks. For example, a single De-Diffusion model can generalize to provide transferable prompts for different text-to-image tools, and also achieves a new state of the art on open-ended vision-language tasks by simply prompting large language models with few-shot examples.","sentences":["We demonstrate text as a strong cross-modal interface.","Rather than relying on deep embeddings to connect image and language as the interface representation, our approach represents an image as text, from which we enjoy the interpretability and flexibility inherent to natural language.","We employ an autoencoder that uses a pre-trained text-to-image diffusion model for decoding.","The encoder is trained to transform an input image into text, which is then fed into the fixed text-to-image diffusion decoder to reconstruct the original input -- a process we term De-Diffusion.","Experiments validate both the precision and comprehensiveness of De-Diffusion text representing images, such that it can be readily ingested by off-the-shelf text-to-image tools and LLMs for diverse multi-modal tasks.","For example, a single De-Diffusion model can generalize to provide transferable prompts for different text-to-image tools, and also achieves a new state of the art on open-ended vision-language tasks by simply prompting large language models with few-shot examples."],"url":"http://arxiv.org/abs/2311.00618v1"}
{"created":"2023-11-01 16:01:01","title":"Controllable Music Production with Diffusion Models and Guidance Gradients","abstract":"We demonstrate how conditional generation from diffusion models can be used to tackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio with sampling-time guidance. The scenarios we consider include continuation, inpainting and regeneration of musical audio, the creation of smooth transitions between two different music tracks, and the transfer of desired stylistic characteristics to existing audio clips. We achieve this by applying guidance at sampling time in a simple framework that supports both reconstruction and classification losses, or any combination of the two. This approach ensures that generated audio can match its surrounding context, or conform to a class distribution or latent representation specified relative to any suitable pre-trained classifier or embedding model.","sentences":["We demonstrate how conditional generation from diffusion models can be used to tackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio with sampling-time guidance.","The scenarios we consider include continuation, inpainting and regeneration of musical audio, the creation of smooth transitions between two different music tracks, and the transfer of desired stylistic characteristics to existing audio clips.","We achieve this by applying guidance at sampling time in a simple framework that supports both reconstruction and classification losses, or any combination of the two.","This approach ensures that generated audio can match its surrounding context, or conform to a class distribution or latent representation specified relative to any suitable pre-trained classifier or embedding model."],"url":"http://arxiv.org/abs/2311.00613v1"}
{"created":"2023-11-01 16:01:00","title":"A Collaborative Filtering-Based Two Stage Model with Item Dependency for Course Recommendation","abstract":"Recommender systems have been studied for decades with numerous promising models been proposed. Among them, Collaborative Filtering (CF) models are arguably the most successful one due to its high accuracy in recommendation and elimination of privacy-concerned personal meta-data from training. This paper extends the usage of CF-based model to the task of course recommendation. We point out several challenges in applying the existing CF-models to build a course recommendation engine, including the lack of rating and meta-data, the imbalance of course registration distribution, and the demand of course dependency modeling. We then propose several ideas to address these challenges. Eventually, we combine a two-stage CF model regularized by course dependency with a graph-based recommender based on course-transition network, to achieve AUC as high as 0.97 with a real-world dataset.","sentences":["Recommender systems have been studied for decades with numerous promising models been proposed.","Among them, Collaborative Filtering (CF) models are arguably the most successful one due to its high accuracy in recommendation and elimination of privacy-concerned personal meta-data from training.","This paper extends the usage of CF-based model to the task of course recommendation.","We point out several challenges in applying the existing CF-models to build a course recommendation engine, including the lack of rating and meta-data, the imbalance of course registration distribution, and the demand of course dependency modeling.","We then propose several ideas to address these challenges.","Eventually, we combine a two-stage CF model regularized by course dependency with a graph-based recommender based on course-transition network, to achieve AUC as high as 0.97 with a real-world dataset."],"url":"http://arxiv.org/abs/2311.00612v1"}
{"created":"2023-11-01 15:53:17","title":"A Systematic Review of Approximability Results for Traveling Salesman Problems leveraging the TSP-T3CO Definition Scheme","abstract":"The traveling salesman (or salesperson) problem, short TSP, is a problem of strong interest to many researchers from mathematics, economics, and computer science. Manifold TSP variants occur in nearly every scientific field and application domain: engineering, physics, biology, life sciences, and manufacturing just to name a few. Several thousand papers are published on theoretical research or application-oriented results each year. This paper provides the first systematic survey on the best currently known approximability and inapproximability results for well-known TSP variants such as the \"standard\" TSP, Path TSP, Bottleneck TSP, Maximum Scatter TSP, Generalized TSP, Clustered TSP, Traveling Purchaser Problem, Profitable Tour Problem, Quota TSP, Prize-Collecting TSP, Orienteering Problem, Time-dependent TSP, TSP with Time Windows, and the Orienteering Problem with Time Windows. The foundation of our survey is the definition scheme T3CO, which we propose as a uniform, easy-to-use and extensible means for the formal and precise definition of TSP variants. Applying T3CO to formally define the variant studied by a paper reveals subtle differences within the same named variant and also brings out the differences between the variants more clearly. We achieve the first comprehensive, concise, and compact representation of approximability results by using T3CO definitions. This makes it easier to understand the approximability landscape and the assumptions under which certain results hold. Open gaps become more evident and results can be compared more easily.","sentences":["The traveling salesman (or salesperson) problem, short TSP, is a problem of strong interest to many researchers from mathematics, economics, and computer science.","Manifold TSP variants occur in nearly every scientific field and application domain: engineering, physics, biology, life sciences, and manufacturing just to name a few.","Several thousand papers are published on theoretical research or application-oriented results each year.","This paper provides the first systematic survey on the best currently known approximability and inapproximability results for well-known TSP variants such as the \"standard\" TSP, Path TSP, Bottleneck TSP, Maximum Scatter TSP, Generalized TSP, Clustered TSP, Traveling Purchaser Problem, Profitable Tour Problem, Quota TSP, Prize-Collecting TSP, Orienteering Problem, Time-dependent TSP, TSP with Time Windows, and the Orienteering Problem with Time Windows.","The foundation of our survey is the definition scheme T3CO, which we propose as a uniform, easy-to-use and extensible means for the formal and precise definition of TSP variants.","Applying T3CO to formally define the variant studied by a paper reveals subtle differences within the same named variant and also brings out the differences between the variants more clearly.","We achieve the first comprehensive, concise, and compact representation of approximability results by using T3CO definitions.","This makes it easier to understand the approximability landscape and the assumptions under which certain results hold.","Open gaps become more evident and results can be compared more easily."],"url":"http://arxiv.org/abs/2311.00604v1"}
{"created":"2023-11-01 15:52:51","title":"Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives","abstract":"Person re-identification (Re-ID) technology plays an increasingly crucial role in intelligent surveillance systems. Widespread occlusion significantly impacts the performance of person Re-ID. Occluded person Re-ID refers to a pedestrian matching method that deals with challenges such as pedestrian information loss, noise interference, and perspective misalignment. It has garnered extensive attention from researchers. Over the past few years, several occlusion-solving person Re-ID methods have been proposed, tackling various sub-problems arising from occlusion. However, there is a lack of comprehensive studies that compare, summarize, and evaluate the potential of occluded person Re-ID methods in detail. In this review, we start by providing a detailed overview of the datasets and evaluation scheme used for occluded person Re-ID. Next, we scientifically classify and analyze existing deep learning-based occluded person Re-ID methods from various perspectives, summarizing them concisely. Furthermore, we conduct a systematic comparison among these methods, identify the state-of-the-art approaches, and present an outlook on the future development of occluded person Re-ID.","sentences":["Person re-identification (Re-ID) technology plays an increasingly crucial role in intelligent surveillance systems.","Widespread occlusion significantly impacts the performance of person Re-ID.","Occluded person Re-ID refers to a pedestrian matching method that deals with challenges such as pedestrian information loss, noise interference, and perspective misalignment.","It has garnered extensive attention from researchers.","Over the past few years, several occlusion-solving person Re-ID methods have been proposed, tackling various sub-problems arising from occlusion.","However, there is a lack of comprehensive studies that compare, summarize, and evaluate the potential of occluded person Re-ID methods in detail.","In this review, we start by providing a detailed overview of the datasets and evaluation scheme used for occluded person Re-ID.","Next, we scientifically classify and analyze existing deep learning-based occluded person Re-ID methods from various perspectives, summarizing them concisely.","Furthermore, we conduct a systematic comparison among these methods, identify the state-of-the-art approaches, and present an outlook on the future development of occluded person Re-ID."],"url":"http://arxiv.org/abs/2311.00603v1"}
{"created":"2023-11-01 15:47:18","title":"Structure Learning with Adaptive Random Neighborhood Informed MCMC","abstract":"In this paper, we introduce a novel MCMC sampler, PARNI-DAG, for a fully-Bayesian approach to the problem of structure learning under observational data. Under the assumption of causal sufficiency, the algorithm allows for approximate sampling directly from the posterior distribution on Directed Acyclic Graphs (DAGs). PARNI-DAG performs efficient sampling of DAGs via locally informed, adaptive random neighborhood proposal that results in better mixing properties. In addition, to ensure better scalability with the number of nodes, we couple PARNI-DAG with a pre-tuning procedure of the sampler's parameters that exploits a skeleton graph derived through some constraint-based or scoring-based algorithms. Thanks to these novel features, PARNI-DAG quickly converges to high-probability regions and is less likely to get stuck in local modes in the presence of high correlation between nodes in high-dimensional settings. After introducing the technical novelties in PARNI-DAG, we empirically demonstrate its mixing efficiency and accuracy in learning DAG structures on a variety of experiments.","sentences":["In this paper, we introduce a novel MCMC sampler, PARNI-DAG, for a fully-Bayesian approach to the problem of structure learning under observational data.","Under the assumption of causal sufficiency, the algorithm allows for approximate sampling directly from the posterior distribution on Directed Acyclic Graphs (DAGs).","PARNI-DAG performs efficient sampling of DAGs via locally informed, adaptive random neighborhood proposal that results in better mixing properties.","In addition, to ensure better scalability with the number of nodes, we couple PARNI-DAG with a pre-tuning procedure of the sampler's parameters that exploits a skeleton graph derived through some constraint-based or scoring-based algorithms.","Thanks to these novel features, PARNI-DAG quickly converges to high-probability regions and is less likely to get stuck in local modes in the presence of high correlation between nodes in high-dimensional settings.","After introducing the technical novelties in PARNI-DAG, we empirically demonstrate its mixing efficiency and accuracy in learning DAG structures on a variety of experiments."],"url":"http://arxiv.org/abs/2311.00599v1"}
{"created":"2023-11-01 15:35:51","title":"Coop: Memory is not a Commodity","abstract":"Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical. Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads to severe memory fragmentation and increases the cost of potential rematerializations. To address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation. We named our method Coop as it is a co-optimization of tensor allocation and tensor rematerialization. We evaluated Coop on eight representative DNNs. The experimental results demonstrate that Coop achieves up to $2\\times$ memory saving and hugely reduces compute overhead, search latency, and memory fragmentation compared to the state-of-the-art baselines.","sentences":["Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed.","However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical.","Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor.","This leads to severe memory fragmentation and increases the cost of potential rematerializations.","To address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used.","Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation.","We named our method Coop as it is a co-optimization of tensor allocation and tensor rematerialization.","We evaluated Coop on eight representative DNNs.","The experimental results demonstrate that Coop achieves up to $2\\times$ memory saving and hugely reduces compute overhead, search latency, and memory fragmentation compared to the state-of-the-art baselines."],"url":"http://arxiv.org/abs/2311.00591v1"}
{"created":"2023-11-01 15:33:38","title":"Boosting Summarization with Normalizing Flows and Aggressive Training","abstract":"This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research.","sentences":["This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization.","Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training.","To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism.","Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time.","Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research."],"url":"http://arxiv.org/abs/2311.00588v1"}
{"created":"2023-11-01 15:32:50","title":"Crosslingual Retrieval Augmented In-context Learning for Bangla","abstract":"The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla. To address this, our paper presents a pioneering approach that utilizes cross-lingual retrieval augmented in-context learning. By strategically sourcing semantically similar prompts from high-resource language, we enable multilingual pretrained language models (MPLMs), especially the generative model BLOOMZ, to successfully boost performance on Bangla tasks. Our extensive evaluation highlights that the cross-lingual retrieval augmented prompts bring steady improvements to MPLMs over the zero-shot performance.","sentences":["The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla.","To address this, our paper presents a pioneering approach that utilizes cross-lingual retrieval augmented in-context learning.","By strategically sourcing semantically similar prompts from high-resource language, we enable multilingual pretrained language models (MPLMs), especially the generative model BLOOMZ, to successfully boost performance on Bangla tasks.","Our extensive evaluation highlights that the cross-lingual retrieval augmented prompts bring steady improvements to MPLMs over the zero-shot performance."],"url":"http://arxiv.org/abs/2311.00587v1"}
{"created":"2023-11-01 15:32:11","title":"PAUMER: Patch Pausing Transformer for Semantic Segmentation","abstract":"We study the problem of improving the efficiency of segmentation transformers by using disparate amounts of computation for different parts of the image. Our method, PAUMER, accomplishes this by pausing computation for patches that are deemed to not need any more computation before the final decoder. We use the entropy of predictions computed from intermediate activations as the pausing criterion, and find this aligns well with semantics of the image. Our method has a unique advantage that a single network trained with the proposed strategy can be effortlessly adapted at inference to various run-time requirements by modulating its pausing parameters. On two standard segmentation datasets, Cityscapes and ADE20K, we show that our method operates with about a $50\\%$ higher throughput with an mIoU drop of about $0.65\\%$ and $4.6\\%$ respectively.","sentences":["We study the problem of improving the efficiency of segmentation transformers by using disparate amounts of computation for different parts of the image.","Our method, PAUMER, accomplishes this by pausing computation for patches that are deemed to not need any more computation before the final decoder.","We use the entropy of predictions computed from intermediate activations as the pausing criterion, and find this aligns well with semantics of the image.","Our method has a unique advantage that a single network trained with the proposed strategy can be effortlessly adapted at inference to various run-time requirements by modulating its pausing parameters.","On two standard segmentation datasets, Cityscapes and ADE20K, we show that our method operates with about a $50\\%$ higher throughput with an mIoU drop of about $0.65\\%$ and $4.6\\%$ respectively."],"url":"http://arxiv.org/abs/2311.00586v1"}
{"created":"2023-11-01 15:27:29","title":"Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value","abstract":"We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.","sentences":["We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost.","We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation.","We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost."],"url":"http://arxiv.org/abs/2311.00582v1"}
{"created":"2023-11-01 15:23:04","title":"Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators","abstract":"Convolution Neural Networks (CNNs) are widely used in various domains. Recent advances in dataflow-based CNN accelerators have enabled CNN inference in resource-constrained edge devices. These dataflow accelerators utilize inherent data reuse of convolution layers to process CNN models efficiently. Concealing the architecture of CNN models is critical for privacy and security. This paper evaluates memory-based side-channel information to recover CNN architectures from dataflow-based CNN inference accelerators. The proposed attack exploits spatial and temporal data reuse of the dataflow mapping on CNN accelerators and architectural hints to recover the structure of CNN models. Experimental results demonstrate that our proposed side-channel attack can recover the structures of popular CNN models, namely Lenet, Alexnet, and VGGnet16.","sentences":["Convolution Neural Networks (CNNs) are widely used in various domains.","Recent advances in dataflow-based CNN accelerators have enabled CNN inference in resource-constrained edge devices.","These dataflow accelerators utilize inherent data reuse of convolution layers to process CNN models efficiently.","Concealing the architecture of CNN models is critical for privacy and security.","This paper evaluates memory-based side-channel information to recover CNN architectures from dataflow-based CNN inference accelerators.","The proposed attack exploits spatial and temporal data reuse of the dataflow mapping on CNN accelerators and architectural hints to recover the structure of CNN models.","Experimental results demonstrate that our proposed side-channel attack can recover the structures of popular CNN models, namely Lenet, Alexnet, and VGGnet16."],"url":"http://arxiv.org/abs/2311.00579v1"}
{"created":"2023-11-01 15:19:54","title":"Transfer learning for improved generalizability in causal physics-informed neural networks for beam simulations","abstract":"This paper introduces a novel methodology for simulating the dynamics of beams on elastic foundations. Specifically, Euler-Bernoulli and Timoshenko beam models on the Winkler foundation are simulated using a transfer learning approach within a causality-respecting physics-informed neural network (PINN) framework. Conventional PINNs encounter challenges in handling large space-time domains, even for problems with closed-form analytical solutions. A causality-respecting PINN loss function is employed to overcome this limitation, effectively capturing the underlying physics. However, it is observed that the causality-respecting PINN lacks generalizability. We propose using solutions to similar problems instead of training from scratch by employing transfer learning while adhering to causality to accelerate convergence and ensure accurate results across diverse scenarios. Numerical experiments on the Euler-Bernoulli beam highlight the efficacy of the proposed approach for various initial conditions, including those with noise in the initial data. Furthermore, the potential of the proposed method is demonstrated for the Timoshenko beam in an extended spatial and temporal domain. Several comparisons suggest that the proposed method accurately captures the inherent dynamics, outperforming the state-of-the-art physics-informed methods under standard $L^2$-norm metric and accelerating convergence.","sentences":["This paper introduces a novel methodology for simulating the dynamics of beams on elastic foundations.","Specifically, Euler-Bernoulli and Timoshenko beam models on the Winkler foundation are simulated using a transfer learning approach within a causality-respecting physics-informed neural network (PINN) framework.","Conventional PINNs encounter challenges in handling large space-time domains, even for problems with closed-form analytical solutions.","A causality-respecting PINN loss function is employed to overcome this limitation, effectively capturing the underlying physics.","However, it is observed that the causality-respecting PINN lacks generalizability.","We propose using solutions to similar problems instead of training from scratch by employing transfer learning while adhering to causality to accelerate convergence and ensure accurate results across diverse scenarios.","Numerical experiments on the Euler-Bernoulli beam highlight the efficacy of the proposed approach for various initial conditions, including those with noise in the initial data.","Furthermore, the potential of the proposed method is demonstrated for the Timoshenko beam in an extended spatial and temporal domain.","Several comparisons suggest that the proposed method accurately captures the inherent dynamics, outperforming the state-of-the-art physics-informed methods under standard $L^2$-norm metric and accelerating convergence."],"url":"http://arxiv.org/abs/2311.00578v1"}
{"created":"2023-11-01 15:13:43","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing","abstract":"LLaVA-Interactive is a research prototype for multimodal human-AI interaction. The system can have multi-turn dialogues with human users by taking multimodal user inputs and generating multimodal responses. Importantly, LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled to align human intents in the interaction. The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of LLaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN. A diverse set of application scenarios is presented to demonstrate the promises of LLaVA-Interactive and to inspire future research in multimodal interactive systems.","sentences":["LLaVA-Interactive is a research prototype for multimodal human-AI interaction.","The system can have multi-turn dialogues with human users by taking multimodal user inputs and generating multimodal responses.","Importantly, LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled to align human intents in the interaction.","The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of LLaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN.","A diverse set of application scenarios is presented to demonstrate the promises of LLaVA-Interactive and to inspire future research in multimodal interactive systems."],"url":"http://arxiv.org/abs/2311.00571v1"}
{"created":"2023-11-01 15:07:27","title":"CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders","abstract":"A vital and rapidly growing application, remote sensing offers vast yet sparsely labeled, spatially aligned multimodal data; this makes self-supervised learning algorithms invaluable. We present CROMA: a framework that combines contrastive and reconstruction self-supervised objectives to learn rich unimodal and multimodal representations. Our method separately encodes masked-out multispectral optical and synthetic aperture radar samples -- aligned in space and time -- and performs cross-modal contrastive learning. Another encoder fuses these sensors, producing joint multimodal encodings that are used to predict the masked patches via a lightweight decoder. We show that these objectives are complementary when leveraged on spatially aligned multimodal data. We also introduce X- and 2D-ALiBi, which spatially biases our cross- and self-attention matrices. These strategies improve representations and allow our models to effectively extrapolate to images up to 17.6x larger at test-time. CROMA outperforms the current SoTA multispectral model, evaluated on: four classification benchmarks -- finetuning (avg. 1.8%), linear (avg. 2.4%) and nonlinear (avg. 1.4%) probing, kNN classification (avg. 3.5%), and K-means clustering (avg. 8.4%); and three segmentation benchmarks (avg. 6.4%). CROMA's rich, optionally multimodal representations can be widely leveraged across remote sensing applications.","sentences":["A vital and rapidly growing application, remote sensing offers vast yet sparsely labeled, spatially aligned multimodal data; this makes self-supervised learning algorithms invaluable.","We present CROMA: a framework that combines contrastive and reconstruction self-supervised objectives to learn rich unimodal and multimodal representations.","Our method separately encodes masked-out multispectral optical and synthetic aperture radar samples -- aligned in space and time -- and performs cross-modal contrastive learning.","Another encoder fuses these sensors, producing joint multimodal encodings that are used to predict the masked patches via a lightweight decoder.","We show that these objectives are complementary when leveraged on spatially aligned multimodal data.","We also introduce X- and 2D-ALiBi, which spatially biases our cross- and self-attention matrices.","These strategies improve representations and allow our models to effectively extrapolate to images up to 17.6x larger at test-time.","CROMA outperforms the current SoTA multispectral model, evaluated on: four classification benchmarks -- finetuning (avg. 1.8%), linear (avg. 2.4%) and nonlinear (avg. 1.4%) probing, kNN classification (avg. 3.5%), and K-means clustering (avg. 8.4%); and three segmentation benchmarks (avg.","6.4%).","CROMA's rich, optionally multimodal representations can be widely leveraged across remote sensing applications."],"url":"http://arxiv.org/abs/2311.00566v1"}
{"created":"2023-11-01 15:07:03","title":"Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status","abstract":"Intensive Care Units (ICU) provide close supervision and continuous care to patients with life-threatening conditions. However, continuous patient assessment in the ICU is still limited due to time constraints and the workload on healthcare providers. Existing patient assessments in the ICU such as pain or mobility assessment are mostly sporadic and administered manually, thus introducing the potential for human errors. Developing Artificial intelligence (AI) tools that can augment human assessments in the ICU can be beneficial for providing more objective and granular monitoring capabilities. For example, capturing the variations in a patient's facial cues related to pain or agitation can help in adjusting pain-related medications or detecting agitation-inducing conditions such as delirium. Additionally, subtle changes in visual cues during or prior to adverse clinical events could potentially aid in continuous patient monitoring when combined with high-resolution physiological signals and Electronic Health Record (EHR) data. In this paper, we examined the association between visual cues and patient condition including acuity status, acute brain dysfunction, and pain. We leveraged our AU-ICU dataset with 107,064 frames collected in the ICU annotated with facial action units (AUs) labels by trained annotators. We developed a new \"masked loss computation\" technique that addresses the data imbalance problem by maximizing data resource utilization. We trained the model using our AU-ICU dataset in conjunction with three external datasets to detect 18 AUs. The SWIN Transformer model achieved 0.57 mean F1-score and 0.89 mean accuracy on the test set. Additionally, we performed AU inference on 634,054 frames to evaluate the association between facial AUs and clinically important patient conditions such as acuity status, acute brain dysfunction, and pain.","sentences":["Intensive Care Units (ICU) provide close supervision and continuous care to patients with life-threatening conditions.","However, continuous patient assessment in the ICU is still limited due to time constraints and the workload on healthcare providers.","Existing patient assessments in the ICU such as pain or mobility assessment are mostly sporadic and administered manually, thus introducing the potential for human errors.","Developing Artificial intelligence (AI) tools that can augment human assessments in the ICU can be beneficial for providing more objective and granular monitoring capabilities.","For example, capturing the variations in a patient's facial cues related to pain or agitation can help in adjusting pain-related medications or detecting agitation-inducing conditions such as delirium.","Additionally, subtle changes in visual cues during or prior to adverse clinical events could potentially aid in continuous patient monitoring when combined with high-resolution physiological signals and Electronic Health Record (EHR) data.","In this paper, we examined the association between visual cues and patient condition including acuity status, acute brain dysfunction, and pain.","We leveraged our AU-ICU dataset with 107,064 frames collected in the ICU annotated with facial action units (AUs) labels by trained annotators.","We developed a new \"masked loss computation\" technique that addresses the data imbalance problem by maximizing data resource utilization.","We trained the model using our AU-ICU dataset in conjunction with three external datasets to detect 18 AUs.","The SWIN Transformer model achieved 0.57 mean F1-score and 0.89 mean accuracy on the test set.","Additionally, we performed AU inference on 634,054 frames to evaluate the association between facial AUs and clinically important patient conditions such as acuity status, acute brain dysfunction, and pain."],"url":"http://arxiv.org/abs/2311.00565v1"}
{"created":"2023-11-01 14:59:41","title":"MNN: Mixed Nearest-Neighbors for Self-Supervised Learning","abstract":"In contrastive self-supervised learning, positive samples are typically drawn from the same image but in different augmented views, resulting in a relatively limited source of positive samples. An effective way to alleviate this problem is to incorporate the relationship between samples, which involves including the top-k nearest neighbors of positive samples in the framework. However, the problem of false neighbors (i.e., neighbors that do not belong to the same category as the positive sample) is an objective but often overlooked challenge due to the query of neighbor samples without human supervision. In this paper, we present a simple Self-supervised learning framework called Mixed Nearest-Neighbors for Self-Supervised Learning (MNN). MNN optimizes the influence of neighbor samples on the semantics of positive samples through an intuitive weighting approach and image mixture operations. The results of our study demonstrate that MNN exhibits exceptional generalization performance and training efficiency on four benchmark datasets.","sentences":["In contrastive self-supervised learning, positive samples are typically drawn from the same image but in different augmented views, resulting in a relatively limited source of positive samples.","An effective way to alleviate this problem is to incorporate the relationship between samples, which involves including the top-k nearest neighbors of positive samples in the framework.","However, the problem of false neighbors (i.e., neighbors that do not belong to the same category as the positive sample) is an objective but often overlooked challenge due to the query of neighbor samples without human supervision.","In this paper, we present a simple Self-supervised learning framework called Mixed Nearest-Neighbors for Self-Supervised Learning (MNN).","MNN optimizes the influence of neighbor samples on the semantics of positive samples through an intuitive weighting approach and image mixture operations.","The results of our study demonstrate that MNN exhibits exceptional generalization performance and training efficiency on four benchmark datasets."],"url":"http://arxiv.org/abs/2311.00562v1"}
{"created":"2023-11-01 14:56:33","title":"Continuous Experimentation and Human Factors An Exploratory Study","abstract":"In todays rapidly evolving technological landscape, the success of tools and systems relies heavily on their ability to meet the needs and expectations of users. User-centered design approaches, with a focus on human factors, have gained increasing attention as they prioritize the human element in the development process. With the increasing complexity of software-based systems, companies are adopting agile development methodologies and emphasizing continuous software experimentation. However, there is limited knowledge on how to effectively execute continuous experimentation with respect to human factors within this context. This research paper presents an exploratory qualitative study for integrating human factors in continuous experimentation, aiming to uncover distinctive characteristics of human factors and continuous software experiments, practical challenges for integrating human factors in continuous software experiments, and best practices associated with the management of continuous human factors experimentation.","sentences":["In todays rapidly evolving technological landscape, the success of tools and systems relies heavily on their ability to meet the needs and expectations of users.","User-centered design approaches, with a focus on human factors, have gained increasing attention as they prioritize the human element in the development process.","With the increasing complexity of software-based systems, companies are adopting agile development methodologies and emphasizing continuous software experimentation.","However, there is limited knowledge on how to effectively execute continuous experimentation with respect to human factors within this context.","This research paper presents an exploratory qualitative study for integrating human factors in continuous experimentation, aiming to uncover distinctive characteristics of human factors and continuous software experiments, practical challenges for integrating human factors in continuous software experiments, and best practices associated with the management of continuous human factors experimentation."],"url":"http://arxiv.org/abs/2311.00560v1"}
{"created":"2023-11-01 14:55:54","title":"Learning to optimize by multi-gradient for multi-objective optimization","abstract":"The development of artificial intelligence (AI) for science has led to the emergence of learning-based research paradigms, necessitating a compelling reevaluation of the design of multi-objective optimization (MOO) methods. The new generation MOO methods should be rooted in automated learning rather than manual design. In this paper, we introduce a new automatic learning paradigm for optimizing MOO problems, and propose a multi-gradient learning to optimize (ML2O) method, which automatically learns a generator (or mappings) from multiple gradients to update directions. As a learning-based method, ML2O acquires knowledge of local landscapes by leveraging information from the current step and incorporates global experience extracted from historical iteration trajectory data. By introducing a new guarding mechanism, we propose a guarded multi-gradient learning to optimize (GML2O) method, and prove that the iterative sequence generated by GML2O converges to a Pareto critical point. The experimental results demonstrate that our learned optimizer outperforms hand-designed competitors on training multi-task learning (MTL) neural network.","sentences":["The development of artificial intelligence (AI) for science has led to the emergence of learning-based research paradigms, necessitating a compelling reevaluation of the design of multi-objective optimization (MOO) methods.","The new generation MOO methods should be rooted in automated learning rather than manual design.","In this paper, we introduce a new automatic learning paradigm for optimizing MOO problems, and propose a multi-gradient learning to optimize (ML2O) method, which automatically learns a generator (or mappings) from multiple gradients to update directions.","As a learning-based method, ML2O acquires knowledge of local landscapes by leveraging information from the current step and incorporates global experience extracted from historical iteration trajectory data.","By introducing a new guarding mechanism, we propose a guarded multi-gradient learning to optimize (GML2O) method, and prove that the iterative sequence generated by GML2O converges to a Pareto critical point.","The experimental results demonstrate that our learned optimizer outperforms hand-designed competitors on training multi-task learning (MTL) neural network."],"url":"http://arxiv.org/abs/2311.00559v1"}
{"created":"2023-11-01 14:55:04","title":"An Exponential Lower Bound for Linear 3-Query Locally Correctable Codes","abstract":"We prove that the blocklength $n$ of a linear $3$-query locally correctable code (LCC) $\\mathcal{L} \\colon {\\mathbb F}^k \\to {\\mathbb F}^n$ with distance $\\delta$ must be at least $n \\geq 2^{\\Omega\\left(\\left(\\frac{\\delta^2 k}{(|{\\mathbb F}|-1)^2}\\right)^{1/8}\\right)}$. In particular, the blocklength of a linear $3$-query LCC with constant distance over any small field grows exponentially with $k$. This improves on the best prior lower bound of $n \\geq \\tilde{\\Omega}(k^3)$ [AGKM23], which holds even for the weaker setting of $3$-query locally decodable codes (LDCs), and comes close to matching the best-known construction of $3$-query LCCs based on binary Reed-Muller codes, which achieve $n \\leq 2^{O(k^{1/2})}$. Because there is a $3$-query LDC with a strictly subexponential blocklength [Yek08, Efr09], as a corollary we obtain the first strong separation between $q$-query LCCs and LDCs for any constant $q \\geq 3$.   Our proof is based on a new upgrade of the method of spectral refutations via Kikuchi matrices developed in recent works [GKM22, HKM23, AGKM23] that reduces establishing (non-)existence of combinatorial objects to proving unsatisfiability of associated XOR instances. Our key conceptual idea is to apply this method with XOR instances obtained via long-chain derivations, a structured variant of low-width resolution for XOR formulas from proof complexity [Gri01, Sch08].","sentences":["We prove that the blocklength $n$ of a linear $3$-query locally correctable code (LCC) $\\mathcal{L} \\colon {\\mathbb F}^k \\to {\\mathbb F}^n$ with distance $\\delta$ must be at least $n \\geq 2^{\\Omega\\left(\\left(\\frac{\\delta^2 k}{(|{\\mathbb F}|-1)^2}\\right)^{1/8}\\right)}$. In particular, the blocklength of a linear $3$-query LCC with constant distance over any small field grows exponentially with $k$. This improves on the best prior lower bound of $n \\geq \\tilde{\\Omega}(k^3)$","[AGKM23], which holds even for the weaker setting of $3$-query locally decodable codes (LDCs),","and comes close to matching the best-known construction of $3$-query LCCs based on binary Reed-Muller codes, which achieve $n \\leq 2^{O(k^{1/2})}$. Because there is a $3$-query LDC with a strictly subexponential blocklength [Yek08, Efr09], as a corollary we obtain the first strong separation between $q$-query LCCs and LDCs for any constant $q \\geq 3$.   ","Our proof is based on a new upgrade of the method of spectral refutations via Kikuchi matrices developed in recent works [GKM22, HKM23, AGKM23] that reduces establishing (non-)existence of combinatorial objects to proving unsatisfiability of associated XOR instances.","Our key conceptual idea is to apply this method with XOR instances obtained via long-chain derivations, a structured variant of low-width resolution for XOR formulas from proof complexity","[Gri01, Sch08]."],"url":"http://arxiv.org/abs/2311.00558v1"}
{"created":"2023-11-01 14:44:01","title":"ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab","abstract":"The challenge of replicating research results has posed a significant impediment to the field of molecular biology. The advent of modern intelligent systems has led to notable progress in various domains. Consequently, we embarked on an investigation of intelligent monitoring systems as a means of tackling the issue of the reproducibility crisis. Specifically, we first curate a comprehensive multimodal dataset, named ProBio, as an initial step towards this objective. This dataset comprises fine-grained hierarchical annotations intended for the purpose of studying activity understanding in BioLab. Next, we devise two challenging benchmarks, transparent solution tracking and multimodal action recognition, to emphasize the unique characteristics and difficulties associated with activity understanding in BioLab settings. Finally, we provide a thorough experimental evaluation of contemporary video understanding models and highlight their limitations in this specialized domain to identify potential avenues for future research. We hope ProBio with associated benchmarks may garner increased focus on modern AI techniques in the realm of molecular biology.","sentences":["The challenge of replicating research results has posed a significant impediment to the field of molecular biology.","The advent of modern intelligent systems has led to notable progress in various domains.","Consequently, we embarked on an investigation of intelligent monitoring systems as a means of tackling the issue of the reproducibility crisis.","Specifically, we first curate a comprehensive multimodal dataset, named ProBio, as an initial step towards this objective.","This dataset comprises fine-grained hierarchical annotations intended for the purpose of studying activity understanding in BioLab.","Next, we devise two challenging benchmarks, transparent solution tracking and multimodal action recognition, to emphasize the unique characteristics and difficulties associated with activity understanding in BioLab settings.","Finally, we provide a thorough experimental evaluation of contemporary video understanding models and highlight their limitations in this specialized domain to identify potential avenues for future research.","We hope ProBio with associated benchmarks may garner increased focus on modern AI techniques in the realm of molecular biology."],"url":"http://arxiv.org/abs/2311.00556v1"}
{"created":"2023-11-01 14:38:37","title":"Generalised DePIN Protocol: A Framework for Decentralized Physical Infrastructure Networks","abstract":"This paper introduces the Generalised DePIN (GDP) protocol, a comprehensive framework for decentralized physical infrastructure networks. GDP establishes a modular system, enabling tailored application across sectors like ridesharing and power systems. Leveraging device onboarding, multi-sensor redundancy, and a reward/penalty mechanism, GDP promotes genuine behavior and ensures network-wide vigilance. Through continuous audits and updates, the protocol remains dynamic, ensuring sustainable decentralized operations.","sentences":["This paper introduces the Generalised DePIN (GDP) protocol, a comprehensive framework for decentralized physical infrastructure networks.","GDP establishes a modular system, enabling tailored application across sectors like ridesharing and power systems.","Leveraging device onboarding, multi-sensor redundancy, and a reward/penalty mechanism, GDP promotes genuine behavior and ensures network-wide vigilance.","Through continuous audits and updates, the protocol remains dynamic, ensuring sustainable decentralized operations."],"url":"http://arxiv.org/abs/2311.00551v1"}
{"created":"2023-11-01 14:29:46","title":"Continual atlas-based segmentation of prostate MRI","abstract":"Continual learning (CL) methods designed for natural image classification often fail to reach basic quality standards for medical image segmentation. Atlas-based segmentation, a well-established approach in medical imaging, incorporates domain knowledge on the region of interest, leading to semantically coherent predictions. This is especially promising for CL, as it allows us to leverage structural information and strike an optimal balance between model rigidity and plasticity over time. When combined with privacy-preserving prototypes, this process offers the advantages of rehearsal-based CL without compromising patient privacy. We propose Atlas Replay, an atlas-based segmentation approach that uses prototypes to generate high-quality segmentation masks through image registration that maintain consistency even as the training distribution changes. We explore how our proposed method performs compared to state-of-the-art CL methods in terms of knowledge transferability across seven publicly available prostate segmentation datasets. Prostate segmentation plays a vital role in diagnosing prostate cancer, however, it poses challenges due to substantial anatomical variations, benign structural differences in older age groups, and fluctuating acquisition parameters. Our results show that Atlas Replay is both robust and generalizes well to yet-unseen domains while being able to maintain knowledge, unlike end-to-end segmentation methods. Our code base is available under https://github.com/MECLabTUDA/Atlas-Replay.","sentences":["Continual learning (CL) methods designed for natural image classification often fail to reach basic quality standards for medical image segmentation.","Atlas-based segmentation, a well-established approach in medical imaging, incorporates domain knowledge on the region of interest, leading to semantically coherent predictions.","This is especially promising for CL, as it allows us to leverage structural information and strike an optimal balance between model rigidity and plasticity over time.","When combined with privacy-preserving prototypes, this process offers the advantages of rehearsal-based CL without compromising patient privacy.","We propose Atlas Replay, an atlas-based segmentation approach that uses prototypes to generate high-quality segmentation masks through image registration that maintain consistency even as the training distribution changes.","We explore how our proposed method performs compared to state-of-the-art CL methods in terms of knowledge transferability across seven publicly available prostate segmentation datasets.","Prostate segmentation plays a vital role in diagnosing prostate cancer, however, it poses challenges due to substantial anatomical variations, benign structural differences in older age groups, and fluctuating acquisition parameters.","Our results show that Atlas Replay is both robust and generalizes well to yet-unseen domains while being able to maintain knowledge, unlike end-to-end segmentation methods.","Our code base is available under https://github.com/MECLabTUDA/Atlas-Replay."],"url":"http://arxiv.org/abs/2311.00548v1"}
{"created":"2023-11-01 14:25:51","title":"Tackling the Abstraction and Reasoning Corpus (ARC) with Object-centric Models and the MDL Principle","abstract":"The Abstraction and Reasoning Corpus (ARC) is a challenging benchmark, introduced to foster AI research towards human-level intelligence. It is a collection of unique tasks about generating colored grids, specified by a few examples only. In contrast to the transformation-based programs of existing work, we introduce object-centric models that are in line with the natural programs produced by humans. Our models can not only perform predictions, but also provide joint descriptions for input/output pairs. The Minimum Description Length (MDL) principle is used to efficiently search the large model space. A diverse range of tasks are solved, and the learned models are similar to the natural programs. We demonstrate the generality of our approach by applying it to a different domain.","sentences":["The Abstraction and Reasoning Corpus (ARC) is a challenging benchmark, introduced to foster AI research towards human-level intelligence.","It is a collection of unique tasks about generating colored grids, specified by a few examples only.","In contrast to the transformation-based programs of existing work, we introduce object-centric models that are in line with the natural programs produced by humans.","Our models can not only perform predictions, but also provide joint descriptions for input/output pairs.","The Minimum Description Length (MDL) principle is used to efficiently search the large model space.","A diverse range of tasks are solved, and the learned models are similar to the natural programs.","We demonstrate the generality of our approach by applying it to a different domain."],"url":"http://arxiv.org/abs/2311.00545v1"}
{"created":"2023-11-01 14:20:18","title":"An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek","abstract":"Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as \"kosmos\" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. We also discuss the challenges of fitting these models.","sentences":["Word meanings change over time, and word senses evolve, emerge or die out in the process.","For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important.","GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training.","These models represent the senses of a given target word such as \"kosmos\" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses.","The models are fitted using MCMC methods to measure temporal changes in these representations.","In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance.","We show empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods.","We also discuss the challenges of fitting these models."],"url":"http://arxiv.org/abs/2311.00541v1"}
{"created":"2023-11-01 14:13:04","title":"Active Noise Control Portable Device Design","abstract":"While our world is filled with its own natural sounds that we can't resist enjoying, it is also chock-full of other sounds that can be irritating, this is noise. Noise not only influences the working efficiency but also the human's health. The problem of reducing noise is one of great importance and great difficulty. The problem has been addressed in many ways over the years. The current methods for noise reducing mostly rely on the materials and transmission medium, which are only effective to some extent for the high frequency noise. However, the effective reduction noise method especially for low frequency noise is very limited.   Here we come up with a noise reduction system consist of a sensor to detect the noise in the environment. Then the noise will be sent to an electronic control system to process the noise, which will generate a reverse phase frequency signal to counteract the disturbance. Finally, the processed smaller noise will be broadcasted by the speaker. Through this smart noise reduction system, even the noise with low-frequency can be eliminated.   The system is also integrated with sleep tracking and music player applications. It can also remember and store settings for the same environment, sense temperature, and smart control of home furniture, fire alarm, etc. This smart system can transfer data easily by Wi-Fi or Bluetooth and controlled by its APP.   In this project, we will present a model of the above technology which can be used in various environments to prevent noise pollution and provide a solution to the people who have difficulties finding a peaceful and quiet environment for sleep, work or study.","sentences":["While our world is filled with its own natural sounds that we can't resist enjoying, it is also chock-full of other sounds that can be irritating, this is noise.","Noise not only influences the working efficiency but also the human's health.","The problem of reducing noise is one of great importance and great difficulty.","The problem has been addressed in many ways over the years.","The current methods for noise reducing mostly rely on the materials and transmission medium, which are only effective to some extent for the high frequency noise.","However, the effective reduction noise method especially for low frequency noise is very limited.   ","Here we come up with a noise reduction system consist of a sensor to detect the noise in the environment.","Then the noise will be sent to an electronic control system to process the noise, which will generate a reverse phase frequency signal to counteract the disturbance.","Finally, the processed smaller noise will be broadcasted by the speaker.","Through this smart noise reduction system, even the noise with low-frequency can be eliminated.   ","The system is also integrated with sleep tracking and music player applications.","It can also remember and store settings for the same environment, sense temperature, and smart control of home furniture, fire alarm, etc.","This smart system can transfer data easily by Wi-Fi or Bluetooth and controlled by its APP.   ","In this project, we will present a model of the above technology which can be used in various environments to prevent noise pollution and provide a solution to the people who have difficulties finding a peaceful and quiet environment for sleep, work or study."],"url":"http://arxiv.org/abs/2311.00535v1"}
{"created":"2023-11-01 14:10:45","title":"The Eclipse Layout Kernel","abstract":"The Eclipse Layout Kernel (ELK) is a collection of graph drawing algorithms that supports compound graph layout and ports as explicit anchor points of edges. It is available as open-source library under an EPL license. Since its beginning, ELK has served both as a research vehicle for graph drawing algorithms, and as a practical tool for solving real-world problems. ELK and its transpiled JavaScript cousin elkjs are now included in numerous academic and commercial projects.   Most of the algorithms realized in ELK are described in a series of publications. In this paper, the technical description concentrates on the key features of the flag-ship algorithm ELK Layered, the algorithm architecture, and usage. However, the main purpose of this paper is to give the broader view that is typically left unpublished. Specifically, we review its history, give a brief overview of technical papers, discuss lessons learned over the past fifteen years, and present example usages. Finally, we reflect on potential threats to open-source graph drawing libraries.","sentences":["The Eclipse Layout Kernel (ELK) is a collection of graph drawing algorithms that supports compound graph layout and ports as explicit anchor points of edges.","It is available as open-source library under an EPL license.","Since its beginning, ELK has served both as a research vehicle for graph drawing algorithms, and as a practical tool for solving real-world problems.","ELK and its transpiled JavaScript cousin elkjs are now included in numerous academic and commercial projects.   ","Most of the algorithms realized in ELK are described in a series of publications.","In this paper, the technical description concentrates on the key features of the flag-ship algorithm ELK Layered, the algorithm architecture, and usage.","However, the main purpose of this paper is to give the broader view that is typically left unpublished.","Specifically, we review its history, give a brief overview of technical papers, discuss lessons learned over the past fifteen years, and present example usages.","Finally, we reflect on potential threats to open-source graph drawing libraries."],"url":"http://arxiv.org/abs/2311.00533v1"}
{"created":"2023-11-01 14:08:56","title":"The Development of LLMs for Embodied Navigation","abstract":"In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN","sentences":["In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications.","The application of LLMs with Embodied Intelligence has emerged as a significant area of focus.","Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making.","LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities.","This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation.","It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets.","Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field.","A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN"],"url":"http://arxiv.org/abs/2311.00530v1"}
{"created":"2023-11-01 13:56:21","title":"A Leakage-based Method for Mitigation of Faulty Reconfigurable Intelligent Surfaces","abstract":"Reconfigurable Intelligent Surfaces (RISs) are expected to be massively deployed in future beyond-5th generation wireless networks, thanks to their ability to programmatically alter the propagation environment, inherent low-cost and low-maintenance nature. Indeed, they are envisioned to be implemented on the facades of buildings or on moving objects. However, such an innovative characteristic may potentially turn into an involuntary negative behavior that needs to be addressed: an undesired signal scattering. In particular, RIS elements may be prone to experience failures due to lack of proper maintenance or external environmental factors. While the resulting Signal-to-Noise-Ratio (SNR) at the intended User Equipment (UE) may not be significantly degraded, we demonstrate the potential risks in terms of unwanted spreading of the transmit signal to non-intended UE. In this regard, we consider the problem of mitigating such undesired effect by proposing two simple yet effective algorithms, which are based on maximizing the Signal-to-Leakage- and-Noise-Ratio (SLNR) over a predefined two-dimensional (2D) area and are applicable in the case of perfect channel-state-information (CSI) and partial CSI, respectively. Numerical and full-wave simulations demonstrate the added gains compared to leakage-unaware and reference schemes.","sentences":["Reconfigurable Intelligent Surfaces (RISs) are expected to be massively deployed in future beyond-5th generation wireless networks, thanks to their ability to programmatically alter the propagation environment, inherent low-cost and low-maintenance nature.","Indeed, they are envisioned to be implemented on the facades of buildings or on moving objects.","However, such an innovative characteristic may potentially turn into an involuntary negative behavior that needs to be addressed: an undesired signal scattering.","In particular, RIS elements may be prone to experience failures due to lack of proper maintenance or external environmental factors.","While the resulting Signal-to-Noise-Ratio (SNR) at the intended User Equipment (UE) may not be significantly degraded, we demonstrate the potential risks in terms of unwanted spreading of the transmit signal to non-intended UE.","In this regard, we consider the problem of mitigating such undesired effect by proposing two simple yet effective algorithms, which are based on maximizing the Signal-to-Leakage- and-Noise-Ratio (SLNR) over a predefined two-dimensional (2D) area and are applicable in the case of perfect channel-state-information (CSI) and partial CSI, respectively.","Numerical and full-wave simulations demonstrate the added gains compared to leakage-unaware and reference schemes."],"url":"http://arxiv.org/abs/2311.00527v1"}
{"created":"2023-11-01 13:50:47","title":"Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning","abstract":"In the field of explainable Artificial Intelligence (XAI), sequential counterfactual (SCF) examples are often used to alter the decision of a trained classifier by implementing a sequence of modifications to the input instance. Although certain test-time algorithms aim to optimize for each new instance individually, recently Reinforcement Learning (RL) methods have been proposed that seek to learn policies for discovering SCFs, thereby enhancing scalability. As is typical in RL, the formulation of the RL problem, including the specification of state space, actions, and rewards, can often be ambiguous. In this work, we identify shortcomings in existing methods that can result in policies with undesired properties, such as a bias towards specific actions. We propose to use the output probabilities of the classifier to create a more informative reward, to mitigate this effect.","sentences":["In the field of explainable Artificial Intelligence (XAI), sequential counterfactual (SCF) examples are often used to alter the decision of a trained classifier by implementing a sequence of modifications to the input instance.","Although certain test-time algorithms aim to optimize for each new instance individually, recently Reinforcement Learning (RL) methods have been proposed that seek to learn policies for discovering SCFs, thereby enhancing scalability.","As is typical in RL, the formulation of the RL problem, including the specification of state space, actions, and rewards, can often be ambiguous.","In this work, we identify shortcomings in existing methods that can result in policies with undesired properties, such as a bias towards specific actions.","We propose to use the output probabilities of the classifier to create a more informative reward, to mitigate this effect."],"url":"http://arxiv.org/abs/2311.00523v1"}
{"created":"2023-11-01 13:49:31","title":"Text Rendering Strategies for Pixel Language Models","abstract":"Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks. This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model. Our analyses show that character bigram rendering leads to a consistently better model but with an anisotropic patch embedding space, driven by a patch frequency bias, highlighting the connections between image patch- and tokenization-based language models.","sentences":["Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling.","However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations.","In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks.","This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model.","Our analyses show that character bigram rendering leads to a consistently better model but with an anisotropic patch embedding space, driven by a patch frequency bias, highlighting the connections between image patch- and tokenization-based language models."],"url":"http://arxiv.org/abs/2311.00522v1"}
{"created":"2023-11-01 13:41:44","title":"Improving Cardiovascular Disease Prediction Through Comparative Analysis of Machine Learning Models: A Case Study on Myocardial Infarction","abstract":"Cardiovascular disease remains a leading cause of mortality in the contemporary world. Its association with smoking, elevated blood pressure, and cholesterol levels underscores the significance of these risk factors. This study addresses the challenge of predicting myocardial illness, a formidable task in medical research. Accurate predictions are pivotal for refining healthcare strategies. This investigation conducts a comparative analysis of six distinct machine learning models: Logistic Regression, Support Vector Machine, Decision Tree, Bagging, XGBoost, and LightGBM. The attained outcomes exhibit promise, with accuracy rates as follows: Logistic Regression (81.00%), Support Vector Machine (75.01%), XGBoost (92.72%), LightGBM (90.60%), Decision Tree (82.30%), and Bagging (83.01%). Notably, XGBoost emerges as the top-performing model. These findings underscore its potential to enhance predictive precision for coronary infarction. As the prevalence of cardiovascular risk factors persists, incorporating advanced machine learning techniques holds the potential to refine proactive medical interventions.","sentences":["Cardiovascular disease remains a leading cause of mortality in the contemporary world.","Its association with smoking, elevated blood pressure, and cholesterol levels underscores the significance of these risk factors.","This study addresses the challenge of predicting myocardial illness, a formidable task in medical research.","Accurate predictions are pivotal for refining healthcare strategies.","This investigation conducts a comparative analysis of six distinct machine learning models: Logistic Regression, Support Vector Machine, Decision Tree, Bagging, XGBoost, and LightGBM.","The attained outcomes exhibit promise, with accuracy rates as follows: Logistic Regression (81.00%), Support Vector Machine (75.01%), XGBoost (92.72%), LightGBM (90.60%), Decision Tree (82.30%), and Bagging (83.01%).","Notably, XGBoost emerges as the top-performing model.","These findings underscore its potential to enhance predictive precision for coronary infarction.","As the prevalence of cardiovascular risk factors persists, incorporating advanced machine learning techniques holds the potential to refine proactive medical interventions."],"url":"http://arxiv.org/abs/2311.00517v1"}
{"created":"2023-11-01 13:37:59","title":"How Hard Is Squash? -- Towards Information Theoretic Analysis of Motor Behavior in Squash","abstract":"Fitts' law has been widely employed as a research method for analyzing tasks within the domain of Human-Computer Interaction (HCI). However, its application to non-computer tasks has remained limited. This study aims to extend the application of Fitts' law to the realm of sports, specifically focusing on squash. Squash is a high-intensity sport that requires quick movements and precise shots. Our research investigates the effectiveness of utilizing Fitts' law to evaluate the task difficulty and effort level associated with executing and responding to various squash shots. By understanding the effort/information rate required for each shot, we can determine which shots are more effective in making the opponent work harder. Additionally, this knowledge can be valuable for coaches in designing training programs. However, since Fitts' law was primarily developed for human-computer interaction, we adapted it to fit the squash scenario. This paper provides an overview of Fitts' law and its relevance to sports, elucidates the motivation driving this investigation, outlines the methodology employed to explore this novel avenue, and presents the obtained results, concluding with key insights. We conducted experiments with different shots and players, collecting data on shot speed, player movement time, and distance traveled. Using this data, we formulated a modified version of Fitts' law specifically for squash. The results provide insights into the difficulty and effectiveness of various shots, offering valuable information for both players and coaches in the sport of squash.","sentences":["Fitts' law has been widely employed as a research method for analyzing tasks within the domain of Human-Computer Interaction (HCI).","However, its application to non-computer tasks has remained limited.","This study aims to extend the application of Fitts' law to the realm of sports, specifically focusing on squash.","Squash is a high-intensity sport that requires quick movements and precise shots.","Our research investigates the effectiveness of utilizing Fitts' law to evaluate the task difficulty and effort level associated with executing and responding to various squash shots.","By understanding the effort/information rate required for each shot, we can determine which shots are more effective in making the opponent work harder.","Additionally, this knowledge can be valuable for coaches in designing training programs.","However, since Fitts' law was primarily developed for human-computer interaction, we adapted it to fit the squash scenario.","This paper provides an overview of Fitts' law and its relevance to sports, elucidates the motivation driving this investigation, outlines the methodology employed to explore this novel avenue, and presents the obtained results, concluding with key insights.","We conducted experiments with different shots and players, collecting data on shot speed, player movement time, and distance traveled.","Using this data, we formulated a modified version of Fitts' law specifically for squash.","The results provide insights into the difficulty and effectiveness of various shots, offering valuable information for both players and coaches in the sport of squash."],"url":"http://arxiv.org/abs/2311.00514v1"}
{"created":"2023-11-01 13:36:20","title":"Rule-Based Error Classification for Analyzing Differences in Frequent Errors","abstract":"Finding and fixing errors is a time-consuming task not only for novice programmers but also for expert programmers. Prior work has identified frequent error patterns among various levels of programmers. However, the differences in the tendencies between novices and experts have yet to be revealed. From the knowledge of the frequent errors in each level of programmers, instructors will be able to provide helpful advice for each level of learners. In this paper, we propose a rule-based error classification tool to classify errors in code pairs consisting of wrong and correct programs. We classify errors for 95,631 code pairs and identify 3.47 errors on average, which are submitted by various levels of programmers on an online judge system. The classified errors are used to analyze the differences in frequent errors between novice and expert programmers. The analyzed results show that, as for the same introductory problems, errors made by novices are due to the lack of knowledge in programming, and the mistakes are considered an essential part of the learning process. On the other hand, errors made by experts are due to misunderstandings caused by the carelessness of reading problems or the challenges of solving problems differently than usual. The proposed tool can be used to create error-labeled datasets and for further code-related educational research.","sentences":["Finding and fixing errors is a time-consuming task not only for novice programmers but also for expert programmers.","Prior work has identified frequent error patterns among various levels of programmers.","However, the differences in the tendencies between novices and experts have yet to be revealed.","From the knowledge of the frequent errors in each level of programmers, instructors will be able to provide helpful advice for each level of learners.","In this paper, we propose a rule-based error classification tool to classify errors in code pairs consisting of wrong and correct programs.","We classify errors for 95,631 code pairs and identify 3.47 errors on average, which are submitted by various levels of programmers on an online judge system.","The classified errors are used to analyze the differences in frequent errors between novice and expert programmers.","The analyzed results show that, as for the same introductory problems, errors made by novices are due to the lack of knowledge in programming, and the mistakes are considered an essential part of the learning process.","On the other hand, errors made by experts are due to misunderstandings caused by the carelessness of reading problems or the challenges of solving problems differently than usual.","The proposed tool can be used to create error-labeled datasets and for further code-related educational research."],"url":"http://arxiv.org/abs/2311.00513v1"}
{"created":"2023-11-01 13:14:23","title":"Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks","abstract":"We investigate MT evaluation metric performance on adversarially-synthesized texts, to shed light on metric robustness. We experiment with word- and character-level attacks on three popular machine translation metrics: BERTScore, BLEURT, and COMET. Our human experiments validate that automatic metrics tend to overpenalize adversarially-degraded translations. We also identify inconsistencies in BERTScore ratings, where it judges the original sentence and the adversarially-degraded one as similar, while judging the degraded translation as notably worse than the original with respect to the reference. We identify patterns of brittleness that motivate more robust metric development.","sentences":["We investigate MT evaluation metric performance on adversarially-synthesized texts, to shed light on metric robustness.","We experiment with word- and character-level attacks on three popular machine translation metrics: BERTScore, BLEURT, and COMET.","Our human experiments validate that automatic metrics tend to overpenalize adversarially-degraded translations.","We also identify inconsistencies in BERTScore ratings, where it judges the original sentence and the adversarially-degraded one as similar, while judging the degraded translation as notably worse than the original with respect to the reference.","We identify patterns of brittleness that motivate more robust metric development."],"url":"http://arxiv.org/abs/2311.00508v1"}
{"created":"2023-11-01 13:08:50","title":"Efficient LLM Inference on CPUs","abstract":"Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.","sentences":["Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks.","However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth.","In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently.","We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs.","We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs.","The code is publicly available at: https://github.com/intel/intel-extension-for-transformers."],"url":"http://arxiv.org/abs/2311.00502v1"}
{"created":"2023-11-01 13:00:46","title":"Intriguing Properties of Data Attribution on Diffusion Models","abstract":"Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a significantly more efficient approach for attributing diffusion models, while the unexpected findings suggest that at least in non-convex settings, constructions guided by theoretical assumptions may lead to inferior attribution performance. The code is available at https://github.com/sail-sg/D-TRAK.","sentences":["Data attribution seeks to trace model outputs back to training data.","With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited.","Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness.","In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench.","Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation.","Our work presents a significantly more efficient approach for attributing diffusion models, while the unexpected findings suggest that at least in non-convex settings, constructions guided by theoretical assumptions may lead to inferior attribution performance.","The code is available at https://github.com/sail-sg/D-TRAK."],"url":"http://arxiv.org/abs/2311.00500v1"}
