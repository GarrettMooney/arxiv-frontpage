{"created":"2023-12-07 18:59:59","title":"Scaling Laws of Synthetic Images for Model Training ... for Now","abstract":"Recent significant advances in text-to-image models unlock the possibility of training vision systems using synthetic images, potentially overcoming the difficulty of collecting curated data at scale. It is unclear, however, how these models behave at scale, as more synthetic data is added to the training set. In this paper we study the scaling laws of synthetic images generated by state of the art text-to-image models, for the training of supervised models: image classifiers with label supervision, and CLIP with language supervision. We identify several factors, including text prompts, classifier-free guidance scale, and types of text-to-image models, that significantly affect scaling behavior. After tuning these factors, we observe that synthetic images demonstrate a scaling trend similar to, but slightly less effective than, real images in CLIP training, while they significantly underperform in scaling when training supervised image classifiers. Our analysis indicates that the main reason for this underperformance is the inability of off-the-shelf text-to-image models to generate certain concepts, a limitation that significantly impairs the training of image classifiers. Our findings also suggest that scaling synthetic data can be particularly effective in scenarios such as: (1) when there is a limited supply of real images for a supervised problem (e.g., fewer than 0.5 million images in ImageNet), (2) when the evaluation dataset diverges significantly from the training data, indicating the out-of-distribution scenario, or (3) when synthetic data is used in conjunction with real images, as demonstrated in the training of CLIP models.","sentences":["Recent significant advances in text-to-image models unlock the possibility of training vision systems using synthetic images, potentially overcoming the difficulty of collecting curated data at scale.","It is unclear, however, how these models behave at scale, as more synthetic data is added to the training set.","In this paper we study the scaling laws of synthetic images generated by state of the art text-to-image models, for the training of supervised models: image classifiers with label supervision, and CLIP with language supervision.","We identify several factors, including text prompts, classifier-free guidance scale, and types of text-to-image models, that significantly affect scaling behavior.","After tuning these factors, we observe that synthetic images demonstrate a scaling trend similar to, but slightly less effective than, real images in CLIP training, while they significantly underperform in scaling when training supervised image classifiers.","Our analysis indicates that the main reason for this underperformance is the inability of off-the-shelf text-to-image models to generate certain concepts, a limitation that significantly impairs the training of image classifiers.","Our findings also suggest that scaling synthetic data can be particularly effective in scenarios such as: (1) when there is a limited supply of real images for a supervised problem (e.g., fewer than 0.5 million images in ImageNet), (2) when the evaluation dataset diverges significantly from the training data, indicating the out-of-distribution scenario, or (3) when synthetic data is used in conjunction with real images, as demonstrated in the training of CLIP models."],"url":"http://arxiv.org/abs/2312.04567v1"}
{"created":"2023-12-07 18:59:58","title":"Gen2Det: Generate to Detect","abstract":"Recently diffusion models have shown improvement in synthetic image quality as well as better control in generation. We motivate and present Gen2Det, a simple modular pipeline to create synthetic training data for object detection for free by leveraging state-of-the-art grounded image generation methods. Unlike existing works which generate individual object instances, require identifying foreground followed by pasting on other images, we simplify to directly generating scene-centric images. In addition to the synthetic data, Gen2Det also proposes a suite of techniques to best utilize the generated data, including image-level filtering, instance-level filtering, and better training recipe to account for imperfections in the generation. Using Gen2Det, we show healthy improvements on object detection and segmentation tasks under various settings and agnostic to detection methods. In the long-tailed detection setting on LVIS, Gen2Det improves the performance on rare categories by a large margin while also significantly improving the performance on other categories, e.g. we see an improvement of 2.13 Box AP and 1.84 Mask AP over just training on real data on LVIS with Mask R-CNN. In the low-data regime setting on COCO, Gen2Det consistently improves both Box and Mask AP by 2.27 and 1.85 points. In the most general detection setting, Gen2Det still demonstrates robust performance gains, e.g. it improves the Box and Mask AP on COCO by 0.45 and 0.32 points.","sentences":["Recently diffusion models have shown improvement in synthetic image quality as well as better control in generation.","We motivate and present Gen2Det, a simple modular pipeline to create synthetic training data for object detection for free by leveraging state-of-the-art grounded image generation methods.","Unlike existing works which generate individual object instances, require identifying foreground followed by pasting on other images, we simplify to directly generating scene-centric images.","In addition to the synthetic data, Gen2Det also proposes a suite of techniques to best utilize the generated data, including image-level filtering, instance-level filtering, and better training recipe to account for imperfections in the generation.","Using Gen2Det, we show healthy improvements on object detection and segmentation tasks under various settings and agnostic to detection methods.","In the long-tailed detection setting on LVIS, Gen2Det improves the performance on rare categories by a large margin while also significantly improving the performance on other categories, e.g. we see an improvement of 2.13 Box AP and 1.84 Mask AP over just training on real data on LVIS with Mask R-CNN.","In the low-data regime setting on COCO, Gen2Det consistently improves both Box and Mask AP by 2.27 and 1.85 points.","In the most general detection setting, Gen2Det still demonstrates robust performance gains, e.g. it improves the Box and Mask AP on COCO by 0.45 and 0.32 points."],"url":"http://arxiv.org/abs/2312.04566v1"}
{"created":"2023-12-07 18:59:56","title":"MuRF: Multi-Baseline Radiance Fields","abstract":"We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward approach to solving sparse view synthesis under multiple different baseline settings (small and large baselines, and different number of input views). To render a target novel view, we discretize the 3D space into planes parallel to the target image plane, and accordingly construct a target view frustum volume. Such a target volume representation is spatially aligned with the target view, which effectively aggregates relevant information from the input views for high-quality rendering. It also facilitates subsequent radiance field regression with a convolutional network thanks to its axis-aligned nature. The 3D context modeled by the convolutional network enables our method to synthesis sharper scene structures than prior works. Our MuRF achieves state-of-the-art performance across multiple different baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K and LLFF). We also show promising zero-shot generalization abilities on the Mip-NeRF 360 dataset, demonstrating the general applicability of MuRF.","sentences":["We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward approach to solving sparse view synthesis under multiple different baseline settings (small and large baselines, and different number of input views).","To render a target novel view, we discretize the 3D space into planes parallel to the target image plane, and accordingly construct a target view frustum volume.","Such a target volume representation is spatially aligned with the target view, which effectively aggregates relevant information from the input views for high-quality rendering.","It also facilitates subsequent radiance field regression with a convolutional network thanks to its axis-aligned nature.","The 3D context modeled by the convolutional network enables our method to synthesis sharper scene structures than prior works.","Our MuRF achieves state-of-the-art performance across multiple different baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K and LLFF).","We also show promising zero-shot generalization abilities on the Mip-NeRF 360 dataset, demonstrating the general applicability of MuRF."],"url":"http://arxiv.org/abs/2312.04565v1"}
{"created":"2023-12-07 18:59:55","title":"EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS","abstract":"Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis. It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training. They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene. We present a technique utilizing quantized embeddings to significantly reduce memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds. Our approach results in scene representations with fewer Gaussians and quantized representations, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes. We reduce memory by more than an order of magnitude all while maintaining the reconstruction quality. We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x less memory and faster training/inference speed. Project page and code is available https://efficientgaussian.github.io","sentences":["Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis.","It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs).","Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training.","They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene.","We present a technique utilizing quantized embeddings to significantly reduce memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds.","Our approach results in scene representations with fewer Gaussians and quantized representations, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes.","We reduce memory by more than an order of magnitude all while maintaining the reconstruction quality.","We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x less memory and faster training/inference speed.","Project page and code is available https://efficientgaussian.github.io"],"url":"http://arxiv.org/abs/2312.04564v1"}
{"created":"2023-12-07 18:59:52","title":"Visual Geometry Grounded Deep Structure From Motion","abstract":"Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.","sentences":["Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images.","Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment.","Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline.","Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner.","To this end, we introduce new mechanisms and simplifications.","First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches.","Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras.","Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer.","We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D."],"url":"http://arxiv.org/abs/2312.04563v1"}
{"created":"2023-12-07 18:59:41","title":"NeRFiller: Completing Scenes via Generative 3D Inpainting","abstract":"We propose NeRFiller, an approach that completes missing portions of a 3D capture via generative 3D inpainting using off-the-shelf 2D visual generative models. Often parts of a captured 3D scene or object are missing due to mesh reconstruction failures or a lack of observations (e.g., contact regions, such as the bottom of objects, or hard-to-reach areas). We approach this challenging 3D inpainting problem by leveraging a 2D inpainting diffusion model. We identify a surprising behavior of these models, where they generate more 3D consistent inpaints when images form a 2$\\times$2 grid, and show how to generalize this behavior to more than four images. We then present an iterative framework to distill these inpainted regions into a single consistent 3D scene. In contrast to related works, we focus on completing scenes rather than deleting foreground objects, and our approach does not require tight 2D object masks or text. We compare our approach to relevant baselines adapted to our setting on a variety of scenes, where NeRFiller creates the most 3D consistent and plausible scene completions. Our project page is at https://ethanweber.me/nerfiller.","sentences":["We propose NeRFiller, an approach that completes missing portions of a 3D capture via generative 3D inpainting using off-the-shelf 2D visual generative models.","Often parts of a captured 3D scene or object are missing due to mesh reconstruction failures or a lack of observations (e.g., contact regions, such as the bottom of objects, or hard-to-reach areas).","We approach this challenging 3D inpainting problem by leveraging a 2D inpainting diffusion model.","We identify a surprising behavior of these models, where they generate more 3D consistent inpaints when images form a 2$\\times$2 grid, and show how to generalize this behavior to more than four images.","We then present an iterative framework to distill these inpainted regions into a single consistent 3D scene.","In contrast to related works, we focus on completing scenes rather than deleting foreground objects, and our approach does not require tight 2D object masks or text.","We compare our approach to relevant baselines adapted to our setting on a variety of scenes, where NeRFiller creates the most 3D consistent and plausible scene completions.","Our project page is at https://ethanweber.me/nerfiller."],"url":"http://arxiv.org/abs/2312.04560v1"}
{"created":"2023-12-07 18:59:41","title":"GenDeF: Learning Generative Deformation Field for Video Generation","abstract":"We offer a new perspective on approaching the task of video generation. Instead of directly synthesizing a sequence of frames, we propose to render a video by warping one static image with a generative deformation field (GenDeF). Such a pipeline enjoys three appealing advantages. First, we can sufficiently reuse a well-trained image generator to synthesize the static image (also called canonical image), alleviating the difficulty in producing a video and thereby resulting in better visual quality. Second, we can easily convert a deformation field to optical flows, making it possible to apply explicit structural regularizations for motion modeling, leading to temporally consistent results. Third, the disentanglement between content and motion allows users to process a synthesized video through processing its corresponding static image without any tuning, facilitating many applications like video editing, keypoint tracking, and video segmentation. Both qualitative and quantitative results on three common video generation benchmarks demonstrate the superiority of our GenDeF method.","sentences":["We offer a new perspective on approaching the task of video generation.","Instead of directly synthesizing a sequence of frames, we propose to render a video by warping one static image with a generative deformation field (GenDeF).","Such a pipeline enjoys three appealing advantages.","First, we can sufficiently reuse a well-trained image generator to synthesize the static image (also called canonical image), alleviating the difficulty in producing a video and thereby resulting in better visual quality.","Second, we can easily convert a deformation field to optical flows, making it possible to apply explicit structural regularizations for motion modeling, leading to temporally consistent results.","Third, the disentanglement between content and motion allows users to process a synthesized video through processing its corresponding static image without any tuning, facilitating many applications like video editing, keypoint tracking, and video segmentation.","Both qualitative and quantitative results on three common video generation benchmarks demonstrate the superiority of our GenDeF method."],"url":"http://arxiv.org/abs/2312.04561v1"}
{"created":"2023-12-07 18:59:33","title":"PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation","abstract":"We present PrimDiffusion, the first diffusion-based framework for 3D human generation. Devising diffusion models for 3D human generation is difficult due to the intensive computational cost of 3D representations and the articulated topology of 3D humans. To tackle these challenges, our key insight is operating the denoising diffusion process directly on a set of volumetric primitives, which models the human body as a number of small volumes with radiance and kinematic information. This volumetric primitives representation marries the capacity of volumetric representations with the efficiency of primitive-based rendering. Our PrimDiffusion framework has three appealing properties: 1) compact and expressive parameter space for the diffusion model, 2) flexible 3D representation that incorporates human prior, and 3) decoder-free rendering for efficient novel-view and novel-pose synthesis. Extensive experiments validate that PrimDiffusion outperforms state-of-the-art methods in 3D human generation. Notably, compared to GAN-based methods, our PrimDiffusion supports real-time rendering of high-quality 3D humans at a resolution of $512\\times512$ once the denoising process is done. We also demonstrate the flexibility of our framework on training-free conditional generation such as texture transfer and 3D inpainting.","sentences":["We present PrimDiffusion, the first diffusion-based framework for 3D human generation.","Devising diffusion models for 3D human generation is difficult due to the intensive computational cost of 3D representations and the articulated topology of 3D humans.","To tackle these challenges, our key insight is operating the denoising diffusion process directly on a set of volumetric primitives, which models the human body as a number of small volumes with radiance and kinematic information.","This volumetric primitives representation marries the capacity of volumetric representations with the efficiency of primitive-based rendering.","Our PrimDiffusion framework has three appealing properties: 1) compact and expressive parameter space for the diffusion model, 2) flexible 3D representation that incorporates human prior, and 3) decoder-free rendering for efficient novel-view and novel-pose synthesis.","Extensive experiments validate that PrimDiffusion outperforms state-of-the-art methods in 3D human generation.","Notably, compared to GAN-based methods, our PrimDiffusion supports real-time rendering of high-quality 3D humans at a resolution of $512\\times512$ once the denoising process is done.","We also demonstrate the flexibility of our framework on training-free conditional generation such as texture transfer and 3D inpainting."],"url":"http://arxiv.org/abs/2312.04559v1"}
{"created":"2023-12-07 18:59:31","title":"MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar","abstract":"The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods.","sentences":["The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds.","Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research.","However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency.","In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos.","We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology.","These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation.","Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering.","Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods."],"url":"http://arxiv.org/abs/2312.04558v1"}
{"created":"2023-12-07 18:59:30","title":"GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation","abstract":"In this study, we explore Transformer-based diffusion models for image and video generation. Despite the dominance of Transformer architectures in various fields due to their flexibility and scalability, the visual generative domain primarily utilizes CNN-based U-Net architectures, particularly in diffusion-based models. We introduce GenTron, a family of Generative models employing Transformer-based diffusion, to address this gap. Our initial step was to adapt Diffusion Transformers (DiTs) from class to text conditioning, a process involving thorough empirical exploration of the conditioning mechanism. We then scale GenTron from approximately 900M to over 3B parameters, observing significant improvements in visual quality. Furthermore, we extend GenTron to text-to-video generation, incorporating novel motion-free guidance to enhance video quality. In human evaluations against SDXL, GenTron achieves a 51.1% win rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text alignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench, underscoring its strengths in compositional generation. We believe this work will provide meaningful insights and serve as a valuable reference for future research.","sentences":["In this study, we explore Transformer-based diffusion models for image and video generation.","Despite the dominance of Transformer architectures in various fields due to their flexibility and scalability, the visual generative domain primarily utilizes CNN-based U-Net architectures, particularly in diffusion-based models.","We introduce GenTron, a family of Generative models employing Transformer-based diffusion, to address this gap.","Our initial step was to adapt Diffusion Transformers (DiTs) from class to text conditioning, a process involving thorough empirical exploration of the conditioning mechanism.","We then scale GenTron from approximately 900M to over 3B parameters, observing significant improvements in visual quality.","Furthermore, we extend GenTron to text-to-video generation, incorporating novel motion-free guidance to enhance video quality.","In human evaluations against SDXL, GenTron achieves a 51.1% win rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text alignment (with a 42.9% draw rate).","GenTron also excels in the T2I-CompBench, underscoring its strengths in compositional generation.","We believe this work will provide meaningful insights and serve as a valuable reference for future research."],"url":"http://arxiv.org/abs/2312.04557v1"}
{"created":"2023-12-07 18:59:29","title":"Large Language Models for Mathematicians","abstract":"Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LMMs to change how mathematicians work.","sentences":["Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code.","For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work.","In this note, we discuss to what extent they can aid professional mathematicians.","We first provide a mathematical description of the transformer model used in all modern language models.","Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models.","Finally, we shed light on the potential of LMMs to change how mathematicians work."],"url":"http://arxiv.org/abs/2312.04556v1"}
{"created":"2023-12-07 18:59:22","title":"Improved Visual Grounding through Self-Consistent Explanations","abstract":"Vision-and-language models trained to match images with text can be combined with visual explanation methods to point to the locations of specific objects in an image. Our work shows that the localization --\"grounding\"-- abilities of these models can be further improved by finetuning for self-consistent visual explanations. We propose a strategy for augmenting existing text-image datasets with paraphrases using a large language model, and SelfEQ, a weakly-supervised strategy on visual explanation maps for paraphrases that encourages self-consistency. Specifically, for an input textual phrase, we attempt to generate a paraphrase and finetune the model so that the phrase and paraphrase map to the same region in the image. We posit that this both expands the vocabulary that the model is able to handle, and improves the quality of the object locations highlighted by gradient-based visual explanation methods (e.g. GradCAM). We demonstrate that SelfEQ improves performance on Flickr30k, ReferIt, and RefCOCO+ over a strong baseline method and several prior works. Particularly, comparing to other methods that do not use any type of box annotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%), 67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on RefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on average).","sentences":["Vision-and-language models trained to match images with text can be combined with visual explanation methods to point to the locations of specific objects in an image.","Our work shows that the localization --\"grounding\"-- abilities of these models can be further improved by finetuning for self-consistent visual explanations.","We propose a strategy for augmenting existing text-image datasets with paraphrases using a large language model, and SelfEQ, a weakly-supervised strategy on visual explanation maps for paraphrases that encourages self-consistency.","Specifically, for an input textual phrase, we attempt to generate a paraphrase and finetune the model so that the phrase and paraphrase map to the same region in the image.","We posit that this both expands the vocabulary that the model is able to handle, and improves the quality of the object locations highlighted by gradient-based visual explanation methods (e.g. GradCAM).","We demonstrate that SelfEQ improves performance on Flickr30k, ReferIt, and RefCOCO+ over a strong baseline method and several prior works.","Particularly, comparing to other methods that do not use any type of box annotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%), 67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on RefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on average)."],"url":"http://arxiv.org/abs/2312.04554v1"}
{"created":"2023-12-07 18:59:21","title":"SPIDeRS: Structured Polarization for Invisible Depth and Reflectance Sensing","abstract":"Can we capture shape and reflectance in stealth? Such capability would be valuable for many application domains in vision, xR, robotics, and HCI. We introduce Structured Polarization, the first depth and reflectance sensing method using patterns of polarized light (SPIDeRS). The key idea is to modulate the angle of linear polarization (AoLP) of projected light at each pixel. The use of polarization makes it invisible and lets us recover not only depth but also directly surface normals and even reflectance. We implement SPIDeRS with a liquid crystal spatial light modulator (SLM) and a polarimetric camera. We derive a novel method for robustly extracting the projected structured polarization pattern from the polarimetric object appearance. We evaluate the effectiveness of SPIDeRS by applying it to a number of real-world objects. The results show that our method successfully reconstructs object shapes of various materials and is robust to diffuse reflection and ambient light. We also demonstrate relighting using recovered surface normals and reflectance. We believe SPIDeRS opens a new avenue of polarization use in visual sensing.","sentences":["Can we capture shape and reflectance in stealth?","Such capability would be valuable for many application domains in vision, xR, robotics, and HCI.","We introduce Structured Polarization, the first depth and reflectance sensing method using patterns of polarized light (SPIDeRS).","The key idea is to modulate the angle of linear polarization (AoLP) of projected light at each pixel.","The use of polarization makes it invisible and lets us recover not only depth but also directly surface normals and even reflectance.","We implement SPIDeRS with a liquid crystal spatial light modulator (SLM) and a polarimetric camera.","We derive a novel method for robustly extracting the projected structured polarization pattern from the polarimetric object appearance.","We evaluate the effectiveness of SPIDeRS by applying it to a number of real-world objects.","The results show that our method successfully reconstructs object shapes of various materials and is robust to diffuse reflection and ambient light.","We also demonstrate relighting using recovered surface normals and reflectance.","We believe SPIDeRS opens a new avenue of polarization use in visual sensing."],"url":"http://arxiv.org/abs/2312.04553v1"}
{"created":"2023-12-07 18:59:20","title":"Generating Illustrated Instructions","abstract":"We introduce the new task of generating Illustrated Instructions, i.e., visual instructions customized to a user's needs. We identify desiderata unique to this task, and formalize it through a suite of automatic and human evaluation metrics, designed to measure the validity, consistency, and efficacy of the generations. We combine the power of large language models (LLMs) together with strong text-to-image generation diffusion models to propose a simple approach called StackedDiffusion, which generates such illustrated instructions given text as input. The resulting model strongly outperforms baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases, users even prefer it to human-generated articles. Most notably, it enables various new and exciting applications far beyond what static articles on the web can provide, such as personalized instructions complete with intermediate steps and pictures in response to a user's individual situation.","sentences":["We introduce the new task of generating Illustrated Instructions, i.e., visual instructions customized to a user's needs.","We identify desiderata unique to this task, and formalize it through a suite of automatic and human evaluation metrics, designed to measure the validity, consistency, and efficacy of the generations.","We combine the power of large language models (LLMs) together with strong text-to-image generation diffusion models to propose a simple approach called StackedDiffusion, which generates such illustrated instructions given text as input.","The resulting model strongly outperforms baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases, users even prefer it to human-generated articles.","Most notably, it enables various new and exciting applications far beyond what static articles on the web can provide, such as personalized instructions complete with intermediate steps and pictures in response to a user's individual situation."],"url":"http://arxiv.org/abs/2312.04552v1"}
{"created":"2023-12-07 18:59:18","title":"Free3D: Consistent Novel View Synthesis without 3D Representation","abstract":"We introduce Free3D, a simple approach designed for open-set novel view synthesis (NVS) from a single image. Similar to Zero-1-to-3, we start from a pre-trained 2D image generator for generalization, and fine-tune it for NVS. Compared to recent and concurrent works, we obtain significant improvements without resorting to an explicit 3D representation, which is slow and memory-consuming or training an additional 3D network. We do so by encoding better the target camera pose via a new per-pixel ray conditioning normalization (RCN) layer. The latter injects pose information in the underlying 2D image generator by telling each pixel its specific viewing direction. We also improve multi-view consistency via a light-weight multi-view attention layer and multi-view noise sharing. We train Free3D on the Objaverse dataset and demonstrate excellent generalization to various new categories in several new datasets, including OminiObject3D and GSO. We hope our simple and effective approach will serve as a solid baseline and help future research in NVS with more accuracy pose. The project page is available at https://chuanxiaz.com/free3d/.","sentences":["We introduce Free3D, a simple approach designed for open-set novel view synthesis (NVS) from a single image.","Similar to Zero-1-to-3, we start from a pre-trained 2D image generator for generalization, and fine-tune it for NVS.","Compared to recent and concurrent works, we obtain significant improvements without resorting to an explicit 3D representation, which is slow and memory-consuming or training an additional 3D network.","We do so by encoding better the target camera pose via a new per-pixel ray conditioning normalization (RCN) layer.","The latter injects pose information in the underlying 2D image generator by telling each pixel its specific viewing direction.","We also improve multi-view consistency via a light-weight multi-view attention layer and multi-view noise sharing.","We train Free3D on the Objaverse dataset and demonstrate excellent generalization to various new categories in several new datasets, including OminiObject3D and GSO.","We hope our simple and effective approach will serve as a solid baseline and help future research in NVS with more accuracy pose.","The project page is available at https://chuanxiaz.com/free3d/."],"url":"http://arxiv.org/abs/2312.04551v1"}
{"created":"2023-12-07 18:59:14","title":"Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?","abstract":"Despite the commercial abundance of UAVs, aerial data acquisition remains challenging, and the existing Asia and North America-centric open-source UAV datasets are small-scale or low-resolution and lack diversity in scene contextuality. Additionally, the color content of the scenes, solar-zenith angle, and population density of different geographies influence the data diversity. These two factors conjointly render suboptimal aerial-visual perception of the deep neural network (DNN) models trained primarily on the ground-view data, including the open-world foundational models.   To pave the way for a transformative era of aerial detection, we present Multiview Aerial Visual RECognition or MAVREC, a video dataset where we record synchronized scenes from different perspectives -- ground camera and drone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard 2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million annotated bounding boxes. This makes MAVREC the largest ground and aerial-view dataset, and the fourth largest among all drone-based datasets across all modalities and tasks. Through our extensive benchmarking on MAVREC, we recognize that augmenting object detectors with ground-view images from the corresponding geographical location is a superior pre-training strategy for aerial detection. Building on this strategy, we benchmark MAVREC with a curriculum-based semi-supervised object detection approach that leverages labeled (ground and aerial) and unlabeled (only aerial) images to enhance the aerial detection. We publicly release the MAVREC dataset: https://mavrec.github.io.","sentences":["Despite the commercial abundance of UAVs, aerial data acquisition remains challenging, and the existing Asia and North America-centric open-source UAV datasets are small-scale or low-resolution and lack diversity in scene contextuality.","Additionally, the color content of the scenes, solar-zenith angle, and population density of different geographies influence the data diversity.","These two factors conjointly render suboptimal aerial-visual perception of the deep neural network (DNN) models trained primarily on the ground-view data, including the open-world foundational models.   ","To pave the way for a transformative era of aerial detection, we present Multiview Aerial Visual RECognition or MAVREC, a video dataset where we record synchronized scenes from different perspectives -- ground camera and drone-mounted camera.","MAVREC consists of around 2.5 hours of industry-standard 2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million annotated bounding boxes.","This makes MAVREC the largest ground and aerial-view dataset, and the fourth largest among all drone-based datasets across all modalities and tasks.","Through our extensive benchmarking on MAVREC, we recognize that augmenting object detectors with ground-view images from the corresponding geographical location is a superior pre-training strategy for aerial detection.","Building on this strategy, we benchmark MAVREC with a curriculum-based semi-supervised object detection approach that leverages labeled (ground and aerial) and unlabeled (only aerial) images to enhance the aerial detection.","We publicly release the MAVREC dataset: https://mavrec.github.io."],"url":"http://arxiv.org/abs/2312.04548v1"}
{"created":"2023-12-07 18:59:14","title":"PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play","abstract":"Learning from unstructured and uncurated data has become the dominant paradigm for generative approaches in language and vision. Such unstructured and unguided behavior data, commonly known as play, is also easier to collect in robotics but much more difficult to learn from due to its inherently multimodal, noisy, and suboptimal nature. In this paper, we study this problem of learning goal-directed skill policies from unstructured play data which is labeled with language in hindsight. Specifically, we leverage advances in diffusion models to learn a multi-task diffusion model to extract robotic skills from play data. Using a conditional denoising diffusion process in the space of states and actions, we can gracefully handle the complexity and multimodality of play data and generate diverse and interesting robot behaviors. To make diffusion models more useful for skill learning, we encourage robotic agents to acquire a vocabulary of skills by introducing discrete bottlenecks into the conditional behavior generation process. In our experiments, we demonstrate the effectiveness of our approach across a wide variety of environments in both simulation and the real world. Results visualizations and videos at https://play-fusion.github.io","sentences":["Learning from unstructured and uncurated data has become the dominant paradigm for generative approaches in language and vision.","Such unstructured and unguided behavior data, commonly known as play, is also easier to collect in robotics but much more difficult to learn from due to its inherently multimodal, noisy, and suboptimal nature.","In this paper, we study this problem of learning goal-directed skill policies from unstructured play data which is labeled with language in hindsight.","Specifically, we leverage advances in diffusion models to learn a multi-task diffusion model to extract robotic skills from play data.","Using a conditional denoising diffusion process in the space of states and actions, we can gracefully handle the complexity and multimodality of play data and generate diverse and interesting robot behaviors.","To make diffusion models more useful for skill learning, we encourage robotic agents to acquire a vocabulary of skills by introducing discrete bottlenecks into the conditional behavior generation process.","In our experiments, we demonstrate the effectiveness of our approach across a wide variety of environments in both simulation and the real world.","Results visualizations and videos at https://play-fusion.github.io"],"url":"http://arxiv.org/abs/2312.04549v1"}
{"created":"2023-12-07 18:58:59","title":"Digital Life Project: Autonomous 3D Characters with Social Intelligence","abstract":"In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: https://digital-life-project.com/","sentences":["In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment.","Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body.","It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity.","Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain.","Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states.","Concurrently, these characters can perform contextually relevant bodily movements.","Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions.","Homepage: https://digital-life-project.com/"],"url":"http://arxiv.org/abs/2312.04547v1"}
{"created":"2023-12-07 18:58:40","title":"Adversarial Learning for Feature Shift Detection and Correction","abstract":"Data shift is a phenomenon present in many real-world applications, and while there are multiple methods attempting to detect shifts, the task of localizing and correcting the features originating such shifts has not been studied in depth. Feature shifts can occur in many datasets, including in multi-sensor data, where some sensors are malfunctioning, or in tabular and structured data, including biomedical, financial, and survey data, where faulty standardization and data processing pipelines can lead to erroneous features. In this work, we explore using the principles of adversarial learning, where the information from several discriminators trained to distinguish between two distributions is used to both detect the corrupted features and fix them in order to remove the distribution shift between datasets. We show that mainstream supervised classifiers, such as random forest or gradient boosting trees, combined with simple iterative heuristics, can localize and correct feature shifts, outperforming current statistical and neural network-based techniques. The code is available at https://github.com/AI-sandbox/DataFix.","sentences":["Data shift is a phenomenon present in many real-world applications, and while there are multiple methods attempting to detect shifts, the task of localizing and correcting the features originating such shifts has not been studied in depth.","Feature shifts can occur in many datasets, including in multi-sensor data, where some sensors are malfunctioning, or in tabular and structured data, including biomedical, financial, and survey data, where faulty standardization and data processing pipelines can lead to erroneous features.","In this work, we explore using the principles of adversarial learning, where the information from several discriminators trained to distinguish between two distributions is used to both detect the corrupted features and fix them in order to remove the distribution shift between datasets.","We show that mainstream supervised classifiers, such as random forest or gradient boosting trees, combined with simple iterative heuristics, can localize and correct feature shifts, outperforming current statistical and neural network-based techniques.","The code is available at https://github.com/AI-sandbox/DataFix."],"url":"http://arxiv.org/abs/2312.04546v1"}
{"created":"2023-12-07 18:58:09","title":"HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image","abstract":"3D content creation from a single image is a long-standing yet highly desirable task. Recent advances introduce 2D diffusion priors, yielding reasonable results. However, existing methods are not hyper-realistic enough for post-generation usage, as users cannot view, render and edit the resulting 3D content from a full range. To address these challenges, we introduce HyperDreamer with several key designs and appealing properties: 1) Viewable: 360 degree mesh modeling with high-resolution textures enables the creation of visually compelling 3D models from a full range of observation points. 2) Renderable: Fine-grained semantic segmentation and data-driven priors are incorporated as guidance to learn reasonable albedo, roughness, and specular properties of the materials, enabling semantic-aware arbitrary material estimation. 3) Editable: For a generated model or their own data, users can interactively select any region via a few clicks and efficiently edit the texture with text-based guidance. Extensive experiments demonstrate the effectiveness of HyperDreamer in modeling region-aware materials with high-resolution textures and enabling user-friendly editing. We believe that HyperDreamer holds promise for advancing 3D content creation and finding applications in various domains.","sentences":["3D content creation from a single image is a long-standing yet highly desirable task.","Recent advances introduce 2D diffusion priors, yielding reasonable results.","However, existing methods are not hyper-realistic enough for post-generation usage, as users cannot view, render and edit the resulting 3D content from a full range.","To address these challenges, we introduce HyperDreamer with several key designs and appealing properties: 1) Viewable: 360 degree mesh modeling with high-resolution textures enables the creation of visually compelling 3D models from a full range of observation points.","2) Renderable: Fine-grained semantic segmentation and data-driven priors are incorporated as guidance to learn reasonable albedo, roughness, and specular properties of the materials, enabling semantic-aware arbitrary material estimation.","3) Editable: For a generated model or their own data, users can interactively select any region via a few clicks and efficiently edit the texture with text-based guidance.","Extensive experiments demonstrate the effectiveness of HyperDreamer in modeling region-aware materials with high-resolution textures and enabling user-friendly editing.","We believe that HyperDreamer holds promise for advancing 3D content creation and finding applications in various domains."],"url":"http://arxiv.org/abs/2312.04543v1"}
{"created":"2023-12-07 18:57:36","title":"SoK: Unintended Interactions among Machine Learning Defenses and Risks","abstract":"Machine learning (ML) models cannot neglect risks to security, privacy, and fairness. Several defenses have been proposed to mitigate such risks. When a defense is effective in mitigating one risk, it may correspond to increased or decreased susceptibility to other risks. Existing research lacks an effective framework to recognize and explain these unintended interactions. We present such a framework, based on the conjecture that overfitting and memorization underlie unintended interactions. We survey existing literature on unintended interactions, accommodating them within our framework. We use our framework to conjecture on two previously unexplored interactions, and empirically validate our conjectures.","sentences":["Machine learning (ML) models cannot neglect risks to security, privacy, and fairness.","Several defenses have been proposed to mitigate such risks.","When a defense is effective in mitigating one risk, it may correspond to increased or decreased susceptibility to other risks.","Existing research lacks an effective framework to recognize and explain these unintended interactions.","We present such a framework, based on the conjecture that overfitting and memorization underlie unintended interactions.","We survey existing literature on unintended interactions, accommodating them within our framework.","We use our framework to conjecture on two previously unexplored interactions, and empirically validate our conjectures."],"url":"http://arxiv.org/abs/2312.04542v1"}
{"created":"2023-12-07 18:57:03","title":"Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations","abstract":"Modeling spatial-temporal interactions among neighboring agents is at the heart of multi-agent problems such as motion forecasting and crowd navigation. Despite notable progress, it remains unclear to which extent modern representations can capture the causal relationships behind agent interactions. In this work, we take an in-depth look at the causal awareness of these representations, from computational formalism to real-world practice. First, we cast doubt on the notion of non-causal robustness studied in the recent CausalAgents benchmark. We show that recent representations are already partially resilient to perturbations of non-causal agents, and yet modeling indirect causal effects involving mediator agents remains challenging. To address this challenge, we introduce a metric learning approach that regularizes latent representations with causal annotations. Our controlled experiments show that this approach not only leads to higher degrees of causal awareness but also yields stronger out-of-distribution robustness. To further operationalize it in practice, we propose a sim-to-real causal transfer method via cross-domain multi-task learning. Experiments on pedestrian datasets show that our method can substantially boost generalization, even in the absence of real-world causal annotations. We hope our work provides a new perspective on the challenges and potential pathways towards causally-aware representations of multi-agent interactions. Our code is available at https://github.com/socialcausality.","sentences":["Modeling spatial-temporal interactions among neighboring agents is at the heart of multi-agent problems such as motion forecasting and crowd navigation.","Despite notable progress, it remains unclear to which extent modern representations can capture the causal relationships behind agent interactions.","In this work, we take an in-depth look at the causal awareness of these representations, from computational formalism to real-world practice.","First, we cast doubt on the notion of non-causal robustness studied in the recent CausalAgents benchmark.","We show that recent representations are already partially resilient to perturbations of non-causal agents, and yet modeling indirect causal effects involving mediator agents remains challenging.","To address this challenge, we introduce a metric learning approach that regularizes latent representations with causal annotations.","Our controlled experiments show that this approach not only leads to higher degrees of causal awareness but also yields stronger out-of-distribution robustness.","To further operationalize it in practice, we propose a sim-to-real causal transfer method via cross-domain multi-task learning.","Experiments on pedestrian datasets show that our method can substantially boost generalization, even in the absence of real-world causal annotations.","We hope our work provides a new perspective on the challenges and potential pathways towards causally-aware representations of multi-agent interactions.","Our code is available at https://github.com/socialcausality."],"url":"http://arxiv.org/abs/2312.04540v1"}
{"created":"2023-12-07 18:55:52","title":"Self-Guided Open-Vocabulary Semantic Segmentation","abstract":"Vision-Language Models (VLMs) have emerged as promising tools for open-ended image understanding tasks, including open vocabulary segmentation. Yet, direct application of such VLMs to segmentation is non-trivial, since VLMs are trained with image-text pairs and naturally lack pixel-level granularity. Recent works have made advancements in bridging this gap, often by leveraging the shared image-text space in which the image and a provided text prompt are represented. In this paper, we challenge the capabilities of VLMs further and tackle open-vocabulary segmentation without the need for any textual input. To this end, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework. Self-Seg is capable of automatically detecting relevant class names from clustered BLIP embeddings and using these for accurate semantic segmentation. In addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) to effectively assess predicted open-vocabulary class names. We achieve state-of-the-art results on Pascal VOC, ADE20K and CityScapes for open-vocabulary segmentation without given class names, as well as competitive performance with methods where class names are given. All code and data will be released.","sentences":["Vision-Language Models (VLMs) have emerged as promising tools for open-ended image understanding tasks, including open vocabulary segmentation.","Yet, direct application of such VLMs to segmentation is non-trivial, since VLMs are trained with image-text pairs and naturally lack pixel-level granularity.","Recent works have made advancements in bridging this gap, often by leveraging the shared image-text space in which the image and a provided text prompt are represented.","In this paper, we challenge the capabilities of VLMs further and tackle open-vocabulary segmentation without the need for any textual input.","To this end, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework.","Self-Seg is capable of automatically detecting relevant class names from clustered BLIP embeddings and using these for accurate semantic segmentation.","In addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) to effectively assess predicted open-vocabulary class names.","We achieve state-of-the-art results on Pascal VOC, ADE20K and CityScapes for open-vocabulary segmentation without given class names, as well as competitive performance with methods where class names are given.","All code and data will be released."],"url":"http://arxiv.org/abs/2312.04539v1"}
{"created":"2023-12-07 18:53:27","title":"Trajeglish: Learning the Language of Driving Scenarios","abstract":"A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.","sentences":["A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs.","In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios.","Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary.","We then model the multi-agent sequence of motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents.","Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%.","We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes.","We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task."],"url":"http://arxiv.org/abs/2312.04535v1"}
{"created":"2023-12-07 18:53:18","title":"PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns","abstract":"In this paper, we propose a novel virtual try-on from unconstrained designs (ucVTON) task to enable photorealistic synthesis of personalized composite clothing on input human images. Unlike prior arts constrained by specific input types, our method allows flexible specification of style (text or image) and texture (full garment, cropped sections, or texture patches) conditions. To address the entanglement challenge when using full garment images as conditions, we develop a two-stage pipeline with explicit disentanglement of style and texture. In the first stage, we generate a human parsing map reflecting the desired style conditioned on the input. In the second stage, we composite textures onto the parsing map areas based on the texture input. To represent complex and non-stationary textures that have never been achieved in previous fashion editing works, we first propose extracting hierarchical and balanced CLIP features and applying position encoding in VTON. Experiments demonstrate superior synthesis quality and personalization enabled by our method. The flexible control over style and texture mixing brings virtual try-on to a new level of user experience for online shopping and fashion design.","sentences":["In this paper, we propose a novel virtual try-on from unconstrained designs (ucVTON) task to enable photorealistic synthesis of personalized composite clothing on input human images.","Unlike prior arts constrained by specific input types, our method allows flexible specification of style (text or image) and texture (full garment, cropped sections, or texture patches) conditions.","To address the entanglement challenge when using full garment images as conditions, we develop a two-stage pipeline with explicit disentanglement of style and texture.","In the first stage, we generate a human parsing map reflecting the desired style conditioned on the input.","In the second stage, we composite textures onto the parsing map areas based on the texture input.","To represent complex and non-stationary textures that have never been achieved in previous fashion editing works, we first propose extracting hierarchical and balanced CLIP features and applying position encoding in VTON.","Experiments demonstrate superior synthesis quality and personalization enabled by our method.","The flexible control over style and texture mixing brings virtual try-on to a new level of user experience for online shopping and fashion design."],"url":"http://arxiv.org/abs/2312.04534v1"}
{"created":"2023-12-07 18:51:19","title":"Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models","abstract":"We introduce Dream2Real, a robotics framework which integrates vision-language models (VLMs) trained on 2D data into a 3D object rearrangement pipeline. This is achieved by the robot autonomously constructing a 3D representation of the scene, where objects can be rearranged virtually and an image of the resulting arrangement rendered. These renders are evaluated by a VLM, so that the arrangement which best satisfies the user instruction is selected and recreated in the real world with pick-and-place. This enables language-conditioned rearrangement to be performed zero-shot, without needing to collect a training dataset of example arrangements. Results on a series of real-world tasks show that this framework is robust to distractors, controllable by language, capable of understanding complex multi-object relations, and readily applicable to both tabletop and 6-DoF rearrangement tasks.","sentences":["We introduce Dream2Real, a robotics framework which integrates vision-language models (VLMs) trained on 2D data into a 3D object rearrangement pipeline.","This is achieved by the robot autonomously constructing a 3D representation of the scene, where objects can be rearranged virtually and an image of the resulting arrangement rendered.","These renders are evaluated by a VLM, so that the arrangement which best satisfies the user instruction is selected and recreated in the real world with pick-and-place.","This enables language-conditioned rearrangement to be performed zero-shot, without needing to collect a training dataset of example arrangements.","Results on a series of real-world tasks show that this framework is robust to distractors, controllable by language, capable of understanding complex multi-object relations, and readily applicable to both tabletop and 6-DoF rearrangement tasks."],"url":"http://arxiv.org/abs/2312.04533v1"}
{"created":"2023-12-07 18:50:01","title":"Camera Height Doesn't Change: Unsupervised Monocular Scale-Aware Road-Scene Depth Estimation","abstract":"Monocular depth estimators either require explicit scale supervision through auxiliary sensors or suffer from scale ambiguity, which renders them difficult to deploy in downstream applications. A possible source of scale is the sizes of objects found in the scene, but inaccurate localization makes them difficult to exploit. In this paper, we introduce a novel scale-aware monocular depth estimation method called StableCamH that does not require any auxiliary sensor or supervision. The key idea is to exploit prior knowledge of object heights in the scene but aggregate the height cues into a single invariant measure common to all frames in a road video sequence, namely the camera height. By formulating monocular depth estimation as camera height optimization, we achieve robust and accurate unsupervised end-to-end training. To realize StableCamH, we devise a novel learning-based size prior that can directly convert car appearance into its dimensions. Extensive experiments on KITTI and Cityscapes show the effectiveness of StableCamH, its state-of-the-art accuracy compared with related methods, and its generalizability. The training framework of StableCamH can be used for any monocular depth estimation method and will hopefully become a fundamental building block for further work.","sentences":["Monocular depth estimators either require explicit scale supervision through auxiliary sensors or suffer from scale ambiguity, which renders them difficult to deploy in downstream applications.","A possible source of scale is the sizes of objects found in the scene, but inaccurate localization makes them difficult to exploit.","In this paper, we introduce a novel scale-aware monocular depth estimation method called StableCamH that does not require any auxiliary sensor or supervision.","The key idea is to exploit prior knowledge of object heights in the scene but aggregate the height cues into a single invariant measure common to all frames in a road video sequence, namely the camera height.","By formulating monocular depth estimation as camera height optimization, we achieve robust and accurate unsupervised end-to-end training.","To realize StableCamH, we devise a novel learning-based size prior that can directly convert car appearance into its dimensions.","Extensive experiments on KITTI and Cityscapes show the effectiveness of StableCamH, its state-of-the-art accuracy compared with related methods, and its generalizability.","The training framework of StableCamH can be used for any monocular depth estimation method and will hopefully become a fundamental building block for further work."],"url":"http://arxiv.org/abs/2312.04530v1"}
{"created":"2023-12-07 18:50:00","title":"Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of Illumination and Reflectance","abstract":"Reflectance bounds the frequency spectrum of illumination in the object appearance. In this paper, we introduce the first stochastic inverse rendering method, which recovers the full frequency spectrum of an illumination jointly with the object reflectance from a single image. Our key idea is to solve this blind inverse problem in the reflectance map, an appearance representation invariant to the underlying geometry, by learning to reverse the image formation with a novel diffusion model which we refer to as the Diffusion Reflectance Map Network (DRMNet). Given an observed reflectance map converted and completed from the single input image, DRMNet generates a reflectance map corresponding to a perfect mirror sphere while jointly estimating the reflectance. The forward process can be understood as gradually filtering a natural illumination with lower and lower frequency reflectance and additive Gaussian noise. DRMNet learns to invert this process with two subnetworks, IllNet and RefNet, which work in concert towards this joint estimation. The network is trained on an extensive synthetic dataset and is demonstrated to generalize to real images, showing state-of-the-art accuracy on established datasets.","sentences":["Reflectance bounds the frequency spectrum of illumination in the object appearance.","In this paper, we introduce the first stochastic inverse rendering method, which recovers the full frequency spectrum of an illumination jointly with the object reflectance from a single image.","Our key idea is to solve this blind inverse problem in the reflectance map, an appearance representation invariant to the underlying geometry, by learning to reverse the image formation with a novel diffusion model which we refer to as the Diffusion Reflectance Map Network (DRMNet).","Given an observed reflectance map converted and completed from the single input image, DRMNet generates a reflectance map corresponding to a perfect mirror sphere while jointly estimating the reflectance.","The forward process can be understood as gradually filtering a natural illumination with lower and lower frequency reflectance and additive Gaussian noise.","DRMNet learns to invert this process with two subnetworks, IllNet and RefNet, which work in concert towards this joint estimation.","The network is trained on an extensive synthetic dataset and is demonstrated to generalize to real images, showing state-of-the-art accuracy on established datasets."],"url":"http://arxiv.org/abs/2312.04529v1"}
{"created":"2023-12-07 18:46:50","title":"Using Large Language Models for Hyperparameter Optimization","abstract":"This paper studies using foundational large language models (LLMs) to make decisions during hyperparameter optimization (HPO). Empirical evaluations demonstrate that in settings with constrained search budgets, LLMs can perform comparably or better than traditional HPO methods like random search and Bayesian optimization on standard benchmarks. Furthermore, we propose to treat the code specifying our model as a hyperparameter, which the LLM outputs, going beyond the capabilities of existing HPO approaches. Our findings suggest that LLMs are a promising tool for improving efficiency in the traditional decision-making problem of hyperparameter optimization.","sentences":["This paper studies using foundational large language models (LLMs) to make decisions during hyperparameter optimization (HPO).","Empirical evaluations demonstrate that in settings with constrained search budgets, LLMs can perform comparably or better than traditional HPO methods like random search and Bayesian optimization on standard benchmarks.","Furthermore, we propose to treat the code specifying our model as a hyperparameter, which the LLM outputs, going beyond the capabilities of existing HPO approaches.","Our findings suggest that LLMs are a promising tool for improving efficiency in the traditional decision-making problem of hyperparameter optimization."],"url":"http://arxiv.org/abs/2312.04528v1"}
{"created":"2023-12-07 18:46:47","title":"Correspondences of the Third Kind: Camera Pose Estimation from Object Reflection","abstract":"Computer vision has long relied on two kinds of correspondences: pixel correspondences in images and 3D correspondences on object surfaces. Is there another kind, and if there is, what can they do for us? In this paper, we introduce correspondences of the third kind we call reflection correspondences and show that they can help estimate camera pose by just looking at objects without relying on the background. Reflection correspondences are point correspondences in the reflected world, i.e., the scene reflected by the object surface. The object geometry and reflectance alters the scene geometrically and radiometrically, respectively, causing incorrect pixel correspondences. Geometry recovered from each image is also hampered by distortions, namely generalized bas-relief ambiguity, leading to erroneous 3D correspondences. We show that reflection correspondences can resolve the ambiguities arising from these distortions. We introduce a neural correspondence estimator and a RANSAC algorithm that fully leverages all three kinds of correspondences for robust and accurate joint camera pose and object shape estimation just from the object appearance. The method expands the horizon of numerous downstream tasks, including camera pose estimation for appearance modeling (e.g., NeRF) and motion estimation of reflective objects (e.g., cars on the road), to name a few, as it relieves the requirement of overlapping background.","sentences":["Computer vision has long relied on two kinds of correspondences: pixel correspondences in images and 3D correspondences on object surfaces.","Is there another kind, and if there is, what can they do for us?","In this paper, we introduce correspondences of the third kind we call reflection correspondences and show that they can help estimate camera pose by just looking at objects without relying on the background.","Reflection correspondences are point correspondences in the reflected world, i.e., the scene reflected by the object surface.","The object geometry and reflectance alters the scene geometrically and radiometrically, respectively, causing incorrect pixel correspondences.","Geometry recovered from each image is also hampered by distortions, namely generalized bas-relief ambiguity, leading to erroneous 3D correspondences.","We show that reflection correspondences can resolve the ambiguities arising from these distortions.","We introduce a neural correspondence estimator and a RANSAC algorithm that fully leverages all three kinds of correspondences for robust and accurate joint camera pose and object shape estimation just from the object appearance.","The method expands the horizon of numerous downstream tasks, including camera pose estimation for appearance modeling (e.g., NeRF) and motion estimation of reflective objects (e.g., cars on the road), to name a few, as it relieves the requirement of overlapping background."],"url":"http://arxiv.org/abs/2312.04527v1"}
{"created":"2023-12-07 18:45:16","title":"Algorithms for the Global Domination Problem","abstract":"A dominating set D in a graph G is a subset of its vertices such that every vertex of the graph which does not belong to set D is adjacent to at least one vertex from set D. A set of vertices of graph G is a global dominating set if it is a dominating set for both, graph G and its complement. The objective is to find a global dominating set with the minimum cardinality. The problem is known to be NP-hard. Neither exact nor approximation algorithm existed . We propose two exact solution methods, one of them being based on an integer linear program (ILP) formulation, three heuristic algorithms and a special purification procedure that further reduces the size of a global dominated set delivered by any of our heuristic algorithms. We show that the problem remains NP-hard for restricted types of graphs and specify some families of graphs for which the heuristics guarantee the optimality. The second exact algorithm turned out to be about twice faster than ILP for graphs with more than 230 vertices and up to 1080 vertices, which were the largest benchmark instances that were solved optimally. The heuristics were tested for the existing 2284 benchmark problem instances with up to 14000 vertices and delivered solutions for the largest instances in less than one minute. Remarkably, for about 52% of the 1000 instances with the obtained optimal solutions, at least one of the heuristics generated an optimal solution, where the average approximation error for the remaining instances was 1.07%.","sentences":["A dominating set D in a graph G is a subset of its vertices such that every vertex of the graph which does not belong to set D is adjacent to at least one vertex from set D. A set of vertices of graph G is a global dominating set if it is a dominating set for both, graph G and its complement.","The objective is to find a global dominating set with the minimum cardinality.","The problem is known to be NP-hard.","Neither exact nor approximation algorithm existed .","We propose two exact solution methods, one of them being based on an integer linear program (ILP) formulation, three heuristic algorithms and a special purification procedure that further reduces the size of a global dominated set delivered by any of our heuristic algorithms.","We show that the problem remains NP-hard for restricted types of graphs and specify some families of graphs for which the heuristics guarantee the optimality.","The second exact algorithm turned out to be about twice faster than ILP for graphs with more than 230 vertices and up to 1080 vertices, which were the largest benchmark instances that were solved optimally.","The heuristics were tested for the existing 2284 benchmark problem instances with up to 14000 vertices and delivered solutions for the largest instances in less than one minute.","Remarkably, for about 52% of the 1000 instances with the obtained optimal solutions, at least one of the heuristics generated an optimal solution, where the average approximation error for the remaining instances was 1.07%."],"url":"http://arxiv.org/abs/2312.04526v1"}
{"created":"2023-12-07 18:43:45","title":"RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models","abstract":"Recent advancements in diffusion-based models have demonstrated significant success in generating images from text. However, video editing models have not yet reached the same level of visual quality and user control. To address this, we introduce RAVE, a zero-shot video editing method that leverages pre-trained text-to-image diffusion models without additional training. RAVE takes an input video and a text prompt to produce high-quality videos while preserving the original motion and semantic structure. It employs a novel noise shuffling strategy, leveraging spatio-temporal interactions between frames, to produce temporally consistent videos faster than existing methods. It is also efficient in terms of memory requirements, allowing it to handle longer videos. RAVE is capable of a wide range of edits, from local attribute modifications to shape transformations. In order to demonstrate the versatility of RAVE, we create a comprehensive video evaluation dataset ranging from object-focused scenes to complex human activities like dancing and typing, and dynamic scenes featuring swimming fish and boats. Our qualitative and quantitative experiments highlight the effectiveness of RAVE in diverse video editing scenarios compared to existing methods. Our code, dataset and videos can be found in https://rave-video.github.io.","sentences":["Recent advancements in diffusion-based models have demonstrated significant success in generating images from text.","However, video editing models have not yet reached the same level of visual quality and user control.","To address this, we introduce RAVE, a zero-shot video editing method that leverages pre-trained text-to-image diffusion models without additional training.","RAVE takes an input video and a text prompt to produce high-quality videos while preserving the original motion and semantic structure.","It employs a novel noise shuffling strategy, leveraging spatio-temporal interactions between frames, to produce temporally consistent videos faster than existing methods.","It is also efficient in terms of memory requirements, allowing it to handle longer videos.","RAVE is capable of a wide range of edits, from local attribute modifications to shape transformations.","In order to demonstrate the versatility of RAVE, we create a comprehensive video evaluation dataset ranging from object-focused scenes to complex human activities like dancing and typing, and dynamic scenes featuring swimming fish and boats.","Our qualitative and quantitative experiments highlight the effectiveness of RAVE in diverse video editing scenarios compared to existing methods.","Our code, dataset and videos can be found in https://rave-video.github.io."],"url":"http://arxiv.org/abs/2312.04524v1"}
{"created":"2023-12-07 18:41:21","title":"Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping","abstract":"The paper explores the industrial multimodal Anomaly Detection (AD) task, which exploits point clouds and RGB images to localize anomalies. We introduce a novel light and fast framework that learns to map features from one modality to the other on nominal samples. At test time, anomalies are detected by pinpointing inconsistencies between observed and mapped features. Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. Moreover, we propose a layer-pruning technique to improve memory and time efficiency with a marginal sacrifice in performance.","sentences":["The paper explores the industrial multimodal Anomaly Detection (AD) task, which exploits point clouds and RGB images to localize anomalies.","We introduce a novel light and fast framework that learns to map features from one modality to the other on nominal samples.","At test time, anomalies are detected by pinpointing inconsistencies between observed and mapped features.","Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods.","Moreover, we propose a layer-pruning technique to improve memory and time efficiency with a marginal sacrifice in performance."],"url":"http://arxiv.org/abs/2312.04521v1"}
{"created":"2023-12-07 18:38:39","title":"Bootstrapping Autonomous Radars with Self-Supervised Learning","abstract":"The perception of autonomous vehicles using radars has attracted increased research interest due its ability to operate in fog and bad weather. However, training radar models is hindered by the cost and difficulty of annotating large-scale radar data. To overcome this bottleneck, we propose a self-supervised learning framework to leverage the large amount of unlabeled radar data to pre-train radar-only embeddings for self-driving perception tasks. The proposed method combines radar-to-radar and radar-to-vision contrastive losses to learn a general representation from unlabeled radar heatmaps paired with their corresponding camera images. When used for downstream object detection, we demonstrate that the proposed self-supervision framework can improve the accuracy of state-of-the-art supervised baselines by 5.8% in mAP.","sentences":["The perception of autonomous vehicles using radars has attracted increased research interest due its ability to operate in fog and bad weather.","However, training radar models is hindered by the cost and difficulty of annotating large-scale radar data.","To overcome this bottleneck, we propose a self-supervised learning framework to leverage the large amount of unlabeled radar data to pre-train radar-only embeddings for self-driving perception tasks.","The proposed method combines radar-to-radar and radar-to-vision contrastive losses to learn a general representation from unlabeled radar heatmaps paired with their corresponding camera images.","When used for downstream object detection, we demonstrate that the proposed self-supervision framework can improve the accuracy of state-of-the-art supervised baselines by 5.8% in mAP."],"url":"http://arxiv.org/abs/2312.04519v1"}
{"created":"2023-12-07 18:36:53","title":"Computing Perfect Bayesian Equilibria in Sequential Auctions","abstract":"We present a best-response based algorithm for computing verifiable $\\varepsilon$-perfect Bayesian equilibria for sequential auctions with combinatorial bidding spaces and incomplete information. Previous work has focused only on computing Bayes-Nash equilibria for static single-round auctions, which our work captures as a special case. Additionally, we prove an upper bound $\\varepsilon$ on the utility loss of our approximate equilibria and present an algorithm to efficiently compute $\\varepsilon$ based on the immediate loss at each subgame. We evaluate the performance of our algorithm by reproducing known results from several auctions previously introduced in the literature, including a model of combinatorial split-award auctions used in procurement.","sentences":["We present a best-response based algorithm for computing verifiable $\\varepsilon$-perfect Bayesian equilibria for sequential auctions with combinatorial bidding spaces and incomplete information.","Previous work has focused only on computing Bayes-Nash equilibria for static single-round auctions, which our work captures as a special case.","Additionally, we prove an upper bound $\\varepsilon$ on the utility loss of our approximate equilibria and present an algorithm to efficiently compute $\\varepsilon$ based on the immediate loss at each subgame.","We evaluate the performance of our algorithm by reproducing known results from several auctions previously introduced in the literature, including a model of combinatorial split-award auctions used in procurement."],"url":"http://arxiv.org/abs/2312.04516v1"}
{"created":"2023-12-07 18:34:57","title":"Efficient Monotonic Multihead Attention","abstract":"We introduce the Efficient Monotonic Multihead Attention (EMMA), a state-of-the-art simultaneous translation model with numerically-stable and unbiased monotonic alignment estimation. In addition, we present improved training and inference strategies, including simultaneous fine-tuning from an offline translation model and reduction of monotonic alignment variance. The experimental results demonstrate that the proposed model attains state-of-the-art performance in simultaneous speech-to-text translation on the Spanish and English translation task.","sentences":["We introduce the Efficient Monotonic Multihead Attention (EMMA), a state-of-the-art simultaneous translation model with numerically-stable and unbiased monotonic alignment estimation.","In addition, we present improved training and inference strategies, including simultaneous fine-tuning from an offline translation model and reduction of monotonic alignment variance.","The experimental results demonstrate that the proposed model attains state-of-the-art performance in simultaneous speech-to-text translation on the Spanish and English translation task."],"url":"http://arxiv.org/abs/2312.04515v1"}
{"created":"2023-12-07 18:34:25","title":"Channel Charting for Streaming CSI Data","abstract":"Channel charting (CC) applies dimensionality reduction to channel state information (CSI) data at the infrastructure basestation side with the goal of extracting pseudo-position information for each user. The self-supervised nature of CC enables predictive tasks that depend on user position without requiring any ground-truth position information. In this work, we focus on the practically relevant streaming CSI data scenario, in which CSI is constantly estimated. To deal with storage limitations, we develop a novel streaming CC architecture that maintains a small core CSI dataset from which the channel charts are learned. Curation of the core CSI dataset is achieved using a min-max-similarity criterion. Numerical validation with measured CSI data demonstrates that our method approaches the accuracy obtained from the complete CSI dataset while using only a fraction of CSI storage and avoiding catastrophic forgetting of old CSI data.","sentences":["Channel charting (CC) applies dimensionality reduction to channel state information (CSI) data at the infrastructure basestation side with the goal of extracting pseudo-position information for each user.","The self-supervised nature of CC enables predictive tasks that depend on user position without requiring any ground-truth position information.","In this work, we focus on the practically relevant streaming CSI data scenario, in which CSI is constantly estimated.","To deal with storage limitations, we develop a novel streaming CC architecture that maintains a small core CSI dataset from which the channel charts are learned.","Curation of the core CSI dataset is achieved using a min-max-similarity criterion.","Numerical validation with measured CSI data demonstrates that our method approaches the accuracy obtained from the complete CSI dataset while using only a fraction of CSI storage and avoiding catastrophic forgetting of old CSI data."],"url":"http://arxiv.org/abs/2312.04514v1"}
{"created":"2023-12-07 18:32:19","title":"MuFuzz: Sequence-Aware Mutation and Seed Mask Guidance for Blockchain Smart Contract Fuzzing","abstract":"As blockchain smart contracts become more widespread and carry more valuable digital assets, they become an increasingly attractive target for attackers. Over the past few years, smart contracts have been subject to a plethora of devastating attacks, resulting in billions of dollars in financial losses. There has been a notable surge of research interest in identifying defects in smart contracts. However, existing smart contract fuzzing tools are still unsatisfactory. They struggle to screen out meaningful transaction sequences and specify critical inputs for each transaction. As a result, they can only trigger a limited range of contract states, making it difficult to unveil complicated vulnerabilities hidden in the deep state space.   In this paper, we shed light on smart contract fuzzing by employing a sequence-aware mutation and seed mask guidance strategy. In particular, we first utilize data-flow-based feedback to determine transaction orders in a meaningful way and further introduce a sequence-aware mutation technique to explore deeper states. Thereafter, we design a mask-guided seed mutation strategy that biases the generated transaction inputs to hit target branches. In addition, we develop a dynamic-adaptive energy adjustment paradigm that balances the fuzzing resource allocation during a fuzzing campaign. We implement our designs into a new smart contract fuzzer named MuFuzz, and extensively evaluate it on three benchmarks. Empirical results demonstrate that MuFuzz outperforms existing tools in terms of both branch coverage and bug finding. Overall, MuFuzz achieves higher branch coverage than state-of-the-art fuzzers (up to 25%) and detects 30% more bugs than existing bug detectors.","sentences":["As blockchain smart contracts become more widespread and carry more valuable digital assets, they become an increasingly attractive target for attackers.","Over the past few years, smart contracts have been subject to a plethora of devastating attacks, resulting in billions of dollars in financial losses.","There has been a notable surge of research interest in identifying defects in smart contracts.","However, existing smart contract fuzzing tools are still unsatisfactory.","They struggle to screen out meaningful transaction sequences and specify critical inputs for each transaction.","As a result, they can only trigger a limited range of contract states, making it difficult to unveil complicated vulnerabilities hidden in the deep state space.   ","In this paper, we shed light on smart contract fuzzing by employing a sequence-aware mutation and seed mask guidance strategy.","In particular, we first utilize data-flow-based feedback to determine transaction orders in a meaningful way and further introduce a sequence-aware mutation technique to explore deeper states.","Thereafter, we design a mask-guided seed mutation strategy that biases the generated transaction inputs to hit target branches.","In addition, we develop a dynamic-adaptive energy adjustment paradigm that balances the fuzzing resource allocation during a fuzzing campaign.","We implement our designs into a new smart contract fuzzer named MuFuzz, and extensively evaluate it on three benchmarks.","Empirical results demonstrate that MuFuzz outperforms existing tools in terms of both branch coverage and bug finding.","Overall, MuFuzz achieves higher branch coverage than state-of-the-art fuzzers (up to 25%) and detects 30% more bugs than existing bug detectors."],"url":"http://arxiv.org/abs/2312.04512v1"}
{"created":"2023-12-07 18:32:04","title":"An LLM Compiler for Parallel Function Calling","abstract":"Large Language Models (LLMs) have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute function calls, using user-provided functions to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has expanded LLMs' scope to include multi-function calling, where LLMs are equipped with a variety of functions and select the proper functions based on the context. Multi-function calling abilities of LLMs have catalyzed LLM-based software development, allowing them to tackle more complex problems. However, current methods for multi-function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multi-function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution strategies and dependencies; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2. We have benchmarked LLMCompiler on a range of tasks including cases with non-trivial inter-dependency between function calls, as well as cases that require dynamic replanning based on intermediate results. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x latency gain over OpenAI's recent parallel function calling, while achieving similar accuracy.","sentences":["Large Language Models (LLMs) have shown remarkable results on various complex reasoning benchmarks.","The reasoning capabilities of LLMs enable them to execute function calls, using user-provided functions to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data.","This development has expanded LLMs' scope to include multi-function calling, where LLMs are equipped with a variety of functions and select the proper functions based on the context.","Multi-function calling abilities of LLMs have catalyzed LLM-based software development, allowing them to tackle more complex problems.","However, current methods for multi-function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior.","To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multi-function calling.","Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution strategies and dependencies; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel.","LLMCompiler automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2.","We have benchmarked LLMCompiler on a range of tasks including cases with non-trivial inter-dependency between function calls, as well as cases that require dynamic replanning based on intermediate results.","We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% as compared to ReAct.","Additionally, LLMCompiler achieves up to 1.35x latency gain over OpenAI's recent parallel function calling, while achieving similar accuracy."],"url":"http://arxiv.org/abs/2312.04511v1"}
{"created":"2023-12-07 18:30:15","title":"A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation","abstract":"Recent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators. However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference. Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling. In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model. Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required. We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques.","sentences":["Recent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators.","However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference.","Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling.","In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model.","Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required.","We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques."],"url":"http://arxiv.org/abs/2312.04510v1"}
{"created":"2023-12-07 18:24:19","title":"Coordination-free Decentralised Federated Learning on Complex Networks: Overcoming Heterogeneity","abstract":"Federated Learning (FL) is a well-known framework for successfully performing a learning task in an edge computing scenario where the devices involved have limited resources and incomplete data representation. The basic assumption of FL is that the devices communicate directly or indirectly with a parameter server that centrally coordinates the whole process, overcoming several challenges associated with it. However, in highly pervasive edge scenarios, the presence of a central controller that oversees the process cannot always be guaranteed, and the interactions (i.e., the connectivity graph) between devices might not be predetermined, resulting in a complex network structure. Moreover, the heterogeneity of data and devices further complicates the learning process. This poses new challenges from a learning standpoint that we address by proposing a communication-efficient Decentralised Federated Learning (DFL) algorithm able to cope with them. Our solution allows devices communicating only with their direct neighbours to train an accurate model, overcoming the heterogeneity induced by data and different training histories. Our results show that the resulting local models generalise better than those trained with competing approaches, and do so in a more communication-efficient way.","sentences":["Federated Learning (FL) is a well-known framework for successfully performing a learning task in an edge computing scenario where the devices involved have limited resources and incomplete data representation.","The basic assumption of FL is that the devices communicate directly or indirectly with a parameter server that centrally coordinates the whole process, overcoming several challenges associated with it.","However, in highly pervasive edge scenarios, the presence of a central controller that oversees the process cannot always be guaranteed, and the interactions (i.e., the connectivity graph) between devices might not be predetermined, resulting in a complex network structure.","Moreover, the heterogeneity of data and devices further complicates the learning process.","This poses new challenges from a learning standpoint that we address by proposing a communication-efficient Decentralised Federated Learning (DFL) algorithm able to cope with them.","Our solution allows devices communicating only with their direct neighbours to train an accurate model, overcoming the heterogeneity induced by data and different training histories.","Our results show that the resulting local models generalise better than those trained with competing approaches, and do so in a more communication-efficient way."],"url":"http://arxiv.org/abs/2312.04504v1"}
{"created":"2023-12-07 18:21:52","title":"Graph Metanetworks for Processing Diverse Neural Architectures","abstract":"Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.","sentences":["Neural networks efficiently encode learned information within their parameters.","Consequently, many tasks can be unified by treating neural networks themselves as input data.","When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces.","However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging.","In this work, we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input.","Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks.","Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers.","We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged.","We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures."],"url":"http://arxiv.org/abs/2312.04501v1"}
{"created":"2023-12-07 18:13:42","title":"AVA: Towards Autonomous Visualization Agents through Visual Perception-Driven Decision-Making","abstract":"With recent advances in multi-modal foundation models, the previously text-only large language models (LLM) have evolved to incorporate visual input, opening up unprecedented opportunities for various applications in visualization. Our work explores the utilization of the visual perception ability of multi-modal LLMs to develop Autonomous Visualization Agents (AVAs) that can interpret and accomplish user-defined visualization objectives through natural language. We propose the first framework for the design of AVAs and present several usage scenarios intended to demonstrate the general applicability of the proposed paradigm. The addition of visual perception allows AVAs to act as the virtual visualization assistant for domain experts who may lack the knowledge or expertise in fine-tuning visualization outputs. Our preliminary exploration and proof-of-concept agents suggest that this approach can be widely applicable whenever the choices of appropriate visualization parameters require the interpretation of previous visual output. Feedback from unstructured interviews with experts in AI research, medical visualization, and radiology has been incorporated, highlighting the practicality and potential of AVAs. Our study indicates that AVAs represent a general paradigm for designing intelligent visualization systems that can achieve high-level visualization goals, which pave the way for developing expert-level visualization agents in the future.","sentences":["With recent advances in multi-modal foundation models, the previously text-only large language models (LLM) have evolved to incorporate visual input, opening up unprecedented opportunities for various applications in visualization.","Our work explores the utilization of the visual perception ability of multi-modal LLMs to develop Autonomous Visualization Agents (AVAs) that can interpret and accomplish user-defined visualization objectives through natural language.","We propose the first framework for the design of AVAs and present several usage scenarios intended to demonstrate the general applicability of the proposed paradigm.","The addition of visual perception allows AVAs to act as the virtual visualization assistant for domain experts who may lack the knowledge or expertise in fine-tuning visualization outputs.","Our preliminary exploration and proof-of-concept agents suggest that this approach can be widely applicable whenever the choices of appropriate visualization parameters require the interpretation of previous visual output.","Feedback from unstructured interviews with experts in AI research, medical visualization, and radiology has been incorporated, highlighting the practicality and potential of AVAs.","Our study indicates that AVAs represent a general paradigm for designing intelligent visualization systems that can achieve high-level visualization goals, which pave the way for developing expert-level visualization agents in the future."],"url":"http://arxiv.org/abs/2312.04494v1"}
{"created":"2023-12-07 18:02:48","title":"On The Maximum Linear Arrangement Problem for Trees","abstract":"Linear arrangements of graphs are a well-known type of graph labeling and are found at the heart of many important computational problems, such as the Minimum Linear Arrangement Problem (minLA). A linear arrangement is usually defined as a permutation of the $n$ vertices of a graph. An intuitive geometric setting is that of vertices lying on consecutive integer positions in the real line, starting at 1; edges are typically drawn as semicircles above the real line. In this paper we study the Maximum Linear Arrangement problem (MaxLA), the maximization variant of minLA and a less studied problem than minLA. We a devise new characterization of maximum arrangements of general graphs, and prove that MaxLA can be solved for cycle graphs in constant time, and for $k$-linear trees ($k\\le2$) in time $O(n)$. We present a simple algorithm that solves a constrained variant of MaxLA, which we call bipartite MaxLA, in time $O(n)$. This algorithm has two promising characteristics. First, it solves MaxLA for most trees consisting of a few tenths of nodes. Second, it produces a high quality approximation to MaxLA for trees where the algorithm fails to solve MaxLA. Furthermore, we conjecture this algorithm solves MaxLA for at least $50\\%$ of all free trees.","sentences":["Linear arrangements of graphs are a well-known type of graph labeling and are found at the heart of many important computational problems, such as the Minimum Linear Arrangement Problem (minLA).","A linear arrangement is usually defined as a permutation of the $n$ vertices of a graph.","An intuitive geometric setting is that of vertices lying on consecutive integer positions in the real line, starting at 1; edges are typically drawn as semicircles above the real line.","In this paper we study the Maximum Linear Arrangement problem (MaxLA), the maximization variant of minLA and a less studied problem than minLA.","We a devise new characterization of maximum arrangements of general graphs, and prove that MaxLA can be solved for cycle graphs in constant time, and for $k$-linear trees ($k\\le2$) in time $O(n)$. We present a simple algorithm that solves a constrained variant of MaxLA, which we call bipartite MaxLA, in time $O(n)$. This algorithm has two promising characteristics.","First, it solves MaxLA for most trees consisting of a few tenths of nodes.","Second, it produces a high quality approximation to MaxLA for trees where the algorithm fails to solve MaxLA.","Furthermore, we conjecture this algorithm solves MaxLA for at least $50\\%$ of all free trees."],"url":"http://arxiv.org/abs/2312.04487v1"}
{"created":"2023-12-07 17:59:53","title":"FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation","abstract":"LiDAR segmentation is crucial for autonomous driving systems. The recent range-view approaches are promising for real-time processing. However, they suffer inevitably from corrupted contextual information and rely heavily on post-processing techniques for prediction refinement. In this work, we propose a simple yet powerful FRNet that restores the contextual information of the range image pixels with corresponding frustum LiDAR points. Firstly, a frustum feature encoder module is used to extract per-point features within the frustum region, which preserves scene consistency and is crucial for point-level predictions. Next, a frustum-point fusion module is introduced to update per-point features hierarchically, which enables each point to extract more surrounding information via the frustum features. Finally, a head fusion module is used to fuse features at different levels for final semantic prediction. Extensive experiments on four popular LiDAR segmentation benchmarks under various task setups demonstrate our superiority. FRNet achieves competitive performance while maintaining high efficiency. The code is publicly available.","sentences":["LiDAR segmentation is crucial for autonomous driving systems.","The recent range-view approaches are promising for real-time processing.","However, they suffer inevitably from corrupted contextual information and rely heavily on post-processing techniques for prediction refinement.","In this work, we propose a simple yet powerful FRNet that restores the contextual information of the range image pixels with corresponding frustum LiDAR points.","Firstly, a frustum feature encoder module is used to extract per-point features within the frustum region, which preserves scene consistency and is crucial for point-level predictions.","Next, a frustum-point fusion module is introduced to update per-point features hierarchically, which enables each point to extract more surrounding information via the frustum features.","Finally, a head fusion module is used to fuse features at different levels for final semantic prediction.","Extensive experiments on four popular LiDAR segmentation benchmarks under various task setups demonstrate our superiority.","FRNet achieves competitive performance while maintaining high efficiency.","The code is publicly available."],"url":"http://arxiv.org/abs/2312.04484v1"}
{"created":"2023-12-07 17:59:07","title":"Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation","abstract":"Despite diffusion models having shown powerful abilities to generate photorealistic images, generating videos that are realistic and diverse still remains in its infancy. One of the key reasons is that current methods intertwine spatial content and temporal dynamics together, leading to a notably increased complexity of text-to-video generation (T2V). In this work, we propose HiGen, a diffusion model-based method that improves performance by decoupling the spatial and temporal factors of videos from two perspectives, i.e., structure level and content level. At the structure level, we decompose the T2V task into two steps, including spatial reasoning and temporal reasoning, using a unified denoiser. Specifically, we generate spatially coherent priors using text during spatial reasoning and then generate temporally coherent motions from these priors during temporal reasoning. At the content level, we extract two subtle cues from the content of the input video that can express motion and appearance changes, respectively. These two cues then guide the model's training for generating videos, enabling flexible content variations and enhancing temporal stability. Through the decoupled paradigm, HiGen can effectively reduce the complexity of this task and generate realistic videos with semantics accuracy and motion stability. Extensive experiments demonstrate the superior performance of HiGen over the state-of-the-art T2V methods.","sentences":["Despite diffusion models having shown powerful abilities to generate photorealistic images, generating videos that are realistic and diverse still remains in its infancy.","One of the key reasons is that current methods intertwine spatial content and temporal dynamics together, leading to a notably increased complexity of text-to-video generation (T2V).","In this work, we propose HiGen, a diffusion model-based method that improves performance by decoupling the spatial and temporal factors of videos from two perspectives, i.e., structure level and content level.","At the structure level, we decompose the T2V task into two steps, including spatial reasoning and temporal reasoning, using a unified denoiser.","Specifically, we generate spatially coherent priors using text during spatial reasoning and then generate temporally coherent motions from these priors during temporal reasoning.","At the content level, we extract two subtle cues from the content of the input video that can express motion and appearance changes, respectively.","These two cues then guide the model's training for generating videos, enabling flexible content variations and enhancing temporal stability.","Through the decoupled paradigm, HiGen can effectively reduce the complexity of this task and generate realistic videos with semantics accuracy and motion stability.","Extensive experiments demonstrate the superior performance of HiGen over the state-of-the-art T2V methods."],"url":"http://arxiv.org/abs/2312.04483v1"}
{"created":"2023-12-07 17:53:02","title":"GSGFormer: Generative Social Graph Transformer for Multimodal Pedestrian Trajectory Prediction","abstract":"Pedestrian trajectory prediction, vital for selfdriving cars and socially-aware robots, is complicated due to intricate interactions between pedestrians, their environment, and other Vulnerable Road Users. This paper presents GSGFormer, an innovative generative model adept at predicting pedestrian trajectories by considering these complex interactions and offering a plethora of potential modal behaviors. We incorporate a heterogeneous graph neural network to capture interactions between pedestrians, semantic maps, and potential destinations. The Transformer module extracts temporal features, while our novel CVAE-Residual-GMM module promotes diverse behavioral modality generation. Through evaluations on multiple public datasets, GSGFormer not only outperforms leading methods with ample data but also remains competitive when data is limited.","sentences":["Pedestrian trajectory prediction, vital for selfdriving cars and socially-aware robots, is complicated due to intricate interactions between pedestrians, their environment, and other Vulnerable Road Users.","This paper presents GSGFormer, an innovative generative model adept at predicting pedestrian trajectories by considering these complex interactions and offering a plethora of potential modal behaviors.","We incorporate a heterogeneous graph neural network to capture interactions between pedestrians, semantic maps, and potential destinations.","The Transformer module extracts temporal features, while our novel CVAE-Residual-GMM module promotes diverse behavioral modality generation.","Through evaluations on multiple public datasets, GSGFormer not only outperforms leading methods with ample data but also remains competitive when data is limited."],"url":"http://arxiv.org/abs/2312.04479v1"}
{"created":"2023-12-07 17:51:43","title":"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator","abstract":"Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter -- we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for linguistic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for \"detect_sarcasm(string)\" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they are used not only to write the code, but also to selectively \"emulate\" the interpreter by generating the expected output of \"detect_sarcasm(string)\" and other lines of code (e.g., that the interpreter could not compile). In this work, we propose Chain of Code (CoT), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format linguistic sub-tasks in a program as flexible pseudocode that the compiler can explicitly catch undefined behaviors and hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. CoT scales well with large and small models alike, and broadens the scope of reasoning questions that LMs can correctly answer by \"thinking in code\". Project webpage: https://chain-of-code.github.io/.","sentences":["Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter -- we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for linguistic ones (and in particular, those that are a mix of both).","For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for \"detect_sarcasm(string)\" that can be executed by the interpreter (handling the edge cases would be insurmountable).","However, LMs may still produce a valid solution if they are used not only to write the code, but also to selectively \"emulate\" the interpreter by generating the expected output of \"detect_sarcasm(string)\" and other lines of code (e.g., that the interpreter could not compile).","In this work, we propose Chain of Code (CoT), a simple yet surprisingly effective extension that improves LM code-driven reasoning.","The key idea is to encourage LMs to format linguistic sub-tasks in a program as flexible pseudocode that the compiler can explicitly catch undefined behaviors and hand off to simulate with an LM (as an \"LMulator\").","Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought.","CoT scales well with large and small models alike, and broadens the scope of reasoning questions that LMs can correctly answer by \"thinking in code\".","Project webpage: https://chain-of-code.github.io/."],"url":"http://arxiv.org/abs/2312.04474v1"}
{"created":"2023-12-07 17:42:04","title":"GaitGuard: Towards Private Gait in Mixed Reality","abstract":"Augmented/Mixed Reality (AR/MR) devices are unique from other mobile systems because of their capability to offer an immersive multi-user collaborative experience. While previous studies have explored privacy and security aspects of multiple user interactions in AR/MR, a less-explored area is the vulnerability of gait privacy. Gait is considered a private state because it is a highly individualistic and a distinctive biometric trait. Thus, preserving gait privacy in emerging AR/MR systems is crucial to safeguard individuals from potential identity tracking and unauthorized profiling.   This paper first introduces GaitExtract, a framework designed to automatically detect gait information in humans, shedding light on the nuances of gait privacy in AR/MR. In this paper, we designed GaitExtract, a framework that can automatically detect the outside gait information of a human and investigate the vulnerability of gait privacy in AR. In a user study with $20$ participants, our findings reveal that participants were uniquely identifiable with an accuracy of up to $78\\%$ using GaitExtract. Consequently, we propose GaitGuard, a system that safeguards gait information of people appearing in the camera view of the AR/MR device.   Furthermore, we tested GaitGuard in an MR collaborative application, achieving $22$ fps while streaming mitigated frames to the collaborative server. Our user-study survey indicated that users are more comfortable with releasing videos of them walking when GaitGuard is applied to the frames. These results underscore the efficacy and practicality of GaitGuard in mitigating gait privacy concerns in MR contexts.","sentences":["Augmented/Mixed Reality (AR/MR) devices are unique from other mobile systems because of their capability to offer an immersive multi-user collaborative experience.","While previous studies have explored privacy and security aspects of multiple user interactions in AR/MR, a less-explored area is the vulnerability of gait privacy.","Gait is considered a private state because it is a highly individualistic and a distinctive biometric trait.","Thus, preserving gait privacy in emerging AR/MR systems is crucial to safeguard individuals from potential identity tracking and unauthorized profiling.   ","This paper first introduces GaitExtract, a framework designed to automatically detect gait information in humans, shedding light on the nuances of gait privacy in AR/MR.","In this paper, we designed GaitExtract, a framework that can automatically detect the outside gait information of a human and investigate the vulnerability of gait privacy in AR.","In a user study with $20$ participants, our findings reveal that participants were uniquely identifiable with an accuracy of up to $78\\%$ using GaitExtract.","Consequently, we propose GaitGuard, a system that safeguards gait information of people appearing in the camera view of the AR/MR device.   ","Furthermore, we tested GaitGuard in an MR collaborative application, achieving $22$ fps while streaming mitigated frames to the collaborative server.","Our user-study survey indicated that users are more comfortable with releasing videos of them walking when GaitGuard is applied to the frames.","These results underscore the efficacy and practicality of GaitGuard in mitigating gait privacy concerns in MR contexts."],"url":"http://arxiv.org/abs/2312.04470v1"}
{"created":"2023-12-07 17:41:44","title":"On the Learnability of Watermarks for Language Models","abstract":"Watermarking of language model outputs enables statistical detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three distinct decoding-based watermarking strategies and various hyperparameter settings, finding that models can learn to generate watermarked text with high detectability. We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks.","sentences":["Watermarking of language model outputs enables statistical detection of model-generated text, which has many applications in the responsible deployment of language models.","Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks.","First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking.","Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text.","To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking.","We test our approach on three distinct decoding-based watermarking strategies and various hyperparameter settings, finding that models can learn to generate watermarked text with high detectability.","We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks."],"url":"http://arxiv.org/abs/2312.04469v1"}
{"created":"2023-12-07 17:39:25","title":"Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion","abstract":"Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content and better represent the emotion expressed by the input speech. Our project website is amuse.is.tue.mpg.de.","sentences":["Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures.","Instead, these methods directly output animations from speech without control over the expressed emotion.","To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion.","Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable.","To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style.","A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors.","Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence.","Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity.","Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences.","Compared to the state of the art, the generated gestures are better synchronized with the speech content and better represent the emotion expressed by the input speech.","Our project website is amuse.is.tue.mpg.de."],"url":"http://arxiv.org/abs/2312.04466v1"}
{"created":"2023-12-07 17:35:49","title":"FitDiff: Robust monocular 3D facial shape and reflectance estimation using Diffusion Models","abstract":"The remarkable progress in 3D face reconstruction has resulted in high-detail and photorealistic facial representations. Recently, Diffusion Models have revolutionized the capabilities of generative methods by achieving far better performance than GANs. In this work, we present FitDiff, a diffusion-based 3D facial avatar generative model. This model accurately generates relightable facial avatars, utilizing an identity embedding extracted from an \"in-the-wild\" 2D facial image. Our multi-modal diffusion model concurrently outputs facial reflectance maps (diffuse and specular albedo and normals) and shapes, showcasing great generalization capabilities. It is solely trained on an annotated subset of a public facial dataset, paired with 3D reconstructions. We revisit the typical 3D facial fitting approach by guiding a reverse diffusion process using perceptual and face recognition losses. Being the first LDM conditioned on face recognition embeddings, FitDiff reconstructs relightable human avatars, that can be used as-is in common rendering engines, starting only from an unconstrained facial image, and achieving state-of-the-art performance.","sentences":["The remarkable progress in 3D face reconstruction has resulted in high-detail and photorealistic facial representations.","Recently, Diffusion Models have revolutionized the capabilities of generative methods by achieving far better performance than GANs.","In this work, we present FitDiff, a diffusion-based 3D facial avatar generative model.","This model accurately generates relightable facial avatars, utilizing an identity embedding extracted from an \"in-the-wild\" 2D facial image.","Our multi-modal diffusion model concurrently outputs facial reflectance maps (diffuse and specular albedo and normals) and shapes, showcasing great generalization capabilities.","It is solely trained on an annotated subset of a public facial dataset, paired with 3D reconstructions.","We revisit the typical 3D facial fitting approach by guiding a reverse diffusion process using perceptual and face recognition losses.","Being the first LDM conditioned on face recognition embeddings, FitDiff reconstructs relightable human avatars, that can be used as-is in common rendering engines, starting only from an unconstrained facial image, and achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2312.04465v1"}
{"created":"2023-12-07 17:35:34","title":"Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement Learning with General Function Approximation","abstract":"To tackle long planning horizon problems in reinforcement learning with general function approximation, we propose the first algorithm, termed as UCRL-WVTR, that achieves both \\emph{horizon-free} and \\emph{instance-dependent}, since it eliminates the polynomial dependency on the planning horizon. The derived regret bound is deemed \\emph{sharp}, as it matches the minimax lower bound when specialized to linear mixture MDPs up to logarithmic factors. Furthermore, UCRL-WVTR is \\emph{computationally efficient} with access to a regression oracle. The achievement of such a horizon-free, instance-dependent, and sharp regret bound hinges upon (i) novel algorithm designs: weighted value-targeted regression and a high-order moment estimator in the context of general function approximation; and (ii) fine-grained analyses: a novel concentration bound of weighted non-linear least squares and a refined analysis which leads to the tight instance-dependent bound. We also conduct comprehensive experiments to corroborate our theoretical findings.","sentences":["To tackle long planning horizon problems in reinforcement learning with general function approximation, we propose the first algorithm, termed as UCRL-WVTR, that achieves both \\emph{horizon-free} and \\emph{instance-dependent}, since it eliminates the polynomial dependency on the planning horizon.","The derived regret bound is deemed \\emph{sharp}, as it matches the minimax lower bound when specialized to linear mixture MDPs up to logarithmic factors.","Furthermore, UCRL-WVTR is \\emph{computationally efficient} with access to a regression oracle.","The achievement of such a horizon-free, instance-dependent, and sharp regret bound hinges upon (i) novel algorithm designs: weighted value-targeted regression and a high-order moment estimator in the context of general function approximation; and (ii) fine-grained analyses: a novel concentration bound of weighted non-linear least squares and a refined analysis which leads to the tight instance-dependent bound.","We also conduct comprehensive experiments to corroborate our theoretical findings."],"url":"http://arxiv.org/abs/2312.04464v1"}
{"created":"2023-12-07 17:33:31","title":"Leveraging Transformer-based Language Models to Automate Requirements Satisfaction Assessment","abstract":"Requirements Satisfaction Assessment (RSA) evaluates whether the set of design elements linked to a single requirement provide sufficient coverage of that requirement -- typically meaning that all concepts in the requirement are addressed by at least one of the design elements. RSA is an important software engineering activity for systems with any form of hierarchical decomposition -- especially safety or mission critical ones. In previous studies, researchers used basic Information Retrieval (IR) models to decompose requirements and design elements into chunks, and then evaluated the extent to which chunks of design elements covered all chunks in the requirement. However, results had low accuracy because many critical concepts that extend across the entirety of the sentence were not well represented when the sentence was parsed into independent chunks. In this paper we leverage recent advances in natural language processing to deliver significantly more accurate results. We propose two major architectures: Satisfaction BERT (Sat-BERT), and Dual-Satisfaction BERT (DSat-BERT), along with their multitask learning variants to improve satisfaction assessments. We perform RSA on five different datasets and compare results from our variants against the chunk-based legacy approach. All BERT-based models significantly outperformed the legacy baseline, and Sat-BERT delivered the best results returning an average improvement of 124.75% in Mean Average Precision.","sentences":["Requirements Satisfaction Assessment (RSA) evaluates whether the set of design elements linked to a single requirement provide sufficient coverage of that requirement -- typically meaning that all concepts in the requirement are addressed by at least one of the design elements.","RSA is an important software engineering activity for systems with any form of hierarchical decomposition -- especially safety or mission critical ones.","In previous studies, researchers used basic Information Retrieval (IR) models to decompose requirements and design elements into chunks, and then evaluated the extent to which chunks of design elements covered all chunks in the requirement.","However, results had low accuracy because many critical concepts that extend across the entirety of the sentence were not well represented when the sentence was parsed into independent chunks.","In this paper we leverage recent advances in natural language processing to deliver significantly more accurate results.","We propose two major architectures: Satisfaction BERT (Sat-BERT), and Dual-Satisfaction BERT (DSat-BERT), along with their multitask learning variants to improve satisfaction assessments.","We perform RSA on five different datasets and compare results from our variants against the chunk-based legacy approach.","All BERT-based models significantly outperformed the legacy baseline, and Sat-BERT delivered the best results returning an average improvement of 124.75% in Mean Average Precision."],"url":"http://arxiv.org/abs/2312.04463v1"}
{"created":"2023-12-07 17:32:29","title":"PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding","abstract":"Recent advances in text-to-image generation have made remarkable progress in synthesizing realistic human photos conditioned on given text prompts. However, existing personalized generation methods cannot simultaneously satisfy the requirements of high efficiency, promising identity (ID) fidelity, and flexible text controllability. In this work, we introduce PhotoMaker, an efficient personalized text-to-image generation method, which mainly encodes an arbitrary number of input ID images into a stack ID embedding for preserving ID information. Such an embedding, serving as a unified ID representation, can not only encapsulate the characteristics of the same input ID comprehensively, but also accommodate the characteristics of different IDs for subsequent integration. This paves the way for more intriguing and practically valuable applications. Besides, to drive the training of our PhotoMaker, we propose an ID-oriented data construction pipeline to assemble the training data. Under the nourishment of the dataset constructed through the proposed pipeline, our PhotoMaker demonstrates better ID preservation ability than test-time fine-tuning based methods, yet provides significant speed improvements, high-quality generation results, strong generalization capabilities, and a wide range of applications. Our project page is available at https://photo-maker.github.io/","sentences":["Recent advances in text-to-image generation have made remarkable progress in synthesizing realistic human photos conditioned on given text prompts.","However, existing personalized generation methods cannot simultaneously satisfy the requirements of high efficiency, promising identity (ID) fidelity, and flexible text controllability.","In this work, we introduce PhotoMaker, an efficient personalized text-to-image generation method, which mainly encodes an arbitrary number of input ID images into a stack ID embedding for preserving ID information.","Such an embedding, serving as a unified ID representation, can not only encapsulate the characteristics of the same input ID comprehensively, but also accommodate the characteristics of different IDs for subsequent integration.","This paves the way for more intriguing and practically valuable applications.","Besides, to drive the training of our PhotoMaker, we propose an ID-oriented data construction pipeline to assemble the training data.","Under the nourishment of the dataset constructed through the proposed pipeline, our PhotoMaker demonstrates better ID preservation ability than test-time fine-tuning based methods, yet provides significant speed improvements, high-quality generation results, strong generalization capabilities, and a wide range of applications.","Our project page is available at https://photo-maker.github.io/"],"url":"http://arxiv.org/abs/2312.04461v1"}
{"created":"2023-12-07 17:24:51","title":"Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use","abstract":"Recent advancements in large language models (LLMs) have significantly expanded their functionality and skills as tool agents. In this paper, we argue that a waveform pattern in the model's attention allocation has an impact on the tool use performance, which degrades when the position of essential information hits the trough zone. To address this issue, we propose a novel inference method named Attention Buckets. This approach enables LLMs to handle context by conducting parallel processes, each featuring a unique RoPE angle base that shapes the attention waveform. Attention Buckets ensures that an attention trough of a particular process can be compensated with an attention peak of another run, reducing the risk of the LLM missing essential information residing within the attention trough. Our extensive experiments on the widely recognized tool use benchmark demonstrate the efficacy of our approach, where a 7B-parameter open-source model enhanced by Attention Buckets achieves SOTA performance on par with GPT-4.","sentences":["Recent advancements in large language models (LLMs) have significantly expanded their functionality and skills as tool agents.","In this paper, we argue that a waveform pattern in the model's attention allocation has an impact on the tool use performance, which degrades when the position of essential information hits the trough zone.","To address this issue, we propose a novel inference method named Attention Buckets.","This approach enables LLMs to handle context by conducting parallel processes, each featuring a unique RoPE angle base that shapes the attention waveform.","Attention Buckets ensures that an attention trough of a particular process can be compensated with an attention peak of another run, reducing the risk of the LLM missing essential information residing within the attention trough.","Our extensive experiments on the widely recognized tool use benchmark demonstrate the efficacy of our approach, where a 7B-parameter open-source model enhanced by Attention Buckets achieves SOTA performance on par with GPT-4."],"url":"http://arxiv.org/abs/2312.04455v1"}
{"created":"2023-12-07 17:17:52","title":"Climb Against Time -- Self-perspective through a psychological game","abstract":"With the rapid development of technology and its place in our lives, so too has the idea of needing to grow up faster, do more, be more and more as we are exposed to so many of our betters billboarding their successes and achievements that very often we can experience burnout, depression, feeling of inadequacy and worse. All because we cannot keep up with their tempos in life, and in this chaos, we often lose the very important fact and truth, our life should be lived at the tempo that fits our actual wants, our capabilities and opportunities. In recent years, since the mid 2010s, video games have entered the mainstream even more than before as a media platform that provides a more interactive experience than others like it. Where the players actions have consequences, outcomes both good and bad, and the experience of the player is highly linked to their capabilities. Based on the type of video game, be it single player or multiplayer, often the solution to the problem the player is facing will vary. With the increase popularity of both buying and creating games, more and more personal stories, talented teams and individuals, unique takes and ideas are being tried and often for the games that do succeed, there is financial gain but more impactful is communities built around the message and/or its execution. These communities usually reside on social media, such as Reddit, X, Tumblr, or on their own community pages made by the developers to directly interact with their players. But this is true often even for games that do not gain much financial success, they gain a certain cult following, especially if the topics of the game are either obscure or whose workings revolve around mental health issues, trauma survival, loss, or just in general very human and emotional topics.","sentences":["With the rapid development of technology and its place in our lives, so too has the idea of needing to grow up faster, do more, be more and more as we are exposed to so many of our betters billboarding their successes and achievements that very often we can experience burnout, depression, feeling of inadequacy and worse.","All because we cannot keep up with their tempos in life, and in this chaos, we often lose the very important fact and truth, our life should be lived at the tempo that fits our actual wants, our capabilities and opportunities.","In recent years, since the mid 2010s, video games have entered the mainstream even more than before as a media platform that provides a more interactive experience than others like it.","Where the players actions have consequences, outcomes both good and bad, and the experience of the player is highly linked to their capabilities.","Based on the type of video game, be it single player or multiplayer, often the solution to the problem the player is facing will vary.","With the increase popularity of both buying and creating games, more and more personal stories, talented teams and individuals, unique takes and ideas are being tried and often for the games that do succeed, there is financial gain but more impactful is communities built around the message and/or its execution.","These communities usually reside on social media, such as Reddit, X, Tumblr, or on their own community pages made by the developers to directly interact with their players.","But this is true often even for games that do not gain much financial success, they gain a certain cult following, especially if the topics of the game are either obscure or whose workings revolve around mental health issues, trauma survival, loss, or just in general very human and emotional topics."],"url":"http://arxiv.org/abs/2312.04449v1"}
{"created":"2023-12-07 17:06:20","title":"OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization","abstract":"The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document \\textit{open} aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.","sentences":["The performance of automatic summarization models has improved dramatically in recent years.","Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper.","Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data.","To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document \\textit{open} aspect-based summarization.","This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets.","We analyze the properties of OpenAsp showcasing its high-quality content.","Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models."],"url":"http://arxiv.org/abs/2312.04440v1"}
{"created":"2023-12-07 16:57:38","title":"Deep3DSketch: 3D modeling from Free-hand Sketches with View- and Structural-Aware Adversarial Training","abstract":"This work aims to investigate the problem of 3D modeling using single free-hand sketches, which is one of the most natural ways we humans express ideas. Although sketch-based 3D modeling can drastically make the 3D modeling process more accessible, the sparsity and ambiguity of sketches bring significant challenges for creating high-fidelity 3D models that reflect the creators' ideas. In this work, we propose a view- and structural-aware deep learning approach, \\textit{Deep3DSketch}, which tackles the ambiguity and fully uses sparse information of sketches, emphasizing the structural information. Specifically, we introduced random pose sampling on both 3D shapes and 2D silhouettes, and an adversarial training scheme with an effective progressive discriminator to facilitate learning of the shape structures. Extensive experiments demonstrated the effectiveness of our approach, which outperforms existing methods -- with state-of-the-art (SOTA) performance on both synthetic and real datasets.","sentences":["This work aims to investigate the problem of 3D modeling using single free-hand sketches, which is one of the most natural ways we humans express ideas.","Although sketch-based 3D modeling can drastically make the 3D modeling process more accessible, the sparsity and ambiguity of sketches bring significant challenges for creating high-fidelity 3D models that reflect the creators' ideas.","In this work, we propose a view- and structural-aware deep learning approach, \\textit{Deep3DSketch}, which tackles the ambiguity and fully uses sparse information of sketches, emphasizing the structural information.","Specifically, we introduced random pose sampling on both 3D shapes and 2D silhouettes, and an adversarial training scheme with an effective progressive discriminator to facilitate learning of the shape structures.","Extensive experiments demonstrated the effectiveness of our approach, which outperforms existing methods -- with state-of-the-art (SOTA) performance on both synthetic and real datasets."],"url":"http://arxiv.org/abs/2312.04435v1"}
{"created":"2023-12-07 16:57:26","title":"DreamVideo: Composing Your Dream Videos with Customized Subject and Motion","abstract":"Customized generation using diffusion models has made impressive progress in image generation, but remains unsatisfactory in the challenging video generation task, as it requires the controllability of both subjects and motions. To that end, we present DreamVideo, a novel approach to generating personalized videos from a few static images of the desired subject and a few videos of target motion. DreamVideo decouples this task into two stages, subject learning and motion learning, by leveraging a pre-trained video diffusion model. The subject learning aims to accurately capture the fine appearance of the subject from provided images, which is achieved by combining textual inversion and fine-tuning of our carefully designed identity adapter. In motion learning, we architect a motion adapter and fine-tune it on the given videos to effectively model the target motion pattern. Combining these two lightweight and efficient adapters allows for flexible customization of any subject with any motion. Extensive experimental results demonstrate the superior performance of our DreamVideo over the state-of-the-art methods for customized video generation. Our project page is at https://dreamvideo-t2v.github.io.","sentences":["Customized generation using diffusion models has made impressive progress in image generation, but remains unsatisfactory in the challenging video generation task, as it requires the controllability of both subjects and motions.","To that end, we present DreamVideo, a novel approach to generating personalized videos from a few static images of the desired subject and a few videos of target motion.","DreamVideo decouples this task into two stages, subject learning and motion learning, by leveraging a pre-trained video diffusion model.","The subject learning aims to accurately capture the fine appearance of the subject from provided images, which is achieved by combining textual inversion and fine-tuning of our carefully designed identity adapter.","In motion learning, we architect a motion adapter and fine-tune it on the given videos to effectively model the target motion pattern.","Combining these two lightweight and efficient adapters allows for flexible customization of any subject with any motion.","Extensive experimental results demonstrate the superior performance of our DreamVideo over the state-of-the-art methods for customized video generation.","Our project page is at https://dreamvideo-t2v.github.io."],"url":"http://arxiv.org/abs/2312.04433v1"}
{"created":"2023-12-07 16:56:24","title":"FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning","abstract":"Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights. This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions. We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition. We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.","sentences":["Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data.","However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors).","Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model.","To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks.","We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights.","This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions.","We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition.","We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model."],"url":"http://arxiv.org/abs/2312.04432v1"}
{"created":"2023-12-07 16:56:19","title":"Content Moderation on Social Media in the EU: Insights From the DSA Transparency Database","abstract":"The Digital Services Act (DSA) requires large social media platforms in the EU to provide clear and specific information whenever they remove or restrict access to certain content. These \"Statements of Reasons\" (SoRs) are collected in the DSA Transparency Database to ensure transparency and scrutiny of content moderation decisions of the providers of online platforms. In this work, we empirically analyze 156 million SoRs within an observation period of two months to provide an early look at content moderation decisions of social media platforms in the EU. Our empirical analysis yields the following main findings: (i) There are vast differences in the frequency of content moderation across platforms. For instance, TikTok performs more than 350 times more content moderation decisions per user than X/Twitter. (ii) Content moderation is most commonly applied for text and videos, whereas images and other content formats undergo moderation less frequently. (ii) The primary reasons for moderation include content falling outside the platform's scope of service, illegal/harmful speech, and pornography/sexualized content, with moderation of misinformation being relatively uncommon. (iii) The majority of rule-breaking content is detected and decided upon via automated means rather than manual intervention. However, X/Twitter reports that it relies solely on non-automated methods. (iv) There is significant variation in the content moderation actions taken across platforms. Altogether, our study implies inconsistencies in how social media platforms implement their obligations under the DSA -- resulting in a fragmented outcome that the DSA is meant to avoid. Our findings have important implications for regulators to clarify existing guidelines or lay out more specific rules that ensure common standards on how social media providers handle rule-breaking content on their platforms.","sentences":["The Digital Services Act (DSA) requires large social media platforms in the EU to provide clear and specific information whenever they remove or restrict access to certain content.","These \"Statements of Reasons\" (SoRs) are collected in the DSA Transparency Database to ensure transparency and scrutiny of content moderation decisions of the providers of online platforms.","In this work, we empirically analyze 156 million SoRs within an observation period of two months to provide an early look at content moderation decisions of social media platforms in the EU.","Our empirical analysis yields the following main findings: (i) There are vast differences in the frequency of content moderation across platforms.","For instance, TikTok performs more than 350 times more content moderation decisions per user than X/Twitter.","(ii) Content moderation is most commonly applied for text and videos, whereas images and other content formats undergo moderation less frequently.","(ii) The primary reasons for moderation include content falling outside the platform's scope of service, illegal/harmful speech, and pornography/sexualized content, with moderation of misinformation being relatively uncommon.","(iii) The majority of rule-breaking content is detected and decided upon via automated means rather than manual intervention.","However, X/Twitter reports that it relies solely on non-automated methods.","(iv) There is significant variation in the content moderation actions taken across platforms.","Altogether, our study implies inconsistencies in how social media platforms implement their obligations under the DSA -- resulting in a fragmented outcome that the DSA is meant to avoid.","Our findings have important implications for regulators to clarify existing guidelines or lay out more specific rules that ensure common standards on how social media providers handle rule-breaking content on their platforms."],"url":"http://arxiv.org/abs/2312.04431v1"}
{"created":"2023-12-07 16:55:04","title":"Approximate Caching for Efficiently Serving Diffusion Models","abstract":"Text-to-image generation using diffusion models has seen explosive popularity owing to their ability in producing high quality images adhering to text prompts. However, production-grade diffusion model serving is a resource intensive task that not only require high-end GPUs which are expensive but also incurs considerable latency. In this paper, we introduce a technique called approximate-caching that can reduce such iterative denoising steps for an image generation based on a prompt by reusing intermediate noise states created during a prior image generation for similar prompts. Based on this idea, we present an end to end text-to-image system, Nirvana, that uses the approximate-caching with a novel cache management-policy Least Computationally Beneficial and Frequently Used (LCBFU) to provide % GPU compute savings, 19.8% end-to-end latency reduction and 19% dollar savings, on average, on two real production workloads. We further present an extensive characterization of real production text-to-image prompts from the perspective of caching, popularity and reuse of intermediate states in a large production environment.","sentences":["Text-to-image generation using diffusion models has seen explosive popularity owing to their ability in producing high quality images adhering to text prompts.","However, production-grade diffusion model serving is a resource intensive task that not only require high-end GPUs which are expensive but also incurs considerable latency.","In this paper, we introduce a technique called approximate-caching that can reduce such iterative denoising steps for an image generation based on a prompt by reusing intermediate noise states created during a prior image generation for similar prompts.","Based on this idea, we present an end to end text-to-image system, Nirvana, that uses the approximate-caching with a novel cache management-policy Least Computationally Beneficial and Frequently Used (LCBFU) to provide % GPU compute savings, 19.8% end-to-end latency reduction and 19% dollar savings, on average, on two real production workloads.","We further present an extensive characterization of real production text-to-image prompts from the perspective of caching, popularity and reuse of intermediate states in a large production environment."],"url":"http://arxiv.org/abs/2312.04429v1"}
{"created":"2023-12-07 16:52:59","title":"Spheroidal Molecular Communication via Diffusion: Signaling Between Homogeneous Cell Aggregates","abstract":"Recent molecular communication (MC) research has integrated more detailed computational models to capture the dynamics of practical biophysical systems. This research focuses on developing realistic models for MC transceivers inspired by spheroids - three-dimensional cell aggregates commonly used in organ-on-chip experimental systems. Potential applications that can be used or modeled with spheroids include nutrient transport in an organ-on-chip system, the release of biomarkers or reception of drug molecules by a cancerous tumor site, or transceiver nanomachines participating in information exchange. In this paper, a simple diffusive MC system is considered where a spheroidal transmitter and receiver are in an unbounded fluid environment. These spheroidal antennas are modeled as porous media for diffusive signaling molecules, then their boundary conditions and effective diffusion coefficients are characterized. Further, for either a point source or spheroidal transmitter, Green's function for concentration (GFC) outside and inside the receiving spheroid is analytically derived and formulated in terms of an infinite series and confirmed by a particle-based simulator (PBS). The provided GFCs enable computation of the transmitted and received signals in the spheroidal communication system. This study shows that the porous structure of the receiving spheroid amplifies diffusion signals but also disperses them, thus there is a trade-off between porosity and information transmission rate. Also, the results reveal that the porous arrangement of the transmitting spheroid not only disperses the received signal but also attenuates it. System performance is also evaluated in terms of bit error rate (BER). Decreasing the porosity of the receiving spheroid is shown to enhance system performance. Conversely, reducing the porosity of the transmitting spheroid can adversely affect system performance.","sentences":["Recent molecular communication (MC) research has integrated more detailed computational models to capture the dynamics of practical biophysical systems.","This research focuses on developing realistic models for MC transceivers inspired by spheroids - three-dimensional cell aggregates commonly used in organ-on-chip experimental systems.","Potential applications that can be used or modeled with spheroids include nutrient transport in an organ-on-chip system, the release of biomarkers or reception of drug molecules by a cancerous tumor site, or transceiver nanomachines participating in information exchange.","In this paper, a simple diffusive MC system is considered where a spheroidal transmitter and receiver are in an unbounded fluid environment.","These spheroidal antennas are modeled as porous media for diffusive signaling molecules, then their boundary conditions and effective diffusion coefficients are characterized.","Further, for either a point source or spheroidal transmitter, Green's function for concentration (GFC) outside and inside the receiving spheroid is analytically derived and formulated in terms of an infinite series and confirmed by a particle-based simulator (PBS).","The provided GFCs enable computation of the transmitted and received signals in the spheroidal communication system.","This study shows that the porous structure of the receiving spheroid amplifies diffusion signals but also disperses them, thus there is a trade-off between porosity and information transmission rate.","Also, the results reveal that the porous arrangement of the transmitting spheroid not only disperses the received signal but also attenuates it.","System performance is also evaluated in terms of bit error rate (BER).","Decreasing the porosity of the receiving spheroid is shown to enhance system performance.","Conversely, reducing the porosity of the transmitting spheroid can adversely affect system performance."],"url":"http://arxiv.org/abs/2312.04427v1"}
{"created":"2023-12-07 16:49:09","title":"Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted Nearby Views","abstract":"Synthesizing multi-view 3D from one single image is a significant and challenging task. For this goal, Zero-1-to-3 methods aim to extend a 2D latent diffusion model to the 3D scope. These approaches generate the target-view image with a single-view source image and the camera pose as condition information. However, the one-to-one manner adopted in Zero-1-to-3 incurs challenges for building geometric and visual consistency across views, especially for complex objects. We propose a cascade generation framework constructed with two Zero-1-to-3 models, named Cascade-Zero123, to tackle this issue, which progressively extracts 3D information from the source image. Specifically, a self-prompting mechanism is designed to generate several nearby views at first. These views are then fed into the second-stage model along with the source image as generation conditions. With self-prompted multiple views as the supplementary information, our Cascade-Zero123 generates more highly consistent novel-view images than Zero-1-to-3. The promotion is significant for various complex and challenging scenes, involving insects, humans, transparent objects, and stacked multiple objects etc. The project page is at https://cascadezero123.github.io/.","sentences":["Synthesizing multi-view 3D from one single image is a significant and challenging task.","For this goal, Zero-1-to-3 methods aim to extend a 2D latent diffusion model to the 3D scope.","These approaches generate the target-view image with a single-view source image and the camera pose as condition information.","However, the one-to-one manner adopted in Zero-1-to-3 incurs challenges for building geometric and visual consistency across views, especially for complex objects.","We propose a cascade generation framework constructed with two Zero-1-to-3 models, named Cascade-Zero123, to tackle this issue, which progressively extracts 3D information from the source image.","Specifically, a self-prompting mechanism is designed to generate several nearby views at first.","These views are then fed into the second-stage model along with the source image as generation conditions.","With self-prompted multiple views as the supplementary information, our Cascade-Zero123 generates more highly consistent novel-view images than Zero-1-to-3.","The promotion is significant for various complex and challenging scenes, involving insects, humans, transparent objects, and stacked multiple objects etc.","The project page is at https://cascadezero123.github.io/."],"url":"http://arxiv.org/abs/2312.04424v1"}
{"created":"2023-12-07 16:48:32","title":"Scalable Knowledge Graph Construction and Inference on Human Genome Variants","abstract":"Real-world knowledge can be represented as a graph consisting of entities and relationships between the entities. The need for efficient and scalable solutions arises when dealing with vast genomic data, like RNA-sequencing. Knowledge graphs offer a powerful approach for various tasks in such large-scale genomic data, such as analysis and inference. In this work, variant-level information extracted from the RNA-sequences of vaccine-na\\\"ive COVID-19 patients have been represented as a unified, large knowledge graph. Variant call format (VCF) files containing the variant-level information were annotated to include further information for each variant. The data records in the annotated files were then converted to Resource Description Framework (RDF) triples. Each VCF file obtained had an associated CADD scores file that contained the raw and Phred-scaled scores for each variant. An ontology was defined for the VCF and CADD scores files. Using this ontology and the extracted information, a large, scalable knowledge graph was created. Available graph storage was then leveraged to query and create datasets for further downstream tasks. We also present a case study using the knowledge graph and perform a classification task using graph machine learning. We also draw comparisons between different Graph Neural Networks (GNNs) for the case study.","sentences":["Real-world knowledge can be represented as a graph consisting of entities and relationships between the entities.","The need for efficient and scalable solutions arises when dealing with vast genomic data, like RNA-sequencing.","Knowledge graphs offer a powerful approach for various tasks in such large-scale genomic data, such as analysis and inference.","In this work, variant-level information extracted from the RNA-sequences of vaccine-na\\\"ive COVID-19 patients have been represented as a unified, large knowledge graph.","Variant call format (VCF) files containing the variant-level information were annotated to include further information for each variant.","The data records in the annotated files were then converted to Resource Description Framework (RDF) triples.","Each VCF file obtained had an associated CADD scores file that contained the raw and Phred-scaled scores for each variant.","An ontology was defined for the VCF and CADD scores files.","Using this ontology and the extracted information, a large, scalable knowledge graph was created.","Available graph storage was then leveraged to query and create datasets for further downstream tasks.","We also present a case study using the knowledge graph and perform a classification task using graph machine learning.","We also draw comparisons between different Graph Neural Networks (GNNs) for the case study."],"url":"http://arxiv.org/abs/2312.04423v1"}
{"created":"2023-12-07 16:40:11","title":"MIST: An Efficient Approach for Software-Defined Multicast in Wireless Mesh Networks","abstract":"Multicasting is a vital information dissemination technique in Software-Defined Networking (SDN). With SDN, a multicast service can incorporate network functions implemented at different nodes, which is referred to as software-defined multicast. Emerging ubiquitous wireless networks for 5G and Beyond (B5G) inherently support multicast. However, the broadcast nature of wireless channels, especially in dense deployments, leads to neighborhood interference as a primary system degradation factor, which introduces a new challenge for software-defined multicast in wireless mesh networks. To tackle this, this paper introduces a novel approach, based on the idea of minimizing both the total length cost of the multicast tree and the interference at the same time. Accordingly, a bicriteria optimization problem is formulated, which is called \\emph{Minimum Interference Steiner Tree (MIST)}. To solve the bicriteria problem, instead of resorting to heuristics, this paper employs an innovative approach that is an approximate algorithm for MIST but with guaranteed performance. Specifically, the approach is a two-stage relaxation algorithm by exploiting the monotone submodularity property of the interference metric and identifying Pareto optimal solutions for MIST. Simulation results demonstrate and validate the performance of the proposed algorithm.","sentences":["Multicasting is a vital information dissemination technique in Software-Defined Networking (SDN).","With SDN, a multicast service can incorporate network functions implemented at different nodes, which is referred to as software-defined multicast.","Emerging ubiquitous wireless networks for 5G and Beyond (B5G) inherently support multicast.","However, the broadcast nature of wireless channels, especially in dense deployments, leads to neighborhood interference as a primary system degradation factor, which introduces a new challenge for software-defined multicast in wireless mesh networks.","To tackle this, this paper introduces a novel approach, based on the idea of minimizing both the total length cost of the multicast tree and the interference at the same time.","Accordingly, a bicriteria optimization problem is formulated, which is called \\emph{Minimum Interference Steiner Tree (MIST)}.","To solve the bicriteria problem, instead of resorting to heuristics, this paper employs an innovative approach that is an approximate algorithm for MIST but with guaranteed performance.","Specifically, the approach is a two-stage relaxation algorithm by exploiting the monotone submodularity property of the interference metric and identifying Pareto optimal solutions for MIST.","Simulation results demonstrate and validate the performance of the proposed algorithm."],"url":"http://arxiv.org/abs/2312.04418v1"}
{"created":"2023-12-07 16:38:32","title":"Temporal Fairness in Multiwinner Voting","abstract":"Multiwinner voting captures a wide variety of settings, from parliamentary elections in democratic systems to product placement in online shopping platforms. There is a large body of work dealing with axiomatic characterizations, computational complexity, and algorithmic analysis of multiwinner voting rules. Although many challenges remain, significant progress has been made in showing existence of fair and representative outcomes as well as efficient algorithmic solutions for many commonly studied settings. However, much of this work focuses on single-shot elections, even though in numerous real-world settings elections are held periodically and repeatedly. Hence, it is imperative to extend the study of multiwinner voting to temporal settings. Recently, there have been several efforts to address this challenge. However, these works are difficult to compare, as they model multi-period voting in very different ways. We propose a unified framework for studying temporal fairness in this domain, drawing connections with various existing bodies of work, and consolidating them within a general framework. We also identify gaps in existing literature, outline multiple opportunities for future work, and put forward a vision for the future of multiwinner voting in temporal settings.","sentences":["Multiwinner voting captures a wide variety of settings, from parliamentary elections in democratic systems to product placement in online shopping platforms.","There is a large body of work dealing with axiomatic characterizations, computational complexity, and algorithmic analysis of multiwinner voting rules.","Although many challenges remain, significant progress has been made in showing existence of fair and representative outcomes as well as efficient algorithmic solutions for many commonly studied settings.","However, much of this work focuses on single-shot elections, even though in numerous real-world settings elections are held periodically and repeatedly.","Hence, it is imperative to extend the study of multiwinner voting to temporal settings.","Recently, there have been several efforts to address this challenge.","However, these works are difficult to compare, as they model multi-period voting in very different ways.","We propose a unified framework for studying temporal fairness in this domain, drawing connections with various existing bodies of work, and consolidating them within a general framework.","We also identify gaps in existing literature, outline multiple opportunities for future work, and put forward a vision for the future of multiwinner voting in temporal settings."],"url":"http://arxiv.org/abs/2312.04417v1"}
{"created":"2023-12-07 16:38:20","title":"Monitoring Sustainable Global Development Along Shared Socioeconomic Pathways","abstract":"Sustainable global development is one of the most prevalent challenges facing the world today, hinging on the equilibrium between socioeconomic growth and environmental sustainability. We propose approaches to monitor and quantify sustainable development along the Shared Socioeconomic Pathways (SSPs), including mathematically derived scoring algorithms, and machine learning methods. These integrate socioeconomic and environmental datasets, to produce an interpretable metric for SSP alignment. An initial study demonstrates promising results, laying the groundwork for the application of different methods to the monitoring of sustainable global development.","sentences":["Sustainable global development is one of the most prevalent challenges facing the world today, hinging on the equilibrium between socioeconomic growth and environmental sustainability.","We propose approaches to monitor and quantify sustainable development along the Shared Socioeconomic Pathways (SSPs), including mathematically derived scoring algorithms, and machine learning methods.","These integrate socioeconomic and environmental datasets, to produce an interpretable metric for SSP alignment.","An initial study demonstrates promising results, laying the groundwork for the application of different methods to the monitoring of sustainable global development."],"url":"http://arxiv.org/abs/2312.04416v1"}
{"created":"2023-12-07 16:34:47","title":"Developing Elementary Federated Learning Algorithms Leveraging the ChatGPT","abstract":"The Python Testbed for Federated Learning Algorithms is a simple Python FL framework easy to use by ML&AI developers who do not need to be professional programmers, and this paper shows that it is also amenable to emerging AI tools. In this paper, we successfully developed three elementary FL algorithms using the following three steps process: (i) specify context, (ii) ask ChatGPT to complete server and clients' callback functions, and (iii) verify the generated code.","sentences":["The Python Testbed for Federated Learning Algorithms is a simple Python FL framework easy to use by ML&AI developers who do not need to be professional programmers, and this paper shows that it is also amenable to emerging AI tools.","In this paper, we successfully developed three elementary FL algorithms using the following three steps process: (i) specify context, (ii) ask ChatGPT to complete server and clients' callback functions, and (iii) verify the generated code."],"url":"http://arxiv.org/abs/2312.04412v1"}
{"created":"2023-12-07 16:26:23","title":"Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models","abstract":"Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with high fidelity and diverse contents. Despite this advancement, latent space smoothness within diffusion models remains largely unexplored. Smooth latent spaces ensure that a perturbation on an input latent corresponds to a steady change in the output image. This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing. In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations. To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth. Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step. In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model. Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks. Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models. Code is available at https://github.com/SHI-Labs/Smooth-Diffusion.","sentences":["Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with high fidelity and diverse contents.","Despite this advancement, latent space smoothness within diffusion models remains largely unexplored.","Smooth latent spaces ensure that a perturbation on an input latent corresponds to a steady change in the output image.","This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing.","In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations.","To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth.","Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step.","In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model.","Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks.","Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models.","Code is available at https://github.com/SHI-Labs/Smooth-Diffusion."],"url":"http://arxiv.org/abs/2312.04410v1"}
{"created":"2023-12-07 16:17:34","title":"On the Impact of Multi-dimensional Local Differential Privacy on Fairness","abstract":"Automated decision systems are increasingly used to make consequential decisions in people's lives. Due to the sensitivity of the manipulated data as well as the resulting decisions, several ethical concerns need to be addressed for the appropriate use of such technologies, in particular, fairness and privacy. Unlike previous work, which focused on centralized differential privacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper, we examine the impact of LDP in the presence of several sensitive attributes (i.e., multi-dimensional data) on fairness. Detailed empirical analysis on synthetic and benchmark datasets revealed very relevant observations. In particular, (1) multi-dimensional LDP is an efficient approach to reduce disparity, (2) the multi-dimensional approach of LDP (independent vs. combined) matters only at low privacy guarantees, and (3) the outcome Y distribution has an important effect on which group is more sensitive to the obfuscation. Last, we summarize our findings in the form of recommendations to guide practitioners in adopting effective privacy-preserving practices while maintaining fairness and utility in ML applications.","sentences":["Automated decision systems are increasingly used to make consequential decisions in people's lives.","Due to the sensitivity of the manipulated data as well as the resulting decisions, several ethical concerns need to be addressed for the appropriate use of such technologies, in particular, fairness and privacy.","Unlike previous work, which focused on centralized differential privacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper, we examine the impact of LDP in the presence of several sensitive attributes (i.e., multi-dimensional data) on fairness.","Detailed empirical analysis on synthetic and benchmark datasets revealed very relevant observations.","In particular, (1) multi-dimensional LDP is an efficient approach to reduce disparity, (2) the multi-dimensional approach of LDP (independent vs. combined) matters only at low privacy guarantees, and (3) the outcome Y distribution has an important effect on which group is more sensitive to the obfuscation.","Last, we summarize our findings in the form of recommendations to guide practitioners in adopting effective privacy-preserving practices while maintaining fairness and utility in ML applications."],"url":"http://arxiv.org/abs/2312.04404v1"}
{"created":"2023-12-07 16:16:50","title":"OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization","abstract":"Vision-language pre-training (VLP) models demonstrate impressive abilities in processing both images and text. However, they are vulnerable to multi-modal adversarial examples (AEs). Investigating the generation of high-transferability adversarial examples is crucial for uncovering VLP models' vulnerabilities in practical scenarios. Recent works have indicated that leveraging data augmentation and image-text modal interactions can enhance the transferability of adversarial examples for VLP models significantly. However, they do not consider the optimal alignment problem between dataaugmented image-text pairs. This oversight leads to adversarial examples that are overly tailored to the source model, thus limiting improvements in transferability. In our research, we first explore the interplay between image sets produced through data augmentation and their corresponding text sets. We find that augmented image samples can align optimally with certain texts while exhibiting less relevance to others. Motivated by this, we propose an Optimal Transport-based Adversarial Attack, dubbed OT-Attack. The proposed method formulates the features of image and text sets as two distinct distributions and employs optimal transport theory to determine the most efficient mapping between them. This optimal mapping informs our generation of adversarial examples to effectively counteract the overfitting issues. Extensive experiments across various network architectures and datasets in image-text matching tasks reveal that our OT-Attack outperforms existing state-of-the-art methods in terms of adversarial transferability.","sentences":["Vision-language pre-training (VLP) models demonstrate impressive abilities in processing both images and text.","However, they are vulnerable to multi-modal adversarial examples (AEs).","Investigating the generation of high-transferability adversarial examples is crucial for uncovering VLP models' vulnerabilities in practical scenarios.","Recent works have indicated that leveraging data augmentation and image-text modal interactions can enhance the transferability of adversarial examples for VLP models significantly.","However, they do not consider the optimal alignment problem between dataaugmented image-text pairs.","This oversight leads to adversarial examples that are overly tailored to the source model, thus limiting improvements in transferability.","In our research, we first explore the interplay between image sets produced through data augmentation and their corresponding text sets.","We find that augmented image samples can align optimally with certain texts while exhibiting less relevance to others.","Motivated by this, we propose an Optimal Transport-based Adversarial Attack, dubbed OT-Attack.","The proposed method formulates the features of image and text sets as two distinct distributions and employs optimal transport theory to determine the most efficient mapping between them.","This optimal mapping informs our generation of adversarial examples to effectively counteract the overfitting issues.","Extensive experiments across various network architectures and datasets in image-text matching tasks reveal that our OT-Attack outperforms existing state-of-the-art methods in terms of adversarial transferability."],"url":"http://arxiv.org/abs/2312.04403v1"}
{"created":"2023-12-07 16:16:47","title":"Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning","abstract":"Semantic segmentation enables robots to perceive and reason about their environments beyond geometry. Most of such systems build upon deep learning approaches. As autonomous robots are commonly deployed in initially unknown environments, pre-training on static datasets cannot always capture the variety of domains and limits the robot's perception performance during missions. Recently, self-supervised and fully supervised active learning methods emerged to improve a robot's vision. These approaches rely on large in-domain pre-training datasets or require substantial human labelling effort. We propose a planning method for semi-supervised active learning of semantic segmentation that substantially reduces human labelling requirements compared to fully supervised approaches. We leverage an adaptive map-based planner guided towards the frontiers of unexplored space with high model uncertainty collecting training data for human labelling. A key aspect of our approach is to combine the sparse high-quality human labels with pseudo labels automatically extracted from highly certain environment map areas. Experimental results show that our method reaches segmentation performance close to fully supervised approaches with drastically reduced human labelling effort while outperforming self-supervised approaches.","sentences":["Semantic segmentation enables robots to perceive and reason about their environments beyond geometry.","Most of such systems build upon deep learning approaches.","As autonomous robots are commonly deployed in initially unknown environments, pre-training on static datasets cannot always capture the variety of domains and limits the robot's perception performance during missions.","Recently, self-supervised and fully supervised active learning methods emerged to improve a robot's vision.","These approaches rely on large in-domain pre-training datasets or require substantial human labelling effort.","We propose a planning method for semi-supervised active learning of semantic segmentation that substantially reduces human labelling requirements compared to fully supervised approaches.","We leverage an adaptive map-based planner guided towards the frontiers of unexplored space with high model uncertainty collecting training data for human labelling.","A key aspect of our approach is to combine the sparse high-quality human labels with pseudo labels automatically extracted from highly certain environment map areas.","Experimental results show that our method reaches segmentation performance close to fully supervised approaches with drastically reduced human labelling effort while outperforming self-supervised approaches."],"url":"http://arxiv.org/abs/2312.04402v1"}
{"created":"2023-12-07 16:10:10","title":"Intelligent Anomaly Detection for Lane Rendering Using Transformer with Self-Supervised Pre-Training and Customized Fine-Tuning","abstract":"The burgeoning navigation services using digital maps provide great convenience to drivers. Nevertheless, the presence of anomalies in lane rendering map images occasionally introduces potential hazards, as such anomalies can be misleading to human drivers and consequently contribute to unsafe driving conditions. In response to this concern and to accurately and effectively detect the anomalies, this paper transforms lane rendering image anomaly detection into a classification problem and proposes a four-phase pipeline consisting of data pre-processing, self-supervised pre-training with the masked image modeling (MiM) method, customized fine-tuning using cross-entropy based loss with label smoothing, and post-processing to tackle it leveraging state-of-the-art deep learning techniques, especially those involving Transformer models. Various experiments verify the effectiveness of the proposed pipeline. Results indicate that the proposed pipeline exhibits superior performance in lane rendering image anomaly detection, and notably, the self-supervised pre-training with MiM can greatly enhance the detection accuracy while significantly reducing the total training time. For instance, employing the Swin Transformer with Uniform Masking as self-supervised pretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an improved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin Transformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an AUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the original 280. In conclusion, the proposed pipeline, with its incorporation of self-supervised pre-training using MiM and other advanced deep learning techniques, emerges as a robust solution for enhancing the accuracy and efficiency of lane rendering image anomaly detection in digital navigation systems.","sentences":["The burgeoning navigation services using digital maps provide great convenience to drivers.","Nevertheless, the presence of anomalies in lane rendering map images occasionally introduces potential hazards, as such anomalies can be misleading to human drivers and consequently contribute to unsafe driving conditions.","In response to this concern and to accurately and effectively detect the anomalies, this paper transforms lane rendering image anomaly detection into a classification problem and proposes a four-phase pipeline consisting of data pre-processing, self-supervised pre-training with the masked image modeling (MiM) method, customized fine-tuning using cross-entropy based loss with label smoothing, and post-processing to tackle it leveraging state-of-the-art deep learning techniques, especially those involving Transformer models.","Various experiments verify the effectiveness of the proposed pipeline.","Results indicate that the proposed pipeline exhibits superior performance in lane rendering image anomaly detection, and notably, the self-supervised pre-training with MiM can greatly enhance the detection accuracy while significantly reducing the total training time.","For instance, employing the Swin Transformer with Uniform Masking as self-supervised pretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an improved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin Transformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an AUC of 0.9498.","The fine-tuning epochs were dramatically reduced to 41 from the original 280.","In conclusion, the proposed pipeline, with its incorporation of self-supervised pre-training using MiM and other advanced deep learning techniques, emerges as a robust solution for enhancing the accuracy and efficiency of lane rendering image anomaly detection in digital navigation systems."],"url":"http://arxiv.org/abs/2312.04398v1"}
{"created":"2023-12-07 16:08:27","title":"On Czerwinski's \"${\\rm P} \\neq {\\rm NP}$ relative to a ${\\rm P}$-complete oracle\"","abstract":"In this paper, we take a closer look at Czerwinski's \"${\\rm P}\\neq{\\rm NP}$ relative to a ${\\rm P}$-complete oracle\" [Cze23]. There are (uncountably) infinitely-many relativized worlds where ${\\rm P}$ and ${\\rm NP}$ differ, and it is well-known that for any ${\\rm P}$-complete problem $A$, ${\\rm P}^A \\neq {\\rm NP}^A \\iff {\\rm P}\\neq {\\rm NP}$. The paper defines two sets ${\\rm D}_{\\rm P}$ and ${\\rm D}_{\\rm NP}$ and builds the purported proof of their main theorem on the claim that an oracle Turing machine with ${\\rm D}_{\\rm NP}$ as its oracle and that accepts ${\\rm D}_{\\rm P}$ must make $\\Theta(2^n)$ queries to the oracle. We invalidate the latter by proving that there is an oracle Turing machine with ${\\rm D}_{\\rm NP}$ as its oracle that accepts ${\\rm D}_{\\rm P}$ and yet only makes one query to the oracle. We thus conclude that Czerwinski's paper [Cze23] fails to establish that ${\\rm P} \\neq {\\rm NP}$.","sentences":["In this paper, we take a closer look at Czerwinski's \"${\\rm P}\\neq{\\rm NP}$ relative to a ${\\rm P}$-complete oracle\" [Cze23].","There are (uncountably) infinitely-many relativized worlds where ${\\rm P}$ and ${\\rm NP}$ differ, and it is well-known that for any ${\\rm P}$-complete problem $A$, ${\\rm P}^A \\neq {\\rm NP}^A \\iff {\\rm P}\\neq {\\rm NP}$.","The paper defines two sets ${\\rm D}_{\\rm P}$ and ${\\rm D}_{\\rm NP}$ and builds the purported proof of their main theorem on the claim that an oracle Turing machine with ${\\rm D}_{\\rm NP}$ as its oracle and that accepts ${\\rm D}_{\\rm P}$ must make $\\Theta(2^n)$ queries to the oracle.","We invalidate the latter by proving that there is an oracle Turing machine with ${\\rm D}_{\\rm NP}$ as its oracle that accepts ${\\rm D}_{\\rm P}$ and yet only makes one query to the oracle.","We thus conclude that Czerwinski's paper [Cze23] fails to establish that ${\\rm P} \\neq {\\rm NP}$."],"url":"http://arxiv.org/abs/2312.04395v1"}
{"created":"2023-12-07 16:06:31","title":"PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction","abstract":"Humans interact with objects all the time. Enabling a humanoid to learn human-object interaction (HOI) is a key step for future smart animation and intelligent robotics systems. However, recent progress in physics-based HOI requires carefully designed task-specific rewards, making the system unscalable and labor-intensive. This work focuses on dynamic HOI imitation: teaching humanoid dynamic interaction skills through imitating kinematic HOI demonstrations. It is quite challenging because of the complexity of the interaction between body parts and objects and the lack of dynamic HOI data. To handle the above issues, we present PhysHOI, the first physics-based whole-body HOI imitation approach without task-specific reward designs. Except for the kinematic HOI representations of humans and objects, we introduce the contact graph to model the contact relations between body parts and objects explicitly. A contact graph reward is also designed, which proved to be critical for precise HOI imitation. Based on the key designs, PhysHOI can imitate diverse HOI tasks simply yet effectively without prior knowledge. To make up for the lack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset that contains eight whole-body basketball skills. We validate PhysHOI on diverse HOI tasks, including whole-body grasping and basketball skills.","sentences":["Humans interact with objects all the time.","Enabling a humanoid to learn human-object interaction (HOI) is a key step for future smart animation and intelligent robotics systems.","However, recent progress in physics-based HOI requires carefully designed task-specific rewards, making the system unscalable and labor-intensive.","This work focuses on dynamic HOI imitation: teaching humanoid dynamic interaction skills through imitating kinematic HOI demonstrations.","It is quite challenging because of the complexity of the interaction between body parts and objects and the lack of dynamic HOI data.","To handle the above issues, we present PhysHOI, the first physics-based whole-body HOI imitation approach without task-specific reward designs.","Except for the kinematic HOI representations of humans and objects, we introduce the contact graph to model the contact relations between body parts and objects explicitly.","A contact graph reward is also designed, which proved to be critical for precise HOI imitation.","Based on the key designs, PhysHOI can imitate diverse HOI tasks simply yet effectively without prior knowledge.","To make up for the lack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset that contains eight whole-body basketball skills.","We validate PhysHOI on diverse HOI tasks, including whole-body grasping and basketball skills."],"url":"http://arxiv.org/abs/2312.04393v1"}
{"created":"2023-12-07 15:55:58","title":"Model-Based Epistemic Variance of Values for Risk-Aware Policy Optimization","abstract":"We consider the problem of quantifying uncertainty over expected cumulative rewards in model-based reinforcement learning. In particular, we focus on characterizing the variance over values induced by a distribution over MDPs. Previous work upper bounds the posterior variance over values by solving a so-called uncertainty Bellman equation (UBE), but the over-approximation may result in inefficient exploration. We propose a new UBE whose solution converges to the true posterior variance over values and leads to lower regret in tabular exploration problems. We identify challenges to apply the UBE theory beyond tabular problems and propose a suitable approximation. Based on this approximation, we introduce a general-purpose policy optimization algorithm, Q-Uncertainty Soft Actor-Critic (QU-SAC), that can be applied for either risk-seeking or risk-averse policy optimization with minimal changes. Experiments in both online and offline RL demonstrate improved performance compared to other uncertainty estimation methods.","sentences":["We consider the problem of quantifying uncertainty over expected cumulative rewards in model-based reinforcement learning.","In particular, we focus on characterizing the variance over values induced by a distribution over MDPs.","Previous work upper bounds the posterior variance over values by solving a so-called uncertainty Bellman equation (UBE), but the over-approximation may result in inefficient exploration.","We propose a new UBE whose solution converges to the true posterior variance over values and leads to lower regret in tabular exploration problems.","We identify challenges to apply the UBE theory beyond tabular problems and propose a suitable approximation.","Based on this approximation, we introduce a general-purpose policy optimization algorithm, Q-Uncertainty Soft Actor-Critic (QU-SAC), that can be applied for either risk-seeking or risk-averse policy optimization with minimal changes.","Experiments in both online and offline RL demonstrate improved performance compared to other uncertainty estimation methods."],"url":"http://arxiv.org/abs/2312.04386v1"}
{"created":"2023-12-07 15:49:39","title":"How much informative is your XAI? A decision-making assessment task to objectively measure the goodness of explanations","abstract":"There is an increasing consensus about the effectiveness of user-centred approaches in the explainable artificial intelligence (XAI) field. Indeed, the number and complexity of personalised and user-centred approaches to XAI have rapidly grown in recent years. Often, these works have a two-fold objective: (1) proposing novel XAI techniques able to consider the users and (2) assessing the \\textit{goodness} of such techniques with respect to others. From these new works, it emerged that user-centred approaches to XAI positively affect the interaction between users and systems. However, so far, the goodness of XAI systems has been measured through indirect measures, such as performance. In this paper, we propose an assessment task to objectively and quantitatively measure the goodness of XAI systems in terms of their \\textit{information power}, which we intended as the amount of information the system provides to the users during the interaction. Moreover, we plan to use our task to objectively compare two XAI techniques in a human-robot decision-making task to understand deeper whether user-centred approaches are more informative than classical ones.","sentences":["There is an increasing consensus about the effectiveness of user-centred approaches in the explainable artificial intelligence (XAI) field.","Indeed, the number and complexity of personalised and user-centred approaches to XAI have rapidly grown in recent years.","Often, these works have a two-fold objective: (1) proposing novel XAI techniques able to consider the users and (2) assessing the \\textit{goodness} of such techniques with respect to others.","From these new works, it emerged that user-centred approaches to XAI positively affect the interaction between users and systems.","However, so far, the goodness of XAI systems has been measured through indirect measures, such as performance.","In this paper, we propose an assessment task to objectively and quantitatively measure the goodness of XAI systems in terms of their \\textit{information power}, which we intended as the amount of information the system provides to the users during the interaction.","Moreover, we plan to use our task to objectively compare two XAI techniques in a human-robot decision-making task to understand deeper whether user-centred approaches are more informative than classical ones."],"url":"http://arxiv.org/abs/2312.04379v1"}
{"created":"2023-12-07 15:47:18","title":"HARQ-IR Aided Short Packet Communications: BLER Analysis and Throughput Maximization","abstract":"This paper introduces hybrid automatic repeat request with incremental redundancy (HARQ-IR) to boost the reliability of short packet communications. The finite blocklength information theory and correlated decoding events tremendously preclude the analysis of average block error rate (BLER). Fortunately, the recursive form of average BLER motivates us to calculate its value through the trapezoidal approximation and Gauss-Laguerre quadrature. Moreover, the asymptotic analysis is performed to derive a simple expression for the average BLER at high signal-to-noise ratio (SNR). Then, we study the maximization of long term average throughput (LTAT) via power allocation meanwhile ensuring the power and the BLER constraints. For tractability, the asymptotic BLER is employed to solve the problem through geometric programming (GP). However, the GP-based solution underestimates the LTAT at low SNR due to a large approximation error in this case. Alternatively, we also develop a deep reinforcement learning (DRL)-based framework to learn power allocation policy. In particular, the optimization problem is transformed into a constrained Markov decision process, which is solved by integrating deep deterministic policy gradient (DDPG) with subgradient method. The numerical results finally demonstrate that the DRL-based method outperforms the GP-based one at low SNR, albeit at the cost of increasing computational burden.","sentences":["This paper introduces hybrid automatic repeat request with incremental redundancy (HARQ-IR) to boost the reliability of short packet communications.","The finite blocklength information theory and correlated decoding events tremendously preclude the analysis of average block error rate (BLER).","Fortunately, the recursive form of average BLER motivates us to calculate its value through the trapezoidal approximation and Gauss-Laguerre quadrature.","Moreover, the asymptotic analysis is performed to derive a simple expression for the average BLER at high signal-to-noise ratio (SNR).","Then, we study the maximization of long term average throughput (LTAT) via power allocation meanwhile ensuring the power and the BLER constraints.","For tractability, the asymptotic BLER is employed to solve the problem through geometric programming (GP).","However, the GP-based solution underestimates the LTAT at low SNR due to a large approximation error in this case.","Alternatively, we also develop a deep reinforcement learning (DRL)-based framework to learn power allocation policy.","In particular, the optimization problem is transformed into a constrained Markov decision process, which is solved by integrating deep deterministic policy gradient (DDPG) with subgradient method.","The numerical results finally demonstrate that the DRL-based method outperforms the GP-based one at low SNR, albeit at the cost of increasing computational burden."],"url":"http://arxiv.org/abs/2312.04377v1"}
{"created":"2023-12-07 15:44:56","title":"Deep Dynamics: Vehicle Dynamics Modeling with a Physics-Informed Neural Network for Autonomous Racing","abstract":"Autonomous racing is a critical research area for autonomous driving, presenting significant challenges in vehicle dynamics modeling, such as balancing model precision and computational efficiency at high speeds (>280kmph), where minor errors in modeling have severe consequences. Existing physics-based models for vehicle dynamics require elaborate testing setups and tuning, which are hard to implement, time-intensive, and cost-prohibitive. Conversely, purely data-driven approaches do not generalize well and cannot adequately ensure physical constraints on predictions. This paper introduces Deep Dynamics, a physics-informed neural network (PINN) for vehicle dynamics modeling of an autonomous racecar. It combines physics coefficient estimation and dynamical equations to accurately predict vehicle states at high speeds and includes a unique Physics Guard layer to ensure internal coefficient estimates remain within their nominal physical ranges. Open-loop and closed-loop performance assessments, using a physics-based simulator and full-scale autonomous Indy racecar data, highlight Deep Dynamics as a promising approach for modeling racecar vehicle dynamics.","sentences":["Autonomous racing is a critical research area for autonomous driving, presenting significant challenges in vehicle dynamics modeling, such as balancing model precision and computational efficiency at high speeds (>280kmph), where minor errors in modeling have severe consequences.","Existing physics-based models for vehicle dynamics require elaborate testing setups and tuning, which are hard to implement, time-intensive, and cost-prohibitive.","Conversely, purely data-driven approaches do not generalize well and cannot adequately ensure physical constraints on predictions.","This paper introduces Deep Dynamics, a physics-informed neural network (PINN) for vehicle dynamics modeling of an autonomous racecar.","It combines physics coefficient estimation and dynamical equations to accurately predict vehicle states at high speeds and includes a unique Physics Guard layer to ensure internal coefficient estimates remain within their nominal physical ranges.","Open-loop and closed-loop performance assessments, using a physics-based simulator and full-scale autonomous Indy racecar data, highlight Deep Dynamics as a promising approach for modeling racecar vehicle dynamics."],"url":"http://arxiv.org/abs/2312.04374v1"}
{"created":"2023-12-07 15:43:52","title":"LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs","abstract":"We present LaMPilot, a novel framework for planning in the field of autonomous driving, rethinking the task as a code-generation process that leverages established behavioral primitives. This approach aims to address the challenge of interpreting and executing spontaneous user instructions such as \"overtake the car ahead,\" which have typically posed difficulties for existing frameworks. We introduce the LaMPilot benchmark specifically designed to quantitatively evaluate the efficacy of Large Language Models (LLMs) in translating human directives into actionable driving policies. We then evaluate a wide range of state-of-the-art code generation language models on tasks from the LaMPilot Benchmark. The results of the experiments showed that GPT-4, with human feedback, achieved an impressive task completion rate of 92.7% and a minimal collision rate of 0.9%. To encourage further investigation in this area, our code and dataset will be made available.","sentences":["We present LaMPilot, a novel framework for planning in the field of autonomous driving, rethinking the task as a code-generation process that leverages established behavioral primitives.","This approach aims to address the challenge of interpreting and executing spontaneous user instructions such as \"overtake the car ahead,\" which have typically posed difficulties for existing frameworks.","We introduce the LaMPilot benchmark specifically designed to quantitatively evaluate the efficacy of Large Language Models (LLMs) in translating human directives into actionable driving policies.","We then evaluate a wide range of state-of-the-art code generation language models on tasks from the LaMPilot Benchmark.","The results of the experiments showed that GPT-4, with human feedback, achieved an impressive task completion rate of 92.7% and a minimal collision rate of 0.9%.","To encourage further investigation in this area, our code and dataset will be made available."],"url":"http://arxiv.org/abs/2312.04372v1"}
{"created":"2023-12-07 15:40:36","title":"SingingHead: A Large-scale 4D Dataset for Singing Head Animation","abstract":"Singing, as a common facial movement second only to talking, can be regarded as a universal language across ethnicities and cultures, plays an important role in emotional communication, art, and entertainment. However, it is often overlooked in the field of audio-driven facial animation due to the lack of singing head datasets and the domain gap between singing and talking in rhythm and amplitude. To this end, we collect a high-quality large-scale singing head dataset, SingingHead, which consists of more than 27 hours of synchronized singing video, 3D facial motion, singing audio, and background music from 76 individuals and 8 types of music. Along with the SingingHead dataset, we argue that 3D and 2D facial animation tasks can be solved together, and propose a unified singing facial animation framework named UniSinger to achieve both singing audio-driven 3D singing head animation and 2D singing portrait video synthesis. Extensive comparative experiments with both SOTA 3D facial animation and 2D portrait animation methods demonstrate the necessity of singing-specific datasets in singing head animation tasks and the promising performance of our unified facial animation framework.","sentences":["Singing, as a common facial movement second only to talking, can be regarded as a universal language across ethnicities and cultures, plays an important role in emotional communication, art, and entertainment.","However, it is often overlooked in the field of audio-driven facial animation due to the lack of singing head datasets and the domain gap between singing and talking in rhythm and amplitude.","To this end, we collect a high-quality large-scale singing head dataset, SingingHead, which consists of more than 27 hours of synchronized singing video, 3D facial motion, singing audio, and background music from 76 individuals and 8 types of music.","Along with the SingingHead dataset, we argue that 3D and 2D facial animation tasks can be solved together, and propose a unified singing facial animation framework named UniSinger to achieve both singing audio-driven 3D singing head animation and 2D singing portrait video synthesis.","Extensive comparative experiments with both SOTA 3D facial animation and 2D portrait animation methods demonstrate the necessity of singing-specific datasets in singing head animation tasks and the promising performance of our unified facial animation framework."],"url":"http://arxiv.org/abs/2312.04369v1"}
