{"created":"2023-08-22 17:59:51","title":"GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues","abstract":"Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose. In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction. As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion. Then, we leverage the spatio-temporal relationship between the body and the object to extract two types of novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion. In the first stage, we introduce a new approach to enforce motion temporal consistency in the latent space (LTC), and generate consistent interaction motions. In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations. Given sequences of noisy body and object motion, GRIP upgrades them to include hand-object interaction. Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets.","sentences":["Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment.","Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality.","Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose.","In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction.","As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion.","Then, we leverage the spatio-temporal relationship between the body and the object to extract two types of novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion.","In the first stage, we introduce a new approach to enforce motion temporal consistency in the latent space (LTC), and generate consistent interaction motions.","In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations.","Given sequences of noisy body and object motion, GRIP upgrades them to include hand-object interaction.","Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets."],"url":"http://arxiv.org/abs/2308.11617v1"}
{"created":"2023-08-22 17:53:58","title":"Delving into Motion-Aware Matching for Monocular 3D Object Tracking","abstract":"Recent advances of monocular 3D object detection facilitate the 3D multi-object tracking task based on low-cost camera sensors. In this paper, we find that the motion cue of objects along different time frames is critical in 3D multi-object tracking, which is less explored in existing monocular-based approaches. In this paper, we propose a motion-aware framework for monocular 3D MOT. To this end, we propose MoMA-M3T, a framework that mainly consists of three motion-aware components. First, we represent the possible movement of an object related to all object tracklets in the feature space as its motion features. Then, we further model the historical object tracklet along the time frame in a spatial-temporal perspective via a motion transformer. Finally, we propose a motion-aware matching module to associate historical object tracklets and current observations as final tracking results. We conduct extensive experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T achieves competitive performance against state-of-the-art methods. Moreover, the proposed tracker is flexible and can be easily plugged into existing image-based 3D object detectors without re-training. Code and models are available at https://github.com/kuanchihhuang/MoMA-M3T.","sentences":["Recent advances of monocular 3D object detection facilitate the 3D multi-object tracking task based on low-cost camera sensors.","In this paper, we find that the motion cue of objects along different time frames is critical in 3D multi-object tracking, which is less explored in existing monocular-based approaches.","In this paper, we propose a motion-aware framework for monocular 3D MOT.","To this end, we propose MoMA-M3T, a framework that mainly consists of three motion-aware components.","First, we represent the possible movement of an object related to all object tracklets in the feature space as its motion features.","Then, we further model the historical object tracklet along the time frame in a spatial-temporal perspective via a motion transformer.","Finally, we propose a motion-aware matching module to associate historical object tracklets and current observations as final tracking results.","We conduct extensive experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T achieves competitive performance against state-of-the-art methods.","Moreover, the proposed tracker is flexible and can be easily plugged into existing image-based 3D object detectors without re-training.","Code and models are available at https://github.com/kuanchihhuang/MoMA-M3T."],"url":"http://arxiv.org/abs/2308.11607v1"}
{"created":"2023-08-22 17:53:55","title":"StoryBench: A Multifaceted Benchmark for Continuous Story Visualization","abstract":"Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefits of training on story-like data algorithmically generated from existing video captions. Finally, we establish guidelines for human evaluation of video stories, and reaffirm the need of better automatic metrics for video generation. StoryBench aims at encouraging future research efforts in this exciting new area.","sentences":["Generating video stories from text prompts is a complex task.","In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames.","Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets.","To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models.","Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts.","We evaluate small yet strong text-to-video baselines, and show the benefits of training on story-like data algorithmically generated from existing video captions.","Finally, we establish guidelines for human evaluation of video stories, and reaffirm the need of better automatic metrics for video generation.","StoryBench aims at encouraging future research efforts in this exciting new area."],"url":"http://arxiv.org/abs/2308.11606v1"}
{"created":"2023-08-22 17:53:26","title":"GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning","abstract":"Large-scale foundation models, such as CLIP, have demonstrated remarkable success in visual recognition tasks by embedding images in a semantically rich space. Self-supervised learning (SSL) has also shown promise in improving visual recognition by learning invariant features. However, the combination of CLIP with SSL is found to face challenges due to the multi-task framework that blends CLIP's contrastive loss and SSL's loss, including difficulties with loss weighting and inconsistency among different views of images in CLIP's output space. To overcome these challenges, we propose a prompt learning-based model called GOPro, which is a unified framework that ensures similarity between various augmented views of input images in a shared image-text embedding space, using a pair of learnable image and text projectors atop CLIP, to promote invariance and generalizability. To automatically learn such prompts, we leverage the visual content and style primitives extracted from pre-trained CLIP and adapt them to the target task. In addition to CLIP's cross-domain contrastive loss, we introduce a visual contrastive loss and a novel prompt consistency loss, considering the different views of the images. GOPro is trained end-to-end on all three loss objectives, combining the strengths of CLIP and SSL in a principled manner. Empirical evaluations demonstrate that GOPro outperforms the state-of-the-art prompting techniques on three challenging domain generalization tasks across multiple benchmarks by a significant margin. Our code is available at https://github.com/mainaksingha01/GOPro.","sentences":["Large-scale foundation models, such as CLIP, have demonstrated remarkable success in visual recognition tasks by embedding images in a semantically rich space.","Self-supervised learning (SSL) has also shown promise in improving visual recognition by learning invariant features.","However, the combination of CLIP with SSL is found to face challenges due to the multi-task framework that blends CLIP's contrastive loss and SSL's loss, including difficulties with loss weighting and inconsistency among different views of images in CLIP's output space.","To overcome these challenges, we propose a prompt learning-based model called GOPro, which is a unified framework that ensures similarity between various augmented views of input images in a shared image-text embedding space, using a pair of learnable image and text projectors atop CLIP, to promote invariance and generalizability.","To automatically learn such prompts, we leverage the visual content and style primitives extracted from pre-trained CLIP and adapt them to the target task.","In addition to CLIP's cross-domain contrastive loss, we introduce a visual contrastive loss and a novel prompt consistency loss, considering the different views of the images.","GOPro is trained end-to-end on all three loss objectives, combining the strengths of CLIP and SSL in a principled manner.","Empirical evaluations demonstrate that GOPro outperforms the state-of-the-art prompting techniques on three challenging domain generalization tasks across multiple benchmarks by a significant margin.","Our code is available at https://github.com/mainaksingha01/GOPro."],"url":"http://arxiv.org/abs/2308.11605v1"}
{"created":"2023-08-22 17:52:44","title":"Semantic Multi-Resolution Communications","abstract":"Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC). This superiority arises from the suboptimality of SSCC when dealing with finite block-length data. Moreover, SSCC falls short in reconstructing data in a multi-user and/or multi-resolution fashion, as it only tries to satisfy the worst channel and/or the highest quality data. To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL). This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data. Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process. These semantic features could be crucial elements such as class labels, essential for classification tasks, or other key attributes that require preservation. Within this framework, each level of encoded data can be carefully designed to retain specific data semantics. As a result, the precision of a semantic classifier can be progressively enhanced across successive layers, emphasizing the preservation of targeted semantics throughout the encoding and decoding stages. We conduct experiments on MNIST and CIFAR10 dataset. The experiment with both datasets illustrates that our proposed method is capable of surpassing the SSCC method in reconstructing data with different resolutions, enabling the extraction of semantic features with heightened confidence in successive layers. This capability is particularly advantageous for prioritizing and preserving more crucial semantic features within the datasets.","sentences":["Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC).","This superiority arises from the suboptimality of SSCC when dealing with finite block-length data.","Moreover, SSCC falls short in reconstructing data in a multi-user and/or multi-resolution fashion, as it only tries to satisfy the worst channel and/or the highest quality data.","To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL).","This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data.","Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process.","These semantic features could be crucial elements such as class labels, essential for classification tasks, or other key attributes that require preservation.","Within this framework, each level of encoded data can be carefully designed to retain specific data semantics.","As a result, the precision of a semantic classifier can be progressively enhanced across successive layers, emphasizing the preservation of targeted semantics throughout the encoding and decoding stages.","We conduct experiments on MNIST and CIFAR10 dataset.","The experiment with both datasets illustrates that our proposed method is capable of surpassing the SSCC method in reconstructing data with different resolutions, enabling the extraction of semantic features with heightened confidence in successive layers.","This capability is particularly advantageous for prioritizing and preserving more crucial semantic features within the datasets."],"url":"http://arxiv.org/abs/2308.11604v1"}
{"created":"2023-08-22 17:48:24","title":"Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model","abstract":"The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.","sentences":["The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains.","With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns.","There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users.","Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts.","Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency).","Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability.","Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla.","Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem."],"url":"http://arxiv.org/abs/2308.11601v1"}
{"created":"2023-08-22 17:47:09","title":"Towards Universal Interaction for Extended Reality","abstract":"Extended Reality (XR) is a rapidly growing field offering unique immersive experiences, social networking, learning, and collaboration opportunities. The continuous advancements in XR technology and industry efforts are gradually moving this technology toward end consumers. However, a universal one-size-fits-all solution for seamless XR interaction still needs to be discovered. Currently, we face a diverse landscape of interaction modalities that depend on the environment, user preferences, task, and device capabilities. Commercially available input methods like handheld controllers, hand gestures, voice commands, and combinations of those need universal flexibility and expressiveness. Additionally, hybrid user interfaces, such as smartwatches and smartphones as ubiquitous input and output devices, expand this interaction design space. In this position paper, we discuss the idea of a universal interaction concept for XR. We present challenges and opportunities for implementing hybrid user interfaces, emphasizing Environment, Task, and User. We explore the potential to enhance user experiences, interaction capabilities, and the development of seamless and efficient XR interaction methods. We examine challenges and aim to stimulate a discussion on the design of generic, universal interfaces for XR.","sentences":["Extended Reality (XR) is a rapidly growing field offering unique immersive experiences, social networking, learning, and collaboration opportunities.","The continuous advancements in XR technology and industry efforts are gradually moving this technology toward end consumers.","However, a universal one-size-fits-all solution for seamless XR interaction still needs to be discovered.","Currently, we face a diverse landscape of interaction modalities that depend on the environment, user preferences, task, and device capabilities.","Commercially available input methods like handheld controllers, hand gestures, voice commands, and combinations of those need universal flexibility and expressiveness.","Additionally, hybrid user interfaces, such as smartwatches and smartphones as ubiquitous input and output devices, expand this interaction design space.","In this position paper, we discuss the idea of a universal interaction concept for XR.","We present challenges and opportunities for implementing hybrid user interfaces, emphasizing Environment, Task, and User.","We explore the potential to enhance user experiences, interaction capabilities, and the development of seamless and efficient XR interaction methods.","We examine challenges and aim to stimulate a discussion on the design of generic, universal interfaces for XR."],"url":"http://arxiv.org/abs/2308.11600v1"}
{"created":"2023-08-22 17:44:18","title":"SeamlessM4T-Massively Multilingual & Multimodal Machine Translation","abstract":"What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation. Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety. Finally, all contributions in this work are open-sourced at this https https://github.com/facebookresearch/seamless_communication.","sentences":["What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages?","While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides.","More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach.","To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages.","To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0.","Subsequently, we created a multimodal corpus of automatically aligned speech translations.","Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text.","On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation.","Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech.","Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model.","Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety.","Finally, all contributions in this work are open-sourced at this https https://github.com/facebookresearch/seamless_communication."],"url":"http://arxiv.org/abs/2308.11596v1"}
{"created":"2023-08-22 17:36:26","title":"Vision-Based Intelligent Robot Grasping Using Sparse Neural Network","abstract":"In the modern era of Deep Learning, network parameters play a vital role in models efficiency but it has its own limitations like extensive computations and memory requirements, which may not be suitable for real time intelligent robot grasping tasks. Current research focuses on how the model efficiency can be maintained by introducing sparsity but without compromising accuracy of the model in the robot grasping domain. More specifically, in this research two light-weighted neural networks have been introduced, namely Sparse-GRConvNet and Sparse-GINNet, which leverage sparsity in the robotic grasping domain for grasp pose generation by integrating the Edge-PopUp algorithm. This algorithm facilitates the identification of the top K% of edges by considering their respective score values. Both the Sparse-GRConvNet and Sparse-GINNet models are designed to generate high-quality grasp poses in real-time at every pixel location, enabling robots to effectively manipulate unfamiliar objects. We extensively trained our models using two benchmark datasets: Cornell Grasping Dataset (CGD) and Jacquard Grasping Dataset (JGD). Both Sparse-GRConvNet and Sparse-GINNet models outperform the current state-of-the-art methods in terms of performance, achieving an impressive accuracy of 97.75% with only 10% of the weight of GR-ConvNet and 50% of the weight of GI-NNet, respectively, on CGD. Additionally, Sparse-GRConvNet achieve an accuracy of 85.77% with 30% of the weight of GR-ConvNet and Sparse-GINNet achieve an accuracy of 81.11% with 10% of the weight of GI-NNet on JGD. To validate the performance of our proposed models, we conducted extensive experiments using the Anukul (Baxter) hardware cobot.","sentences":["In the modern era of Deep Learning, network parameters play a vital role in models efficiency but it has its own limitations like extensive computations and memory requirements, which may not be suitable for real time intelligent robot grasping tasks.","Current research focuses on how the model efficiency can be maintained by introducing sparsity but without compromising accuracy of the model in the robot grasping domain.","More specifically, in this research two light-weighted neural networks have been introduced, namely Sparse-GRConvNet and Sparse-GINNet, which leverage sparsity in the robotic grasping domain for grasp pose generation by integrating the Edge-PopUp algorithm.","This algorithm facilitates the identification of the top K% of edges by considering their respective score values.","Both the Sparse-GRConvNet and Sparse-GINNet models are designed to generate high-quality grasp poses in real-time at every pixel location, enabling robots to effectively manipulate unfamiliar objects.","We extensively trained our models using two benchmark datasets:","Cornell Grasping Dataset (CGD) and Jacquard Grasping Dataset (JGD).","Both Sparse-GRConvNet and Sparse-GINNet models outperform the current state-of-the-art methods in terms of performance, achieving an impressive accuracy of 97.75% with only 10% of the weight of GR-ConvNet and 50% of the weight of GI-NNet, respectively, on CGD.","Additionally, Sparse-GRConvNet achieve an accuracy of 85.77% with 30% of the weight of GR-ConvNet and Sparse-GINNet achieve an accuracy of 81.11% with 10% of the weight of GI-NNet on JGD.","To validate the performance of our proposed models, we conducted extensive experiments using the Anukul (Baxter) hardware cobot."],"url":"http://arxiv.org/abs/2308.11590v1"}
{"created":"2023-08-22 17:23:00","title":"G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid Model","abstract":"This study introduces a novel framework, G3Reg, for fast and robust global registration of LiDAR point clouds. In contrast to conventional complex keypoints and descriptors, we extract fundamental geometric primitives including planes, clusters, and lines (PCL) from the raw point cloud to obtain low-level semantic segments. Each segment is formulated as a unified Gaussian Ellipsoid Model (GEM) by employing a probability ellipsoid to ensure the ground truth centers are encompassed with a certain degree of probability. Utilizing these GEMs, we then present a distrust-and-verify scheme based on a Pyramid Compatibility Graph for Global Registration (PAGOR). Specifically, we establish an upper bound, which can be traversed based on the confidence level for compatibility testing to construct the pyramid graph. Gradually, we solve multiple maximum cliques (MAC) for each level of the graph, generating numerous transformation candidates. In the verification phase, we adopt a precise and efficient metric for point cloud alignment quality, founded on geometric primitives, to identify the optimal candidate. The performance of the algorithm is extensively validated on three publicly available datasets and a self-collected multi-session dataset, without changing any parameter settings in the experimental evaluation. The results exhibit superior robustness and real-time performance of the G3Reg framework compared to state-of-the-art methods. Furthermore, we demonstrate the potential for integrating individual GEM and PAGOR components into other algorithmic frameworks to enhance their efficacy. To advance further research and promote community understanding, we have publicly shared the source code.","sentences":["This study introduces a novel framework, G3Reg, for fast and robust global registration of LiDAR point clouds.","In contrast to conventional complex keypoints and descriptors, we extract fundamental geometric primitives including planes, clusters, and lines (PCL) from the raw point cloud to obtain low-level semantic segments.","Each segment is formulated as a unified Gaussian Ellipsoid Model (GEM) by employing a probability ellipsoid to ensure the ground truth centers are encompassed with a certain degree of probability.","Utilizing these GEMs, we then present a distrust-and-verify scheme based on a Pyramid Compatibility Graph for Global Registration (PAGOR).","Specifically, we establish an upper bound, which can be traversed based on the confidence level for compatibility testing to construct the pyramid graph.","Gradually, we solve multiple maximum cliques (MAC) for each level of the graph, generating numerous transformation candidates.","In the verification phase, we adopt a precise and efficient metric for point cloud alignment quality, founded on geometric primitives, to identify the optimal candidate.","The performance of the algorithm is extensively validated on three publicly available datasets and a self-collected multi-session dataset, without changing any parameter settings in the experimental evaluation.","The results exhibit superior robustness and real-time performance of the G3Reg framework compared to state-of-the-art methods.","Furthermore, we demonstrate the potential for integrating individual GEM and PAGOR components into other algorithmic frameworks to enhance their efficacy.","To advance further research and promote community understanding, we have publicly shared the source code."],"url":"http://arxiv.org/abs/2308.11573v1"}
{"created":"2023-08-22 17:14:19","title":"SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation","abstract":"Recent studies show that self-attentions behave like low-pass filters (as opposed to convolutions) and enhancing their high-pass filtering capability improves model performance. Contrary to this idea, we investigate existing convolution-based models with spectral analysis and observe that improving the low-pass filtering in convolution operations also leads to performance improvement. To account for this observation, we hypothesize that utilizing optimal token mixers that capture balanced representations of both high- and low-frequency components can enhance the performance of models. We verify this by decomposing visual features into the frequency domain and combining them in a balanced manner. To handle this, we replace the balancing problem with a mask filtering problem in the frequency domain. Then, we introduce a novel token-mixer named SPAM and leverage it to derive a MetaFormer model termed as SPANet. Experimental results show that the proposed method provides a way to achieve this balance, and the balanced representations of both high- and low-frequency components can improve the performance of models on multiple computer vision tasks. Our code is available at $\\href{https://doranlyong.github.io/projects/spanet/}{\\text{https://doranlyong.github.io/projects/spanet/}}$.","sentences":["Recent studies show that self-attentions behave like low-pass filters (as opposed to convolutions) and enhancing their high-pass filtering capability improves model performance.","Contrary to this idea, we investigate existing convolution-based models with spectral analysis and observe that improving the low-pass filtering in convolution operations also leads to performance improvement.","To account for this observation, we hypothesize that utilizing optimal token mixers that capture balanced representations of both high- and low-frequency components can enhance the performance of models.","We verify this by decomposing visual features into the frequency domain and combining them in a balanced manner.","To handle this, we replace the balancing problem with a mask filtering problem in the frequency domain.","Then, we introduce a novel token-mixer named SPAM and leverage it to derive a MetaFormer model termed as SPANet.","Experimental results show that the proposed method provides a way to achieve this balance, and the balanced representations of both high- and low-frequency components can improve the performance of models on multiple computer vision tasks.","Our code is available at $\\href{https://doranlyong.github.io/projects/spanet/}{\\text{https://doranlyong.github.io/projects/spanet/}}$."],"url":"http://arxiv.org/abs/2308.11568v1"}
{"created":"2023-08-22 16:59:31","title":"Using ChatGPT as a CAT tool in Easy Language translation","abstract":"This study sets out to investigate the feasibility of using ChatGPT to translate citizen-oriented administrative texts into German Easy Language, a simplified, controlled language variety that is adapted to the needs of people with reading impairments. We use ChatGPT to translate selected texts from websites of German public authorities using two strategies, i.e. linguistic and holistic. We analyse the quality of the generated texts based on different criteria, such as correctness, readability, and syntactic complexity. The results indicated that the generated texts are easier than the standard texts, but that they still do not fully meet the established Easy Language standards. Additionally, the content is not always rendered correctly.","sentences":["This study sets out to investigate the feasibility of using ChatGPT to translate citizen-oriented administrative texts into German Easy Language, a simplified, controlled language variety that is adapted to the needs of people with reading impairments.","We use ChatGPT to translate selected texts from websites of German public authorities using two strategies, i.e. linguistic and holistic.","We analyse the quality of the generated texts based on different criteria, such as correctness, readability, and syntactic complexity.","The results indicated that the generated texts are easier than the standard texts, but that they still do not fully meet the established Easy Language standards.","Additionally, the content is not always rendered correctly."],"url":"http://arxiv.org/abs/2308.11563v1"}
{"created":"2023-08-22 16:45:35","title":"Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation","abstract":"This report details the method of the winning entry of the AVDN Challenge in ICCV 2023. The competition addresses the Aerial Navigation from Dialog History (ANDH) task, which requires a drone agent to associate dialog history with aerial observations to reach the destination. For better cross-modal grounding abilities of the drone agent, we propose a Target-Grounded Graph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leverages a graph-aware transformer to capture spatiotemporal dependency, which is beneficial for navigation state tracking and robust action planning. TG-GAT first leverages a graph-aware transformer to capture spatiotemporal dependencies for more robust action planning. In addition, an auxiliary visual grounding task is devised to boost the agent's awareness of referred landmarks. Moreover, a hybrid augmentation strategy based on large language models is utilized to mitigate data scarcity limitations. Our TG-GAT framework won the AVDN Challenge 2023, with 2.2% and 3.0% absolute improvements over the baseline on SPL and SR metrics, respectively. The code is available at https://github.com/yifeisu/avdn-challenge.","sentences":["This report details the method of the winning entry of the AVDN Challenge in ICCV 2023.","The competition addresses the Aerial Navigation from Dialog History (ANDH) task, which requires a drone agent to associate dialog history with aerial observations to reach the destination.","For better cross-modal grounding abilities of the drone agent, we propose a Target-Grounded Graph-Aware Transformer (TG-GAT) framework.","Concretely, TG-GAT first leverages a graph-aware transformer to capture spatiotemporal dependency, which is beneficial for navigation state tracking and robust action planning.","TG-GAT first leverages a graph-aware transformer to capture spatiotemporal dependencies for more robust action planning.","In addition, an auxiliary visual grounding task is devised to boost the agent's awareness of referred landmarks.","Moreover, a hybrid augmentation strategy based on large language models is utilized to mitigate data scarcity limitations.","Our TG-GAT framework won the AVDN Challenge 2023, with 2.2% and 3.0% absolute improvements over the baseline on SPL and SR metrics, respectively.","The code is available at https://github.com/yifeisu/avdn-challenge."],"url":"http://arxiv.org/abs/2308.11561v1"}
{"created":"2023-08-22 16:38:17","title":"Tight Lower Bound on Equivalence Testing in Conditional Sampling Model","abstract":"We study the equivalence testing problem where the goal is to determine if the given two unknown distributions on $[n]$ are equal or $\\epsilon$-far in the total variation distance in the conditional sampling model (CFGM, SICOMP16; CRS, SICOMP15) wherein a tester can get a sample from the distribution conditioned on any subset. Equivalence testing is a central problem in distribution testing, and there has been a plethora of work on this topic in various sampling models.   Despite significant efforts over the years, there remains a gap in the current best-known upper bound of $\\tilde{O}(\\log \\log n)$ [FJOPS, COLT 2015] and lower bound of $\\Omega(\\sqrt{\\log \\log n})$[ACK, RANDOM 2015, Theory of Computing 2018].   Closing this gap has been repeatedly posed as an open problem (listed as problems 66 and 87 at sublinear.info). In this paper, we completely resolve the query complexity of this problem by showing a lower bound of $\\tilde{\\Omega}(\\log \\log n)$. For that purpose, we develop a novel and generic proof technique that enables us to break the $\\sqrt{\\log \\log n}$ barrier, not only for the equivalence testing problem but also for other distribution testing problems, such as uniblock property.","sentences":["We study the equivalence testing problem where the goal is to determine if the given two unknown distributions on $[n]$ are equal or $\\epsilon$-far in the total variation distance in the conditional sampling model (CFGM, SICOMP16; CRS, SICOMP15) wherein a tester can get a sample from the distribution conditioned on any subset.","Equivalence testing is a central problem in distribution testing, and there has been a plethora of work on this topic in various sampling models.   ","Despite significant efforts over the years, there remains a gap in the current best-known upper bound of $\\tilde{O}(\\log \\log n)$","[FJOPS, COLT 2015] and lower bound of $\\Omega(\\sqrt{\\log \\log n})$[ACK, RANDOM 2015, Theory of Computing 2018].   ","Closing this gap has been repeatedly posed as an open problem (listed as problems 66 and 87 at sublinear.info).","In this paper, we completely resolve the query complexity of this problem by showing a lower bound of $\\tilde{\\Omega}(\\log \\log n)$.","For that purpose, we develop a novel and generic proof technique that enables us to break the $\\sqrt{\\log \\log n}$ barrier, not only for the equivalence testing problem but also for other distribution testing problems, such as uniblock property."],"url":"http://arxiv.org/abs/2308.11558v1"}
{"created":"2023-08-22 16:32:46","title":"Multi-event Video-Text Retrieval","abstract":"Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR.","sentences":["Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet.","A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task.","However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events.","This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference.","In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task.","We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task.","Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task.","We believe this work serves as a strong foundation for future studies.","Code is available at https://github.com/gengyuanmax/MeVTR."],"url":"http://arxiv.org/abs/2308.11551v1"}
{"created":"2023-08-22 16:16:27","title":"Refugee status determination: how cooperation with machine learning tools can lead to more justice","abstract":"Previous research on refugee status adjudications has shown that prediction of the outcome of an application can be derived from very few features with satisfactory accuracy. Recent research work has achieved between 70 and 90% accuracy using text analytics on various legal fields among which refugee status determination. Some studies report predictions derived from the judge identity only. Additionally most features used for prediction are non-substantive and external features ranging from news reports, date and time of the hearing or weather. On the other hand, literature shows that noise is ubiquitous in human judgments and significantly affects the outcome of decisions. It has been demonstrated that noise is a significant factor impacting legal decisions. We use the term \"noise\" in the sense described by D. Kahneman, as a measure of how human beings are unavoidably influenced by external factors when making a decision. In the context of refugee status determination, it means for instance that two judges would take different decisions when presented with the same application. This article explores ways that machine learning can help reduce noise in refugee law decision making. We are not suggesting that this proposed methodology should be exclusive from other approaches to improve decisions such as training of decision makers, skills acquisition or judgment aggregation, but rather that it is a path worth exploring. We investigate how artificial intelligence and specifically data-driven applications can be used to benefit all parties involved in refugee status adjudications. We specifically look at decisions taken in Canada and in the United States. Our research aims at reducing arbitrariness and unfairness that derive from noisy decisions, based on the assumption that if two cases or applications are alike they should be treated in the same way and induce the same outcome.","sentences":["Previous research on refugee status adjudications has shown that prediction of the outcome of an application can be derived from very few features with satisfactory accuracy.","Recent research work has achieved between 70 and 90% accuracy using text analytics on various legal fields among which refugee status determination.","Some studies report predictions derived from the judge identity only.","Additionally most features used for prediction are non-substantive and external features ranging from news reports, date and time of the hearing or weather.","On the other hand, literature shows that noise is ubiquitous in human judgments and significantly affects the outcome of decisions.","It has been demonstrated that noise is a significant factor impacting legal decisions.","We use the term \"noise\" in the sense described by D. Kahneman, as a measure of how human beings are unavoidably influenced by external factors when making a decision.","In the context of refugee status determination, it means for instance that two judges would take different decisions when presented with the same application.","This article explores ways that machine learning can help reduce noise in refugee law decision making.","We are not suggesting that this proposed methodology should be exclusive from other approaches to improve decisions such as training of decision makers, skills acquisition or judgment aggregation, but rather that it is a path worth exploring.","We investigate how artificial intelligence and specifically data-driven applications can be used to benefit all parties involved in refugee status adjudications.","We specifically look at decisions taken in Canada and in the United States.","Our research aims at reducing arbitrariness and unfairness that derive from noisy decisions, based on the assumption that if two cases or applications are alike they should be treated in the same way and induce the same outcome."],"url":"http://arxiv.org/abs/2308.11541v1"}
{"created":"2023-08-22 16:05:18","title":"BELB: a Biomedical Entity Linking Benchmark","abstract":"Biomedical entity linking (BEL) is the task of grounding entity mentions to a knowledge base. It plays a vital role in information extraction pipelines for the life sciences literature. We review recent work in the field and find that, as the task is absent from existing benchmarks for biomedical text mining, different studies adopt different experimental setups making comparisons based on published numbers problematic. Furthermore, neural systems are tested primarily on instances linked to the broad coverage knowledge base UMLS, leaving their performance to more specialized ones, e.g. genes or variants, understudied. We therefore developed BELB, a Biomedical Entity Linking Benchmark, providing access in a unified format to 11 corpora linked to 7 knowledge bases and spanning six entity types: gene, disease, chemical, species, cell line and variant. BELB greatly reduces preprocessing overhead in testing BEL systems on multiple corpora offering a standardized testbed for reproducible experiments. Using BELB we perform an extensive evaluation of six rule-based entity-specific systems and three recent neural approaches leveraging pre-trained language models. Our results reveal a mixed picture showing that neural approaches fail to perform consistently across entity types, highlighting the need of further studies towards entity-agnostic models.","sentences":["Biomedical entity linking (BEL) is the task of grounding entity mentions to a knowledge base.","It plays a vital role in information extraction pipelines for the life sciences literature.","We review recent work in the field and find that, as the task is absent from existing benchmarks for biomedical text mining, different studies adopt different experimental setups making comparisons based on published numbers problematic.","Furthermore, neural systems are tested primarily on instances linked to the broad coverage knowledge base UMLS, leaving their performance to more specialized ones, e.g. genes or variants, understudied.","We therefore developed BELB, a Biomedical Entity Linking Benchmark, providing access in a unified format to 11 corpora linked to 7 knowledge bases and spanning six entity types: gene, disease, chemical, species, cell line and variant.","BELB greatly reduces preprocessing overhead in testing BEL systems on multiple corpora offering a standardized testbed for reproducible experiments.","Using BELB we perform an extensive evaluation of six rule-based entity-specific systems and three recent neural approaches leveraging pre-trained language models.","Our results reveal a mixed picture showing that neural approaches fail to perform consistently across entity types, highlighting the need of further studies towards entity-agnostic models."],"url":"http://arxiv.org/abs/2308.11537v1"}
{"created":"2023-08-22 15:59:25","title":"A free from local minima algorithm for training regressive MLP neural networks","abstract":"In this article an innovative method for training regressive MLP networks is presented, which is not subject to local minima. The Error-Back-Propagation algorithm, proposed by William-Hinton-Rummelhart, has had the merit of favouring the development of machine learning techniques, which has permeated every branch of research and technology since the mid-1980s. This extraordinary success is largely due to the black-box approach, but this same factor was also seen as a limitation, as soon more challenging problems were approached. One of the most critical aspects of the training algorithms was that of local minima of the loss function, typically the mean squared error of the output on the training set. In fact, as the most popular training algorithms are driven by the derivatives of the loss function, there is no possibility to evaluate if a reached minimum is local or global. The algorithm presented in this paper avoids the problem of local minima, as the training is based on the properties of the distribution of the training set, or better on its image internal to the neural network. The performance of the algorithm is shown for a well-known benchmark.","sentences":["In this article an innovative method for training regressive MLP networks is presented, which is not subject to local minima.","The Error-Back-Propagation algorithm, proposed by William-Hinton-Rummelhart, has had the merit of favouring the development of machine learning techniques, which has permeated every branch of research and technology since the mid-1980s.","This extraordinary success is largely due to the black-box approach, but this same factor was also seen as a limitation, as soon more challenging problems were approached.","One of the most critical aspects of the training algorithms was that of local minima of the loss function, typically the mean squared error of the output on the training set.","In fact, as the most popular training algorithms are driven by the derivatives of the loss function, there is no possibility to evaluate if a reached minimum is local or global.","The algorithm presented in this paper avoids the problem of local minima, as the training is based on the properties of the distribution of the training set, or better on its image internal to the neural network.","The performance of the algorithm is shown for a well-known benchmark."],"url":"http://arxiv.org/abs/2308.11532v1"}
{"created":"2023-08-22 15:59:21","title":"Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law","abstract":"Our project aims at helping and supporting stakeholders in refugee status adjudications, such as lawyers, judges, governing bodies, and claimants, in order to make better decisions through data-driven intelligence and increase the understanding and transparency of the refugee application process for all involved parties. This PhD project has two primary objectives: (1) to retrieve past cases, and (2) to analyze legal decision-making processes on a dataset of Canadian cases. In this paper, we present the current state of our work, which includes a completed experiment on part (1) and ongoing efforts related to part (2). We believe that NLP-based solutions are well-suited to address these challenges, and we investigate the feasibility of automating all steps involved. In addition, we introduce a novel benchmark for future NLP research in refugee law. Our methodology aims to be inclusive to all end-users and stakeholders, with expected benefits including reduced time-to-decision, fairer and more transparent outcomes, and improved decision quality.","sentences":["Our project aims at helping and supporting stakeholders in refugee status adjudications, such as lawyers, judges, governing bodies, and claimants, in order to make better decisions through data-driven intelligence and increase the understanding and transparency of the refugee application process for all involved parties.","This PhD project has two primary objectives: (1) to retrieve past cases, and (2) to analyze legal decision-making processes on a dataset of Canadian cases.","In this paper, we present the current state of our work, which includes a completed experiment on part (1) and ongoing efforts related to part (2).","We believe that NLP-based solutions are well-suited to address these challenges, and we investigate the feasibility of automating all steps involved.","In addition, we introduce a novel benchmark for future NLP research in refugee law.","Our methodology aims to be inclusive to all end-users and stakeholders, with expected benefits including reduced time-to-decision, fairer and more transparent outcomes, and improved decision quality."],"url":"http://arxiv.org/abs/2308.11531v1"}
{"created":"2023-08-22 15:59:06","title":"Furnishing Sound Event Detection with Language Model Abilities","abstract":"Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification. Evaluation results show that the proposed method achieves accurate sequences of sound event detection.","sentences":["Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality.","In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain.","Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location.","The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic.","Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences.","We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification.","Evaluation results show that the proposed method achieves accurate sequences of sound event detection."],"url":"http://arxiv.org/abs/2308.11530v1"}
{"created":"2023-08-22 15:56:40","title":"Redistricting for Proportionality","abstract":"American democracy is currently heavily reliant on plurality in single-member districts, or PSMD, as a system of election. But public perceptions of fairness are often keyed to partisan proportionality, or the degree of congruence between each party's share of the the vote and its share of representation. PSMD has not tended to secure proportional outcomes historically, partially due to gerrymandering, where line-drawers intentionally extract more advantage for their side. But it is now increasingly clear that even blind PSMD is frequently disproportional, and in unpredictable ways that depend on local political geography. In this paper we consider whether it is feasible to bring PSMD into alignment with a proportionality norm by targeting proportional outcomes in the design and selection of districts. We do this mainly through a close examination of the \"Freedom to Vote Test,\" a redistricting reform proposed in draft legislation in 2021. We find that applying the test with a proportionality target makes for sound policy: it performs well in legal battleground states and has a workable exception to handle edge cases where proportionality is out of reach.","sentences":["American democracy is currently heavily reliant on plurality in single-member districts, or PSMD, as a system of election.","But public perceptions of fairness are often keyed to partisan proportionality, or the degree of congruence between each party's share of the the vote and its share of representation.","PSMD has not tended to secure proportional outcomes historically, partially due to gerrymandering, where line-drawers intentionally extract more advantage for their side.","But it is now increasingly clear that even blind PSMD is frequently disproportional, and in unpredictable ways that depend on local political geography.","In this paper we consider whether it is feasible to bring PSMD into alignment with a proportionality norm by targeting proportional outcomes in the design and selection of districts.","We do this mainly through a close examination of the \"Freedom to Vote Test,\" a redistricting reform proposed in draft legislation in 2021.","We find that applying the test with a proportionality target makes for sound policy: it performs well in legal battleground states and has a workable exception to handle edge cases where proportionality is out of reach."],"url":"http://arxiv.org/abs/2308.11529v1"}
{"created":"2023-08-22 15:52:37","title":"ReLiCADA -- Reservoir Computing using Linear Cellular Automata Design Algorithm","abstract":"In this paper, we present a novel algorithm to optimize the design of Reservoir Computing using Cellular Automata models for time series applications. Besides selecting the models' hyperparameters, the proposed algorithm particularly solves the open problem of linear Cellular Automaton rule selection. The selection method pre-selects only a few promising candidate rules out of an exponentially growing rule space. When applied to relevant benchmark datasets, the selected rules achieve low errors, with the best rules being among the top 5% of the overall rule space. The algorithm was developed based on mathematical analysis of linear Cellular Automaton properties and is backed by almost one million experiments, adding up to a computational runtime of nearly one year. Comparisons to other state-of-the-art time series models show that the proposed Reservoir Computing using Cellular Automata models have lower computational complexity, at the same time, achieve lower errors. Hence, our approach reduces the time needed for training and hyperparameter optimization by up to several orders of magnitude.","sentences":["In this paper, we present a novel algorithm to optimize the design of Reservoir Computing using Cellular Automata models for time series applications.","Besides selecting the models' hyperparameters, the proposed algorithm particularly solves the open problem of linear Cellular Automaton rule selection.","The selection method pre-selects only a few promising candidate rules out of an exponentially growing rule space.","When applied to relevant benchmark datasets, the selected rules achieve low errors, with the best rules being among the top 5% of the overall rule space.","The algorithm was developed based on mathematical analysis of linear Cellular Automaton properties and is backed by almost one million experiments, adding up to a computational runtime of nearly one year.","Comparisons to other state-of-the-art time series models show that the proposed Reservoir Computing using Cellular Automata models have lower computational complexity, at the same time, achieve lower errors.","Hence, our approach reduces the time needed for training and hyperparameter optimization by up to several orders of magnitude."],"url":"http://arxiv.org/abs/2308.11522v1"}
{"created":"2023-08-22 15:47:58","title":"EM for Mixture of Linear Regression with Clustered Data","abstract":"Modern data-driven and distributed learning frameworks deal with diverse massive data generated by clients spread across heterogeneous environments. Indeed, data heterogeneity is a major bottleneck in scaling up many distributed learning paradigms. In many settings however, heterogeneous data may be generated in clusters with shared structures, as is the case in several applications such as federated learning where a common latent variable governs the distribution of all the samples generated by a client. It is therefore natural to ask how the underlying clustered structures in distributed data can be exploited to improve learning schemes. In this paper, we tackle this question in the special case of estimating $d$-dimensional parameters of a two-component mixture of linear regressions problem where each of $m$ nodes generates $n$ samples with a shared latent variable. We employ the well-known Expectation-Maximization (EM) method to estimate the maximum likelihood parameters from $m$ batches of dependent samples each containing $n$ measurements. Discarding the clustered structure in the mixture model, EM is known to require $O(\\log(mn/d))$ iterations to reach the statistical accuracy of $O(\\sqrt{d/(mn)})$. In contrast, we show that if initialized properly, EM on the structured data requires only $O(1)$ iterations to reach the same statistical accuracy, as long as $m$ grows up as $e^{o(n)}$. Our analysis establishes and combines novel asymptotic optimization and generalization guarantees for population and empirical EM with dependent samples, which may be of independent interest.","sentences":["Modern data-driven and distributed learning frameworks deal with diverse massive data generated by clients spread across heterogeneous environments.","Indeed, data heterogeneity is a major bottleneck in scaling up many distributed learning paradigms.","In many settings however, heterogeneous data may be generated in clusters with shared structures, as is the case in several applications such as federated learning where a common latent variable governs the distribution of all the samples generated by a client.","It is therefore natural to ask how the underlying clustered structures in distributed data can be exploited to improve learning schemes.","In this paper, we tackle this question in the special case of estimating $d$-dimensional parameters of a two-component mixture of linear regressions problem where each of $m$ nodes generates $n$ samples with a shared latent variable.","We employ the well-known Expectation-Maximization (EM) method to estimate the maximum likelihood parameters from $m$ batches of dependent samples each containing $n$ measurements.","Discarding the clustered structure in the mixture model, EM is known to require $O(\\log(mn/d))$ iterations to reach the statistical accuracy of $O(\\sqrt{d/(mn)})$.","In contrast, we show that if initialized properly, EM on the structured data requires only $O(1)$ iterations to reach the same statistical accuracy, as long as $m$ grows up as $e^{o(n)}$. Our analysis establishes and combines novel asymptotic optimization and generalization guarantees for population and empirical EM with dependent samples, which may be of independent interest."],"url":"http://arxiv.org/abs/2308.11518v1"}
{"created":"2023-08-22 15:40:03","title":"TrackFlow: Multi-Object Tracking with Normalizing Flows","abstract":"The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a candidate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations. Our experiments, conducted on both simulated and real benchmarks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms.","sentences":["The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches.","In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates.","More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU).","To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost.","However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality.","We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a candidate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations.","Our experiments, conducted on both simulated and real benchmarks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms."],"url":"http://arxiv.org/abs/2308.11513v1"}
{"created":"2023-08-22 15:39:47","title":"L^2R: Lifelong Learning for First-stage Retrieval with Backward-Compatible Representations","abstract":"First-stage retrieval is a critical task that aims to retrieve relevant document candidates from a large-scale collection. While existing retrieval models have achieved impressive performance, they are mostly studied on static data sets, ignoring that in the real-world, the data on the Web is continuously growing with potential distribution drift. Consequently, retrievers trained on static old data may not suit new-coming data well and inevitably produce sub-optimal results. In this work, we study lifelong learning for first-stage retrieval, especially focusing on the setting where the emerging documents are unlabeled since relevance annotation is expensive and may not keep up with data emergence. Under this setting, we aim to develop model updating with two goals: (1) to effectively adapt to the evolving distribution with the unlabeled new-coming data, and (2) to avoid re-inferring all embeddings of old documents to efficiently update the index each time the model is updated.   We first formalize the task and then propose a novel Lifelong Learning method for the first-stage Retrieval, namely L^2R. L^2R adopts the typical memory mechanism for lifelong learning, and incorporates two crucial components: (1) selecting diverse support negatives for model training and memory updating for effective model adaptation, and (2) a ranking alignment objective to ensure the backward-compatibility of representations to save the cost of index rebuilding without hurting the model performance. For evaluation, we construct two new benchmarks from LoTTE and Multi-CPR datasets to simulate the document distribution drift in realistic retrieval scenarios. Extensive experiments show that L^2R significantly outperforms competitive lifelong learning baselines.","sentences":["First-stage retrieval is a critical task that aims to retrieve relevant document candidates from a large-scale collection.","While existing retrieval models have achieved impressive performance, they are mostly studied on static data sets, ignoring that in the real-world, the data on the Web is continuously growing with potential distribution drift.","Consequently, retrievers trained on static old data may not suit new-coming data well and inevitably produce sub-optimal results.","In this work, we study lifelong learning for first-stage retrieval, especially focusing on the setting where the emerging documents are unlabeled since relevance annotation is expensive and may not keep up with data emergence.","Under this setting, we aim to develop model updating with two goals: (1) to effectively adapt to the evolving distribution with the unlabeled new-coming data, and (2) to avoid re-inferring all embeddings of old documents to efficiently update the index each time the model is updated.   ","We first formalize the task and then propose a novel Lifelong Learning method for the first-stage Retrieval, namely L^2R. L^2R adopts the typical memory mechanism for lifelong learning, and incorporates two crucial components: (1) selecting diverse support negatives for model training and memory updating for effective model adaptation, and (2) a ranking alignment objective to ensure the backward-compatibility of representations to save the cost of index rebuilding without hurting the model performance.","For evaluation, we construct two new benchmarks from LoTTE and Multi-CPR datasets to simulate the document distribution drift in realistic retrieval scenarios.","Extensive experiments show that L^2R significantly outperforms competitive lifelong learning baselines."],"url":"http://arxiv.org/abs/2308.11512v1"}
{"created":"2023-08-22 15:39:29","title":"Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models","abstract":"We explore element-wise convex combinations of two permutation-aligned neural network parameter vectors $\\Theta_A$ and $\\Theta_B$ of size $d$. We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its vicinity. Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability. We also make several novel observations regarding linear mode connectivity and model re-basin. We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model. Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are non-vacuous in the sense that there are significant functional differences between the resulting models.","sentences":["We explore element-wise convex combinations of two permutation-aligned neural network parameter vectors $\\Theta_A$ and $\\Theta_B$ of size $d$. We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its vicinity.","Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability.","We also make several novel observations regarding linear mode connectivity and model re-basin.","We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model.","Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are non-vacuous in the sense that there are significant functional differences between the resulting models."],"url":"http://arxiv.org/abs/2308.11511v1"}
{"created":"2023-08-22 15:38:39","title":"SwinFace: A Multi-task Transformer for Face Recognition, Expression Recognition, Age Estimation and Attribute Estimation","abstract":"In recent years, vision transformers have been introduced into face recognition and analysis and have achieved performance breakthroughs. However, most previous methods generally train a single model or an ensemble of models to perform the desired task, which ignores the synergy among different tasks and fails to achieve improved prediction accuracy, increased data efficiency, and reduced training time. This paper presents a multi-purpose algorithm for simultaneous face recognition, facial expression recognition, age estimation, and face attribute estimation (40 attributes including gender) based on a single Swin Transformer. Our design, the SwinFace, consists of a single shared backbone together with a subnet for each set of related tasks. To address the conflicts among multiple tasks and meet the different demands of tasks, a Multi-Level Channel Attention (MLCA) module is integrated into each task-specific analysis subnet, which can adaptively select the features from optimal levels and channels to perform the desired tasks. Extensive experiments show that the proposed model has a better understanding of the face and achieves excellent performance for all tasks. Especially, it achieves 90.97% accuracy on RAF-DB and 0.22 $\\epsilon$-error on CLAP2015, which are state-of-the-art results on facial expression recognition and age estimation respectively. The code and models will be made publicly available at https://github.com/lxq1000/SwinFace.","sentences":["In recent years, vision transformers have been introduced into face recognition and analysis and have achieved performance breakthroughs.","However, most previous methods generally train a single model or an ensemble of models to perform the desired task, which ignores the synergy among different tasks and fails to achieve improved prediction accuracy, increased data efficiency, and reduced training time.","This paper presents a multi-purpose algorithm for simultaneous face recognition, facial expression recognition, age estimation, and face attribute estimation (40 attributes including gender) based on a single Swin Transformer.","Our design, the SwinFace, consists of a single shared backbone together with a subnet for each set of related tasks.","To address the conflicts among multiple tasks and meet the different demands of tasks, a Multi-Level Channel Attention (MLCA) module is integrated into each task-specific analysis subnet, which can adaptively select the features from optimal levels and channels to perform the desired tasks.","Extensive experiments show that the proposed model has a better understanding of the face and achieves excellent performance for all tasks.","Especially, it achieves 90.97% accuracy on RAF-DB and 0.22 $\\epsilon$-error on CLAP2015, which are state-of-the-art results on facial expression recognition and age estimation respectively.","The code and models will be made publicly available at https://github.com/lxq1000/SwinFace."],"url":"http://arxiv.org/abs/2308.11509v1"}
{"created":"2023-08-22 15:28:49","title":"Unsupervised Prototype Adapter for Vision-Language Models","abstract":"Recently, large-scale pre-trained vision-language models (e.g. CLIP and ALIGN) have demonstrated remarkable effectiveness in acquiring transferable visual representations. To leverage the valuable knowledge encoded within these models for downstream tasks, several fine-tuning approaches, including prompt tuning methods and adapter-based methods, have been developed to adapt vision-language models effectively with supervision. However, these methods rely on the availability of annotated samples, which can be labor-intensive and time-consuming to acquire, thus limiting scalability. To address this issue, in this work, we design an unsupervised fine-tuning approach for vision-language models called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for the unannotated target datasets, we leverage the text-image aligning capability of CLIP to automatically select the most confident samples for each class. Utilizing these selected samples, we generate class prototypes, which serve as the initialization for the learnable prototype model. After fine-tuning, the prototype model prediction is combined with the original CLIP's prediction by a residual connection to perform downstream recognition tasks. Our extensive experimental results on image recognition and domain generalization show that the proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter, and also the state-of-the-art UPL method by large margins.","sentences":["Recently, large-scale pre-trained vision-language models (e.g. CLIP and ALIGN) have demonstrated remarkable effectiveness in acquiring transferable visual representations.","To leverage the valuable knowledge encoded within these models for downstream tasks, several fine-tuning approaches, including prompt tuning methods and adapter-based methods, have been developed to adapt vision-language models effectively with supervision.","However, these methods rely on the availability of annotated samples, which can be labor-intensive and time-consuming to acquire, thus limiting scalability.","To address this issue, in this work, we design an unsupervised fine-tuning approach for vision-language models called Unsupervised Prototype Adapter (UP-Adapter).","Specifically, for the unannotated target datasets, we leverage the text-image aligning capability of CLIP to automatically select the most confident samples for each class.","Utilizing these selected samples, we generate class prototypes, which serve as the initialization for the learnable prototype model.","After fine-tuning, the prototype model prediction is combined with the original CLIP's prediction by a residual connection to perform downstream recognition tasks.","Our extensive experimental results on image recognition and domain generalization show that the proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter, and also the state-of-the-art UPL method by large margins."],"url":"http://arxiv.org/abs/2308.11507v1"}
{"created":"2023-08-22 15:27:52","title":"LCCo: Lending CLIP to Co-Segmentation","abstract":"This paper studies co-segmenting the common semantic object in a set of images. Existing works either rely on carefully engineered networks to mine the implicit semantic information in visual features or require extra data (i.e., classification labels) for training. In this paper, we leverage the contrastive language-image pre-training framework (CLIP) for the task. With a backbone segmentation network that independently processes each image from the set, we introduce semantics from CLIP into the backbone features, refining them in a coarse-to-fine manner with three key modules: i) an image set feature correspondence module, encoding global consistent semantic information of the image set; ii) a CLIP interaction module, using CLIP-mined common semantics of the image set to refine the backbone feature; iii) a CLIP regularization module, drawing CLIP towards this co-segmentation task, identifying the best CLIP semantic and using it to regularize the backbone feature. Experiments on four standard co-segmentation benchmark datasets show that the performance of our method outperforms state-of-the-art methods.","sentences":["This paper studies co-segmenting the common semantic object in a set of images.","Existing works either rely on carefully engineered networks to mine the implicit semantic information in visual features or require extra data (i.e., classification labels) for training.","In this paper, we leverage the contrastive language-image pre-training framework (CLIP) for the task.","With a backbone segmentation network that independently processes each image from the set, we introduce semantics from CLIP into the backbone features, refining them in a coarse-to-fine manner with three key modules: i) an image set feature correspondence module, encoding global consistent semantic information of the image set; ii) a CLIP interaction module, using CLIP-mined common semantics of the image set to refine the backbone feature; iii) a CLIP regularization module, drawing CLIP towards this co-segmentation task, identifying the best CLIP semantic and using it to regularize the backbone feature.","Experiments on four standard co-segmentation benchmark datasets show that the performance of our method outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2308.11506v1"}
{"created":"2023-08-22 15:20:26","title":"Four years of multi-modal odometry and mapping on the rail vehicles","abstract":"Precise, seamless, and efficient train localization as well as long-term railway environment monitoring is the essential property towards reliability, availability, maintainability, and safety (RAMS) engineering for railroad systems. Simultaneous localization and mapping (SLAM) is right at the core of solving the two problems concurrently. In this end, we propose a high-performance and versatile multi-modal framework in this paper, targeted for the odometry and mapping task for various rail vehicles. Our system is built atop an inertial-centric state estimator that tightly couples light detection and ranging (LiDAR), visual, optionally satellite navigation and map-based localization information with the convenience and extendibility of loosely coupled methods. The inertial sensors IMU and wheel encoder are treated as the primary sensor, which achieves the observations from subsystems to constrain the accelerometer and gyroscope biases. Compared to point-only LiDAR-inertial methods, our approach leverages more geometry information by introducing both track plane and electric power pillars into state estimation. The Visual-inertial subsystem also utilizes the environmental structure information by employing both lines and points. Besides, the method is capable of handling sensor failures by automatic reconfiguration bypassing failure modules. Our proposed method has been extensively tested in the long-during railway environments over four years, including general-speed, high-speed and metro, both passenger and freight traffic are investigated. Further, we aim to share, in an open way, the experience, problems, and successes of our group with the robotics community so that those that work in such environments can avoid these errors. In this view, we open source some of the datasets to benefit the research community.","sentences":["Precise, seamless, and efficient train localization as well as long-term railway environment monitoring is the essential property towards reliability, availability, maintainability, and safety (RAMS) engineering for railroad systems.","Simultaneous localization and mapping (SLAM) is right at the core of solving the two problems concurrently.","In this end, we propose a high-performance and versatile multi-modal framework in this paper, targeted for the odometry and mapping task for various rail vehicles.","Our system is built atop an inertial-centric state estimator that tightly couples light detection and ranging (LiDAR), visual, optionally satellite navigation and map-based localization information with the convenience and extendibility of loosely coupled methods.","The inertial sensors IMU and wheel encoder are treated as the primary sensor, which achieves the observations from subsystems to constrain the accelerometer and gyroscope biases.","Compared to point-only LiDAR-inertial methods, our approach leverages more geometry information by introducing both track plane and electric power pillars into state estimation.","The Visual-inertial subsystem also utilizes the environmental structure information by employing both lines and points.","Besides, the method is capable of handling sensor failures by automatic reconfiguration bypassing failure modules.","Our proposed method has been extensively tested in the long-during railway environments over four years, including general-speed, high-speed and metro, both passenger and freight traffic are investigated.","Further, we aim to share, in an open way, the experience, problems, and successes of our group with the robotics community so that those that work in such environments can avoid these errors.","In this view, we open source some of the datasets to benefit the research community."],"url":"http://arxiv.org/abs/2308.11501v1"}
{"created":"2023-08-22 15:14:40","title":"A LiDAR-Inertial SLAM Tightly-Coupled with Dropout-Tolerant GNSS Fusion for Autonomous Mine Service Vehicles","abstract":"Multi-modal sensor integration has become a crucial prerequisite for the real-world navigation systems. Recent studies have reported successful deployment of such system in many fields. However, it is still challenging for navigation tasks in mine scenes due to satellite signal dropouts, degraded perception, and observation degeneracy. To solve this problem, we propose a LiDAR-inertial odometry method in this paper, utilizing both Kalman filter and graph optimization. The front-end consists of multiple parallel running LiDAR-inertial odometries, where the laser points, IMU, and wheel odometer information are tightly fused in an error-state Kalman filter. Instead of the commonly used feature points, we employ surface elements for registration. The back-end construct a pose graph and jointly optimize the pose estimation results from inertial, LiDAR odometry, and global navigation satellite system (GNSS). Since the vehicle has a long operation time inside the tunnel, the largely accumulated drift may be not fully by the GNSS measurements. We hereby leverage a loop closure based re-initialization process to achieve full alignment. In addition, the system robustness is improved through handling data loss, stream consistency, and estimation error. The experimental results show that our system has a good tolerance to the long-period degeneracy with the cooperation different LiDARs and surfel registration, achieving meter-level accuracy even for tens of minutes running during GNSS dropouts.","sentences":["Multi-modal sensor integration has become a crucial prerequisite for the real-world navigation systems.","Recent studies have reported successful deployment of such system in many fields.","However, it is still challenging for navigation tasks in mine scenes due to satellite signal dropouts, degraded perception, and observation degeneracy.","To solve this problem, we propose a LiDAR-inertial odometry method in this paper, utilizing both Kalman filter and graph optimization.","The front-end consists of multiple parallel running LiDAR-inertial odometries, where the laser points, IMU, and wheel odometer information are tightly fused in an error-state Kalman filter.","Instead of the commonly used feature points, we employ surface elements for registration.","The back-end construct a pose graph and jointly optimize the pose estimation results from inertial, LiDAR odometry, and global navigation satellite system (GNSS).","Since the vehicle has a long operation time inside the tunnel, the largely accumulated drift may be not fully by the GNSS measurements.","We hereby leverage a loop closure based re-initialization process to achieve full alignment.","In addition, the system robustness is improved through handling data loss, stream consistency, and estimation error.","The experimental results show that our system has a good tolerance to the long-period degeneracy with the cooperation different LiDARs and surfel registration, achieving meter-level accuracy even for tens of minutes running during GNSS dropouts."],"url":"http://arxiv.org/abs/2308.11492v1"}
{"created":"2023-08-22 15:10:45","title":"Can Authorship Representation Learning Capture Stylistic Features?","abstract":"Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the surrogate authorship prediction task are indeed sensitive to writing style. As a consequence, authorship representations may be expected to be robust to certain kinds of data shift, such as topic drift over time. Additionally, our findings may open the door to downstream applications that require stylistic representations, such as style transfer.","sentences":["Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics.","At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content.","However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic.","In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments.","The results of these experiments suggest that representations learned for the surrogate authorship prediction task are indeed sensitive to writing style.","As a consequence, authorship representations may be expected to be robust to certain kinds of data shift, such as topic drift over time.","Additionally, our findings may open the door to downstream applications that require stylistic representations, such as style transfer."],"url":"http://arxiv.org/abs/2308.11490v1"}
{"created":"2023-08-22 15:10:42","title":"Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition","abstract":"We are concerned with a challenging scenario in unpaired multiview video learning. In this case, the model aims to learn comprehensive multiview representations while the cross-view semantic information exhibits variations. We propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle this unpaired multiview learning problem. The key idea is to build cross-view pseudo-pairs and do view-invariant alignment by leveraging the semantic information of videos. To facilitate the data efficiency of multiview learning, we further perform video-text alignment for first-person and third-person videos, to fully leverage the semantic knowledge to improve video representations. Extensive experiments on multiple benchmark datasets verify the effectiveness of our framework. Our method also outperforms multiple existing view-alignment methods, under the more challenging scenario than typical paired or unpaired multimodal or multiview learning. Our code is available at https://github.com/wqtwjt1996/SUM-L.","sentences":["We are concerned with a challenging scenario in unpaired multiview video learning.","In this case, the model aims to learn comprehensive multiview representations while the cross-view semantic information exhibits variations.","We propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle this unpaired multiview learning problem.","The key idea is to build cross-view pseudo-pairs and do view-invariant alignment by leveraging the semantic information of videos.","To facilitate the data efficiency of multiview learning, we further perform video-text alignment for first-person and third-person videos, to fully leverage the semantic knowledge to improve video representations.","Extensive experiments on multiple benchmark datasets verify the effectiveness of our framework.","Our method also outperforms multiple existing view-alignment methods, under the more challenging scenario than typical paired or unpaired multimodal or multiview learning.","Our code is available at https://github.com/wqtwjt1996/SUM-L."],"url":"http://arxiv.org/abs/2308.11489v1"}
{"created":"2023-08-22 15:08:02","title":"Opening the Vocabulary of Egocentric Actions","abstract":"Human actions in egocentric videos are often hand-object interactions composed from a verb (performed by the hand) applied to an object. Despite their extensive scaling up, egocentric datasets still face two limitations - sparsity of action compositions and a closed set of interacting objects. This paper proposes a novel open vocabulary action recognition task. Given a set of verbs and objects observed during training, the goal is to generalize the verbs to an open vocabulary of actions with seen and novel objects. To this end, we decouple the verb and object predictions via an object-agnostic verb encoder and a prompt-based object encoder. The prompting leverages CLIP representations to predict an open vocabulary of interacting objects. We create open vocabulary benchmarks on the EPIC-KITCHENS-100 and Assembly101 datasets; whereas closed-action methods fail to generalize, our proposed method is effective. In addition, our object encoder significantly outperforms existing open-vocabulary visual recognition methods in recognizing novel interacting objects.","sentences":["Human actions in egocentric videos are often hand-object interactions composed from a verb (performed by the hand) applied to an object.","Despite their extensive scaling up, egocentric datasets still face two limitations - sparsity of action compositions and a closed set of interacting objects.","This paper proposes a novel open vocabulary action recognition task.","Given a set of verbs and objects observed during training, the goal is to generalize the verbs to an open vocabulary of actions with seen and novel objects.","To this end, we decouple the verb and object predictions via an object-agnostic verb encoder and a prompt-based object encoder.","The prompting leverages CLIP representations to predict an open vocabulary of interacting objects.","We create open vocabulary benchmarks on the EPIC-KITCHENS-100 and Assembly101 datasets; whereas closed-action methods fail to generalize, our proposed method is effective.","In addition, our object encoder significantly outperforms existing open-vocabulary visual recognition methods in recognizing novel interacting objects."],"url":"http://arxiv.org/abs/2308.11488v1"}
{"created":"2023-08-22 15:06:14","title":"Free Lunch for Gait Recognition: A Novel Relation Descriptor","abstract":"Gait recognition is to seek correct matches for query individuals by their unique walking patterns at a long distance. However, current methods focus solely on individual gait features, disregarding inter-personal relationships. In this paper, we reconsider gait representation, asserting that gait is not just an aggregation of individual features, but also the relationships among different subjects' gait features once reference gaits are established. From this perspective, we redefine classifier weights as reference-anchored gaits, allowing each person's gait to be described by their relationship with these references. In our work, we call this novel descriptor Relationship Descriptor (RD). This Relationship Descriptor offers two benefits: emphasizing meaningful features and enhancing robustness. To be specific, The normalized dot product between gait features and classifier weights signifies a similarity relation, where each dimension indicates the similarity between the test sample and each training ID's gait prototype, respectively. Despite its potential, the direct use of relationship descriptors poses dimensionality challenges since the dimension of RD depends on the training set's identity count. To address this, we propose a Farthest Anchored gaits Selection algorithm and a dimension reduction method to boost gait recognition performance. Our method can be built on top of off-the-shelf pre-trained classification-based models without extra parameters. We show that RD achieves higher recognition performance than directly using extracted features. We evaluate the effectiveness of our method on the popular GREW, Gait3D, CASIA-B, and OU-MVLP, showing that our method consistently outperforms the baselines and achieves state-of-the-art performances.","sentences":["Gait recognition is to seek correct matches for query individuals by their unique walking patterns at a long distance.","However, current methods focus solely on individual gait features, disregarding inter-personal relationships.","In this paper, we reconsider gait representation, asserting that gait is not just an aggregation of individual features, but also the relationships among different subjects' gait features once reference gaits are established.","From this perspective, we redefine classifier weights as reference-anchored gaits, allowing each person's gait to be described by their relationship with these references.","In our work, we call this novel descriptor Relationship Descriptor (RD).","This Relationship Descriptor offers two benefits: emphasizing meaningful features and enhancing robustness.","To be specific, The normalized dot product between gait features and classifier weights signifies a similarity relation, where each dimension indicates the similarity between the test sample and each training ID's gait prototype, respectively.","Despite its potential, the direct use of relationship descriptors poses dimensionality challenges since the dimension of RD depends on the training set's identity count.","To address this, we propose a Farthest Anchored gaits Selection algorithm and a dimension reduction method to boost gait recognition performance.","Our method can be built on top of off-the-shelf pre-trained classification-based models without extra parameters.","We show that RD achieves higher recognition performance than directly using extracted features.","We evaluate the effectiveness of our method on the popular GREW, Gait3D, CASIA-B, and OU-MVLP, showing that our method consistently outperforms the baselines and achieves state-of-the-art performances."],"url":"http://arxiv.org/abs/2308.11487v1"}
{"created":"2023-08-22 15:03:16","title":"Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features","abstract":"Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir","sentences":["Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption.","Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task.","We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features.","Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval.","We use contrastive learning in both stages of training.","Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval.","Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir"],"url":"http://arxiv.org/abs/2308.11485v1"}
{"created":"2023-08-22 14:59:17","title":"Pose2Gait: Extracting Gait Features from Monocular Video of Individuals with Dementia","abstract":"Video-based ambient monitoring of gait for older adults with dementia has the potential to detect negative changes in health and allow clinicians and caregivers to intervene early to prevent falls or hospitalizations. Computer vision-based pose tracking models can process video data automatically and extract joint locations; however, publicly available models are not optimized for gait analysis on older adults or clinical populations. In this work we train a deep neural network to map from a two dimensional pose sequence, extracted from a video of an individual walking down a hallway toward a wall-mounted camera, to a set of three-dimensional spatiotemporal gait features averaged over the walking sequence. The data of individuals with dementia used in this work was captured at two sites using a wall-mounted system to collect the video and depth information used to train and evaluate our model. Our Pose2Gait model is able to extract velocity and step length values from the video that are correlated with the features from the depth camera, with Spearman's correlation coefficients of .83 and .60 respectively, showing that three dimensional spatiotemporal features can be predicted from monocular video. Future work remains to improve the accuracy of other features, such as step time and step width, and test the utility of the predicted values for detecting meaningful changes in gait during longitudinal ambient monitoring.","sentences":["Video-based ambient monitoring of gait for older adults with dementia has the potential to detect negative changes in health and allow clinicians and caregivers to intervene early to prevent falls or hospitalizations.","Computer vision-based pose tracking models can process video data automatically and extract joint locations; however, publicly available models are not optimized for gait analysis on older adults or clinical populations.","In this work we train a deep neural network to map from a two dimensional pose sequence, extracted from a video of an individual walking down a hallway toward a wall-mounted camera, to a set of three-dimensional spatiotemporal gait features averaged over the walking sequence.","The data of individuals with dementia used in this work was captured at two sites using a wall-mounted system to collect the video and depth information used to train and evaluate our model.","Our Pose2Gait model is able to extract velocity and step length values from the video that are correlated with the features from the depth camera, with Spearman's correlation coefficients of .83 and .60 respectively, showing that three dimensional spatiotemporal features can be predicted from monocular video.","Future work remains to improve the accuracy of other features, such as step time and step width, and test the utility of the predicted values for detecting meaningful changes in gait during longitudinal ambient monitoring."],"url":"http://arxiv.org/abs/2308.11484v1"}
{"created":"2023-08-22 14:54:59","title":"Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks.","However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models.","As these models become more powerful, it becomes imperative to understand and address these limitations.","In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs.","Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting.","Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias.","We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement.","We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options.","Conversely, to mitigate bias, we recommend placing these choices among the adjacent options.","To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks."],"url":"http://arxiv.org/abs/2308.11483v1"}
{"created":"2023-08-22 14:52:44","title":"Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection","abstract":"Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a generative model of existing detection scores with a Gaussian mixture. By doing so, we present an ensemble approach that offers a more consistent and comprehensive solution for broad OOD detection, demonstrating superior performance compared to existing methods. Our code to download BROAD and reproduce our experiments is publicly available.","sentences":["Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs.","However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts.","This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs.","In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them.","We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity).","Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts.","In other words, they only reliably detect unexpected inputs that they have been specifically designed to expect.","As a first step toward broad OOD detection, we learn a generative model of existing detection scores with a Gaussian mixture.","By doing so, we present an ensemble approach that offers a more consistent and comprehensive solution for broad OOD detection, demonstrating superior performance compared to existing methods.","Our code to download BROAD and reproduce our experiments is publicly available."],"url":"http://arxiv.org/abs/2308.11480v1"}
{"created":"2023-08-22 14:44:16","title":"Towards Autonomous Excavation Planning","abstract":"Excavation plans are crucial in construction projects, dictating the dirt disposal strategy and excavation sequence based on the final geometry and machinery available. While most construction processes rely heavily on coarse sequence planning and local execution planning driven by human expertise and intuition, fully automated planning tools are notably absent from the industry. This paper introduces a fully autonomous excavation planning system. Initially, the site is mapped, followed by user selection of the desired excavation geometry. The system then invokes a global planner to determine the sequence of poses for the excavator, ensuring complete site coverage. For each pose, a local excavation planner decides how to move the soil around the machine, and a digging planner subsequently dictates the sequence of digging trajectories to complete a patch. We showcased our system by autonomously excavating the largest pit documented so far, achieving an average digging cycle time of roughly 30 seconds, comparable to the one of a human operator.","sentences":["Excavation plans are crucial in construction projects, dictating the dirt disposal strategy and excavation sequence based on the final geometry and machinery available.","While most construction processes rely heavily on coarse sequence planning and local execution planning driven by human expertise and intuition, fully automated planning tools are notably absent from the industry.","This paper introduces a fully autonomous excavation planning system.","Initially, the site is mapped, followed by user selection of the desired excavation geometry.","The system then invokes a global planner to determine the sequence of poses for the excavator, ensuring complete site coverage.","For each pose, a local excavation planner decides how to move the soil around the machine, and a digging planner subsequently dictates the sequence of digging trajectories to complete a patch.","We showcased our system by autonomously excavating the largest pit documented so far, achieving an average digging cycle time of roughly 30 seconds, comparable to the one of a human operator."],"url":"http://arxiv.org/abs/2308.11478v1"}
{"created":"2023-08-22 14:43:36","title":"Revisiting column-generation-based matheuristic for learning classification trees","abstract":"Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \\cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their corresponding constraints. We conclude by presenting computational results that show that these modifications result in better scalability.","sentences":["Decision trees are highly interpretable models for solving classification problems in machine learning (ML).","The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy.","Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets.","\\cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees.","This approach improves scalability and can work with large datasets.","In this paper, we describe improvements to this column generation approach.","First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances.","Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes.","Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their corresponding constraints.","We conclude by presenting computational results that show that these modifications result in better scalability."],"url":"http://arxiv.org/abs/2308.11477v1"}
{"created":"2023-08-22 14:42:27","title":"Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect Dense Retrieval","abstract":"Grounded on pre-trained language models (PLMs), dense retrieval has been studied extensively on plain text. In contrast, there has been little research on retrieving data with multiple aspects using dense models. In the scenarios such as product search, the aspect information plays an essential role in relevance matching, e.g., category: Electronics, Computers, and Pet Supplies. A common way of leveraging aspect information for multi-aspect retrieval is to introduce an auxiliary classification objective, i.e., using item contents to predict the annotated value IDs of item aspects. However, by learning the value embeddings from scratch, this approach may not capture the various semantic similarities between the values sufficiently. To address this limitation, we leverage the aspect information as text strings rather than class IDs during pre-training so that their semantic similarities can be naturally captured in the PLMs. To facilitate effective retrieval with the aspect strings, we propose mutual prediction objectives between the text of the item aspect and content. In this way, our model makes more sufficient use of aspect information than conducting undifferentiated masked language modeling (MLM) on the concatenated text of aspects and content. Extensive experiments on two real-world datasets (product and mini-program search) show that our approach can outperform competitive baselines both treating aspect values as classes and conducting the same MLM for aspect and content strings. Code and related dataset will be available at the URL \\footnote{https://github.com/sunxiaojie99/ATTEMPT}.","sentences":["Grounded on pre-trained language models (PLMs), dense retrieval has been studied extensively on plain text.","In contrast, there has been little research on retrieving data with multiple aspects using dense models.","In the scenarios such as product search, the aspect information plays an essential role in relevance matching, e.g., category: Electronics, Computers, and Pet Supplies.","A common way of leveraging aspect information for multi-aspect retrieval is to introduce an auxiliary classification objective, i.e., using item contents to predict the annotated value IDs of item aspects.","However, by learning the value embeddings from scratch, this approach may not capture the various semantic similarities between the values sufficiently.","To address this limitation, we leverage the aspect information as text strings rather than class IDs during pre-training so that their semantic similarities can be naturally captured in the PLMs.","To facilitate effective retrieval with the aspect strings, we propose mutual prediction objectives between the text of the item aspect and content.","In this way, our model makes more sufficient use of aspect information than conducting undifferentiated masked language modeling (MLM) on the concatenated text of aspects and content.","Extensive experiments on two real-world datasets (product and mini-program search) show that our approach can outperform competitive baselines both treating aspect values as classes and conducting the same MLM for aspect and content strings.","Code and related dataset will be available at the URL \\footnote{https://github.com/sunxiaojie99/ATTEMPT}."],"url":"http://arxiv.org/abs/2308.11474v1"}
{"created":"2023-08-22 14:39:17","title":"IT3D: Improved Text-to-3D Generation with Explicit View Synthesis","abstract":"Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models. For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data. We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches.","sentences":["Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs).","Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs.","This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues.","Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models.","Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively.","To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models.","For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data.","We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches."],"url":"http://arxiv.org/abs/2308.11473v1"}
{"created":"2023-08-22 14:36:59","title":"Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)","abstract":"This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image segmentation, our findings demonstrate the system's capability to successfully execute landing maneuvers at altitudes as low as 20 meters. However, this approach is vulnerable to intermittent and occasionally abrupt fluctuations in the segmentation between frames in a video stream. To address this challenge, we enhance the image segmentation output by introducing what we call a dynamic focus: a masking mechanism that self adjusts according to the current landing stage. This dynamic focus guides the control system to avoid regions beyond the drone's safety radius projected onto the ground, thus mitigating the problems with fluctuations. Through the implementation of this supplementary layer, our experiments have reached improvements in the landing success rate of almost tenfold when compared to global segmentation. All the source code is open source and available online (github.com/MISTLab/DOVESEI).","sentences":["This work targets what we consider to be the foundational step for urban airborne robots, a safe landing.","Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation.","We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation.","This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology.","Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters.","This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras.","Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods.","Utilizing monocular cameras and image segmentation, our findings demonstrate the system's capability to successfully execute landing maneuvers at altitudes as low as 20 meters.","However, this approach is vulnerable to intermittent and occasionally abrupt fluctuations in the segmentation between frames in a video stream.","To address this challenge, we enhance the image segmentation output by introducing what we call a dynamic focus: a masking mechanism that self adjusts according to the current landing stage.","This dynamic focus guides the control system to avoid regions beyond the drone's safety radius projected onto the ground, thus mitigating the problems with fluctuations.","Through the implementation of this supplementary layer, our experiments have reached improvements in the landing success rate of almost tenfold when compared to global segmentation.","All the source code is open source and available online (github.com/MISTLab/DOVESEI)."],"url":"http://arxiv.org/abs/2308.11471v1"}
{"created":"2023-08-22 14:29:19","title":"Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms","abstract":"The aim of this work is to perform a multitemporal analysis using the Google Earth Engine (GEE) platform for the detection of changes in urban areas using optical data and specific machine learning (ML) algorithms. As a case study, Cairo City has been identified, in Egypt country, as one of the five most populous megacities of the last decade in the world. Classification and change detection analysis of the region of interest (ROI) have been carried out from July 2013 to July 2021. Results demonstrate the validity of the proposed method in identifying changed and unchanged urban areas over the selected period. Furthermore, this work aims to evidence the growing significance of GEE as an efficient cloud-based solution for managing large quantities of satellite data.","sentences":["The aim of this work is to perform a multitemporal analysis using the Google Earth Engine (GEE) platform for the detection of changes in urban areas using optical data and specific machine learning (ML) algorithms.","As a case study, Cairo City has been identified, in Egypt country, as one of the five most populous megacities of the last decade in the world.","Classification and change detection analysis of the region of interest (ROI) have been carried out from July 2013 to July 2021.","Results demonstrate the validity of the proposed method in identifying changed and unchanged urban areas over the selected period.","Furthermore, this work aims to evidence the growing significance of GEE as an efficient cloud-based solution for managing large quantities of satellite data."],"url":"http://arxiv.org/abs/2308.11468v1"}
{"created":"2023-08-22 14:25:15","title":"Sentence-Level Multimodal and Language-Agnostic Representations","abstract":"We introduce SONAR, a new multilingual and multimodal fixed-size sentence embedding space. Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. Speech segments can be embedded in the same SONAR embedding space using language-specific speech encoders trained in a teacher-student setting on speech transcription data. Our encoders outperform existing speech encoders on similarity search tasks. We also provide a text decoder for 200 languages, which allows us to perform text-to-text and speech-to-text machine translation, including for zero-shot language and modality combinations. Our text-to-text results are competitive compared to the state-of-the-art NLLB~1B model, despite the fixed-size bottleneck representation. Our zero-shot speech-to-text translation results compare favorably with strong supervised baselines such as Whisper.","sentences":["We introduce SONAR, a new multilingual and multimodal fixed-size sentence embedding space.","Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks.","Speech segments can be embedded in the same SONAR embedding space using language-specific speech encoders trained in a teacher-student setting on speech transcription data.","Our encoders outperform existing speech encoders on similarity search tasks.","We also provide a text decoder for 200 languages, which allows us to perform text-to-text and speech-to-text machine translation, including for zero-shot language and modality combinations.","Our text-to-text results are competitive compared to the state-of-the-art","NLLB~1B model, despite the fixed-size bottleneck representation.","Our zero-shot speech-to-text translation results compare favorably with strong supervised baselines such as Whisper."],"url":"http://arxiv.org/abs/2308.11466v1"}
{"created":"2023-08-22 14:23:21","title":"Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning","abstract":"Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods can be tailored to accommodate model-homogeneous FL methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their capabilities to handle the system heterogeneity. Copious experimental results validate the effectiveness of InCo Aggregation, spotlighting internal cross-layer gradients as a promising avenue to enhance the performance in heterogenous FL.","sentences":["Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios.","To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge.","In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities.","Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients.","Furthermore, our methods can be tailored to accommodate model-homogeneous FL methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their capabilities to handle the system heterogeneity.","Copious experimental results validate the effectiveness of InCo Aggregation, spotlighting internal cross-layer gradients as a promising avenue to enhance the performance in heterogenous FL."],"url":"http://arxiv.org/abs/2308.11464v1"}
{"created":"2023-08-22 14:13:51","title":"Sequencing Stochastic Jobs with a Single Sample","abstract":"This paper revisits the well known single machine scheduling problem to minimize total weighted completion times. The twist is that job sizes are stochastic from unknown distributions, and the scheduler has access to only a single sample from each of the distributions. For this restricted information regime, we analyze the simplest and probably only reasonable scheduling algorithm, namely to schedule by ordering the jobs by weight over sampled processing times. In general, this algorithm can be tricked by adversarial input distributions, performing in expectation arbitrarily worse even in comparison to choosing a random schedule. The paper suggests notions to capture the idea that this algorithm, on reasonable inputs, should exhibit a provably good expected performance. Specifically, we identify three natural classes of input distributions, such that for these classes, the algorithm performs better than random on any input.","sentences":["This paper revisits the well known single machine scheduling problem to minimize total weighted completion times.","The twist is that job sizes are stochastic from unknown distributions, and the scheduler has access to only a single sample from each of the distributions.","For this restricted information regime, we analyze the simplest and probably only reasonable scheduling algorithm, namely to schedule by ordering the jobs by weight over sampled processing times.","In general, this algorithm can be tricked by adversarial input distributions, performing in expectation arbitrarily worse even in comparison to choosing a random schedule.","The paper suggests notions to capture the idea that this algorithm, on reasonable inputs, should exhibit a provably good expected performance.","Specifically, we identify three natural classes of input distributions, such that for these classes, the algorithm performs better than random on any input."],"url":"http://arxiv.org/abs/2308.11461v1"}
{"created":"2023-08-22 14:05:43","title":"Deep learning-based denoising streamed from mobile phones improves speech-in-noise understanding for hearing aid users","abstract":"The hearing loss of almost half a billion people is commonly treated with hearing aids. However, current hearing aids often do not work well in real-world noisy environments. We present a deep learning based denoising system that runs in real time on iPhone 7 and Samsung Galaxy S10 (25ms algorithmic latency). The denoised audio is streamed to the hearing aid, resulting in a total delay of around 75ms. In tests with hearing aid users having moderate to severe hearing loss, our denoising system improves audio across three tests: 1) listening for subjective audio ratings, 2) listening for objective speech intelligibility, and 3) live conversations in a noisy environment for subjective ratings. Subjective ratings increase by more than 40%, for both the listening test and the live conversation compared to a fitted hearing aid as a baseline. Speech reception thresholds, measuring speech understanding in noise, improve by 1.6 dB SRT. Ours is the first denoising system that is implemented on a mobile device, streamed directly to users' hearing aids using only a single channel as audio input while improving user satisfaction on all tested aspects, including speech intelligibility. This includes overall preference of the denoised and streamed signal over the hearing aid, thereby accepting the higher latency for the significant improvement in speech understanding.","sentences":["The hearing loss of almost half a billion people is commonly treated with hearing aids.","However, current hearing aids often do not work well in real-world noisy environments.","We present a deep learning based denoising system that runs in real time on iPhone 7 and Samsung Galaxy S10 (25ms algorithmic latency).","The denoised audio is streamed to the hearing aid, resulting in a total delay of around 75ms.","In tests with hearing aid users having moderate to severe hearing loss, our denoising system improves audio across three tests: 1) listening for subjective audio ratings, 2) listening for objective speech intelligibility, and 3) live conversations in a noisy environment for subjective ratings.","Subjective ratings increase by more than 40%, for both the listening test and the live conversation compared to a fitted hearing aid as a baseline.","Speech reception thresholds, measuring speech understanding in noise, improve by 1.6 dB SRT.","Ours is the first denoising system that is implemented on a mobile device, streamed directly to users' hearing aids using only a single channel as audio input while improving user satisfaction on all tested aspects, including speech intelligibility.","This includes overall preference of the denoised and streamed signal over the hearing aid, thereby accepting the higher latency for the significant improvement in speech understanding."],"url":"http://arxiv.org/abs/2308.11456v1"}
{"created":"2023-08-22 14:05:37","title":"A Survey on Self-Supervised Representation Learning","abstract":"Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.","sentences":["Learning meaningful representations is at the heart of many tasks in the field of modern machine learning.","Recently, a lot of methods were introduced that allow learning of image representations without supervision.","These representations can then be used in downstream tasks like classification or object detection.","The quality of these representations is close to supervised learning, while no labeled images are needed.","This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other.","Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study.","Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning."],"url":"http://arxiv.org/abs/2308.11455v1"}
{"created":"2023-08-22 13:59:47","title":"Food Image Classification and Segmentation with Attention-based Multiple Instance Learning","abstract":"The demand for accurate food quantification has increased in the recent years, driven by the needs of applications in dietary monitoring. At the same time, computer vision approaches have exhibited great potential in automating tasks within the food domain. Traditionally, the development of machine learning models for these problems relies on training data sets with pixel-level class annotations. However, this approach introduces challenges arising from data collection and ground truth generation that quickly become costly and error-prone since they must be performed in multiple settings and for thousands of classes. To overcome these challenges, the paper presents a weakly supervised methodology for training food image classification and semantic segmentation models without relying on pixel-level annotations. The proposed methodology is based on a multiple instance learning approach in combination with an attention-based mechanism. At test time, the models are used for classification and, concurrently, the attention mechanism generates semantic heat maps which are used for food class segmentation. In the paper, we conduct experiments on two meta-classes within the FoodSeg103 data set to verify the feasibility of the proposed approach and we explore the functioning properties of the attention mechanism.","sentences":["The demand for accurate food quantification has increased in the recent years, driven by the needs of applications in dietary monitoring.","At the same time, computer vision approaches have exhibited great potential in automating tasks within the food domain.","Traditionally, the development of machine learning models for these problems relies on training data sets with pixel-level class annotations.","However, this approach introduces challenges arising from data collection and ground truth generation that quickly become costly and error-prone since they must be performed in multiple settings and for thousands of classes.","To overcome these challenges, the paper presents a weakly supervised methodology for training food image classification and semantic segmentation models without relying on pixel-level annotations.","The proposed methodology is based on a multiple instance learning approach in combination with an attention-based mechanism.","At test time, the models are used for classification and, concurrently, the attention mechanism generates semantic heat maps which are used for food class segmentation.","In the paper, we conduct experiments on two meta-classes within the FoodSeg103 data set to verify the feasibility of the proposed approach and we explore the functioning properties of the attention mechanism."],"url":"http://arxiv.org/abs/2308.11452v1"}
{"created":"2023-08-22 13:58:45","title":"Towards Discriminative Representations with Contrastive Instances for Real-Time UAV Tracking","abstract":"Maintaining high efficiency and high precision are two fundamental challenges in UAV tracking due to the constraints of computing resources, battery capacity, and UAV maximum load. Discriminative correlation filters (DCF)-based trackers can yield high efficiency on a single CPU but with inferior precision. Lightweight Deep learning (DL)-based trackers can achieve a good balance between efficiency and precision but performance gains are limited by the compression rate. High compression rate often leads to poor discriminative representations. To this end, this paper aims to enhance the discriminative power of feature representations from a new feature-learning perspective. Specifically, we attempt to learn more disciminative representations with contrastive instances for UAV tracking in a simple yet effective manner, which not only requires no manual annotations but also allows for developing and deploying a lightweight model. We are the first to explore contrastive learning for UAV tracking. Extensive experiments on four UAV benchmarks, including UAV123@10fps, DTB70, UAVDT and VisDrone2018, show that the proposed DRCI tracker significantly outperforms state-of-the-art UAV tracking methods.","sentences":["Maintaining high efficiency and high precision are two fundamental challenges in UAV tracking due to the constraints of computing resources, battery capacity, and UAV maximum load.","Discriminative correlation filters (DCF)-based trackers can yield high efficiency on a single CPU but with inferior precision.","Lightweight Deep learning (DL)-based trackers can achieve a good balance between efficiency and precision but performance gains are limited by the compression rate.","High compression rate often leads to poor discriminative representations.","To this end, this paper aims to enhance the discriminative power of feature representations from a new feature-learning perspective.","Specifically, we attempt to learn more disciminative representations with contrastive instances for UAV tracking in a simple yet effective manner, which not only requires no manual annotations but also allows for developing and deploying a lightweight model.","We are the first to explore contrastive learning for UAV tracking.","Extensive experiments on four UAV benchmarks, including UAV123@10fps, DTB70, UAVDT and VisDrone2018, show that the proposed DRCI tracker significantly outperforms state-of-the-art UAV tracking methods."],"url":"http://arxiv.org/abs/2308.11450v1"}
{"created":"2023-08-22 13:55:57","title":"Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding","abstract":"Self-supervised pretraining (SSP) has emerged as a popular technique in machine learning, enabling the extraction of meaningful feature representations without labelled data. In the realm of computer vision, pretrained vision transformers (ViTs) have played a pivotal role in advancing transfer learning. Nonetheless, the escalating cost of finetuning these large models has posed a challenge due to the explosion of model size. This study endeavours to evaluate the effectiveness of pure self-supervised learning (SSL) techniques in computer vision tasks, obviating the need for finetuning, with the intention of emulating human-like capabilities in generalisation and recognition of unseen objects. To this end, we propose an evaluation protocol for zero-shot segmentation based on a prompting patch. Given a point on the target object as a prompt, the algorithm calculates the similarity map between the selected patch and other patches, upon that, a simple thresholding is applied to segment the target. Another evaluation is intra-object and inter-object similarity to gauge discriminatory ability of SSP ViTs. Insights from zero-shot segmentation from prompting and discriminatory abilities of SSP led to the design of a simple SSP approach, termed MMC. This approaches combines Masked image modelling for encouraging similarity of local features, Momentum based self-distillation for transferring semantics from global to local features, and global Contrast for promoting semantics of global features, to enhance discriminative representations of SSP ViTs. Consequently, our proposed method significantly reduces the overlap of intra-object and inter-object similarities, thereby facilitating effective object segmentation within an image. Our experiments reveal that MMC delivers top-tier results in zero-shot semantic segmentation across various datasets.","sentences":["Self-supervised pretraining (SSP) has emerged as a popular technique in machine learning, enabling the extraction of meaningful feature representations without labelled data.","In the realm of computer vision, pretrained vision transformers (ViTs) have played a pivotal role in advancing transfer learning.","Nonetheless, the escalating cost of finetuning these large models has posed a challenge due to the explosion of model size.","This study endeavours to evaluate the effectiveness of pure self-supervised learning (SSL) techniques in computer vision tasks, obviating the need for finetuning, with the intention of emulating human-like capabilities in generalisation and recognition of unseen objects.","To this end, we propose an evaluation protocol for zero-shot segmentation based on a prompting patch.","Given a point on the target object as a prompt, the algorithm calculates the similarity map between the selected patch and other patches, upon that, a simple thresholding is applied to segment the target.","Another evaluation is intra-object and inter-object similarity to gauge discriminatory ability of SSP ViTs.","Insights from zero-shot segmentation from prompting and discriminatory abilities of SSP led to the design of a simple SSP approach, termed MMC.","This approaches combines Masked image modelling for encouraging similarity of local features, Momentum based self-distillation for transferring semantics from global to local features, and global Contrast for promoting semantics of global features, to enhance discriminative representations of SSP ViTs.","Consequently, our proposed method significantly reduces the overlap of intra-object and inter-object similarities, thereby facilitating effective object segmentation within an image.","Our experiments reveal that MMC delivers top-tier results in zero-shot semantic segmentation across various datasets."],"url":"http://arxiv.org/abs/2308.11448v1"}
{"created":"2023-08-22 13:55:36","title":"Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification","abstract":"Aspect-based sentiment classification is a crucial problem in fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspect according to its context. Previous works have made remarkable progress in leveraging attention mechanism to extract opinion words for different aspects. However, a persistent challenge is the effective management of semantic mismatches, which stem from attention mechanisms that fall short in adequately aligning opinions words with their corresponding aspect in multi-aspect sentences. To address this issue, we propose a novel Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual association between opinion words and the corresponding aspect. Specifically, we first introduce a neighboring span enhanced module which highlights various compositions of neighboring words and given aspects. In addition, we design a multi-perspective attention mechanism that align relevant opinion information with respect to the given aspect. Extensive experiments on three benchmark datasets demonstrate that our model achieves state-of-the-art results. The source code is available at https://github.com/AONE-NLP/ABSA-AOAN.","sentences":["Aspect-based sentiment classification is a crucial problem in fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspect according to its context.","Previous works have made remarkable progress in leveraging attention mechanism to extract opinion words for different aspects.","However, a persistent challenge is the effective management of semantic mismatches, which stem from attention mechanisms that fall short in adequately aligning opinions words with their corresponding aspect in multi-aspect sentences.","To address this issue, we propose a novel Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual association between opinion words and the corresponding aspect.","Specifically, we first introduce a neighboring span enhanced module which highlights various compositions of neighboring words and given aspects.","In addition, we design a multi-perspective attention mechanism that align relevant opinion information with respect to the given aspect.","Extensive experiments on three benchmark datasets demonstrate that our model achieves state-of-the-art results.","The source code is available at https://github.com/AONE-NLP/ABSA-AOAN."],"url":"http://arxiv.org/abs/2308.11447v1"}
{"created":"2023-08-22 13:53:43","title":"Exploration of Rashomon Set Assists Explanations for Medical Data","abstract":"The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\\texttt{Rashomon_DETECT}$ algorithm. This algorithm compares profiles illustrating prediction dependencies on variable values generated by eXplainable Artificial Intelligence (XAI) techniques. To quantify differences in variable effects among models, we introduce the Profile Disparity Index (PDI) based on measures from functional data analysis. To illustrate the effectiveness of our approach, we showcase its application in predicting survival among hemophagocytic lymphohistiocytosis (HLH) patients - a foundational case study. Additionally, we benchmark our approach on other medical data sets, demonstrating its versatility and utility in various contexts.","sentences":["The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric.","However, this approach leads to abandoning a more profound analysis of slightly inferior models.","Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions.","This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis.","This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach.","The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\\texttt{Rashomon_DETECT}$ algorithm.","This algorithm compares profiles illustrating prediction dependencies on variable values generated by eXplainable Artificial Intelligence (XAI) techniques.","To quantify differences in variable effects among models, we introduce the Profile Disparity Index (PDI) based on measures from functional data analysis.","To illustrate the effectiveness of our approach, we showcase its application in predicting survival among hemophagocytic lymphohistiocytosis (HLH) patients - a foundational case study.","Additionally, we benchmark our approach on other medical data sets, demonstrating its versatility and utility in various contexts."],"url":"http://arxiv.org/abs/2308.11446v1"}
{"created":"2023-08-22 13:50:49","title":"Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging","abstract":"Fast Adversarial Training (FAT) not only improves the model robustness but also reduces the training cost of standard adversarial training. However, fast adversarial training often suffers from Catastrophic Overfitting (CO), which results in poor robustness performance. Catastrophic Overfitting describes the phenomenon of a sudden and significant decrease in robust accuracy during the training of fast adversarial training. Many effective techniques have been developed to prevent Catastrophic Overfitting and improve the model robustness from different perspectives. However, these techniques adopt inconsistent training settings and require different training costs, i.e, training time and memory costs, leading to unfair comparisons. In this paper, we conduct a comprehensive study of over 10 fast adversarial training methods in terms of adversarial robustness and training costs. We revisit the effectiveness and efficiency of fast adversarial training techniques in preventing Catastrophic Overfitting from the perspective of model local nonlinearity and propose an effective Lipschitz regularization method for fast adversarial training. Furthermore, we explore the effect of data augmentation and weight averaging in fast adversarial training and propose a simple yet effective auto weight averaging method to improve robustness further. By assembling these techniques, we propose a FGSM-based fast adversarial training method equipped with Lipschitz regularization and Auto Weight averaging, abbreviated as FGSM-LAW. Experimental evaluations on four benchmark databases demonstrate the superiority of the proposed method over state-of-the-art fast adversarial training methods and the advanced standard adversarial training methods.","sentences":["Fast Adversarial Training (FAT) not only improves the model robustness but also reduces the training cost of standard adversarial training.","However, fast adversarial training often suffers from Catastrophic Overfitting (CO), which results in poor robustness performance.","Catastrophic Overfitting describes the phenomenon of a sudden and significant decrease in robust accuracy during the training of fast adversarial training.","Many effective techniques have been developed to prevent Catastrophic Overfitting and improve the model robustness from different perspectives.","However, these techniques adopt inconsistent training settings and require different training costs, i.e, training time and memory costs, leading to unfair comparisons.","In this paper, we conduct a comprehensive study of over 10 fast adversarial training methods in terms of adversarial robustness and training costs.","We revisit the effectiveness and efficiency of fast adversarial training techniques in preventing Catastrophic Overfitting from the perspective of model local nonlinearity and propose an effective Lipschitz regularization method for fast adversarial training.","Furthermore, we explore the effect of data augmentation and weight averaging in fast adversarial training and propose a simple yet effective auto weight averaging method to improve robustness further.","By assembling these techniques, we propose a FGSM-based fast adversarial training method equipped with Lipschitz regularization and Auto Weight averaging, abbreviated as FGSM-LAW.","Experimental evaluations on four benchmark databases demonstrate the superiority of the proposed method over state-of-the-art fast adversarial training methods and the advanced standard adversarial training methods."],"url":"http://arxiv.org/abs/2308.11443v1"}
{"created":"2023-08-22 13:46:12","title":"SDeMorph: Towards Better Facial De-morphing from Single Morph","abstract":"Face Recognition Systems (FRS) are vulnerable to morph attacks. A face morph is created by combining multiple identities with the intention to fool FRS and making it match the morph with multiple identities. Current Morph Attack Detection (MAD) can detect the morph but are unable to recover the identities used to create the morph with satisfactory outcomes. Existing work in de-morphing is mostly reference-based, i.e. they require the availability of one identity to recover the other. Sudipta et al. \\cite{ref9} proposed a reference-free de-morphing technique but the visual realism of outputs produced were feeble. In this work, we propose SDeMorph (Stably Diffused De-morpher), a novel de-morphing method that is reference-free and recovers the identities of bona fides. Our method produces feature-rich outputs that are of significantly high quality in terms of definition and facial fidelity. Our method utilizes Denoising Diffusion Probabilistic Models (DDPM) by destroying the input morphed signal and then reconstructing it back using a branched-UNet. Experiments on ASML, FRLL-FaceMorph, FRLL-MorDIFF, and SMDD datasets support the effectiveness of the proposed method.","sentences":["Face Recognition Systems (FRS) are vulnerable to morph attacks.","A face morph is created by combining multiple identities with the intention to fool FRS and making it match the morph with multiple identities.","Current Morph Attack Detection (MAD) can detect the morph but are unable to recover the identities used to create the morph with satisfactory outcomes.","Existing work in de-morphing is mostly reference-based, i.e. they require the availability of one identity to recover the other.","Sudipta et al. \\cite{ref9} proposed a reference-free de-morphing technique but the visual realism of outputs produced were feeble.","In this work, we propose SDeMorph (Stably Diffused De-morpher), a novel de-morphing method that is reference-free and recovers the identities of bona fides.","Our method produces feature-rich outputs that are of significantly high quality in terms of definition and facial fidelity.","Our method utilizes Denoising Diffusion Probabilistic Models (DDPM) by destroying the input morphed signal and then reconstructing it back using a branched-UNet.","Experiments on ASML, FRLL-FaceMorph, FRLL-MorDIFF, and SMDD datasets support the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2308.11442v1"}
{"created":"2023-08-22 13:45:35","title":"Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection","abstract":"Latest methods represent shapes with open surfaces using unsigned distance functions (UDFs). They train neural networks to learn UDFs and reconstruct surfaces with the gradients around the zero level set of the UDF. However, the differential networks struggle from learning the zero level set where the UDF is not differentiable, which leads to large errors on unsigned distances and gradients around the zero level set, resulting in highly fragmented and discontinuous surfaces. To resolve this problem, we propose to learn a more continuous zero level set in UDFs with level set projections. Our insight is to guide the learning of zero level set using the rest non-zero level sets via a projection procedure. Our idea is inspired from the observations that the non-zero level sets are much smoother and more continuous than the zero level set. We pull the non-zero level sets onto the zero level set with gradient constraints which align gradients over different level sets and correct unsigned distance errors on the zero level set, leading to a smoother and more continuous unsigned distance field. We conduct comprehensive experiments in surface reconstruction for point clouds, real scans or depth maps, and further explore the performance in unsupervised point cloud upsampling and unsupervised point normal estimation with the learned UDF, which demonstrate our non-trivial improvements over the state-of-the-art methods. Code is available at https://github.com/junshengzhou/LevelSetUDF .","sentences":["Latest methods represent shapes with open surfaces using unsigned distance functions (UDFs).","They train neural networks to learn UDFs and reconstruct surfaces with the gradients around the zero level set of the UDF.","However, the differential networks struggle from learning the zero level set where the UDF is not differentiable, which leads to large errors on unsigned distances and gradients around the zero level set, resulting in highly fragmented and discontinuous surfaces.","To resolve this problem, we propose to learn a more continuous zero level set in UDFs with level set projections.","Our insight is to guide the learning of zero level set using the rest non-zero level sets via a projection procedure.","Our idea is inspired from the observations that the non-zero level sets are much smoother and more continuous than the zero level set.","We pull the non-zero level sets onto the zero level set with gradient constraints which align gradients over different level sets and correct unsigned distance errors on the zero level set, leading to a smoother and more continuous unsigned distance field.","We conduct comprehensive experiments in surface reconstruction for point clouds, real scans or depth maps, and further explore the performance in unsupervised point cloud upsampling and unsupervised point normal estimation with the learned UDF, which demonstrate our non-trivial improvements over the state-of-the-art methods.","Code is available at https://github.com/junshengzhou/LevelSetUDF ."],"url":"http://arxiv.org/abs/2308.11441v1"}
{"created":"2023-08-22 13:42:15","title":"PoseGraphNet++: Enriching 3D Human Pose with Orientation Estimation","abstract":"Existing kinematic skeleton-based 3D human pose estimation methods only predict joint positions. Although this is sufficient to compute the yaw and pitch of the bone rotations, the roll around the axis of the bones remains unresolved by these methods. In this paper, we propose a novel 2D-to-3D lifting Graph Convolution Network named PoseGraphNet++ to predict the complete human pose including the joint positions and the bone orientations. We employ node and edge convolutions to utilize the joint and bone features. Our model is evaluated on multiple benchmark datasets, and its performance is either on par with or better than the state-of-the-art in terms of both position and rotation metrics. Through extensive ablation studies, we show that PoseGraphNet++ benefits from exploiting the mutual relationship between the joints and the bones.","sentences":["Existing kinematic skeleton-based 3D human pose estimation methods only predict joint positions.","Although this is sufficient to compute the yaw and pitch of the bone rotations, the roll around the axis of the bones remains unresolved by these methods.","In this paper, we propose a novel 2D-to-3D lifting Graph Convolution Network named PoseGraphNet++ to predict the complete human pose including the joint positions and the bone orientations.","We employ node and edge convolutions to utilize the joint and bone features.","Our model is evaluated on multiple benchmark datasets, and its performance is either on par with or better than the state-of-the-art in terms of both position and rotation metrics.","Through extensive ablation studies, we show that PoseGraphNet++ benefits from exploiting the mutual relationship between the joints and the bones."],"url":"http://arxiv.org/abs/2308.11440v1"}
{"created":"2023-08-22 13:30:37","title":"A Survey on Large Language Model based Autonomous Agents","abstract":"Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository for the related references at https://github.com/Paitesanshi/LLM-Agent-Survey.","sentences":["Autonomous agents have long been a prominent research topic in the academic community.","Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions.","Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence.","This has sparked an upsurge in studies investigating autonomous agents based on LLMs.","To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications.","In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective.","More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work.","Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering.","Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents.","Based on the previous studies, we also present several challenges and future directions in this field.","To keep track of this field and continuously update our survey, we maintain a repository for the related references at https://github.com/Paitesanshi/LLM-Agent-Survey."],"url":"http://arxiv.org/abs/2308.11432v1"}
{"created":"2023-08-22 13:15:29","title":"AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block","abstract":"The future of the arts and artificial intelligence (AI) is promising as technology advances. As the use of AI in design becomes more widespread, art practice may not be a human-only art form and could instead become a digitally integrated experience. With enhanced creativity and collaboration, arts and AI could work together towards creating artistic outputs that are visually appealing and meet the needs of the artist and viewer. While it is uncertain how far the integration will go, arts and AI will likely influence one another. This workshop pictorial puts forward first-person research that shares interactions between an HCI researcher and AI as they try to escape the creative block. The pictorial paper explores two questions: How can AI support artists' creativity, and what does it mean to be explainable in this context? HIs, ChatGPT and Midjourney were engaged; the result was a series of reflections that require further discussion and explorations in the XAIxArts community: Transparency of attribution, the creation process, ethics of asking, and inspiration vs copying.","sentences":["The future of the arts and artificial intelligence (AI) is promising as technology advances.","As the use of AI in design becomes more widespread, art practice may not be a human-only art form and could instead become a digitally integrated experience.","With enhanced creativity and collaboration, arts and AI could work together towards creating artistic outputs that are visually appealing and meet the needs of the artist and viewer.","While it is uncertain how far the integration will go, arts and AI will likely influence one another.","This workshop pictorial puts forward first-person research that shares interactions between an HCI researcher and AI as they try to escape the creative block.","The pictorial paper explores two questions: How can AI support artists' creativity, and what does it mean to be explainable in this context?","HIs, ChatGPT and Midjourney were engaged; the result was a series of reflections that require further discussion and explorations in the XAIxArts community:","Transparency of attribution, the creation process, ethics of asking, and inspiration vs copying."],"url":"http://arxiv.org/abs/2308.11424v1"}
{"created":"2023-08-22 13:12:13","title":"Recommending Analogical APIs via Knowledge Graph Embedding","abstract":"Library migration, which re-implements the same software behavior by using a different library instead of using the current one, has been widely observed in software evolution. One essential part of library migration is to find an analogical API that could provide the same functionality as current ones. However, given the large number of libraries/APIs, manually finding an analogical API could be very time-consuming and error-prone. Researchers have developed multiple automated analogical API recommendation techniques. Documentation-based methods have particularly attracted significant interest. Despite their potential, these methods have limitations, such as a lack of comprehensive semantic understanding in documentation and scalability challenges. In this work, we propose KGE4AR, a novel documentation-based approach that leverages knowledge graph (KG) embedding to recommend analogical APIs during library migration. Specifically, KGE4AR proposes a novel unified API KG to comprehensively and structurally represent three types of knowledge in documentation, which can better capture the high-level semantics. Moreover, KGE4AR then proposes to embed the unified API KG into vectors, enabling more effective and scalable similarity calculation. We build KGE4AR' s unified API KG for 35,773 Java libraries and assess it in two API recommendation scenarios: with and without target libraries. Our results show that KGE4AR substantially outperforms state-of-the-art documentation-based techniques in both evaluation scenarios in terms of all metrics (e.g., 47.1%-143.0% and 11.7%-80.6% MRR improvements in each scenario). Additionally, we explore KGE4AR' s scalability, confirming its effective scaling with the growing number of libraries.","sentences":["Library migration, which re-implements the same software behavior by using a different library instead of using the current one, has been widely observed in software evolution.","One essential part of library migration is to find an analogical API that could provide the same functionality as current ones.","However, given the large number of libraries/APIs, manually finding an analogical API could be very time-consuming and error-prone.","Researchers have developed multiple automated analogical API recommendation techniques.","Documentation-based methods have particularly attracted significant interest.","Despite their potential, these methods have limitations, such as a lack of comprehensive semantic understanding in documentation and scalability challenges.","In this work, we propose KGE4AR, a novel documentation-based approach that leverages knowledge graph (KG) embedding to recommend analogical APIs during library migration.","Specifically, KGE4AR proposes a novel unified API KG to comprehensively and structurally represent three types of knowledge in documentation, which can better capture the high-level semantics.","Moreover, KGE4AR then proposes to embed the unified API KG into vectors, enabling more effective and scalable similarity calculation.","We build KGE4AR' s unified API KG for 35,773 Java libraries and assess it in two API recommendation scenarios: with and without target libraries.","Our results show that KGE4AR substantially outperforms state-of-the-art documentation-based techniques in both evaluation scenarios in terms of all metrics (e.g., 47.1%-143.0% and 11.7%-80.6% MRR improvements in each scenario).","Additionally, we explore KGE4AR' s scalability, confirming its effective scaling with the growing number of libraries."],"url":"http://arxiv.org/abs/2308.11422v1"}
{"created":"2023-08-22 13:08:29","title":"TurboViT: Generating Fast Vision Transformers via Generative Architecture Search","abstract":"Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years. However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements. As such, there has been significant research recently on the design of efficient vision transformer architectures. In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency. Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns. The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (>2.47$\\times$ smaller than FasterViT-0 while achieving same accuracy) and computational complexity (>3.4$\\times$ fewer FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other state-of-the-art efficient vision transformer network architecture designs within a similar range of accuracy on the ImageNet-1K dataset. Furthermore, TurboViT demonstrated strong inference latency and throughput in both low-latency and batch processing scenarios (>3.21$\\times$ lower latency and >3.18$\\times$ higher throughput compared to FasterViT-0 for low-latency scenario). These promising results demonstrate the efficacy of leveraging generative architecture search for generating efficient transformer architecture designs for high-throughput scenarios.","sentences":["Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years.","However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements.","As such, there has been significant research recently on the design of efficient vision transformer architectures.","In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency.","Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns.","The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (>2.47$\\times$ smaller than FasterViT-0 while achieving same accuracy) and computational complexity (>3.4$\\times$ fewer FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other state-of-the-art efficient vision transformer network architecture designs within a similar range of accuracy on the ImageNet-1K dataset.","Furthermore, TurboViT demonstrated strong inference latency and throughput in both low-latency and batch processing scenarios (>3.21$\\times$ lower latency and >3.18$\\times$ higher throughput compared to FasterViT-0 for low-latency scenario).","These promising results demonstrate the efficacy of leveraging generative architecture search for generating efficient transformer architecture designs for high-throughput scenarios."],"url":"http://arxiv.org/abs/2308.11421v1"}
{"created":"2023-08-22 13:06:20","title":"Supply Function Equilibrium in Networked Electricity Markets","abstract":"We study deregulated power markets with strategic power suppliers. In deregulated markets, each supplier submits its supply function (i.e., the amount of electricity it is willing to produce at various prices) to the independent system operator (ISO), who based on the submitted supply functions, dispatches the suppliers to clear the market with minimal total generation cost. If all suppliers reported their true marginal cost functions as supply functions, the market outcome would be efficient (i.e., the total generation cost is minimized). However, when suppliers are strategic and aim to maximize their own profits, the reported supply functions are not necessarily the true marginal cost functions, and the resulting market outcome may be inefficient. The efficiency loss depends crucially on the topology of the underlying transmission network. This paper provides an analytical upper bound of the efficiency loss due to strategic suppliers, and proves that the bound is tight under a large class of transmission networks (i.e., weakly cyclic networks). Our upper bound sheds light on how the efficiency loss depends on the transmission network topology (e.g., the degrees of nodes, the admittances and flow limits of transmission lines).","sentences":["We study deregulated power markets with strategic power suppliers.","In deregulated markets, each supplier submits its supply function (i.e., the amount of electricity it is willing to produce at various prices) to the independent system operator (ISO), who based on the submitted supply functions, dispatches the suppliers to clear the market with minimal total generation cost.","If all suppliers reported their true marginal cost functions as supply functions, the market outcome would be efficient (i.e., the total generation cost is minimized).","However, when suppliers are strategic and aim to maximize their own profits, the reported supply functions are not necessarily the true marginal cost functions, and the resulting market outcome may be inefficient.","The efficiency loss depends crucially on the topology of the underlying transmission network.","This paper provides an analytical upper bound of the efficiency loss due to strategic suppliers, and proves that the bound is tight under a large class of transmission networks (i.e., weakly cyclic networks).","Our upper bound sheds light on how the efficiency loss depends on the transmission network topology (e.g., the degrees of nodes, the admittances and flow limits of transmission lines)."],"url":"http://arxiv.org/abs/2308.11420v1"}
{"created":"2023-08-22 13:02:23","title":"ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes","abstract":"We present ScanNet++, a large-scale dataset that couples together capture of high-quality and commodity-level geometry and color of indoor scenes. Each scene is captured with a high-end laser scanner at sub-millimeter resolution, along with registered 33-megapixel images from a DSLR camera, and RGB-D streams from an iPhone. Scene reconstructions are further annotated with an open vocabulary of semantics, with label-ambiguous scenarios explicitly annotated for comprehensive semantic understanding. ScanNet++ enables a new real-world benchmark for novel view synthesis, both from high-quality RGB capture, and importantly also from commodity-level images, in addition to a new benchmark for 3D semantic scene understanding that comprehensively encapsulates diverse and ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.","sentences":["We present ScanNet++, a large-scale dataset that couples together capture of high-quality and commodity-level geometry and color of indoor scenes.","Each scene is captured with a high-end laser scanner at sub-millimeter resolution, along with registered 33-megapixel images from a DSLR camera, and RGB-D streams from an iPhone.","Scene reconstructions are further annotated with an open vocabulary of semantics, with label-ambiguous scenarios explicitly annotated for comprehensive semantic understanding.","ScanNet++ enables a new real-world benchmark for novel view synthesis, both from high-quality RGB capture, and importantly also from commodity-level images, in addition to a new benchmark for 3D semantic scene understanding that comprehensively encapsulates diverse and ambiguous semantic labeling scenarios.","Currently, ScanNet++ contains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames."],"url":"http://arxiv.org/abs/2308.11417v1"}
{"created":"2023-08-22 13:02:09","title":"Consistency-Checking Problems: A Gateway to Parameterized Sample Complexity","abstract":"Recently, Brand, Ganian and Simonov introduced a parameterized refinement of the classical PAC-learning sample complexity framework. A crucial outcome of their investigation is that for a very wide range of learning problems, there is a direct and provable correspondence between fixed-parameter PAC-learnability (in the sample complexity setting) and the fixed-parameter tractability of a corresponding \"consistency checking\" search problem (in the setting of computational complexity). The latter can be seen as generalizations of classical search problems where instead of receiving a single instance, one receives multiple yes- and no-examples and is tasked with finding a solution which is consistent with the provided examples.   Apart from a few initial results, consistency checking problems are almost entirely unexplored from a parameterized complexity perspective. In this article, we provide an overview of these problems and their connection to parameterized sample complexity, with the primary aim of facilitating further research in this direction. Afterwards, we establish the fixed-parameter (in)-tractability for some of the arguably most natural consistency checking problems on graphs, and show that their complexity-theoretic behavior is surprisingly very different from that of classical decision problems. Our new results cover consistency checking variants of problems as diverse as (k-)Path, Matching, 2-Coloring, Independent Set and Dominating Set, among others.","sentences":["Recently, Brand, Ganian and Simonov introduced a parameterized refinement of the classical PAC-learning sample complexity framework.","A crucial outcome of their investigation is that for a very wide range of learning problems, there is a direct and provable correspondence between fixed-parameter PAC-learnability (in the sample complexity setting) and the fixed-parameter tractability of a corresponding \"consistency checking\" search problem (in the setting of computational complexity).","The latter can be seen as generalizations of classical search problems where instead of receiving a single instance, one receives multiple yes- and no-examples and is tasked with finding a solution which is consistent with the provided examples.   ","Apart from a few initial results, consistency checking problems are almost entirely unexplored from a parameterized complexity perspective.","In this article, we provide an overview of these problems and their connection to parameterized sample complexity, with the primary aim of facilitating further research in this direction.","Afterwards, we establish the fixed-parameter (in)-tractability for some of the arguably most natural consistency checking problems on graphs, and show that their complexity-theoretic behavior is surprisingly very different from that of classical decision problems.","Our new results cover consistency checking variants of problems as diverse as (k-)Path, Matching, 2-Coloring, Independent Set and Dominating Set, among others."],"url":"http://arxiv.org/abs/2308.11416v1"}
{"created":"2023-08-22 13:00:13","title":"Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm","abstract":"Extracting relational triples (subject, predicate, object) from text enables the transformation of unstructured text data into structured knowledge. The named entity recognition (NER) and the relation extraction (RE) are two foundational subtasks in this knowledge generation pipeline. The integration of subtasks poses a considerable challenge due to their disparate nature. This paper presents a novel approach that converts the triple extraction task into a graph labeling problem, capitalizing on the structural information of dependency parsing and graph recursive neural networks (GRNNs). To integrate subtasks, this paper proposes a dynamic feedback forest algorithm that connects the representations of subtasks by inference operations during model training. Experimental results demonstrate the effectiveness of the proposed method.","sentences":["Extracting relational triples (subject, predicate, object) from text enables the transformation of unstructured text data into structured knowledge.","The named entity recognition (NER) and the relation extraction (RE) are two foundational subtasks in this knowledge generation pipeline.","The integration of subtasks poses a considerable challenge due to their disparate nature.","This paper presents a novel approach that converts the triple extraction task into a graph labeling problem, capitalizing on the structural information of dependency parsing and graph recursive neural networks (GRNNs).","To integrate subtasks, this paper proposes a dynamic feedback forest algorithm that connects the representations of subtasks by inference operations during model training.","Experimental results demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2308.11411v1"}
{"created":"2023-08-22 12:54:48","title":"MatFuse: Controllable Material Generation with Diffusion Models","abstract":"Creating high quality and realistic materials in computer graphics is a challenging and time-consuming task, which requires great expertise. In this paper, we present MatFuse, a novel unified approach that harnesses the generative power of diffusion models (DM) to simplify the creation of SVBRDF maps. Our DM-based pipeline integrates multiple sources of conditioning, such as color palettes, sketches, and pictures, enabling fine-grained control and flexibility in material synthesis. This design allows for the combination of diverse information sources (e.g., sketch + image embedding), enhancing creative possibilities in line with the principle of compositionality. We demonstrate the generative capabilities of the proposed method under various conditioning settings; on the SVBRDF estimation task, we show that our method yields performance comparable to state-of-the-art approaches, both qualitatively and quantitatively.","sentences":["Creating high quality and realistic materials in computer graphics is a challenging and time-consuming task, which requires great expertise.","In this paper, we present MatFuse, a novel unified approach that harnesses the generative power of diffusion models (DM) to simplify the creation of SVBRDF maps.","Our DM-based pipeline integrates multiple sources of conditioning, such as color palettes, sketches, and pictures, enabling fine-grained control and flexibility in material synthesis.","This design allows for the combination of diverse information sources (e.g., sketch + image embedding), enhancing creative possibilities in line with the principle of compositionality.","We demonstrate the generative capabilities of the proposed method under various conditioning settings; on the SVBRDF estimation task, we show that our method yields performance comparable to state-of-the-art approaches, both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2308.11408v1"}
{"created":"2023-08-22 12:53:09","title":"Achievable Sum-rate of variants of QAM over Gaussian Multiple Access Channel with and without security","abstract":"The performance of next generation wireless systems (5G/6G and beyond) at the physical layer is primarily driven by the choice of digital modulation techniques that are bandwidth and power efficient, while maintaining high data rates. Achievable rates for Gaussian input and some finite constellations (BPSK/QPSK/QAM) are well studied in the literature. However, new variants of Quadrature Amplitude Modulation (QAM) such as Cross-QAM (XQAM), Star-QAM (S-QAM), Amplitude and phase shift keying (APSK), and Hexagonal Quadrature Amplitude Modulation (H-QAM) are not studied in the context of achievable rates for meeting the demand of high data rates. In this paper, we study achievable rate region for different variants of M-QAM like Cross-QAM, H-QAM, Star-QAM and APSK. We also compute mutual information corresponding to the sum rate of Gaussian Multiple Access Channel (G-MAC), for hybrid constellation scheme, e.g., user 1 transmits using Star-QAM and user 2 by H-QAM. From the results, it is observed that S-QAM gives the maximum sum-rate when users transmit same constellations. Also, it has been found that when hybrid constellation is used, the combination of Star-QAM \\& H-QAM gives the maximum rate. In the next part of the paper, we consider a scenario wherein an adversary is also present at the receiver side and is trying to decode the information. We model this scenario as Gaussian Multiple Access Wiretap Channel (G-MAW-WT). We then compute the achievable secrecy sum rate of two user G-MAC-WT with discrete inputs from different variants of QAM (viz, X-QAM, H-QAM and S-QAM).It has been found that at higher values of SNR, S-QAM gives better values of SSR than the other variants. For hybrid inputs of QAM, at lower values of SNR, combination of APSK and S-QAM gives better results and at higher values of SNR, combination of HQAM and APSK gives greater value of SSR.","sentences":["The performance of next generation wireless systems (5G/6G and beyond) at the physical layer is primarily driven by the choice of digital modulation techniques that are bandwidth and power efficient, while maintaining high data rates.","Achievable rates for Gaussian input and some finite constellations (BPSK/QPSK/QAM) are well studied in the literature.","However, new variants of Quadrature Amplitude Modulation (QAM) such as Cross-QAM (XQAM), Star-QAM (S-QAM), Amplitude and phase shift keying (APSK), and Hexagonal Quadrature Amplitude Modulation (H-QAM) are not studied in the context of achievable rates for meeting the demand of high data rates.","In this paper, we study achievable rate region for different variants of M-QAM like Cross-QAM, H-QAM, Star-QAM and APSK.","We also compute mutual information corresponding to the sum rate of Gaussian Multiple Access Channel (G-MAC), for hybrid constellation scheme, e.g., user 1 transmits using Star-QAM and user 2 by H-QAM.","From the results, it is observed that S-QAM gives the maximum sum-rate when users transmit same constellations.","Also, it has been found that when hybrid constellation is used, the combination of Star-QAM \\& H-QAM gives the maximum rate.","In the next part of the paper, we consider a scenario wherein an adversary is also present at the receiver side and is trying to decode the information.","We model this scenario as Gaussian Multiple Access Wiretap Channel (G-MAW-WT).","We then compute the achievable secrecy sum rate of two user G-MAC-WT with discrete inputs from different variants of QAM (viz, X-QAM, H-QAM and S-QAM).It has been found that at higher values of SNR, S-QAM gives better values of SSR than the other variants.","For hybrid inputs of QAM, at lower values of SNR, combination of APSK and S-QAM gives better results and at higher values of SNR, combination of HQAM and APSK gives greater value of SSR."],"url":"http://arxiv.org/abs/2308.11405v1"}
{"created":"2023-08-22 12:53:09","title":"Designing an attack-defense game: how to increase robustness of financial transaction models via a competition","abstract":"Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.   To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break it, and what techniques one should use to make it more robust, and introduction additional way to attack models or increase their robustness.   Our analysis continues with a meta-study on the used approaches with their power, numerical experiments, and accompanied ablations studies. We show that the developed attacks and defenses outperform existing alternatives from the literature while being practical in terms of execution, proving the validity of the competition as a tool for uncovering vulnerabilities of machine learning models and mitigating them in various domains.","sentences":["Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical.","The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks.","We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.   ","To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data.","The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions.","Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break it, and what techniques one should use to make it more robust, and introduction additional way to attack models or increase their robustness.   ","Our analysis continues with a meta-study on the used approaches with their power, numerical experiments, and accompanied ablations studies.","We show that the developed attacks and defenses outperform existing alternatives from the literature while being practical in terms of execution, proving the validity of the competition as a tool for uncovering vulnerabilities of machine learning models and mitigating them in various domains."],"url":"http://arxiv.org/abs/2308.11406v1"}
{"created":"2023-08-22 12:42:56","title":"Parameterized Complexity of Simultaneous Planarity","abstract":"Given $k$ input graphs $G_1, \\dots ,G_k$, where each pair $G_i$, $G_j$ with $i \\neq j$ shares the same graph $G$, the problem Simultaneous Embedding With Fixed Edges (SEFE) asks whether there exists a planar drawing for each input graph such that all drawings coincide on $G$. While SEFE is still open for the case of two input graphs, the problem is NP-complete for $k \\geq 3$ [Schaefer, JGAA 13]. In this work, we explore the parameterized complexity of SEFE. We show that SEFE is FPT with respect to $k$ plus the vertex cover number or the feedback edge set number of the the union graph $G^\\cup = G_1 \\cup \\dots \\cup G_k$. Regarding the shared graph $G$, we show that SEFE is NP-complete, even if $G$ is a tree with maximum degree 4. Together with a known NP-hardness reduction [Angelini et al., TCS 15], this allows us to conclude that several parameters of $G$, including the maximum degree, the maximum number of degree-1 neighbors, the vertex cover number, and the number of cutvertices are intractable. We also settle the tractability of all pairs of these parameters. We give FPT algorithms for the vertex cover number plus either of the first two parameters and for the number of cutvertices plus the maximum degree, whereas we prove all remaining combinations to be intractable.","sentences":["Given $k$ input graphs $G_1, \\dots ,G_k$, where each pair $G_i$, $G_j$ with $i \\neq j$ shares the same graph $G$, the problem Simultaneous Embedding With Fixed Edges (SEFE) asks whether there exists a planar drawing for each input graph such that all drawings coincide on $G$. While SEFE is still open for the case of two input graphs, the problem is NP-complete for $k \\geq 3$ [Schaefer, JGAA 13].","In this work, we explore the parameterized complexity of SEFE.","We show that SEFE is FPT with respect to $k$ plus the vertex cover number or the feedback edge set number of the the union graph $G^\\cup = G_1 \\cup \\dots \\cup G_k$. Regarding the shared graph $G$, we show that SEFE is NP-complete, even if $G$ is a tree with maximum degree 4.","Together with a known NP-hardness reduction [Angelini et al., TCS 15], this allows us to conclude that several parameters of $G$, including the maximum degree, the maximum number of degree-1 neighbors, the vertex cover number, and the number of cutvertices are intractable.","We also settle the tractability of all pairs of these parameters.","We give FPT algorithms for the vertex cover number plus either of the first two parameters and for the number of cutvertices plus the maximum degree, whereas we prove all remaining combinations to be intractable."],"url":"http://arxiv.org/abs/2308.11401v1"}
{"created":"2023-08-22 12:37:29","title":"Towards an Understanding of Large Language Models in Software Engineering Tasks","abstract":"Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in tasks such as text generation and reasoning. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on the application and evaluation of LLMs in the field of software engineering. Therefore, this paper is the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases, and selected 123 papers for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize.","sentences":["Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in tasks such as text generation and reasoning.","Derivative products, like ChatGPT, have been extensively deployed and highly sought after.","Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus.","However, there is still a lack of systematic research on the application and evaluation of LLMs in the field of software engineering.","Therefore, this paper is the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering?","(2) Can LLMs effectively handle software engineering tasks?","To find the answers, we have collected related literature as extensively as possible from seven mainstream databases, and selected 123 papers for analysis.","We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs.","Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize."],"url":"http://arxiv.org/abs/2308.11396v1"}
{"created":"2023-08-22 12:36:33","title":"ULGss: A Strategy to construct a Library of Universal Logic Gates for $N$-variable Boolean Logic beyond NAND and NOR","abstract":"In literature, NAND and NOR are two logic gates that display functional completeness, hence regarded as Universal gates. So, the present effort is focused on exploring a library of universal gates in binary that are still unexplored in literature along with a broad and systematic approach to classify the logic connectives. The study shows that the number of Universal Gates in any logic system grows exponentially with the number of input variables $N$. It is revealed that there are $56$ Universal gates in binary for $N=3$. It is shown that the ratio of the count of Universal gates to the total number of Logic gates is $\\approx $ $\\frac{1}{4}$ or 0.25. Adding constants $0,1$ allow for the creation of $4$ additional (for $N=2$) and $169$ additional Universal Gates (for $N=3$). In this article, the mathematical and logical underpinnings of the concept of universal logic gates are presented, along with a search strategy $ULG_{SS}$ exploring multiple paths leading to their identification. A fast-track approach has been introduced that uses the hexadecimal representation of a logic gate to quickly ascertain its attribute.","sentences":["In literature, NAND and NOR are two logic gates that display functional completeness, hence regarded as Universal gates.","So, the present effort is focused on exploring a library of universal gates in binary that are still unexplored in literature along with a broad and systematic approach to classify the logic connectives.","The study shows that the number of Universal Gates in any logic system grows exponentially with the number of input variables $N$. It is revealed that there are $56$ Universal gates in binary for $N=3$. It is shown that the ratio of the count of Universal gates to the total number of Logic gates is $\\approx $ $\\frac{1}{4}$ or 0.25.","Adding constants $0,1$ allow for the creation of $4$ additional (for $N=2$) and $169$ additional Universal Gates (for $N=3$).","In this article, the mathematical and logical underpinnings of the concept of universal logic gates are presented, along with a search strategy $ULG_{SS}$ exploring multiple paths leading to their identification.","A fast-track approach has been introduced that uses the hexadecimal representation of a logic gate to quickly ascertain its attribute."],"url":"http://arxiv.org/abs/2308.11395v1"}
