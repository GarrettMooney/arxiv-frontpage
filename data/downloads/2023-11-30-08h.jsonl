{"created":"2023-11-29 18:59:59","title":"Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models","abstract":"We address the problem of synthesizing multi-view optical illusions: images that change appearance upon a transformation, such as a flip or rotation. We propose a simple, zero-shot method for obtaining these illusions from off-the-shelf text-to-image diffusion models. During the reverse diffusion process, we estimate the noise from different views of a noisy image. We then combine these noise estimates together and denoise the image. A theoretical analysis suggests that this method works precisely for views that can be written as orthogonal transformations, of which permutations are a subset. This leads to the idea of a visual anagram--an image that changes appearance under some rearrangement of pixels. This includes rotations and flips, but also more exotic pixel permutations such as a jigsaw rearrangement. Our approach also naturally extends to illusions with more than two views. We provide both qualitative and quantitative results demonstrating the effectiveness and flexibility of our method. Please see our project webpage for additional visualizations and results: https://dangeng.github.io/visual_anagrams/","sentences":["We address the problem of synthesizing multi-view optical illusions: images that change appearance upon a transformation, such as a flip or rotation.","We propose a simple, zero-shot method for obtaining these illusions from off-the-shelf text-to-image diffusion models.","During the reverse diffusion process, we estimate the noise from different views of a noisy image.","We then combine these noise estimates together and denoise the image.","A theoretical analysis suggests that this method works precisely for views that can be written as orthogonal transformations, of which permutations are a subset.","This leads to the idea of a visual anagram--an image that changes appearance under some rearrangement of pixels.","This includes rotations and flips, but also more exotic pixel permutations such as a jigsaw rearrangement.","Our approach also naturally extends to illusions with more than two views.","We provide both qualitative and quantitative results demonstrating the effectiveness and flexibility of our method.","Please see our project webpage for additional visualizations and results: https://dangeng.github.io/visual_anagrams/"],"url":"http://arxiv.org/abs/2311.17919v1"}
{"created":"2023-11-29 18:59:59","title":"Do text-free diffusion models learn discriminative visual representations?","abstract":"While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which addresses both families of tasks simultaneously. We identify diffusion models, a state-of-the-art method for generative tasks, as a prime candidate. Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high-fidelity, diverse, novel images. We find that the intermediate feature maps of the U-Net are diverse, discriminative feature representations. We propose a novel attention mechanism for pooling feature maps and further leverage this mechanism as DifFormer, a transformer feature fusion of features from different diffusion U-Net blocks and noise steps. We also develop DifFeed, a novel feedback mechanism tailored to diffusion. We find that diffusion models are better than GANs, and, with our fusion and feedback mechanisms, can compete with state-of-the-art unsupervised image representation learning methods for discriminative tasks - image classification with full and semi-supervision, transfer for fine-grained classification, object detection and segmentation, and semantic segmentation. Our project website (https://mgwillia.github.io/diffssl/) and code (https://github.com/soumik-kanad/diffssl) are available publicly.","sentences":["While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which addresses both families of tasks simultaneously.","We identify diffusion models, a state-of-the-art method for generative tasks, as a prime candidate.","Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high-fidelity, diverse, novel images.","We find that the intermediate feature maps of the U-Net are diverse, discriminative feature representations.","We propose a novel attention mechanism for pooling feature maps and further leverage this mechanism as DifFormer, a transformer feature fusion of features from different diffusion U-Net blocks and noise steps.","We also develop DifFeed, a novel feedback mechanism tailored to diffusion.","We find that diffusion models are better than GANs, and, with our fusion and feedback mechanisms, can compete with state-of-the-art unsupervised image representation learning methods for discriminative tasks - image classification with full and semi-supervision, transfer for fine-grained classification, object detection and segmentation, and semantic segmentation.","Our project website (https://mgwillia.github.io/diffssl/) and code (https://github.com/soumik-kanad/diffssl) are available publicly."],"url":"http://arxiv.org/abs/2311.17921v1"}
{"created":"2023-11-29 18:59:59","title":"A Simple Recipe for Language-guided Domain Generalized Segmentation","abstract":"Generalization to new domains not seen during training is one of the long-standing goals and challenges in deploying neural networks in real-world applications. Existing generalization techniques necessitate substantial data augmentation, potentially sourced from external datasets, and aim at learning invariant representations by imposing various alignment constraints. Large-scale pretraining has recently shown promising generalization capabilities, along with the potential of bridging different modalities. For instance, the recent advent of vision-language models like CLIP has opened the doorway for vision models to exploit the textual modality. In this paper, we introduce a simple framework for generalizing semantic segmentation networks by employing language as the source of randomization. Our recipe comprises three key ingredients: i) the preservation of the intrinsic CLIP robustness through minimal fine-tuning, ii) language-driven local style augmentation, and iii) randomization by locally mixing the source and augmented styles during training. Extensive experiments report state-of-the-art results on various generalization benchmarks. The code will be made available.","sentences":["Generalization to new domains not seen during training is one of the long-standing goals and challenges in deploying neural networks in real-world applications.","Existing generalization techniques necessitate substantial data augmentation, potentially sourced from external datasets, and aim at learning invariant representations by imposing various alignment constraints.","Large-scale pretraining has recently shown promising generalization capabilities, along with the potential of bridging different modalities.","For instance, the recent advent of vision-language models like CLIP has opened the doorway for vision models to exploit the textual modality.","In this paper, we introduce a simple framework for generalizing semantic segmentation networks by employing language as the source of randomization.","Our recipe comprises three key ingredients: i) the preservation of the intrinsic CLIP robustness through minimal fine-tuning, ii) language-driven local style augmentation, and iii) randomization by locally mixing the source and augmented styles during training.","Extensive experiments report state-of-the-art results on various generalization benchmarks.","The code will be made available."],"url":"http://arxiv.org/abs/2311.17922v1"}
{"created":"2023-11-29 18:59:47","title":"Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving","abstract":"In autonomous driving, predicting future events in advance and evaluating the foreseeable risks empowers autonomous vehicles to better plan their actions, enhancing safety and efficiency on the road. To this end, we propose Drive-WM, the first driving world model compatible with existing end-to-end planning models. Through a joint spatial-temporal modeling facilitated by view factorization, our model generates high-fidelity multiview videos in driving scenes. Building on its powerful generation ability, we showcase the potential of applying the world model for safe driving planning for the first time. Particularly, our Drive-WM enables driving into multiple futures based on distinct driving maneuvers, and determines the optimal trajectory according to the image-based rewards. Evaluation on real-world driving datasets verifies that our method could generate high-quality, consistent, and controllable multiview videos, opening up possibilities for real-world simulations and safe planning.","sentences":["In autonomous driving, predicting future events in advance and evaluating the foreseeable risks empowers autonomous vehicles to better plan their actions, enhancing safety and efficiency on the road.","To this end, we propose Drive-WM, the first driving world model compatible with existing end-to-end planning models.","Through a joint spatial-temporal modeling facilitated by view factorization, our model generates high-fidelity multiview videos in driving scenes.","Building on its powerful generation ability, we showcase the potential of applying the world model for safe driving planning for the first time.","Particularly, our Drive-WM enables driving into multiple futures based on distinct driving maneuvers, and determines the optimal trajectory according to the image-based rewards.","Evaluation on real-world driving datasets verifies that our method could generate high-quality, consistent, and controllable multiview videos, opening up possibilities for real-world simulations and safe planning."],"url":"http://arxiv.org/abs/2311.17918v1"}
{"created":"2023-11-29 18:59:32","title":"AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text","abstract":"We study the problem of creating high-fidelity and animatable 3D avatars from only textual descriptions. Existing text-to-avatar methods are either limited to static avatars which cannot be animated or struggle to generate animatable avatars with promising quality and precise pose control. To address these limitations, we propose AvatarStudio, a coarse-to-fine generative model that generates explicit textured 3D meshes for animatable human avatars. Specifically, AvatarStudio begins with a low-resolution NeRF-based representation for coarse generation, followed by incorporating SMPL-guided articulation into the explicit mesh representation to support avatar animation and high resolution rendering. To ensure view consistency and pose controllability of the resulting avatars, we introduce a 2D diffusion model conditioned on DensePose for Score Distillation Sampling supervision. By effectively leveraging the synergy between the articulated mesh representation and the DensePose-conditional diffusion model, AvatarStudio can create high-quality avatars from text that are ready for animation, significantly outperforming previous methods. Moreover, it is competent for many applications, e.g., multimodal avatar animations and style-guided avatar creation. For more results, please refer to our project page: http://jeff95.me/projects/avatarstudio.html","sentences":["We study the problem of creating high-fidelity and animatable 3D avatars from only textual descriptions.","Existing text-to-avatar methods are either limited to static avatars which cannot be animated or struggle to generate animatable avatars with promising quality and precise pose control.","To address these limitations, we propose AvatarStudio, a coarse-to-fine generative model that generates explicit textured 3D meshes for animatable human avatars.","Specifically, AvatarStudio begins with a low-resolution NeRF-based representation for coarse generation, followed by incorporating SMPL-guided articulation into the explicit mesh representation to support avatar animation and high resolution rendering.","To ensure view consistency and pose controllability of the resulting avatars, we introduce a 2D diffusion model conditioned on DensePose for Score Distillation Sampling supervision.","By effectively leveraging the synergy between the articulated mesh representation and the DensePose-conditional diffusion model, AvatarStudio can create high-quality avatars from text that are ready for animation, significantly outperforming previous methods.","Moreover, it is competent for many applications, e.g., multimodal avatar animations and style-guided avatar creation.","For more results, please refer to our project page: http://jeff95.me/projects/avatarstudio.html"],"url":"http://arxiv.org/abs/2311.17917v1"}
{"created":"2023-11-29 18:57:07","title":"OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation","abstract":"Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment. Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources, incurring inevitable additional costs. In this paper, we present OPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy, serving as a nearly free lunch to alleviate the hallucination issue without additional data, knowledge, or training. Our approach begins with an interesting observation that, most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens, but not all the previous tokens. Such partial over-trust inclination results in the neglecting of image tokens and describes the image content with hallucination. Statistically, we observe an 80%$\\sim$95% co-currency rate between hallucination contents and such knowledge aggregation patterns. Based on the observation, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue, along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens, and re-allocate the token selection if necessary. With extensive experiments, OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality. Our code is available at: https://github.com/shikiw/OPERA.","sentences":["Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment.","Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources, incurring inevitable additional costs.","In this paper, we present OPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy, serving as a nearly free lunch to alleviate the hallucination issue without additional data, knowledge, or training.","Our approach begins with an interesting observation that, most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens, but not all the previous tokens.","Such partial over-trust inclination results in the neglecting of image tokens and describes the image content with hallucination.","Statistically, we observe an 80%$\\sim$95% co-currency rate between hallucination contents and such knowledge aggregation patterns.","Based on the observation, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue, along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens, and re-allocate the token selection if necessary.","With extensive experiments, OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality.","Our code is available at: https://github.com/shikiw/OPERA."],"url":"http://arxiv.org/abs/2311.17911v1"}
{"created":"2023-11-29 18:56:32","title":"HUGS: Human Gaussian Splats","abstract":"Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs","sentences":["Recent advances in neural rendering have improved both training and rendering times by orders of magnitude.","While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment.","In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS).","Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes.","We utilize the SMPL body model to initialize the human Gaussians.","To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.","Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians.","We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation.","Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene.","We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work.","Our code will be announced here: https://github.com/apple/ml-hugs"],"url":"http://arxiv.org/abs/2311.17910v1"}
{"created":"2023-11-29 18:55:38","title":"CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting","abstract":"With the onset of diffusion-based generative models and their ability to generate text-conditioned images, content generation has received a massive invigoration. Recently, these models have been shown to provide useful guidance for the generation of 3D graphics assets. However, existing work in text-conditioned 3D generation faces fundamental constraints: (i) inability to generate detailed, multi-object scenes, (ii) inability to textually control multi-object configurations, and (iii) physically realistic scene composition. In this work, we propose CG3D, a method for compositionally generating scalable 3D assets that resolves these constraints. We find that explicit Gaussian radiance fields, parameterized to allow for compositions of objects, possess the capability to enable semantically and physically consistent scenes. By utilizing a guidance framework built around this explicit representation, we show state of the art results, capable of even exceeding the guiding diffusion model in terms of object combinations and physics accuracy.","sentences":["With the onset of diffusion-based generative models and their ability to generate text-conditioned images, content generation has received a massive invigoration.","Recently, these models have been shown to provide useful guidance for the generation of 3D graphics assets.","However, existing work in text-conditioned 3D generation faces fundamental constraints: (i) inability to generate detailed, multi-object scenes, (ii) inability to textually control multi-object configurations, and (iii) physically realistic scene composition.","In this work, we propose CG3D, a method for compositionally generating scalable 3D assets that resolves these constraints.","We find that explicit Gaussian radiance fields, parameterized to allow for compositions of objects, possess the capability to enable semantically and physically consistent scenes.","By utilizing a guidance framework built around this explicit representation, we show state of the art results, capable of even exceeding the guiding diffusion model in terms of object combinations and physics accuracy."],"url":"http://arxiv.org/abs/2311.17907v1"}
{"created":"2023-11-29 18:54:50","title":"SLO/GO Degradation-Loss Sensitivity in Climate-Human System Coupling","abstract":"The potential of extreme environmental change driven by a destabilized climate system is an alarming prospect for humanity. But the intricate, subtle ways Earth's climate couples to social and economic systems raise the question of when more incremental climate change signals the need for alarm. Questions about incremental sensitivity are particularly crucial for human systems that are organized by optimization. Optimization is most valuable in resolving complex interactions among multiple factors, however, those interactions can obscure coupling to underlying drivers such as environmental degradation. Here, using Multi-Objective Land Allocation as an example, we show that model features that are common across non-convex optimization problems drive hypersensitivities in climate-induced degradation--loss response. We show that catastrophic losses in human systems can occur well before catastrophic climate collapse. We find punctuated insensitive/hypersensitive degradation--loss response, which we trace to the contrasting effects of environmental degradation on subleading, local versus global optima (SLO/GO). We argue that the SLO/GO response we identify in land-allocation problems traces to features that are common across non-convex optimization problems more broadly. Given the broad range of human systems that rely on non-convex optimization, our results therefore suggest that substantial social and economic risks could be lurking in a broad range in human systems that are coupled to the environment, even in the absence of catastrophic changes to the environment itself.","sentences":["The potential of extreme environmental change driven by a destabilized climate system is an alarming prospect for humanity.","But the intricate, subtle ways Earth's climate couples to social and economic systems raise the question of when more incremental climate change signals the need for alarm.","Questions about incremental sensitivity are particularly crucial for human systems that are organized by optimization.","Optimization is most valuable in resolving complex interactions among multiple factors, however, those interactions can obscure coupling to underlying drivers such as environmental degradation.","Here, using Multi-Objective Land Allocation as an example, we show that model features that are common across non-convex optimization problems drive hypersensitivities in climate-induced degradation--loss response.","We show that catastrophic losses in human systems can occur well before catastrophic climate collapse.","We find punctuated insensitive/hypersensitive degradation--loss response, which we trace to the contrasting effects of environmental degradation on subleading, local versus global optima (SLO/GO).","We argue that the SLO/GO response we identify in land-allocation problems traces to features that are common across non-convex optimization problems more broadly.","Given the broad range of human systems that rely on non-convex optimization, our results therefore suggest that substantial social and economic risks could be lurking in a broad range in human systems that are coupled to the environment, even in the absence of catastrophic changes to the environment itself."],"url":"http://arxiv.org/abs/2311.17905v1"}
{"created":"2023-11-29 18:53:47","title":"Language-conditioned Detection Transformer","abstract":"We present a new open-vocabulary detection framework. Our framework uses both image-level labels and detailed detection annotations when available. Our framework proceeds in three steps. We first train a language-conditioned object detector on fully-supervised detection data. This detector gets to see the presence or absence of ground truth classes during training, and conditions prediction on the set of present classes. We use this detector to pseudo-label images with image-level labels. Our detector provides much more accurate pseudo-labels than prior approaches with its conditioning mechanism. Finally, we train an unconditioned open-vocabulary detector on the pseudo-annotated images. The resulting detector, named DECOLA, shows strong zero-shot performance in open-vocabulary LVIS benchmark as well as direct zero-shot transfer benchmarks on LVIS, COCO, Object365, and OpenImages. DECOLA outperforms the prior arts by 17.1 AP-rare and 9.4 mAP on zero-shot LVIS benchmark. DECOLA achieves state-of-the-art results in various model sizes, architectures, and datasets by only training on open-sourced data and academic-scale computing. Code is available at https://github.com/janghyuncho/DECOLA.","sentences":["We present a new open-vocabulary detection framework.","Our framework uses both image-level labels and detailed detection annotations when available.","Our framework proceeds in three steps.","We first train a language-conditioned object detector on fully-supervised detection data.","This detector gets to see the presence or absence of ground truth classes during training, and conditions prediction on the set of present classes.","We use this detector to pseudo-label images with image-level labels.","Our detector provides much more accurate pseudo-labels than prior approaches with its conditioning mechanism.","Finally, we train an unconditioned open-vocabulary detector on the pseudo-annotated images.","The resulting detector, named DECOLA, shows strong zero-shot performance in open-vocabulary LVIS benchmark as well as direct zero-shot transfer benchmarks on LVIS, COCO, Object365, and OpenImages.","DECOLA outperforms the prior arts by 17.1 AP-rare and 9.4 mAP on zero-shot LVIS benchmark.","DECOLA achieves state-of-the-art results in various model sizes, architectures, and datasets by only training on open-sourced data and academic-scale computing.","Code is available at https://github.com/janghyuncho/DECOLA."],"url":"http://arxiv.org/abs/2311.17902v1"}
{"created":"2023-11-29 18:53:34","title":"SODA: Bottleneck Diffusion Models for Representation Learning","abstract":"We introduce SODA, a self-supervised diffusion model, designed for representation learning. The model incorporates an image encoder, which distills a source view into a compact representation, that, in turn, guides the generation of related novel views. We show that by imposing a tight bottleneck between the encoder and a denoising decoder, and leveraging novel view synthesis as a self-supervised objective, we can turn diffusion models into strong representation learners, capable of capturing visual semantics in an unsupervised manner. To the best of our knowledge, SODA is the first diffusion model to succeed at ImageNet linear-probe classification, and, at the same time, it accomplishes reconstruction, editing and synthesis tasks across a wide range of datasets. Further investigation reveals the disentangled nature of its emergent latent space, that serves as an effective interface to control and manipulate the model's produced images. All in all, we aim to shed light on the exciting and promising potential of diffusion models, not only for image generation, but also for learning rich and robust representations.","sentences":["We introduce SODA, a self-supervised diffusion model, designed for representation learning.","The model incorporates an image encoder, which distills a source view into a compact representation, that, in turn, guides the generation of related novel views.","We show that by imposing a tight bottleneck between the encoder and a denoising decoder, and leveraging novel view synthesis as a self-supervised objective, we can turn diffusion models into strong representation learners, capable of capturing visual semantics in an unsupervised manner.","To the best of our knowledge, SODA is the first diffusion model to succeed at ImageNet linear-probe classification, and, at the same time, it accomplishes reconstruction, editing and synthesis tasks across a wide range of datasets.","Further investigation reveals the disentangled nature of its emergent latent space, that serves as an effective interface to control and manipulate the model's produced images.","All in all, we aim to shed light on the exciting and promising potential of diffusion models, not only for image generation, but also for learning rich and robust representations."],"url":"http://arxiv.org/abs/2311.17901v1"}
{"created":"2023-11-29 18:51:46","title":"Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis","abstract":"Hallucinations and unfaithful synthesis due to inaccurate prompts with insufficient semantic details are widely observed in multimodal generative models. A prevalent strategy to align multiple modalities is to fine-tune the generator with a large number of annotated text-image pairs. However, such a procedure is labor-consuming and resource-draining. The key question we ask is: can we enhance the quality and faithfulness of text-driven generative models beyond extensive text-image pair annotations? To address this question, we propose Knowledge Pursuit Prompting (KPP), a zero-shot framework that iteratively incorporates external knowledge to help generators produce reliable visual content. Instead of training generators to handle generic prompts, KPP employs a recursive knowledge query process to gather informative external facts from the knowledge base, instructs a language model to compress the acquired knowledge for prompt refinement, and utilizes text-driven generators for visual synthesis. The entire process is zero-shot, without accessing the architectures and parameters of generative models. We evaluate the framework across multiple text-driven generative tasks (image, 3D rendering, and video) on datasets of different domains. We further demonstrate the extensibility and adaptability of KPP through varying foundation model bases and instructions. Our results show that KPP is capable of generating faithful and semantically rich content across diverse visual domains, offering a promising solution to improve multimodal generative models.","sentences":["Hallucinations and unfaithful synthesis due to inaccurate prompts with insufficient semantic details are widely observed in multimodal generative models.","A prevalent strategy to align multiple modalities is to fine-tune the generator with a large number of annotated text-image pairs.","However, such a procedure is labor-consuming and resource-draining.","The key question we ask is: can we enhance the quality and faithfulness of text-driven generative models beyond extensive text-image pair annotations?","To address this question, we propose Knowledge Pursuit Prompting (KPP), a zero-shot framework that iteratively incorporates external knowledge to help generators produce reliable visual content.","Instead of training generators to handle generic prompts, KPP employs a recursive knowledge query process to gather informative external facts from the knowledge base, instructs a language model to compress the acquired knowledge for prompt refinement, and utilizes text-driven generators for visual synthesis.","The entire process is zero-shot, without accessing the architectures and parameters of generative models.","We evaluate the framework across multiple text-driven generative tasks (image, 3D rendering, and video) on datasets of different domains.","We further demonstrate the extensibility and adaptability of KPP through varying foundation model bases and instructions.","Our results show that KPP is capable of generating faithful and semantically rich content across diverse visual domains, offering a promising solution to improve multimodal generative models."],"url":"http://arxiv.org/abs/2311.17898v1"}
{"created":"2023-11-29 18:47:17","title":"Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation","abstract":"In this paper, we propose a simple yet effective approach for self-supervised video object segmentation (VOS). Our key insight is that the inherent structural dependencies present in DINO-pretrained Transformers can be leveraged to establish robust spatio-temporal correspondences in videos. Furthermore, simple clustering on this correspondence cue is sufficient to yield competitive segmentation results. Previous self-supervised VOS techniques majorly resort to auxiliary modalities or utilize iterative slot attention to assist in object discovery, which restricts their general applicability and imposes higher computational requirements. To deal with these challenges, we develop a simplified architecture that capitalizes on the emerging objectness from DINO-pretrained Transformers, bypassing the need for additional modalities or slot attention. Specifically, we first introduce a single spatio-temporal Transformer block to process the frame-wise DINO features and establish spatio-temporal dependencies in the form of self-attention. Subsequently, utilizing these attention maps, we implement hierarchical clustering to generate object segmentation masks. To train the spatio-temporal block in a fully self-supervised manner, we employ semantic and dynamic motion consistency coupled with entropy normalization. Our method demonstrates state-of-the-art performance across multiple unsupervised VOS benchmarks and particularly excels in complex real-world multi-object video segmentation tasks such as DAVIS-17-Unsupervised and YouTube-VIS-19. The code and model checkpoints will be released at https://github.com/shvdiwnkozbw/SSL-UVOS.","sentences":["In this paper, we propose a simple yet effective approach for self-supervised video object segmentation (VOS).","Our key insight is that the inherent structural dependencies present in DINO-pretrained Transformers can be leveraged to establish robust spatio-temporal correspondences in videos.","Furthermore, simple clustering on this correspondence cue is sufficient to yield competitive segmentation results.","Previous self-supervised VOS techniques majorly resort to auxiliary modalities or utilize iterative slot attention to assist in object discovery, which restricts their general applicability and imposes higher computational requirements.","To deal with these challenges, we develop a simplified architecture that capitalizes on the emerging objectness from DINO-pretrained Transformers, bypassing the need for additional modalities or slot attention.","Specifically, we first introduce a single spatio-temporal Transformer block to process the frame-wise DINO features and establish spatio-temporal dependencies in the form of self-attention.","Subsequently, utilizing these attention maps, we implement hierarchical clustering to generate object segmentation masks.","To train the spatio-temporal block in a fully self-supervised manner, we employ semantic and dynamic motion consistency coupled with entropy normalization.","Our method demonstrates state-of-the-art performance across multiple unsupervised VOS benchmarks and particularly excels in complex real-world multi-object video segmentation tasks such as DAVIS-17-Unsupervised and YouTube-VIS-19.","The code and model checkpoints will be released at https://github.com/shvdiwnkozbw/SSL-UVOS."],"url":"http://arxiv.org/abs/2311.17893v1"}
{"created":"2023-11-29 18:46:29","title":"A Pipeline For Discourse Circuits From CCG","abstract":"There is a significant disconnect between linguistic theory and modern NLP practice, which relies heavily on inscrutable black-box architectures. DisCoCirc is a newly proposed model for meaning that aims to bridge this divide, by providing neuro-symbolic models that incorporate linguistic structure. DisCoCirc represents natural language text as a `circuit' that captures the core semantic information of the text. These circuits can then be interpreted as modular machine learning models. Additionally, DisCoCirc fulfils another major aim of providing an NLP model that can be implemented on near-term quantum computers.   In this paper we describe a software pipeline that converts English text to its DisCoCirc representation. The pipeline achieves coverage over a large fragment of the English language. It relies on Combinatory Categorial Grammar (CCG) parses of the input text as well as coreference resolution information. This semantic and syntactic information is used in several steps to convert the text into a simply-typed $\\lambda$-calculus term, and then into a circuit diagram. This pipeline will enable the application of the DisCoCirc framework to NLP tasks, using both classical and quantum approaches.","sentences":["There is a significant disconnect between linguistic theory and modern NLP practice, which relies heavily on inscrutable black-box architectures.","DisCoCirc is a newly proposed model for meaning that aims to bridge this divide, by providing neuro-symbolic models that incorporate linguistic structure.","DisCoCirc represents natural language text as a `circuit' that captures the core semantic information of the text.","These circuits can then be interpreted as modular machine learning models.","Additionally, DisCoCirc fulfils another major aim of providing an NLP model that can be implemented on near-term quantum computers.   ","In this paper we describe a software pipeline that converts English text to its DisCoCirc representation.","The pipeline achieves coverage over a large fragment of the English language.","It relies on Combinatory Categorial Grammar (CCG) parses of the input text as well as coreference resolution information.","This semantic and syntactic information is used in several steps to convert the text into a simply-typed $\\lambda$-calculus term, and then into a circuit diagram.","This pipeline will enable the application of the DisCoCirc framework to NLP tasks, using both classical and quantum approaches."],"url":"http://arxiv.org/abs/2311.17892v1"}
{"created":"2023-11-29 18:44:12","title":"Pose Anything: A Graph-Based Approach for Category-Agnostic Pose Estimation","abstract":"Traditional 2D pose estimation models are limited by their category-specific design, making them suitable only for predefined object categories. This restriction becomes particularly challenging when dealing with novel objects due to the lack of relevant training data.   To address this limitation, category-agnostic pose estimation (CAPE) was introduced. CAPE aims to enable keypoint localization for arbitrary object categories using a single model, requiring minimal support images with annotated keypoints. This approach not only enables object pose generation based on arbitrary keypoint definitions but also significantly reduces the associated costs, paving the way for versatile and adaptable pose estimation applications.   We present a novel approach to CAPE that leverages the inherent geometrical relations between keypoints through a newly designed Graph Transformer Decoder. By capturing and incorporating this crucial structural information, our method enhances the accuracy of keypoint localization, marking a significant departure from conventional CAPE techniques that treat keypoints as isolated entities.   We validate our approach on the MP-100 benchmark, a comprehensive dataset comprising over 20,000 images spanning more than 100 categories. Our method outperforms the prior state-of-the-art by substantial margins, achieving remarkable improvements of 2.16% and 1.82% under 1-shot and 5-shot settings, respectively. Furthermore, our method's end-to-end training demonstrates both scalability and efficiency compared to previous CAPE approaches.","sentences":["Traditional 2D pose estimation models are limited by their category-specific design, making them suitable only for predefined object categories.","This restriction becomes particularly challenging when dealing with novel objects due to the lack of relevant training data.   ","To address this limitation, category-agnostic pose estimation (CAPE) was introduced.","CAPE aims to enable keypoint localization for arbitrary object categories using a single model, requiring minimal support images with annotated keypoints.","This approach not only enables object pose generation based on arbitrary keypoint definitions but also significantly reduces the associated costs, paving the way for versatile and adaptable pose estimation applications.   ","We present a novel approach to CAPE that leverages the inherent geometrical relations between keypoints through a newly designed Graph Transformer Decoder.","By capturing and incorporating this crucial structural information, our method enhances the accuracy of keypoint localization, marking a significant departure from conventional CAPE techniques that treat keypoints as isolated entities.   ","We validate our approach on the MP-100 benchmark, a comprehensive dataset comprising over 20,000 images spanning more than 100 categories.","Our method outperforms the prior state-of-the-art by substantial margins, achieving remarkable improvements of 2.16% and 1.82% under 1-shot and 5-shot settings, respectively.","Furthermore, our method's end-to-end training demonstrates both scalability and efficiency compared to previous CAPE approaches."],"url":"http://arxiv.org/abs/2311.17891v1"}
{"created":"2023-11-29 18:40:19","title":"Scale Ratio Tuning of Group Based Job Scheduling in HPC Systems","abstract":"During the initialization of a supercomputer job, no useful calculations are performed. A high proportion of initialization time results in idle computing resources and less computational efficiency. Certain methods and algorithms combining jobs into groups are used to optimize scheduling of jobs with high initialization proportion. The article considers the influence of the scale ratio setting in algorithm for the job groups formation, on the performance metrics of the workload manager. The study was carried out on the developed by authors Aleabased workload manager model. The model makes it possible to conduct a large number of experiments in reasonable time without losing the accuracy of the simulation. We performed a series of experiments involving various characteristics of the workload. The article represents the results of a study of the scale ratio influence on efficiency metrics for different initialization time proportions and input workflows with varying intensity and homogeneity. The presented results allow the workload managers administrators to set a scale ratio that provides an appropriate balance with contradictory efficiency metrics.","sentences":["During the initialization of a supercomputer job, no useful calculations are performed.","A high proportion of initialization time results in idle computing resources and less computational efficiency.","Certain methods and algorithms combining jobs into groups are used to optimize scheduling of jobs with high initialization proportion.","The article considers the influence of the scale ratio setting in algorithm for the job groups formation, on the performance metrics of the workload manager.","The study was carried out on the developed by authors Aleabased workload manager model.","The model makes it possible to conduct a large number of experiments in reasonable time without losing the accuracy of the simulation.","We performed a series of experiments involving various characteristics of the workload.","The article represents the results of a study of the scale ratio influence on efficiency metrics for different initialization time proportions and input workflows with varying intensity and homogeneity.","The presented results allow the workload managers administrators to set a scale ratio that provides an appropriate balance with contradictory efficiency metrics."],"url":"http://arxiv.org/abs/2311.17889v1"}
{"created":"2023-11-29 18:23:18","title":"TSDF-Sampling: Efficient Sampling for Neural Surface Field using Truncated Signed Distance Field","abstract":"Multi-view neural surface reconstruction has exhibited impressive results. However, a notable limitation is the prohibitively slow inference time when compared to traditional techniques, primarily attributed to the dense sampling, required to maintain the rendering quality. This paper introduces a novel approach that substantially reduces the number of samplings by incorporating the Truncated Signed Distance Field (TSDF) of the scene. While prior works have proposed importance sampling, their dependence on initial uniform samples over the entire space makes them unable to avoid performance degradation when trying to use less number of samples. In contrast, our method leverages the TSDF volume generated only by the trained views, and it proves to provide a reasonable bound on the sampling from upcoming novel views. As a result, we achieve high rendering quality by fully exploiting the continuous neural SDF estimation within the bounds given by the TSDF volume. Notably, our method is the first approach that can be robustly plug-and-play into a diverse array of neural surface field models, as long as they use the volume rendering technique. Our empirical results show an 11-fold increase in inference speed without compromising performance. The result videos are available at our project page: https://tsdf-sampling.github.io/","sentences":["Multi-view neural surface reconstruction has exhibited impressive results.","However, a notable limitation is the prohibitively slow inference time when compared to traditional techniques, primarily attributed to the dense sampling, required to maintain the rendering quality.","This paper introduces a novel approach that substantially reduces the number of samplings by incorporating the Truncated Signed Distance Field (TSDF) of the scene.","While prior works have proposed importance sampling, their dependence on initial uniform samples over the entire space makes them unable to avoid performance degradation when trying to use less number of samples.","In contrast, our method leverages the TSDF volume generated only by the trained views, and it proves to provide a reasonable bound on the sampling from upcoming novel views.","As a result, we achieve high rendering quality by fully exploiting the continuous neural SDF estimation within the bounds given by the TSDF volume.","Notably, our method is the first approach that can be robustly plug-and-play into a diverse array of neural surface field models, as long as they use the volume rendering technique.","Our empirical results show an 11-fold increase in inference speed without compromising performance.","The result videos are available at our project page: https://tsdf-sampling.github.io/"],"url":"http://arxiv.org/abs/2311.17878v1"}
{"created":"2023-11-29 18:21:24","title":"Enhancing Post-Hoc Explanation Benchmark Reliability for Image Classification","abstract":"Deep neural networks, while powerful for image classification, often operate as \"black boxes,\" complicating the understanding of their decision-making processes. Various explanation methods, particularly those generating saliency maps, aim to address this challenge. However, the inconsistency issues of faithfulness metrics hinder reliable benchmarking of explanation methods. This paper employs an approach inspired by psychometrics, utilizing Krippendorf's alpha to quantify the benchmark reliability of post-hoc methods in image classification. The study proposes model training modifications, including feeding perturbed samples and employing focal loss, to enhance robustness and calibration. Empirical evaluations demonstrate significant improvements in benchmark reliability across metrics, datasets, and post-hoc methods. This pioneering work establishes a foundation for more reliable evaluation practices in the realm of post-hoc explanation methods, emphasizing the importance of model robustness in the assessment process.","sentences":["Deep neural networks, while powerful for image classification, often operate as \"black boxes,\" complicating the understanding of their decision-making processes.","Various explanation methods, particularly those generating saliency maps, aim to address this challenge.","However, the inconsistency issues of faithfulness metrics hinder reliable benchmarking of explanation methods.","This paper employs an approach inspired by psychometrics, utilizing Krippendorf's alpha to quantify the benchmark reliability of post-hoc methods in image classification.","The study proposes model training modifications, including feeding perturbed samples and employing focal loss, to enhance robustness and calibration.","Empirical evaluations demonstrate significant improvements in benchmark reliability across metrics, datasets, and post-hoc methods.","This pioneering work establishes a foundation for more reliable evaluation practices in the realm of post-hoc explanation methods, emphasizing the importance of model robustness in the assessment process."],"url":"http://arxiv.org/abs/2311.17876v1"}
{"created":"2023-11-29 18:20:16","title":"FisherRF: Active View Selection and Uncertainty Quantification for Radiance Fields using Fisher Information","abstract":"This study addresses the challenging problem of active view selection and uncertainty quantification within the domain of Radiance Fields. Neural Radiance Fields (NeRF) have greatly advanced image rendering and reconstruction, but the limited availability of 2D images poses uncertainties stemming from occlusions, depth ambiguities, and imaging errors. Efficiently selecting informative views becomes crucial, and quantifying NeRF model uncertainty presents intricate challenges. Existing approaches either depend on model architecture or are based on assumptions regarding density distributions that are not generally applicable. By leveraging Fisher Information, we efficiently quantify observed information within Radiance Fields without ground truth data. This can be used for the next best view selection and pixel-wise uncertainty quantification. Our method overcomes existing limitations on model architecture and effectiveness, achieving state-of-the-art results in both view selection and uncertainty quantification, demonstrating its potential to advance the field of Radiance Fields. Our method with the 3D Gaussian Splatting backend could perform view selections at 70 fps.","sentences":["This study addresses the challenging problem of active view selection and uncertainty quantification within the domain of Radiance Fields.","Neural Radiance Fields (NeRF) have greatly advanced image rendering and reconstruction, but the limited availability of 2D images poses uncertainties stemming from occlusions, depth ambiguities, and imaging errors.","Efficiently selecting informative views becomes crucial, and quantifying NeRF model uncertainty presents intricate challenges.","Existing approaches either depend on model architecture or are based on assumptions regarding density distributions that are not generally applicable.","By leveraging Fisher Information, we efficiently quantify observed information within Radiance Fields without ground truth data.","This can be used for the next best view selection and pixel-wise uncertainty quantification.","Our method overcomes existing limitations on model architecture and effectiveness, achieving state-of-the-art results in both view selection and uncertainty quantification, demonstrating its potential to advance the field of Radiance Fields.","Our method with the 3D Gaussian Splatting backend could perform view selections at 70 fps."],"url":"http://arxiv.org/abs/2311.17874v1"}
{"created":"2023-11-29 18:19:24","title":"Assessing the reliability of multistate flow networks considering distance constraints","abstract":"Evaluating the reliability of complex technical networks, such as those in energy distribution, logistics, and transportation systems, is of paramount importance. These networks are often represented as multistate flow networks (MFNs). While there has been considerable research on assessing MFN reliability, many studies still need to pay more attention to a critical factor: transmission distance constraints. These constraints are typical in real-world applications, such as Internet infrastructure, where controlling the distances between data centers, network nodes, and end-users is vital for ensuring low latency and efficient data transmission. This paper addresses the evaluation of MFN reliability under distance constraints. Specifically, it focuses on determining the probability that a minimum of $d$ flow units can be transmitted successfully from a source node to a sink node, using only paths with lengths not exceeding a predefined distance limit of $\\lambda $. We introduce an effective algorithm to tackle this challenge, provide a benchmark example to illustrate its application and analyze its computational complexity.","sentences":["Evaluating the reliability of complex technical networks, such as those in energy distribution, logistics, and transportation systems, is of paramount importance.","These networks are often represented as multistate flow networks (MFNs).","While there has been considerable research on assessing MFN reliability, many studies still need to pay more attention to a critical factor: transmission distance constraints.","These constraints are typical in real-world applications, such as Internet infrastructure, where controlling the distances between data centers, network nodes, and end-users is vital for ensuring low latency and efficient data transmission.","This paper addresses the evaluation of MFN reliability under distance constraints.","Specifically, it focuses on determining the probability that a minimum of $d$ flow units can be transmitted successfully from a source node to a sink node, using only paths with lengths not exceeding a predefined distance limit of $\\lambda $.","We introduce an effective algorithm to tackle this challenge, provide a benchmark example to illustrate its application and analyze its computational complexity."],"url":"http://arxiv.org/abs/2311.17872v1"}
{"created":"2023-11-29 18:17:35","title":"SAIBench: A Structural Interpretation of AI for Science Through Benchmarks","abstract":"Artificial Intelligence for Science (AI4S) is an emerging research field that utilizes machine learning advancements to tackle complex scientific computational issues, aiming to enhance computational efficiency and accuracy. However, the data-driven nature of AI4S lacks the correctness or accuracy assurances of conventional scientific computing, posing challenges when deploying AI4S models in real-world applications. To mitigate these, more comprehensive benchmarking procedures are needed to better understand AI4S models. This paper introduces a novel benchmarking approach, known as structural interpretation, which addresses two key requirements: identifying the trusted operating range in the problem space and tracing errors back to their computational components. This method partitions both the problem and metric spaces, facilitating a structural exploration of these spaces. The practical utility and effectiveness of structural interpretation are illustrated through its application to three distinct AI4S workloads: machine-learning force fields (MLFF), jet tagging, and precipitation nowcasting. The benchmarks effectively model the trusted operating range, trace errors, and reveal novel perspectives for refining the model, training process, and data sampling strategy. This work is part of the SAIBench project, an AI4S benchmarking suite.","sentences":["Artificial Intelligence for Science (AI4S) is an emerging research field that utilizes machine learning advancements to tackle complex scientific computational issues, aiming to enhance computational efficiency and accuracy.","However, the data-driven nature of AI4S lacks the correctness or accuracy assurances of conventional scientific computing, posing challenges when deploying AI4S models in real-world applications.","To mitigate these, more comprehensive benchmarking procedures are needed to better understand AI4S models.","This paper introduces a novel benchmarking approach, known as structural interpretation, which addresses two key requirements: identifying the trusted operating range in the problem space and tracing errors back to their computational components.","This method partitions both the problem and metric spaces, facilitating a structural exploration of these spaces.","The practical utility and effectiveness of structural interpretation are illustrated through its application to three distinct AI4S workloads: machine-learning force fields (MLFF), jet tagging, and precipitation nowcasting.","The benchmarks effectively model the trusted operating range, trace errors, and reveal novel perspectives for refining the model, training process, and data sampling strategy.","This work is part of the SAIBench project, an AI4S benchmarking suite."],"url":"http://arxiv.org/abs/2311.17869v1"}
{"created":"2023-11-29 18:14:34","title":"Space-Optimal Profile Estimation in Data Streams with Applications to Symmetric Functions","abstract":"We revisit the problem of estimating the profile (also known as the rarity) in the data stream model. Given a sequence of $m$ elements from a universe of size $n$, its profile is a vector $\\phi$ whose $i$-th entry $\\phi_i$ represents the number of distinct elements that appear in the stream exactly $i$ times. A classic paper by Datar and Muthukrishan from 2002 gave an algorithm which estimates any entry $\\phi_i$ up to an additive error of $\\pm \\epsilon D$ using $O(1/\\epsilon^2 (\\log n + \\log m))$ bits of space, where $D$ is the number of distinct elements in the stream. In this paper, we considerably improve on this result by designing an algorithm which simultaneously estimates many coordinates of the profile vector $\\phi$ up to small overall error. We give an algorithm which, with constant probability, produces an estimated profile $\\hat\\phi$ with the following guarantees in terms of space and estimation error:   - For any constant $\\tau$, with $O(1 / \\epsilon^2 + \\log n)$ bits of space, $\\sum_{i=1}^\\tau |\\phi_i - \\hat\\phi_i| \\leq \\epsilon D$.   - With $O(1/ \\epsilon^2\\log (1/\\epsilon) + \\log n + \\log \\log m)$ bits of space, $\\sum_{i=1}^m |\\phi_i - \\hat\\phi_i| \\leq \\epsilon m$.   In addition to bounding the error across multiple coordinates, our space bounds separate the terms that depend on $1/\\epsilon$ and those that depend on $n$ and $m$. We prove matching lower bounds on space in both regimes. Application of our profile estimation algorithm gives estimates within error $\\pm \\epsilon D$ of several symmetric functions of frequencies in $O(1/\\epsilon^2 + \\log n)$ bits. This generalizes space-optimal algorithms for the distinct elements problems to other problems including estimating the Huber and Tukey losses as well as frequency cap statistics.","sentences":["We revisit the problem of estimating the profile (also known as the rarity) in the data stream model.","Given a sequence of $m$ elements from a universe of size $n$, its profile is a vector $\\phi$ whose $i$-th entry $\\phi_i$ represents the number of distinct elements that appear in the stream exactly $i$ times.","A classic paper by Datar and Muthukrishan from 2002 gave an algorithm which estimates any entry $\\phi_i$ up to an additive error of $\\pm \\epsilon D$ using $O(1/\\epsilon^2 (\\log n + \\log m))$ bits of space, where $D$ is the number of distinct elements in the stream.","In this paper, we considerably improve on this result by designing an algorithm which simultaneously estimates many coordinates of the profile vector $\\phi$ up to small overall error.","We give an algorithm which, with constant probability, produces an estimated profile $\\hat\\phi$ with the following guarantees in terms of space and estimation error:   -","For any constant $\\tau$, with $O(1 / \\epsilon^2 + \\log n)$ bits of space, $\\sum_{i=1}^\\tau |\\phi_i - \\hat\\phi_i| \\leq \\epsilon D$.   - With $O(1/ \\epsilon^2\\log (1/\\epsilon) + \\log n + \\log \\log m)$ bits of space, $\\sum_{i=1}^m |\\phi_i - \\hat\\phi_i| \\leq \\epsilon m$.   ","In addition to bounding the error across multiple coordinates, our space bounds separate the terms that depend on $1/\\epsilon$ and those that depend on $n$ and $m$. We prove matching lower bounds on space in both regimes.","Application of our profile estimation algorithm gives estimates within error $\\pm \\epsilon D$ of several symmetric functions of frequencies in $O(1/\\epsilon^2 + \\log n)$ bits.","This generalizes space-optimal algorithms for the distinct elements problems to other problems including estimating the Huber and Tukey losses as well as frequency cap statistics."],"url":"http://arxiv.org/abs/2311.17868v1"}
{"created":"2023-11-29 18:07:42","title":"Evaluation of a measurement system for PET imaging studies","abstract":"Positron Emission Tomography (PET) enables functional imaging of deep brain structures, but the bulk and weight of current systems preclude their use during many natural human activities, such as locomotion. The proposed long-term solution is to construct a robotic system that can support an imaging system surrounding the subject's head, and then move the system to accommodate natural motion. This requires a system to measure the motion of the head with respect to the imaging ring, for use by both the robotic system and the image reconstruction software. We report here the design and experimental evaluation of a parallel string encoder mechanism for sensing this motion. Our preliminary results indicate that the measurement system may achieve accuracy within 0.5 mm, especially for small motions, with improved accuracy possible through kinematic calibration.","sentences":["Positron Emission Tomography (PET) enables functional imaging of deep brain structures, but the bulk and weight of current systems preclude their use during many natural human activities, such as locomotion.","The proposed long-term solution is to construct a robotic system that can support an imaging system surrounding the subject's head, and then move the system to accommodate natural motion.","This requires a system to measure the motion of the head with respect to the imaging ring, for use by both the robotic system and the image reconstruction software.","We report here the design and experimental evaluation of a parallel string encoder mechanism for sensing this motion.","Our preliminary results indicate that the measurement system may achieve accuracy within 0.5 mm, especially for small motions, with improved accuracy possible through kinematic calibration."],"url":"http://arxiv.org/abs/2311.17863v1"}
{"created":"2023-11-29 18:06:37","title":"Method for robotic motion compensation during PET imaging of mobile subjects","abstract":"Studies of the human brain during natural activities, such as locomotion, would benefit from the ability to image deep brain structures during these activities. While Positron Emission Tomography (PET) can image these structures, the bulk and weight of current scanners are not compatible with the desire for a wearable device. This has motivated the design of a robotic system to support a PET imaging system around the subject's head and to move the system to accommodate natural motion. We report here the design and experimental evaluation of a prototype robotic system that senses motion of a subject's head, using parallel string encoders connected between the robot-supported imaging ring and a helmet worn by the subject. This measurement is used to robotically move the imaging ring (coarse motion correction) and to compensate for residual motion during image reconstruction (fine motion correction). Minimization of latency and measurement error are the key design goals, respectively, for coarse and fine motion correction. The system is evaluated using recorded human head motions during locomotion, with a mock imaging system consisting of lasers and cameras, and is shown to provide an overall system latency of about 80 ms, which is sufficient for coarse motion correction and collision avoidance, as well as a measurement accuracy of about 0.5 mm for fine motion correction.","sentences":["Studies of the human brain during natural activities, such as locomotion, would benefit from the ability to image deep brain structures during these activities.","While Positron Emission Tomography (PET) can image these structures, the bulk and weight of current scanners are not compatible with the desire for a wearable device.","This has motivated the design of a robotic system to support a PET imaging system around the subject's head and to move the system to accommodate natural motion.","We report here the design and experimental evaluation of a prototype robotic system that senses motion of a subject's head, using parallel string encoders connected between the robot-supported imaging ring and a helmet worn by the subject.","This measurement is used to robotically move the imaging ring (coarse motion correction) and to compensate for residual motion during image reconstruction (fine motion correction).","Minimization of latency and measurement error are the key design goals, respectively, for coarse and fine motion correction.","The system is evaluated using recorded human head motions during locomotion, with a mock imaging system consisting of lasers and cameras, and is shown to provide an overall system latency of about 80 ms, which is sufficient for coarse motion correction and collision avoidance, as well as a measurement accuracy of about 0.5 mm for fine motion correction."],"url":"http://arxiv.org/abs/2311.17861v1"}
{"created":"2023-11-29 18:05:45","title":"On the Verification of the Correctness of a Subgraph Construction Algorithm","abstract":"We automatically verify the crucial steps in the original proof of correctness of an algorithm which, given a geometric graph satisfying certain additional properties removes edges in a systematic way for producing a connected graph in which edges do not (geometrically) intersect. The challenge in this case is representing and reasoning about geometric properties of graphs in the Euclidean plane, about their vertices and edges, and about connectivity. For modelling the geometric aspects, we use an axiomatization of plane geometry; for representing the graph structure we use additional predicates; for representing certain classes of paths in geometric graphs we use linked lists.","sentences":["We automatically verify the crucial steps in the original proof of correctness of an algorithm which, given a geometric graph satisfying certain additional properties removes edges in a systematic way for producing a connected graph in which edges do not (geometrically) intersect.","The challenge in this case is representing and reasoning about geometric properties of graphs in the Euclidean plane, about their vertices and edges, and about connectivity.","For modelling the geometric aspects, we use an axiomatization of plane geometry; for representing the graph structure we use additional predicates; for representing certain classes of paths in geometric graphs we use linked lists."],"url":"http://arxiv.org/abs/2311.17860v1"}
{"created":"2023-11-29 18:04:07","title":"Gaussian Shell Maps for Efficient 3D Human Generation","abstract":"Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of $512 \\times 512$ pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.","sentences":["Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production.","3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets.","Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers.","Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold.","In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells.","The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose.","Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features.","These Gaussians are efficiently and differentiably rendered.","The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses.","Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of $512 \\times 512$ pixels.","We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion."],"url":"http://arxiv.org/abs/2311.17857v1"}
{"created":"2023-11-29 18:02:29","title":"Leveraging Graph Diffusion Models for Network Refinement Tasks","abstract":"Most real-world networks are noisy and incomplete samples from an unknown target distribution. Refining them by correcting corruptions or inferring unobserved regions typically improves downstream performance. Inspired by the impressive generative capabilities that have been used to correct corruptions in images, and the similarities between \"in-painting\" and filling in missing nodes and edges conditioned on the observed graph, we propose a novel graph generative framework, SGDM, which is based on subgraph diffusion. Our framework not only improves the scalability and fidelity of graph diffusion models, but also leverages the reverse process to perform novel, conditional generation tasks. In particular, through extensive empirical analysis and a set of novel metrics, we demonstrate that our proposed model effectively supports the following refinement tasks for partially observable networks: T1: denoising extraneous subgraphs, T2: expanding existing subgraphs and T3: performing \"style\" transfer by regenerating a particular subgraph to match the characteristics of a different node or subgraph.","sentences":["Most real-world networks are noisy and incomplete samples from an unknown target distribution.","Refining them by correcting corruptions or inferring unobserved regions typically improves downstream performance.","Inspired by the impressive generative capabilities that have been used to correct corruptions in images, and the similarities between \"in-painting\" and filling in missing nodes and edges conditioned on the observed graph, we propose a novel graph generative framework, SGDM, which is based on subgraph diffusion.","Our framework not only improves the scalability and fidelity of graph diffusion models, but also leverages the reverse process to perform novel, conditional generation tasks.","In particular, through extensive empirical analysis and a set of novel metrics, we demonstrate that our proposed model effectively supports the following refinement tasks for partially observable networks: T1: denoising extraneous subgraphs, T2: expanding existing subgraphs and T3: performing \"style\" transfer by regenerating a particular subgraph to match the characteristics of a different node or subgraph."],"url":"http://arxiv.org/abs/2311.17856v1"}
{"created":"2023-11-29 18:00:41","title":"Maximum Entropy Model Correction in Reinforcement Learning","abstract":"We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model's next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.","sentences":["We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error.","If the model is accurate enough, it accelerates the convergence to the true value function too.","One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model's next-state distributions based on a Maximum Entropy density estimation formulation.","Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna.","We show that MoCoVI and MoCoDyna's convergence can be much faster than the conventional model-free algorithms.","Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function."],"url":"http://arxiv.org/abs/2311.17855v1"}
{"created":"2023-11-29 17:59:18","title":"On the Adversarial Robustness of Graph Contrastive Learning Methods","abstract":"Contrastive learning (CL) has emerged as a powerful framework for learning representations of images and text in a self-supervised manner while enhancing model robustness against adversarial attacks. More recently, researchers have extended the principles of contrastive learning to graph-structured data, giving birth to the field of graph contrastive learning (GCL). However, whether GCL methods can deliver the same advantages in adversarial robustness as their counterparts in the image and text domains remains an open question. In this paper, we introduce a comprehensive robustness evaluation protocol tailored to assess the robustness of GCL models. We subject these models to adaptive adversarial attacks targeting the graph structure, specifically in the evasion scenario. We evaluate node and graph classification tasks using diverse real-world datasets and attack strategies. With our work, we aim to offer insights into the robustness of GCL methods and hope to open avenues for potential future research directions.","sentences":["Contrastive learning (CL) has emerged as a powerful framework for learning representations of images and text in a self-supervised manner while enhancing model robustness against adversarial attacks.","More recently, researchers have extended the principles of contrastive learning to graph-structured data, giving birth to the field of graph contrastive learning (GCL).","However, whether GCL methods can deliver the same advantages in adversarial robustness as their counterparts in the image and text domains remains an open question.","In this paper, we introduce a comprehensive robustness evaluation protocol tailored to assess the robustness of GCL models.","We subject these models to adaptive adversarial attacks targeting the graph structure, specifically in the evasion scenario.","We evaluate node and graph classification tasks using diverse real-world datasets and attack strategies.","With our work, we aim to offer insights into the robustness of GCL methods and hope to open avenues for potential future research directions."],"url":"http://arxiv.org/abs/2311.17853v1"}
{"created":"2023-11-29 17:56:51","title":"A Computing-in-Memory-based One-Class Hyperdimensional Computing Model for Outlier Detection","abstract":"In this work, we present ODHD, an algorithm for outlier detection based on hyperdimensional computing (HDC), a non-classical learning paradigm. Along with the HDC-based algorithm, we propose IM-ODHD, a computing-in-memory (CiM) implementation based on hardware/software (HW/SW) codesign for improved latency and energy efficiency. The training and testing phases of ODHD may be performed with conventional CPU/GPU hardware or our IM-ODHD, SRAM-based CiM architecture using the proposed HW/SW codesign techniques. We evaluate the performance of ODHD on six datasets from different application domains using three metrics, namely accuracy, F1 score, and ROC-AUC, and compare it with multiple baseline methods such as OCSVM, isolation forest, and autoencoder. The experimental results indicate that ODHD outperforms all the baseline methods in terms of these three metrics on every dataset for both CPU/GPU and CiM implementations. Furthermore, we perform an extensive design space exploration to demonstrate the tradeoff between delay, energy efficiency, and performance of ODHD. We demonstrate that the HW/SW codesign implementation of the outlier detection on IM-ODHD is able to outperform the GPU-based implementation of ODHD by at least 293x/419x in terms of training/testing latency (and on average 16.0x/15.9x in terms of training/testing energy consumption).","sentences":["In this work, we present ODHD, an algorithm for outlier detection based on hyperdimensional computing (HDC), a non-classical learning paradigm.","Along with the HDC-based algorithm, we propose IM-ODHD, a computing-in-memory (CiM) implementation based on hardware/software (HW/SW) codesign for improved latency and energy efficiency.","The training and testing phases of ODHD may be performed with conventional CPU/GPU hardware or our IM-ODHD, SRAM-based CiM architecture using the proposed HW/SW codesign techniques.","We evaluate the performance of ODHD on six datasets from different application domains using three metrics, namely accuracy, F1 score, and ROC-AUC, and compare it with multiple baseline methods such as OCSVM, isolation forest, and autoencoder.","The experimental results indicate that ODHD outperforms all the baseline methods in terms of these three metrics on every dataset for both CPU/GPU and CiM implementations.","Furthermore, we perform an extensive design space exploration to demonstrate the tradeoff between delay, energy efficiency, and performance of ODHD.","We demonstrate that the HW/SW codesign implementation of the outlier detection on IM-ODHD is able to outperform the GPU-based implementation of ODHD by at least 293x/419x in terms of training/testing latency (and on average 16.0x/15.9x in terms of training/testing energy consumption)."],"url":"http://arxiv.org/abs/2311.17852v1"}
{"created":"2023-11-29 17:54:22","title":"Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects","abstract":"Unlabeled 3D objects present an opportunity to leverage pretrained vision language models (VLMs) on a range of annotation tasks -- from describing object semantics to physical properties. An accurate response must take into account the full appearance of the object in 3D, various ways of phrasing the question/prompt, and changes in other factors that affect the response. We present a method to marginalize over any factors varied across VLM queries, utilizing the VLM's scores for sampled responses. We first show that this probabilistic aggregation can outperform a language model (e.g., GPT4) for summarization, for instance avoiding hallucinations when there are contrasting details between responses. Secondly, we show that aggregated annotations are useful for prompt-chaining; they help improve downstream VLM predictions (e.g., of object material when the object's type is specified as an auxiliary input in the prompt). Such auxiliary inputs allow ablating and measuring the contribution of visual reasoning over language-only reasoning. Using these evaluations, we show how VLMs can approach, without additional training or in-context learning, the quality of human-verified type and material annotations on the large-scale Objaverse dataset.","sentences":["Unlabeled 3D objects present an opportunity to leverage pretrained vision language models (VLMs) on a range of annotation tasks -- from describing object semantics to physical properties.","An accurate response must take into account the full appearance of the object in 3D, various ways of phrasing the question/prompt, and changes in other factors that affect the response.","We present a method to marginalize over any factors varied across VLM queries, utilizing the VLM's scores for sampled responses.","We first show that this probabilistic aggregation can outperform a language model (e.g., GPT4) for summarization, for instance avoiding hallucinations when there are contrasting details between responses.","Secondly, we show that aggregated annotations are useful for prompt-chaining; they help improve downstream VLM predictions (e.g., of object material when the object's type is specified as an auxiliary input in the prompt).","Such auxiliary inputs allow ablating and measuring the contribution of visual reasoning over language-only reasoning.","Using these evaluations, we show how VLMs can approach, without additional training or in-context learning, the quality of human-verified type and material annotations on the large-scale Objaverse dataset."],"url":"http://arxiv.org/abs/2311.17851v1"}
{"created":"2023-11-29 17:51:18","title":"Traversing automata with current state uncertainty under LTL$_f$ constraints","abstract":"In this paper, we consider a problem which we call LTL$_f$ model checking on paths: given a DFA $\\mathcal{A}$ and a formula $\\phi$ in LTL on finite traces, does there exist a word $w$ such that every path starting in a state of $\\mathcal{A}$ and labeled by $w$ satisfies $\\phi$? The original motivation for this problem comes from the constrained parts orienting problem, introduced in [Petra Wolf, \"Synchronization Under Dynamic Constraints\", FSTTCS 2020], where the input constraints restrict the order in which certain states are visited for the first or the last time while reading a word $w$ which is also required to synchronize $\\mathcal{A}$. We identify very general conditions under which LTL$_f$ model checking on paths is solvable in polynomial space. For the particular constraints in the parts orienting problem, we consider PSPACE-complete cases and one NP-complete case. The former provide very strong lower bound for LTL$_f$ model checking on paths. The latter is related to (classical) LTL$_f$ model checking for formulas with the until modality only and with no nesting of operators. We also consider LTL$_f$ model checking of the power-set automaton of a given DFA, and get similar results for this setting. For all our problems, we consider the case where the required word must also be synchronizing, and prove that if the problem does not become trivial, then this additional constraint does not change the complexity.","sentences":["In this paper, we consider a problem which we call LTL$_f$ model checking on paths: given a DFA $\\mathcal{A}$ and a formula $\\phi$ in LTL on finite traces, does there exist a word $w$ such that every path starting in a state of $\\mathcal{A}$ and labeled by $w$ satisfies $\\phi$?","The original motivation for this problem comes from the constrained parts orienting problem, introduced in [Petra Wolf, \"Synchronization Under Dynamic Constraints\", FSTTCS 2020], where the input constraints restrict the order in which certain states are visited for the first or the last time while reading a word $w$ which is also required to synchronize $\\mathcal{A}$. We identify very general conditions under which LTL$_f$ model checking on paths is solvable in polynomial space.","For the particular constraints in the parts orienting problem, we consider PSPACE-complete cases and one NP-complete case.","The former provide very strong lower bound for LTL$_f$ model checking on paths.","The latter is related to (classical) LTL$_f$ model checking for formulas with the until modality only and with no nesting of operators.","We also consider LTL$_f$ model checking of the power-set automaton of a given DFA, and get similar results for this setting.","For all our problems, we consider the case where the required word must also be synchronizing, and prove that if the problem does not become trivial, then this additional constraint does not change the complexity."],"url":"http://arxiv.org/abs/2311.17849v1"}
{"created":"2023-11-29 17:49:48","title":"FastSample: Accelerating Distributed Graph Neural Network Training for Billion-Scale Graphs","abstract":"Training Graph Neural Networks(GNNs) on a large monolithic graph presents unique challenges as the graph cannot fit within a single machine and it cannot be decomposed into smaller disconnected components. Distributed sampling-based training distributes the graph across multiple machines and trains the GNN on small parts of the graph that are randomly sampled every training iteration. We show that in a distributed environment, the sampling overhead is a significant component of the training time for large-scale graphs. We propose FastSample which is composed of two synergistic techniques that greatly reduce the distributed sampling time: 1)a new graph partitioning method that eliminates most of the communication rounds in distributed sampling , 2)a novel highly optimized sampling kernel that reduces memory movement during sampling. We test FastSample on large-scale graph benchmarks and show that FastSample speeds up distributed sampling-based GNN training by up to 2x with no loss in accuracy.","sentences":["Training Graph Neural Networks(GNNs) on a large monolithic graph presents unique challenges as the graph cannot fit within a single machine and it cannot be decomposed into smaller disconnected components.","Distributed sampling-based training distributes the graph across multiple machines and trains the GNN on small parts of the graph that are randomly sampled every training iteration.","We show that in a distributed environment, the sampling overhead is a significant component of the training time for large-scale graphs.","We propose FastSample which is composed of two synergistic techniques that greatly reduce the distributed sampling time: 1)a new graph partitioning method that eliminates most of the communication rounds in distributed sampling , 2)a novel highly optimized sampling kernel that reduces memory movement during sampling.","We test FastSample on large-scale graph benchmarks and show that FastSample speeds up distributed sampling-based GNN training by up to 2x with no loss in accuracy."],"url":"http://arxiv.org/abs/2311.17847v1"}
{"created":"2023-11-29 17:49:33","title":"Towards Real-World Focus Stacking with Deep Learning","abstract":"Focus stacking is widely used in micro, macro, and landscape photography to reconstruct all-in-focus images from multiple frames obtained with focus bracketing, that is, with shallow depth of field and different focus planes. Existing deep learning approaches to the underlying multi-focus image fusion problem have limited applicability to real-world imagery since they are designed for very short image sequences (two to four images), and are typically trained on small, low-resolution datasets either acquired by light-field cameras or generated synthetically. We introduce a new dataset consisting of 94 high-resolution bursts of raw images with focus bracketing, with pseudo ground truth computed from the data using state-of-the-art commercial software. This dataset is used to train the first deep learning algorithm for focus stacking capable of handling bursts of sufficient length for real-world applications. Qualitative experiments demonstrate that it is on par with existing commercial solutions in the long-burst, realistic regime while being significantly more tolerant to noise. The code and dataset are available at https://github.com/araujoalexandre/FocusStackingDataset.","sentences":["Focus stacking is widely used in micro, macro, and landscape photography to reconstruct all-in-focus images from multiple frames obtained with focus bracketing, that is, with shallow depth of field and different focus planes.","Existing deep learning approaches to the underlying multi-focus image fusion problem have limited applicability to real-world imagery since they are designed for very short image sequences (two to four images), and are typically trained on small, low-resolution datasets either acquired by light-field cameras or generated synthetically.","We introduce a new dataset consisting of 94 high-resolution bursts of raw images with focus bracketing, with pseudo ground truth computed from the data using state-of-the-art commercial software.","This dataset is used to train the first deep learning algorithm for focus stacking capable of handling bursts of sufficient length for real-world applications.","Qualitative experiments demonstrate that it is on par with existing commercial solutions in the long-burst, realistic regime while being significantly more tolerant to noise.","The code and dataset are available at https://github.com/araujoalexandre/FocusStackingDataset."],"url":"http://arxiv.org/abs/2311.17846v1"}
{"created":"2023-11-29 17:46:25","title":"Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning","abstract":"In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.","sentences":["In this study, we are interested in imbuing robots with the capability of physically-grounded task planning.","Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning.","However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs.","We argue that a task planner should be an inherently grounded, unified multimodal system.","To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps.","ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes.","It also supports flexible multimodal goal specification and naturally incorporates visual feedback.","Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks."],"url":"http://arxiv.org/abs/2311.17842v1"}
{"created":"2023-11-29 17:46:17","title":"Fast list-decoding of univariate multiplicity and folded Reed-Solomon codes","abstract":"We show that the known list-decoding algorithms for univariate multiplicity and folded Reed-Solomon (FRS) codes can be made to run in nearly-linear time. This yields, to our knowledge, the first known family of codes that can be decoded in nearly linear time, even as they approach the list decoding capacity. Univariate multiplicity codes and FRS codes are natural variants of Reed-Solomon codes that were discovered and studied for their applications to list-decoding. It is known that for every $\\epsilon >0$, and rate $R \\in (0,1)$, there exist explicit families of these codes that have rate $R$ and can be list-decoded from a $(1-R-\\epsilon)$ fraction of errors with constant list size in polynomial time (Guruswami & Wang (IEEE Trans. Inform. Theory, 2013) and Kopparty, Ron-Zewi, Saraf & Wootters (SIAM J. Comput. 2023)). In this work, we present randomized algorithms that perform the above tasks in nearly linear time. Our algorithms have two main components. The first builds upon the lattice-based approach of Alekhnovich (IEEE Trans. Inf. Theory 2005), who designed a nearly linear time list-decoding algorithm for Reed-Solomon codes approaching the Johnson radius. As part of the second component, we design nearly-linear time algorithms for two natural algebraic problems. The first algorithm solves linear differential equations of the form $Q\\left(x, f(x), \\frac{df}{dx}, \\dots,\\frac{d^m f}{dx^m}\\right) \\equiv 0$ where $Q$ has the form $Q(x,y_0,\\dots,y_m) = \\tilde{Q}(x) + \\sum_{i = 0}^m Q_i(x)\\cdot y_i$. The second solves functional equations of the form $Q\\left(x, f(x), f(\\gamma x), \\dots,f(\\gamma^m x)\\right) \\equiv 0$ where $\\gamma$ is a high-order field element. These algorithms can be viewed as generalizations of classical algorithms of Sieveking (Computing 1972) and Kung (Numer. Math. 1974) for computing the modular inverse of a power series, and might be of independent interest.","sentences":["We show that the known list-decoding algorithms for univariate multiplicity and folded Reed-Solomon (FRS) codes can be made to run in nearly-linear time.","This yields, to our knowledge, the first known family of codes that can be decoded in nearly linear time, even as they approach the list decoding capacity.","Univariate multiplicity codes and FRS codes are natural variants of Reed-Solomon codes that were discovered and studied for their applications to list-decoding.","It is known that for every $\\epsilon >0$, and rate $R \\in (0,1)$, there exist explicit families of these codes that have rate $R$ and can be list-decoded from a $(1-R-\\epsilon)$ fraction of errors with constant list size in polynomial time (Guruswami & Wang (IEEE Trans.","Inform.","Theory, 2013) and Kopparty, Ron-Zewi, Saraf & Wootters (SIAM J. Comput. 2023)).","In this work, we present randomized algorithms that perform the above tasks in nearly linear time.","Our algorithms have two main components.","The first builds upon the lattice-based approach of Alekhnovich (IEEE Trans.","Inf.","Theory 2005), who designed a nearly linear time list-decoding algorithm for Reed-Solomon codes approaching the Johnson radius.","As part of the second component, we design nearly-linear time algorithms for two natural algebraic problems.","The first algorithm solves linear differential equations of the form $Q\\left(x, f(x), \\frac{df}{dx}, \\dots,\\frac{d^m f}{dx^m}\\right) \\equiv 0$ where $Q$ has the form $Q(x,y_0,\\dots,y_m)","= \\tilde{Q}(x)","+ \\sum_{i = 0}^m Q_i(x)\\cdot","y_i$.","The second solves functional equations of the form $Q\\left(x, f(x), f(\\gamma x), \\dots,f(\\gamma^m x)\\right)","\\equiv 0$ where $\\gamma$ is a high-order field element.","These algorithms can be viewed as generalizations of classical algorithms of Sieveking (Computing 1972) and Kung (Numer.","Math. 1974) for computing the modular inverse of a power series, and might be of independent interest."],"url":"http://arxiv.org/abs/2311.17841v1"}
{"created":"2023-11-29 17:42:05","title":"A quasi-polynomial time algorithm for Multi-Dimensional Scaling via LP hierarchies","abstract":"Multi-dimensional Scaling (MDS) is a family of methods for embedding pair-wise dissimilarities between $n$ objects into low-dimensional space. MDS is widely used as a data visualization tool in the social and biological sciences, statistics, and machine learning. We study the Kamada-Kawai formulation of MDS: given a set of non-negative dissimilarities $\\{d_{i,j}\\}_{i , j \\in [n]}$ over $n$ points, the goal is to find an embedding $\\{x_1,\\dots,x_n\\} \\subset \\mathbb{R}^k$ that minimizes \\[ \\text{OPT} = \\min_{x} \\mathbb{E}_{i,j \\in [n]} \\left[ \\left(1-\\frac{\\|x_i - x_j\\|}{d_{i,j}}\\right)^2 \\right] \\]   Despite its popularity, our theoretical understanding of MDS is extremely limited. Recently, Demaine, Hesterberg, Koehler, Lynch, and Urschel (arXiv:2109.11505) gave the first approximation algorithm with provable guarantees for Kamada-Kawai, which achieves an embedding with cost $\\text{OPT} +\\epsilon$ in $n^2 \\cdot 2^{\\tilde{\\mathcal{O}}(k \\Delta^4 / \\epsilon^2)}$ time, where $\\Delta$ is the aspect ratio of the input dissimilarities. In this work, we give the first approximation algorithm for MDS with quasi-polynomial dependency on $\\Delta$: for target dimension $k$, we achieve a solution with cost $\\mathcal{O}(\\text{OPT}^{ \\hspace{0.04in}1/k } \\cdot \\log(\\Delta/\\epsilon) )+ \\epsilon$ in time $n^{ \\mathcal{O}(1)} \\cdot 2^{\\tilde{\\mathcal{O}}( k^2 (\\log(\\Delta)/\\epsilon)^{k/2 + 1} ) }$.   Our approach is based on a novel analysis of a conditioning-based rounding scheme for the Sherali-Adams LP Hierarchy. Crucially, our analysis exploits the geometry of low-dimensional Euclidean space, allowing us to avoid an exponential dependence on the aspect ratio $\\Delta$. We believe our geometry-aware treatment of the Sherali-Adams Hierarchy is an important step towards developing general-purpose techniques for efficient metric optimization algorithms.","sentences":["Multi-dimensional Scaling (MDS) is a family of methods for embedding pair-wise dissimilarities between $n$ objects into low-dimensional space.","MDS is widely used as a data visualization tool in the social and biological sciences, statistics, and machine learning.","We study the Kamada-Kawai formulation of MDS: given a set of non-negative dissimilarities $\\{d_{i,j}\\}_{i , j \\in","[n]}$ over $n$ points, the goal is to find an embedding $\\{x_1,\\dots,x_n\\} \\subset \\mathbb{R}^k$ that minimizes \\[ \\text{OPT} = \\min_{x} \\mathbb{E}_{i,j \\in [n]} \\left[ \\left(1-\\frac{\\|x_i - x_j\\|}{d_{i,j}}\\right)^2 \\right] \\]   Despite its popularity, our theoretical understanding of MDS is extremely limited.","Recently, Demaine, Hesterberg, Koehler, Lynch, and Urschel (arXiv:2109.11505) gave the first approximation algorithm with provable guarantees for Kamada-Kawai, which achieves an embedding with cost $\\text{OPT} +\\epsilon$ in $n^2 \\cdot 2^{\\tilde{\\mathcal{O}}(k \\Delta^4 / \\epsilon^2)}$ time, where $\\Delta$ is the aspect ratio of the input dissimilarities.","In this work, we give the first approximation algorithm for MDS with quasi-polynomial dependency on $\\Delta$: for target dimension $k$, we achieve a solution with cost $\\mathcal{O}(\\text{OPT}^{ \\hspace{0.04in}1/k } \\cdot \\log(\\Delta/\\epsilon) )","+ \\epsilon$ in time $n^{ \\mathcal{O}(1)} \\cdot 2^{\\tilde{\\mathcal{O}}( k^2 (\\log(\\Delta)/\\epsilon)^{k/2 + 1} ) }$.   ","Our approach is based on a novel analysis of a conditioning-based rounding scheme for the Sherali-Adams LP Hierarchy.","Crucially, our analysis exploits the geometry of low-dimensional Euclidean space, allowing us to avoid an exponential dependence on the aspect ratio $\\Delta$. We believe our geometry-aware treatment of the Sherali-Adams Hierarchy is an important step towards developing general-purpose techniques for efficient metric optimization algorithms."],"url":"http://arxiv.org/abs/2311.17840v1"}
{"created":"2023-11-29 17:39:16","title":"Anytime Replanning of Robot Coverage Paths for Partially Unknown Environments","abstract":"In this paper, we propose a method to replan coverage paths for a robot operating in an environment with initially unknown static obstacles. Existing coverage approaches reduce coverage time by covering along the minimum number of coverage lines (straight-line paths). However, recomputing such paths online can be computationally expensive resulting in robot stoppages that increase coverage time. A naive alternative is greedy detour replanning, i.e., replanning with minimum deviation from the initial path, which is efficient to compute but may result in unnecessary detours. In this work, we propose an anytime coverage replanning approach named OARP-Replan that performs near-optimal replans to an interrupted coverage path within a given time budget. We do this by solving linear relaxations of mixed-integer linear programs (MILPs) to identify sections of the interrupted path that can be optimally replanned within the time budget. We validate our approach in simulation using maps of real-world environments and compare our approach against a greedy detour replanner and other state-of-the-art approaches.","sentences":["In this paper, we propose a method to replan coverage paths for a robot operating in an environment with initially unknown static obstacles.","Existing coverage approaches reduce coverage time by covering along the minimum number of coverage lines (straight-line paths).","However, recomputing such paths online can be computationally expensive resulting in robot stoppages that increase coverage time.","A naive alternative is greedy detour replanning, i.e., replanning with minimum deviation from the initial path, which is efficient to compute but may result in unnecessary detours.","In this work, we propose an anytime coverage replanning approach named OARP-Replan that performs near-optimal replans to an interrupted coverage path within a given time budget.","We do this by solving linear relaxations of mixed-integer linear programs (MILPs) to identify sections of the interrupted path that can be optimally replanned within the time budget.","We validate our approach in simulation using maps of real-world environments and compare our approach against a greedy detour replanner and other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2311.17837v1"}
{"created":"2023-11-29 17:36:49","title":"SPiC-E : Structural Priors in 3D Diffusion Models using Cross Entity Attention","abstract":"We are witnessing rapid progress in automatically generating and manipulating 3D assets due to the availability of pretrained text-image diffusion models. However, time-consuming optimization procedures are required for synthesizing each sample, hindering their potential for democratizing 3D content creation. Conversely, 3D diffusion models now train on million-scale 3D datasets, yielding high-quality text-conditional 3D samples within seconds. In this work, we present SPiC-E - a neural network that adds structural guidance to 3D diffusion models, extending their usage beyond text-conditional generation. At its core, our framework introduces a cross-entity attention mechanism that allows for multiple entities (in particular, paired input and guidance 3D shapes) to interact via their internal representations within the denoising network. We utilize this mechanism for learning task-specific structural priors in 3D diffusion models from auxiliary guidance shapes. We show that our approach supports a variety of applications, including 3D stylization, semantic shape editing and text-conditional abstraction-to-3D, which transforms primitive-based abstractions into highly-expressive shapes. Extensive experiments demonstrate that SPiC-E achieves SOTA performance over these tasks while often being considerably faster than alternative methods. Importantly, this is accomplished without tailoring our approach for any specific task.","sentences":["We are witnessing rapid progress in automatically generating and manipulating 3D assets due to the availability of pretrained text-image diffusion models.","However, time-consuming optimization procedures are required for synthesizing each sample, hindering their potential for democratizing 3D content creation.","Conversely, 3D diffusion models now train on million-scale 3D datasets, yielding high-quality text-conditional 3D samples within seconds.","In this work, we present SPiC-E - a neural network that adds structural guidance to 3D diffusion models, extending their usage beyond text-conditional generation.","At its core, our framework introduces a cross-entity attention mechanism that allows for multiple entities (in particular, paired input and guidance 3D shapes) to interact via their internal representations within the denoising network.","We utilize this mechanism for learning task-specific structural priors in 3D diffusion models from auxiliary guidance shapes.","We show that our approach supports a variety of applications, including 3D stylization, semantic shape editing and text-conditional abstraction-to-3D, which transforms primitive-based abstractions into highly-expressive shapes.","Extensive experiments demonstrate that SPiC-E achieves SOTA performance over these tasks while often being considerably faster than alternative methods.","Importantly, this is accomplished without tailoring our approach for any specific task."],"url":"http://arxiv.org/abs/2311.17834v1"}
{"created":"2023-11-29 17:35:29","title":"Analyzing and Explaining Image Classifiers via Diffusion Guidance","abstract":"While deep learning has led to huge progress in complex image classification tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call into question how reliably these classifiers work in the wild. Furthermore, for safety-critical tasks the black-box nature of their decisions is problematic, and explanations or at least methods which make decisions plausible are needed urgently. In this paper, we address these problems by generating images that optimize a classifier-derived objective using a framework for guided image generation. We analyze the behavior and decisions of image classifiers by visual counterfactual explanations (VCEs), detection of systematic mistakes by analyzing images where classifiers maximally disagree, and visualization of neurons to verify potential spurious features. In this way, we validate existing observations, e.g. the shape bias of adversarially robust models, as well as novel failure modes, e.g. systematic errors of zero-shot CLIP classifiers, or identify harmful spurious features. Moreover, our VCEs outperform previous work while being more versatile.","sentences":["While deep learning has led to huge progress in complex image classification tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call into question how reliably these classifiers work in the wild.","Furthermore, for safety-critical tasks the black-box nature of their decisions is problematic, and explanations or at least methods which make decisions plausible are needed urgently.","In this paper, we address these problems by generating images that optimize a classifier-derived objective using a framework for guided image generation.","We analyze the behavior and decisions of image classifiers by visual counterfactual explanations (VCEs), detection of systematic mistakes by analyzing images where classifiers maximally disagree, and visualization of neurons to verify potential spurious features.","In this way, we validate existing observations, e.g. the shape bias of adversarially robust models, as well as novel failure modes, e.g. systematic errors of zero-shot CLIP classifiers, or identify harmful spurious features.","Moreover, our VCEs outperform previous work while being more versatile."],"url":"http://arxiv.org/abs/2311.17833v1"}
{"created":"2023-11-29 17:22:28","title":"Anomalous Behavior Detection in Trajectory Data of Older Drivers","abstract":"Given a road network and a set of trajectory data, the anomalous behavior detection (ABD) problem is to identify drivers that show significant directional deviations, hardbrakings, and accelerations in their trips. The ABD problem is important in many societal applications, including Mild Cognitive Impairment (MCI) detection and safe route recommendations for older drivers. The ABD problem is computationally challenging due to the large size of temporally-detailed trajectories dataset. In this paper, we propose an Edge-Attributed Matrix that can represent the key properties of temporally-detailed trajectory datasets and identify abnormal driving behaviors. Experiments using real-world datasets demonstrated that our approach identifies abnormal driving behaviors.","sentences":["Given a road network and a set of trajectory data, the anomalous behavior detection (ABD) problem is to identify drivers that show significant directional deviations, hardbrakings, and accelerations in their trips.","The ABD problem is important in many societal applications, including Mild Cognitive Impairment (MCI) detection and safe route recommendations for older drivers.","The ABD problem is computationally challenging due to the large size of temporally-detailed trajectories dataset.","In this paper, we propose an Edge-Attributed Matrix that can represent the key properties of temporally-detailed trajectory datasets and identify abnormal driving behaviors.","Experiments using real-world datasets demonstrated that our approach identifies abnormal driving behaviors."],"url":"http://arxiv.org/abs/2311.17822v1"}
{"created":"2023-11-29 17:10:16","title":"A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures","abstract":"In recent years, the field of Deep Learning has seen many disruptive and impactful advancements. Given the increasing complexity of deep neural networks, the need for efficient hardware accelerators has become more and more pressing to design heterogeneous HPC platforms. The design of Deep Learning accelerators requires a multidisciplinary approach, combining expertise from several areas, spanning from computer architecture to approximate computing, computational models, and machine learning algorithms. Several methodologies and tools have been proposed to design accelerators for Deep Learning, including hardware-software co-design approaches, high-level synthesis methods, specific customized compilers, and methodologies for design space exploration, modeling, and simulation. These methodologies aim to maximize the exploitable parallelism and minimize data movement to achieve high performance and energy efficiency. This survey provides a holistic review of the most influential design methodologies and EDA tools proposed in recent years to implement Deep Learning accelerators, offering the reader a wide perspective in this rapidly evolving field. In particular, this work complements the previous survey proposed by the same authors in [203], which focuses on Deep Learning hardware accelerators for heterogeneous HPC platforms.","sentences":["In recent years, the field of Deep Learning has seen many disruptive and impactful advancements.","Given the increasing complexity of deep neural networks, the need for efficient hardware accelerators has become more and more pressing to design heterogeneous HPC platforms.","The design of Deep Learning accelerators requires a multidisciplinary approach, combining expertise from several areas, spanning from computer architecture to approximate computing, computational models, and machine learning algorithms.","Several methodologies and tools have been proposed to design accelerators for Deep Learning, including hardware-software co-design approaches, high-level synthesis methods, specific customized compilers, and methodologies for design space exploration, modeling, and simulation.","These methodologies aim to maximize the exploitable parallelism and minimize data movement to achieve high performance and energy efficiency.","This survey provides a holistic review of the most influential design methodologies and EDA tools proposed in recent years to implement Deep Learning accelerators, offering the reader a wide perspective in this rapidly evolving field.","In particular, this work complements the previous survey proposed by the same authors in [203], which focuses on Deep Learning hardware accelerators for heterogeneous HPC platforms."],"url":"http://arxiv.org/abs/2311.17815v1"}
{"created":"2023-11-29 17:04:15","title":"Higher-Order DisCoCat (Peirce-Lambek-Montague semantics)","abstract":"We propose a new definition of higher-order DisCoCat (categorical compositional distributional) models where the meaning of a word is not a diagram, but a diagram-valued higher-order function. Our models can be seen as a variant of Montague semantics based on a lambda calculus where the primitives act on string diagrams rather than logical formulae. As a special case, we show how to translate from the Lambek calculus into Peirce's system beta for first-order logic. This allows us to give a purely diagrammatic treatment of higher-order and non-linear processes in natural language semantics: adverbs, prepositions, negation and quantifiers. The theoretical definition presented in this article comes with a proof-of-concept implementation in DisCoPy, the Python library for string diagrams.","sentences":["We propose a new definition of higher-order DisCoCat (categorical compositional distributional) models where the meaning of a word is not a diagram, but a diagram-valued higher-order function.","Our models can be seen as a variant of Montague semantics based on a lambda calculus where the primitives act on string diagrams rather than logical formulae.","As a special case, we show how to translate from the Lambek calculus into Peirce's system beta for first-order logic.","This allows us to give a purely diagrammatic treatment of higher-order and non-linear processes in natural language semantics: adverbs, prepositions, negation and quantifiers.","The theoretical definition presented in this article comes with a proof-of-concept implementation in DisCoPy, the Python library for string diagrams."],"url":"http://arxiv.org/abs/2311.17813v1"}
{"created":"2023-11-29 17:03:37","title":"DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation","abstract":"Following language instructions to navigate in unseen environments is a challenging task for autonomous embodied agents. With strong representation capabilities, pretrained vision-and-language models are widely used in VLN. However, most of them are trained on web-crawled general-purpose datasets, which incurs a considerable domain gap when used for VLN tasks. To address the problem, we propose a novel and model-agnostic domain-aware prompt learning (DAP) framework. For equipping the pretrained models with specific object-level and scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost prompt tuning paradigm to learn soft visual prompts for extracting in-domain image semantics. Specifically, we first generate a set of in-domain image-text pairs with the help of the CLIP model. Then we introduce soft visual prompts in the input space of the visual encoder in a pretrained model. DAP injects in-domain visual knowledge into the visual encoder of the pretrained model in an efficient way. Experimental results on both R2R and REVERIE show the superiority of DAP compared to existing state-of-the-art methods.","sentences":["Following language instructions to navigate in unseen environments is a challenging task for autonomous embodied agents.","With strong representation capabilities, pretrained vision-and-language models are widely used in VLN.","However, most of them are trained on web-crawled general-purpose datasets, which incurs a considerable domain gap when used for VLN tasks.","To address the problem, we propose a novel and model-agnostic domain-aware prompt learning (DAP) framework.","For equipping the pretrained models with specific object-level and scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost prompt tuning paradigm to learn soft visual prompts for extracting in-domain image semantics.","Specifically, we first generate a set of in-domain image-text pairs with the help of the CLIP model.","Then we introduce soft visual prompts in the input space of the visual encoder in a pretrained model.","DAP injects in-domain visual knowledge into the visual encoder of the pretrained model in an efficient way.","Experimental results on both R2R and REVERIE show the superiority of DAP compared to existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2311.17812v1"}
{"created":"2023-11-29 16:59:45","title":"Coloring the Past: Neural Historical Buildings Reconstruction from Archival Photography","abstract":"Historical buildings are a treasure and milestone of human cultural heritage. Reconstructing the 3D models of these building hold significant value. The rapid development of neural rendering methods makes it possible to recover the 3D shape only based on archival photographs. However, this task presents considerable challenges due to the limitations of such datasets. Historical photographs are often limited in number and the scenes in these photos might have altered over time. The radiometric quality of these images is also often sub-optimal. To address these challenges, we introduce an approach to reconstruct the geometry of historical buildings, employing volumetric rendering techniques. We leverage dense point clouds as a geometric prior and introduce a color appearance embedding loss to recover the color of the building given limited available color images. We aim for our work to spark increased interest and focus on preserving historical buildings. Thus, we also introduce a new historical dataset of the Hungarian National Theater, providing a new benchmark for the reconstruction method.","sentences":["Historical buildings are a treasure and milestone of human cultural heritage.","Reconstructing the 3D models of these building hold significant value.","The rapid development of neural rendering methods makes it possible to recover the 3D shape only based on archival photographs.","However, this task presents considerable challenges due to the limitations of such datasets.","Historical photographs are often limited in number and the scenes in these photos might have altered over time.","The radiometric quality of these images is also often sub-optimal.","To address these challenges, we introduce an approach to reconstruct the geometry of historical buildings, employing volumetric rendering techniques.","We leverage dense point clouds as a geometric prior and introduce a color appearance embedding loss to recover the color of the building given limited available color images.","We aim for our work to spark increased interest and focus on preserving historical buildings.","Thus, we also introduce a new historical dataset of the Hungarian National Theater, providing a new benchmark for the reconstruction method."],"url":"http://arxiv.org/abs/2311.17810v1"}
{"created":"2023-11-29 16:54:25","title":"Aggregation Model Hyperparameters Matter in Digital Pathology","abstract":"Digital pathology has significantly advanced disease detection and pathologist efficiency through the analysis of gigapixel whole-slide images (WSI). In this process, WSIs are first divided into patches, for which a feature extractor model is applied to obtain feature vectors, which are subsequently processed by an aggregation model to predict the respective WSI label. With the rapid evolution of representation learning, numerous new feature extractor models, often termed foundational models, have emerged. Traditional evaluation methods, however, rely on fixed aggregation model hyperparameters, a framework we identify as potentially biasing the results. Our study uncovers a co-dependence between feature extractor models and aggregation model hyperparameters, indicating that performance comparability can be skewed based on the chosen hyperparameters. By accounting for this co-dependency, we find that the performance of many current feature extractor models is notably similar. We support this insight by evaluating seven feature extractor models across three different datasets with 162 different aggregation model configurations. This comprehensive approach provides a more nuanced understanding of the relationship between feature extractors and aggregation models, leading to a fairer and more accurate assessment of feature extractor models in digital pathology.","sentences":["Digital pathology has significantly advanced disease detection and pathologist efficiency through the analysis of gigapixel whole-slide images (WSI).","In this process, WSIs are first divided into patches, for which a feature extractor model is applied to obtain feature vectors, which are subsequently processed by an aggregation model to predict the respective WSI label.","With the rapid evolution of representation learning, numerous new feature extractor models, often termed foundational models, have emerged.","Traditional evaluation methods, however, rely on fixed aggregation model hyperparameters, a framework we identify as potentially biasing the results.","Our study uncovers a co-dependence between feature extractor models and aggregation model hyperparameters, indicating that performance comparability can be skewed based on the chosen hyperparameters.","By accounting for this co-dependency, we find that the performance of many current feature extractor models is notably similar.","We support this insight by evaluating seven feature extractor models across three different datasets with 162 different aggregation model configurations.","This comprehensive approach provides a more nuanced understanding of the relationship between feature extractors and aggregation models, leading to a fairer and more accurate assessment of feature extractor models in digital pathology."],"url":"http://arxiv.org/abs/2311.17804v1"}
{"created":"2023-11-29 16:51:21","title":"Towards Efficient Hyperdimensional Computing Using Photonics","abstract":"Over the past few years, silicon photonics-based computing has emerged as a promising alternative to CMOS-based computing for Deep Neural Networks (DNN). Unfortunately, the non-linear operations and the high-precision requirements of DNNs make it extremely challenging to design efficient silicon photonics-based systems for DNN inference and training. Hyperdimensional Computing (HDC) is an emerging, brain-inspired machine learning technique that enjoys several advantages over existing DNNs, including being lightweight, requiring low-precision operands, and being robust to noise introduced by the nonidealities in the hardware. For HDC, computing in-memory (CiM) approaches have been widely used, as CiM reduces the data transfer cost if the operands can fit into the memory. However, inefficient multi-bit operations, high write latency, and low endurance make CiM ill-suited for HDC. On the other hand, the existing electro-photonic DNN accelerators are inefficient for HDC because they are specifically optimized for matrix multiplication in DNNs and consume a lot of power with high-precision data converters.   In this paper, we argue that photonic computing and HDC complement each other better than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC, the first-ever electro-photonic accelerator for HDC training and inference, supporting the basic, record-based, and graph encoding schemes. Evaluating with popular datasets, we show that our accelerator can achieve two to five orders of magnitude lower EDP than the state-of-the-art electro-photonic DNN accelerators for implementing HDC training and inference. PhotoHDC also achieves four orders of magnitude lower energy-delay product than CiM-based accelerators for both HDC training and inference.","sentences":["Over the past few years, silicon photonics-based computing has emerged as a promising alternative to CMOS-based computing for Deep Neural Networks (DNN).","Unfortunately, the non-linear operations and the high-precision requirements of DNNs make it extremely challenging to design efficient silicon photonics-based systems for DNN inference and training.","Hyperdimensional Computing (HDC) is an emerging, brain-inspired machine learning technique that enjoys several advantages over existing DNNs, including being lightweight, requiring low-precision operands, and being robust to noise introduced by the nonidealities in the hardware.","For HDC, computing in-memory (CiM) approaches have been widely used, as CiM reduces the data transfer cost if the operands can fit into the memory.","However, inefficient multi-bit operations, high write latency, and low endurance make CiM ill-suited for HDC.","On the other hand, the existing electro-photonic DNN accelerators are inefficient for HDC because they are specifically optimized for matrix multiplication in DNNs and consume a lot of power with high-precision data converters.   ","In this paper, we argue that photonic computing and HDC complement each other better than photonic computing and DNNs, or CiM and HDC.","We propose PhotoHDC, the first-ever electro-photonic accelerator for HDC training and inference, supporting the basic, record-based, and graph encoding schemes.","Evaluating with popular datasets, we show that our accelerator can achieve two to five orders of magnitude lower EDP than the state-of-the-art electro-photonic DNN accelerators for implementing HDC training and inference.","PhotoHDC also achieves four orders of magnitude lower energy-delay product than CiM-based accelerators for both HDC training and inference."],"url":"http://arxiv.org/abs/2311.17801v1"}
{"created":"2023-11-29 16:46:24","title":"Learning to Simulate: Generative Metamodeling via Quantile Regression","abstract":"Stochastic simulation models, while effective in capturing the dynamics of complex systems, are often too slow to run for real-time decision-making. Metamodeling techniques are widely used to learn the relationship between a summary statistic of the outputs (e.g., the mean or quantile) and the inputs of the simulator, so that it can be used in real time. However, this methodology requires the knowledge of an appropriate summary statistic in advance, making it inflexible for many practical situations. In this paper, we propose a new metamodeling concept, called generative metamodeling, which aims to construct a \"fast simulator of the simulator\". This technique can generate random outputs substantially faster than the original simulation model, while retaining an approximately equal conditional distribution given the same inputs. Once constructed, a generative metamodel can instantaneously generate a large amount of random outputs as soon as the inputs are specified, thereby facilitating the immediate computation of any summary statistic for real-time decision-making. Furthermore, we propose a new algorithm -- quantile-regression-based generative metamodeling (QRGMM) -- and study its convergence and rate of convergence. Extensive numerical experiments are conducted to investigate the empirical performance of QRGMM, compare it with other state-of-the-art generative algorithms, and demonstrate its usefulness in practical real-time decision-making.","sentences":["Stochastic simulation models, while effective in capturing the dynamics of complex systems, are often too slow to run for real-time decision-making.","Metamodeling techniques are widely used to learn the relationship between a summary statistic of the outputs (e.g., the mean or quantile) and the inputs of the simulator, so that it can be used in real time.","However, this methodology requires the knowledge of an appropriate summary statistic in advance, making it inflexible for many practical situations.","In this paper, we propose a new metamodeling concept, called generative metamodeling, which aims to construct a \"fast simulator of the simulator\".","This technique can generate random outputs substantially faster than the original simulation model, while retaining an approximately equal conditional distribution given the same inputs.","Once constructed, a generative metamodel can instantaneously generate a large amount of random outputs as soon as the inputs are specified, thereby facilitating the immediate computation of any summary statistic for real-time decision-making.","Furthermore, we propose a new algorithm -- quantile-regression-based generative metamodeling (QRGMM) -- and study its convergence and rate of convergence.","Extensive numerical experiments are conducted to investigate the empirical performance of QRGMM, compare it with other state-of-the-art generative algorithms, and demonstrate its usefulness in practical real-time decision-making."],"url":"http://arxiv.org/abs/2311.17797v1"}
{"created":"2023-11-29 16:45:43","title":"Marginal Laplacian Score","abstract":"High-dimensional imbalanced data poses a machine learning challenge. In the absence of sufficient or high-quality labels, unsupervised feature selection methods are crucial for the success of subsequent algorithms. Therefore, there is a growing need for unsupervised feature selection algorithms focused on imbalanced data. Thus, we propose a Marginal Laplacian Score (MLS) a modification of the well-known Laplacian Score (LS) to be better suited for imbalance data. We introduce an assumption that the minority class or anomalous appear more frequently in the margin of the features. Consequently, MLS aims to preserve the local structure of the data set's margin. As MLS is better suited for handling imbalanced data, we propose its integration into modern feature selection methods that utilize the Laplacian score. We integrate the MLS algorithm into the Differentiable Unsupervised Feature Selection (DUFS), resulting in DUFS-MLS. The proposed methods demonstrate robust and improved performance on synthetic and public data sets.","sentences":["High-dimensional imbalanced data poses a machine learning challenge.","In the absence of sufficient or high-quality labels, unsupervised feature selection methods are crucial for the success of subsequent algorithms.","Therefore, there is a growing need for unsupervised feature selection algorithms focused on imbalanced data.","Thus, we propose a Marginal Laplacian Score (MLS) a modification of the well-known Laplacian Score (LS) to be better suited for imbalance data.","We introduce an assumption that the minority class or anomalous appear more frequently in the margin of the features.","Consequently, MLS aims to preserve the local structure of the data set's margin.","As MLS is better suited for handling imbalanced data, we propose its integration into modern feature selection methods that utilize the Laplacian score.","We integrate the MLS algorithm into the Differentiable Unsupervised Feature Selection (DUFS), resulting in DUFS-MLS.","The proposed methods demonstrate robust and improved performance on synthetic and public data sets."],"url":"http://arxiv.org/abs/2311.17795v1"}
{"created":"2023-11-29 16:35:13","title":"FAT-HuBERT: Front-end Adaptive Training of Hidden-unit BERT for Distortion-Invariant Robust Speech Recognition","abstract":"Advancements in monaural speech enhancement (SE) techniques have greatly improved the perceptual quality of speech. However, integrating these techniques into automatic speech recognition (ASR) systems has not yielded the expected performance gains, primarily due to the introduction of distortions during the SE process. In this paper, we propose a novel approach called FAT-HuBERT, which leverages distortion-invariant self-supervised learning (SSL) to enhance the robustness of ASR. To address the distortions introduced by the SE frontends, we introduce layer-wise fusion modules that incorporate features extracted from both observed noisy signals and enhanced signals. During training, the SE frontend is randomly selected from a pool of models. We evaluate the performance of FAT-HuBERT on simulated noisy speech generated from LibriSpeech as well as real-world noisy speech from the CHiME-4 1-channel dataset. The experimental results demonstrate a significant relative reduction in word error rate (WER).","sentences":["Advancements in monaural speech enhancement (SE) techniques have greatly improved the perceptual quality of speech.","However, integrating these techniques into automatic speech recognition (ASR) systems has not yielded the expected performance gains, primarily due to the introduction of distortions during the SE process.","In this paper, we propose a novel approach called FAT-HuBERT, which leverages distortion-invariant self-supervised learning (SSL) to enhance the robustness of ASR.","To address the distortions introduced by the SE frontends, we introduce layer-wise fusion modules that incorporate features extracted from both observed noisy signals and enhanced signals.","During training, the SE frontend is randomly selected from a pool of models.","We evaluate the performance of FAT-HuBERT on simulated noisy speech generated from LibriSpeech as well as real-world noisy speech from the CHiME-4 1-channel dataset.","The experimental results demonstrate a significant relative reduction in word error rate (WER)."],"url":"http://arxiv.org/abs/2311.17790v1"}
{"created":"2023-11-29 16:34:39","title":"The Symmetric alpha-Stable Privacy Mechanism","abstract":"With the rapid growth of digital platforms, there is increasing apprehension about how personal data is being collected, stored, and used by various entities. These concerns range from data breaches and cyber-attacks to potential misuse of personal information for targeted advertising and surveillance. As a result, differential privacy (DP) has emerged as a prominent tool for quantifying a system's level of protection. The Gaussian mechanism is commonly used because the Gaussian density is closed under convolution, a common method utilized when aggregating datasets. However, the Gaussian mechanism only satisfies approximate differential privacy. In this work, we present novel analysis of the Symmetric alpha-Stable (SaS) mechanism. We prove that the mechanism is purely differentially private while remaining closed under convolution. From our analysis, we believe the SaS Mechanism is an appealing choice for privacy focused applications.","sentences":["With the rapid growth of digital platforms, there is increasing apprehension about how personal data is being collected, stored, and used by various entities.","These concerns range from data breaches and cyber-attacks to potential misuse of personal information for targeted advertising and surveillance.","As a result, differential privacy (DP) has emerged as a prominent tool for quantifying a system's level of protection.","The Gaussian mechanism is commonly used because the Gaussian density is closed under convolution, a common method utilized when aggregating datasets.","However, the Gaussian mechanism only satisfies approximate differential privacy.","In this work, we present novel analysis of the Symmetric alpha-Stable (SaS) mechanism.","We prove that the mechanism is purely differentially private while remaining closed under convolution.","From our analysis, we believe the SaS Mechanism is an appealing choice for privacy focused applications."],"url":"http://arxiv.org/abs/2311.17789v1"}
{"created":"2023-11-29 16:34:15","title":"Collaborative software design and modeling in virtual reality","abstract":"Context: Software engineering is becoming more and more distributed. Developers and other stakeholders are often located in different locations, departments, and countries and operating within different time zones. Most online software design and modeling tools are not adequate for distributed collaboration since they do not support awareness and lack features for effective communication.   Objective: The aim of our research is to support distributed software design activities in Virtual Reality (VR).   Method: Using design science research methodology, we design and evaluate a tool for collaborative design in VR. We evaluate the collaboration efficiency and recall of design information when using the VR software design environment compared to a non-VR software design environment. Moreover, we collect the perceptions and preferences of users to explore the opportunities and challenges that were incurred by using the VR software design environment.   Results: We find that there is no significant difference in the efficiency and recall of design information when using the VR compared to the non-VR environment. Furthermore, we find that developers are more satisfied with collaboration in VR.   Conclusion: The results of our research and similar studies show that working in VR is not yet faster or more efficient than working on standard desktops. It is very important to improve the interface in VR (gestures with haptics, keyboard and voice input), as confirmed by the difference in results between the first and second evaluation.","sentences":["Context: Software engineering is becoming more and more distributed.","Developers and other stakeholders are often located in different locations, departments, and countries and operating within different time zones.","Most online software design and modeling tools are not adequate for distributed collaboration since they do not support awareness and lack features for effective communication.   ","Objective: The aim of our research is to support distributed software design activities in Virtual Reality (VR).   ","Method: Using design science research methodology, we design and evaluate a tool for collaborative design in VR.","We evaluate the collaboration efficiency and recall of design information when using the VR software design environment compared to a non-VR software design environment.","Moreover, we collect the perceptions and preferences of users to explore the opportunities and challenges that were incurred by using the VR software design environment.   ","Results:","We find that there is no significant difference in the efficiency and recall of design information when using the VR compared to the non-VR environment.","Furthermore, we find that developers are more satisfied with collaboration in VR.   ","Conclusion: The results of our research and similar studies show that working in VR is not yet faster or more efficient than working on standard desktops.","It is very important to improve the interface in VR (gestures with haptics, keyboard and voice input), as confirmed by the difference in results between the first and second evaluation."],"url":"http://arxiv.org/abs/2311.17787v1"}
{"created":"2023-11-29 16:33:19","title":"DSS: Synthesizing long Digital Ink using Data augmentation, Style encoding and Split generation","abstract":"As text generative models can give increasingly long answers, we tackle the problem of synthesizing long text in digital ink. We show that the commonly used models for this task fail to generalize to long-form data and how this problem can be solved by augmenting the training data, changing the model architecture and the inference procedure. These methods use contrastive learning technique and are tailored specifically for the handwriting domain. They can be applied to any encoder-decoder model that works with digital ink. We demonstrate that our method reduces the character error rate on long-form English data by half compared to baseline RNN and by 16% compared to the previous approach that aims at addressing the same problem. We show that all three parts of the method improve recognizability of generated inks. In addition, we evaluate synthesized data in a human study and find that people perceive most of generated data as real.","sentences":["As text generative models can give increasingly long answers, we tackle the problem of synthesizing long text in digital ink.","We show that the commonly used models for this task fail to generalize to long-form data and how this problem can be solved by augmenting the training data, changing the model architecture and the inference procedure.","These methods use contrastive learning technique and are tailored specifically for the handwriting domain.","They can be applied to any encoder-decoder model that works with digital ink.","We demonstrate that our method reduces the character error rate on long-form English data by half compared to baseline RNN and by 16% compared to the previous approach that aims at addressing the same problem.","We show that all three parts of the method improve recognizability of generated inks.","In addition, we evaluate synthesized data in a human study and find that people perceive most of generated data as real."],"url":"http://arxiv.org/abs/2311.17786v1"}
{"created":"2023-11-29 16:26:24","title":"Propagate & Distill: Towards Effective Graph Learners Using Propagation-Embracing MLPs","abstract":"Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve semisupervised node classification on graphs, by training a student MLP by knowledge distillation from a teacher graph neural network (GNN). While previous studies have focused mostly on training the student MLP by matching the output probability distributions between the teacher and student models during distillation, it has not been systematically studied how to inject the structural information in an explicit and interpretable manner. Inspired by GNNs that separate feature transformation $T$ and propagation $\\Pi$, we re-frame the distillation process as making the student MLP learn both $T$ and $\\Pi$. Although this can be achieved by applying the inverse propagation $\\Pi^{-1}$ before distillation from the teacher, it still comes with a high computational cost from large matrix multiplications during training. To solve this problem, we propose Propagate & Distill (P&D), which propagates the output of the teacher before distillation, which can be interpreted as an approximate process of the inverse propagation. We demonstrate that P&D can readily improve the performance of the student MLP.","sentences":["Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve semisupervised node classification on graphs, by training a student MLP by knowledge distillation from a teacher graph neural network (GNN).","While previous studies have focused mostly on training the student MLP by matching the output probability distributions between the teacher and student models during distillation, it has not been systematically studied how to inject the structural information in an explicit and interpretable manner.","Inspired by GNNs that separate feature transformation $T$ and propagation $\\Pi$, we re-frame the distillation process as making the student MLP learn both $T$ and $\\Pi$. Although this can be achieved by applying the inverse propagation $\\Pi^{-1}$ before distillation from the teacher, it still comes with a high computational cost from large matrix multiplications during training.","To solve this problem, we propose Propagate & Distill (P&D), which propagates the output of the teacher before distillation, which can be interpreted as an approximate process of the inverse propagation.","We demonstrate that P&D can readily improve the performance of the student MLP."],"url":"http://arxiv.org/abs/2311.17781v1"}
{"created":"2023-11-29 16:26:00","title":"$Q_{bias}$ -- A Dataset on Media Bias in Search Queries and Query Suggestions","abstract":"This publication describes the motivation and generation of $Q_{bias}$, a large dataset of Google and Bing search queries, a scraping tool and dataset for biased news articles, as well as language models for the investigation of bias in online search. Web search engines are a major factor and trusted source in information search, especially in the political domain. However, biased information can influence opinion formation and lead to biased opinions. To interact with search engines, users formulate search queries and interact with search query suggestions provided by the search engines. A lack of datasets on search queries inhibits research on the subject. We use $Q_{bias}$ to evaluate different approaches to fine-tuning transformer-based language models with the goal of producing models capable of biasing text with left and right political stance. Additionally to this work we provided datasets and language models for biasing texts that allow further research on bias in online information search.","sentences":["This publication describes the motivation and generation of $Q_{bias}$, a large dataset of Google and Bing search queries, a scraping tool and dataset for biased news articles, as well as language models for the investigation of bias in online search.","Web search engines are a major factor and trusted source in information search, especially in the political domain.","However, biased information can influence opinion formation and lead to biased opinions.","To interact with search engines, users formulate search queries and interact with search query suggestions provided by the search engines.","A lack of datasets on search queries inhibits research on the subject.","We use $Q_{bias}$ to evaluate different approaches to fine-tuning transformer-based language models with the goal of producing models capable of biasing text with left and right political stance.","Additionally to this work we provided datasets and language models for biasing texts that allow further research on bias in online information search."],"url":"http://arxiv.org/abs/2311.17780v1"}
{"created":"2023-11-29 16:23:42","title":"Understanding the leakage process for multi-scale water infrastructure asset management: necessity for a dialogue between sociological and data sciences","abstract":"Reducing water losses is one of the most pressing issues for modern water utilities. To that end, improving the efficiency of the pipe leakage and repair process and aiding the selection of the pipes that are to be renewed or rehabilitated are essential. To help addressing these tasks, in this work, we develop a model predicting the probability of a pipe to be leaking. This work is set the context of a multidisciplinary project with Soci{\\'e}t{\\'e} Wallone des Eaux and it is aligned with their goal to improve their Infrastructure Asset Management in the short and the long terms. Developing and feeding this leakage probability model relies on an intense data processing phase, mobilizing data and water engineering sciences, since the raw data from SWDE is not ready to be used in the model. Complementarily, we thus employ techniques from sociology (e.g., interviews, analyses of the human/non-human actors and of the tools, sociotechnical translations) in order to complete the data, to improve our understanding of its production, and to increase its value and its availability for the prediction of the pipe leakage probability. This model will be implemented in SWDE's information system and used for strategies to reduce water losses.","sentences":["Reducing water losses is one of the most pressing issues for modern water utilities.","To that end, improving the efficiency of the pipe leakage and repair process and aiding the selection of the pipes that are to be renewed or rehabilitated are essential.","To help addressing these tasks, in this work, we develop a model predicting the probability of a pipe to be leaking.","This work is set the context of a multidisciplinary project with Soci{\\'e}t{\\'e} Wallone des Eaux and it is aligned with their goal to improve their Infrastructure Asset Management in the short and the long terms.","Developing and feeding this leakage probability model relies on an intense data processing phase, mobilizing data and water engineering sciences, since the raw data from SWDE is not ready to be used in the model.","Complementarily, we thus employ techniques from sociology (e.g., interviews, analyses of the human/non-human actors and of the tools, sociotechnical translations) in order to complete the data, to improve our understanding of its production, and to increase its value and its availability for the prediction of the pipe leakage probability.","This model will be implemented in SWDE's information system and used for strategies to reduce water losses."],"url":"http://arxiv.org/abs/2311.17777v1"}
{"created":"2023-11-29 16:23:06","title":"One-Shot Open Affordance Learning with Foundation Models","abstract":"We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances. While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances. To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess the potential for data-limited affordance learning. We then propose a vision-language framework with simple and effective designs that boost the alignment between visual features and affordance text embeddings. Experiments on two affordance segmentation benchmarks show that the proposed method outperforms state-of-the-art models with less than 1% of the full training data, and exhibits reasonable generalization capability on unseen objects and affordances.","sentences":["We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances.","While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances.","To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess the potential for data-limited affordance learning.","We then propose a vision-language framework with simple and effective designs that boost the alignment between visual features and affordance text embeddings.","Experiments on two affordance segmentation benchmarks show that the proposed method outperforms state-of-the-art models with less than 1% of the full training data, and exhibits reasonable generalization capability on unseen objects and affordances."],"url":"http://arxiv.org/abs/2311.17776v1"}
{"created":"2023-11-29 16:19:34","title":"Enumeration of Minimum Weight Codewords of Pre-Transformed Polar Codes by Tree Intersection","abstract":"Pre-transformed polar codes (PTPCs) form a class of codes that perform close to the finite-length capacity bounds. The minimum distance and the number of minimum weight codewords are two decisive properties for their performance. In this work, we propose an efficient algorithm to determine the number of minimum weight codewords of general PTPCs, which eliminates all redundant visits of nodes of the search tree, reducing the computational complexity from state-of-the-art algorithms typically by several orders of magnitude. The algorithm is demonstrated for randomly pre-transformed Reed-Muller (RM) codes and polarization-adjusted convolutional (PAC) codes. Optimal convolutional polynomials for PAC codes are designed, minimizing the number of minimum weight codewords.","sentences":["Pre-transformed polar codes (PTPCs) form a class of codes that perform close to the finite-length capacity bounds.","The minimum distance and the number of minimum weight codewords are two decisive properties for their performance.","In this work, we propose an efficient algorithm to determine the number of minimum weight codewords of general PTPCs, which eliminates all redundant visits of nodes of the search tree, reducing the computational complexity from state-of-the-art algorithms typically by several orders of magnitude.","The algorithm is demonstrated for randomly pre-transformed Reed-Muller (RM) codes and polarization-adjusted convolutional (PAC) codes.","Optimal convolutional polynomials for PAC codes are designed, minimizing the number of minimum weight codewords."],"url":"http://arxiv.org/abs/2311.17774v1"}
{"created":"2023-11-29 16:11:45","title":"Supervising the Centroid Baseline for Extractive Multi-Document Summarization","abstract":"The centroid method is a simple approach for extractive multi-document summarization and many improvements to its pipeline have been proposed. We further refine it by adding a beam search process to the sentence selection and also a centroid estimation attention model that leads to improved results. We demonstrate this in several multi-document summarization datasets, including in a multilingual scenario.","sentences":["The centroid method is a simple approach for extractive multi-document summarization and many improvements to its pipeline have been proposed.","We further refine it by adding a beam search process to the sentence selection and also a centroid estimation attention model that leads to improved results.","We demonstrate this in several multi-document summarization datasets, including in a multilingual scenario."],"url":"http://arxiv.org/abs/2311.17771v1"}
{"created":"2023-11-29 16:11:33","title":"PillarNeSt: Embracing Backbone Scaling and Pretraining for Pillar-based 3D Object Detection","abstract":"This paper shows the effectiveness of 2D backbone scaling and pretraining for pillar-based 3D object detectors. Pillar-based methods mainly employ randomly initialized 2D convolution neural network (ConvNet) for feature extraction and fail to enjoy the benefits from the backbone scaling and pretraining in the image domain. To show the scaling-up capacity in point clouds, we introduce the dense ConvNet pretrained on large-scale image datasets (e.g., ImageNet) as the 2D backbone of pillar-based detectors. The ConvNets are adaptively designed based on the model size according to the specific features of point clouds, such as sparsity and irregularity. Equipped with the pretrained ConvNets, our proposed pillar-based detector, termed PillarNeSt, outperforms the existing 3D object detectors by a large margin on the nuScenes and Argoversev2 datasets. Our code shall be released upon acceptance.","sentences":["This paper shows the effectiveness of 2D backbone scaling and pretraining for pillar-based 3D object detectors.","Pillar-based methods mainly employ randomly initialized 2D convolution neural network (ConvNet) for feature extraction and fail to enjoy the benefits from the backbone scaling and pretraining in the image domain.","To show the scaling-up capacity in point clouds, we introduce the dense ConvNet pretrained on large-scale image datasets (e.g., ImageNet) as the 2D backbone of pillar-based detectors.","The ConvNets are adaptively designed based on the model size according to the specific features of point clouds, such as sparsity and irregularity.","Equipped with the pretrained ConvNets, our proposed pillar-based detector, termed PillarNeSt, outperforms the existing 3D object detectors by a large margin on the nuScenes and Argoversev2 datasets.","Our code shall be released upon acceptance."],"url":"http://arxiv.org/abs/2311.17770v1"}
{"created":"2023-11-29 16:08:17","title":"A Simple and General Operational Framework to Deploy Optimal Routes with Source Routing","abstract":"Source Routing, currently facilitated by Segment Routing (SR), enables precise control of forwarding paths by specifying detours (or segments) to deviate IP packets along routes with advanced properties beyond typical shortest IGP paths. Computing the desired optimal segment lists, known as encoding, leads to interesting challenges as the number of detours is tightly constrained for hardware performance. Existing solutions either lack generality, correctness, optimality, or practical computing efficiency-in particular for sparse realistic networks. In this paper, we address all such challenges with GOFOR-SR. Our framework extends usual path computation algorithms to inherently look at optimal and feasible segment lists, streamlining the deployment of TE-compliant paths. By integrating encoding within the path computation itself and modifying the distance comparison method, GOFOR allows algorithms with various optimization objectives to efficiently compute optimal segment lists. Despite the loss of substructure optimality induced by SR, GOFOR proves particularly efficient, inducing only a linear overhead at worst. It also offers different strategies and path diversity options for intricate TE-aware loadbalancing. We formally prove the correctness and optimality of GOFOR, implement our framework for various practical usecases, and demonstrate its performance and benefits on both real and challenging topologies.","sentences":["Source Routing, currently facilitated by Segment Routing (SR), enables precise control of forwarding paths by specifying detours (or segments) to deviate IP packets along routes with advanced properties beyond typical shortest IGP paths.","Computing the desired optimal segment lists, known as encoding, leads to interesting challenges as the number of detours is tightly constrained for hardware performance.","Existing solutions either lack generality, correctness, optimality, or practical computing efficiency-in particular for sparse realistic networks.","In this paper, we address all such challenges with GOFOR-SR.","Our framework extends usual path computation algorithms to inherently look at optimal and feasible segment lists, streamlining the deployment of TE-compliant paths.","By integrating encoding within the path computation itself and modifying the distance comparison method, GOFOR allows algorithms with various optimization objectives to efficiently compute optimal segment lists.","Despite the loss of substructure optimality induced by SR, GOFOR proves particularly efficient, inducing only a linear overhead at worst.","It also offers different strategies and path diversity options for intricate TE-aware loadbalancing.","We formally prove the correctness and optimality of GOFOR, implement our framework for various practical usecases, and demonstrate its performance and benefits on both real and challenging topologies."],"url":"http://arxiv.org/abs/2311.17769v1"}
{"created":"2023-11-29 16:06:17","title":"Robustness Approaches for the Examination Timetabling Problem under Data Uncertainty","abstract":"In the literature the examination timetabling problem (ETTP) is often considered a post-enrollment problem (PE-ETTP). In the real world, universities often schedule their exams before students register using information from previous terms. A direct consequence of this approach is the uncertainty present in the resulting models. In this work we discuss several approaches available in the robust optimization literature. We consider the implications of each approach in respect to the examination timetabling problem and present how the most favorable approaches can be applied to the ETTP. Afterwards we analyze the impact of some possible implementations of the given robustness approaches on two real world instances and several random instances generated by our instance generation framework which we introduce in this work.","sentences":["In the literature the examination timetabling problem (ETTP) is often considered a post-enrollment problem (PE-ETTP).","In the real world, universities often schedule their exams before students register using information from previous terms.","A direct consequence of this approach is the uncertainty present in the resulting models.","In this work we discuss several approaches available in the robust optimization literature.","We consider the implications of each approach in respect to the examination timetabling problem and present how the most favorable approaches can be applied to the ETTP.","Afterwards we analyze the impact of some possible implementations of the given robustness approaches on two real world instances and several random instances generated by our instance generation framework which we introduce in this work."],"url":"http://arxiv.org/abs/2311.17766v1"}
{"created":"2023-11-29 15:58:30","title":"Robust Scheduling in Cloud Environment Based on Heuristic Optimization Algorithm","abstract":"Aiming at analyzing performance in cloud computing, some unpredictable perturbations which may lead to performance downgrade are essential factors that should not be neglected. To avoid performance downgrade in cloud computing system, it is reasonable to measure the impact of the perturbations, and further propose a robust scheduling strategy to maintain the performance of the system at an acceptable level. In this paper, we first describe the supply-demand relationship of service between cloud service providers and customers, in which the profit and waiting time are objectives they most concerned. Then, on the basis of introducing the lowest acceptable profit and longest acceptable waiting time for cloud service providers and customers respectively, we define a robustness metric method to declare that the number and speed of servers should be adequately configured in a feasible region, such that the performance of cloud computing system can stay at an acceptable level when it is subject to the perturbations. Subsequently, we discuss the robustness metric method in several cases, and propose heuristic optimization algorithm to enhance the robustness of the system as much as possible. At last, the performances of the proposed algorithm are validated by comparing with DE and PSO algorithm, the results show the superiority of the proposed algorithm.","sentences":["Aiming at analyzing performance in cloud computing, some unpredictable perturbations which may lead to performance downgrade are essential factors that should not be neglected.","To avoid performance downgrade in cloud computing system, it is reasonable to measure the impact of the perturbations, and further propose a robust scheduling strategy to maintain the performance of the system at an acceptable level.","In this paper, we first describe the supply-demand relationship of service between cloud service providers and customers, in which the profit and waiting time are objectives they most concerned.","Then, on the basis of introducing the lowest acceptable profit and longest acceptable waiting time for cloud service providers and customers respectively, we define a robustness metric method to declare that the number and speed of servers should be adequately configured in a feasible region, such that the performance of cloud computing system can stay at an acceptable level when it is subject to the perturbations.","Subsequently, we discuss the robustness metric method in several cases, and propose heuristic optimization algorithm to enhance the robustness of the system as much as possible.","At last, the performances of the proposed algorithm are validated by comparing with DE and PSO algorithm, the results show the superiority of the proposed algorithm."],"url":"http://arxiv.org/abs/2311.17757v1"}
{"created":"2023-11-29 15:56:58","title":"Cinematic Behavior Transfer via NeRF-based Differentiable Filming","abstract":"In the evolving landscape of digital media and video production, the precise manipulation and reproduction of visual elements like camera movements and character actions are highly desired. Existing SLAM methods face limitations in dynamic scenes and human pose estimation often focuses on 2D projections, neglecting 3D statuses. To address these issues, we first introduce a reverse filming behavior estimation technique. It optimizes camera trajectories by leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then introduce a cinematic transfer pipeline that is able to transfer various shot types to a new 2D video or a 3D virtual environment. The incorporation of 3D engine workflow enables superior rendering and control abilities, which also achieves a higher rating in the user study.","sentences":["In the evolving landscape of digital media and video production, the precise manipulation and reproduction of visual elements like camera movements and character actions are highly desired.","Existing SLAM methods face limitations in dynamic scenes and human pose estimation often focuses on 2D projections, neglecting 3D statuses.","To address these issues, we first introduce a reverse filming behavior estimation technique.","It optimizes camera trajectories by leveraging NeRF as a differentiable renderer and refining SMPL tracks.","We then introduce a cinematic transfer pipeline that is able to transfer various shot types to a new 2D video or a 3D virtual environment.","The incorporation of 3D engine workflow enables superior rendering and control abilities, which also achieves a higher rating in the user study."],"url":"http://arxiv.org/abs/2311.17754v1"}
{"created":"2023-11-29 15:56:31","title":"BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment","abstract":"Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms. As undesirable artifacts, banding destroys the original image structure, thus degrading users' quality of experience (QoE). In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e. mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality ratings. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts. A dual convolutional neural network is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters. Experiments demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.","sentences":["Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.","As undesirable artifacts, banding destroys the original image structure, thus degrading users' quality of experience (QoE).","In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.","Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e. mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.","A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality ratings.","Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.","A dual convolutional neural network is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.","The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.","Experiments demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels.","These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment."],"url":"http://arxiv.org/abs/2311.17752v1"}
{"created":"2023-11-29 15:54:15","title":"Addressing Membership Inference Attack in Federated Learning with Model Compression","abstract":"Federated Learning (FL) has been proposed as a privacy-preserving solution for machine learning. However, recent works have shown that Federated Learning can leak private client data through membership attacks. In this paper, we show that the effectiveness of these attacks on the clients negatively correlates with the size of the client datasets and model complexity. Based on this finding, we propose model-agnostic Federated Learning as a privacy-enhancing solution because it enables the use of models of varying complexity in the clients. To this end, we present $\\texttt{MaPP-FL}$, a novel privacy-aware FL approach that leverages model compression on the clients while keeping a full model on the server. We compare the performance of $\\texttt{MaPP-FL}$ against state-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, and FEMNIST vision datasets. Our experiments show the effectiveness of $\\texttt{MaPP-FL}$ in preserving the clients' and the server's privacy while achieving competitive classification accuracies.","sentences":["Federated Learning (FL) has been proposed as a privacy-preserving solution for machine learning.","However, recent works have shown that Federated Learning can leak private client data through membership attacks.","In this paper, we show that the effectiveness of these attacks on the clients negatively correlates with the size of the client datasets and model complexity.","Based on this finding, we propose model-agnostic Federated Learning as a privacy-enhancing solution because it enables the use of models of varying complexity in the clients.","To this end, we present $\\texttt{MaPP-FL}$, a novel privacy-aware FL approach that leverages model compression on the clients while keeping a full model on the server.","We compare the performance of $\\texttt{MaPP-FL}$ against state-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, and FEMNIST vision datasets.","Our experiments show the effectiveness of $\\texttt{MaPP-FL}$ in preserving the clients' and the server's privacy while achieving competitive classification accuracies."],"url":"http://arxiv.org/abs/2311.17750v1"}
{"created":"2023-11-29 15:49:31","title":"Variational Bayes image restoration with compressive autoencoders","abstract":"Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization of the latent MAP. In this work, we propose to use compressive autoencoders for latent estimation. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. We then introduce the Variational Bayes Latent Estimation (VBLE) algorithm, which performs this estimation within the framework of variational inference. This allows for fast and easy (approximate) posterior sampling. Experimental results on image datasets BSD and FFHQ demonstrate that VBLE reaches similar performance than state-of-the-art plug-and-play methods, while being able to quantify uncertainties faster than other existing posterior sampling techniques.","sentences":["Regularization of inverse problems is of paramount importance in computational imaging.","The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers.","While state-of-the-art plug-and-play methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization.","However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers.","Besides, their complexity hampers the optimization of the latent MAP.","In this work, we propose to use compressive autoencoders for latent estimation.","These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models.","We then introduce the Variational Bayes Latent Estimation (VBLE) algorithm, which performs this estimation within the framework of variational inference.","This allows for fast and easy (approximate) posterior sampling.","Experimental results on image datasets BSD and FFHQ demonstrate that VBLE reaches similar performance than state-of-the-art plug-and-play methods, while being able to quantify uncertainties faster than other existing posterior sampling techniques."],"url":"http://arxiv.org/abs/2311.17744v1"}
{"created":"2023-11-29 15:49:24","title":"Mukhyansh: A Headline Generation Dataset for Indic Languages","abstract":"The task of headline generation within the realm of Natural Language Processing (NLP) holds immense significance, as it strives to distill the true essence of textual content into concise and attention-grabbing summaries. While noteworthy progress has been made in headline generation for widely spoken languages like English, there persist numerous challenges when it comes to generating headlines in low-resource languages, such as the rich and diverse Indian languages. A prominent obstacle that specifically hinders headline generation in Indian languages is the scarcity of high-quality annotated data. To address this crucial gap, we proudly present Mukhyansh, an extensive multilingual dataset, tailored for Indian language headline generation. Comprising an impressive collection of over 3.39 million article-headline pairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu, Tamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati. We present a comprehensive evaluation of several state-of-the-art baseline models. Additionally, through an empirical analysis of existing works, we demonstrate that Mukhyansh outperforms all other models, achieving an impressive average ROUGE-L score of 31.43 across all 8 languages.","sentences":["The task of headline generation within the realm of Natural Language Processing (NLP) holds immense significance, as it strives to distill the true essence of textual content into concise and attention-grabbing summaries.","While noteworthy progress has been made in headline generation for widely spoken languages like English, there persist numerous challenges when it comes to generating headlines in low-resource languages, such as the rich and diverse Indian languages.","A prominent obstacle that specifically hinders headline generation in Indian languages is the scarcity of high-quality annotated data.","To address this crucial gap, we proudly present Mukhyansh, an extensive multilingual dataset, tailored for Indian language headline generation.","Comprising an impressive collection of over 3.39 million article-headline pairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu, Tamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati.","We present a comprehensive evaluation of several state-of-the-art baseline models.","Additionally, through an empirical analysis of existing works, we demonstrate that Mukhyansh outperforms all other models, achieving an impressive average ROUGE-L score of 31.43 across all 8 languages."],"url":"http://arxiv.org/abs/2311.17743v1"}
{"created":"2023-11-29 15:48:45","title":"Robust Localization and Tracking of UAVs in OTFS-based Networks","abstract":"We consider the problem of accurately localizing N unmanned aerial vehicles (UAV) in 3D space where the UAVs are part of a swarm and communicate with each other through orthogonal time-frequency space (OTFS) modulated signals. Each receiving UAV estimates the multipath wireless channel on each link formed by the line-of-sight (LoS) transmission and by the single reflections from the remaining N-2 UAVs. The estimated power delay profiles are communicated to an edge server, which is in charge of computing the exact location and speed of the UAVs. To obtain the UAVs locations and velocities, we propose an iterative algorithm, named Turbo Iterative Positioning (TIP), which, using a belief-propagation approach, effectively exploits the time difference of arrival (TDoA) measurements between the LoS and the non-LoS paths. Enabling a full cold start (no prior knowledge), our solution first maps each TDoA's profile element to a specific ID of the reflecting UAV's. The Doppler shifts measured by the OTFS receivers associated with each path are also used to estimate the UAV's velocities. The localization of the N UAVs is then derived via gradient descent optimization, with the aid of turbo-like iterations that can progressively correct some of the residual errors in the initial ID mapping operation. Our numerical results, obtained also using real-world traces, show how the multipath links are beneficial to achieving very accurate localization and speed of all UAVs, even with a limited delay-Doppler resolution. Robustness of our scheme is proven by its performance approaching the Cramer-Rao bound.","sentences":["We consider the problem of accurately localizing N unmanned aerial vehicles (UAV) in 3D space where the UAVs are part of a swarm and communicate with each other through orthogonal time-frequency space (OTFS) modulated signals.","Each receiving UAV estimates the multipath wireless channel on each link formed by the line-of-sight (LoS) transmission and by the single reflections from the remaining N-2 UAVs.","The estimated power delay profiles are communicated to an edge server, which is in charge of computing the exact location and speed of the UAVs.","To obtain the UAVs locations and velocities, we propose an iterative algorithm, named Turbo Iterative Positioning (TIP), which, using a belief-propagation approach, effectively exploits the time difference of arrival (TDoA) measurements between the LoS and the non-LoS paths.","Enabling a full cold start (no prior knowledge), our solution first maps each TDoA's profile element to a specific ID of the reflecting UAV's.","The Doppler shifts measured by the OTFS receivers associated with each path are also used to estimate the UAV's velocities.","The localization of the N UAVs is then derived via gradient descent optimization, with the aid of turbo-like iterations that can progressively correct some of the residual errors in the initial ID mapping operation.","Our numerical results, obtained also using real-world traces, show how the multipath links are beneficial to achieving very accurate localization and speed of all UAVs, even with a limited delay-Doppler resolution.","Robustness of our scheme is proven by its performance approaching the Cramer-Rao bound."],"url":"http://arxiv.org/abs/2311.17742v1"}
{"created":"2023-11-29 15:44:39","title":"End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data","abstract":"Joint rich and normalized automatic speech recognition (ASR), that produces transcriptions both with and without punctuation and capitalization, remains a challenge. End-to-end (E2E) ASR models offer both convenience and the ability to perform such joint transcription of speech. Training such models requires paired speech and rich text data, which is not widely available. In this paper, we compare two different approaches to train a stateless Transducer-based E2E joint rich and normalized ASR system, ready for streaming applications, with a limited amount of rich labeled data. The first approach uses a language model to generate pseudo-rich transcriptions of normalized training data. The second approach uses a single decoder conditioned on the type of the output. The first approach leads to E2E rich ASR which perform better on out-of-domain data, with up to 9% relative reduction in errors. The second approach demonstrates the feasibility of an E2E joint rich and normalized ASR system using as low as 5% rich training data with moderate (2.42% absolute) increase in errors.","sentences":["Joint rich and normalized automatic speech recognition (ASR), that produces transcriptions both with and without punctuation and capitalization, remains a challenge.","End-to-end (E2E) ASR models offer both convenience and the ability to perform such joint transcription of speech.","Training such models requires paired speech and rich text data, which is not widely available.","In this paper, we compare two different approaches to train a stateless Transducer-based E2E joint rich and normalized ASR system, ready for streaming applications, with a limited amount of rich labeled data.","The first approach uses a language model to generate pseudo-rich transcriptions of normalized training data.","The second approach uses a single decoder conditioned on the type of the output.","The first approach leads to E2E rich ASR which perform better on out-of-domain data, with up to 9% relative reduction in errors.","The second approach demonstrates the feasibility of an E2E joint rich and normalized ASR system using as low as 5% rich training data with moderate (2.42% absolute) increase in errors."],"url":"http://arxiv.org/abs/2311.17741v1"}
{"created":"2023-11-29 15:40:11","title":"GenZI: Zero-Shot 3D Human-Scene Interaction Generation","abstract":"Can we synthesize 3D humans interacting with scenes without learning from any 3D human-scene interaction data? We propose GenZI, the first zero-shot approach to generating 3D human-scene interactions. Key to GenZI is our distillation of interaction priors from large vision-language models (VLMs), which have learned a rich semantic space of 2D human-scene compositions. Given a natural language description and a coarse point location of the desired interaction in a 3D scene, we first leverage VLMs to imagine plausible 2D human interactions inpainted into multiple rendered views of the scene. We then formulate a robust iterative optimization to synthesize the pose and shape of a 3D human model in the scene, guided by consistency with the 2D interaction hypotheses. In contrast to existing learning-based approaches, GenZI circumvents the conventional need for captured 3D interaction data, and allows for flexible control of the 3D interaction synthesis with easy-to-use text prompts. Extensive experiments show that our zero-shot approach has high flexibility and generality, making it applicable to diverse scene types, including both indoor and outdoor environments.","sentences":["Can we synthesize 3D humans interacting with scenes without learning from any 3D human-scene interaction data?","We propose GenZI, the first zero-shot approach to generating 3D human-scene interactions.","Key to GenZI is our distillation of interaction priors from large vision-language models (VLMs), which have learned a rich semantic space of 2D human-scene compositions.","Given a natural language description and a coarse point location of the desired interaction in a 3D scene, we first leverage VLMs to imagine plausible 2D human interactions inpainted into multiple rendered views of the scene.","We then formulate a robust iterative optimization to synthesize the pose and shape of a 3D human model in the scene, guided by consistency with the 2D interaction hypotheses.","In contrast to existing learning-based approaches, GenZI circumvents the conventional need for captured 3D interaction data, and allows for flexible control of the 3D interaction synthesis with easy-to-use text prompts.","Extensive experiments show that our zero-shot approach has high flexibility and generality, making it applicable to diverse scene types, including both indoor and outdoor environments."],"url":"http://arxiv.org/abs/2311.17737v1"}
{"created":"2023-11-29 15:34:55","title":"Know your audience","abstract":"Distributed function computation is the problem, for a networked system of $n$ autonomous agents, to collectively compute the value $f(v_1, \\ldots, v_n)$ of some input values, each initially private to one agent in the network. Here, we study and organize results pertaining to distributed function computation in anonymous networks, both for the static and the dynamic case, under a communication model of directed and synchronous message exchanges, but with varying assumptions in the degree of awareness or control that a single agent has over its outneighbors.   Our main argument is three-fold. First, in the \"blind broadcast\" model, where in each round an agent merely casts out a unique message without any knowledge or control over its addressees, the computable functions are those that only depend on the set of the input values, but not on their multiplicities or relative frequencies in the input. Second, in contrast, when we assume either that a) in each round, the agents know how many outneighbors they have; b) all communications links in the network are bidirectional; or c) the agents may address each of their outneighbors individually, then the set of computable functions grows to contain all functions that depend on the relative frequencies of each value in the input - such as the average - but not on their multiplicities - thus, not the sum. Third, however, if one or several agents are distinguished as leaders, or if the cardinality of the network is known, then under any of the above three assumptions it becomes possible to recover the complete multiset of the input values, and thus compute any function of the distributed input as long as it is invariant under permutation of its arguments. In the case of dynamic networks, we also discuss the impact of multiple connectivity assumptions.","sentences":["Distributed function computation is the problem, for a networked system of $n$ autonomous agents, to collectively compute the value $f(v_1, \\ldots, v_n)$ of some input values, each initially private to one agent in the network.","Here, we study and organize results pertaining to distributed function computation in anonymous networks, both for the static and the dynamic case, under a communication model of directed and synchronous message exchanges, but with varying assumptions in the degree of awareness or control that a single agent has over its outneighbors.   ","Our main argument is three-fold.","First, in the \"blind broadcast\" model, where in each round an agent merely casts out a unique message without any knowledge or control over its addressees, the computable functions are those that only depend on the set of the input values, but not on their multiplicities or relative frequencies in the input.","Second, in contrast, when we assume either that a) in each round, the agents know how many outneighbors they have; b) all communications links in the network are bidirectional; or c) the agents may address each of their outneighbors individually, then the set of computable functions grows to contain all functions that depend on the relative frequencies of each value in the input - such as the average - but not on their multiplicities - thus, not the sum.","Third, however, if one or several agents are distinguished as leaders, or if the cardinality of the network is known, then under any of the above three assumptions it becomes possible to recover the complete multiset of the input values, and thus compute any function of the distributed input as long as it is invariant under permutation of its arguments.","In the case of dynamic networks, we also discuss the impact of multiple connectivity assumptions."],"url":"http://arxiv.org/abs/2311.17728v1"}
{"created":"2023-11-29 15:33:16","title":"Multi-dimensional Energy Limitation in Sphere Shaping for Nonlinear Interference Noise Mitigation","abstract":"We propose Four-Dimensional (4D) energy limit enumerative sphere shaping (ESS) of $M$-QAM signaling to minimize rate loss and improve the transmission performance over non-linear WDM optical-fiber systems. Simulation results show that the proposed scheme outperforms the conventional ESS by $0.19$~bit/4D-symbol in achievable information rate over a $205$-km single-span link and a WDM transmission of five polarization-division-multiplexed channels with $400$-Gbit/s net rate per channel. We also study the achieved performance over several shaping block lengths and show that the achieved gains do not scale well over multi-span systems.","sentences":["We propose Four-Dimensional (4D) energy limit enumerative sphere shaping (ESS) of $M$-QAM signaling to minimize rate loss and improve the transmission performance over non-linear WDM optical-fiber systems.","Simulation results show that the proposed scheme outperforms the conventional ESS by $0.19$~bit/4D-symbol in achievable information rate over a $205$-km single-span link and a WDM transmission of five polarization-division-multiplexed channels with $400$-Gbit/s net rate per channel.","We also study the achieved performance over several shaping block lengths and show that the achieved gains do not scale well over multi-span systems."],"url":"http://arxiv.org/abs/2311.17726v1"}
{"created":"2023-11-29 15:21:35","title":"SenTest: Evaluating Robustness of Sentence Encoders","abstract":"Contrastive learning has proven to be an effective method for pre-training models using weakly labeled data in the vision domain. Sentence transformers are the NLP counterparts to this architecture, and have been growing in popularity due to their rich and effective sentence representations. Having effective sentence representations is paramount in multiple tasks, such as information retrieval, retrieval augmented generation (RAG), and sentence comparison. Keeping in mind the deployability factor of transformers, evaluating the robustness of sentence transformers is of utmost importance. This work focuses on evaluating the robustness of the sentence encoders. We employ several adversarial attacks to evaluate its robustness. This system uses character-level attacks in the form of random character substitution, word-level attacks in the form of synonym replacement, and sentence-level attacks in the form of intra-sentence word order shuffling. The results of the experiments strongly undermine the robustness of sentence encoders. The models produce significantly different predictions as well as embeddings on perturbed datasets. The accuracy of the models can fall up to 15 percent on perturbed datasets as compared to unperturbed datasets. Furthermore, the experiments demonstrate that these embeddings does capture the semantic and syntactic structure (sentence order) of sentences. However, existing supervised classification strategies fail to leverage this information, and merely function as n-gram detectors.","sentences":["Contrastive learning has proven to be an effective method for pre-training models using weakly labeled data in the vision domain.","Sentence transformers are the NLP counterparts to this architecture, and have been growing in popularity due to their rich and effective sentence representations.","Having effective sentence representations is paramount in multiple tasks, such as information retrieval, retrieval augmented generation (RAG), and sentence comparison.","Keeping in mind the deployability factor of transformers, evaluating the robustness of sentence transformers is of utmost importance.","This work focuses on evaluating the robustness of the sentence encoders.","We employ several adversarial attacks to evaluate its robustness.","This system uses character-level attacks in the form of random character substitution, word-level attacks in the form of synonym replacement, and sentence-level attacks in the form of intra-sentence word order shuffling.","The results of the experiments strongly undermine the robustness of sentence encoders.","The models produce significantly different predictions as well as embeddings on perturbed datasets.","The accuracy of the models can fall up to 15 percent on perturbed datasets as compared to unperturbed datasets.","Furthermore, the experiments demonstrate that these embeddings does capture the semantic and syntactic structure (sentence order) of sentences.","However, existing supervised classification strategies fail to leverage this information, and merely function as n-gram detectors."],"url":"http://arxiv.org/abs/2311.17722v1"}
{"created":"2023-11-29 15:19:49","title":"Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers","abstract":"Concept erasure in text-to-image diffusion models aims to disable pre-trained diffusion models from generating images related to a target concept. To perform reliable concept erasure, the properties of robustness and locality are desirable. The former refrains the model from producing images associated with the target concept for any paraphrased or learned prompts, while the latter preserves the model ability in generating images for non-target concepts. In this paper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler), which learns a lightweight Eraser to perform concept erasing and enhances locality and robustness with the proposed concept-localized regularization and adversarial prompt learning, respectively. Comprehensive quantitative and qualitative experiments with various concept prompts verify the superiority of Receler over the previous erasing methods on the above two desirable properties.","sentences":["Concept erasure in text-to-image diffusion models aims to disable pre-trained diffusion models from generating images related to a target concept.","To perform reliable concept erasure, the properties of robustness and locality are desirable.","The former refrains the model from producing images associated with the target concept for any paraphrased or learned prompts, while the latter preserves the model ability in generating images for non-target concepts.","In this paper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler), which learns a lightweight Eraser to perform concept erasing and enhances locality and robustness with the proposed concept-localized regularization and adversarial prompt learning, respectively.","Comprehensive quantitative and qualitative experiments with various concept prompts verify the superiority of Receler over the previous erasing methods on the above two desirable properties."],"url":"http://arxiv.org/abs/2311.17717v1"}
