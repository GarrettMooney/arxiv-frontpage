{"created":"2023-09-26 17:59:52","title":"Generating Visual Scenes from Touch","abstract":"An emerging line of work has sought to generate plausible imagery from touch. Existing approaches, however, tackle only narrow aspects of the visuo-tactile synthesis problem, and lag significantly behind the quality of cross-modal synthesis methods in other domains. We draw on recent advances in latent diffusion to create a model for synthesizing images from tactile signals (and vice versa) and apply it to a number of visuo-tactile synthesis tasks. Using this model, we significantly outperform prior work on the tactile-driven stylization problem, i.e., manipulating an image to match a touch signal, and we are the first to successfully generate images from touch without additional sources of information about the scene. We also successfully use our model to address two novel synthesis problems: generating images that do not contain the touch sensor or the hand holding it, and estimating an image's shading from its reflectance and touch.","sentences":["An emerging line of work has sought to generate plausible imagery from touch.","Existing approaches, however, tackle only narrow aspects of the visuo-tactile synthesis problem, and lag significantly behind the quality of cross-modal synthesis methods in other domains.","We draw on recent advances in latent diffusion to create a model for synthesizing images from tactile signals (and vice versa) and apply it to a number of visuo-tactile synthesis tasks.","Using this model, we significantly outperform prior work on the tactile-driven stylization problem, i.e., manipulating an image to match a touch signal, and we are the first to successfully generate images from touch without additional sources of information about the scene.","We also successfully use our model to address two novel synthesis problems: generating images that do not contain the touch sensor or the hand holding it, and estimating an image's shading from its reflectance and touch."],"url":"http://arxiv.org/abs/2309.15117v1"}
{"created":"2023-09-26 17:58:20","title":"InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition","abstract":"We propose InternLM-XComposer, a vision-language large model that enables advanced image-text comprehension and composition. The innovative nature of our model is highlighted by three appealing properties: 1) Interleaved Text-Image Composition: InternLM-XComposer can effortlessly generate coherent and contextual articles that seamlessly integrate images, providing a more engaging and immersive reading experience. Simply provide a title, and our system will generate the corresponding manuscript. It can intelligently identify the areas in the text where images would enhance the content and automatically insert the most appropriate visual candidates. 2) Comprehension with Rich Multilingual Knowledge: The text-image comprehension is empowered by training on extensive multi-modal multilingual concepts with carefully crafted strategies, resulting in a deep understanding of visual content. 3) State-of-the-art Performance: Our model consistently achieves state-of-the-art results across various mainstream benchmarks for vision-language foundational models, including MME Benchmark, MMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark). Collectively, InternLM-XComposer seamlessly blends advanced text-image comprehension and composition, revolutionizing vision-language interaction and offering new insights and opportunities. The InternLM-XComposer models with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.","sentences":["We propose InternLM-XComposer, a vision-language large model that enables advanced image-text comprehension and composition.","The innovative nature of our model is highlighted by three appealing properties: 1) Interleaved Text-Image Composition: InternLM-XComposer can effortlessly generate coherent and contextual articles that seamlessly integrate images, providing a more engaging and immersive reading experience.","Simply provide a title, and our system will generate the corresponding manuscript.","It can intelligently identify the areas in the text where images would enhance the content and automatically insert the most appropriate visual candidates.","2) Comprehension with Rich Multilingual Knowledge:","The text-image comprehension is empowered by training on extensive multi-modal multilingual concepts with carefully crafted strategies, resulting in a deep understanding of visual content.","3) State-of-the-art Performance:","Our model consistently achieves state-of-the-art results across various mainstream benchmarks for vision-language foundational models, including MME Benchmark, MMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).","Collectively, InternLM-XComposer seamlessly blends advanced text-image comprehension and composition, revolutionizing vision-language interaction and offering new insights and opportunities.","The InternLM-XComposer models with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer."],"url":"http://arxiv.org/abs/2309.15112v1"}
{"created":"2023-09-26 17:57:44","title":"SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem","abstract":"In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \\:\\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\\textit{signal-heavy}$ phase, where SGD maintains and balances the features. We leverage the simultaneous training of the layers to show that it is sufficient for only a small fraction of the neurons to learn features, since those neurons will be amplified by the simultaneous growth of their second layer weights.","sentences":["In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function.","We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \\:\\text{polylog}(d)$ samples.","Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss.","To our knowledge, this work is the first to give a sample complexity of $\\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training.","Our main technique is showing that the network evolves in two phases: a $\\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\\textit{signal-heavy}$ phase, where SGD maintains and balances the features.","We leverage the simultaneous training of the layers to show that it is sufficient for only a small fraction of the neurons to learn features, since those neurons will be amplified by the simultaneous growth of their second layer weights."],"url":"http://arxiv.org/abs/2309.15111v1"}
{"created":"2023-09-26 17:56:31","title":"Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow","abstract":"Dense visual correspondence plays a vital role in robotic perception. This work focuses on establishing the dense correspondence between a pair of images that captures dynamic scenes undergoing substantial transformations. We introduce Doduo to learn general dense visual correspondence from in-the-wild images and videos without ground truth supervision. Given a pair of images, it estimates the dense flow field encoding the displacement of each pixel in one image to its corresponding pixel in the other image. Doduo uses flow-based warping to acquire supervisory signals for the training. Incorporating semantic priors with self-supervised flow training, Doduo produces accurate dense correspondence robust to the dynamic changes of the scenes. Trained on an in-the-wild video dataset, Doduo illustrates superior performance on point-level correspondence estimation over existing self-supervised correspondence learning baselines. We also apply Doduo to articulation estimation and zero-shot goal-conditioned manipulation, underlining its practical applications in robotics. Code and additional visualizations are available at https://ut-austin-rpl.github.io/Doduo","sentences":["Dense visual correspondence plays a vital role in robotic perception.","This work focuses on establishing the dense correspondence between a pair of images that captures dynamic scenes undergoing substantial transformations.","We introduce Doduo to learn general dense visual correspondence from in-the-wild images and videos without ground truth supervision.","Given a pair of images, it estimates the dense flow field encoding the displacement of each pixel in one image to its corresponding pixel in the other image.","Doduo uses flow-based warping to acquire supervisory signals for the training.","Incorporating semantic priors with self-supervised flow training, Doduo produces accurate dense correspondence robust to the dynamic changes of the scenes.","Trained on an in-the-wild video dataset, Doduo illustrates superior performance on point-level correspondence estimation over existing self-supervised correspondence learning baselines.","We also apply Doduo to articulation estimation and zero-shot goal-conditioned manipulation, underlining its practical applications in robotics.","Code and additional visualizations are available at https://ut-austin-rpl.github.io/Doduo"],"url":"http://arxiv.org/abs/2309.15110v1"}
{"created":"2023-09-26 17:56:21","title":"DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation","abstract":"3D perception based on the representations learned from multi-camera bird's-eye-view (BEV) is trending as cameras are cost-effective for mass production in autonomous driving industry. However, there exists a distinct performance gap between multi-camera BEV and LiDAR based 3D object detection. One key reason is that LiDAR captures accurate depth and other geometry measurements, while it is notoriously challenging to infer such 3D information from merely image input. In this work, we propose to boost the representation learning of a multi-camera BEV based student detector by training it to imitate the features of a well-trained LiDAR based teacher detector. We propose effective balancing strategy to enforce the student to focus on learning the crucial features from the teacher, and generalize knowledge transfer to multi-scale layers with temporal fusion. We conduct extensive evaluations on multiple representative models of multi-camera BEV. Experiments reveal that our approach renders significant improvement over the student models, leading to the state-of-the-art performance on the popular benchmark nuScenes.","sentences":["3D perception based on the representations learned from multi-camera bird's-eye-view (BEV) is trending as cameras are cost-effective for mass production in autonomous driving industry.","However, there exists a distinct performance gap between multi-camera BEV and LiDAR based 3D object detection.","One key reason is that LiDAR captures accurate depth and other geometry measurements, while it is notoriously challenging to infer such 3D information from merely image input.","In this work, we propose to boost the representation learning of a multi-camera BEV based student detector by training it to imitate the features of a well-trained LiDAR based teacher detector.","We propose effective balancing strategy to enforce the student to focus on learning the crucial features from the teacher, and generalize knowledge transfer to multi-scale layers with temporal fusion.","We conduct extensive evaluations on multiple representative models of multi-camera BEV.","Experiments reveal that our approach renders significant improvement over the student models, leading to the state-of-the-art performance on the popular benchmark nuScenes."],"url":"http://arxiv.org/abs/2309.15109v1"}
{"created":"2023-09-26 17:52:41","title":"Efficient, traceable, and numerical error-free implementation of the MMS voting rule","abstract":"I propose an alternative algorithm to compute the MMS voting rule. Instead of using linear programming, in this new algorithm the maximin support value of a committee is computed using a sequence of maximum flow problems.","sentences":["I propose an alternative algorithm to compute the MMS voting rule.","Instead of using linear programming, in this new algorithm the maximin support value of a committee is computed using a sequence of maximum flow problems."],"url":"http://arxiv.org/abs/2309.15104v1"}
{"created":"2023-09-26 17:52:03","title":"LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models","abstract":"This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously a) accomplish the synthesis of visually realistic and temporally coherent videos while b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: 1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. 2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications.","sentences":["This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis.","It is a highly desirable yet challenging task to simultaneously a) accomplish the synthesis of visually realistic and temporally coherent videos while b) preserving the strong creative generation nature of the pre-trained T2I model.","To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model.","Our key insights are two-fold: 1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data.","2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes.","To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal.","Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively.","Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications."],"url":"http://arxiv.org/abs/2309.15103v1"}
{"created":"2023-09-26 17:50:37","title":"Local Positional Encoding for Multi-Layer Perceptrons","abstract":"A multi-layer perceptron (MLP) is a type of neural networks which has a long history of research and has been studied actively recently in computer vision and graphics fields. One of the well-known problems of an MLP is the capability of expressing high-frequency signals from low-dimensional inputs. There are several studies for input encodings to improve the reconstruction quality of an MLP by applying pre-processing against the input data. This paper proposes a novel input encoding method, local positional encoding, which is an extension of positional and grid encodings. Our proposed method combines these two encoding techniques so that a small MLP learns high-frequency signals by using positional encoding with fewer frequencies under the lower resolution of the grid to consider the local position and scale in each grid cell. We demonstrate the effectiveness of our proposed method by applying it to common 2D and 3D regression tasks where it shows higher-quality results compared to positional and grid encodings, and comparable results to hierarchical variants of grid encoding such as multi-resolution grid encoding with equivalent memory footprint.","sentences":["A multi-layer perceptron (MLP) is a type of neural networks which has a long history of research and has been studied actively recently in computer vision and graphics fields.","One of the well-known problems of an MLP is the capability of expressing high-frequency signals from low-dimensional inputs.","There are several studies for input encodings to improve the reconstruction quality of an MLP by applying pre-processing against the input data.","This paper proposes a novel input encoding method, local positional encoding, which is an extension of positional and grid encodings.","Our proposed method combines these two encoding techniques so that a small MLP learns high-frequency signals by using positional encoding with fewer frequencies under the lower resolution of the grid to consider the local position and scale in each grid cell.","We demonstrate the effectiveness of our proposed method by applying it to common 2D and 3D regression tasks where it shows higher-quality results compared to positional and grid encodings, and comparable results to hierarchical variants of grid encoding such as multi-resolution grid encoding with equivalent memory footprint."],"url":"http://arxiv.org/abs/2309.15101v1"}
{"created":"2023-09-26 17:48:55","title":"Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models","abstract":"We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as Constraint Satisfaction Problems and use this framework to investigate how the model interacts internally with factual constraints. Specifically, we discover a strong positive relation between the model's attention to constraint tokens and the factual accuracy of its responses. In our curated suite of 11 datasets with over 40,000 prompts, we study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing self-attention patterns, that can predict constraint satisfaction and factual errors, and allows early error identification. The approach and findings demonstrate how using the mechanistic understanding of factuality in LLMs can enhance reliability.","sentences":["We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text.","We propose modeling factual queries as Constraint Satisfaction Problems and use this framework to investigate how the model interacts internally with factual constraints.","Specifically, we discover a strong positive relation between the model's attention to constraint tokens and the factual accuracy of its responses.","In our curated suite of 11 datasets with over 40,000 prompts, we study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B).","We propose SAT Probe, a method probing self-attention patterns, that can predict constraint satisfaction and factual errors, and allows early error identification.","The approach and findings demonstrate how using the mechanistic understanding of factuality in LLMs can enhance reliability."],"url":"http://arxiv.org/abs/2309.15098v1"}
{"created":"2023-09-26 17:43:58","title":"Case Study: Ensemble Decision-Based Annotation of Unconstrained Real Estate Images","abstract":"We describe a proof-of-concept for annotating real estate images using simple iterative rule-based semi-supervised learning. In this study, we have gained important insights into the content characteristics and uniqueness of individual image classes as well as essential requirements for a practical implementation.","sentences":["We describe a proof-of-concept for annotating real estate images using simple iterative rule-based semi-supervised learning.","In this study, we have gained important insights into the content characteristics and uniqueness of individual image classes as well as essential requirements for a practical implementation."],"url":"http://arxiv.org/abs/2309.15097v1"}
{"created":"2023-09-26 17:42:52","title":"Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs","abstract":"Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the training data. A consequence of this lack of dependence on the targets is that the NTK cannot perform better than the optimal MKL kernel on the training set. By using iterative reweighting, we improve the weights induced by the NTK to obtain the optimal MKL kernel which is equivalent to the solution of the exact convex reformulation of the gated ReLU network. We also provide several numerical simulations corroborating our theory. Additionally, we provide an analysis of the prediction error of the resulting optimal kernel via consistency results for the group lasso.","sentences":["Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1)","Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks.","The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs.","In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK.","Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the training data.","A consequence of this lack of dependence on the targets is that the NTK cannot perform better than the optimal MKL kernel on the training set.","By using iterative reweighting, we improve the weights induced by the NTK to obtain the optimal MKL kernel which is equivalent to the solution of the exact convex reformulation of the gated ReLU network.","We also provide several numerical simulations corroborating our theory.","Additionally, we provide an analysis of the prediction error of the resulting optimal kernel via consistency results for the group lasso."],"url":"http://arxiv.org/abs/2309.15096v1"}
{"created":"2023-09-26 17:40:29","title":"Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process","abstract":"This scientific paper explores two distinct approaches for identifying and approximating the simulation model, particularly in the context of the snap process crucial to medical device assembly. Simulation models play a pivotal role in providing engineers with insights into industrial processes, enabling experimentation and troubleshooting before physical assembly. However, their complexity often results in time-consuming computations.   To mitigate this complexity, we present two distinct methods for identifying simulation models: one utilizing Spline functions and the other harnessing Machine Learning (ML) models. Our goal is to create adaptable models that accurately represent the snap process and can accommodate diverse scenarios. Such models hold promise for enhancing process understanding and aiding in decision-making, especially when data availability is limited.","sentences":["This scientific paper explores two distinct approaches for identifying and approximating the simulation model, particularly in the context of the snap process crucial to medical device assembly.","Simulation models play a pivotal role in providing engineers with insights into industrial processes, enabling experimentation and troubleshooting before physical assembly.","However, their complexity often results in time-consuming computations.   ","To mitigate this complexity, we present two distinct methods for identifying simulation models:","one utilizing Spline functions and the other harnessing Machine Learning (ML) models.","Our goal is to create adaptable models that accurately represent the snap process and can accommodate diverse scenarios.","Such models hold promise for enhancing process understanding and aiding in decision-making, especially when data availability is limited."],"url":"http://arxiv.org/abs/2309.15094v1"}
{"created":"2023-09-26 17:36:26","title":"VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning","abstract":"Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency groupings of the entities and backgrounds. Next, guided by this output from the video planner, our video generator, Layout2Vid, has explicit control over spatial layouts and can maintain temporal consistency of entities/backgrounds across scenes, while only trained with image-level annotations. Our experiments demonstrate that VideoDirectorGPT framework substantially improves layout and movement control in both single- and multi-scene video generation and can generate multi-scene videos with visual consistency across scenes, while achieving competitive performance with SOTAs in open-domain single-scene T2V generation. We also demonstrate that our framework can dynamically control the strength for layout guidance and can also generate videos with user-provided images. We hope our framework can inspire future work on better integrating the planning ability of LLMs into consistent long video generation.","sentences":["Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos).","Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models.","This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation?","In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation.","Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency groupings of the entities and backgrounds.","Next, guided by this output from the video planner, our video generator, Layout2Vid, has explicit control over spatial layouts and can maintain temporal consistency of entities/backgrounds across scenes, while only trained with image-level annotations.","Our experiments demonstrate that VideoDirectorGPT framework substantially improves layout and movement control in both single- and multi-scene video generation and can generate multi-scene videos with visual consistency across scenes, while achieving competitive performance with SOTAs in open-domain single-scene T2V generation.","We also demonstrate that our framework can dynamically control the strength for layout guidance and can also generate videos with user-provided images.","We hope our framework can inspire future work on better integrating the planning ability of LLMs into consistent long video generation."],"url":"http://arxiv.org/abs/2309.15091v1"}
{"created":"2023-09-26 17:31:57","title":"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models","abstract":"Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs. All the code necessary to reproduce our results is available at https://github.com/castorini/rank_llm.","sentences":["Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints.","This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations.","To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting.","Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4.","We hope our work provides the foundation for future research on reranking with modern LLMs.","All the code necessary to reproduce our results is available at https://github.com/castorini/rank_llm."],"url":"http://arxiv.org/abs/2309.15088v1"}
{"created":"2023-09-26 17:31:35","title":"Privacy-preserving and Privacy-attacking Approaches for Speech and Audio -- A Survey","abstract":"In contemporary society, voice-controlled devices, such as smartphones and home assistants, have become pervasive due to their advanced capabilities and functionality. The always-on nature of their microphones offers users the convenience of readily accessing these devices. However, recent research and events have revealed that such voice-controlled devices are prone to various forms of malicious attacks, hence making it a growing concern for both users and researchers to safeguard against such attacks. Despite the numerous studies that have investigated adversarial attacks and privacy preservation for images, a conclusive study of this nature has not been conducted for the audio domain. Therefore, this paper aims to examine existing approaches for privacy-preserving and privacy-attacking strategies for audio and speech. To achieve this goal, we classify the attack and defense scenarios into several categories and provide detailed analysis of each approach. We also interpret the dissimilarities between the various approaches, highlight their contributions, and examine their limitations. Our investigation reveals that voice-controlled devices based on neural networks are inherently susceptible to specific types of attacks. Although it is possible to enhance the robustness of such models to certain forms of attack, more sophisticated approaches are required to comprehensively safeguard user privacy.","sentences":["In contemporary society, voice-controlled devices, such as smartphones and home assistants, have become pervasive due to their advanced capabilities and functionality.","The always-on nature of their microphones offers users the convenience of readily accessing these devices.","However, recent research and events have revealed that such voice-controlled devices are prone to various forms of malicious attacks, hence making it a growing concern for both users and researchers to safeguard against such attacks.","Despite the numerous studies that have investigated adversarial attacks and privacy preservation for images, a conclusive study of this nature has not been conducted for the audio domain.","Therefore, this paper aims to examine existing approaches for privacy-preserving and privacy-attacking strategies for audio and speech.","To achieve this goal, we classify the attack and defense scenarios into several categories and provide detailed analysis of each approach.","We also interpret the dissimilarities between the various approaches, highlight their contributions, and examine their limitations.","Our investigation reveals that voice-controlled devices based on neural networks are inherently susceptible to specific types of attacks.","Although it is possible to enhance the robustness of such models to certain forms of attack, more sophisticated approaches are required to comprehensively safeguard user privacy."],"url":"http://arxiv.org/abs/2309.15087v1"}
{"created":"2023-09-26 17:31:02","title":"Video-adverb retrieval with compositional adverb-action embeddings","abstract":"Retrieving adverbs that describe an action in a video poses a crucial step towards fine-grained video understanding. We propose a framework for video-to-adverb retrieval (and vice versa) that aligns video embeddings with their matching compositional adverb-action text embedding in a joint embedding space. The compositional adverb-action text embedding is learned using a residual gating mechanism, along with a novel training objective consisting of triplet losses and a regression target. Our method achieves state-of-the-art performance on five recent benchmarks for video-adverb retrieval. Furthermore, we introduce dataset splits to benchmark video-adverb retrieval for unseen adverb-action compositions on subsets of the MSR-VTT Adverbs and ActivityNet Adverbs datasets. Our proposed framework outperforms all prior works for the generalisation task of retrieving adverbs from videos for unseen adverb-action compositions. Code and dataset splits are available at https://hummelth.github.io/ReGaDa/.","sentences":["Retrieving adverbs that describe an action in a video poses a crucial step towards fine-grained video understanding.","We propose a framework for video-to-adverb retrieval (and vice versa) that aligns video embeddings with their matching compositional adverb-action text embedding in a joint embedding space.","The compositional adverb-action text embedding is learned using a residual gating mechanism, along with a novel training objective consisting of triplet losses and a regression target.","Our method achieves state-of-the-art performance on five recent benchmarks for video-adverb retrieval.","Furthermore, we introduce dataset splits to benchmark video-adverb retrieval for unseen adverb-action compositions on subsets of the MSR-VTT Adverbs and ActivityNet Adverbs datasets.","Our proposed framework outperforms all prior works for the generalisation task of retrieving adverbs from videos for unseen adverb-action compositions.","Code and dataset splits are available at https://hummelth.github.io/ReGaDa/."],"url":"http://arxiv.org/abs/2309.15086v1"}
{"created":"2023-09-26 17:27:22","title":"The Surveillance AI Pipeline","abstract":"A rapidly growing number of voices have argued that AI research, and computer vision in particular, is closely tied to mass surveillance. Yet the direct path from computer vision research to surveillance has remained obscured and difficult to assess. This study reveals the Surveillance AI pipeline. We obtain three decades of computer vision research papers and downstream patents (more than 20,000 documents) and present a rich qualitative and quantitative analysis. This analysis exposes the nature and extent of the Surveillance AI pipeline, its institutional roots and evolution, and ongoing patterns of obfuscation. We first perform an in-depth content analysis of computer vision papers and downstream patents, identifying and quantifying key features and the many, often subtly expressed, forms of surveillance that appear. On the basis of this analysis, we present a topology of Surveillance AI that characterizes the prevalent targeting of human data, practices of data transferal, and institutional data use. We find stark evidence of close ties between computer vision and surveillance. The majority (68%) of annotated computer vision papers and patents self-report their technology enables data extraction about human bodies and body parts and even more (90%) enable data extraction about humans in general.","sentences":["A rapidly growing number of voices have argued that AI research, and computer vision in particular, is closely tied to mass surveillance.","Yet the direct path from computer vision research to surveillance has remained obscured and difficult to assess.","This study reveals the Surveillance AI pipeline.","We obtain three decades of computer vision research papers and downstream patents (more than 20,000 documents) and present a rich qualitative and quantitative analysis.","This analysis exposes the nature and extent of the Surveillance AI pipeline, its institutional roots and evolution, and ongoing patterns of obfuscation.","We first perform an in-depth content analysis of computer vision papers and downstream patents, identifying and quantifying key features and the many, often subtly expressed, forms of surveillance that appear.","On the basis of this analysis, we present a topology of Surveillance AI that characterizes the prevalent targeting of human data, practices of data transferal, and institutional data use.","We find stark evidence of close ties between computer vision and surveillance.","The majority (68%) of annotated computer vision papers and patents self-report their technology enables data extraction about human bodies and body parts and even more (90%) enable data extraction about humans in general."],"url":"http://arxiv.org/abs/2309.15084v1"}
{"created":"2023-09-26 17:23:55","title":"RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation","abstract":"Recently, the RGB images and point clouds fusion methods have been proposed to jointly estimate 2D optical flow and 3D scene flow. However, as both conventional RGB cameras and LiDAR sensors adopt a frame-based data acquisition mechanism, their performance is limited by the fixed low sampling rates, especially in highly-dynamic scenes. By contrast, the event camera can asynchronously capture the intensity changes with a very high temporal resolution, providing complementary dynamic information of the observed scenes. In this paper, we incorporate RGB images, Point clouds and Events for joint optical flow and scene flow estimation with our proposed multi-stage multimodal fusion model, RPEFlow. First, we present an attention fusion module with a cross-attention mechanism to implicitly explore the internal cross-modal correlation for 2D and 3D branches, respectively. Second, we introduce a mutual information regularization term to explicitly model the complementary information of three modalities for effective multimodal feature learning. We also contribute a new synthetic dataset to advocate further research. Experiments on both synthetic and real datasets show that our model outperforms the existing state-of-the-art by a wide margin. Code and dataset is available at https://npucvr.github.io/RPEFlow.","sentences":["Recently, the RGB images and point clouds fusion methods have been proposed to jointly estimate 2D optical flow and 3D scene flow.","However, as both conventional RGB cameras and LiDAR sensors adopt a frame-based data acquisition mechanism, their performance is limited by the fixed low sampling rates, especially in highly-dynamic scenes.","By contrast, the event camera can asynchronously capture the intensity changes with a very high temporal resolution, providing complementary dynamic information of the observed scenes.","In this paper, we incorporate RGB images, Point clouds and Events for joint optical flow and scene flow estimation with our proposed multi-stage multimodal fusion model, RPEFlow.","First, we present an attention fusion module with a cross-attention mechanism to implicitly explore the internal cross-modal correlation for 2D and 3D branches, respectively.","Second, we introduce a mutual information regularization term to explicitly model the complementary information of three modalities for effective multimodal feature learning.","We also contribute a new synthetic dataset to advocate further research.","Experiments on both synthetic and real datasets show that our model outperforms the existing state-of-the-art by a wide margin.","Code and dataset is available at https://npucvr.github.io/RPEFlow."],"url":"http://arxiv.org/abs/2309.15082v1"}
{"created":"2023-09-26 17:19:40","title":"Towards High Efficient Long-horizon Planning with Expert-guided Motion-encoding Tree Search","abstract":"Autonomous driving holds promise for increased safety, optimized traffic management, and a new level of convenience in transportation. While model-based reinforcement learning approaches such as MuZero enables long-term planning, the exponentially increase of the number of search nodes as the tree goes deeper significantly effect the searching efficiency.To deal with this problem, in this paper we proposed the expert-guided motion-encoding tree search (EMTS) algorithm. EMTS extends the MuZero algorithm by representing possible motions with a comprehensive motion primitives latent space and incorporating expert policies toimprove the searching efficiency. The comprehensive motion primitives latent space enables EMTS to sample arbitrary trajectories instead of raw action to reduce the depth of the search tree. And the incorporation of expert policies guided the search and training phases the EMTS algorithm to enable early convergence. In the experiment section, the EMTS algorithm is compared with other four algorithms in three challenging scenarios. The experiment result verifies the effectiveness and the searching efficiency of the proposed EMTS algorithm.","sentences":["Autonomous driving holds promise for increased safety, optimized traffic management, and a new level of convenience in transportation.","While model-based reinforcement learning approaches such as MuZero enables long-term planning, the exponentially increase of the number of search nodes as the tree goes deeper significantly effect the searching efficiency.","To deal with this problem, in this paper we proposed the expert-guided motion-encoding tree search (EMTS) algorithm.","EMTS extends the MuZero algorithm by representing possible motions with a comprehensive motion primitives latent space and incorporating expert policies toimprove the searching efficiency.","The comprehensive motion primitives latent space enables EMTS to sample arbitrary trajectories instead of raw action to reduce the depth of the search tree.","And the incorporation of expert policies guided the search and training phases the EMTS algorithm to enable early convergence.","In the experiment section, the EMTS algorithm is compared with other four algorithms in three challenging scenarios.","The experiment result verifies the effectiveness and the searching efficiency of the proposed EMTS algorithm."],"url":"http://arxiv.org/abs/2309.15079v1"}
{"created":"2023-09-26 16:55:42","title":"Logic Locking based Trojans: A Friend Turns Foe","abstract":"Logic locking and hardware Trojans are two fields in hardware security that have been mostly developed independently from each other. In this paper, we identify the relationship between these two fields. We find that a common structure that exists in many logic locking techniques has desirable properties of hardware Trojans (HWT). We then construct a novel type of HWT, called Trojans based on Logic Locking (TroLL), in a way that can evade state-of-the-art ATPG-based HWT detection techniques. In an effort to detect TroLL, we propose customization of existing state-of-the-art ATPG-based HWT detection approaches as well as adapting the SAT-based attacks on logic locking to HWT detection. In our experiments, we use random sampling as reference. It is shown that the customized ATPG-based approaches are the best performing but only offer limited improvement over random sampling. Moreover, their efficacy also diminishes as TroLL's triggers become longer, i.e., have more bits specified). We thereby highlight the need to find a scalable HWT detection approach for TroLL.","sentences":["Logic locking and hardware Trojans are two fields in hardware security that have been mostly developed independently from each other.","In this paper, we identify the relationship between these two fields.","We find that a common structure that exists in many logic locking techniques has desirable properties of hardware Trojans (HWT).","We then construct a novel type of HWT, called Trojans based on Logic Locking (TroLL), in a way that can evade state-of-the-art ATPG-based HWT detection techniques.","In an effort to detect TroLL, we propose customization of existing state-of-the-art ATPG-based HWT detection approaches as well as adapting the SAT-based attacks on logic locking to HWT detection.","In our experiments, we use random sampling as reference.","It is shown that the customized ATPG-based approaches are the best performing but only offer limited improvement over random sampling.","Moreover, their efficacy also diminishes as TroLL's triggers become longer, i.e., have more bits specified).","We thereby highlight the need to find a scalable HWT detection approach for TroLL."],"url":"http://arxiv.org/abs/2309.15067v1"}
{"created":"2023-09-26 16:50:20","title":"Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding","abstract":"Versatile and adaptive semantic understanding would enable autonomous systems to comprehend and interact with their surroundings. Existing fixed-class models limit the adaptability of indoor mobile and assistive autonomous systems. In this work, we introduce LEXIS, a real-time indoor Simultaneous Localization and Mapping (SLAM) system that harnesses the open-vocabulary nature of Large Language Models (LLMs) to create a unified approach to scene understanding and place recognition. The approach first builds a topological SLAM graph of the environment (using visual-inertial odometry) and embeds Contrastive Language-Image Pretraining (CLIP) features in the graph nodes. We use this representation for flexible room classification and segmentation, serving as a basis for room-centric place recognition. This allows loop closure searches to be directed towards semantically relevant places. Our proposed system is evaluated using both public, simulated data and real-world data, covering office and home environments. It successfully categorizes rooms with varying layouts and dimensions and outperforms the state-of-the-art (SOTA). For place recognition and trajectory estimation tasks we achieve equivalent performance to the SOTA, all also utilizing the same pre-trained model. Lastly, we demonstrate the system's potential for planning.","sentences":["Versatile and adaptive semantic understanding would enable autonomous systems to comprehend and interact with their surroundings.","Existing fixed-class models limit the adaptability of indoor mobile and assistive autonomous systems.","In this work, we introduce LEXIS, a real-time indoor Simultaneous Localization and Mapping (SLAM) system that harnesses the open-vocabulary nature of Large Language Models (LLMs) to create a unified approach to scene understanding and place recognition.","The approach first builds a topological SLAM graph of the environment (using visual-inertial odometry) and embeds Contrastive Language-Image Pretraining (CLIP) features in the graph nodes.","We use this representation for flexible room classification and segmentation, serving as a basis for room-centric place recognition.","This allows loop closure searches to be directed towards semantically relevant places.","Our proposed system is evaluated using both public, simulated data and real-world data, covering office and home environments.","It successfully categorizes rooms with varying layouts and dimensions and outperforms the state-of-the-art (SOTA).","For place recognition and trajectory estimation tasks we achieve equivalent performance to the SOTA, all also utilizing the same pre-trained model.","Lastly, we demonstrate the system's potential for planning."],"url":"http://arxiv.org/abs/2309.15065v1"}
{"created":"2023-09-26 16:34:18","title":"Near Real-Time Position Tracking for Robot-Guided Evacuation","abstract":"During the evacuation of a building, the rapid and accurate tracking of human evacuees can be used by a guide robot to increase the effectiveness of the evacuation [1],[2]. This paper introduces a near real-time human position tracking solution tailored for evacuation robots. Using a pose detector, our system first identifies human joints in the camera frame in near real-time and then translates the position of these pixels into real-world coordinates via a simple calibration process. We run multiple trials of the system in action in an indoor lab environment and show that the system can achieve an accuracy of 0.55 meters when compared to ground truth. The system can also achieve an average of 3 frames per second (FPS) which was sufficient for our study on robot-guided human evacuation. The potential of our approach extends beyond mere tracking, paving the way for evacuee motion prediction, allowing the robot to proactively respond to human movements during an evacuation.","sentences":["During the evacuation of a building, the rapid and accurate tracking of human evacuees can be used by a guide robot to increase the effectiveness of the evacuation [1],[2].","This paper introduces a near real-time human position tracking solution tailored for evacuation robots.","Using a pose detector, our system first identifies human joints in the camera frame in near real-time and then translates the position of these pixels into real-world coordinates via a simple calibration process.","We run multiple trials of the system in action in an indoor lab environment and show that the system can achieve an accuracy of 0.55 meters when compared to ground truth.","The system can also achieve an average of 3 frames per second (FPS) which was sufficient for our study on robot-guided human evacuation.","The potential of our approach extends beyond mere tracking, paving the way for evacuee motion prediction, allowing the robot to proactively respond to human movements during an evacuation."],"url":"http://arxiv.org/abs/2309.15054v1"}
{"created":"2023-09-26 16:26:17","title":"When Prolog meets generative models: a new approach for managing knowledge and planning in robotic applications","abstract":"In this paper, we propose a robot oriented knowledge management system based on the use of the Prolog language. Our framework hinges on a special organisation of knowledge base that enables: 1. its efficient population from natural language texts using semi-automated procedures based on Large Language Models, 2. the bumpless generation of temporal parallel plans for multi-robot systems through a sequence of transformations, 3. the automated translation of the plan into an executable formalism (the behaviour trees). The framework is supported by a set of open source tools and is shown on a realistic application.","sentences":["In this paper, we propose a robot oriented knowledge management system based on the use of the Prolog language.","Our framework hinges on a special organisation of knowledge base that enables: 1. its efficient population from natural language texts using semi-automated procedures based on Large Language Models, 2.","the bumpless generation of temporal parallel plans for multi-robot systems through a sequence of transformations, 3.","the automated translation of the plan into an executable formalism (the behaviour trees).","The framework is supported by a set of open source tools and is shown on a realistic application."],"url":"http://arxiv.org/abs/2309.15049v1"}
{"created":"2023-09-26 16:25:57","title":"Class Incremental Learning via Likelihood Ratio Based Task Prediction","abstract":"Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal because additional information (e.g., the replay data and the learned tasks) available in CIL can be exploited to design a better and principled method for task-id prediction. We call the new method TPLR (Task-id Prediction based on Likelihood Ratio}). TPLR markedly outperforms strong CIL baselines.","sentences":["Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially.","Each task consists of a set of unique classes.","The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample.","Predicting the task-id for each test sample is a challenging problem.","An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting.","The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier.","The OOD detector can perform both within-task (in-distribution (IND))","class prediction and OOD detection.","The OOD detection capability is the key for task-id prediction during inference for each test sample.","However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal because additional information (e.g., the replay data and the learned tasks) available in CIL can be exploited to design a better and principled method for task-id prediction.","We call the new method TPLR (Task-id Prediction based on Likelihood Ratio}).","TPLR markedly outperforms strong CIL baselines."],"url":"http://arxiv.org/abs/2309.15048v1"}
{"created":"2023-09-26 16:24:57","title":"Modeling Evacuee Behavior for Robot-Guided Emergency Evacuation","abstract":"This paper considers the problem of developing suitable behavior models of human evacuees during a robot-guided emergency evacuation. We describe our recent research developing behavior models of evacuees and potential future uses of these models. This paper considers how behavior models can contribute to the development and design of emergency evacuation simulations in order to improve social navigation during an evacuation.","sentences":["This paper considers the problem of developing suitable behavior models of human evacuees during a robot-guided emergency evacuation.","We describe our recent research developing behavior models of evacuees and potential future uses of these models.","This paper considers how behavior models can contribute to the development and design of emergency evacuation simulations in order to improve social navigation during an evacuation."],"url":"http://arxiv.org/abs/2309.15045v1"}
{"created":"2023-09-26 16:16:05","title":"Zero-Energy-Device for 6G: First Real-Time Backscatter Communication thanks to the Detection of Pilots from an Ambient Commercial Cellular Network","abstract":"Ambient backscatter communication technology (AmBC) and a novel device category called zero-energy devices (ZED) have recently emerged as potential components for the forthcoming 6th generation (6G) networks. A ZED communicates with a smartphone without emitting additional radio waves, by backscattering ambient waves from base stations. Thanks to its very low consumption, a ZED powers itself by harvesting ambient light energy. However, the time variations of data traffic in cellular networks prevents AmBC to work properly. Recent works have demonstrated experimentally that a backscatter device could be detected by listening only ambient pilot signals (which are steady) instead of the whole ambient signal (which is bursty) of 4G. However, these experiments were run with a 4G base station emulator and a bulky energy greedy backscatter device. In this paper, for the first time, we demonstrate real-time AmBC on the field, with Orange commercial 4G network as ambient source and Orange Zero-Energy Device.","sentences":["Ambient backscatter communication technology (AmBC) and a novel device category called zero-energy devices (ZED) have recently emerged as potential components for the forthcoming 6th generation (6G) networks.","A ZED communicates with a smartphone without emitting additional radio waves, by backscattering ambient waves from base stations.","Thanks to its very low consumption, a ZED powers itself by harvesting ambient light energy.","However, the time variations of data traffic in cellular networks prevents AmBC to work properly.","Recent works have demonstrated experimentally that a backscatter device could be detected by listening only ambient pilot signals (which are steady) instead of the whole ambient signal (which is bursty) of 4G.","However, these experiments were run with a 4G base station emulator and a bulky energy greedy backscatter device.","In this paper, for the first time, we demonstrate real-time AmBC on the field, with Orange commercial 4G network as ambient source and Orange Zero-Energy Device."],"url":"http://arxiv.org/abs/2309.15040v1"}
{"created":"2023-09-26 16:15:54","title":"Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data","abstract":"Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.   This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combines machine learning and survival analysis since these approaches are less computationally heavy, can be combined into an ensemble (the Survival Ensemble), and can be reproduced in most medical institutions.   We test the Survival Ensemble in some studies. Firstly, we obtain a significant difference between values of the primary metric (Average Precision) with 22.8% (ROC AUC 83.7%, F1 17.8%) for the Survival Ensemble versus 15.1% (ROC AUC 84.9%, F1 21.4%) for the Baseline. Secondly, the performance of the Survival Ensemble is also confirmed during the ablation study. Thirdly, our method exceeds age baselines by a significant margin. Fourthly, in the blind retrospective out-of-time experiment, the proposed method is reliable in cancer patient detection (9 out of 100 selected). Such results exceed the estimates of medical screenings, e.g., the best Number Needed to Screen (9 out of 1000 screenings).","sentences":["Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale.","Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data.","These aspects affect the mass implementation of cancer screening methods.","For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.   ","This paper presents a novel method for mass cancer risk prediction using EHR data.","Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR.","We formulate the problem as a binary classification.","This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer).","As a baseline, we implement a solution based on a recurrent neural network (RNN).","We propose a method that combines machine learning and survival analysis since these approaches are less computationally heavy, can be combined into an ensemble (the Survival Ensemble), and can be reproduced in most medical institutions.   ","We test the Survival Ensemble in some studies.","Firstly, we obtain a significant difference between values of the primary metric (Average Precision) with 22.8% (ROC AUC 83.7%, F1 17.8%) for the Survival Ensemble versus 15.1% (ROC AUC 84.9%, F1 21.4%) for the Baseline.","Secondly, the performance of the Survival Ensemble is also confirmed during the ablation study.","Thirdly, our method exceeds age baselines by a significant margin.","Fourthly, in the blind retrospective out-of-time experiment, the proposed method is reliable in cancer patient detection (9 out of 100 selected).","Such results exceed the estimates of medical screenings, e.g., the best Number Needed to Screen (9 out of 1000 screenings)."],"url":"http://arxiv.org/abs/2309.15039v1"}
{"created":"2023-09-26 16:12:57","title":"HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning","abstract":"Online continual learning (OCL) aims to continuously learn new data from a single pass over the online data stream. It generally suffers from the catastrophic forgetting issue. Existing replay-based methods effectively alleviate this issue by replaying part of old data in a proxy-based or contrastive-based replay manner. In this paper, we conduct a comprehensive analysis of these two replay manners and find they can be complementary. Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR), which replaces anchor-to-sample pairs with anchor-to-proxy pairs in the contrastive-based loss to alleviate the phenomenon of forgetting. Based on PCR, we further develop a more advanced method named holistic proxy-based contrastive replay (HPCR), which consists of three components. The contrastive component conditionally incorporates anchor-to-sample pairs to PCR, learning more fine-grained semantic information with a large training batch. The second is a temperature component that decouples the temperature coefficient into two parts based on their impacts on the gradient and sets different values for them to learn more novel knowledge. The third is a distillation component that constrains the learning process to keep more historical knowledge. Experiments on four datasets consistently demonstrate the superiority of HPCR over various state-of-the-art methods.","sentences":["Online continual learning (OCL) aims to continuously learn new data from a single pass over the online data stream.","It generally suffers from the catastrophic forgetting issue.","Existing replay-based methods effectively alleviate this issue by replaying part of old data in a proxy-based or contrastive-based replay manner.","In this paper, we conduct a comprehensive analysis of these two replay manners and find they can be complementary.","Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR), which replaces anchor-to-sample pairs with anchor-to-proxy pairs in the contrastive-based loss to alleviate the phenomenon of forgetting.","Based on PCR, we further develop a more advanced method named holistic proxy-based contrastive replay (HPCR), which consists of three components.","The contrastive component conditionally incorporates anchor-to-sample pairs to PCR, learning more fine-grained semantic information with a large training batch.","The second is a temperature component that decouples the temperature coefficient into two parts based on their impacts on the gradient and sets different values for them to learn more novel knowledge.","The third is a distillation component that constrains the learning process to keep more historical knowledge.","Experiments on four datasets consistently demonstrate the superiority of HPCR over various state-of-the-art methods."],"url":"http://arxiv.org/abs/2309.15038v1"}
{"created":"2023-09-26 16:07:38","title":"STAR-RIS Assisted Full-Duplex Communication Networks","abstract":"Different from conventional reconfigurable intelligent surfaces (RIS), a recent innovation called simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged, aimed at achieving complete 360-degree coverage in communication networks. Additionally, fullduplex (FD) technology is recognized as a potent approach for enhancing spectral efficiency by enabling simultaneous transmission and reception within the same time and frequency resources. In this study, we investigate the performance of a STAR-RIS-assisted FD communication system. The STAR-RIS is strategically placed at the cell-edge to facilitate communication for users located in this challenging region, while cell-center users can communicate directly with the FD base station (BS). We employ a non-orthogonal multiple access (NOMA) pairing scheme and account for system impairments, such as self-interference at the BS and imperfect successive interference cancellation (SIC). We derive closed-form expressions for the ergodic rates in both the up-link and down-link communications and extend our analysis to bidirectional communication between cell-center and cell-edge users. Furthermore, we formulate an optimization problem aimed at maximizing the ergodic sum-rate. This optimization involves adjusting the amplitudes and phase-shifts of the STAR-RIS elements and allocating total transmit power efficiently. To gain deeper insights into the achievable rates of STAR-RIS-aided FD systems, we explore the impact of various system parameters through numerical results.","sentences":["Different from conventional reconfigurable intelligent surfaces (RIS), a recent innovation called simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged, aimed at achieving complete 360-degree coverage in communication networks.","Additionally, fullduplex (FD) technology is recognized as a potent approach for enhancing spectral efficiency by enabling simultaneous transmission and reception within the same time and frequency resources.","In this study, we investigate the performance of a STAR-RIS-assisted FD communication system.","The STAR-RIS is strategically placed at the cell-edge to facilitate communication for users located in this challenging region, while cell-center users can communicate directly with the FD base station (BS).","We employ a non-orthogonal multiple access (NOMA) pairing scheme and account for system impairments, such as self-interference at the BS and imperfect successive interference cancellation (SIC).","We derive closed-form expressions for the ergodic rates in both the up-link and down-link communications and extend our analysis to bidirectional communication between cell-center and cell-edge users.","Furthermore, we formulate an optimization problem aimed at maximizing the ergodic sum-rate.","This optimization involves adjusting the amplitudes and phase-shifts of the STAR-RIS elements and allocating total transmit power efficiently.","To gain deeper insights into the achievable rates of STAR-RIS-aided FD systems, we explore the impact of various system parameters through numerical results."],"url":"http://arxiv.org/abs/2309.15037v1"}
{"created":"2023-09-26 16:01:15","title":"Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic Relevance for Canine Cutaneous Mast Cell Tumors","abstract":"Variation in nuclear size and shape is an important criterion of malignancy for many tumor types; however, categorical estimates by pathologists have poor reproducibility. Measurements of nuclear characteristics (morphometry) can improve reproducibility, but manual methods are time consuming. In this study, we evaluated fully automated morphometry using a deep learning-based algorithm in 96 canine cutaneous mast cell tumors with information on patient survival. Algorithmic morphometry was compared with karyomegaly estimates by 11 pathologists, manual nuclear morphometry of 12 cells by 9 pathologists, and the mitotic count as a benchmark. The prognostic value of automated morphometry was high with an area under the ROC curve regarding the tumor-specific survival of 0.943 (95% CI: 0.889 - 0.996) for the standard deviation (SD) of nuclear area, which was higher than manual morphometry of all pathologists combined (0.868, 95% CI: 0.737 - 0.991) and the mitotic count (0.885, 95% CI: 0.765 - 1.00). At the proposed thresholds, the hazard ratio for algorithmic morphometry (SD of nuclear area $\\geq 9.0 \\mu m^2$) was 18.3 (95% CI: 5.0 - 67.1), for manual morphometry (SD of nuclear area $\\geq 10.9 \\mu m^2$) 9.0 (95% CI: 6.0 - 13.4), for karyomegaly estimates 7.6 (95% CI: 5.7 - 10.1), and for the mitotic count 30.5 (95% CI: 7.8 - 118.0). Inter-rater reproducibility for karyomegaly estimates was fair ($\\kappa$ = 0.226) with highly variable sensitivity/specificity values for the individual pathologists. Reproducibility for manual morphometry (SD of nuclear area) was good (ICC = 0.654). This study supports the use of algorithmic morphometry as a prognostic test to overcome the limitations of estimates and manual measurements.","sentences":["Variation in nuclear size and shape is an important criterion of malignancy for many tumor types; however, categorical estimates by pathologists have poor reproducibility.","Measurements of nuclear characteristics (morphometry) can improve reproducibility, but manual methods are time consuming.","In this study, we evaluated fully automated morphometry using a deep learning-based algorithm in 96 canine cutaneous mast cell tumors with information on patient survival.","Algorithmic morphometry was compared with karyomegaly estimates by 11 pathologists, manual nuclear morphometry of 12 cells by 9 pathologists, and the mitotic count as a benchmark.","The prognostic value of automated morphometry was high with an area under the ROC curve regarding the tumor-specific survival of 0.943 (95% CI: 0.889 - 0.996) for the standard deviation (SD) of nuclear area, which was higher than manual morphometry of all pathologists combined (0.868, 95% CI: 0.737 - 0.991) and the mitotic count (0.885, 95% CI: 0.765 - 1.00).","At the proposed thresholds, the hazard ratio for algorithmic morphometry (SD of nuclear area $\\geq 9.0 \\mu m^2$) was 18.3 (95% CI: 5.0 - 67.1), for manual morphometry (SD of nuclear area $\\geq 10.9 \\mu m^2$) 9.0 (95% CI: 6.0 - 13.4), for karyomegaly estimates 7.6 (95% CI: 5.7 - 10.1), and for the mitotic count 30.5 (95% CI: 7.8 - 118.0).","Inter-rater reproducibility for karyomegaly estimates was fair ($\\kappa$ = 0.226) with highly variable sensitivity/specificity values for the individual pathologists.","Reproducibility for manual morphometry (SD of nuclear area) was good (ICC = 0.654).","This study supports the use of algorithmic morphometry as a prognostic test to overcome the limitations of estimates and manual measurements."],"url":"http://arxiv.org/abs/2309.15031v1"}
{"created":"2023-09-26 16:00:05","title":"Quadratic Detection in Noncoherent Massive SIMO Systems over Correlated Channels","abstract":"With the goal of enabling ultrareliable and low-latency wireless communications for industrial internet of things (IIoT), this paper studies the use of energy-based modulations in noncoherent massive single input multiple output (SIMO) systems. We consider a one-shot communication over a channel with correlated Rayleigh fading and colored Gaussian noise. We first provide a theoretical analysis on the limitations of non-negative pulse-amplitude modulation (PAM) in systems of this kind, based on maximum likelihood detection. The existence of a fundamental error floor at high signal-to-noise ratio (SNR) regimes is proved for constellations with more than two energy levels, when no (statistical) channel state information is available at the transmitter. In the main body of the paper, we present a design framework for quadratic detectors that generalizes the widely-used energy detector, to better exploit the statistical knowledge of the channel. This allows us to design receivers optimized according to information-theoretic criteria that exhibit lower error rates at moderate and high SNR. We subsequently derive an analytic approximation for the error probability of a general class of quadratic detectors in the large array regime. Finally, we introduce an improved reception scheme based on the combination of quadratic detectors and assess its capabilities numerically.","sentences":["With the goal of enabling ultrareliable and low-latency wireless communications for industrial internet of things (IIoT), this paper studies the use of energy-based modulations in noncoherent massive single input multiple output (SIMO) systems.","We consider a one-shot communication over a channel with correlated Rayleigh fading and colored Gaussian noise.","We first provide a theoretical analysis on the limitations of non-negative pulse-amplitude modulation (PAM) in systems of this kind, based on maximum likelihood detection.","The existence of a fundamental error floor at high signal-to-noise ratio (SNR) regimes is proved for constellations with more than two energy levels, when no (statistical) channel state information is available at the transmitter.","In the main body of the paper, we present a design framework for quadratic detectors that generalizes the widely-used energy detector, to better exploit the statistical knowledge of the channel.","This allows us to design receivers optimized according to information-theoretic criteria that exhibit lower error rates at moderate and high SNR.","We subsequently derive an analytic approximation for the error probability of a general class of quadratic detectors in the large array regime.","Finally, we introduce an improved reception scheme based on the combination of quadratic detectors and assess its capabilities numerically."],"url":"http://arxiv.org/abs/2309.15030v1"}
{"created":"2023-09-26 15:57:57","title":"Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding","abstract":"Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS greatly improves the preferability of generated text compared to the standard practice of using only the PPO policy. Our results demonstrate the promise of search algorithms even on top of the aligned language models from PPO, and the under-explored benefit of the value network.","sentences":["Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO).","In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top.","The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network.","More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation.","Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test.","Evaluation on four text generation tasks demonstrate that PPO-MCTS greatly improves the preferability of generated text compared to the standard practice of using only the PPO policy.","Our results demonstrate the promise of search algorithms even on top of the aligned language models from PPO, and the under-explored benefit of the value network."],"url":"http://arxiv.org/abs/2309.15028v1"}
{"created":"2023-09-26 15:56:14","title":"Instance complexity of Boolean functions","abstract":"In the area of query complexity of Boolean functions, the most widely studied cost measure of an algorithm is the worst-case number of queries made by it on an input. Motivated by the most natural cost measure studied in online algorithms, the competitive ratio, we consider a different cost measure for query algorithms for Boolean functions that captures the ratio of the cost of the algorithm and the cost of an optimal algorithm that knows the input in advance. The cost of an algorithm is its largest cost over all inputs. Grossman, Komargodski and Naor [ITCS'20] introduced this measure for Boolean functions, and dubbed it instance complexity. Grossman et al. showed, among other results, that monotone Boolean functions with instance complexity 1 are precisely those that depend on one or two variables.   We complement the above-mentioned result of Grossman et al. by completely characterizing the instance complexity of symmetric Boolean functions. As a corollary we conclude that the only symmetric Boolean functions with instance complexity 1 are the Parity function and its complement. We also study the instance complexity of some graph properties like Connectivity and k-clique containment.   In all the Boolean functions we study above, and those studied by Grossman et al., the instance complexity turns out to be the ratio of query complexity to minimum certificate complexity. It is a natural question to ask if this is the correct bound for all Boolean functions. We show a negative answer in a very strong sense, by analyzing the instance complexity of the Greater-Than and Odd-Max-Bit functions. We show that the above-mentioned ratio is linear in the input size for both of these functions, while we exhibit algorithms for which the instance complexity is a constant.","sentences":["In the area of query complexity of Boolean functions, the most widely studied cost measure of an algorithm is the worst-case number of queries made by it on an input.","Motivated by the most natural cost measure studied in online algorithms, the competitive ratio, we consider a different cost measure for query algorithms for Boolean functions that captures the ratio of the cost of the algorithm and the cost of an optimal algorithm that knows the input in advance.","The cost of an algorithm is its largest cost over all inputs.","Grossman, Komargodski and Naor","[ITCS'20] introduced this measure for Boolean functions, and dubbed it instance complexity.","Grossman et al. showed, among other results, that monotone Boolean functions with instance complexity 1 are precisely those that depend on one or two variables.   ","We complement the above-mentioned result of Grossman et al.","by completely characterizing the instance complexity of symmetric Boolean functions.","As a corollary we conclude that the only symmetric Boolean functions with instance complexity 1 are the Parity function and its complement.","We also study the instance complexity of some graph properties like Connectivity and k-clique containment.   ","In all the Boolean functions we study above, and those studied by Grossman et al., the instance complexity turns out to be the ratio of query complexity to minimum certificate complexity.","It is a natural question to ask if this is the correct bound for all Boolean functions.","We show a negative answer in a very strong sense, by analyzing the instance complexity of the Greater-Than and Odd-Max-Bit functions.","We show that the above-mentioned ratio is linear in the input size for both of these functions, while we exhibit algorithms for which the instance complexity is a constant."],"url":"http://arxiv.org/abs/2309.15026v1"}
{"created":"2023-09-26 15:49:23","title":"Large Language Model Alignment: A Survey","abstract":"Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.   This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.   Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.","sentences":["Recent years have witnessed remarkable progress made in large language models (LLMs).","Such advancements, while garnering significant attention, have concurrently elicited various concerns.","The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental.","Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.   ","This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain.","Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment.","We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks.","To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies.","After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.   ","Our aspiration for this survey extends beyond merely spurring research interests in this realm.","We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs."],"url":"http://arxiv.org/abs/2309.15025v1"}
{"created":"2023-09-26 15:46:06","title":"Synthia's Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio","abstract":"Despite significant advancements in deep learning for vision and natural language, unsupervised domain adaptation in audio remains relatively unexplored. We, in part, attribute this to the lack of an appropriate benchmark dataset. To address this gap, we present Synthia's melody, a novel audio data generation framework capable of simulating an infinite variety of 4-second melodies with user-specified confounding structures characterised by musical keys, timbre, and loudness. Unlike existing datasets collected under observational settings, Synthia's melody is free of unobserved biases, ensuring the reproducibility and comparability of experiments. To showcase its utility, we generate two types of distribution shifts-domain shift and sample selection bias-and evaluate the performance of acoustic deep learning models under these shifts. Our evaluations reveal that Synthia's melody provides a robust testbed for examining the susceptibility of these models to varying levels of distribution shift.","sentences":["Despite significant advancements in deep learning for vision and natural language, unsupervised domain adaptation in audio remains relatively unexplored.","We, in part, attribute this to the lack of an appropriate benchmark dataset.","To address this gap, we present Synthia's melody, a novel audio data generation framework capable of simulating an infinite variety of 4-second melodies with user-specified confounding structures characterised by musical keys, timbre, and loudness.","Unlike existing datasets collected under observational settings, Synthia's melody is free of unobserved biases, ensuring the reproducibility and comparability of experiments.","To showcase its utility, we generate two types of distribution shifts-domain shift and sample selection bias-and evaluate the performance of acoustic deep learning models under these shifts.","Our evaluations reveal that Synthia's melody provides a robust testbed for examining the susceptibility of these models to varying levels of distribution shift."],"url":"http://arxiv.org/abs/2309.15024v1"}
{"created":"2023-09-26 15:38:52","title":"IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging","abstract":"Multi-frame high dynamic range (HDR) imaging aims to reconstruct ghost-free images with photo-realistic details from content-complementary but spatially misaligned low dynamic range (LDR) images. Existing HDR algorithms are prone to producing ghosting artifacts as their methods fail to capture long-range dependencies between LDR frames with large motion in dynamic scenes. To address this issue, we propose a novel image fusion transformer, referred to as IFT, which presents a fast global patch searching (FGPS) module followed by a self-cross fusion module (SCF) for ghost-free HDR imaging. The FGPS searches the patches from supporting frames that have the closest dependency to each patch of the reference frame for long-range dependency modeling, while the SCF conducts intra-frame and inter-frame feature fusion on the patches obtained by the FGPS with linear complexity to input resolution. By matching similar patches between frames, objects with large motion ranges in dynamic scenes can be aligned, which can effectively alleviate the generation of artifacts. In addition, the proposed FGPS and SCF can be integrated into various deep HDR methods as efficient plug-in modules. Extensive experiments on multiple benchmarks show that our method achieves state-of-the-art performance both quantitatively and qualitatively.","sentences":["Multi-frame high dynamic range (HDR) imaging aims to reconstruct ghost-free images with photo-realistic details from content-complementary but spatially misaligned low dynamic range (LDR) images.","Existing HDR algorithms are prone to producing ghosting artifacts as their methods fail to capture long-range dependencies between LDR frames with large motion in dynamic scenes.","To address this issue, we propose a novel image fusion transformer, referred to as IFT, which presents a fast global patch searching (FGPS) module followed by a self-cross fusion module (SCF) for ghost-free HDR imaging.","The FGPS searches the patches from supporting frames that have the closest dependency to each patch of the reference frame for long-range dependency modeling, while the SCF conducts intra-frame and inter-frame feature fusion on the patches obtained by the FGPS with linear complexity to input resolution.","By matching similar patches between frames, objects with large motion ranges in dynamic scenes can be aligned, which can effectively alleviate the generation of artifacts.","In addition, the proposed FGPS and SCF can be integrated into various deep HDR methods as efficient plug-in modules.","Extensive experiments on multiple benchmarks show that our method achieves state-of-the-art performance both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2309.15019v1"}
{"created":"2023-09-26 15:38:26","title":"Unidirectional brain-computer interface: Artificial neural network encoding natural images to fMRI response in the visual cortex","abstract":"While significant advancements in artificial intelligence (AI) have catalyzed progress across various domains, its full potential in understanding visual perception remains underexplored. We propose an artificial neural network dubbed VISION, an acronym for \"Visual Interface System for Imaging Output of Neural activity,\" to mimic the human brain and show how it can foster neuroscientific inquiries. Using visual and contextual inputs, this multimodal model predicts the brain's functional magnetic resonance imaging (fMRI) scan response to natural images. VISION successfully predicts human hemodynamic responses as fMRI voxel values to visual inputs with an accuracy exceeding state-of-the-art performance by 45%. We further probe the trained networks to reveal representational biases in different visual areas, generate experimentally testable hypotheses, and formulate an interpretable metric to associate these hypotheses with cortical functions. With both a model and evaluation metric, the cost and time burdens associated with designing and implementing functional analysis on the visual cortex could be reduced. Our work suggests that the evolution of computational models may shed light on our fundamental understanding of the visual cortex and provide a viable approach toward reliable brain-machine interfaces.","sentences":["While significant advancements in artificial intelligence (AI) have catalyzed progress across various domains, its full potential in understanding visual perception remains underexplored.","We propose an artificial neural network dubbed VISION, an acronym for \"Visual Interface System for Imaging Output of Neural activity,\" to mimic the human brain and show how it can foster neuroscientific inquiries.","Using visual and contextual inputs, this multimodal model predicts the brain's functional magnetic resonance imaging (fMRI) scan response to natural images.","VISION successfully predicts human hemodynamic responses as fMRI voxel values to visual inputs with an accuracy exceeding state-of-the-art performance by 45%.","We further probe the trained networks to reveal representational biases in different visual areas, generate experimentally testable hypotheses, and formulate an interpretable metric to associate these hypotheses with cortical functions.","With both a model and evaluation metric, the cost and time burdens associated with designing and implementing functional analysis on the visual cortex could be reduced.","Our work suggests that the evolution of computational models may shed light on our fundamental understanding of the visual cortex and provide a viable approach toward reliable brain-machine interfaces."],"url":"http://arxiv.org/abs/2309.15018v1"}
{"created":"2023-09-26 15:36:55","title":"Studying the association between Gitcoin's issues and resolving outcomes","abstract":"The development of open-source software (OSS) projects usually have been driven through collaborations among contributors and strongly relies on volunteering. Thus, allocating software practitioners (e.g., contributors) to a particular task is non-trivial and draws attention away from the development. Therefore, a number of bug bounty platforms have emerged to address this problem through bounty rewards. Especially, Gitcoin, a new bounty platform, introduces a bounty reward mechanism that allows individual issue owners (backers) to define a reward value using cryptocurrencies rather than using crowdfunding mechanisms. Although a number of studies have investigated the phenomenon on bounty platforms, those rely on different bounty reward systems. Our study thus investigates the association between the Gitcoin bounties and their outcomes (i.e., success and non-success). We empirically study over 4,000 issues with Gitcoin bounties using statistical analysis and machine learning techniques. We also conducted a comparative study with the Bountysource platform to gain insights into the usage of both platforms. Our study highlights the importance of factors such as the length of the project, issue description, type of bounty issue, and the bounty value, which are found to be highly correlated with the outcome of bounty issues. These findings can provide useful guidance to practitioners.","sentences":["The development of open-source software (OSS) projects usually have been driven through collaborations among contributors and strongly relies on volunteering.","Thus, allocating software practitioners (e.g., contributors) to a particular task is non-trivial and draws attention away from the development.","Therefore, a number of bug bounty platforms have emerged to address this problem through bounty rewards.","Especially, Gitcoin, a new bounty platform, introduces a bounty reward mechanism that allows individual issue owners (backers) to define a reward value using cryptocurrencies rather than using crowdfunding mechanisms.","Although a number of studies have investigated the phenomenon on bounty platforms, those rely on different bounty reward systems.","Our study thus investigates the association between the Gitcoin bounties and their outcomes (i.e., success and non-success).","We empirically study over 4,000 issues with Gitcoin bounties using statistical analysis and machine learning techniques.","We also conducted a comparative study with the Bountysource platform to gain insights into the usage of both platforms.","Our study highlights the importance of factors such as the length of the project, issue description, type of bounty issue, and the bounty value, which are found to be highly correlated with the outcome of bounty issues.","These findings can provide useful guidance to practitioners."],"url":"http://arxiv.org/abs/2309.15017v1"}
{"created":"2023-09-26 15:36:29","title":"Question-Answering Approach to Evaluate Legal Summaries","abstract":"Traditional evaluation metrics like ROUGE compare lexical overlap between the reference and generated summaries without taking argumentative structure into account, which is important for legal summaries. In this paper, we propose a novel legal summarization evaluation framework that utilizes GPT-4 to generate a set of question-answer pairs that cover main points and information in the reference summary. GPT-4 is then used to generate answers based on the generated summary for the questions from the reference summary. Finally, GPT-4 grades the answers from the reference summary and the generated summary. We examined the correlation between GPT-4 grading with human grading. The results suggest that this question-answering approach with GPT-4 can be a useful tool for gauging the quality of the summary.","sentences":["Traditional evaluation metrics like ROUGE compare lexical overlap between the reference and generated summaries without taking argumentative structure into account, which is important for legal summaries.","In this paper, we propose a novel legal summarization evaluation framework that utilizes GPT-4 to generate a set of question-answer pairs that cover main points and information in the reference summary.","GPT-4 is then used to generate answers based on the generated summary for the questions from the reference summary.","Finally, GPT-4 grades the answers from the reference summary and the generated summary.","We examined the correlation between GPT-4 grading with human grading.","The results suggest that this question-answering approach with GPT-4 can be a useful tool for gauging the quality of the summary."],"url":"http://arxiv.org/abs/2309.15016v1"}
{"created":"2023-09-26 15:32:09","title":"Updated Corpora and Benchmarks for Long-Form Speech Recognition","abstract":"The vast majority of ASR research uses corpora in which both the training and test data have been pre-segmented into utterances. In most real-word ASR use-cases, however, test audio is not segmented, leading to a mismatch between inference-time conditions and models trained on segmented utterances. In this paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and VoxPopuli-en - with updated transcription and alignments to enable their use for long-form ASR research. We use these reconstituted corpora to study the train-test mismatch problem for transducers and attention-based encoder-decoders (AEDs), confirming that AEDs are more susceptible to this issue. Finally, we benchmark a simple long-form training for these models, showing its efficacy for model robustness under this domain shift.","sentences":["The vast majority of ASR research uses corpora in which both the training and test data have been pre-segmented into utterances.","In most real-word ASR use-cases, however, test audio is not segmented, leading to a mismatch between inference-time conditions and models trained on segmented utterances.","In this paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and VoxPopuli-en - with updated transcription and alignments to enable their use for long-form ASR research.","We use these reconstituted corpora to study the train-test mismatch problem for transducers and attention-based encoder-decoders (AEDs), confirming that AEDs are more susceptible to this issue.","Finally, we benchmark a simple long-form training for these models, showing its efficacy for model robustness under this domain shift."],"url":"http://arxiv.org/abs/2309.15013v1"}
{"created":"2023-09-26 15:18:44","title":"Automating question generation from educational text","abstract":"The use of question-based activities (QBAs) is wide-spread in education, traditionally forming an integral part of the learning and assessment process. In this paper, we design and evaluate an automated question generation tool for formative and summative assessment in schools. We present an expert survey of one hundred and four teachers, demonstrating the need for automated generation of QBAs, as a tool that can significantly reduce the workload of teachers and facilitate personalized learning experiences. Leveraging the recent advancements in generative AI, we then present a modular framework employing transformer based language models for automatic generation of multiple-choice questions (MCQs) from textual content. The presented solution, with distinct modules for question generation, correct answer prediction, and distractor formulation, enables us to evaluate different language models and generation techniques. Finally, we perform an extensive quantitative and qualitative evaluation, demonstrating trade-offs in the use of different techniques and models.","sentences":["The use of question-based activities (QBAs) is wide-spread in education, traditionally forming an integral part of the learning and assessment process.","In this paper, we design and evaluate an automated question generation tool for formative and summative assessment in schools.","We present an expert survey of one hundred and four teachers, demonstrating the need for automated generation of QBAs, as a tool that can significantly reduce the workload of teachers and facilitate personalized learning experiences.","Leveraging the recent advancements in generative AI, we then present a modular framework employing transformer based language models for automatic generation of multiple-choice questions (MCQs) from textual content.","The presented solution, with distinct modules for question generation, correct answer prediction, and distractor formulation, enables us to evaluate different language models and generation techniques.","Finally, we perform an extensive quantitative and qualitative evaluation, demonstrating trade-offs in the use of different techniques and models."],"url":"http://arxiv.org/abs/2309.15004v1"}
{"created":"2023-09-26 15:13:09","title":"Object-Centric Open-Vocabulary Image-Retrieval with Aggregated Features","abstract":"The task of open-vocabulary object-centric image retrieval involves the retrieval of images containing a specified object of interest, delineated by an open-set text query. As working on large image datasets becomes standard, solving this task efficiently has gained significant practical importance. Applications include targeted performance analysis of retrieved images using ad-hoc queries and hard example mining during training. Recent advancements in contrastive-based open vocabulary systems have yielded remarkable breakthroughs, facilitating large-scale open vocabulary image retrieval. However, these approaches use a single global embedding per image, thereby constraining the system's ability to retrieve images containing relatively small object instances. Alternatively, incorporating local embeddings from detection pipelines faces scalability challenges, making it unsuitable for retrieval from large databases.   In this work, we present a simple yet effective approach to object-centric open-vocabulary image retrieval. Our approach aggregates dense embeddings extracted from CLIP into a compact representation, essentially combining the scalability of image retrieval pipelines with the object identification capabilities of dense detection methods. We show the effectiveness of our scheme to the task by achieving significantly better results than global feature approaches on three datasets, increasing accuracy by up to 15 mAP points. We further integrate our scheme into a large scale retrieval framework and demonstrate our method's advantages in terms of scalability and interpretability.","sentences":["The task of open-vocabulary object-centric image retrieval involves the retrieval of images containing a specified object of interest, delineated by an open-set text query.","As working on large image datasets becomes standard, solving this task efficiently has gained significant practical importance.","Applications include targeted performance analysis of retrieved images using ad-hoc queries and hard example mining during training.","Recent advancements in contrastive-based open vocabulary systems have yielded remarkable breakthroughs, facilitating large-scale open vocabulary image retrieval.","However, these approaches use a single global embedding per image, thereby constraining the system's ability to retrieve images containing relatively small object instances.","Alternatively, incorporating local embeddings from detection pipelines faces scalability challenges, making it unsuitable for retrieval from large databases.   ","In this work, we present a simple yet effective approach to object-centric open-vocabulary image retrieval.","Our approach aggregates dense embeddings extracted from CLIP into a compact representation, essentially combining the scalability of image retrieval pipelines with the object identification capabilities of dense detection methods.","We show the effectiveness of our scheme to the task by achieving significantly better results than global feature approaches on three datasets, increasing accuracy by up to 15 mAP points.","We further integrate our scheme into a large scale retrieval framework and demonstrate our method's advantages in terms of scalability and interpretability."],"url":"http://arxiv.org/abs/2309.14999v1"}
{"created":"2023-09-26 15:12:55","title":"An Ensemble Model for Distorted Images in Real Scenarios","abstract":"Image acquisition conditions and environments can significantly affect high-level tasks in computer vision, and the performance of most computer vision algorithms will be limited when trained on distortion-free datasets. Even with updates in hardware such as sensors and deep learning methods, it will still not work in the face of variable conditions in real-world applications. In this paper, we apply the object detector YOLOv7 to detect distorted images from the dataset CDCOCO. Through carefully designed optimizations including data enhancement, detection box ensemble, denoiser ensemble, super-resolution models, and transfer learning, our model achieves excellent performance on the CDCOCO test set. Our denoising detection model can denoise and repair distorted images, making the model useful in a variety of real-world scenarios and environments.","sentences":["Image acquisition conditions and environments can significantly affect high-level tasks in computer vision, and the performance of most computer vision algorithms will be limited when trained on distortion-free datasets.","Even with updates in hardware such as sensors and deep learning methods, it will still not work in the face of variable conditions in real-world applications.","In this paper, we apply the object detector YOLOv7 to detect distorted images from the dataset CDCOCO.","Through carefully designed optimizations including data enhancement, detection box ensemble, denoiser ensemble, super-resolution models, and transfer learning, our model achieves excellent performance on the CDCOCO test set.","Our denoising detection model can denoise and repair distorted images, making the model useful in a variety of real-world scenarios and environments."],"url":"http://arxiv.org/abs/2309.14998v1"}
{"created":"2023-09-26 15:12:29","title":"IAIFNet: An Illumination-Aware Infrared and Visible Image Fusion Network","abstract":"Infrared and visible image fusion (IVIF) is used to generate fusion images with comprehensive features of both images, which is beneficial for downstream vision tasks. However, current methods rarely consider the illumination condition in low-light environments, and the targets in the fused images are often not prominent. To address the above issues, we propose an Illumination-Aware Infrared and Visible Image Fusion Network, named as IAIFNet. In our framework, an illumination enhancement network first estimates the incident illumination maps of input images. Afterwards, with the help of proposed adaptive differential fusion module (ADFM) and salient target aware module (STAM), an image fusion network effectively integrates the salient features of the illumination-enhanced infrared and visible images into a fusion image of high visual quality. Extensive experimental results verify that our method outperforms five state-of-the-art methods of fusing infrared and visible images.","sentences":["Infrared and visible image fusion (IVIF) is used to generate fusion images with comprehensive features of both images, which is beneficial for downstream vision tasks.","However, current methods rarely consider the illumination condition in low-light environments, and the targets in the fused images are often not prominent.","To address the above issues, we propose an Illumination-Aware Infrared and Visible Image Fusion Network, named as IAIFNet.","In our framework, an illumination enhancement network first estimates the incident illumination maps of input images.","Afterwards, with the help of proposed adaptive differential fusion module (ADFM) and salient target aware module (STAM), an image fusion network effectively integrates the salient features of the illumination-enhanced infrared and visible images into a fusion image of high visual quality.","Extensive experimental results verify that our method outperforms five state-of-the-art methods of fusing infrared and visible images."],"url":"http://arxiv.org/abs/2309.14997v1"}
{"created":"2023-09-26 15:11:33","title":"Implementation-Oblivious Transparent Checkpoint-Restart for MPI","abstract":"This work presents experience with traditional use cases of checkpointing on a novel platform. A single codebase (MANA) transparently checkpoints production workloads for major available MPI implementations: \"develop once, run everywhere\". The new platform enables application developers to compile their application against any of the available standards-compliant MPI implementations, and test each MPI implementation according to performance or other features.","sentences":["This work presents experience with traditional use cases of checkpointing on a novel platform.","A single codebase (MANA) transparently checkpoints production workloads for major available MPI implementations: \"develop once, run everywhere\".","The new platform enables application developers to compile their application against any of the available standards-compliant MPI implementations, and test each MPI implementation according to performance or other features."],"url":"http://arxiv.org/abs/2309.14996v1"}
{"created":"2023-09-26 15:03:05","title":"Measurement Models For Sailboats Price vs. Features And Regional Areas","abstract":"In this study, we investigated the relationship between sailboat technical specifications and their prices, as well as regional pricing influences. Utilizing a dataset encompassing characteristics like length, beam, draft, displacement, sail area, and waterline, we applied multiple machine learning models to predict sailboat prices. The gradient descent model demonstrated superior performance, producing the lowest MSE and MAE. Our analysis revealed that monohulled boats are generally more affordable than catamarans, and that certain specifications such as length, beam, displacement, and sail area directly correlate with higher prices. Interestingly, lower draft was associated with higher listing prices. We also explored regional price determinants and found that the United States tops the list in average sailboat prices, followed by Europe, Hong Kong, and the Caribbean. Contrary to our initial hypothesis, a country's GDP showed no direct correlation with sailboat prices. Utilizing a 50% cross-validation method, our models yielded consistent results across test groups. Our research offers a machine learning-enhanced perspective on sailboat pricing, aiding prospective buyers in making informed decisions.","sentences":["In this study, we investigated the relationship between sailboat technical specifications and their prices, as well as regional pricing influences.","Utilizing a dataset encompassing characteristics like length, beam, draft, displacement, sail area, and waterline, we applied multiple machine learning models to predict sailboat prices.","The gradient descent model demonstrated superior performance, producing the lowest MSE and MAE.","Our analysis revealed that monohulled boats are generally more affordable than catamarans, and that certain specifications such as length, beam, displacement, and sail area directly correlate with higher prices.","Interestingly, lower draft was associated with higher listing prices.","We also explored regional price determinants and found that the United States tops the list in average sailboat prices, followed by Europe, Hong Kong, and the Caribbean.","Contrary to our initial hypothesis, a country's GDP showed no direct correlation with sailboat prices.","Utilizing a 50% cross-validation method, our models yielded consistent results across test groups.","Our research offers a machine learning-enhanced perspective on sailboat pricing, aiding prospective buyers in making informed decisions."],"url":"http://arxiv.org/abs/2309.14994v1"}
{"created":"2023-09-26 15:01:48","title":"Exploring ChatGPT Approach to Bidirectional Traceability Problem between Design Models and Code","abstract":"This study explores the capabilities of Large Language Models (LLMs), particularly OpenAI's ChatGPT, in addressing the challenges associated with software modeling, explicitly focusing on the bidirectional traceability problem between design models and code. The objective of this study is to demonstrate the proficiency of ChatGPT in understanding and integrating specific requirements into design models and code and its potential to offer solutions to the bidirectional traceability problem through a case study. The findings indicate that ChatGPT is capable of generating design models and code from natural language requirements, thereby bridging the gap between these requirements and software modeling. Despite its limitations in suggesting a specific method to resolve the problem using ChatGPT itself, it exhibited the capacity to provide corrections to be consistent between design models and code. As a result, the study concludes that achieving bidirectional traceability between design models and code is feasible using ChatGPT.","sentences":["This study explores the capabilities of Large Language Models (LLMs), particularly OpenAI's ChatGPT, in addressing the challenges associated with software modeling, explicitly focusing on the bidirectional traceability problem between design models and code.","The objective of this study is to demonstrate the proficiency of ChatGPT in understanding and integrating specific requirements into design models and code and its potential to offer solutions to the bidirectional traceability problem through a case study.","The findings indicate that ChatGPT is capable of generating design models and code from natural language requirements, thereby bridging the gap between these requirements and software modeling.","Despite its limitations in suggesting a specific method to resolve the problem using ChatGPT itself, it exhibited the capacity to provide corrections to be consistent between design models and code.","As a result, the study concludes that achieving bidirectional traceability between design models and code is feasible using ChatGPT."],"url":"http://arxiv.org/abs/2309.14992v1"}
{"created":"2023-09-26 15:01:43","title":"Robust Sequential DeepFake Detection","abstract":"Since photorealistic faces can be readily generated by facial manipulation technologies nowadays, potential malicious abuse of these technologies has drawn great concerns. Numerous deepfake detection methods are thus proposed. However, existing methods only focus on detecting one-step facial manipulation. As the emergence of easy-accessible facial editing applications, people can easily manipulate facial components using multi-step operations in a sequential manner. This new threat requires us to detect a sequence of facial manipulations, which is vital for both detecting deepfake media and recovering original faces afterwards. Motivated by this observation, we emphasize the need and propose a novel research problem called Detecting Sequential DeepFake Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only demanding a binary label prediction, detecting Seq-DeepFake manipulation requires correctly predicting a sequential vector of facial manipulation operations. To support a large-scale investigation, we construct the first Seq-DeepFake dataset, where face images are manipulated sequentially with corresponding annotations of sequential facial manipulation vectors. Based on this new dataset, we cast detecting Seq-DeepFake manipulation as a specific image-to-sequence task and propose a concise yet effective Seq-DeepFake Transformer (SeqFakeFormer). To better reflect real-world deepfake data distributions, we further apply various perturbations on the original Seq-DeepFake dataset and construct the more challenging Sequential DeepFake dataset with perturbations (Seq-DeepFake-P). To exploit deeper correlation between images and sequences when facing Seq-DeepFake-P, a dedicated Seq-DeepFake Transformer with Image-Sequence Reasoning (SeqFakeFormer++) is devised, which builds stronger correspondence between image-sequence pairs for more robust Seq-DeepFake detection.","sentences":["Since photorealistic faces can be readily generated by facial manipulation technologies nowadays, potential malicious abuse of these technologies has drawn great concerns.","Numerous deepfake detection methods are thus proposed.","However, existing methods only focus on detecting one-step facial manipulation.","As the emergence of easy-accessible facial editing applications, people can easily manipulate facial components using multi-step operations in a sequential manner.","This new threat requires us to detect a sequence of facial manipulations, which is vital for both detecting deepfake media and recovering original faces afterwards.","Motivated by this observation, we emphasize the need and propose a novel research problem called Detecting Sequential DeepFake Manipulation (Seq-DeepFake).","Unlike the existing deepfake detection task only demanding a binary label prediction, detecting Seq-DeepFake manipulation requires correctly predicting a sequential vector of facial manipulation operations.","To support a large-scale investigation, we construct the first Seq-DeepFake dataset, where face images are manipulated sequentially with corresponding annotations of sequential facial manipulation vectors.","Based on this new dataset, we cast detecting Seq-DeepFake manipulation as a specific image-to-sequence task and propose a concise yet effective Seq-DeepFake Transformer (SeqFakeFormer).","To better reflect real-world deepfake data distributions, we further apply various perturbations on the original Seq-DeepFake dataset and construct the more challenging Sequential DeepFake dataset with perturbations (Seq-DeepFake-P).","To exploit deeper correlation between images and sequences when facing Seq-DeepFake-P, a dedicated Seq-DeepFake Transformer with Image-Sequence Reasoning (SeqFakeFormer++) is devised, which builds stronger correspondence between image-sequence pairs for more robust Seq-DeepFake detection."],"url":"http://arxiv.org/abs/2309.14991v1"}
{"created":"2023-09-26 15:01:21","title":"Tempo Adaption in Non-stationary Reinforcement Learning","abstract":"We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($\\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\\mathfrak{t} \\in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\\mathfrak{t}_k$ allocates $\\Delta \\mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\\mathfrak{t}_{k+1}=\\mathfrak{t}_{k}+\\Delta \\mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \\textit{interaction times} ($\\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfrak{t}_K$), significantly impacting the sub-optimality gap of policy. We propose a Proactively Synchronizing Tempo (ProST) framework that computes optimal $\\{ \\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfrak{t}_K \\} (= \\{ \\mathfrak{t} \\}_{1:K})$. Our main contribution is that we show optimal $\\{ \\mathfrak{t} \\}_{1:K}$ trades-off between the policy training time (agent tempo) and how fast the environment changes (environment tempo). Theoretically, this work establishes an optimal $\\{ \\mathfrak{t} \\}_{1:K}$ as a function of the degree of the environment's non-stationarity while also achieving a sublinear dynamic regret. Our experimental evaluation on various high dimensional non-stationary environments shows that the ProST framework achieves a higher online return at optimal $\\{ \\mathfrak{t} \\}_{1:K}$ than the existing methods.","sentences":["We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications.","In reality, environmental changes occur over wall-clock time ($\\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\\mathfrak{t} \\in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\\mathfrak{t}_k$ allocates $\\Delta \\mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\\mathfrak{t}_{k+1}=\\mathfrak{t}_{k}+\\Delta \\mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \\textit{interaction times} ($\\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfrak{t}_K$), significantly impacting the sub-optimality gap of policy.","We propose a Proactively Synchronizing Tempo (ProST) framework that computes optimal $\\{ \\mathfrak{t}_1,\\mathfrak{t}_2,...,\\mathfrak{t}_K \\} (= \\{ \\mathfrak{t} \\}_{1:K})$.","Our main contribution is that we show optimal $\\{ \\mathfrak{t} \\}_{1:K}$ trades-off between the policy training time (agent tempo) and how fast the environment changes (environment tempo).","Theoretically, this work establishes an optimal $\\{ \\mathfrak{t} \\}_{1:K}$ as a function of the degree of the environment's non-stationarity while also achieving a sublinear dynamic regret.","Our experimental evaluation on various high dimensional non-stationary environments shows that the ProST framework achieves a higher online return at optimal $\\{ \\mathfrak{t} \\}_{1:K}$ than the existing methods."],"url":"http://arxiv.org/abs/2309.14989v1"}
{"created":"2023-09-26 14:57:02","title":"Types and Semantics for Extensible Data Types (Extended Version)","abstract":"Developing and maintaining software commonly requires (1) adding new data type constructors to existing applications, but also (2) adding new functions that work on existing data. Most programming languages have native support for defining data types and functions in a way that supports either (1) or (2), but not both. This lack of native support makes it difficult to use and extend libraries. A theoretically well-studied solution is to define data types and functions using initial algebra semantics. While it is possible to encode this solution in existing programming languages, such encodings add syntactic and interpretive overhead, and commonly fail to take advantage of the map and fold fusion laws of initial algebras which compilers could exploit to generate more efficient code. A solution to these is to provide native support for initial algebra semantics. In this paper, we develop such a solution and present a type discipline and core calculus for a language with native support for initial algebra semantics.","sentences":["Developing and maintaining software commonly requires (1) adding new data type constructors to existing applications, but also (2) adding new functions that work on existing data.","Most programming languages have native support for defining data types and functions in a way that supports either (1) or (2), but not both.","This lack of native support makes it difficult to use and extend libraries.","A theoretically well-studied solution is to define data types and functions using initial algebra semantics.","While it is possible to encode this solution in existing programming languages, such encodings add syntactic and interpretive overhead, and commonly fail to take advantage of the map and fold fusion laws of initial algebras which compilers could exploit to generate more efficient code.","A solution to these is to provide native support for initial algebra semantics.","In this paper, we develop such a solution and present a type discipline and core calculus for a language with native support for initial algebra semantics."],"url":"http://arxiv.org/abs/2309.14985v1"}
{"created":"2023-09-26 14:56:56","title":"The Role of Document Embedding in Research Paper Recommender Systems: To Breakdown or to Bolster Disciplinary Borders?","abstract":"In the extensive recommender systems literature, novelty and diversity have been identified as key properties of useful recommendations. However, these properties have received limited attention in the specific sub-field of research paper recommender systems. In this work, we argue for the importance of offering novel and diverse research paper recommendations to scientists. This approach aims to reduce siloed reading, break down filter bubbles, and promote interdisciplinary research. We propose a novel framework for evaluating the novelty and diversity of research paper recommendations that leverages methods from network analysis and natural language processing. Using this framework, we show that the choice of representational method within a larger research paper recommendation system can have a measurable impact on the nature of downstream recommendations, specifically on their novelty and diversity. We introduce a novel paper embedding method, which we demonstrate offers more innovative and diverse recommendations without sacrificing precision, compared to other state-of-the-art baselines.","sentences":["In the extensive recommender systems literature, novelty and diversity have been identified as key properties of useful recommendations.","However, these properties have received limited attention in the specific sub-field of research paper recommender systems.","In this work, we argue for the importance of offering novel and diverse research paper recommendations to scientists.","This approach aims to reduce siloed reading, break down filter bubbles, and promote interdisciplinary research.","We propose a novel framework for evaluating the novelty and diversity of research paper recommendations that leverages methods from network analysis and natural language processing.","Using this framework, we show that the choice of representational method within a larger research paper recommendation system can have a measurable impact on the nature of downstream recommendations, specifically on their novelty and diversity.","We introduce a novel paper embedding method, which we demonstrate offers more innovative and diverse recommendations without sacrificing precision, compared to other state-of-the-art baselines."],"url":"http://arxiv.org/abs/2309.14984v1"}
{"created":"2023-09-26 14:52:51","title":"MoCaE: Mixture of Calibrated Experts Significantly Improves Object Detection","abstract":"We propose an extremely simple and highly effective approach to faithfully combine different object detectors to obtain a Mixture of Experts (MoE) that has a superior accuracy to the individual experts in the mixture. We find that naively combining these experts in a similar way to the well-known Deep Ensembles (DEs), does not result in an effective MoE. We identify the incompatibility between the confidence score distribution of different detectors to be the primary reason for such failure cases. Therefore, to construct the MoE, our proposal is to first calibrate each individual detector against a target calibration function. Then, filter and refine all the predictions from different detectors in the mixture. We term this approach as MoCaE and demonstrate its effectiveness through extensive experiments on object detection, instance segmentation and rotated object detection tasks. Specifically, MoCaE improves (i) three strong object detectors on COCO test-dev by $2.4$ $\\mathrm{AP}$ by reaching $59.0$ $\\mathrm{AP}$; (ii) instance segmentation methods on the challenging long-tailed LVIS dataset by $2.3$ $\\mathrm{AP}$; and (iii) all existing rotated object detectors by reaching $82.62$ $\\mathrm{AP_{50}}$ on DOTA dataset, establishing a new state-of-the-art (SOTA). Code will be made public.","sentences":["We propose an extremely simple and highly effective approach to faithfully combine different object detectors to obtain a Mixture of Experts (MoE) that has a superior accuracy to the individual experts in the mixture.","We find that naively combining these experts in a similar way to the well-known Deep Ensembles (DEs), does not result in an effective MoE. We identify the incompatibility between the confidence score distribution of different detectors to be the primary reason for such failure cases.","Therefore, to construct the MoE, our proposal is to first calibrate each individual detector against a target calibration function.","Then, filter and refine all the predictions from different detectors in the mixture.","We term this approach as MoCaE and demonstrate its effectiveness through extensive experiments on object detection, instance segmentation and rotated object detection tasks.","Specifically, MoCaE improves (i) three strong object detectors on COCO test-dev by $2.4$ $\\mathrm{AP}$ by reaching $59.0$ $\\mathrm{AP}$; (ii) instance segmentation methods on the challenging long-tailed LVIS dataset by $2.3$ $\\mathrm{AP}$; and (iii) all existing rotated object detectors by reaching $82.62$ $\\mathrm{AP_{50}}$ on DOTA dataset, establishing a new state-of-the-art (SOTA).","Code will be made public."],"url":"http://arxiv.org/abs/2309.14976v1"}
{"created":"2023-09-26 14:48:29","title":"Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild","abstract":"While humans can use parts of their arms other than the hands for manipulations like gathering and supporting, whether robots can effectively learn and perform the same type of operations remains relatively unexplored. As these manipulations require joint-level control to regulate the complete poses of the robots, we develop AirExo, a low-cost, adaptable, and portable dual-arm exoskeleton, for teleoperation and demonstration collection. As collecting teleoperated data is expensive and time-consuming, we further leverage AirExo to collect cheap in-the-wild demonstrations at scale. Under our in-the-wild learning framework, we show that with only 3 minutes of the teleoperated demonstrations, augmented by diverse and extensive in-the-wild data collected by AirExo, robots can learn a policy that is comparable to or even better than one learned from teleoperated demonstrations lasting over 20 minutes. Experiments demonstrate that our approach enables the model to learn a more general and robust policy across the various stages of the task, enhancing the success rates in task completion even with the presence of disturbances. Project website: https://airexo.github.io/","sentences":["While humans can use parts of their arms other than the hands for manipulations like gathering and supporting, whether robots can effectively learn and perform the same type of operations remains relatively unexplored.","As these manipulations require joint-level control to regulate the complete poses of the robots, we develop AirExo, a low-cost, adaptable, and portable dual-arm exoskeleton, for teleoperation and demonstration collection.","As collecting teleoperated data is expensive and time-consuming, we further leverage AirExo to collect cheap in-the-wild demonstrations at scale.","Under our in-the-wild learning framework, we show that with only 3 minutes of the teleoperated demonstrations, augmented by diverse and extensive in-the-wild data collected by AirExo, robots can learn a policy that is comparable to or even better than one learned from teleoperated demonstrations lasting over 20 minutes.","Experiments demonstrate that our approach enables the model to learn a more general and robust policy across the various stages of the task, enhancing the success rates in task completion even with the presence of disturbances.","Project website: https://airexo.github.io/"],"url":"http://arxiv.org/abs/2309.14975v1"}
{"created":"2023-09-26 14:44:48","title":"Improving Unsupervised Visual Program Inference with Code Rewriting Families","abstract":"Programs offer compactness and structure that makes them an attractive representation for visual data. We explore how code rewriting can be used to improve systems for inferring programs from visual data. We first propose Sparse Intermittent Rewrite Injection (SIRI), a framework for unsupervised bootstrapped learning. SIRI sparsely applies code rewrite operations over a dataset of training programs, injecting the improved programs back into the training set. We design a family of rewriters for visual programming domains: parameter optimization, code pruning, and code grafting. For three shape programming languages in 2D and 3D, we show that using SIRI with our family of rewriters improves performance: better reconstructions and faster convergence rates, compared with bootstrapped learning methods that do not use rewriters or use them naively. Finally, we demonstrate that our family of rewriters can be effectively used at test time to improve the output of SIRI predictions. For 2D and 3D CSG, we outperform or match the reconstruction performance of recent domain-specific neural architectures, while producing more parsimonious programs that use significantly fewer primitives.","sentences":["Programs offer compactness and structure that makes them an attractive representation for visual data.","We explore how code rewriting can be used to improve systems for inferring programs from visual data.","We first propose Sparse Intermittent Rewrite Injection (SIRI), a framework for unsupervised bootstrapped learning.","SIRI sparsely applies code rewrite operations over a dataset of training programs, injecting the improved programs back into the training set.","We design a family of rewriters for visual programming domains: parameter optimization, code pruning, and code grafting.","For three shape programming languages in 2D and 3D, we show that using SIRI with our family of rewriters improves performance: better reconstructions and faster convergence rates, compared with bootstrapped learning methods that do not use rewriters or use them naively.","Finally, we demonstrate that our family of rewriters can be effectively used at test time to improve the output of SIRI predictions.","For 2D and 3D CSG, we outperform or match the reconstruction performance of recent domain-specific neural architectures, while producing more parsimonious programs that use significantly fewer primitives."],"url":"http://arxiv.org/abs/2309.14972v1"}
{"created":"2023-09-26 14:44:08","title":"Minimizing Energy Consumption for 5G NR Beam Management for RedCap Devices","abstract":"In 5G New Radio (NR), beam management entails periodic and continuous transmission and reception of control signals in the form of synchronization signal blocks (SSBs), used to perform initial access and/or channel estimation. However, this procedure demands continuous energy consumption, which is particularly challenging to handle for low-cost, low-complexity, and battery-constrained devices, such as RedCap devices to support mid-market Internet of Things (IoT) use cases. In this context, this work aims at reducing the energy consumption during beam management for RedCap devices, while ensuring that the desired Quality of Service (QoS) requirements are met. To do so, we formalize an optimization problem in an Indoor Factory (InF) scenario to select the best beam management parameters, including the beam update periodicity and the beamwidth, to minimize energy consumption based on users' distribution and their speed. The analysis yields the regions of feasibility, i.e., the upper limit(s) on the beam management parameters for RedCap devices, that we use to provide design guidelines accordingly.","sentences":["In 5G New Radio (NR), beam management entails periodic and continuous transmission and reception of control signals in the form of synchronization signal blocks (SSBs), used to perform initial access and/or channel estimation.","However, this procedure demands continuous energy consumption, which is particularly challenging to handle for low-cost, low-complexity, and battery-constrained devices, such as RedCap devices to support mid-market Internet of Things (IoT) use cases.","In this context, this work aims at reducing the energy consumption during beam management for RedCap devices, while ensuring that the desired Quality of Service (QoS) requirements are met.","To do so, we formalize an optimization problem in an Indoor Factory (InF) scenario to select the best beam management parameters, including the beam update periodicity and the beamwidth, to minimize energy consumption based on users' distribution and their speed.","The analysis yields the regions of feasibility, i.e., the upper limit(s) on the beam management parameters for RedCap devices, that we use to provide design guidelines accordingly."],"url":"http://arxiv.org/abs/2309.14971v1"}
{"created":"2023-09-26 14:37:31","title":"A novel approach for holographic 3D content generation without depth map","abstract":"In preparation for observing holographic 3D content, acquiring a set of RGB color and depth map images per scene is necessary to generate computer-generated holograms (CGHs) when using the fast Fourier transform (FFT) algorithm. However, in real-world situations, these paired formats of RGB color and depth map images are not always fully available. We propose a deep learning-based method to synthesize the volumetric digital holograms using only the given RGB image, so that we can overcome environments where RGB color and depth map images are partially provided. The proposed method uses only the input of RGB image to estimate its depth map and then generate its CGH sequentially. Through experiments, we demonstrate that the volumetric hologram generated through our proposed model is more accurate than that of competitive models, under the situation that only RGB color data can be provided.","sentences":["In preparation for observing holographic 3D content, acquiring a set of RGB color and depth map images per scene is necessary to generate computer-generated holograms (CGHs) when using the fast Fourier transform (FFT) algorithm.","However, in real-world situations, these paired formats of RGB color and depth map images are not always fully available.","We propose a deep learning-based method to synthesize the volumetric digital holograms using only the given RGB image, so that we can overcome environments where RGB color and depth map images are partially provided.","The proposed method uses only the input of RGB image to estimate its depth map and then generate its CGH sequentially.","Through experiments, we demonstrate that the volumetric hologram generated through our proposed model is more accurate than that of competitive models, under the situation that only RGB color data can be provided."],"url":"http://arxiv.org/abs/2309.14967v1"}
{"created":"2023-09-26 14:36:19","title":"Interactively Learning Social Media Representations Improves News Source Factuality Detection","abstract":"The rise of social media has enabled the widespread propagation of fake news, text that is published with an intent to spread misinformation and sway beliefs. Rapidly detecting fake news, especially as new events arise, is important to prevent misinformation.   While prior works have tackled this problem using supervised learning systems, automatedly modeling the complexities of the social media landscape that enables the spread of fake news is challenging. On the contrary, having humans fact check all news is not scalable. Thus, in this paper, we propose to approach this problem interactively, where humans can interact to help an automated system learn a better social media representation quality. On real world events, our experiments show performance improvements in detecting factuality of news sources, even after few human interactions.","sentences":["The rise of social media has enabled the widespread propagation of fake news, text that is published with an intent to spread misinformation and sway beliefs.","Rapidly detecting fake news, especially as new events arise, is important to prevent misinformation.   ","While prior works have tackled this problem using supervised learning systems, automatedly modeling the complexities of the social media landscape that enables the spread of fake news is challenging.","On the contrary, having humans fact check all news is not scalable.","Thus, in this paper, we propose to approach this problem interactively, where humans can interact to help an automated system learn a better social media representation quality.","On real world events, our experiments show performance improvements in detecting factuality of news sources, even after few human interactions."],"url":"http://arxiv.org/abs/2309.14966v1"}
{"created":"2023-09-26 14:29:45","title":"GridFormer: Towards Accurate Table Structure Recognition via Grid Prediction","abstract":"All tables can be represented as grids. Based on this observation, we propose GridFormer, a novel approach for interpreting unconstrained table structures by predicting the vertex and edge of a grid. First, we propose a flexible table representation in the form of an MXN grid. In this representation, the vertexes and edges of the grid store the localization and adjacency information of the table. Then, we introduce a DETR-style table structure recognizer to efficiently predict this multi-objective information of the grid in a single shot. Specifically, given a set of learned row and column queries, the recognizer directly outputs the vertexes and edges information of the corresponding rows and columns. Extensive experiments on five challenging benchmarks which include wired, wireless, multi-merge-cell, oriented, and distorted tables demonstrate the competitive performance of our model over other methods.","sentences":["All tables can be represented as grids.","Based on this observation, we propose GridFormer, a novel approach for interpreting unconstrained table structures by predicting the vertex and edge of a grid.","First, we propose a flexible table representation in the form of an MXN grid.","In this representation, the vertexes and edges of the grid store the localization and adjacency information of the table.","Then, we introduce a DETR-style table structure recognizer to efficiently predict this multi-objective information of the grid in a single shot.","Specifically, given a set of learned row and column queries, the recognizer directly outputs the vertexes and edges information of the corresponding rows and columns.","Extensive experiments on five challenging benchmarks which include wired, wireless, multi-merge-cell, oriented, and distorted tables demonstrate the competitive performance of our model over other methods."],"url":"http://arxiv.org/abs/2309.14962v1"}
{"created":"2023-09-26 14:08:03","title":"Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher","abstract":"Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods. When the labeled dataset is coming from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over mixing these source domains and performing a UDA, as observed by recent studies in MSDA. Existing MSDA methods learn domain invariant and domain-specific parameters (for each source domain) for the adaptation. However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly proportional to the number of source domains used. This paper proposes a novel MSDA method called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead of domain-specific subnets to preserve domain-specific information. These prototypes are learned using a contrastive loss, aligning the same categories across domains and separating different categories far apart. Because of the use of prototypes, the parameter size of our method does not increase significantly with the number of source domains, thus reducing memory issues and possible overfitting. Empirical studies show PMT outperforms state-of-the-art MSDA methods on several challenging object detection datasets.","sentences":["Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods.","When the labeled dataset is coming from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over mixing these source domains and performing a UDA, as observed by recent studies in MSDA.","Existing MSDA methods learn domain invariant and domain-specific parameters (for each source domain) for the adaptation.","However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly proportional to the number of source domains used.","This paper proposes a novel MSDA method called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead of domain-specific subnets to preserve domain-specific information.","These prototypes are learned using a contrastive loss, aligning the same categories across domains and separating different categories far apart.","Because of the use of prototypes, the parameter size of our method does not increase significantly with the number of source domains, thus reducing memory issues and possible overfitting.","Empirical studies show PMT outperforms state-of-the-art MSDA methods on several challenging object detection datasets."],"url":"http://arxiv.org/abs/2309.14950v1"}
{"created":"2023-09-26 14:06:26","title":"Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with Balanced Normalization","abstract":"Test-Time Adaptation aims to adapt source domain model to testing data at inference stage with success demonstrated in adapting to unseen corruptions. However, these attempts may fail under more challenging real-world scenarios. Existing works mainly consider real-world test-time adaptation under non-i.i.d. data stream and continual domain shift. In this work, we first complement the existing real-world TTA protocol with a globally class imbalanced testing set. We demonstrate that combining all settings together poses new challenges to existing methods. We argue the failure of state-of-the-art methods is first caused by indiscriminately adapting normalization layers to imbalanced testing data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap out the regular batchnorm at inference stage. The new batchnorm layer is capable of adapting without biasing towards majority classes. We are further inspired by the success of self-training~(ST) in learning from unlabeled data and adapt ST for test-time adaptation. However, ST alone is prone to over adaption which is responsible for the poor performance under continual domain shift. Hence, we propose to improve self-training under continual domain shift by regularizing model updates with an anchored loss. The final TTA model, termed as TRIBE, is built upon a tri-net architecture with balanced batchnorm layers. We evaluate TRIBE on four datasets representing real-world TTA settings. TRIBE consistently achieves the state-of-the-art performance across multiple evaluation protocols. The code is available at \\url{https://github.com/Gorilla-Lab-SCUT/TRIBE}.","sentences":["Test-Time Adaptation aims to adapt source domain model to testing data at inference stage with success demonstrated in adapting to unseen corruptions.","However, these attempts may fail under more challenging real-world scenarios.","Existing works mainly consider real-world test-time adaptation under non-i.i.d. data stream and continual domain shift.","In this work, we first complement the existing real-world TTA protocol with a globally class imbalanced testing set.","We demonstrate that combining all settings together poses new challenges to existing methods.","We argue the failure of state-of-the-art methods is first caused by indiscriminately adapting normalization layers to imbalanced testing data.","To remedy this shortcoming, we propose a balanced batchnorm layer to swap out the regular batchnorm at inference stage.","The new batchnorm layer is capable of adapting without biasing towards majority classes.","We are further inspired by the success of self-training~(ST) in learning from unlabeled data and adapt ST for test-time adaptation.","However, ST alone is prone to over adaption which is responsible for the poor performance under continual domain shift.","Hence, we propose to improve self-training under continual domain shift by regularizing model updates with an anchored loss.","The final TTA model, termed as TRIBE, is built upon a tri-net architecture with balanced batchnorm layers.","We evaluate TRIBE on four datasets representing real-world TTA settings.","TRIBE consistently achieves the state-of-the-art performance across multiple evaluation protocols.","The code is available at \\url{https://github.com/Gorilla-Lab-SCUT/TRIBE}."],"url":"http://arxiv.org/abs/2309.14949v1"}
{"created":"2023-09-26 14:00:25","title":"Integration of Large Language Models within Cognitive Architectures for Autonomous Robots","abstract":"The usage of Large Language Models (LLMs) has increased recently, not only due to the significant improvements in their accuracy but also because of the use of the quantization that allows running these models without intense hardware requirements. As a result, the LLMs have proliferated. It implies the creation of a great variety of LLMs with different capabilities. This way, this paper proposes the integration of LLMs in cognitive architectures for autonomous robots. Specifically, we present the design, development and deployment of the llama\\_ros tool that allows the easy use and integration of LLMs in ROS 2-based environments, afterward integrated with the state-of-the-art cognitive architecture MERLIN2 for updating a PDDL-based planner system. This proposal is evaluated quantitatively and qualitatively, measuring the impact of incorporating the LLMs in the cognitive architecture.","sentences":["The usage of Large Language Models (LLMs) has increased recently, not only due to the significant improvements in their accuracy but also because of the use of the quantization that allows running these models without intense hardware requirements.","As a result, the LLMs have proliferated.","It implies the creation of a great variety of LLMs with different capabilities.","This way, this paper proposes the integration of LLMs in cognitive architectures for autonomous robots.","Specifically, we present the design, development and deployment of the llama\\_ros tool that allows the easy use and integration of LLMs in ROS 2-based environments, afterward integrated with the state-of-the-art cognitive architecture MERLIN2 for updating a PDDL-based planner system.","This proposal is evaluated quantitatively and qualitatively, measuring the impact of incorporating the LLMs in the cognitive architecture."],"url":"http://arxiv.org/abs/2309.14945v1"}
{"created":"2023-09-26 13:48:38","title":"Modeling Multi-aspect Preferences and Intents for Multi-behavioral Sequential Recommendation","abstract":"Multi-behavioral sequential recommendation has recently attracted increasing attention. However, existing methods suffer from two major limitations. Firstly, user preferences and intents can be described in fine-grained detail from multiple perspectives; yet, these methods fail to capture their multi-aspect nature. Secondly, user behaviors may contain noises, and most existing methods could not effectively deal with noises. In this paper, we present an attentive recurrent model with multiple projections to capture Multi-Aspect preferences and INTents (MAINT in short). To extract multi-aspect preferences from target behaviors, we propose a multi-aspect projection mechanism for generating multiple preference representations from multiple aspects. To extract multi-aspect intents from multi-typed behaviors, we propose a behavior-enhanced LSTM and a multi-aspect refinement attention mechanism. The attention mechanism can filter out noises and generate multiple intent representations from different aspects. To adaptively fuse user preferences and intents, we propose a multi-aspect gated fusion mechanism. Extensive experiments conducted on real-world datasets have demonstrated the effectiveness of our model.","sentences":["Multi-behavioral sequential recommendation has recently attracted increasing attention.","However, existing methods suffer from two major limitations.","Firstly, user preferences and intents can be described in fine-grained detail from multiple perspectives; yet, these methods fail to capture their multi-aspect nature.","Secondly, user behaviors may contain noises, and most existing methods could not effectively deal with noises.","In this paper, we present an attentive recurrent model with multiple projections to capture Multi-Aspect preferences and INTents (MAINT in short).","To extract multi-aspect preferences from target behaviors, we propose a multi-aspect projection mechanism for generating multiple preference representations from multiple aspects.","To extract multi-aspect intents from multi-typed behaviors, we propose a behavior-enhanced LSTM and a multi-aspect refinement attention mechanism.","The attention mechanism can filter out noises and generate multiple intent representations from different aspects.","To adaptively fuse user preferences and intents, we propose a multi-aspect gated fusion mechanism.","Extensive experiments conducted on real-world datasets have demonstrated the effectiveness of our model."],"url":"http://arxiv.org/abs/2309.14938v1"}
{"created":"2023-09-26 13:48:30","title":"Virtual Reality as a Tool for Studying Diversity and Inclusion in Human-Robot Interaction: Advantages and Challenges","abstract":"This paper investigates the potential of Virtual Reality (VR) as a research tool for studying diversity and inclusion characteristics in the context of human-robot interactions (HRI). Some exclusive advantages of using VR in HRI are discussed, such as a controllable environment, the possibility to manipulate the variables related to the robot and the human-robot interaction, flexibility in the design of the robot and the environment, and advanced measurement methods related e.g. to eye tracking and physiological data. At the same time, the challenges of researching diversity and inclusion in HRI are described, especially in accessibility, cyber sickness and bias when developing VR-environments. Furthermore, solutions to these challenges are being discussed to fully harness the benefits of VR for the studying of diversity and inclusion.","sentences":["This paper investigates the potential of Virtual Reality (VR) as a research tool for studying diversity and inclusion characteristics in the context of human-robot interactions (HRI).","Some exclusive advantages of using VR in HRI are discussed, such as a controllable environment, the possibility to manipulate the variables related to the robot and the human-robot interaction, flexibility in the design of the robot and the environment, and advanced measurement methods related e.g. to eye tracking and physiological data.","At the same time, the challenges of researching diversity and inclusion in HRI are described, especially in accessibility, cyber sickness and bias when developing VR-environments.","Furthermore, solutions to these challenges are being discussed to fully harness the benefits of VR for the studying of diversity and inclusion."],"url":"http://arxiv.org/abs/2309.14937v1"}
{"created":"2023-09-26 13:48:04","title":"Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives","abstract":"Machine learning (ML) methods offer a wide range of configurable hyperparameters that have a significant influence on their performance. While accuracy is a commonly used performance objective, in many settings, it is not sufficient. Optimizing the ML models with respect to multiple objectives such as accuracy, confidence, fairness, calibration, privacy, latency, and memory consumption is becoming crucial. To that end, hyperparameter optimization, the approach to systematically optimize the hyperparameters, which is already challenging for a single objective, is even more challenging for multiple objectives. In addition, the differences in objective scales, the failures, and the presence of outlier values in objectives make the problem even harder. We propose a multi-objective Bayesian optimization (MoBO) algorithm that addresses these problems through uniform objective normalization and randomized weights in scalarization. We increase the efficiency of our approach by imposing constraints on the objective to avoid exploring unnecessary configurations (e.g., insufficient accuracy). Finally, we leverage an approach to parallelize the MoBO which results in a 5x speed-up when using 16x more workers.","sentences":["Machine learning (ML) methods offer a wide range of configurable hyperparameters that have a significant influence on their performance.","While accuracy is a commonly used performance objective, in many settings, it is not sufficient.","Optimizing the ML models with respect to multiple objectives such as accuracy, confidence, fairness, calibration, privacy, latency, and memory consumption is becoming crucial.","To that end, hyperparameter optimization, the approach to systematically optimize the hyperparameters, which is already challenging for a single objective, is even more challenging for multiple objectives.","In addition, the differences in objective scales, the failures, and the presence of outlier values in objectives make the problem even harder.","We propose a multi-objective Bayesian optimization (MoBO) algorithm that addresses these problems through uniform objective normalization and randomized weights in scalarization.","We increase the efficiency of our approach by imposing constraints on the objective to avoid exploring unnecessary configurations (e.g., insufficient accuracy).","Finally, we leverage an approach to parallelize the MoBO which results in a 5x speed-up when using 16x more workers."],"url":"http://arxiv.org/abs/2309.14936v1"}
{"created":"2023-09-26 13:43:06","title":"FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing","abstract":"Text-conditional image editing is a very useful task that has recently emerged with immeasurable potential. Most current real image editing methods first need to complete the reconstruction of the image, and then editing is carried out by various methods based on the reconstruction. Most methods use DDIM Inversion for reconstruction, however, DDIM Inversion often fails to guarantee reconstruction performance, i.e., it fails to produce results that preserve the original image content. To address the problem of reconstruction failure, we propose FEC, which consists of three sampling methods, each designed for different editing types and settings. Our three methods of FEC achieve two important goals in image editing task: 1) ensuring successful reconstruction, i.e., sampling to get a generated result that preserves the texture and features of the original real image. 2) these sampling methods can be paired with many editing methods and greatly improve the performance of these editing methods to accomplish various editing tasks. In addition, none of our sampling methods require fine-tuning of the diffusion model or time-consuming training on large-scale datasets. Hence the cost of time as well as the use of computer memory and computation can be significantly reduced.","sentences":["Text-conditional image editing is a very useful task that has recently emerged with immeasurable potential.","Most current real image editing methods first need to complete the reconstruction of the image, and then editing is carried out by various methods based on the reconstruction.","Most methods use DDIM Inversion for reconstruction, however, DDIM Inversion often fails to guarantee reconstruction performance, i.e., it fails to produce results that preserve the original image content.","To address the problem of reconstruction failure, we propose FEC, which consists of three sampling methods, each designed for different editing types and settings.","Our three methods of FEC achieve two important goals in image editing task: 1) ensuring successful reconstruction, i.e., sampling to get a generated result that preserves the texture and features of the original real image.","2) these sampling methods can be paired with many editing methods and greatly improve the performance of these editing methods to accomplish various editing tasks.","In addition, none of our sampling methods require fine-tuning of the diffusion model or time-consuming training on large-scale datasets.","Hence the cost of time as well as the use of computer memory and computation can be significantly reduced."],"url":"http://arxiv.org/abs/2309.14934v1"}
{"created":"2023-09-26 13:41:30","title":"Addressing Data Misalignment in Image-LiDAR Fusion on Point Cloud Segmentation","abstract":"With the advent of advanced multi-sensor fusion models, there has been a notable enhancement in the performance of perception tasks within in terms of autonomous driving. Despite these advancements, the challenges persist, particularly in the fusion of data from cameras and LiDAR sensors. A critial concern is the accurate alignment of data from these disparate sensors. Our observations indicate that the projected positions of LiDAR points often misalign on the corresponding image. Furthermore, fusion models appear to struggle in accurately segmenting these misaligned points. In this paper, we would like to address this problem carefully, with a specific focus on the nuScenes dataset and the SOTA of fusion models 2DPASS, and providing the possible solutions or potential improvements.","sentences":["With the advent of advanced multi-sensor fusion models, there has been a notable enhancement in the performance of perception tasks within in terms of autonomous driving.","Despite these advancements, the challenges persist, particularly in the fusion of data from cameras and LiDAR sensors.","A critial concern is the accurate alignment of data from these disparate sensors.","Our observations indicate that the projected positions of LiDAR points often misalign on the corresponding image.","Furthermore, fusion models appear to struggle in accurately segmenting these misaligned points.","In this paper, we would like to address this problem carefully, with a specific focus on the nuScenes dataset and the SOTA of fusion models 2DPASS, and providing the possible solutions or potential improvements."],"url":"http://arxiv.org/abs/2309.14932v1"}
{"created":"2023-09-26 13:36:45","title":"Interaction-Aware Sampling-Based MPC with Learned Local Goal Predictions","abstract":"Motion planning for autonomous robots in tight, interaction-rich, and mixed human-robot environments is challenging. State-of-the-art methods typically separate prediction and planning, predicting other agents' trajectories first and then planning the ego agent's motion in the remaining free space. However, agents' lack of awareness of their influence on others can lead to the freezing robot problem. We build upon Interaction-Aware Model Predictive Path Integral (IA-MPPI) control and combine it with learning-based trajectory predictions, thereby relaxing its reliance on communicated short-term goals for other agents. We apply this framework to Autonomous Surface Vessels (ASVs) navigating urban canals. By generating an artificial dataset in real sections of Amsterdam's canals, adapting and training a prediction model for our domain, and proposing heuristics to extract local goals, we enable effective cooperation in planning. Our approach improves autonomous robot navigation in complex, crowded environments, with potential implications for multi-agent systems and human-robot interaction.","sentences":["Motion planning for autonomous robots in tight, interaction-rich, and mixed human-robot environments is challenging.","State-of-the-art methods typically separate prediction and planning, predicting other agents' trajectories first and then planning the ego agent's motion in the remaining free space.","However, agents' lack of awareness of their influence on others can lead to the freezing robot problem.","We build upon Interaction-Aware Model Predictive Path Integral (IA-MPPI) control and combine it with learning-based trajectory predictions, thereby relaxing its reliance on communicated short-term goals for other agents.","We apply this framework to Autonomous Surface Vessels (ASVs) navigating urban canals.","By generating an artificial dataset in real sections of Amsterdam's canals, adapting and training a prediction model for our domain, and proposing heuristics to extract local goals, we enable effective cooperation in planning.","Our approach improves autonomous robot navigation in complex, crowded environments, with potential implications for multi-agent systems and human-robot interaction."],"url":"http://arxiv.org/abs/2309.14931v1"}
{"created":"2023-09-26 13:35:31","title":"Noise-Tolerant Unsupervised Adapter for Vision-Language Models","abstract":"Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leveraging knowledge distillation from large-scale vision language models. Extensive experiments show that NtUA achieves superior performance consistently across multiple widely adopted benchmarks.","sentences":["Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks.","While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks.","We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples.","NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs.","It consists of two complementary designs.","The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence.","The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leveraging knowledge distillation from large-scale vision language models.","Extensive experiments show that NtUA achieves superior performance consistently across multiple widely adopted benchmarks."],"url":"http://arxiv.org/abs/2309.14928v1"}
{"created":"2023-09-26 13:30:57","title":"A Democratic Platform for Engaging with Disabled Community in Generative AI Development","abstract":"Artificial Intelligence (AI) systems, especially generative AI technologies are becoming more relevant in our society. Tools like ChatGPT are being used by members of the disabled community e.g., Autistic people may use it to help compose emails. The growing impact and popularity of generative AI tools have prompted us to examine their relevance within the disabled community. The design and development phases often neglect this marginalized group, leading to inaccurate predictions and unfair discrimination directed towards them. This could result from bias in data sets, algorithms, and systems at various phases of creation and implementation. This workshop paper proposes a platform to involve the disabled community while building generative AI systems. With this platform, our aim is to gain insight into the factors that contribute to bias in the outputs generated by generative AI when used by the disabled community. Furthermore, we expect to comprehend which algorithmic factors are the main contributors to the output's incorrectness or irrelevancy. The proposed platform calls on both disabled and non-disabled people from various geographical and cultural backgrounds to collaborate asynchronously and remotely in a democratic approach to decision-making.","sentences":["Artificial Intelligence (AI) systems, especially generative AI technologies are becoming more relevant in our society.","Tools like ChatGPT are being used by members of the disabled community e.g., Autistic people may use it to help compose emails.","The growing impact and popularity of generative AI tools have prompted us to examine their relevance within the disabled community.","The design and development phases often neglect this marginalized group, leading to inaccurate predictions and unfair discrimination directed towards them.","This could result from bias in data sets, algorithms, and systems at various phases of creation and implementation.","This workshop paper proposes a platform to involve the disabled community while building generative AI systems.","With this platform, our aim is to gain insight into the factors that contribute to bias in the outputs generated by generative AI when used by the disabled community.","Furthermore, we expect to comprehend which algorithmic factors are the main contributors to the output's incorrectness or irrelevancy.","The proposed platform calls on both disabled and non-disabled people from various geographical and cultural backgrounds to collaborate asynchronously and remotely in a democratic approach to decision-making."],"url":"http://arxiv.org/abs/2309.14921v1"}
{"created":"2023-09-26 13:23:50","title":"Ethical Challenges in Gamified Education Research and Development: An Umbrella Review and Potential Directions","abstract":"Gamification is a technological, economic, cultural, and societal development toward promoting a more game-like reality. As this emergent phenomenon has been gradually consolidated into our daily lives, especially in educational settings, many scholars and practitioners face a major challenge ahead: how to understand and mitigate the unethical impacts of gamification when researching and developing such educational technologies? Thus, this study explores ethical challenges in gamified educational applications and proposes potential solutions to address them based on an umbrella review. After analysing secondary studies, this study details and proposes recommendations on addressing some ethical challenges in gamified education, such as power dynamics and paternalism, lack of voluntarity and confidentiality, cognitive manipulation, and social comparison. Research and development decision-making processes affected by such challenges are also elaborated, and potential actions to mitigate their effects in gamification planning, conducting and communication are further introduced. Thus, this chapter provides an understanding of ethical challenges posed by the literature in gamified education and a set of guidelines for future research and development.","sentences":["Gamification is a technological, economic, cultural, and societal development toward promoting a more game-like reality.","As this emergent phenomenon has been gradually consolidated into our daily lives, especially in educational settings, many scholars and practitioners face a major challenge ahead: how to understand and mitigate the unethical impacts of gamification when researching and developing such educational technologies?","Thus, this study explores ethical challenges in gamified educational applications and proposes potential solutions to address them based on an umbrella review.","After analysing secondary studies, this study details and proposes recommendations on addressing some ethical challenges in gamified education, such as power dynamics and paternalism, lack of voluntarity and confidentiality, cognitive manipulation, and social comparison.","Research and development decision-making processes affected by such challenges are also elaborated, and potential actions to mitigate their effects in gamification planning, conducting and communication are further introduced.","Thus, this chapter provides an understanding of ethical challenges posed by the literature in gamified education and a set of guidelines for future research and development."],"url":"http://arxiv.org/abs/2309.14918v1"}
{"created":"2023-09-26 13:22:45","title":"Rate-compatible LDPC Codes based on Primitive Polynomials and Golomb Rulers","abstract":"We introduce and study a family of rate-compatible Low-Density Parity-Check (LDPC) codes characterized by very simple encoders. The design of these codes starts from simplex codes, which are defined by parity-check matrices having a straightforward form stemming from the coefficients of a primitive polynomial. For this reason, we call the new codes Primitive Rate-Compatible LDPC (PRC-LDPC) codes. By applying puncturing to these codes, we obtain a bit-level granularity of their code rates. We show that, in order to achieve good LDPC codes, the underlying polynomials, besides being primitive, must meet some more stringent conditions with respect to those of classical punctured simplex codes. We leverage non-modular Golomb rulers to take the new requirements into account. We characterize the minimum distance properties of PRC-LDPC codes, and study and discuss their encoding and decoding complexity. Finally, we assess their error rate performance under iterative decoding.","sentences":["We introduce and study a family of rate-compatible Low-Density Parity-Check (LDPC) codes characterized by very simple encoders.","The design of these codes starts from simplex codes, which are defined by parity-check matrices having a straightforward form stemming from the coefficients of a primitive polynomial.","For this reason, we call the new codes Primitive Rate-Compatible LDPC (PRC-LDPC) codes.","By applying puncturing to these codes, we obtain a bit-level granularity of their code rates.","We show that, in order to achieve good LDPC codes, the underlying polynomials, besides being primitive, must meet some more stringent conditions with respect to those of classical punctured simplex codes.","We leverage non-modular Golomb rulers to take the new requirements into account.","We characterize the minimum distance properties of PRC-LDPC codes, and study and discuss their encoding and decoding complexity.","Finally, we assess their error rate performance under iterative decoding."],"url":"http://arxiv.org/abs/2309.14917v1"}
{"created":"2023-09-26 13:22:33","title":"PHRIT: Parametric Hand Representation with Implicit Template","abstract":"We propose PHRIT, a novel approach for parametric hand mesh modeling with an implicit template that combines the advantages of both parametric meshes and implicit representations. Our method represents deformable hand shapes using signed distance fields (SDFs) with part-based shape priors, utilizing a deformation field to execute the deformation. The model offers efficient high-fidelity hand reconstruction by deforming the canonical template at infinite resolution. Additionally, it is fully differentiable and can be easily used in hand modeling since it can be driven by the skeleton and shape latent codes. We evaluate PHRIT on multiple downstream tasks, including skeleton-driven hand reconstruction, shapes from point clouds, and single-view 3D reconstruction, demonstrating that our approach achieves realistic and immersive hand modeling with state-of-the-art performance.","sentences":["We propose PHRIT, a novel approach for parametric hand mesh modeling with an implicit template that combines the advantages of both parametric meshes and implicit representations.","Our method represents deformable hand shapes using signed distance fields (SDFs) with part-based shape priors, utilizing a deformation field to execute the deformation.","The model offers efficient high-fidelity hand reconstruction by deforming the canonical template at infinite resolution.","Additionally, it is fully differentiable and can be easily used in hand modeling since it can be driven by the skeleton and shape latent codes.","We evaluate PHRIT on multiple downstream tasks, including skeleton-driven hand reconstruction, shapes from point clouds, and single-view 3D reconstruction, demonstrating that our approach achieves realistic and immersive hand modeling with state-of-the-art performance."],"url":"http://arxiv.org/abs/2309.14916v1"}
