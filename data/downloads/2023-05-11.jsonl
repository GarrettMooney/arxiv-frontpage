{"created":"2023-05-10","title":"Automatic Evaluation of Attribution by Large Language Models","abstract":"A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims. However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution by LLMs. We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization. To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on the curated test set and simulated test examples from existing benchmark questions highlight both promising signals as well as remaining challenges for the automatic evaluation of attribution. We hope our testbed, modeling methodology, and insights will help lay the foundation for future studies on this important problem.","sentences":["A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims.","However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem.","Although human evaluation is common practice, it is costly and time-consuming.","In this paper, we investigate the automatic evaluation of attribution by LLMs.","We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs.","The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization.","To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing.","Our results on the curated test set and simulated test examples from existing benchmark questions highlight both promising signals as well as remaining challenges for the automatic evaluation of attribution.","We hope our testbed, modeling methodology, and insights will help lay the foundation for future studies on this important problem."],"url":"http://arxiv.org/abs/2305.06311v1"}
{"created":"2023-05-10","title":"When ChatGPT for Computer Vision Will Come? From 2D to 3D","abstract":"ChatGPT and its improved variant GPT4 have revolutionized the NLP field with a single model solving almost all text related tasks. However, such a model for computer vision does not exist, especially for 3D vision. This article first provides a brief view on the progress of deep learning in text, image and 3D fields from the model perspective. Moreover, this work further discusses how AIGC evolves from the data perspective. On top of that, this work presents an outlook on the development of AIGC in 3D from the data perspective.","sentences":["ChatGPT and its improved variant GPT4 have revolutionized the NLP field with a single model solving almost all text related tasks.","However, such a model for computer vision does not exist, especially for 3D vision.","This article first provides a brief view on the progress of deep learning in text, image and 3D fields from the model perspective.","Moreover, this work further discusses how AIGC evolves from the data perspective.","On top of that, this work presents an outlook on the development of AIGC in 3D from the data perspective."],"url":"http://arxiv.org/abs/2305.06133v1"}
{"created":"2023-05-10","title":"A Glimpse in ChatGPT Capabilities and its impact for AI research","abstract":"Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development. These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering. However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA. In this paper, we will examine the impact of LLMs on AI research. The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing. We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range of capabilities in a single system is a strong sign of approaching general intelligence. Innovations integrating such models will also expand along the maturation of such AI systems and exhibit unforeseeable applications that will have important impacts on several aspects of our societies.","sentences":["Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development.","These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering.","However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA.","In this paper, we will examine the impact of LLMs on AI research.","The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing.","We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range of capabilities in a single system is a strong sign of approaching general intelligence.","Innovations integrating such models will also expand along the maturation of such AI systems and exhibit unforeseeable applications that will have important impacts on several aspects of our societies."],"url":"http://arxiv.org/abs/2305.06087v1"}
{"created":"2023-05-10","title":"Fast Distributed Inference Serving for Large Language Models","abstract":"Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactively offloads and uploads intermediate states between GPU memory and host memory for LLM inference. We build a system prototype of FastServe based on NVIDIA FasterTransformer. Experimental results show that compared to the state-of-the-art solution Orca, FastServe improves the average and tail JCT by up to 5.1$\\times$ and 6.4$\\times$, respectively.","sentences":["Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT.","The interactive nature of these applications demand low job completion time (JCT) for model inference.","Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT.","We present FastServe, a distributed inference serving system for LLMs.","FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token.","FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler.","Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join.","The higher priority queues than the joined queue are skipped to reduce demotions.","We design an efficient GPU memory management mechanism that proactively offloads and uploads intermediate states between GPU memory and host memory for LLM inference.","We build a system prototype of FastServe based on NVIDIA FasterTransformer.","Experimental results show that compared to the state-of-the-art solution Orca, FastServe improves the average and tail JCT by up to 5.1$\\times$ and 6.4$\\times$, respectively."],"url":"http://arxiv.org/abs/2305.05920v1"}
{"created":"2023-05-10","title":"Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks","abstract":"The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to the state-of-the-art finetuned models as well as pretrained domain-specific generative models. Our experiments provide qualitative studies, through which we hope to help understand the capability of the existing models and facilitate further improvements.","sentences":["The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input.","Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted.","In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting.","Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets.","The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks.","We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to the state-of-the-art finetuned models as well as pretrained domain-specific generative models.","Our experiments provide qualitative studies, through which we hope to help understand the capability of the existing models and facilitate further improvements."],"url":"http://arxiv.org/abs/2305.05862v1"}
{"created":"2023-05-10","title":"HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion","abstract":"Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis.","sentences":["Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing.","To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints.","Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition.","This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion.","While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP.","To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions.","We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis."],"url":"http://arxiv.org/abs/2305.06356v1"}
{"created":"2023-05-10","title":"VideoChat: Chat-Centric Video Understanding","abstract":"In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything","sentences":["In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system.","It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference.","To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations.","This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems.","Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research.","Access our code and data at https://github.com/OpenGVLab/Ask-Anything"],"url":"http://arxiv.org/abs/2305.06355v1"}
{"created":"2023-05-10","title":"RECKONING: Reasoning through Dynamic Knowledge Encoding","abstract":"Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (i.e., in-context reasoning). However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (i.e., not necessarily random noise). In these situations, the model fails to distinguish the knowledge that is necessary to answer the question, leading to spurious reasoning and degraded performance. This reasoning failure contrasts with the model's apparent ability to distinguish its contextual knowledge from all the knowledge it has memorized during pre-training. Following this observation, we propose teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question. Our method, RECKONING, is a bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge through back-propagation, allowing them to then answer questions using the updated parameters. During training, the inner loop rapidly adapts a copy of the model weights to encode contextual knowledge into its parameters. In the outer loop, the model learns to uses the updated weights to reproduce and answer reasoning questions about the memorized knowledge. Our experiments on two multi-hop reasoning datasets show that RECKONING's performance improves over the in-context reasoning baseline (by up to 4.5%). We also find that compared to in-context reasoning, RECKONING generalizes better to longer reasoning chains unseen during training, is more robust to distractors in the context, and is more computationally efficient when multiple questions are asked about the same knowledge.","sentences":["Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (i.e., in-context reasoning).","However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (i.e., not necessarily random noise).","In these situations, the model fails to distinguish the knowledge that is necessary to answer the question, leading to spurious reasoning and degraded performance.","This reasoning failure contrasts with the model's apparent ability to distinguish its contextual knowledge from all the knowledge it has memorized during pre-training.","Following this observation, we propose teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question.","Our method, RECKONING, is a bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge through back-propagation, allowing them to then answer questions using the updated parameters.","During training, the inner loop rapidly adapts a copy of the model weights to encode contextual knowledge into its parameters.","In the outer loop, the model learns to uses the updated weights to reproduce and answer reasoning questions about the memorized knowledge.","Our experiments on two multi-hop reasoning datasets show that RECKONING's performance improves over the in-context reasoning baseline (by up to 4.5%).","We also find that compared to in-context reasoning, RECKONING generalizes better to longer reasoning chains unseen during training, is more robust to distractors in the context, and is more computationally efficient when multiple questions are asked about the same knowledge."],"url":"http://arxiv.org/abs/2305.06349v1"}
{"created":"2023-05-10","title":"Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs","abstract":"Vision and Language (VL) models have demonstrated remarkable zero-shot performance in a variety of tasks. However, recent studies have shown that even the best VL models struggle to capture aspects of scene understanding, such as object attributes, relationships, and action states. In contrast, obtaining structured annotations, e.g., scene graphs (SGs) that could improve these models is time-consuming, costly, and tedious, and thus cannot be used on a large scale. Here we ask, can small datasets containing SG annotations provide sufficient information for enhancing structured understanding of VL models? We show that it is indeed possible to improve VL models using such data by utilizing a specialized model architecture and a new training paradigm. Our approach captures structure-related information for both the visual and textual encoders by directly supervising both components when learning from SG labels. We use scene graph supervision to generate fine-grained captions based on various graph augmentations highlighting different compositional aspects of the scene, and to predict SG information using an open vocabulary approach by adding special ``Adaptive SG tokens'' to the visual encoder. Moreover, we design a new adaptation technique tailored specifically to the SG tokens that allows better learning of the graph prediction task while still maintaining zero-shot capabilities. Our model shows strong performance improvements on the Winoground and VL-checklist datasets with only a mild degradation in zero-shot performance.","sentences":["Vision and Language (VL) models have demonstrated remarkable zero-shot performance in a variety of tasks.","However, recent studies have shown that even the best VL models struggle to capture aspects of scene understanding, such as object attributes, relationships, and action states.","In contrast, obtaining structured annotations, e.g., scene graphs (SGs) that could improve these models is time-consuming, costly, and tedious, and thus cannot be used on a large scale.","Here we ask, can small datasets containing SG annotations provide sufficient information for enhancing structured understanding of VL models?","We show that it is indeed possible to improve VL models using such data by utilizing a specialized model architecture and a new training paradigm.","Our approach captures structure-related information for both the visual and textual encoders by directly supervising both components when learning from SG labels.","We use scene graph supervision to generate fine-grained captions based on various graph augmentations highlighting different compositional aspects of the scene, and to predict SG information using an open vocabulary approach by adding special ``Adaptive SG tokens'' to the visual encoder.","Moreover, we design a new adaptation technique tailored specifically to the SG tokens that allows better learning of the graph prediction task while still maintaining zero-shot capabilities.","Our model shows strong performance improvements on the Winoground and VL-checklist datasets with only a mild degradation in zero-shot performance."],"url":"http://arxiv.org/abs/2305.06343v1"}
{"created":"2023-05-10","title":"K-UniMorph: Korean Universal Morphology and its Feature Schema","abstract":"We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from Sylak-Glassman et al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus. Furthermore, we carry out the inflection task using three different Korean word forms: letters, syllables and morphemes. Finally, we discuss and describe future perspectives on Korean morphological paradigms and the dataset.","sentences":["We present in this work a new Universal Morphology dataset for Korean.","Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages.","Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics.","For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata.","This dataset adopts morphological feature schema from Sylak-Glassman et al.","(2015) and Sylak-Glassman (2016) for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean.","During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus.","Furthermore, we carry out the inflection task using three different Korean word forms: letters, syllables and morphemes.","Finally, we discuss and describe future perspectives on Korean morphological paradigms and the dataset."],"url":"http://arxiv.org/abs/2305.06335v1"}
{"created":"2023-05-10","title":"Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks","abstract":"Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge. Unlike mesh-based models, they require watertight geometry and object-wise semantics at the fa\\c{c}ade level. The principal challenge of such demanding semantic 3D reconstruction is reliable fa\\c{c}ade-level semantic segmentation of 3D input data. We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving fa\\c{c}ade-level semantic 3D segmentation. To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts. These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a library of fa\\c{c}ade objects. Extensive experiments on the TUM city campus datasets demonstrate the superior performance of the proposed Scan2LoD3 over the state-of-the-art methods in fa\\c{c}ade-level detection, semantic segmentation, and LoD3 building model reconstruction. We believe our method can foster the development of probability-driven semantic 3D reconstruction at LoD3 since not only the high-definition reconstruction but also reconstruction confidence becomes pivotal for various applications such as autonomous driving and urban simulations.","sentences":["Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge.","Unlike mesh-based models, they require watertight geometry and object-wise semantics at the fa\\c{c}ade level.","The principal challenge of such demanding semantic 3D reconstruction is reliable fa\\c{c}ade-level semantic segmentation of 3D input data.","We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving fa\\c{c}ade-level semantic 3D segmentation.","To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts.","These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images.","To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a library of fa\\c{c}ade objects.","Extensive experiments on the TUM city campus datasets demonstrate the superior performance of the proposed Scan2LoD3 over the state-of-the-art methods in fa\\c{c}ade-level detection, semantic segmentation, and LoD3 building model reconstruction.","We believe our method can foster the development of probability-driven semantic 3D reconstruction at LoD3 since not only the high-definition reconstruction but also reconstruction confidence becomes pivotal for various applications such as autonomous driving and urban simulations."],"url":"http://arxiv.org/abs/2305.06314v1"}
{"created":"2023-05-10","title":"Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning","abstract":"Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis. Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies. Moreover, their updates are long and expensive, making them unsuitable to emerging practices. Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis. Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers. We experimented on a synthetic but realistic dataset to differentially diagnose anemia and its subtypes and particularly evaluated the robustness of various approaches to noise and missing data as those are frequent in EHRs. Within the DRL algorithms, Dueling DQN with Prioritized Experience Replay, and Dueling Double DQN with Prioritized Experience Replay show the best and most stable performances. In the presence of imperfect data, the DRL algorithms show competitive, but less stable performances when compared to the classifiers (Random Forest and XGBoost); although they enable the progressive generation of a pathway to the suggested diagnosis, which can both guide or explain the decision process.","sentences":["Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis.","Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies.","Moreover, their updates are long and expensive, making them unsuitable to emerging practices.","Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis.","Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers.","We experimented on a synthetic but realistic dataset to differentially diagnose anemia and its subtypes and particularly evaluated the robustness of various approaches to noise and missing data as those are frequent in EHRs.","Within the DRL algorithms, Dueling DQN with Prioritized Experience Replay, and Dueling Double DQN with Prioritized Experience Replay show the best and most stable performances.","In the presence of imperfect data, the DRL algorithms show competitive, but less stable performances when compared to the classifiers (Random Forest and XGBoost); although they enable the progressive generation of a pathway to the suggested diagnosis, which can both guide or explain the decision process."],"url":"http://arxiv.org/abs/2305.06295v1"}
{"created":"2023-05-10","title":"Joint Metrics Matter: A Better Standard for Trajectory Forecasting","abstract":"Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents. Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group. Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research. In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the-art (SOTA) trajectory forecasting methods with respect to multi-agent metrics (joint metrics): JADE, JFDE, and collision rate. We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn from the ETH / UCY and Stanford Drone datasets. We introduce a new loss function incorporating joint metrics that, when applied to a SOTA trajectory forecasting method, achieves a 7% improvement in JADE / JFDE on the ETH / UCY datasets with respect to the previous SOTA. Our results also indicate that optimizing for joint metrics naturally leads to an improvement in interaction modeling, as evidenced by a 16% decrease in mean collision rate on the ETH / UCY datasets with respect to the previous SOTA.","sentences":["Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents.","Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group.","Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research.","In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the-art (SOTA) trajectory forecasting methods with respect to multi-agent metrics (joint metrics): JADE, JFDE, and collision rate.","We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn from the ETH / UCY and Stanford Drone datasets.","We introduce a new loss function incorporating joint metrics that, when applied to a SOTA trajectory forecasting method, achieves a 7% improvement in JADE / JFDE on the ETH / UCY datasets with respect to the previous SOTA.","Our results also indicate that optimizing for joint metrics naturally leads to an improvement in interaction modeling, as evidenced by a 16% decrease in mean collision rate on the ETH / UCY datasets with respect to the previous SOTA."],"url":"http://arxiv.org/abs/2305.06292v1"}
{"created":"2023-05-10","title":"A Multi-modal Garden Dataset and Hybrid 3D Dense Reconstruction Framework Based on Panoramic Stereo Images for a Trimming Robot","abstract":"Recovering an outdoor environment's surface mesh is vital for an agricultural robot during task planning and remote visualization. Our proposed solution is based on a newly-designed panoramic stereo camera along with a hybrid novel software framework that consists of three fusion modules. The panoramic stereo camera with a pentagon shape consists of 5 stereo vision camera pairs to stream synchronized panoramic stereo images for the following three fusion modules. In the disparity fusion module, rectified stereo images produce the initial disparity maps using multiple stereo vision algorithms. Then, these initial disparity maps, along with the intensity images, are input into a disparity fusion network to produce refined disparity maps. Next, the refined disparity maps are converted into full-view point clouds or single-view point clouds for the pose fusion module. The pose fusion module adopts a two-stage global-coarse-to-local-fine strategy. In the first stage, each pair of full-view point clouds is registered by a global point cloud matching algorithm to estimate the transformation for a global pose graph's edge, which effectively implements loop closure. In the second stage, a local point cloud matching algorithm is used to match single-view point clouds in different nodes. Next, we locally refine the poses of all corresponding edges in the global pose graph using three proposed rules, thus constructing a refined pose graph. The refined pose graph is optimized to produce a global pose trajectory for volumetric fusion. In the volumetric fusion module, the global poses of all the nodes are used to integrate the single-view point clouds into the volume to produce the mesh of the whole garden. The proposed framework and its three fusion modules are tested on a real outdoor garden dataset to show the superiority of the performance.","sentences":["Recovering an outdoor environment's surface mesh is vital for an agricultural robot during task planning and remote visualization.","Our proposed solution is based on a newly-designed panoramic stereo camera along with a hybrid novel software framework that consists of three fusion modules.","The panoramic stereo camera with a pentagon shape consists of 5 stereo vision camera pairs to stream synchronized panoramic stereo images for the following three fusion modules.","In the disparity fusion module, rectified stereo images produce the initial disparity maps using multiple stereo vision algorithms.","Then, these initial disparity maps, along with the intensity images, are input into a disparity fusion network to produce refined disparity maps.","Next, the refined disparity maps are converted into full-view point clouds or single-view point clouds for the pose fusion module.","The pose fusion module adopts a two-stage global-coarse-to-local-fine strategy.","In the first stage, each pair of full-view point clouds is registered by a global point cloud matching algorithm to estimate the transformation for a global pose graph's edge, which effectively implements loop closure.","In the second stage, a local point cloud matching algorithm is used to match single-view point clouds in different nodes.","Next, we locally refine the poses of all corresponding edges in the global pose graph using three proposed rules, thus constructing a refined pose graph.","The refined pose graph is optimized to produce a global pose trajectory for volumetric fusion.","In the volumetric fusion module, the global poses of all the nodes are used to integrate the single-view point clouds into the volume to produce the mesh of the whole garden.","The proposed framework and its three fusion modules are tested on a real outdoor garden dataset to show the superiority of the performance."],"url":"http://arxiv.org/abs/2305.06278v1"}
{"created":"2023-05-10","title":"Flexible cost-penalized Bayesian model selection: developing inclusion paths with an application to diagnosis of heart disease","abstract":"We propose a Bayesian model selection approach that allows medical practitioners to select among predictor variables while taking their respective costs into account. Medical procedures almost always incur costs in time and/or money. These costs might exceed their usefulness for modeling the outcome of interest. We develop Bayesian model selection that uses flexible model priors to penalize costly predictors a priori and select a subset of predictors useful relative to their costs. Our approach (i) gives the practitioner control over the magnitude of cost penalization, (ii) enables the prior to scale well with sample size, and (iii) enables the creation of our proposed inclusion path visualization, which can be used to make decisions about individual candidate predictors using both probabilistic and visual tools. We demonstrate the effectiveness of our inclusion path approach and the importance of being able to adjust the magnitude of the prior's cost penalization through a dataset pertaining to heart disease diagnosis in patients at the Cleveland Clinic Foundation, where several candidate predictors with various costs were recorded for patients, and through simulated data.","sentences":["We propose a Bayesian model selection approach that allows medical practitioners to select among predictor variables while taking their respective costs into account.","Medical procedures almost always incur costs in time and/or money.","These costs might exceed their usefulness for modeling the outcome of interest.","We develop Bayesian model selection that uses flexible model priors to penalize costly predictors a priori and select a subset of predictors useful relative to their costs.","Our approach (i) gives the practitioner control over the magnitude of cost penalization, (ii) enables the prior to scale well with sample size, and (iii) enables the creation of our proposed inclusion path visualization, which can be used to make decisions about individual candidate predictors using both probabilistic and visual tools.","We demonstrate the effectiveness of our inclusion path approach and the importance of being able to adjust the magnitude of the prior's cost penalization through a dataset pertaining to heart disease diagnosis in patients at the Cleveland Clinic Foundation, where several candidate predictors with various costs were recorded for patients, and through simulated data."],"url":"http://arxiv.org/abs/2305.06262v1"}
{"created":"2023-05-10","title":"Supervised learning with probabilistic morphisms and kernel mean embeddings","abstract":"In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\\mathcal{X}$ and a label space $\\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\\mu_{\\mathcal{Y}|\\mathcal{X}}$ for a probability measure $\\mu$ on $\\mathcal{X} \\times \\mathcal{Y}$ relative to the projection $\\Pi_{\\mathcal{X}}: \\mathcal{X}\\times\\mathcal{Y}\\to \\mathcal{X}$ as a solution of a linear operator equation. If $\\mathcal{Y}$ is a separable metrizable topological space with the Borel $\\sigma$-algebra $ \\mathcal{B} (\\mathcal{Y})$, I propose another characterization of a regular conditional probability measure $\\mu_{\\mathcal{Y}|\\mathcal{X}}$ as a minimizer of a mean square error on the space of Markov kernels, called probabilistic morphisms, from $\\mathcal{X}$ to $\\mathcal{Y}$, using kernel mean embedding. Using these results and using inner measure to quantify generalizability of a learning algorithm, I give a generalization of a result due to Cucker-Smale, which concerns the learnability of a regression model, to a setting of a conditional probability estimation problem. I also give a variant of Vapnik's method of solving stochastic ill-posed problem, using inner measure and discuss its applications.","sentences":["In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\\mathcal{X}$ and a label space $\\mathcal{Y}$, which are measurable spaces.","A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\\mu_{\\mathcal{Y}|\\mathcal{X}}$ for a probability measure $\\mu$ on $\\mathcal{X} \\times \\mathcal{Y}$ relative to the projection $\\Pi_{\\mathcal{X}}: \\mathcal{X}\\times\\mathcal{Y}\\to \\mathcal{X}$ as a solution of a linear operator equation.","If $\\mathcal{Y}$ is a separable metrizable topological space with the Borel $\\sigma$-algebra $ \\mathcal{B} (\\mathcal{Y})$, I propose another characterization of a regular conditional probability measure $\\mu_{\\mathcal{Y}|\\mathcal{X}}$ as a minimizer of a mean square error on the space of Markov kernels, called probabilistic morphisms, from $\\mathcal{X}$ to $\\mathcal{Y}$, using kernel mean embedding.","Using these results and using inner measure to quantify generalizability of a learning algorithm, I give a generalization of a result due to Cucker-Smale, which concerns the learnability of a regression model, to a setting of a conditional probability estimation problem.","I also give a variant of Vapnik's method of solving stochastic ill-posed problem, using inner measure and discuss its applications."],"url":"http://arxiv.org/abs/2305.06348v1"}
{"created":"2023-05-10","title":"Self-Supervised Instance Segmentation by Grasping","abstract":"Instance segmentation is a fundamental skill for many robotic applications. We propose a self-supervised method that uses grasp interactions to collect segmentation supervision for an instance segmentation model. When a robot grasps an item, the mask of that grasped item can be inferred from the images of the scene before and after the grasp. Leveraging this insight, we learn a grasp segmentation model to segment the grasped object from before and after grasp images. Such a model can segment grasped objects from thousands of grasp interactions without costly human annotation. Using the segmented grasped objects, we can \"cut\" objects from their original scenes and \"paste\" them into new scenes to generate instance supervision. We show that our grasp segmentation model provides a 5x error reduction when segmenting grasped objects compared with traditional image subtraction approaches. Combined with our \"cut-and-paste\" generation method, instance segmentation models trained with our method achieve better performance than a model trained with 10x the amount of labeled data. On a real robotic grasping system, our instance segmentation model reduces the rate of grasp errors by over 3x compared to an image subtraction baseline.","sentences":["Instance segmentation is a fundamental skill for many robotic applications.","We propose a self-supervised method that uses grasp interactions to collect segmentation supervision for an instance segmentation model.","When a robot grasps an item, the mask of that grasped item can be inferred from the images of the scene before and after the grasp.","Leveraging this insight, we learn a grasp segmentation model to segment the grasped object from before and after grasp images.","Such a model can segment grasped objects from thousands of grasp interactions without costly human annotation.","Using the segmented grasped objects, we can \"cut\" objects from their original scenes and \"paste\" them into new scenes to generate instance supervision.","We show that our grasp segmentation model provides a 5x error reduction when segmenting grasped objects compared with traditional image subtraction approaches.","Combined with our \"cut-and-paste\" generation method, instance segmentation models trained with our method achieve better performance than a model trained with 10x the amount of labeled data.","On a real robotic grasping system, our instance segmentation model reduces the rate of grasp errors by over 3x compared to an image subtraction baseline."],"url":"http://arxiv.org/abs/2305.06305v1"}
{"created":"2023-05-10","title":"HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion","abstract":"Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis.","sentences":["Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing.","To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints.","Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition.","This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion.","While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP.","To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions.","We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis."],"url":"http://arxiv.org/abs/2305.06356v1"}
{"created":"2023-05-10","title":"Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)","abstract":"Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine. In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision. We consider both single- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to \\emph{synthesize} evidence reported across a collection of articles. We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries. We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents. We release all data and annotations used in this work.","sentences":["Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in few- and zero-shot settings.","However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine.","In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision.","We consider both single- and multi-document settings.","In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to \\emph{synthesize} evidence reported across a collection of articles.","We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries.","We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents.","We release all data and annotations used in this work."],"url":"http://arxiv.org/abs/2305.06299v1"}
{"created":"2023-05-10","title":"Cost-benefit of green infrastructures for water management: A sustainability assessment of full-scale constructed wetlands in Northern and Southern Italy","abstract":"Sustainable water management has become an urgent challenge due to irregular water availability patterns and water quality issues. The effect of climate change exacerbates this phenomenon in water-scarce areas, such as the Mediterranean region, stimulating the implementation of solutions aiming to mitigate or improve environmental, social, and economic conditions. A novel solution inspired by nature, technology-oriented, explored in the past years, is constructed wetlands. Commonly applied for different types of wastewater due to its low cost and simple maintenance, they are considered a promising solution to remove pollutants while creating an improved ecosystem by increasing biodiversity around them. This research aims to assess the sustainability of two typologies of constructed wetlands in two Italian areas: Sicily, with a vertical subsurface flow constructed wetland, and Emilia Romagna, with a surface flow constructed wetland. The assessment is performed by applying a cost-benefit analysis combining primary and secondary data sources. The analysis considered the market and non-market values in both proposed scenarios to establish the feasibility of the two options and identify the most convenient one. Results show that both constructed wetlands bring more benefits (benefits-cost ratio, BCR) than costs (BCR > 0). In the case of Sicily, the BCR is lower (1) in the constructed wetland scenario, while in its absence it is almost double. If other ecosystem services are included the constructed wetland scenario reach a BCR of 4 and a ROI of 5, showing a better performance from a costing perspective than the absence one. In Emilia Romagna, the constructed wetland scenario shows a high BCR (10) and ROI (9), while the scenario in absence has obtained a negative present value indicating that the cost do not cover the benefits expected.","sentences":["Sustainable water management has become an urgent challenge due to irregular water availability patterns and water quality issues.","The effect of climate change exacerbates this phenomenon in water-scarce areas, such as the Mediterranean region, stimulating the implementation of solutions aiming to mitigate or improve environmental, social, and economic conditions.","A novel solution inspired by nature, technology-oriented, explored in the past years, is constructed wetlands.","Commonly applied for different types of wastewater due to its low cost and simple maintenance, they are considered a promising solution to remove pollutants while creating an improved ecosystem by increasing biodiversity around them.","This research aims to assess the sustainability of two typologies of constructed wetlands in two Italian areas: Sicily, with a vertical subsurface flow constructed wetland, and Emilia Romagna, with a surface flow constructed wetland.","The assessment is performed by applying a cost-benefit analysis combining primary and secondary data sources.","The analysis considered the market and non-market values in both proposed scenarios to establish the feasibility of the two options and identify the most convenient one.","Results show that both constructed wetlands bring more benefits (benefits-cost ratio, BCR) than costs (BCR > 0).","In the case of Sicily, the BCR is lower (1) in the constructed wetland scenario, while in its absence it is almost double.","If other ecosystem services are included the constructed wetland scenario reach a BCR of 4 and a ROI of 5, showing a better performance from a costing perspective than the absence one.","In Emilia Romagna, the constructed wetland scenario shows a high BCR (10) and ROI (9), while the scenario in absence has obtained a negative present value indicating that the cost do not cover the benefits expected."],"url":"http://arxiv.org/abs/2305.06284v1"}
{"created":"2023-05-10","title":"View Correspondence Network for Implicit Light Field Representation","abstract":"We present a novel technique for implicit neural representation of light fields at continuously defined viewpoints with high quality and fidelity. Our implicit neural representation maps 4D coordinates defining two-plane parameterization of the light fields to the corresponding color values. We leverage periodic activations to achieve high expressivity and accurate reconstruction for complex data manifolds while keeping low storage and inference time requirements. However, na\\\"ively trained non-3D structured networks do not adequately satisfy the multi-view consistency; instead, they perform alpha blending of nearby viewpoints. In contrast, our View Correspondence Network, or VICON, leverages stereo matching, optimization by automatic differentiation with respect to the input space, and multi-view pixel correspondence to provide a novel implicit representation of the light fields faithful to the novel views that are unseen during the training. Experimental results show VICON superior to the state-of-the-art non-3D implicit light field representations both qualitatively and quantitatively. Moreover, our implicit representation captures a larger field of view (FoV), surpassing the extent of the observable scene by the cameras of the ground truth renderings.","sentences":["We present a novel technique for implicit neural representation of light fields at continuously defined viewpoints with high quality and fidelity.","Our implicit neural representation maps 4D coordinates defining two-plane parameterization of the light fields to the corresponding color values.","We leverage periodic activations to achieve high expressivity and accurate reconstruction for complex data manifolds while keeping low storage and inference time requirements.","However, na\\\"ively trained non-3D structured networks do not adequately satisfy the multi-view consistency; instead, they perform alpha blending of nearby viewpoints.","In contrast, our View Correspondence Network, or VICON, leverages stereo matching, optimization by automatic differentiation with respect to the input space, and multi-view pixel correspondence to provide a novel implicit representation of the light fields faithful to the novel views that are unseen during the training.","Experimental results show VICON superior to the state-of-the-art non-3D implicit light field representations both qualitatively and quantitatively.","Moreover, our implicit representation captures a larger field of view (FoV), surpassing the extent of the observable scene by the cameras of the ground truth renderings."],"url":"http://arxiv.org/abs/2305.06233v1"}
{"created":"2023-05-10","title":"RiverBench: an Open RDF Streaming Benchmark Suite","abstract":"RDF data streaming has been explored by the Semantic Web community from many angles, resulting in multiple task formulations and streaming methods. However, for many existing formulations of the problem, reliably benchmarking streaming solutions has been challenging due to the lack of well-described and appropriately diverse benchmark datasets. Existing datasets and evaluations, except a few notable cases, suffer from unclear streaming task scopes, underspecified benchmarks, and errors in the data. To address these issues, we firstly systematize the different RDF data streaming tasks in a clear taxonomy and outline practical requirements for benchmark datasets. We then propose RiverBench, an open and collaborative RDF streaming benchmark suite that applies these principles in practice. RiverBench leverages continuous, community-driven processes, established best practices (e.g., FAIR), and built-in quality guarantees. The suite distributes datasets in a common, accessible format, with clear documentation, licensing, and machine-readable metadata. The current release includes a diverse collection of non-synthetic datasets generated by the Semantic Web community, representing many applications of RDF data streaming, all major task formulations, and emerging RDF features (RDF-star). Finally, we present a list of research applications for the suite, demonstrating its versatility and value even beyond the realm of RDF streaming.","sentences":["RDF data streaming has been explored by the Semantic Web community from many angles, resulting in multiple task formulations and streaming methods.","However, for many existing formulations of the problem, reliably benchmarking streaming solutions has been challenging due to the lack of well-described and appropriately diverse benchmark datasets.","Existing datasets and evaluations, except a few notable cases, suffer from unclear streaming task scopes, underspecified benchmarks, and errors in the data.","To address these issues, we firstly systematize the different RDF data streaming tasks in a clear taxonomy and outline practical requirements for benchmark datasets.","We then propose RiverBench, an open and collaborative RDF streaming benchmark suite that applies these principles in practice.","RiverBench leverages continuous, community-driven processes, established best practices (e.g., FAIR), and built-in quality guarantees.","The suite distributes datasets in a common, accessible format, with clear documentation, licensing, and machine-readable metadata.","The current release includes a diverse collection of non-synthetic datasets generated by the Semantic Web community, representing many applications of RDF data streaming, all major task formulations, and emerging RDF features (RDF-star).","Finally, we present a list of research applications for the suite, demonstrating its versatility and value even beyond the realm of RDF streaming."],"url":"http://arxiv.org/abs/2305.06226v1"}
{"created":"2023-05-10","title":"Differential Privacy for Protecting Private Patterns in Data Streams","abstract":"Complex event processing (CEP) is a powerful and increasingly more important tool to analyse data streams for Internet of Things (IoT) applications. These data streams often contain private information that requires proper protection. However, privacy protection in CEP systems is still in its infancy, and most existing privacy-preserving mechanisms (PPMs) are adopted from those designed for data streams. Such approaches undermine the quality of the entire data stream and limit the performance of IoT applications. In this paper, we attempt to break the limitation and establish a new foundation for PPMs of CEP by proposing a novel pattern-level differential privacy (DP) guarantee. We introduce two PPMs that guarantee pattern-level DP. They operate only on data that correlate with private patterns rather than on the entire data stream, leading to higher data quality. One of the PPMs provides adaptive privacy protection and brings more granularity and generalization. We evaluate the performance of the proposed PPMs with two experiments on a real-world dataset and on a synthetic dataset. The results of the experiments indicate that our proposed privacy guarantee and its PPMs can deliver better data quality under equally strong privacy guarantees, compared to multiple well-known PPMs designed for data streams.","sentences":["Complex event processing (CEP) is a powerful and increasingly more important tool to analyse data streams for Internet of Things (IoT) applications.","These data streams often contain private information that requires proper protection.","However, privacy protection in CEP systems is still in its infancy, and most existing privacy-preserving mechanisms (PPMs) are adopted from those designed for data streams.","Such approaches undermine the quality of the entire data stream and limit the performance of IoT applications.","In this paper, we attempt to break the limitation and establish a new foundation for PPMs of CEP by proposing a novel pattern-level differential privacy (DP) guarantee.","We introduce two PPMs that guarantee pattern-level DP.","They operate only on data that correlate with private patterns rather than on the entire data stream, leading to higher data quality.","One of the PPMs provides adaptive privacy protection and brings more granularity and generalization.","We evaluate the performance of the proposed PPMs with two experiments on a real-world dataset and on a synthetic dataset.","The results of the experiments indicate that our proposed privacy guarantee and its PPMs can deliver better data quality under equally strong privacy guarantees, compared to multiple well-known PPMs designed for data streams."],"url":"http://arxiv.org/abs/2305.06105v1"}
{"created":"2023-05-10","title":"Few-shot Link Prediction on N-ary Facts","abstract":"N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation meta information from limited instances to predict a missing entity in a query instance. To validate the effectiveness of FLEN, we construct three datasets based on existing benchmark data. Our experimental results show that FLEN significantly outperforms existing related models in both few-shot link prediction on n-ary facts and binary facts.","sentences":["N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs).","Link prediction on n-ary facts is to predict a missing element in an n-ary fact.","This helps populate and enrich KGs and further promotes numerous downstream applications.","Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts.","However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios.","Thus, this paper introduces a new task, few-shot link prediction on n-ary facts.","It aims to predict a missing entity in an n-ary fact with limited labeled instances.","We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules.","FLEN captures relation meta information from limited instances to predict a missing entity in a query instance.","To validate the effectiveness of FLEN, we construct three datasets based on existing benchmark data.","Our experimental results show that FLEN significantly outperforms existing related models in both few-shot link prediction on n-ary facts and binary facts."],"url":"http://arxiv.org/abs/2305.06104v1"}
{"created":"2023-05-10","title":"Building Interoperable Electronic Health Records as Purpose-Driven Knowledge Graphs","abstract":"When building a new application we are increasingly confronted with the need of reusing and integrating pre-existing knowledge. Nevertheless, it is a fact that this prior knowledge is virtually impossible to reuse as-is. This is true also in domains, e.g., eHealth, where a lot of effort has been put into developing high-quality standards and reference ontologies, e.g. FHIR1. In this paper, we propose an integrated methodology, called iTelos, which enables data and knowledge reuse towards the construction of Interoperable Electronic Health Records (iEHR). The key intuition is that the data level and the schema level of an application should be developed independently, thus allowing for maximum flexibility in the reuse of the prior knowledge, but under the overall guidance of the needs to be satisfied, formalized as competence queries. This intuition is implemented by codifying all the requirements, including those concerning reuse, as part of a purpose defined a priori, which is then used to drive a middle-out development process where the application schema and data are continuously aligned. The proposed methodology is validated through its application to a large-scale case study.","sentences":["When building a new application we are increasingly confronted with the need of reusing and integrating pre-existing knowledge.","Nevertheless, it is a fact that this prior knowledge is virtually impossible to reuse as-is.","This is true also in domains, e.g., eHealth, where a lot of effort has been put into developing high-quality standards and reference ontologies, e.g. FHIR1.","In this paper, we propose an integrated methodology, called iTelos, which enables data and knowledge reuse towards the construction of Interoperable Electronic Health Records (iEHR).","The key intuition is that the data level and the schema level of an application should be developed independently, thus allowing for maximum flexibility in the reuse of the prior knowledge, but under the overall guidance of the needs to be satisfied, formalized as competence queries.","This intuition is implemented by codifying all the requirements, including those concerning reuse, as part of a purpose defined a priori, which is then used to drive a middle-out development process where the application schema and data are continuously aligned.","The proposed methodology is validated through its application to a large-scale case study."],"url":"http://arxiv.org/abs/2305.06088v1"}
{"created":"2023-05-10","title":"ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base","abstract":"Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.","sentences":["Analogical reasoning is a fundamental cognitive ability of humans.","However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training.","In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs).","ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control.","Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2305.05994v1"}
{"created":"2023-05-10","title":"Uncertainty-Aware Semi-Supervised Learning for Prostate MRI Zonal Segmentation","abstract":"Quality of deep convolutional neural network predictions strongly depends on the size of the training dataset and the quality of the annotations. Creating annotations, especially for 3D medical image segmentation, is time-consuming and requires expert knowledge. We propose a novel semi-supervised learning (SSL) approach that requires only a relatively small number of annotations while being able to use the remaining unlabeled data to improve model performance. Our method uses a pseudo-labeling technique that employs recent deep learning uncertainty estimation models. By using the estimated uncertainty, we were able to rank pseudo-labels and automatically select the best pseudo-annotations generated by the supervised model. We applied this to prostate zonal segmentation in T2-weighted MRI scans. Our proposed model outperformed the semi-supervised model in experiments with the ProstateX dataset and an external test set, by leveraging only a subset of unlabeled data rather than the full collection of 4953 cases, our proposed model demonstrated improved performance. The segmentation dice similarity coefficient in the transition zone and peripheral zone increased from 0.835 and 0.727 to 0.852 and 0.751, respectively, for fully supervised model and the uncertainty-aware semi-supervised learning model (USSL). Our USSL model demonstrates the potential to allow deep learning models to be trained on large datasets without requiring full annotation. Our code is available at https://github.com/DIAGNijmegen/prostateMR-USSL.","sentences":["Quality of deep convolutional neural network predictions strongly depends on the size of the training dataset and the quality of the annotations.","Creating annotations, especially for 3D medical image segmentation, is time-consuming and requires expert knowledge.","We propose a novel semi-supervised learning (SSL) approach that requires only a relatively small number of annotations while being able to use the remaining unlabeled data to improve model performance.","Our method uses a pseudo-labeling technique that employs recent deep learning uncertainty estimation models.","By using the estimated uncertainty, we were able to rank pseudo-labels and automatically select the best pseudo-annotations generated by the supervised model.","We applied this to prostate zonal segmentation in T2-weighted MRI scans.","Our proposed model outperformed the semi-supervised model in experiments with the ProstateX dataset and an external test set, by leveraging only a subset of unlabeled data rather than the full collection of 4953 cases, our proposed model demonstrated improved performance.","The segmentation dice similarity coefficient in the transition zone and peripheral zone increased from 0.835 and 0.727 to 0.852 and 0.751, respectively, for fully supervised model and the uncertainty-aware semi-supervised learning model (USSL).","Our USSL model demonstrates the potential to allow deep learning models to be trained on large datasets without requiring full annotation.","Our code is available at https://github.com/DIAGNijmegen/prostateMR-USSL."],"url":"http://arxiv.org/abs/2305.05984v1"}
{"created":"2023-05-10","title":"DPMLBench: Holistic Evaluation of Differentially Private Machine Learning","abstract":"Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection. Combined with powerful machine learning techniques, differentially private machine learning (DPML) is increasingly important. As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice. Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss. However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms. More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.   We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks. We first present a taxonomy of where improvements are located in the machine learning life cycle. Based on our taxonomy, we jointly perform an extensive measurement study of the improved DPML algorithms. We also cover state-of-the-art label differential privacy (Label DP) algorithms in the evaluation. According to our empirical results, DP can effectively defend against MIAs, and sensitivity-bounding techniques such as per-sample gradient clipping play an important role in defense. We also explore some improvements that can maintain model utility and defend against MIAs more effectively. Experiments show that Label DP algorithms achieve less utility loss but are fragile to MIAs. To support our evaluation, we implement a modular re-usable software, DPMLBench, which enables sensitive data owners to deploy DPML algorithms and serves as a benchmark tool for researchers and practitioners.","sentences":["Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection.","Combined with powerful machine learning techniques, differentially private machine learning (DPML) is increasingly important.","As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice.","Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss.","However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms.","More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.   ","We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks.","We first present a taxonomy of where improvements are located in the machine learning life cycle.","Based on our taxonomy, we jointly perform an extensive measurement study of the improved DPML algorithms.","We also cover state-of-the-art label differential privacy (Label DP) algorithms in the evaluation.","According to our empirical results, DP can effectively defend against MIAs, and sensitivity-bounding techniques such as per-sample gradient clipping play an important role in defense.","We also explore some improvements that can maintain model utility and defend against MIAs more effectively.","Experiments show that Label DP algorithms achieve less utility loss but are fragile to MIAs.","To support our evaluation, we implement a modular re-usable software, DPMLBench, which enables sensitive data owners to deploy DPML algorithms and serves as a benchmark tool for researchers and practitioners."],"url":"http://arxiv.org/abs/2305.05900v1"}
{"created":"2023-05-10","title":"Automatic Evaluation of Attribution by Large Language Models","abstract":"A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims. However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution by LLMs. We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization. To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on the curated test set and simulated test examples from existing benchmark questions highlight both promising signals as well as remaining challenges for the automatic evaluation of attribution. We hope our testbed, modeling methodology, and insights will help lay the foundation for future studies on this important problem.","sentences":["A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims.","However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem.","Although human evaluation is common practice, it is costly and time-consuming.","In this paper, we investigate the automatic evaluation of attribution by LLMs.","We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs.","The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization.","To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing.","Our results on the curated test set and simulated test examples from existing benchmark questions highlight both promising signals as well as remaining challenges for the automatic evaluation of attribution.","We hope our testbed, modeling methodology, and insights will help lay the foundation for future studies on this important problem."],"url":"http://arxiv.org/abs/2305.06311v1"}
{"created":"2023-05-10","title":"When ChatGPT for Computer Vision Will Come? From 2D to 3D","abstract":"ChatGPT and its improved variant GPT4 have revolutionized the NLP field with a single model solving almost all text related tasks. However, such a model for computer vision does not exist, especially for 3D vision. This article first provides a brief view on the progress of deep learning in text, image and 3D fields from the model perspective. Moreover, this work further discusses how AIGC evolves from the data perspective. On top of that, this work presents an outlook on the development of AIGC in 3D from the data perspective.","sentences":["ChatGPT and its improved variant GPT4 have revolutionized the NLP field with a single model solving almost all text related tasks.","However, such a model for computer vision does not exist, especially for 3D vision.","This article first provides a brief view on the progress of deep learning in text, image and 3D fields from the model perspective.","Moreover, this work further discusses how AIGC evolves from the data perspective.","On top of that, this work presents an outlook on the development of AIGC in 3D from the data perspective."],"url":"http://arxiv.org/abs/2305.06133v1"}
{"created":"2023-05-10","title":"A Glimpse in ChatGPT Capabilities and its impact for AI research","abstract":"Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development. These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering. However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA. In this paper, we will examine the impact of LLMs on AI research. The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing. We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range of capabilities in a single system is a strong sign of approaching general intelligence. Innovations integrating such models will also expand along the maturation of such AI systems and exhibit unforeseeable applications that will have important impacts on several aspects of our societies.","sentences":["Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development.","These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering.","However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA.","In this paper, we will examine the impact of LLMs on AI research.","The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing.","We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range of capabilities in a single system is a strong sign of approaching general intelligence.","Innovations integrating such models will also expand along the maturation of such AI systems and exhibit unforeseeable applications that will have important impacts on several aspects of our societies."],"url":"http://arxiv.org/abs/2305.06087v1"}
{"created":"2023-05-10","title":"Fast Distributed Inference Serving for Large Language Models","abstract":"Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactively offloads and uploads intermediate states between GPU memory and host memory for LLM inference. We build a system prototype of FastServe based on NVIDIA FasterTransformer. Experimental results show that compared to the state-of-the-art solution Orca, FastServe improves the average and tail JCT by up to 5.1$\\times$ and 6.4$\\times$, respectively.","sentences":["Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT.","The interactive nature of these applications demand low job completion time (JCT) for model inference.","Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT.","We present FastServe, a distributed inference serving system for LLMs.","FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token.","FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler.","Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join.","The higher priority queues than the joined queue are skipped to reduce demotions.","We design an efficient GPU memory management mechanism that proactively offloads and uploads intermediate states between GPU memory and host memory for LLM inference.","We build a system prototype of FastServe based on NVIDIA FasterTransformer.","Experimental results show that compared to the state-of-the-art solution Orca, FastServe improves the average and tail JCT by up to 5.1$\\times$ and 6.4$\\times$, respectively."],"url":"http://arxiv.org/abs/2305.05920v1"}
{"created":"2023-05-10","title":"Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks","abstract":"The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to the state-of-the-art finetuned models as well as pretrained domain-specific generative models. Our experiments provide qualitative studies, through which we hope to help understand the capability of the existing models and facilitate further improvements.","sentences":["The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input.","Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted.","In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting.","Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets.","The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks.","We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to the state-of-the-art finetuned models as well as pretrained domain-specific generative models.","Our experiments provide qualitative studies, through which we hope to help understand the capability of the existing models and facilitate further improvements."],"url":"http://arxiv.org/abs/2305.05862v1"}
{"created":"2023-05-10","title":"HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion","abstract":"Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis.","sentences":["Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing.","To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints.","Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition.","This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion.","While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP.","To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions.","We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis."],"url":"http://arxiv.org/abs/2305.06356v1"}
{"created":"2023-05-10","title":"VideoChat: Chat-Centric Video Understanding","abstract":"In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything","sentences":["In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system.","It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference.","To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations.","This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems.","Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research.","Access our code and data at https://github.com/OpenGVLab/Ask-Anything"],"url":"http://arxiv.org/abs/2305.06355v1"}
{"created":"2023-05-10","title":"RECKONING: Reasoning through Dynamic Knowledge Encoding","abstract":"Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (i.e., in-context reasoning). However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (i.e., not necessarily random noise). In these situations, the model fails to distinguish the knowledge that is necessary to answer the question, leading to spurious reasoning and degraded performance. This reasoning failure contrasts with the model's apparent ability to distinguish its contextual knowledge from all the knowledge it has memorized during pre-training. Following this observation, we propose teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question. Our method, RECKONING, is a bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge through back-propagation, allowing them to then answer questions using the updated parameters. During training, the inner loop rapidly adapts a copy of the model weights to encode contextual knowledge into its parameters. In the outer loop, the model learns to uses the updated weights to reproduce and answer reasoning questions about the memorized knowledge. Our experiments on two multi-hop reasoning datasets show that RECKONING's performance improves over the in-context reasoning baseline (by up to 4.5%). We also find that compared to in-context reasoning, RECKONING generalizes better to longer reasoning chains unseen during training, is more robust to distractors in the context, and is more computationally efficient when multiple questions are asked about the same knowledge.","sentences":["Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (i.e., in-context reasoning).","However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (i.e., not necessarily random noise).","In these situations, the model fails to distinguish the knowledge that is necessary to answer the question, leading to spurious reasoning and degraded performance.","This reasoning failure contrasts with the model's apparent ability to distinguish its contextual knowledge from all the knowledge it has memorized during pre-training.","Following this observation, we propose teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question.","Our method, RECKONING, is a bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge through back-propagation, allowing them to then answer questions using the updated parameters.","During training, the inner loop rapidly adapts a copy of the model weights to encode contextual knowledge into its parameters.","In the outer loop, the model learns to uses the updated weights to reproduce and answer reasoning questions about the memorized knowledge.","Our experiments on two multi-hop reasoning datasets show that RECKONING's performance improves over the in-context reasoning baseline (by up to 4.5%).","We also find that compared to in-context reasoning, RECKONING generalizes better to longer reasoning chains unseen during training, is more robust to distractors in the context, and is more computationally efficient when multiple questions are asked about the same knowledge."],"url":"http://arxiv.org/abs/2305.06349v1"}
{"created":"2023-05-10","title":"Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs","abstract":"Vision and Language (VL) models have demonstrated remarkable zero-shot performance in a variety of tasks. However, recent studies have shown that even the best VL models struggle to capture aspects of scene understanding, such as object attributes, relationships, and action states. In contrast, obtaining structured annotations, e.g., scene graphs (SGs) that could improve these models is time-consuming, costly, and tedious, and thus cannot be used on a large scale. Here we ask, can small datasets containing SG annotations provide sufficient information for enhancing structured understanding of VL models? We show that it is indeed possible to improve VL models using such data by utilizing a specialized model architecture and a new training paradigm. Our approach captures structure-related information for both the visual and textual encoders by directly supervising both components when learning from SG labels. We use scene graph supervision to generate fine-grained captions based on various graph augmentations highlighting different compositional aspects of the scene, and to predict SG information using an open vocabulary approach by adding special ``Adaptive SG tokens'' to the visual encoder. Moreover, we design a new adaptation technique tailored specifically to the SG tokens that allows better learning of the graph prediction task while still maintaining zero-shot capabilities. Our model shows strong performance improvements on the Winoground and VL-checklist datasets with only a mild degradation in zero-shot performance.","sentences":["Vision and Language (VL) models have demonstrated remarkable zero-shot performance in a variety of tasks.","However, recent studies have shown that even the best VL models struggle to capture aspects of scene understanding, such as object attributes, relationships, and action states.","In contrast, obtaining structured annotations, e.g., scene graphs (SGs) that could improve these models is time-consuming, costly, and tedious, and thus cannot be used on a large scale.","Here we ask, can small datasets containing SG annotations provide sufficient information for enhancing structured understanding of VL models?","We show that it is indeed possible to improve VL models using such data by utilizing a specialized model architecture and a new training paradigm.","Our approach captures structure-related information for both the visual and textual encoders by directly supervising both components when learning from SG labels.","We use scene graph supervision to generate fine-grained captions based on various graph augmentations highlighting different compositional aspects of the scene, and to predict SG information using an open vocabulary approach by adding special ``Adaptive SG tokens'' to the visual encoder.","Moreover, we design a new adaptation technique tailored specifically to the SG tokens that allows better learning of the graph prediction task while still maintaining zero-shot capabilities.","Our model shows strong performance improvements on the Winoground and VL-checklist datasets with only a mild degradation in zero-shot performance."],"url":"http://arxiv.org/abs/2305.06343v1"}
{"created":"2023-05-10","title":"K-UniMorph: Korean Universal Morphology and its Feature Schema","abstract":"We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from Sylak-Glassman et al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus. Furthermore, we carry out the inflection task using three different Korean word forms: letters, syllables and morphemes. Finally, we discuss and describe future perspectives on Korean morphological paradigms and the dataset.","sentences":["We present in this work a new Universal Morphology dataset for Korean.","Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages.","Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics.","For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata.","This dataset adopts morphological feature schema from Sylak-Glassman et al.","(2015) and Sylak-Glassman (2016) for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean.","During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus.","Furthermore, we carry out the inflection task using three different Korean word forms: letters, syllables and morphemes.","Finally, we discuss and describe future perspectives on Korean morphological paradigms and the dataset."],"url":"http://arxiv.org/abs/2305.06335v1"}
{"created":"2023-05-10","title":"Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks","abstract":"Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge. Unlike mesh-based models, they require watertight geometry and object-wise semantics at the fa\\c{c}ade level. The principal challenge of such demanding semantic 3D reconstruction is reliable fa\\c{c}ade-level semantic segmentation of 3D input data. We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving fa\\c{c}ade-level semantic 3D segmentation. To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts. These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a library of fa\\c{c}ade objects. Extensive experiments on the TUM city campus datasets demonstrate the superior performance of the proposed Scan2LoD3 over the state-of-the-art methods in fa\\c{c}ade-level detection, semantic segmentation, and LoD3 building model reconstruction. We believe our method can foster the development of probability-driven semantic 3D reconstruction at LoD3 since not only the high-definition reconstruction but also reconstruction confidence becomes pivotal for various applications such as autonomous driving and urban simulations.","sentences":["Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge.","Unlike mesh-based models, they require watertight geometry and object-wise semantics at the fa\\c{c}ade level.","The principal challenge of such demanding semantic 3D reconstruction is reliable fa\\c{c}ade-level semantic segmentation of 3D input data.","We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving fa\\c{c}ade-level semantic 3D segmentation.","To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts.","These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images.","To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a library of fa\\c{c}ade objects.","Extensive experiments on the TUM city campus datasets demonstrate the superior performance of the proposed Scan2LoD3 over the state-of-the-art methods in fa\\c{c}ade-level detection, semantic segmentation, and LoD3 building model reconstruction.","We believe our method can foster the development of probability-driven semantic 3D reconstruction at LoD3 since not only the high-definition reconstruction but also reconstruction confidence becomes pivotal for various applications such as autonomous driving and urban simulations."],"url":"http://arxiv.org/abs/2305.06314v1"}
{"created":"2023-05-10","title":"Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning","abstract":"Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis. Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies. Moreover, their updates are long and expensive, making them unsuitable to emerging practices. Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis. Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers. We experimented on a synthetic but realistic dataset to differentially diagnose anemia and its subtypes and particularly evaluated the robustness of various approaches to noise and missing data as those are frequent in EHRs. Within the DRL algorithms, Dueling DQN with Prioritized Experience Replay, and Dueling Double DQN with Prioritized Experience Replay show the best and most stable performances. In the presence of imperfect data, the DRL algorithms show competitive, but less stable performances when compared to the classifiers (Random Forest and XGBoost); although they enable the progressive generation of a pathway to the suggested diagnosis, which can both guide or explain the decision process.","sentences":["Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis.","Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies.","Moreover, their updates are long and expensive, making them unsuitable to emerging practices.","Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis.","Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers.","We experimented on a synthetic but realistic dataset to differentially diagnose anemia and its subtypes and particularly evaluated the robustness of various approaches to noise and missing data as those are frequent in EHRs.","Within the DRL algorithms, Dueling DQN with Prioritized Experience Replay, and Dueling Double DQN with Prioritized Experience Replay show the best and most stable performances.","In the presence of imperfect data, the DRL algorithms show competitive, but less stable performances when compared to the classifiers (Random Forest and XGBoost); although they enable the progressive generation of a pathway to the suggested diagnosis, which can both guide or explain the decision process."],"url":"http://arxiv.org/abs/2305.06295v1"}
{"created":"2023-05-10","title":"Joint Metrics Matter: A Better Standard for Trajectory Forecasting","abstract":"Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents. Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group. Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research. In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the-art (SOTA) trajectory forecasting methods with respect to multi-agent metrics (joint metrics): JADE, JFDE, and collision rate. We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn from the ETH / UCY and Stanford Drone datasets. We introduce a new loss function incorporating joint metrics that, when applied to a SOTA trajectory forecasting method, achieves a 7% improvement in JADE / JFDE on the ETH / UCY datasets with respect to the previous SOTA. Our results also indicate that optimizing for joint metrics naturally leads to an improvement in interaction modeling, as evidenced by a 16% decrease in mean collision rate on the ETH / UCY datasets with respect to the previous SOTA.","sentences":["Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents.","Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group.","Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research.","In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the-art (SOTA) trajectory forecasting methods with respect to multi-agent metrics (joint metrics): JADE, JFDE, and collision rate.","We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn from the ETH / UCY and Stanford Drone datasets.","We introduce a new loss function incorporating joint metrics that, when applied to a SOTA trajectory forecasting method, achieves a 7% improvement in JADE / JFDE on the ETH / UCY datasets with respect to the previous SOTA.","Our results also indicate that optimizing for joint metrics naturally leads to an improvement in interaction modeling, as evidenced by a 16% decrease in mean collision rate on the ETH / UCY datasets with respect to the previous SOTA."],"url":"http://arxiv.org/abs/2305.06292v1"}
{"created":"2023-05-10","title":"A Multi-modal Garden Dataset and Hybrid 3D Dense Reconstruction Framework Based on Panoramic Stereo Images for a Trimming Robot","abstract":"Recovering an outdoor environment's surface mesh is vital for an agricultural robot during task planning and remote visualization. Our proposed solution is based on a newly-designed panoramic stereo camera along with a hybrid novel software framework that consists of three fusion modules. The panoramic stereo camera with a pentagon shape consists of 5 stereo vision camera pairs to stream synchronized panoramic stereo images for the following three fusion modules. In the disparity fusion module, rectified stereo images produce the initial disparity maps using multiple stereo vision algorithms. Then, these initial disparity maps, along with the intensity images, are input into a disparity fusion network to produce refined disparity maps. Next, the refined disparity maps are converted into full-view point clouds or single-view point clouds for the pose fusion module. The pose fusion module adopts a two-stage global-coarse-to-local-fine strategy. In the first stage, each pair of full-view point clouds is registered by a global point cloud matching algorithm to estimate the transformation for a global pose graph's edge, which effectively implements loop closure. In the second stage, a local point cloud matching algorithm is used to match single-view point clouds in different nodes. Next, we locally refine the poses of all corresponding edges in the global pose graph using three proposed rules, thus constructing a refined pose graph. The refined pose graph is optimized to produce a global pose trajectory for volumetric fusion. In the volumetric fusion module, the global poses of all the nodes are used to integrate the single-view point clouds into the volume to produce the mesh of the whole garden. The proposed framework and its three fusion modules are tested on a real outdoor garden dataset to show the superiority of the performance.","sentences":["Recovering an outdoor environment's surface mesh is vital for an agricultural robot during task planning and remote visualization.","Our proposed solution is based on a newly-designed panoramic stereo camera along with a hybrid novel software framework that consists of three fusion modules.","The panoramic stereo camera with a pentagon shape consists of 5 stereo vision camera pairs to stream synchronized panoramic stereo images for the following three fusion modules.","In the disparity fusion module, rectified stereo images produce the initial disparity maps using multiple stereo vision algorithms.","Then, these initial disparity maps, along with the intensity images, are input into a disparity fusion network to produce refined disparity maps.","Next, the refined disparity maps are converted into full-view point clouds or single-view point clouds for the pose fusion module.","The pose fusion module adopts a two-stage global-coarse-to-local-fine strategy.","In the first stage, each pair of full-view point clouds is registered by a global point cloud matching algorithm to estimate the transformation for a global pose graph's edge, which effectively implements loop closure.","In the second stage, a local point cloud matching algorithm is used to match single-view point clouds in different nodes.","Next, we locally refine the poses of all corresponding edges in the global pose graph using three proposed rules, thus constructing a refined pose graph.","The refined pose graph is optimized to produce a global pose trajectory for volumetric fusion.","In the volumetric fusion module, the global poses of all the nodes are used to integrate the single-view point clouds into the volume to produce the mesh of the whole garden.","The proposed framework and its three fusion modules are tested on a real outdoor garden dataset to show the superiority of the performance."],"url":"http://arxiv.org/abs/2305.06278v1"}
{"created":"2023-05-10","title":"Flexible cost-penalized Bayesian model selection: developing inclusion paths with an application to diagnosis of heart disease","abstract":"We propose a Bayesian model selection approach that allows medical practitioners to select among predictor variables while taking their respective costs into account. Medical procedures almost always incur costs in time and/or money. These costs might exceed their usefulness for modeling the outcome of interest. We develop Bayesian model selection that uses flexible model priors to penalize costly predictors a priori and select a subset of predictors useful relative to their costs. Our approach (i) gives the practitioner control over the magnitude of cost penalization, (ii) enables the prior to scale well with sample size, and (iii) enables the creation of our proposed inclusion path visualization, which can be used to make decisions about individual candidate predictors using both probabilistic and visual tools. We demonstrate the effectiveness of our inclusion path approach and the importance of being able to adjust the magnitude of the prior's cost penalization through a dataset pertaining to heart disease diagnosis in patients at the Cleveland Clinic Foundation, where several candidate predictors with various costs were recorded for patients, and through simulated data.","sentences":["We propose a Bayesian model selection approach that allows medical practitioners to select among predictor variables while taking their respective costs into account.","Medical procedures almost always incur costs in time and/or money.","These costs might exceed their usefulness for modeling the outcome of interest.","We develop Bayesian model selection that uses flexible model priors to penalize costly predictors a priori and select a subset of predictors useful relative to their costs.","Our approach (i) gives the practitioner control over the magnitude of cost penalization, (ii) enables the prior to scale well with sample size, and (iii) enables the creation of our proposed inclusion path visualization, which can be used to make decisions about individual candidate predictors using both probabilistic and visual tools.","We demonstrate the effectiveness of our inclusion path approach and the importance of being able to adjust the magnitude of the prior's cost penalization through a dataset pertaining to heart disease diagnosis in patients at the Cleveland Clinic Foundation, where several candidate predictors with various costs were recorded for patients, and through simulated data."],"url":"http://arxiv.org/abs/2305.06262v1"}
{"created":"2023-05-10","title":"Supervised learning with probabilistic morphisms and kernel mean embeddings","abstract":"In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\\mathcal{X}$ and a label space $\\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\\mu_{\\mathcal{Y}|\\mathcal{X}}$ for a probability measure $\\mu$ on $\\mathcal{X} \\times \\mathcal{Y}$ relative to the projection $\\Pi_{\\mathcal{X}}: \\mathcal{X}\\times\\mathcal{Y}\\to \\mathcal{X}$ as a solution of a linear operator equation. If $\\mathcal{Y}$ is a separable metrizable topological space with the Borel $\\sigma$-algebra $ \\mathcal{B} (\\mathcal{Y})$, I propose another characterization of a regular conditional probability measure $\\mu_{\\mathcal{Y}|\\mathcal{X}}$ as a minimizer of a mean square error on the space of Markov kernels, called probabilistic morphisms, from $\\mathcal{X}$ to $\\mathcal{Y}$, using kernel mean embedding. Using these results and using inner measure to quantify generalizability of a learning algorithm, I give a generalization of a result due to Cucker-Smale, which concerns the learnability of a regression model, to a setting of a conditional probability estimation problem. I also give a variant of Vapnik's method of solving stochastic ill-posed problem, using inner measure and discuss its applications.","sentences":["In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\\mathcal{X}$ and a label space $\\mathcal{Y}$, which are measurable spaces.","A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\\mu_{\\mathcal{Y}|\\mathcal{X}}$ for a probability measure $\\mu$ on $\\mathcal{X} \\times \\mathcal{Y}$ relative to the projection $\\Pi_{\\mathcal{X}}: \\mathcal{X}\\times\\mathcal{Y}\\to \\mathcal{X}$ as a solution of a linear operator equation.","If $\\mathcal{Y}$ is a separable metrizable topological space with the Borel $\\sigma$-algebra $ \\mathcal{B} (\\mathcal{Y})$, I propose another characterization of a regular conditional probability measure $\\mu_{\\mathcal{Y}|\\mathcal{X}}$ as a minimizer of a mean square error on the space of Markov kernels, called probabilistic morphisms, from $\\mathcal{X}$ to $\\mathcal{Y}$, using kernel mean embedding.","Using these results and using inner measure to quantify generalizability of a learning algorithm, I give a generalization of a result due to Cucker-Smale, which concerns the learnability of a regression model, to a setting of a conditional probability estimation problem.","I also give a variant of Vapnik's method of solving stochastic ill-posed problem, using inner measure and discuss its applications."],"url":"http://arxiv.org/abs/2305.06348v1"}
{"created":"2023-05-10","title":"Self-Supervised Instance Segmentation by Grasping","abstract":"Instance segmentation is a fundamental skill for many robotic applications. We propose a self-supervised method that uses grasp interactions to collect segmentation supervision for an instance segmentation model. When a robot grasps an item, the mask of that grasped item can be inferred from the images of the scene before and after the grasp. Leveraging this insight, we learn a grasp segmentation model to segment the grasped object from before and after grasp images. Such a model can segment grasped objects from thousands of grasp interactions without costly human annotation. Using the segmented grasped objects, we can \"cut\" objects from their original scenes and \"paste\" them into new scenes to generate instance supervision. We show that our grasp segmentation model provides a 5x error reduction when segmenting grasped objects compared with traditional image subtraction approaches. Combined with our \"cut-and-paste\" generation method, instance segmentation models trained with our method achieve better performance than a model trained with 10x the amount of labeled data. On a real robotic grasping system, our instance segmentation model reduces the rate of grasp errors by over 3x compared to an image subtraction baseline.","sentences":["Instance segmentation is a fundamental skill for many robotic applications.","We propose a self-supervised method that uses grasp interactions to collect segmentation supervision for an instance segmentation model.","When a robot grasps an item, the mask of that grasped item can be inferred from the images of the scene before and after the grasp.","Leveraging this insight, we learn a grasp segmentation model to segment the grasped object from before and after grasp images.","Such a model can segment grasped objects from thousands of grasp interactions without costly human annotation.","Using the segmented grasped objects, we can \"cut\" objects from their original scenes and \"paste\" them into new scenes to generate instance supervision.","We show that our grasp segmentation model provides a 5x error reduction when segmenting grasped objects compared with traditional image subtraction approaches.","Combined with our \"cut-and-paste\" generation method, instance segmentation models trained with our method achieve better performance than a model trained with 10x the amount of labeled data.","On a real robotic grasping system, our instance segmentation model reduces the rate of grasp errors by over 3x compared to an image subtraction baseline."],"url":"http://arxiv.org/abs/2305.06305v1"}
{"created":"2023-05-10","title":"HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion","abstract":"Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis.","sentences":["Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing.","To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints.","Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition.","This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion.","While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP.","To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions.","We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis."],"url":"http://arxiv.org/abs/2305.06356v1"}
{"created":"2023-05-10","title":"Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)","abstract":"Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine. In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision. We consider both single- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to \\emph{synthesize} evidence reported across a collection of articles. We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries. We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents. We release all data and annotations used in this work.","sentences":["Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in few- and zero-shot settings.","However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine.","In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision.","We consider both single- and multi-document settings.","In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to \\emph{synthesize} evidence reported across a collection of articles.","We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries.","We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents.","We release all data and annotations used in this work."],"url":"http://arxiv.org/abs/2305.06299v1"}
{"created":"2023-05-10","title":"Cost-benefit of green infrastructures for water management: A sustainability assessment of full-scale constructed wetlands in Northern and Southern Italy","abstract":"Sustainable water management has become an urgent challenge due to irregular water availability patterns and water quality issues. The effect of climate change exacerbates this phenomenon in water-scarce areas, such as the Mediterranean region, stimulating the implementation of solutions aiming to mitigate or improve environmental, social, and economic conditions. A novel solution inspired by nature, technology-oriented, explored in the past years, is constructed wetlands. Commonly applied for different types of wastewater due to its low cost and simple maintenance, they are considered a promising solution to remove pollutants while creating an improved ecosystem by increasing biodiversity around them. This research aims to assess the sustainability of two typologies of constructed wetlands in two Italian areas: Sicily, with a vertical subsurface flow constructed wetland, and Emilia Romagna, with a surface flow constructed wetland. The assessment is performed by applying a cost-benefit analysis combining primary and secondary data sources. The analysis considered the market and non-market values in both proposed scenarios to establish the feasibility of the two options and identify the most convenient one. Results show that both constructed wetlands bring more benefits (benefits-cost ratio, BCR) than costs (BCR > 0). In the case of Sicily, the BCR is lower (1) in the constructed wetland scenario, while in its absence it is almost double. If other ecosystem services are included the constructed wetland scenario reach a BCR of 4 and a ROI of 5, showing a better performance from a costing perspective than the absence one. In Emilia Romagna, the constructed wetland scenario shows a high BCR (10) and ROI (9), while the scenario in absence has obtained a negative present value indicating that the cost do not cover the benefits expected.","sentences":["Sustainable water management has become an urgent challenge due to irregular water availability patterns and water quality issues.","The effect of climate change exacerbates this phenomenon in water-scarce areas, such as the Mediterranean region, stimulating the implementation of solutions aiming to mitigate or improve environmental, social, and economic conditions.","A novel solution inspired by nature, technology-oriented, explored in the past years, is constructed wetlands.","Commonly applied for different types of wastewater due to its low cost and simple maintenance, they are considered a promising solution to remove pollutants while creating an improved ecosystem by increasing biodiversity around them.","This research aims to assess the sustainability of two typologies of constructed wetlands in two Italian areas: Sicily, with a vertical subsurface flow constructed wetland, and Emilia Romagna, with a surface flow constructed wetland.","The assessment is performed by applying a cost-benefit analysis combining primary and secondary data sources.","The analysis considered the market and non-market values in both proposed scenarios to establish the feasibility of the two options and identify the most convenient one.","Results show that both constructed wetlands bring more benefits (benefits-cost ratio, BCR) than costs (BCR > 0).","In the case of Sicily, the BCR is lower (1) in the constructed wetland scenario, while in its absence it is almost double.","If other ecosystem services are included the constructed wetland scenario reach a BCR of 4 and a ROI of 5, showing a better performance from a costing perspective than the absence one.","In Emilia Romagna, the constructed wetland scenario shows a high BCR (10) and ROI (9), while the scenario in absence has obtained a negative present value indicating that the cost do not cover the benefits expected."],"url":"http://arxiv.org/abs/2305.06284v1"}
{"created":"2023-05-10","title":"View Correspondence Network for Implicit Light Field Representation","abstract":"We present a novel technique for implicit neural representation of light fields at continuously defined viewpoints with high quality and fidelity. Our implicit neural representation maps 4D coordinates defining two-plane parameterization of the light fields to the corresponding color values. We leverage periodic activations to achieve high expressivity and accurate reconstruction for complex data manifolds while keeping low storage and inference time requirements. However, na\\\"ively trained non-3D structured networks do not adequately satisfy the multi-view consistency; instead, they perform alpha blending of nearby viewpoints. In contrast, our View Correspondence Network, or VICON, leverages stereo matching, optimization by automatic differentiation with respect to the input space, and multi-view pixel correspondence to provide a novel implicit representation of the light fields faithful to the novel views that are unseen during the training. Experimental results show VICON superior to the state-of-the-art non-3D implicit light field representations both qualitatively and quantitatively. Moreover, our implicit representation captures a larger field of view (FoV), surpassing the extent of the observable scene by the cameras of the ground truth renderings.","sentences":["We present a novel technique for implicit neural representation of light fields at continuously defined viewpoints with high quality and fidelity.","Our implicit neural representation maps 4D coordinates defining two-plane parameterization of the light fields to the corresponding color values.","We leverage periodic activations to achieve high expressivity and accurate reconstruction for complex data manifolds while keeping low storage and inference time requirements.","However, na\\\"ively trained non-3D structured networks do not adequately satisfy the multi-view consistency; instead, they perform alpha blending of nearby viewpoints.","In contrast, our View Correspondence Network, or VICON, leverages stereo matching, optimization by automatic differentiation with respect to the input space, and multi-view pixel correspondence to provide a novel implicit representation of the light fields faithful to the novel views that are unseen during the training.","Experimental results show VICON superior to the state-of-the-art non-3D implicit light field representations both qualitatively and quantitatively.","Moreover, our implicit representation captures a larger field of view (FoV), surpassing the extent of the observable scene by the cameras of the ground truth renderings."],"url":"http://arxiv.org/abs/2305.06233v1"}
{"created":"2023-05-10","title":"RiverBench: an Open RDF Streaming Benchmark Suite","abstract":"RDF data streaming has been explored by the Semantic Web community from many angles, resulting in multiple task formulations and streaming methods. However, for many existing formulations of the problem, reliably benchmarking streaming solutions has been challenging due to the lack of well-described and appropriately diverse benchmark datasets. Existing datasets and evaluations, except a few notable cases, suffer from unclear streaming task scopes, underspecified benchmarks, and errors in the data. To address these issues, we firstly systematize the different RDF data streaming tasks in a clear taxonomy and outline practical requirements for benchmark datasets. We then propose RiverBench, an open and collaborative RDF streaming benchmark suite that applies these principles in practice. RiverBench leverages continuous, community-driven processes, established best practices (e.g., FAIR), and built-in quality guarantees. The suite distributes datasets in a common, accessible format, with clear documentation, licensing, and machine-readable metadata. The current release includes a diverse collection of non-synthetic datasets generated by the Semantic Web community, representing many applications of RDF data streaming, all major task formulations, and emerging RDF features (RDF-star). Finally, we present a list of research applications for the suite, demonstrating its versatility and value even beyond the realm of RDF streaming.","sentences":["RDF data streaming has been explored by the Semantic Web community from many angles, resulting in multiple task formulations and streaming methods.","However, for many existing formulations of the problem, reliably benchmarking streaming solutions has been challenging due to the lack of well-described and appropriately diverse benchmark datasets.","Existing datasets and evaluations, except a few notable cases, suffer from unclear streaming task scopes, underspecified benchmarks, and errors in the data.","To address these issues, we firstly systematize the different RDF data streaming tasks in a clear taxonomy and outline practical requirements for benchmark datasets.","We then propose RiverBench, an open and collaborative RDF streaming benchmark suite that applies these principles in practice.","RiverBench leverages continuous, community-driven processes, established best practices (e.g., FAIR), and built-in quality guarantees.","The suite distributes datasets in a common, accessible format, with clear documentation, licensing, and machine-readable metadata.","The current release includes a diverse collection of non-synthetic datasets generated by the Semantic Web community, representing many applications of RDF data streaming, all major task formulations, and emerging RDF features (RDF-star).","Finally, we present a list of research applications for the suite, demonstrating its versatility and value even beyond the realm of RDF streaming."],"url":"http://arxiv.org/abs/2305.06226v1"}
{"created":"2023-05-10","title":"Differential Privacy for Protecting Private Patterns in Data Streams","abstract":"Complex event processing (CEP) is a powerful and increasingly more important tool to analyse data streams for Internet of Things (IoT) applications. These data streams often contain private information that requires proper protection. However, privacy protection in CEP systems is still in its infancy, and most existing privacy-preserving mechanisms (PPMs) are adopted from those designed for data streams. Such approaches undermine the quality of the entire data stream and limit the performance of IoT applications. In this paper, we attempt to break the limitation and establish a new foundation for PPMs of CEP by proposing a novel pattern-level differential privacy (DP) guarantee. We introduce two PPMs that guarantee pattern-level DP. They operate only on data that correlate with private patterns rather than on the entire data stream, leading to higher data quality. One of the PPMs provides adaptive privacy protection and brings more granularity and generalization. We evaluate the performance of the proposed PPMs with two experiments on a real-world dataset and on a synthetic dataset. The results of the experiments indicate that our proposed privacy guarantee and its PPMs can deliver better data quality under equally strong privacy guarantees, compared to multiple well-known PPMs designed for data streams.","sentences":["Complex event processing (CEP) is a powerful and increasingly more important tool to analyse data streams for Internet of Things (IoT) applications.","These data streams often contain private information that requires proper protection.","However, privacy protection in CEP systems is still in its infancy, and most existing privacy-preserving mechanisms (PPMs) are adopted from those designed for data streams.","Such approaches undermine the quality of the entire data stream and limit the performance of IoT applications.","In this paper, we attempt to break the limitation and establish a new foundation for PPMs of CEP by proposing a novel pattern-level differential privacy (DP) guarantee.","We introduce two PPMs that guarantee pattern-level DP.","They operate only on data that correlate with private patterns rather than on the entire data stream, leading to higher data quality.","One of the PPMs provides adaptive privacy protection and brings more granularity and generalization.","We evaluate the performance of the proposed PPMs with two experiments on a real-world dataset and on a synthetic dataset.","The results of the experiments indicate that our proposed privacy guarantee and its PPMs can deliver better data quality under equally strong privacy guarantees, compared to multiple well-known PPMs designed for data streams."],"url":"http://arxiv.org/abs/2305.06105v1"}
{"created":"2023-05-10","title":"Few-shot Link Prediction on N-ary Facts","abstract":"N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation meta information from limited instances to predict a missing entity in a query instance. To validate the effectiveness of FLEN, we construct three datasets based on existing benchmark data. Our experimental results show that FLEN significantly outperforms existing related models in both few-shot link prediction on n-ary facts and binary facts.","sentences":["N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs).","Link prediction on n-ary facts is to predict a missing element in an n-ary fact.","This helps populate and enrich KGs and further promotes numerous downstream applications.","Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts.","However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios.","Thus, this paper introduces a new task, few-shot link prediction on n-ary facts.","It aims to predict a missing entity in an n-ary fact with limited labeled instances.","We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules.","FLEN captures relation meta information from limited instances to predict a missing entity in a query instance.","To validate the effectiveness of FLEN, we construct three datasets based on existing benchmark data.","Our experimental results show that FLEN significantly outperforms existing related models in both few-shot link prediction on n-ary facts and binary facts."],"url":"http://arxiv.org/abs/2305.06104v1"}
{"created":"2023-05-10","title":"Building Interoperable Electronic Health Records as Purpose-Driven Knowledge Graphs","abstract":"When building a new application we are increasingly confronted with the need of reusing and integrating pre-existing knowledge. Nevertheless, it is a fact that this prior knowledge is virtually impossible to reuse as-is. This is true also in domains, e.g., eHealth, where a lot of effort has been put into developing high-quality standards and reference ontologies, e.g. FHIR1. In this paper, we propose an integrated methodology, called iTelos, which enables data and knowledge reuse towards the construction of Interoperable Electronic Health Records (iEHR). The key intuition is that the data level and the schema level of an application should be developed independently, thus allowing for maximum flexibility in the reuse of the prior knowledge, but under the overall guidance of the needs to be satisfied, formalized as competence queries. This intuition is implemented by codifying all the requirements, including those concerning reuse, as part of a purpose defined a priori, which is then used to drive a middle-out development process where the application schema and data are continuously aligned. The proposed methodology is validated through its application to a large-scale case study.","sentences":["When building a new application we are increasingly confronted with the need of reusing and integrating pre-existing knowledge.","Nevertheless, it is a fact that this prior knowledge is virtually impossible to reuse as-is.","This is true also in domains, e.g., eHealth, where a lot of effort has been put into developing high-quality standards and reference ontologies, e.g. FHIR1.","In this paper, we propose an integrated methodology, called iTelos, which enables data and knowledge reuse towards the construction of Interoperable Electronic Health Records (iEHR).","The key intuition is that the data level and the schema level of an application should be developed independently, thus allowing for maximum flexibility in the reuse of the prior knowledge, but under the overall guidance of the needs to be satisfied, formalized as competence queries.","This intuition is implemented by codifying all the requirements, including those concerning reuse, as part of a purpose defined a priori, which is then used to drive a middle-out development process where the application schema and data are continuously aligned.","The proposed methodology is validated through its application to a large-scale case study."],"url":"http://arxiv.org/abs/2305.06088v1"}
{"created":"2023-05-10","title":"ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base","abstract":"Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.","sentences":["Analogical reasoning is a fundamental cognitive ability of humans.","However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training.","In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs).","ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control.","Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2305.05994v1"}
{"created":"2023-05-10","title":"Uncertainty-Aware Semi-Supervised Learning for Prostate MRI Zonal Segmentation","abstract":"Quality of deep convolutional neural network predictions strongly depends on the size of the training dataset and the quality of the annotations. Creating annotations, especially for 3D medical image segmentation, is time-consuming and requires expert knowledge. We propose a novel semi-supervised learning (SSL) approach that requires only a relatively small number of annotations while being able to use the remaining unlabeled data to improve model performance. Our method uses a pseudo-labeling technique that employs recent deep learning uncertainty estimation models. By using the estimated uncertainty, we were able to rank pseudo-labels and automatically select the best pseudo-annotations generated by the supervised model. We applied this to prostate zonal segmentation in T2-weighted MRI scans. Our proposed model outperformed the semi-supervised model in experiments with the ProstateX dataset and an external test set, by leveraging only a subset of unlabeled data rather than the full collection of 4953 cases, our proposed model demonstrated improved performance. The segmentation dice similarity coefficient in the transition zone and peripheral zone increased from 0.835 and 0.727 to 0.852 and 0.751, respectively, for fully supervised model and the uncertainty-aware semi-supervised learning model (USSL). Our USSL model demonstrates the potential to allow deep learning models to be trained on large datasets without requiring full annotation. Our code is available at https://github.com/DIAGNijmegen/prostateMR-USSL.","sentences":["Quality of deep convolutional neural network predictions strongly depends on the size of the training dataset and the quality of the annotations.","Creating annotations, especially for 3D medical image segmentation, is time-consuming and requires expert knowledge.","We propose a novel semi-supervised learning (SSL) approach that requires only a relatively small number of annotations while being able to use the remaining unlabeled data to improve model performance.","Our method uses a pseudo-labeling technique that employs recent deep learning uncertainty estimation models.","By using the estimated uncertainty, we were able to rank pseudo-labels and automatically select the best pseudo-annotations generated by the supervised model.","We applied this to prostate zonal segmentation in T2-weighted MRI scans.","Our proposed model outperformed the semi-supervised model in experiments with the ProstateX dataset and an external test set, by leveraging only a subset of unlabeled data rather than the full collection of 4953 cases, our proposed model demonstrated improved performance.","The segmentation dice similarity coefficient in the transition zone and peripheral zone increased from 0.835 and 0.727 to 0.852 and 0.751, respectively, for fully supervised model and the uncertainty-aware semi-supervised learning model (USSL).","Our USSL model demonstrates the potential to allow deep learning models to be trained on large datasets without requiring full annotation.","Our code is available at https://github.com/DIAGNijmegen/prostateMR-USSL."],"url":"http://arxiv.org/abs/2305.05984v1"}
{"created":"2023-05-10","title":"DPMLBench: Holistic Evaluation of Differentially Private Machine Learning","abstract":"Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection. Combined with powerful machine learning techniques, differentially private machine learning (DPML) is increasingly important. As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice. Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss. However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms. More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.   We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks. We first present a taxonomy of where improvements are located in the machine learning life cycle. Based on our taxonomy, we jointly perform an extensive measurement study of the improved DPML algorithms. We also cover state-of-the-art label differential privacy (Label DP) algorithms in the evaluation. According to our empirical results, DP can effectively defend against MIAs, and sensitivity-bounding techniques such as per-sample gradient clipping play an important role in defense. We also explore some improvements that can maintain model utility and defend against MIAs more effectively. Experiments show that Label DP algorithms achieve less utility loss but are fragile to MIAs. To support our evaluation, we implement a modular re-usable software, DPMLBench, which enables sensitive data owners to deploy DPML algorithms and serves as a benchmark tool for researchers and practitioners.","sentences":["Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection.","Combined with powerful machine learning techniques, differentially private machine learning (DPML) is increasingly important.","As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice.","Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss.","However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms.","More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.   ","We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks.","We first present a taxonomy of where improvements are located in the machine learning life cycle.","Based on our taxonomy, we jointly perform an extensive measurement study of the improved DPML algorithms.","We also cover state-of-the-art label differential privacy (Label DP) algorithms in the evaluation.","According to our empirical results, DP can effectively defend against MIAs, and sensitivity-bounding techniques such as per-sample gradient clipping play an important role in defense.","We also explore some improvements that can maintain model utility and defend against MIAs more effectively.","Experiments show that Label DP algorithms achieve less utility loss but are fragile to MIAs.","To support our evaluation, we implement a modular re-usable software, DPMLBench, which enables sensitive data owners to deploy DPML algorithms and serves as a benchmark tool for researchers and practitioners."],"url":"http://arxiv.org/abs/2305.05900v1"}
