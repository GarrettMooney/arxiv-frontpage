{"created":"2023-08-09 17:55:50","title":"Scene-Generalizable Interactive Segmentation of Radiance Fields","abstract":"Existing methods for interactive segmentation in radiance fields entail scene-specific optimization and thus cannot generalize across different scenes, which greatly limits their applicability. In this work we make the first attempt at Scene-Generalizable Interactive Segmentation in Radiance Fields (SGISRF) and propose a novel SGISRF method, which can perform 3D object segmentation for novel (unseen) scenes represented by radiance fields, guided by only a few interactive user clicks in a given set of multi-view 2D images. In particular, the proposed SGISRF focuses on addressing three crucial challenges with three specially designed techniques. First, we devise the Cross-Dimension Guidance Propagation to encode the scarce 2D user clicks into informative 3D guidance representations. Second, the Uncertainty-Eliminated 3D Segmentation module is designed to achieve efficient yet effective 3D segmentation. Third, Concealment-Revealed Supervised Learning scheme is proposed to reveal and correct the concealed 3D segmentation errors resulted from the supervision in 2D space with only 2D mask annotations. Extensive experiments on two real-world challenging benchmarks covering diverse scenes demonstrate 1) effectiveness and scene-generalizability of the proposed method, 2) favorable performance compared to classical method requiring scene-specific optimization.","sentences":["Existing methods for interactive segmentation in radiance fields entail scene-specific optimization and thus cannot generalize across different scenes, which greatly limits their applicability.","In this work we make the first attempt at Scene-Generalizable Interactive Segmentation in Radiance Fields (SGISRF) and propose a novel SGISRF method, which can perform 3D object segmentation for novel (unseen) scenes represented by radiance fields, guided by only a few interactive user clicks in a given set of multi-view 2D images.","In particular, the proposed SGISRF focuses on addressing three crucial challenges with three specially designed techniques.","First, we devise the Cross-Dimension Guidance Propagation to encode the scarce 2D user clicks into informative 3D guidance representations.","Second, the Uncertainty-Eliminated 3D Segmentation module is designed to achieve efficient yet effective 3D segmentation.","Third, Concealment-Revealed Supervised Learning scheme is proposed to reveal and correct the concealed 3D segmentation errors resulted from the supervision in 2D space with only 2D mask annotations.","Extensive experiments on two real-world challenging benchmarks covering diverse scenes demonstrate 1) effectiveness and scene-generalizability of the proposed method, 2) favorable performance compared to classical method requiring scene-specific optimization."],"url":"http://arxiv.org/abs/2308.05104v1"}
{"created":"2023-08-09 17:53:36","title":"DOST -- Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels","abstract":"The enormous demand for annotated data brought forth by deep learning techniques has been accompanied by the problem of annotation noise. Although this issue has been widely discussed in machine learning literature, it has been relatively unexplored in the context of \"multi-label classification\" (MLC) tasks which feature more complicated kinds of noise. Additionally, when the domain in question has certain logical constraints, noisy annotations often exacerbate their violations, making such a system unacceptable to an expert. This paper studies the effect of label noise on domain rule violation incidents in the MLC task, and incorporates domain rules into our learning algorithm to mitigate the effect of noise. We propose the Domain Obedient Self-supervised Training (DOST) paradigm which not only makes deep learning models more aligned to domain rules, but also improves learning performance in key metrics and minimizes the effect of annotation noise. This novel approach uses domain guidance to detect offending annotations and deter rule-violating predictions in a self-supervised manner, thus making it more \"data efficient\" and domain compliant. Empirical studies, performed over two large scale multi-label classification datasets, demonstrate that our method results in improvement across the board, and often entirely counteracts the effect of noise.","sentences":["The enormous demand for annotated data brought forth by deep learning techniques has been accompanied by the problem of annotation noise.","Although this issue has been widely discussed in machine learning literature, it has been relatively unexplored in the context of \"multi-label classification\" (MLC) tasks which feature more complicated kinds of noise.","Additionally, when the domain in question has certain logical constraints, noisy annotations often exacerbate their violations, making such a system unacceptable to an expert.","This paper studies the effect of label noise on domain rule violation incidents in the MLC task, and incorporates domain rules into our learning algorithm to mitigate the effect of noise.","We propose the Domain Obedient Self-supervised Training (DOST) paradigm which not only makes deep learning models more aligned to domain rules, but also improves learning performance in key metrics and minimizes the effect of annotation noise.","This novel approach uses domain guidance to detect offending annotations and deter rule-violating predictions in a self-supervised manner, thus making it more \"data efficient\" and domain compliant.","Empirical studies, performed over two large scale multi-label classification datasets, demonstrate that our method results in improvement across the board, and often entirely counteracts the effect of noise."],"url":"http://arxiv.org/abs/2308.05101v1"}
{"created":"2023-08-09 17:46:01","title":"Optimal Flexible Consensus and its Application to Ethereum","abstract":"Classic BFT consensus protocols guarantee safety and liveness for all clients if fewer than one-third of replicas are faulty. However, in applications such as high-value payments, some clients may want to prioritize safety over liveness. Flexible consensus allows each client to opt for a higher safety resilience, albeit at the expense of reduced liveness resilience. We present the first construction that allows optimal safety--liveness tradeoff for every client simultaneously. This construction is modular and is realized as an add-on applied on top of an existing consensus protocol. The add-on consists of an additional round of voting and permanent locking done by the replicas, to sidestep a sub-optimal quorum-intersection-based constraint present in previous solutions. We adapt our construction to the existing Ethereum protocol to derive optimal flexible confirmation rules that clients can adopt unilaterally without requiring system-wide changes. This is possible because existing Ethereum protocol features can double as the extra voting and locking. We demonstrate an implementation using Ethereum's consensus API.","sentences":["Classic BFT consensus protocols guarantee safety and liveness for all clients if fewer than one-third of replicas are faulty.","However, in applications such as high-value payments, some clients may want to prioritize safety over liveness.","Flexible consensus allows each client to opt for a higher safety resilience, albeit at the expense of reduced liveness resilience.","We present the first construction that allows optimal safety--liveness tradeoff for every client simultaneously.","This construction is modular and is realized as an add-on applied on top of an existing consensus protocol.","The add-on consists of an additional round of voting and permanent locking done by the replicas, to sidestep a sub-optimal quorum-intersection-based constraint present in previous solutions.","We adapt our construction to the existing Ethereum protocol to derive optimal flexible confirmation rules that clients can adopt unilaterally without requiring system-wide changes.","This is possible because existing Ethereum protocol features can double as the extra voting and locking.","We demonstrate an implementation using Ethereum's consensus API."],"url":"http://arxiv.org/abs/2308.05096v1"}
{"created":"2023-08-09 17:45:04","title":"LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation","abstract":"In the text-to-image generation field, recent remarkable progress in Stable Diffusion makes it possible to generate rich kinds of novel photorealistic images. However, current models still face misalignment issues (e.g., problematic spatial relation understanding and numeration failure) in complex natural scenes, which impedes the high-faithfulness text-to-image generation. Although recent efforts have been made to improve controllability by giving fine-grained guidance (e.g., sketch and scribbles), this issue has not been fundamentally tackled since users have to provide such guidance information manually. In this work, we strive to synthesize high-fidelity images that are semantically aligned with a given textual prompt without any guidance. Toward this end, we propose a coarse-to-fine paradigm to achieve layout planning and image generation. Concretely, we first generate the coarse-grained layout conditioned on a given textual prompt via in-context learning based on Large Language Models. Afterward, we propose a fine-grained object-interaction diffusion method to synthesize high-faithfulness images conditioned on the prompt and the automatically generated layout. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art models in terms of layout and image generation. Our code and settings are available at \\url{https://layoutllm-t2i.github.io}.","sentences":["In the text-to-image generation field, recent remarkable progress in Stable Diffusion makes it possible to generate rich kinds of novel photorealistic images.","However, current models still face misalignment issues (e.g., problematic spatial relation understanding and numeration failure) in complex natural scenes, which impedes the high-faithfulness text-to-image generation.","Although recent efforts have been made to improve controllability by giving fine-grained guidance (e.g., sketch and scribbles), this issue has not been fundamentally tackled since users have to provide such guidance information manually.","In this work, we strive to synthesize high-fidelity images that are semantically aligned with a given textual prompt without any guidance.","Toward this end, we propose a coarse-to-fine paradigm to achieve layout planning and image generation.","Concretely, we first generate the coarse-grained layout conditioned on a given textual prompt via in-context learning based on Large Language Models.","Afterward, we propose a fine-grained object-interaction diffusion method to synthesize high-faithfulness images conditioned on the prompt and the automatically generated layout.","Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art models in terms of layout and image generation.","Our code and settings are available at \\url{https://layoutllm-t2i.github.io}."],"url":"http://arxiv.org/abs/2308.05095v1"}
{"created":"2023-08-09 17:40:12","title":"A degree of image identification at sub-human scales could be possible with more advanced clusters","abstract":"The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi.","sentences":["The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from.","Initial research on this topic solely considered data volume scaling.","Here, we scale both the volume of data and the quality of the image.","This scaling experiment is a self-supervised learning method that may be done without any outside financing.","We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.","We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi."],"url":"http://arxiv.org/abs/2308.05092v1"}
{"created":"2023-08-09 17:27:57","title":"Organizational Bulk Email Systems: Their Role and Performance in Remote Work","abstract":"The COVID-19 pandemic has forced many employees to work from home. Organizational bulk emails now play a critical role to reach employees with central information in this work-from-home environment. However, we know from our own recent work that organizational bulk email has problems: recipients fail to retain the bulk messages they received from the organization; recipients and senders have different opinions on which bulk messages were important; and communicators lack technology support to better target and design messages. In this position paper, first we review the prior work on evaluating, designing, and prototyping organizational communication systems. Second we review our recent findings and some research techniques we found useful in studying organizational communication. Last we propose a research agenda to study organizational communications in remote work environment and suggest some key questions and potential study directions.","sentences":["The COVID-19 pandemic has forced many employees to work from home.","Organizational bulk emails now play a critical role to reach employees with central information in this work-from-home environment.","However, we know from our own recent work that organizational bulk email has problems: recipients fail to retain the bulk messages they received from the organization; recipients and senders have different opinions on which bulk messages were important; and communicators lack technology support to better target and design messages.","In this position paper, first we review the prior work on evaluating, designing, and prototyping organizational communication systems.","Second we review our recent findings and some research techniques we found useful in studying organizational communication.","Last we propose a research agenda to study organizational communications in remote work environment and suggest some key questions and potential study directions."],"url":"http://arxiv.org/abs/2308.05085v1"}
{"created":"2023-08-09 17:20:14","title":"Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling","abstract":"Video Semantic Role Labeling (VidSRL) aims to detect the salient events from given videos, by recognizing the predict-argument event structures and the interrelationships between events. While recent endeavors have put forth methods for VidSRL, they can be mostly subject to two key drawbacks, including the lack of fine-grained spatial scene perception and the insufficiently modeling of video temporality. Towards this end, this work explores a novel holistic spatio-temporal scene graph (namely HostSG) representation based on the existing dynamic scene graph structures, which well model both the fine-grained spatial semantics and temporal dynamics of videos for VidSRL. Built upon the HostSG, we present a nichetargeting VidSRL framework. A scene-event mapping mechanism is first designed to bridge the gap between the underlying scene structure and the high-level event semantic structure, resulting in an overall hierarchical scene-event (termed ICE) graph structure. We further perform iterative structure refinement to optimize the ICE graph, such that the overall structure representation can best coincide with end task demand. Finally, three subtask predictions of VidSRL are jointly decoded, where the end-to-end paradigm effectively avoids error propagation. On the benchmark dataset, our framework boosts significantly over the current best-performing model. Further analyses are shown for a better understanding of the advances of our methods.","sentences":["Video Semantic Role Labeling (VidSRL) aims to detect the salient events from given videos, by recognizing the predict-argument event structures and the interrelationships between events.","While recent endeavors have put forth methods for VidSRL, they can be mostly subject to two key drawbacks, including the lack of fine-grained spatial scene perception and the insufficiently modeling of video temporality.","Towards this end, this work explores a novel holistic spatio-temporal scene graph (namely HostSG) representation based on the existing dynamic scene graph structures, which well model both the fine-grained spatial semantics and temporal dynamics of videos for VidSRL.","Built upon the HostSG, we present a nichetargeting VidSRL framework.","A scene-event mapping mechanism is first designed to bridge the gap between the underlying scene structure and the high-level event semantic structure, resulting in an overall hierarchical scene-event (termed ICE) graph structure.","We further perform iterative structure refinement to optimize the ICE graph, such that the overall structure representation can best coincide with end task demand.","Finally, three subtask predictions of VidSRL are jointly decoded, where the end-to-end paradigm effectively avoids error propagation.","On the benchmark dataset, our framework boosts significantly over the current best-performing model.","Further analyses are shown for a better understanding of the advances of our methods."],"url":"http://arxiv.org/abs/2308.05081v1"}
{"created":"2023-08-09 17:15:24","title":"Ergodic Capacity of Dyadic Fading Channels in Ultra Low-SNR Regime","abstract":"In a mobile wireless channel, the small-scale multipath fading induces temporal channel fluctuations in the form of peaks and deep fades. The channel capacity degradation with fading severity in the high signal-to-noise ratio (SNR) regime is well known in the wireless communication literature: the probability of deep fades increases significantly with higher fading severity resulting in poor performance. In this paper, we focus on double-fading pinhole channels under perfect CSIT to show a very counter-intuitive result that - higher fading severity enables higher ergodic capacity at sufficiently low SNR. The underlying reason is that at low SNRs, ergodic capacity depends crucially on the probability distribution of channel peaks (simply tail distribution); for the pinhole channel, the tail distribution improves with increased fading severity. This allows a transmitter operating at low SNR to exploit channel peaks more efficiently resulting in a net improvement in achievable spectral efficiency. We derive a new key result quantifying the above dependence for the double-Nakagami-$m$ fading pinhole channel - that is, the ergodic capacity ${C} \\propto (m_T m_R)^{-1}$ at low SNR, where $m_T m_R$ is the product of fading (severity) parameters of the two independent Nakagami-$m$ fadings involved.","sentences":["In a mobile wireless channel, the small-scale multipath fading induces temporal channel fluctuations in the form of peaks and deep fades.","The channel capacity degradation with fading severity in the high signal-to-noise ratio (SNR) regime is well known in the wireless communication literature: the probability of deep fades increases significantly with higher fading severity resulting in poor performance.","In this paper, we focus on double-fading pinhole channels under perfect CSIT to show a very counter-intuitive result that - higher fading severity enables higher ergodic capacity at sufficiently low SNR.","The underlying reason is that at low SNRs, ergodic capacity depends crucially on the probability distribution of channel peaks (simply tail distribution); for the pinhole channel, the tail distribution improves with increased fading severity.","This allows a transmitter operating at low SNR to exploit channel peaks more efficiently resulting in a net improvement in achievable spectral efficiency.","We derive a new key result quantifying the above dependence for the double-Nakagami-$m$ fading pinhole channel - that is, the ergodic capacity ${C} \\propto (m_T m_R)^{-1}$ at low SNR, where $m_T m_R$ is the product of fading (severity) parameters of the two independent Nakagami-$m$ fadings involved."],"url":"http://arxiv.org/abs/2308.05078v1"}
{"created":"2023-08-09 17:12:23","title":"CHERI Performance Enhancement for the MicroPython Interpreter","abstract":"During our port of the MicroPython bytecode interpreter to the CHERI-based Arm Morello platform, we encountered a number of serious performance degradations. This paper explores several of these of these performance issues in detail, in each case characterizing the cause of the problem, the fix, and the corresponding performance improvement over a set of standard Python benchmarks.   While we recognize that Morello is a prototypical physical instantiation of the CHERI concept, we show that it is possible to eliminate certain kinds of software-induced runtime overhead that occur due to the larger size of architectural capabilities relative to native pointers. In our case, we reduce a geometric mean benchmark slowdown from 5x (before optimization) to 2x (after optimization) relative to AArch64 execution. The worst-case slowdowns are greatly improved, from 100x (before optimization) to 2x (after optimization).   The key insight is that pointer size assumptions pervade systems code; whereas previous CHERI porting projects highlighted compile-time and execution-time errors exposed by pointer size assumptions, we instead focus on the performance implications of such assumptions.","sentences":["During our port of the MicroPython bytecode interpreter to the CHERI-based Arm Morello platform, we encountered a number of serious performance degradations.","This paper explores several of these of these performance issues in detail, in each case characterizing the cause of the problem, the fix, and the corresponding performance improvement over a set of standard Python benchmarks.   ","While we recognize that Morello is a prototypical physical instantiation of the CHERI concept, we show that it is possible to eliminate certain kinds of software-induced runtime overhead that occur due to the larger size of architectural capabilities relative to native pointers.","In our case, we reduce a geometric mean benchmark slowdown from 5x (before optimization) to 2x (after optimization) relative to AArch64 execution.","The worst-case slowdowns are greatly improved, from 100x (before optimization) to 2x (after optimization).   ","The key insight is that pointer size assumptions pervade systems code; whereas previous CHERI porting projects highlighted compile-time and execution-time errors exposed by pointer size assumptions, we instead focus on the performance implications of such assumptions."],"url":"http://arxiv.org/abs/2308.05076v1"}
{"created":"2023-08-09 17:08:29","title":"Bayesian Inverse Transition Learning for Offline Settings","abstract":"Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data. A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have. Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients. Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets. We also explain how combining uncertainty estimation with these constraints can help us infer a partial ranking of actions that produce higher returns, and helps us infer safer and more informative policies for planning.","sentences":["Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data.","A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have.","Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients.","Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets.","We also explain how combining uncertainty estimation with these constraints can help us infer a partial ranking of actions that produce higher returns, and helps us infer safer and more informative policies for planning."],"url":"http://arxiv.org/abs/2308.05075v1"}
{"created":"2023-08-09 17:07:20","title":"Drones4Good: Supporting Disaster Relief Through Remote Sensing and AI","abstract":"In order to respond effectively in the aftermath of a disaster, emergency services and relief organizations rely on timely and accurate information about the affected areas. Remote sensing has the potential to significantly reduce the time and effort required to collect such information by enabling a rapid survey of large areas. To achieve this, the main challenge is the automatic extraction of relevant information from remotely sensed data. In this work, we show how the combination of drone-based data with deep learning methods enables automated and large-scale situation assessment. In addition, we demonstrate the integration of onboard image processing techniques for the deployment of autonomous drone-based aid delivery. The results show the feasibility of a rapid and large-scale image analysis in the field, and that onboard image processing can increase the safety of drone-based aid deliveries.","sentences":["In order to respond effectively in the aftermath of a disaster, emergency services and relief organizations rely on timely and accurate information about the affected areas.","Remote sensing has the potential to significantly reduce the time and effort required to collect such information by enabling a rapid survey of large areas.","To achieve this, the main challenge is the automatic extraction of relevant information from remotely sensed data.","In this work, we show how the combination of drone-based data with deep learning methods enables automated and large-scale situation assessment.","In addition, we demonstrate the integration of onboard image processing techniques for the deployment of autonomous drone-based aid delivery.","The results show the feasibility of a rapid and large-scale image analysis in the field, and that onboard image processing can increase the safety of drone-based aid deliveries."],"url":"http://arxiv.org/abs/2308.05074v1"}
{"created":"2023-08-09 17:00:43","title":"Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized Herculaneum Papyri","abstract":"Recent advancements in Digital Document Restoration (DDR) have led to significant breakthroughs in analyzing highly damaged written artifacts. Among those, there has been an increasing interest in applying Artificial Intelligence techniques for virtually unwrapping and automatically detecting ink on the Herculaneum papyri collection. This collection consists of carbonized scrolls and fragments of documents, which have been digitized via X-ray tomography to allow the development of ad-hoc deep learning-based DDR solutions. In this work, we propose a modification of the Fast Fourier Convolution operator for volumetric data and apply it in a segmentation architecture for ink detection on the challenging Herculaneum papyri, demonstrating its suitability via deep experimental analysis. To encourage the research on this task and the application of the proposed operator to other tasks involving volumetric data, we will release our implementation (https://github.com/aimagelab/vffc)","sentences":["Recent advancements in Digital Document Restoration (DDR) have led to significant breakthroughs in analyzing highly damaged written artifacts.","Among those, there has been an increasing interest in applying Artificial Intelligence techniques for virtually unwrapping and automatically detecting ink on the Herculaneum papyri collection.","This collection consists of carbonized scrolls and fragments of documents, which have been digitized via X-ray tomography to allow the development of ad-hoc deep learning-based DDR solutions.","In this work, we propose a modification of the Fast Fourier Convolution operator for volumetric data and apply it in a segmentation architecture for ink detection on the challenging Herculaneum papyri, demonstrating its suitability via deep experimental analysis.","To encourage the research on this task and the application of the proposed operator to other tasks involving volumetric data, we will release our implementation (https://github.com/aimagelab/vffc)"],"url":"http://arxiv.org/abs/2308.05070v1"}
{"created":"2023-08-09 16:57:32","title":"Controlling Tail Risk in Online Ski-Rental","abstract":"The classical ski-rental problem admits a textbook 2-competitive deterministic algorithm, and a simple randomized algorithm that is $\\frac{e}{e-1}$-competitive in expectation. The randomized algorithm, while optimal in expectation, has a large variance in its performance: it has more than a 37% chance of competitive ratio exceeding 2, and a $\\Theta(1/n)$ chance of the competitive ratio exceeding $n$!   We ask what happens to the optimal solution if we insist that the tail risk, i.e., the chance of the competitive ratio exceeding a specific value, is bounded by some constant $\\delta$. We find that this additional modification significantly changes the structure of the optimal solution. The probability of purchasing skis on a given day becomes non-monotone, discontinuous, and arbitrarily large (for sufficiently small tail risk $\\delta$ and large purchase cost $n$).","sentences":["The classical ski-rental problem admits a textbook 2-competitive deterministic algorithm, and a simple randomized algorithm that is $\\frac{e}{e-1}$-competitive in expectation.","The randomized algorithm, while optimal in expectation, has a large variance in its performance: it has more than a 37% chance of competitive ratio exceeding 2, and a $\\Theta(1/n)$ chance of the competitive ratio exceeding $n$!   ","We ask what happens to the optimal solution if we insist that the tail risk, i.e., the chance of the competitive ratio exceeding a specific value, is bounded by some constant $\\delta$. We find that this additional modification significantly changes the structure of the optimal solution.","The probability of purchasing skis on a given day becomes non-monotone, discontinuous, and arbitrarily large (for sufficiently small tail risk $\\delta$ and large purchase cost $n$)."],"url":"http://arxiv.org/abs/2308.05067v1"}
{"created":"2023-08-09 16:47:35","title":"CERMET: Coding for Energy Reduction with Multiple Encryption Techniques -- $It's\\ easy\\ being\\ green$","abstract":"This paper presents CERMET, an energy-efficient hardware architecture designed for hardware-constrained cryptosystems. CERMET employs a base cryptosystem in conjunction with network coding to provide both information-theoretic and computational security while reducing energy consumption per bit. This paper introduces the hardware architecture for the system and explores various optimizations to enhance its performance. The universality of the approach is demonstrated by designing the architecture to accommodate both asymmetric and symmetric cryptosystems. The analysis reveals that the benefits of this proposed approach are multifold, reducing energy per bit and area without compromising security or throughput. The optimized hardware architectures can achieve below 1 pJ/bit operations for AES-256. Furthermore, for a public key cryptosystem based on Elliptic Curve Cryptography (ECC), a remarkable 14.6X reduction in energy per bit and a 9.3X reduction in area are observed, bringing it to less than 1 nJ/bit.","sentences":["This paper presents CERMET, an energy-efficient hardware architecture designed for hardware-constrained cryptosystems.","CERMET employs a base cryptosystem in conjunction with network coding to provide both information-theoretic and computational security while reducing energy consumption per bit.","This paper introduces the hardware architecture for the system and explores various optimizations to enhance its performance.","The universality of the approach is demonstrated by designing the architecture to accommodate both asymmetric and symmetric cryptosystems.","The analysis reveals that the benefits of this proposed approach are multifold, reducing energy per bit and area without compromising security or throughput.","The optimized hardware architectures can achieve below 1 pJ/bit operations for AES-256.","Furthermore, for a public key cryptosystem based on Elliptic Curve Cryptography (ECC), a remarkable 14.6X reduction in energy per bit and a 9.3X reduction in area are observed, bringing it to less than 1 nJ/bit."],"url":"http://arxiv.org/abs/2308.05063v1"}
{"created":"2023-08-09 16:47:04","title":"Competitions in AI -- Robustly Ranking Solvers Using Statistical Resampling","abstract":"Solver competitions play a prominent role in assessing and advancing the state of the art for solving many problems in AI and beyond. Notably, in many areas of AI, competitions have had substantial impact in guiding research and applications for many years, and for a solver to be ranked highly in a competition carries considerable weight. But to which extent can we expect competition results to generalise to sets of problem instances different from those used in a particular competition? This is the question we investigate here, using statistical resampling techniques. We show that the rankings resulting from the standard interpretation of competition results can be very sensitive to even minor changes in the benchmark instance set used as the basis for assessment and can therefore not be expected to carry over to other samples from the same underlying instance distribution. To address this problem, we introduce a novel approach to statistically meaningful analysis of competition results based on resampling performance data. Our approach produces confidence intervals of competition scores as well as statistically robust solver rankings with bounded error. Applied to recent SAT, AI planning and computer vision competitions, our analysis reveals frequent statistical ties in solver performance as well as some inversions of ranks compared to the official results based on simple scoring.","sentences":["Solver competitions play a prominent role in assessing and advancing the state of the art for solving many problems in AI and beyond.","Notably, in many areas of AI, competitions have had substantial impact in guiding research and applications for many years, and for a solver to be ranked highly in a competition carries considerable weight.","But to which extent can we expect competition results to generalise to sets of problem instances different from those used in a particular competition?","This is the question we investigate here, using statistical resampling techniques.","We show that the rankings resulting from the standard interpretation of competition results can be very sensitive to even minor changes in the benchmark instance set used as the basis for assessment and can therefore not be expected to carry over to other samples from the same underlying instance distribution.","To address this problem, we introduce a novel approach to statistically meaningful analysis of competition results based on resampling performance data.","Our approach produces confidence intervals of competition scores as well as statistically robust solver rankings with bounded error.","Applied to recent SAT, AI planning and computer vision competitions, our analysis reveals frequent statistical ties in solver performance as well as some inversions of ranks compared to the official results based on simple scoring."],"url":"http://arxiv.org/abs/2308.05062v1"}
{"created":"2023-08-09 16:44:25","title":"Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language","abstract":"In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of \"captions\" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as \"ICON-LM\", based on a language-model-like architecture. We demonstrate the viability of \"ICON-LM\" for scientific machine learning tasks, which creates a new path for the application of language models.","sentences":["In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates.","However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator.","To address this, we present a transformation of in-context operator learning into a multi-modal paradigm.","We propose the use of \"captions\" to integrate human knowledge about the operator, expressed through natural language descriptions and equations.","We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs.","Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as \"ICON-LM\", based on a language-model-like architecture.","We demonstrate the viability of \"ICON-LM\" for scientific machine learning tasks, which creates a new path for the application of language models."],"url":"http://arxiv.org/abs/2308.05061v1"}
{"created":"2023-08-09 16:41:27","title":"Evaluating SZZ Implementations: An Empirical Study on the Linux Kernel","abstract":"The SZZ algorithm is used to connect bug-fixing commits to the earlier commits that introduced bugs. This algorithm has many applications and many variants have been devised. However, there are some types of commits that cannot be traced by the SZZ algorithm, referred to as \"ghost commits\". The evaluation of how these ghost commits impact the SZZ algorithm remains limited. Moreover, these algorithms have been evaluated on datasets created by software engineering researchers from information in bug trackers and version controlled histories. Since Oct 2013, the Linux kernel developers have started labelling bug-fixing patches with the commit identifiers of the corresponding bug-inducing commit(s) as a standard practice. As of v6.1-rc5, 76,046 pairs of bug-fixing patches and bug-inducing commits are available. This provides a unique opportunity to evaluate the SZZ algorithm on a large dataset that has been created and reviewed by project developers, entirely independently of the biases of software engineering researchers.   In this paper, we apply six SZZ algorithms to 76,046 pairs of bug-fixing patches and bug-introducing commits from the Linux kernel. Our findings reveal that SZZ algorithms experience a more significant decline in recall on our dataset (13.8%) as compared to prior findings reported by Rosa et al., and the disparities between the individual SZZ algorithms diminish. Moreover, we find that 17.47% of bug-fixing commits are ghost commits. Finally, we propose Tracing-Commit SZZ (TC-SZZ), that traces all commits in the change history of lines modified or deleted in bug-fixing commits. Applying TC-SZZ to all failure cases, excluding ghost commits, we found that TC-SZZ could identify 17.7% of them. Our further analysis found that 34.6% of bug-inducing commits were in the function history, 27.5% in the file history (but not in the function history), and...","sentences":["The SZZ algorithm is used to connect bug-fixing commits to the earlier commits that introduced bugs.","This algorithm has many applications and many variants have been devised.","However, there are some types of commits that cannot be traced by the SZZ algorithm, referred to as \"ghost commits\".","The evaluation of how these ghost commits impact the SZZ algorithm remains limited.","Moreover, these algorithms have been evaluated on datasets created by software engineering researchers from information in bug trackers and version controlled histories.","Since Oct 2013, the Linux kernel developers have started labelling bug-fixing patches with the commit identifiers of the corresponding bug-inducing commit(s) as a standard practice.","As of v6.1-rc5, 76,046 pairs of bug-fixing patches and bug-inducing commits are available.","This provides a unique opportunity to evaluate the SZZ algorithm on a large dataset that has been created and reviewed by project developers, entirely independently of the biases of software engineering researchers.   ","In this paper, we apply six SZZ algorithms to 76,046 pairs of bug-fixing patches and bug-introducing commits from the Linux kernel.","Our findings reveal that SZZ algorithms experience a more significant decline in recall on our dataset (13.8%) as compared to prior findings reported by Rosa et al., and the disparities between the individual SZZ algorithms diminish.","Moreover, we find that 17.47% of bug-fixing commits are ghost commits.","Finally, we propose Tracing-Commit SZZ (TC-SZZ), that traces all commits in the change history of lines modified or deleted in bug-fixing commits.","Applying TC-SZZ to all failure cases, excluding ghost commits, we found that TC-SZZ could identify 17.7% of them.","Our further analysis found that 34.6% of bug-inducing commits were in the function history, 27.5% in the file history (but not in the function history), and..."],"url":"http://arxiv.org/abs/2308.05060v1"}
{"created":"2023-08-09 16:41:00","title":"A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique","abstract":"Deep learning has revolutionized industries like computer vision, natural language processing, and speech recognition. However, back propagation, the main method for training deep neural networks, faces challenges like computational overhead and vanishing gradients. In this paper, we propose a novel instant parameter update methodology that eliminates the need for computing gradients at each layer. Our approach accelerates learning, avoids the vanishing gradient problem, and outperforms state-of-the-art methods on benchmark data sets. This research presents a promising direction for efficient and effective deep neural network training.","sentences":["Deep learning has revolutionized industries like computer vision, natural language processing, and speech recognition.","However, back propagation, the main method for training deep neural networks, faces challenges like computational overhead and vanishing gradients.","In this paper, we propose a novel instant parameter update methodology that eliminates the need for computing gradients at each layer.","Our approach accelerates learning, avoids the vanishing gradient problem, and outperforms state-of-the-art methods on benchmark data sets.","This research presents a promising direction for efficient and effective deep neural network training."],"url":"http://arxiv.org/abs/2308.05059v1"}
{"created":"2023-08-09 16:34:40","title":"Enhancement of Direct LEO Satellite-to-Smartphone Communications by Distributed Beamforming","abstract":"The low earth orbit (LEO) satellite network is undergoing rapid development with the maturing of satellite communications and rocket launch technologies, and the demand for a global coverage network. However, current satellite communication networks are constrained by limited transmitting signal power, resulting in the use of large-size and energy-consuming ground terminals to provide additional gain. This paper proposes a novel technology called distributed beamforming to address such challenges and support direct communications from LEO satellites to smartphones. The proposed distributed beamforming technique is based on the superposition of electromagnetic (EM) waves and aims to enhance the received signal strength. Furthermore, we utilize EM wave superposition to increase the link budget and provide the coverage pattern formed by the distributed antenna array, which will be affected by the array structure and the transmitter parameters. In addition, the impact of Doppler frequency shift and time misalignment on the performance of distributed beamforming is investigated. Numerical results show that the enhancement of the received power depends on the angle formed by those radiated beams and can be up to the square of the number of beams; namely, a maximum enhancement of 6 dB could be obtained by using two satellites and a maximum of 12 dB increase through four satellites, which provide a clear guideline for the design of distributed beamforming for future satellite communications.","sentences":["The low earth orbit (LEO) satellite network is undergoing rapid development with the maturing of satellite communications and rocket launch technologies, and the demand for a global coverage network.","However, current satellite communication networks are constrained by limited transmitting signal power, resulting in the use of large-size and energy-consuming ground terminals to provide additional gain.","This paper proposes a novel technology called distributed beamforming to address such challenges and support direct communications from LEO satellites to smartphones.","The proposed distributed beamforming technique is based on the superposition of electromagnetic (EM) waves and aims to enhance the received signal strength.","Furthermore, we utilize EM wave superposition to increase the link budget and provide the coverage pattern formed by the distributed antenna array, which will be affected by the array structure and the transmitter parameters.","In addition, the impact of Doppler frequency shift and time misalignment on the performance of distributed beamforming is investigated.","Numerical results show that the enhancement of the received power depends on the angle formed by those radiated beams and can be up to the square of the number of beams; namely, a maximum enhancement of 6 dB could be obtained by using two satellites and a maximum of 12 dB increase through four satellites, which provide a clear guideline for the design of distributed beamforming for future satellite communications."],"url":"http://arxiv.org/abs/2308.05055v1"}
{"created":"2023-08-09 16:32:39","title":"Designing Cellular Networks for UAV Corridors via Bayesian Optimization","abstract":"As traditional cellular base stations (BSs) are optimized for 2D ground service, providing 3D connectivity to uncrewed aerial vehicles (UAVs) requires re-engineering of the existing infrastructure. In this paper, we propose a new methodology for designing cellular networks that cater for both ground users and UAV corridors based on Bayesian optimization. We present a case study in which we maximize the signal-to-interference-plus-noise ratio (SINR) for both populations of users by optimizing the electrical antenna tilts and the transmit power employed at each BS. Our proposed optimized network significantly boosts the UAV performance, with a 23.4dB gain in mean SINR compared to an all-downtilt, full-power baseline. At the same time, this optimal tradeoff nearly preserves the performance on the ground, even attaining a gain of 1.3dB in mean SINR with respect to said baseline. Thanks to its ability to optimize black-box stochastic functions, the proposed framework is amenable to maximize any desired function of the SINR or even the capacity per area.","sentences":["As traditional cellular base stations (BSs) are optimized for 2D ground service, providing 3D connectivity to uncrewed aerial vehicles (UAVs) requires re-engineering of the existing infrastructure.","In this paper, we propose a new methodology for designing cellular networks that cater for both ground users and UAV corridors based on Bayesian optimization.","We present a case study in which we maximize the signal-to-interference-plus-noise ratio (SINR) for both populations of users by optimizing the electrical antenna tilts and the transmit power employed at each BS.","Our proposed optimized network significantly boosts the UAV performance, with a 23.4dB gain in mean SINR compared to an all-downtilt, full-power baseline.","At the same time, this optimal tradeoff nearly preserves the performance on the ground, even attaining a gain of 1.3dB in mean SINR with respect to said baseline.","Thanks to its ability to optimize black-box stochastic functions, the proposed framework is amenable to maximize any desired function of the SINR or even the capacity per area."],"url":"http://arxiv.org/abs/2308.05052v1"}
{"created":"2023-08-09 16:29:31","title":"PAT: Position-Aware Transformer for Dense Multi-Label Action Detection","abstract":"We present PAT, a transformer-based network that learns complex temporal co-occurrence action dependencies in a video by exploiting multi-scale temporal features. In existing methods, the self-attention mechanism in transformers loses the temporal positional information, which is essential for robust action detection. To address this issue, we (i) embed relative positional encoding in the self-attention mechanism and (ii) exploit multi-scale temporal relationships by designing a novel non hierarchical network, in contrast to the recent transformer-based approaches that use a hierarchical structure. We argue that joining the self-attention mechanism with multiple sub-sampling processes in the hierarchical approaches results in increased loss of positional information. We evaluate the performance of our proposed approach on two challenging dense multi-label benchmark datasets, and show that PAT improves the current state-of-the-art result by 1.1% and 0.6% mAP on the Charades and MultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-art mAP at 26.5% and 44.6%, respectively. We also perform extensive ablation studies to examine the impact of the different components of our proposed network.","sentences":["We present PAT, a transformer-based network that learns complex temporal co-occurrence action dependencies in a video by exploiting multi-scale temporal features.","In existing methods, the self-attention mechanism in transformers loses the temporal positional information, which is essential for robust action detection.","To address this issue, we (i) embed relative positional encoding in the self-attention mechanism and (ii) exploit multi-scale temporal relationships by designing a novel non hierarchical network, in contrast to the recent transformer-based approaches that use a hierarchical structure.","We argue that joining the self-attention mechanism with multiple sub-sampling processes in the hierarchical approaches results in increased loss of positional information.","We evaluate the performance of our proposed approach on two challenging dense multi-label benchmark datasets, and show that PAT improves the current state-of-the-art result by 1.1% and 0.6% mAP on the Charades and MultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-art mAP at 26.5% and 44.6%, respectively.","We also perform extensive ablation studies to examine the impact of the different components of our proposed network."],"url":"http://arxiv.org/abs/2308.05051v1"}
{"created":"2023-08-09 16:19:43","title":"RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction","abstract":"We present RadGraph2, a novel dataset for extracting information from radiology reports that focuses on capturing changes in disease state and device placement over time. We introduce a hierarchical schema that organizes entities based on their relationships and show that using this hierarchy during training improves the performance of an information extraction model. Specifically, we propose a modification to the DyGIE++ framework, resulting in our model HGIE, which outperforms previous models in entity and relation extraction tasks. We demonstrate that RadGraph2 enables models to capture a wider variety of findings and perform better at relation extraction compared to those trained on the original RadGraph dataset. Our work provides the foundation for developing automated systems that can track disease progression over time and develop information extraction models that leverage the natural hierarchy of labels in the medical domain.","sentences":["We present RadGraph2, a novel dataset for extracting information from radiology reports that focuses on capturing changes in disease state and device placement over time.","We introduce a hierarchical schema that organizes entities based on their relationships and show that using this hierarchy during training improves the performance of an information extraction model.","Specifically, we propose a modification to the DyGIE++ framework, resulting in our model HGIE, which outperforms previous models in entity and relation extraction tasks.","We demonstrate that RadGraph2 enables models to capture a wider variety of findings and perform better at relation extraction compared to those trained on the original RadGraph dataset.","Our work provides the foundation for developing automated systems that can track disease progression over time and develop information extraction models that leverage the natural hierarchy of labels in the medical domain."],"url":"http://arxiv.org/abs/2308.05046v1"}
{"created":"2023-08-09 16:17:55","title":"Scalable Hypergraph Visualization","abstract":"Hypergraph visualization has many applications in network data analysis. Recently, a polygon-based representation for hypergraphs has been proposed with demonstrated benefits. However, the polygon-based layout often suffers from excessive self-intersections when the input dataset is relatively large. In this paper, we propose a framework in which the hypergraph is iteratively simplified through a set of atomic operations. Then, the layout of the simplest hypergraph is optimized and used as the foundation for a reverse process that brings the simplest hypergraph back to the original one, but with an improved layout. At the core of our approach is the set of atomic simplification operations and an operation priority measure to guide the simplification process. In addition, we introduce necessary definitions and conditions for hypergraph planarity within the polygon representation. We extend our approach to handle simultaneous simplification and layout optimization for both the hypergraph and its dual. We demonstrate the utility of our approach with datasets from a number of real-world applications.","sentences":["Hypergraph visualization has many applications in network data analysis.","Recently, a polygon-based representation for hypergraphs has been proposed with demonstrated benefits.","However, the polygon-based layout often suffers from excessive self-intersections when the input dataset is relatively large.","In this paper, we propose a framework in which the hypergraph is iteratively simplified through a set of atomic operations.","Then, the layout of the simplest hypergraph is optimized and used as the foundation for a reverse process that brings the simplest hypergraph back to the original one, but with an improved layout.","At the core of our approach is the set of atomic simplification operations and an operation priority measure to guide the simplification process.","In addition, we introduce necessary definitions and conditions for hypergraph planarity within the polygon representation.","We extend our approach to handle simultaneous simplification and layout optimization for both the hypergraph and its dual.","We demonstrate the utility of our approach with datasets from a number of real-world applications."],"url":"http://arxiv.org/abs/2308.05043v1"}
{"created":"2023-08-09 16:14:19","title":"Neural Field Movement Primitives for Joint Modelling of Scenes and Motions","abstract":"This paper presents a novel Learning from Demonstration (LfD) method that uses neural fields to learn new skills efficiently and accurately. It achieves this by utilizing a shared embedding to learn both scene and motion representations in a generative way. Our method smoothly maps each expert demonstration to a scene-motion embedding and learns to model them without requiring hand-crafted task parameters or large datasets. It achieves data efficiency by enforcing scene and motion generation to be smooth with respect to changes in the embedding space. At inference time, our method can retrieve scene-motion embeddings using test time optimization, and generate precise motion trajectories for novel scenes. The proposed method is versatile and can employ images, 3D shapes, and any other scene representations that can be modeled using neural fields. Additionally, it can generate both end-effector positions and joint angle-based trajectories. Our method is evaluated on tasks that require accurate motion trajectory generation, where the underlying task parametrization is based on object positions and geometric scene changes. Experimental results demonstrate that the proposed method outperforms the baseline approaches and generalizes to novel scenes. Furthermore, in real-world experiments, we show that our method can successfully model multi-valued trajectories, it is robust to the distractor objects introduced at inference time, and it can generate 6D motions.","sentences":["This paper presents a novel Learning from Demonstration (LfD) method that uses neural fields to learn new skills efficiently and accurately.","It achieves this by utilizing a shared embedding to learn both scene and motion representations in a generative way.","Our method smoothly maps each expert demonstration to a scene-motion embedding and learns to model them without requiring hand-crafted task parameters or large datasets.","It achieves data efficiency by enforcing scene and motion generation to be smooth with respect to changes in the embedding space.","At inference time, our method can retrieve scene-motion embeddings using test time optimization, and generate precise motion trajectories for novel scenes.","The proposed method is versatile and can employ images, 3D shapes, and any other scene representations that can be modeled using neural fields.","Additionally, it can generate both end-effector positions and joint angle-based trajectories.","Our method is evaluated on tasks that require accurate motion trajectory generation, where the underlying task parametrization is based on object positions and geometric scene changes.","Experimental results demonstrate that the proposed method outperforms the baseline approaches and generalizes to novel scenes.","Furthermore, in real-world experiments, we show that our method can successfully model multi-valued trajectories, it is robust to the distractor objects introduced at inference time, and it can generate 6D motions."],"url":"http://arxiv.org/abs/2308.05040v1"}
{"created":"2023-08-09 16:10:05","title":"Xenophobic Events vs. Refugee Population -- Using GDELT to Identify Countries with Disproportionate Coverage","abstract":"In this preliminary study, we used the Global Database of Events, Language, and Tone (GDELT) database to examine xenophobic events reported in the media during 2022. We collected a dataset of 2,778 unique events and created a choropleth map illustrating the frequency of events scaled by the refugee population's proportion in each host country. We identified the top 10 countries with the highest scaled event frequencies among those with more than 50,000 refugees. Contrary to the belief that hosting a significant number of forced migrants results in higher xenophobic incidents, our findings indicate a potential connection to political factors. We also categorized the 20 root event codes in the CAMEO event data as either \"Direct\" or \"Indirect\". Almost 90% of the events related to refugees in 2022 were classified as \"Indirect\".","sentences":["In this preliminary study, we used the Global Database of Events, Language, and Tone (GDELT) database to examine xenophobic events reported in the media during 2022.","We collected a dataset of 2,778 unique events and created a choropleth map illustrating the frequency of events scaled by the refugee population's proportion in each host country.","We identified the top 10 countries with the highest scaled event frequencies among those with more than 50,000 refugees.","Contrary to the belief that hosting a significant number of forced migrants results in higher xenophobic incidents, our findings indicate a potential connection to political factors.","We also categorized the 20 root event codes in the CAMEO event data as either \"Direct\" or \"Indirect\".","Almost 90% of the events related to refugees in 2022 were classified as \"Indirect\"."],"url":"http://arxiv.org/abs/2308.05038v1"}
{"created":"2023-08-09 16:08:32","title":"Expert load matters: operating networks at high accuracy and low manual effort","abstract":"In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes. Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized. The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as confidence operating characteristic (COC) curve. In this paper, we argue that deep neural networks should be trained by taking into account both accuracy and expert load and, to that end, propose a new complementary loss function for classification that maximizes the area under this COC curve. This promotes simultaneously the increase in network accuracy and the reduction in number of samples delegated to humans. We perform experiments on multiple computer vision and medical image datasets for classification. Our results demonstrate that the proposed loss improves classification accuracy and delegates less number of decisions to experts, achieves better out-of-distribution samples detection and on par calibration performance compared to existing loss functions.","sentences":["In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts.","Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes.","Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized.","The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare.","The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as confidence operating characteristic (COC) curve.","In this paper, we argue that deep neural networks should be trained by taking into account both accuracy and expert load and, to that end, propose a new complementary loss function for classification that maximizes the area under this COC curve.","This promotes simultaneously the increase in network accuracy and the reduction in number of samples delegated to humans.","We perform experiments on multiple computer vision and medical image datasets for classification.","Our results demonstrate that the proposed loss improves classification accuracy and delegates less number of decisions to experts, achieves better out-of-distribution samples detection and on par calibration performance compared to existing loss functions."],"url":"http://arxiv.org/abs/2308.05035v1"}
{"created":"2023-08-09 16:04:55","title":"Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance","abstract":"Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approaches sacrifice at least one and struggle to achieve comparable detection performance.   Kairos leverages a novel graph neural network-based encoder-decoder architecture that learns the temporal evolution of a provenance graph's structural changes to quantify the degree of anomalousness for each system event. Then, based on this fine-grained information, Kairos reconstructs attack footprints, generating compact summary graphs that accurately describe malicious activity over a stream of system audit logs. Using state-of-the-art benchmark datasets, we demonstrate that Kairos outperforms previous approaches.","sentences":["Provenance graphs are structured audit logs that describe the history of a system's execution.","Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats.","Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?).","We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approaches sacrifice at least one and struggle to achieve comparable detection performance.   ","Kairos leverages a novel graph neural network-based encoder-decoder architecture that learns the temporal evolution of a provenance graph's structural changes to quantify the degree of anomalousness for each system event.","Then, based on this fine-grained information, Kairos reconstructs attack footprints, generating compact summary graphs that accurately describe malicious activity over a stream of system audit logs.","Using state-of-the-art benchmark datasets, we demonstrate that Kairos outperforms previous approaches."],"url":"http://arxiv.org/abs/2308.05034v1"}
{"created":"2023-08-09 15:59:42","title":"Density Crop-guided Semi-supervised Object Detection in Aerial Images","abstract":"One of the important bottlenecks in training modern object detectors is the need for labeled images where bounding box annotations have to be produced for each object present in the image. This bottleneck is further exacerbated in aerial images where the annotators have to label small objects often distributed in clusters on high-resolution images. In recent days, the mean-teacher approach trained with pseudo-labels and weak-strong augmentation consistency is gaining popularity for semi-supervised object detection. However, a direct adaptation of such semi-supervised detectors for aerial images where small clustered objects are often present, might not lead to optimal results. In this paper, we propose a density crop-guided semi-supervised detector that identifies the cluster of small objects during training and also exploits them to improve performance at inference. During training, image crops of clusters identified from labeled and unlabeled images are used to augment the training set, which in turn increases the chance of detecting small objects and creating good pseudo-labels for small objects on the unlabeled images. During inference, the detector is not only able to detect the objects of interest but also regions with a high density of small objects (density crops) so that detections from the input image and detections from image crops are combined, resulting in an overall more accurate object prediction, especially for small objects. Empirical studies on the popular benchmarks of VisDrone and DOTA datasets show the effectiveness of our density crop-guided semi-supervised detector with an average improvement of more than 2\\% over the basic mean-teacher method in COCO style AP. Our code is available at: https://github.com/akhilpm/DroneSSOD.","sentences":["One of the important bottlenecks in training modern object detectors is the need for labeled images where bounding box annotations have to be produced for each object present in the image.","This bottleneck is further exacerbated in aerial images where the annotators have to label small objects often distributed in clusters on high-resolution images.","In recent days, the mean-teacher approach trained with pseudo-labels and weak-strong augmentation consistency is gaining popularity for semi-supervised object detection.","However, a direct adaptation of such semi-supervised detectors for aerial images where small clustered objects are often present, might not lead to optimal results.","In this paper, we propose a density crop-guided semi-supervised detector that identifies the cluster of small objects during training and also exploits them to improve performance at inference.","During training, image crops of clusters identified from labeled and unlabeled images are used to augment the training set, which in turn increases the chance of detecting small objects and creating good pseudo-labels for small objects on the unlabeled images.","During inference, the detector is not only able to detect the objects of interest but also regions with a high density of small objects (density crops) so that detections from the input image and detections from image crops are combined, resulting in an overall more accurate object prediction, especially for small objects.","Empirical studies on the popular benchmarks of VisDrone and DOTA datasets show the effectiveness of our density crop-guided semi-supervised detector with an average improvement of more than 2\\% over the basic mean-teacher method in COCO style AP.","Our code is available at: https://github.com/akhilpm/DroneSSOD."],"url":"http://arxiv.org/abs/2308.05032v1"}
{"created":"2023-08-09 15:46:25","title":"An End-to-End Framework of Road User Detection, Tracking, and Prediction from Monocular Images","abstract":"Perception that involves multi-object detection and tracking, and trajectory prediction are two major tasks of autonomous driving. However, they are currently mostly studied separately, which results in most trajectory prediction modules being developed based on ground truth trajectories without taking into account that trajectories extracted from the detection and tracking modules in real-world scenarios are noisy. These noisy trajectories can have a significant impact on the performance of the trajectory predictor and can lead to serious prediction errors. In this paper, we build an end-to-end framework for detection, tracking, and trajectory prediction called ODTP (Online Detection, Tracking and Prediction). It adopts the state-of-the-art online multi-object tracking model, QD-3DT, for perception and trains the trajectory predictor, DCENet++, directly based on the detection results without purely relying on ground truth trajectories. We evaluate the performance of ODTP on the widely used nuScenes dataset for autonomous driving. Extensive experiments show that ODPT achieves high performance end-to-end trajectory prediction. DCENet++, with the enhanced dynamic maps, predicts more accurate trajectories than its base model. It is also more robust when compared with other generative and deterministic trajectory prediction models trained on noisy detection results.","sentences":["Perception that involves multi-object detection and tracking, and trajectory prediction are two major tasks of autonomous driving.","However, they are currently mostly studied separately, which results in most trajectory prediction modules being developed based on ground truth trajectories without taking into account that trajectories extracted from the detection and tracking modules in real-world scenarios are noisy.","These noisy trajectories can have a significant impact on the performance of the trajectory predictor and can lead to serious prediction errors.","In this paper, we build an end-to-end framework for detection, tracking, and trajectory prediction called ODTP (Online Detection, Tracking and Prediction).","It adopts the state-of-the-art online multi-object tracking model, QD-3DT, for perception and trains the trajectory predictor, DCENet++, directly based on the detection results without purely relying on ground truth trajectories.","We evaluate the performance of ODTP on the widely used nuScenes dataset for autonomous driving.","Extensive experiments show that ODPT achieves high performance end-to-end trajectory prediction.","DCENet++, with the enhanced dynamic maps, predicts more accurate trajectories than its base model.","It is also more robust when compared with other generative and deterministic trajectory prediction models trained on noisy detection results."],"url":"http://arxiv.org/abs/2308.05026v1"}
{"created":"2023-08-09 15:38:36","title":"Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution","abstract":"Transformer-based methods have exhibited remarkable potential in single image super-resolution (SISR) by effectively extracting long-range dependencies. However, most of the current research in this area has prioritized the design of transformer blocks to capture global information, while overlooking the importance of incorporating high-frequency priors, which we believe could be beneficial. In our study, we conducted a series of experiments and found that transformer structures are more adept at capturing low-frequency information, but have limited capacity in constructing high-frequency representations when compared to their convolutional counterparts. Our proposed solution, the cross-refinement adaptive feature modulation transformer (CRAFT), integrates the strengths of both convolutional and transformer structures. It comprises three key components: the high-frequency enhancement residual block (HFERB) for extracting high-frequency information, the shift rectangle window attention block (SRWAB) for capturing global information, and the hybrid fusion block (HFB) for refining the global representation. Our experiments on multiple datasets demonstrate that CRAFT outperforms state-of-the-art methods by up to 0.29dB while using fewer parameters. The source code will be made available at: https://github.com/AVC2-UESTC/CRAFT-SR.git.","sentences":["Transformer-based methods have exhibited remarkable potential in single image super-resolution (SISR) by effectively extracting long-range dependencies.","However, most of the current research in this area has prioritized the design of transformer blocks to capture global information, while overlooking the importance of incorporating high-frequency priors, which we believe could be beneficial.","In our study, we conducted a series of experiments and found that transformer structures are more adept at capturing low-frequency information, but have limited capacity in constructing high-frequency representations when compared to their convolutional counterparts.","Our proposed solution, the cross-refinement adaptive feature modulation transformer (CRAFT), integrates the strengths of both convolutional and transformer structures.","It comprises three key components: the high-frequency enhancement residual block (HFERB) for extracting high-frequency information, the shift rectangle window attention block (SRWAB) for capturing global information, and the hybrid fusion block (HFB) for refining the global representation.","Our experiments on multiple datasets demonstrate that CRAFT outperforms state-of-the-art methods by up to 0.29dB while using fewer parameters.","The source code will be made available at: https://github.com/AVC2-UESTC/CRAFT-SR.git."],"url":"http://arxiv.org/abs/2308.05022v1"}
{"created":"2023-08-09 15:35:14","title":"An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures","abstract":"As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5s categorizations had an average accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.","sentences":["As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe.","High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity.","One way to prevent future breaches is by studying past failures.","However, traditional methods of analyzing these failures require manually reading and summarizing reports about them.","Automated support could reduce costs and allow analysis of more failures.","Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures.","In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches.","We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF).","We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact.","GPT 3.5s categorizations had an average accuracy of 68% and Bard had an accuracy of 58% over these dimensions.","We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts.","Future work can improve LLM performance in this context, and study a broader range of articles and failures."],"url":"http://arxiv.org/abs/2308.04898v1"}
{"created":"2023-08-09 15:31:17","title":"Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization","abstract":"While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem. Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution gap between forward and backward processes. We further introduce a bootstrapping algorithm to reduce the computation cost of the regularizer. Our experimental results on multiple image datasets show that our regularization effectively handles error propagation and significantly improves the performance of vanilla diffusion models.","sentences":["While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules.","However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation.","In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem.","Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant.","We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module.","Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution gap between forward and backward processes.","We further introduce a bootstrapping algorithm to reduce the computation cost of the regularizer.","Our experimental results on multiple image datasets show that our regularization effectively handles error propagation and significantly improves the performance of vanilla diffusion models."],"url":"http://arxiv.org/abs/2308.05021v1"}
{"created":"2023-08-09 15:30:48","title":"ProWis: A Visual Approach for Building, Managing, and Analyzing Weather Simulation Ensembles at Runtime","abstract":"Weather forecasting is essential for decision-making and is usually performed using numerical modeling. Numerical weather models, in turn, are complex tools that require specialized training and laborious setup and are challenging even for weather experts. Moreover, weather simulations are data-intensive computations and may take hours to days to complete. When the simulation is finished, the experts face challenges analyzing its outputs, a large mass of spatiotemporal and multivariate data. From the simulation setup to the analysis of results, working with weather simulations involves several manual and error-prone steps. The complexity of the problem increases exponentially when the experts must deal with ensembles of simulations, a frequent task in their daily duties. To tackle these challenges, we propose ProWis: an interactive and provenance-oriented system to help weather experts build, manage, and analyze simulation ensembles at runtime. Our system follows a human-in-the-loop approach to enable the exploration of multiple atmospheric variables and weather scenarios. ProWis was built in close collaboration with weather experts, and we demonstrate its effectiveness by presenting two case studies of rainfall events in Brazil.","sentences":["Weather forecasting is essential for decision-making and is usually performed using numerical modeling.","Numerical weather models, in turn, are complex tools that require specialized training and laborious setup and are challenging even for weather experts.","Moreover, weather simulations are data-intensive computations and may take hours to days to complete.","When the simulation is finished, the experts face challenges analyzing its outputs, a large mass of spatiotemporal and multivariate data.","From the simulation setup to the analysis of results, working with weather simulations involves several manual and error-prone steps.","The complexity of the problem increases exponentially when the experts must deal with ensembles of simulations, a frequent task in their daily duties.","To tackle these challenges, we propose ProWis: an interactive and provenance-oriented system to help weather experts build, manage, and analyze simulation ensembles at runtime.","Our system follows a human-in-the-loop approach to enable the exploration of multiple atmospheric variables and weather scenarios.","ProWis was built in close collaboration with weather experts, and we demonstrate its effectiveness by presenting two case studies of rainfall events in Brazil."],"url":"http://arxiv.org/abs/2308.05019v1"}
{"created":"2023-08-09 15:27:21","title":"When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis","abstract":"Novel Class Discovery (NCD) aims at inferring novel classes in an unlabeled set by leveraging prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for NCD. This paper bridges the gap by providing an analytical framework to formalize and investigate when and how known classes can help discover novel classes. Tailored to the NCD problem, we introduce a graph-theoretic representation that can be learned by a novel NCD Spectral Contrastive Loss (NSCL). Minimizing this objective is equivalent to factorizing the graph's adjacency matrix, which allows us to derive a provable error bound and provide the sufficient and necessary condition for NCD. Empirically, NSCL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees.","sentences":["Novel Class Discovery (NCD) aims at inferring novel classes in an unlabeled set by leveraging prior knowledge from a labeled set with known classes.","Despite its importance, there is a lack of theoretical foundations for NCD.","This paper bridges the gap by providing an analytical framework to formalize and investigate when and how known classes can help discover novel classes.","Tailored to the NCD problem, we introduce a graph-theoretic representation that can be learned by a novel NCD Spectral Contrastive Loss (NSCL).","Minimizing this objective is equivalent to factorizing the graph's adjacency matrix, which allows us to derive a provable error bound and provide the sufficient and necessary condition for NCD.","Empirically, NSCL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees."],"url":"http://arxiv.org/abs/2308.05017v1"}
{"created":"2023-08-09 15:14:16","title":"An Empirical Study of Bugs in Open-Source Federated Learning Framework","abstract":"Federated learning (FL), as a decentralized machine learning solution to the protection of users' private data, has become an important learning paradigm in recent years, especially since the enforcement of stricter laws and regulations in most countries. Therefore, a variety of FL frameworks are released to facilitate the development and application of federated learning. Despite the considerable amount of research on the security and privacy of FL models and systems, the security issues in FL frameworks have not been systematically studied yet. In this paper, we conduct the first empirical study on 1,112 FL framework bugs to investigate their characteristics. These bugs are manually collected, classified, and labeled from 12 open-source FL frameworks on GitHub. In detail, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fix patterns of these bugs and investigate their correlations and distributions on 23 logical components and two main application scenarios. From the results of our study, we present nine findings, discuss their implications, and propound several suggestions to FL framework developers and security researchers on the FL frameworks.","sentences":["Federated learning (FL), as a decentralized machine learning solution to the protection of users' private data, has become an important learning paradigm in recent years, especially since the enforcement of stricter laws and regulations in most countries.","Therefore, a variety of FL frameworks are released to facilitate the development and application of federated learning.","Despite the considerable amount of research on the security and privacy of FL models and systems, the security issues in FL frameworks have not been systematically studied yet.","In this paper, we conduct the first empirical study on 1,112 FL framework bugs to investigate their characteristics.","These bugs are manually collected, classified, and labeled from 12 open-source FL frameworks on GitHub.","In detail, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fix patterns of these bugs and investigate their correlations and distributions on 23 logical components and two main application scenarios.","From the results of our study, we present nine findings, discuss their implications, and propound several suggestions to FL framework developers and security researchers on the FL frameworks."],"url":"http://arxiv.org/abs/2308.05014v1"}
{"created":"2023-08-09 15:11:46","title":"Dual Intents Graph Modeling for User-centric Group Discovery","abstract":"Online groups have become increasingly prevalent, providing users with space to share experiences and explore interests. Therefore, user-centric group discovery task, i.e., recommending groups to users can help both users' online experiences and platforms' long-term developments. Existing recommender methods can not deal with this task as modeling user-group participation into a bipartite graph overlooks their item-side interests. Although there exist a few works attempting to address this task, they still fall short in fully preserving the social context and ensuring effective interest representation learning.   In this paper, we focus on exploring the intents that motivate users to participate in groups, which can be categorized into different types, like the social-intent and the personal interest-intent. The former refers to users joining a group affected by their social links, while the latter relates to users joining groups with like-minded people for self-enjoyment. To comprehend different intents, we propose a novel model, DiRec, that first models each intent separately and then fuses them together for predictions. Specifically, for social-intent, we introduce the hypergraph structure to model the relationship between groups and members, leading to a richer understanding of the social context. As for interest-intent, we employ novel structural refinement on the interactive graph to uncover more intricate user behaviors and group interests, realizing better representation learning of interests. Furthermore, we also observe the intent overlapping in real-world scenarios and devise a novel self-supervised learning loss that encourages such alignment for final recommendations. Extensive experiments on three public datasets show the significant improvement of DiRec over the state-of-the-art methods.","sentences":["Online groups have become increasingly prevalent, providing users with space to share experiences and explore interests.","Therefore, user-centric group discovery task, i.e., recommending groups to users can help both users' online experiences and platforms' long-term developments.","Existing recommender methods can not deal with this task as modeling user-group participation into a bipartite graph overlooks their item-side interests.","Although there exist a few works attempting to address this task, they still fall short in fully preserving the social context and ensuring effective interest representation learning.   ","In this paper, we focus on exploring the intents that motivate users to participate in groups, which can be categorized into different types, like the social-intent and the personal interest-intent.","The former refers to users joining a group affected by their social links, while the latter relates to users joining groups with like-minded people for self-enjoyment.","To comprehend different intents, we propose a novel model, DiRec, that first models each intent separately and then fuses them together for predictions.","Specifically, for social-intent, we introduce the hypergraph structure to model the relationship between groups and members, leading to a richer understanding of the social context.","As for interest-intent, we employ novel structural refinement on the interactive graph to uncover more intricate user behaviors and group interests, realizing better representation learning of interests.","Furthermore, we also observe the intent overlapping in real-world scenarios and devise a novel self-supervised learning loss that encourages such alignment for final recommendations.","Extensive experiments on three public datasets show the significant improvement of DiRec over the state-of-the-art methods."],"url":"http://arxiv.org/abs/2308.05013v1"}
{"created":"2023-08-09 15:11:37","title":"MetRoBERTa: Leveraging Traditional Customer Relationship Management Data to Develop a Transit-Topic-Aware Language Model","abstract":"Transit riders' feedback provided in ridership surveys, customer relationship management (CRM) channels, and in more recent times, through social media is key for transit agencies to better gauge the efficacy of their services and initiatives. Getting a holistic understanding of riders' experience through the feedback shared in those instruments is often challenging, mostly due to the open-ended, unstructured nature of text feedback. In this paper, we propose leveraging traditional transit CRM feedback to develop and deploy a transit-topic-aware large language model (LLM) capable of classifying open-ended text feedback to relevant transit-specific topics. First, we utilize semi-supervised learning to engineer a training dataset of 11 broad transit topics detected in a corpus of 6 years of customer feedback provided to the Washington Metropolitan Area Transit Authority (WMATA). We then use this dataset to train and thoroughly evaluate a language model based on the RoBERTa architecture. We compare our LLM, MetRoBERTa, to classical machine learning approaches utilizing keyword-based and lexicon representations. Our model outperforms those methods across all evaluation metrics, providing an average topic classification accuracy of 90%. Finally, we provide a value proposition of this work demonstrating how the language model, alongside additional text processing tools, can be applied to add structure to open-ended text sources of feedback like Twitter. The framework and results we present provide a pathway for an automated, generalizable approach for ingesting, visualizing, and reporting transit riders' feedback at scale, enabling agencies to better understand and improve customer experience.","sentences":["Transit riders' feedback provided in ridership surveys, customer relationship management (CRM) channels, and in more recent times, through social media is key for transit agencies to better gauge the efficacy of their services and initiatives.","Getting a holistic understanding of riders' experience through the feedback shared in those instruments is often challenging, mostly due to the open-ended, unstructured nature of text feedback.","In this paper, we propose leveraging traditional transit CRM feedback to develop and deploy a transit-topic-aware large language model (LLM) capable of classifying open-ended text feedback to relevant transit-specific topics.","First, we utilize semi-supervised learning to engineer a training dataset of 11 broad transit topics detected in a corpus of 6 years of customer feedback provided to the Washington Metropolitan Area Transit Authority (WMATA).","We then use this dataset to train and thoroughly evaluate a language model based on the RoBERTa architecture.","We compare our LLM, MetRoBERTa, to classical machine learning approaches utilizing keyword-based and lexicon representations.","Our model outperforms those methods across all evaluation metrics, providing an average topic classification accuracy of 90%.","Finally, we provide a value proposition of this work demonstrating how the language model, alongside additional text processing tools, can be applied to add structure to open-ended text sources of feedback like Twitter.","The framework and results we present provide a pathway for an automated, generalizable approach for ingesting, visualizing, and reporting transit riders' feedback at scale, enabling agencies to better understand and improve customer experience."],"url":"http://arxiv.org/abs/2308.05012v1"}
{"created":"2023-08-09 15:10:53","title":"Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories","abstract":"With the increasing volume of astronomical data generated by modern survey telescopes, automated pipelines and machine learning techniques have become crucial for analyzing and extracting knowledge from these datasets. Anomaly detection, i.e. the task of identifying irregular or unexpected patterns in the data, is a complex challenge in astronomy. In this paper, we propose Multi-Class Deep Support Vector Data Description (MCDSVDD), an extension of the state-of-the-art anomaly detection algorithm One-Class Deep SVDD, specifically designed to handle different inlier categories with distinct data distributions. MCDSVDD uses a neural network to map the data into hyperspheres, where each hypersphere represents a specific inlier category. The distance of each sample from the centers of these hyperspheres determines the anomaly score. We evaluate the effectiveness of MCDSVDD by comparing its performance with several anomaly detection algorithms on a large dataset of astronomical light-curves obtained from the Zwicky Transient Facility. Our results demonstrate the efficacy of MCDSVDD in detecting anomalous sources while leveraging the presence of different inlier categories. The code and the data needed to reproduce our results are publicly available at https://github.com/mperezcarrasco/AnomalyALeRCE.","sentences":["With the increasing volume of astronomical data generated by modern survey telescopes, automated pipelines and machine learning techniques have become crucial for analyzing and extracting knowledge from these datasets.","Anomaly detection, i.e. the task of identifying irregular or unexpected patterns in the data, is a complex challenge in astronomy.","In this paper, we propose Multi-Class Deep Support Vector Data Description (MCDSVDD), an extension of the state-of-the-art anomaly detection algorithm One-Class Deep SVDD, specifically designed to handle different inlier categories with distinct data distributions.","MCDSVDD uses a neural network to map the data into hyperspheres, where each hypersphere represents a specific inlier category.","The distance of each sample from the centers of these hyperspheres determines the anomaly score.","We evaluate the effectiveness of MCDSVDD by comparing its performance with several anomaly detection algorithms on a large dataset of astronomical light-curves obtained from the Zwicky Transient Facility.","Our results demonstrate the efficacy of MCDSVDD in detecting anomalous sources while leveraging the presence of different inlier categories.","The code and the data needed to reproduce our results are publicly available at https://github.com/mperezcarrasco/AnomalyALeRCE."],"url":"http://arxiv.org/abs/2308.05011v1"}
{"created":"2023-08-09 15:08:29","title":"Random-Walk Metaball-Imaging Discrete Element Lattice Boltzmann Method for 3D Solute Transport in Fluid-Particle Systems with Complex Granular Morphologies","abstract":"Solute transport in fluid-particle systems is a fundamental process in numerous scientific and engineering disciplines. The simulation of it necessitates the consideration of solid particles with intricate shapes and sizes. To address this challenge, this study proposes the Random-Walk Metaball-Imaging Discrete Element Lattice Boltzmann Method (RW-MI-DELBM). In this model, we reconstruct particle geometries with the Metaball-Imaging algorithm, capture the particle behavior using the Discrete Element Method (DEM), simulate fluid behavior by the Lattice Boltzmann Method (LBM), and represent solute behavior through the Random Walk Method (RWM). Through the integration of these techniques with specially designed boundary conditions, we achieve to simulate the solute transport in fluid-particle systems comprising complex particle morphologies. Thorough validations, including analytical soluutions and experiments, are performed to assess the robustness and accuracy of this framework. The results demonstrate that the proposed framework can accurately capture the complex dynamics of solute transport under strict mass conservation. In particular, an investigation is carried out to assess the influence of particle morphologies on solute transport in a 3D oscillator, with a focus on identifying correlations between shape features and dispersion coefficients. Notably, all selected shape features exhibited strong correlations with the dispersion coefficient, indicating the significant influence of particle shapes on transport phenomena. However, due to the complexity of the relationship and the limited number of simulations, no clear patterns could be observed. Further comprehensive analyses incorporating a broader range of shape features and varying conditions are necessary to fully comprehend their collective influence on the dispersion coefficient.","sentences":["Solute transport in fluid-particle systems is a fundamental process in numerous scientific and engineering disciplines.","The simulation of it necessitates the consideration of solid particles with intricate shapes and sizes.","To address this challenge, this study proposes the Random-Walk Metaball-Imaging Discrete Element Lattice Boltzmann Method (RW-MI-DELBM).","In this model, we reconstruct particle geometries with the Metaball-Imaging algorithm, capture the particle behavior using the Discrete Element Method (DEM), simulate fluid behavior by the Lattice Boltzmann Method (LBM), and represent solute behavior through the Random Walk Method (RWM).","Through the integration of these techniques with specially designed boundary conditions, we achieve to simulate the solute transport in fluid-particle systems comprising complex particle morphologies.","Thorough validations, including analytical soluutions and experiments, are performed to assess the robustness and accuracy of this framework.","The results demonstrate that the proposed framework can accurately capture the complex dynamics of solute transport under strict mass conservation.","In particular, an investigation is carried out to assess the influence of particle morphologies on solute transport in a 3D oscillator, with a focus on identifying correlations between shape features and dispersion coefficients.","Notably, all selected shape features exhibited strong correlations with the dispersion coefficient, indicating the significant influence of particle shapes on transport phenomena.","However, due to the complexity of the relationship and the limited number of simulations, no clear patterns could be observed.","Further comprehensive analyses incorporating a broader range of shape features and varying conditions are necessary to fully comprehend their collective influence on the dispersion coefficient."],"url":"http://arxiv.org/abs/2308.05007v1"}
{"created":"2023-08-09 14:50:02","title":"Directed differential equation discovery using modified mutation and cross-over operators","abstract":"The discovery of equations with knowledge of the process origin is a tempting prospect. However, most equation discovery tools rely on gradient methods, which offer limited control over parameters. An alternative approach is the evolutionary equation discovery, which allows modification of almost every optimization stage. In this paper, we examine the modifications that can be introduced into the evolutionary operators of the equation discovery algorithm, taking inspiration from directed evolution techniques employed in fields such as chemistry and biology. The resulting approach, dubbed directed equation discovery, demonstrates a greater ability to converge towards accurate solutions than the conventional method. To support our findings, we present experiments based on Burgers', wave, and Korteweg--de Vries equations.","sentences":["The discovery of equations with knowledge of the process origin is a tempting prospect.","However, most equation discovery tools rely on gradient methods, which offer limited control over parameters.","An alternative approach is the evolutionary equation discovery, which allows modification of almost every optimization stage.","In this paper, we examine the modifications that can be introduced into the evolutionary operators of the equation discovery algorithm, taking inspiration from directed evolution techniques employed in fields such as chemistry and biology.","The resulting approach, dubbed directed equation discovery, demonstrates a greater ability to converge towards accurate solutions than the conventional method.","To support our findings, we present experiments based on Burgers', wave, and Korteweg--de Vries equations."],"url":"http://arxiv.org/abs/2308.04996v1"}
{"created":"2023-08-09 14:48:31","title":"IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models","abstract":"The availability of large-scale authentic face databases has been crucial to the significant advances made in face recognition research over the past decade. However, legal and ethical concerns led to the recent retraction of many of these databases by their creators, raising questions about the continuity of future face recognition research without one of its key resources. Synthetic datasets have emerged as a promising alternative to privacy-sensitive authentic data for face recognition development. However, recent synthetic datasets that are used to train face recognition models suffer either from limitations in intra-class diversity or cross-class (identity) discrimination, leading to less optimal accuracies, far away from the accuracies achieved by models trained on authentic data. This paper targets this issue by proposing IDiff-Face, a novel approach based on conditional latent diffusion models for synthetic identity generation with realistic identity variations for face recognition training. Through extensive evaluations, our proposed synthetic-based face recognition approach pushed the limits of state-of-the-art performances, achieving, for example, 98.00% accuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from the recent synthetic-based face recognition solutions with 95.40% and bridging the gap to authentic-based face recognition with 99.82% accuracy.","sentences":["The availability of large-scale authentic face databases has been crucial to the significant advances made in face recognition research over the past decade.","However, legal and ethical concerns led to the recent retraction of many of these databases by their creators, raising questions about the continuity of future face recognition research without one of its key resources.","Synthetic datasets have emerged as a promising alternative to privacy-sensitive authentic data for face recognition development.","However, recent synthetic datasets that are used to train face recognition models suffer either from limitations in intra-class diversity or cross-class (identity) discrimination, leading to less optimal accuracies, far away from the accuracies achieved by models trained on authentic data.","This paper targets this issue by proposing IDiff-Face, a novel approach based on conditional latent diffusion models for synthetic identity generation with realistic identity variations for face recognition training.","Through extensive evaluations, our proposed synthetic-based face recognition approach pushed the limits of state-of-the-art performances, achieving, for example, 98.00% accuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from the recent synthetic-based face recognition solutions with 95.40% and bridging the gap to authentic-based face recognition with 99.82% accuracy."],"url":"http://arxiv.org/abs/2308.04995v1"}
{"created":"2023-08-09 14:45:13","title":"AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities","abstract":"Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text and image) for a comprehensive understanding of entities. Despite the recent progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature of entities, limiting the ability to comprehend entities from various perspectives. In this paper, we construct AspectMMKG, the first MMKG with aspect-related images by matching images to different entity aspects. Specifically, we collect aspect-related images from a knowledge base, and further extract aspect-related sentences from the knowledge base as queries to retrieve a large number of aspect-related images via an online image search engine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and 645,383 aspect-related images. We demonstrate the usability of AspectMMKG in entity aspect linking (EAL) downstream task and show that previous EAL models achieve a new state-of-the-art performance with the help of AspectMMKG. To facilitate the research on aspect-related MMKG, we further propose an aspect-related image retrieval (AIR) model, that aims to correct and expand aspect-related images in AspectMMKG. We train an AIR model to learn the relationship between entity image and entity aspect-related images by incorporating entity image, aspect, and aspect image information. Experimental results indicate that the AIR model could retrieve suitable images for a given entity w.r.t different aspects.","sentences":["Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text and image) for a comprehensive understanding of entities.","Despite the recent progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature of entities, limiting the ability to comprehend entities from various perspectives.","In this paper, we construct AspectMMKG, the first MMKG with aspect-related images by matching images to different entity aspects.","Specifically, we collect aspect-related images from a knowledge base, and further extract aspect-related sentences from the knowledge base as queries to retrieve a large number of aspect-related images via an online image search engine.","Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and 645,383 aspect-related images.","We demonstrate the usability of AspectMMKG in entity aspect linking (EAL) downstream task and show that previous EAL models achieve a new state-of-the-art performance with the help of AspectMMKG.","To facilitate the research on aspect-related MMKG, we further propose an aspect-related image retrieval (AIR) model, that aims to correct and expand aspect-related images in AspectMMKG.","We train an AIR model to learn the relationship between entity image and entity aspect-related images by incorporating entity image, aspect, and aspect image information.","Experimental results indicate that the AIR model could retrieve suitable images for a given entity w.r.t different aspects."],"url":"http://arxiv.org/abs/2308.04992v1"}
{"created":"2023-08-09 14:43:10","title":"Foreground Object Search by Distilling Composite Image Feature","abstract":"Foreground object search (FOS) aims to find compatible foreground objects for a given background image, producing realistic composite image. We observe that competitive retrieval performance could be achieved by using a discriminator to predict the compatibility of composite image, but this approach has unaffordable time cost. To this end, we propose a novel FOS method via distilling composite feature (DiscoFOS). Specifically, the abovementioned discriminator serves as teacher network. The student network employs two encoders to extract foreground feature and background feature. Their interaction output is enforced to match the composite image feature from the teacher network. Additionally, previous works did not release their datasets, so we contribute two datasets for FOS task: S-FOSD dataset with synthetic composite images and R-FOSD dataset with real composite images. Extensive experiments on our two datasets demonstrate the superiority of the proposed method over previous approaches. The dataset and code are available at https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD.","sentences":["Foreground object search (FOS) aims to find compatible foreground objects for a given background image, producing realistic composite image.","We observe that competitive retrieval performance could be achieved by using a discriminator to predict the compatibility of composite image, but this approach has unaffordable time cost.","To this end, we propose a novel FOS method via distilling composite feature (DiscoFOS).","Specifically, the abovementioned discriminator serves as teacher network.","The student network employs two encoders to extract foreground feature and background feature.","Their interaction output is enforced to match the composite image feature from the teacher network.","Additionally, previous works did not release their datasets, so we contribute two datasets for FOS task: S-FOSD dataset with synthetic composite images and R-FOSD dataset with real composite images.","Extensive experiments on our two datasets demonstrate the superiority of the proposed method over previous approaches.","The dataset and code are available at https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD."],"url":"http://arxiv.org/abs/2308.04990v1"}
{"created":"2023-08-09 14:40:51","title":"Self-supervised Landmark Learning with Deformation Reconstruction and Cross-subject Consistency Objectives","abstract":"A Point Distribution Model (PDM) is the basis of a Statistical Shape Model (SSM) that relies on a set of landmark points to represent a shape and characterize the shape variation. In this work, we present a self-supervised approach to extract landmark points from a given registration model for the PDMs. Based on the assumption that the landmarks are the points that have the most influence on registration, existing works learn a point-based registration model with a small number of points to estimate the landmark points that influence the deformation the most. However, such approaches assume that the deformation can be captured by point-based registration and quality landmarks can be learned solely with the deformation capturing objective. We argue that data with complicated deformations can not easily be modeled with point-based registration when only a limited number of points is used to extract influential landmark points. Further, landmark consistency is not assured in existing approaches In contrast, we propose to extract landmarks based on a given registration model, which is tailored for the target data, so we can obtain more accurate correspondences. Secondly, to establish the anatomical consistency of the predicted landmarks, we introduce a landmark discovery loss to explicitly encourage the model to predict the landmarks that are anatomically consistent across subjects. We conduct experiments on an osteoarthritis progression prediction task and show our method outperforms existing image-based and point-based approaches.","sentences":["A Point Distribution Model (PDM) is the basis of a Statistical Shape Model (SSM) that relies on a set of landmark points to represent a shape and characterize the shape variation.","In this work, we present a self-supervised approach to extract landmark points from a given registration model for the PDMs.","Based on the assumption that the landmarks are the points that have the most influence on registration, existing works learn a point-based registration model with a small number of points to estimate the landmark points that influence the deformation the most.","However, such approaches assume that the deformation can be captured by point-based registration and quality landmarks can be learned solely with the deformation capturing objective.","We argue that data with complicated deformations can not easily be modeled with point-based registration when only a limited number of points is used to extract influential landmark points.","Further, landmark consistency is not assured in existing approaches In contrast, we propose to extract landmarks based on a given registration model, which is tailored for the target data, so we can obtain more accurate correspondences.","Secondly, to establish the anatomical consistency of the predicted landmarks, we introduce a landmark discovery loss to explicitly encourage the model to predict the landmarks that are anatomically consistent across subjects.","We conduct experiments on an osteoarthritis progression prediction task and show our method outperforms existing image-based and point-based approaches."],"url":"http://arxiv.org/abs/2308.04987v1"}
{"created":"2023-08-09 14:31:57","title":"Exploring Multilingual Text Data Distillation","abstract":"With the rise of deep learning, large datasets and complex models have become common, requiring significant computing power. To address this, data distillation has emerged as a technique to quickly train models with lower memory and time requirements. However, data distillation on text-based datasets hasn't been explored much because of the challenges rising due to its discrete nature. Additionally, existing dataset distillation methods often struggle to generalize to new architectures. In the paper, we propose several data distillation techniques for multilingual text classification datasets using language-model-based learning methods. We conduct experiments to analyze their performance in terms of classification strength, and cross-architecture generalization. Furthermore, we investigate the language-specific fairness of the data summaries generated by these methods. Our approach builds upon existing techniques, enhancing cross-architecture generalization in the text data distillation domain.","sentences":["With the rise of deep learning, large datasets and complex models have become common, requiring significant computing power.","To address this, data distillation has emerged as a technique to quickly train models with lower memory and time requirements.","However, data distillation on text-based datasets hasn't been explored much because of the challenges rising due to its discrete nature.","Additionally, existing dataset distillation methods often struggle to generalize to new architectures.","In the paper, we propose several data distillation techniques for multilingual text classification datasets using language-model-based learning methods.","We conduct experiments to analyze their performance in terms of classification strength, and cross-architecture generalization.","Furthermore, we investigate the language-specific fairness of the data summaries generated by these methods.","Our approach builds upon existing techniques, enhancing cross-architecture generalization in the text data distillation domain."],"url":"http://arxiv.org/abs/2308.04982v1"}
{"created":"2023-08-09 14:22:18","title":"Transferable Models for Bioacoustics with Human Language Supervision","abstract":"Passive acoustic monitoring offers a scalable, non-invasive method for tracking global biodiversity and anthropogenic impacts on species. Although deep learning has become a vital tool for processing this data, current models are inflexible, typically cover only a handful of species, and are limited by data scarcity. In this work, we propose BioLingual, a new model for bioacoustics based on contrastive language-audio pretraining. We first aggregate bioacoustic archives into a language-audio dataset, called AnimalSpeak, with over a million audio-caption pairs holding information on species, vocalization context, and animal behavior. After training on this dataset to connect language and audio representations, our model can identify over a thousand species' calls across taxa, complete bioacoustic tasks zero-shot, and retrieve animal vocalization recordings from natural text queries. When fine-tuned, BioLingual sets a new state-of-the-art on nine tasks in the Benchmark of Animal Sounds. Given its broad taxa coverage and ability to be flexibly queried in human language, we believe this model opens new paradigms in ecological monitoring and research, including free-text search on the world's acoustic monitoring archives. We open-source our models, dataset, and code.","sentences":["Passive acoustic monitoring offers a scalable, non-invasive method for tracking global biodiversity and anthropogenic impacts on species.","Although deep learning has become a vital tool for processing this data, current models are inflexible, typically cover only a handful of species, and are limited by data scarcity.","In this work, we propose BioLingual, a new model for bioacoustics based on contrastive language-audio pretraining.","We first aggregate bioacoustic archives into a language-audio dataset, called AnimalSpeak, with over a million audio-caption pairs holding information on species, vocalization context, and animal behavior.","After training on this dataset to connect language and audio representations, our model can identify over a thousand species' calls across taxa, complete bioacoustic tasks zero-shot, and retrieve animal vocalization recordings from natural text queries.","When fine-tuned, BioLingual sets a new state-of-the-art on nine tasks in the Benchmark of Animal Sounds.","Given its broad taxa coverage and ability to be flexibly queried in human language, we believe this model opens new paradigms in ecological monitoring and research, including free-text search on the world's acoustic monitoring archives.","We open-source our models, dataset, and code."],"url":"http://arxiv.org/abs/2308.04978v1"}
{"created":"2023-08-09 14:14:57","title":"can-train-and-test: A Curated CAN Dataset for Automotive Intrusion Detection","abstract":"When it comes to in-vehicle networks (IVNs), the controller area network -- CAN -- bus dominates the market; automobiles manufactured and sold around the world depend on the CAN bus for safety-critical communications between various components of the vehicle (e.g., the engine, the transmission, the steering column). Unfortunately, the CAN bus is inherently insecure; in fact, it completely lacks controls such as authentication, authorization, and confidentiality (i.e., encryption). Therefore, researchers have travailed to develop automotive security enhancements. The automotive intrusion detection system (IDS) is especially popular in the literature -- due to its relatively low cost in terms of money, resource utilization, and implementation effort. That said, developing and evaluating an automotive IDS is often challenging; if researchers do not have access to a test vehicle, then they are forced to depend on publicly available CAN data -- which is not without limitations. Lack of access to adequate CAN data, then, becomes a barrier to entry into automotive security research.   We seek to lower that barrier to entry by introducing a new CAN dataset to facilitate the development and evaluation of automotive IDSs. Our dataset, dubbed can-train-and-test, provides CAN data from four different vehicles produced by two different manufacturers. The attack captures for each vehicle model are equivalent, enabling researchers to assess the ability of a given IDS to generalize to different vehicle models and even different vehicle manufacturers. Our dataset contains replayable .log files as well as labeled and unlabeled .csv files, thereby meeting a variety of development and evaluation needs. Furthermore, can-train-and-test offers nine unique attacks, ranging from denial of service (DoS) to gear spoofing to standstill...","sentences":["When it comes to in-vehicle networks (IVNs), the controller area network -- CAN -- bus dominates the market; automobiles manufactured and sold around the world depend on the CAN bus for safety-critical communications between various components of the vehicle (e.g., the engine, the transmission, the steering column).","Unfortunately, the CAN bus is inherently insecure; in fact, it completely lacks controls such as authentication, authorization, and confidentiality (i.e., encryption).","Therefore, researchers have travailed to develop automotive security enhancements.","The automotive intrusion detection system (IDS) is especially popular in the literature -- due to its relatively low cost in terms of money, resource utilization, and implementation effort.","That said, developing and evaluating an automotive IDS is often challenging; if researchers do not have access to a test vehicle, then they are forced to depend on publicly available CAN data -- which is not without limitations.","Lack of access to adequate CAN data, then, becomes a barrier to entry into automotive security research.   ","We seek to lower that barrier to entry by introducing a new CAN dataset to facilitate the development and evaluation of automotive IDSs.","Our dataset, dubbed can-train-and-test, provides CAN data from four different vehicles produced by two different manufacturers.","The attack captures for each vehicle model are equivalent, enabling researchers to assess the ability of a given IDS to generalize to different vehicle models and even different vehicle manufacturers.","Our dataset contains replayable .log files as well as labeled and unlabeled .csv files, thereby meeting a variety of development and evaluation needs.","Furthermore, can-train-and-test offers nine unique attacks, ranging from denial of service (DoS) to gear spoofing to standstill..."],"url":"http://arxiv.org/abs/2308.04972v1"}
{"created":"2023-08-09 13:58:03","title":"Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning","abstract":"ModSecurity is widely recognized as the standard open-source Web Application Firewall (WAF), maintained by the OWASP Foundation. It detects malicious requests by matching them against the Core Rule Set, identifying well-known attack patterns. Each rule in the CRS is manually assigned a weight, based on the severity of the corresponding attack, and a request is detected as malicious if the sum of the weights of the firing rules exceeds a given threshold. In this work, we show that this simple strategy is largely ineffective for detecting SQL injection (SQLi) attacks, as it tends to block many legitimate requests, while also being vulnerable to adversarial SQLi attacks, i.e., attacks intentionally manipulated to evade detection. To overcome these issues, we design a robust machine learning model, named AdvModSec, which uses the CRS rules as input features, and it is trained to detect adversarial SQLi attacks. Our experiments show that AdvModSec, being trained on the traffic directed towards the protected web services, achieves a better trade-off between detection and false positive rates, improving the detection rate of the vanilla version of ModSecurity with CRS by 21%. Moreover, our approach is able to improve its adversarial robustness against adversarial SQLi attacks by 42%, thereby taking a step forward towards building more robust and trustworthy WAFs.","sentences":["ModSecurity is widely recognized as the standard open-source Web Application Firewall (WAF), maintained by the OWASP Foundation.","It detects malicious requests by matching them against the Core Rule Set, identifying well-known attack patterns.","Each rule in the CRS is manually assigned a weight, based on the severity of the corresponding attack, and a request is detected as malicious if the sum of the weights of the firing rules exceeds a given threshold.","In this work, we show that this simple strategy is largely ineffective for detecting SQL injection (SQLi) attacks, as it tends to block many legitimate requests, while also being vulnerable to adversarial SQLi attacks, i.e., attacks intentionally manipulated to evade detection.","To overcome these issues, we design a robust machine learning model, named AdvModSec, which uses the CRS rules as input features, and it is trained to detect adversarial SQLi attacks.","Our experiments show that AdvModSec, being trained on the traffic directed towards the protected web services, achieves a better trade-off between detection and false positive rates, improving the detection rate of the vanilla version of ModSecurity with CRS by 21%.","Moreover, our approach is able to improve its adversarial robustness against adversarial SQLi attacks by 42%, thereby taking a step forward towards building more robust and trustworthy WAFs."],"url":"http://arxiv.org/abs/2308.04964v1"}
{"created":"2023-08-09 13:52:41","title":"CasCIFF: A Cross-Domain Information Fusion Framework Tailored for Cascade Prediction in Social Networks","abstract":"Existing approaches for information cascade prediction fall into three main categories: feature-driven methods, point process-based methods, and deep learning-based methods. Among them, deep learning-based methods, characterized by its superior learning and representation capabilities, mitigates the shortcomings inherent of the other methods. However, current deep learning methods still face several persistent challenges. In particular, accurate representation of user attributes remains problematic due to factors such as fake followers and complex network configurations. Previous algorithms that focus on the sequential order of user activations often neglect the rich insights offered by activation timing. Furthermore, these techniques often fail to holistically integrate temporal and structural aspects, thus missing the nuanced propagation trends inherent in information cascades.To address these issues, we propose the Cross-Domain Information Fusion Framework (CasCIFF), which is tailored for information cascade prediction. This framework exploits multi-hop neighborhood information to make user embeddings robust. When embedding cascades, the framework intentionally incorporates timestamps, endowing it with the ability to capture evolving patterns of information diffusion. In particular, the CasCIFF seamlessly integrates the tasks of user classification and cascade prediction into a consolidated framework, thereby allowing the extraction of common features that prove useful for all tasks, a strategy anchored in the principles of multi-task learning.","sentences":["Existing approaches for information cascade prediction fall into three main categories: feature-driven methods, point process-based methods, and deep learning-based methods.","Among them, deep learning-based methods, characterized by its superior learning and representation capabilities, mitigates the shortcomings inherent of the other methods.","However, current deep learning methods still face several persistent challenges.","In particular, accurate representation of user attributes remains problematic due to factors such as fake followers and complex network configurations.","Previous algorithms that focus on the sequential order of user activations often neglect the rich insights offered by activation timing.","Furthermore, these techniques often fail to holistically integrate temporal and structural aspects, thus missing the nuanced propagation trends inherent in information cascades.","To address these issues, we propose the Cross-Domain Information Fusion Framework (CasCIFF), which is tailored for information cascade prediction.","This framework exploits multi-hop neighborhood information to make user embeddings robust.","When embedding cascades, the framework intentionally incorporates timestamps, endowing it with the ability to capture evolving patterns of information diffusion.","In particular, the CasCIFF seamlessly integrates the tasks of user classification and cascade prediction into a consolidated framework, thereby allowing the extraction of common features that prove useful for all tasks, a strategy anchored in the principles of multi-task learning."],"url":"http://arxiv.org/abs/2308.04961v1"}
{"created":"2023-08-09 13:50:00","title":"Representation Learning for Audio Privacy Preservation using Source Separation and Robust Adversarial Learning","abstract":"Privacy preservation has long been a concern in smart acoustic monitoring systems, where speech can be passively recorded along with a target signal in the system's operating environment. In this study, we propose the integration of two commonly used approaches in privacy preservation: source separation and adversarial representation learning. The proposed system learns the latent representation of audio recordings such that it prevents differentiating between speech and non-speech recordings. Initially, the source separation network filters out some of the privacy-sensitive data, and during the adversarial learning process, the system will learn privacy-preserving representation on the filtered signal. We demonstrate the effectiveness of our proposed method by comparing our method against systems without source separation, without adversarial learning, and without both. Overall, our results suggest that the proposed system can significantly improve speech privacy preservation compared to that of using source separation or adversarial learning solely while maintaining good performance in the acoustic monitoring task.","sentences":["Privacy preservation has long been a concern in smart acoustic monitoring systems, where speech can be passively recorded along with a target signal in the system's operating environment.","In this study, we propose the integration of two commonly used approaches in privacy preservation: source separation and adversarial representation learning.","The proposed system learns the latent representation of audio recordings such that it prevents differentiating between speech and non-speech recordings.","Initially, the source separation network filters out some of the privacy-sensitive data, and during the adversarial learning process, the system will learn privacy-preserving representation on the filtered signal.","We demonstrate the effectiveness of our proposed method by comparing our method against systems without source separation, without adversarial learning, and without both.","Overall, our results suggest that the proposed system can significantly improve speech privacy preservation compared to that of using source separation or adversarial learning solely while maintaining good performance in the acoustic monitoring task."],"url":"http://arxiv.org/abs/2308.04960v1"}
{"created":"2023-08-09 13:44:35","title":"Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks","abstract":"Advanced Air Mobility (AAM) introduces a new, efficient mode of transportation with the use of vehicle autonomy and electrified aircraft to provide increasingly autonomous transportation between previously underserved markets. Safe and efficient navigation of low altitude aircraft through highly dense environments requires the integration of a multitude of complex observations, such as surveillance, knowledge of vehicle dynamics, and weather. The processing and reasoning on these observations pose challenges due to the various sources of uncertainty in the information while ensuring cooperation with a variable number of aircraft in the airspace. These challenges coupled with the requirement to make safety-critical decisions in real-time rule out the use of conventional separation assurance techniques. We present a decentralized reinforcement learning framework to provide autonomous self-separation capabilities within AAM corridors with the use of speed and vertical maneuvers. The problem is formulated as a Markov Decision Process and solved by developing a novel extension to the sample-efficient, off-policy soft actor-critic (SAC) algorithm. We introduce the use of attention networks for variable-length observation processing and a distributed computing architecture to achieve high training sample throughput as compared to existing approaches. A comprehensive numerical study shows that the proposed framework can ensure safe and efficient separation of aircraft in high density, dynamic environments with various sources of uncertainty.","sentences":["Advanced Air Mobility (AAM) introduces a new, efficient mode of transportation with the use of vehicle autonomy and electrified aircraft to provide increasingly autonomous transportation between previously underserved markets.","Safe and efficient navigation of low altitude aircraft through highly dense environments requires the integration of a multitude of complex observations, such as surveillance, knowledge of vehicle dynamics, and weather.","The processing and reasoning on these observations pose challenges due to the various sources of uncertainty in the information while ensuring cooperation with a variable number of aircraft in the airspace.","These challenges coupled with the requirement to make safety-critical decisions in real-time rule out the use of conventional separation assurance techniques.","We present a decentralized reinforcement learning framework to provide autonomous self-separation capabilities within AAM corridors with the use of speed and vertical maneuvers.","The problem is formulated as a Markov Decision Process and solved by developing a novel extension to the sample-efficient, off-policy soft actor-critic (SAC) algorithm.","We introduce the use of attention networks for variable-length observation processing and a distributed computing architecture to achieve high training sample throughput as compared to existing approaches.","A comprehensive numerical study shows that the proposed framework can ensure safe and efficient separation of aircraft in high density, dynamic environments with various sources of uncertainty."],"url":"http://arxiv.org/abs/2308.04958v1"}
{"created":"2023-08-09 13:38:58","title":"Wirelessly Powered Federated Learning Networks: Joint Power Transfer, Data Sensing, Model Training, and Resource Allocation","abstract":"Federated learning (FL) has found many successes in wireless networks; however, the implementation of FL has been hindered by the energy limitation of mobile devices (MDs) and the availability of training data at MDs. How to integrate wireless power transfer and mobile crowdsensing towards sustainable FL solutions is a research topic entirely missing from the open literature. This work for the first time investigates a resource allocation problem in collaborative sensing-assisted sustainable FL (S2FL) networks with the goal of minimizing the total completion time. We investigate a practical harvesting-sensing-training-transmitting protocol in which energy-limited MDs first harvest energy from RF signals, use it to gain a reward for user participation, sense the training data from the environment, train the local models at MDs, and transmit the model updates to the server. The total completion time minimization problem of jointly optimizing power transfer, transmit power allocation, data sensing, bandwidth allocation, local model training, and data transmission is complicated due to the non-convex objective function, highly non-convex constraints, and strongly coupled variables. We propose a computationally-efficient path-following algorithm to obtain the optimal solution via the decomposition technique. In particular, inner convex approximations are developed for the resource allocation subproblem, and the subproblems are performed alternatively in an iterative fashion. Simulation results are provided to evaluate the effectiveness of the proposed S2FL algorithm in reducing the completion time up to 21.45% in comparison with other benchmark schemes. Further, we investigate an extension of our work from frequency division multiple access (FDMA) to non-orthogonal multiple access (NOMA) and show that NOMA can speed up the total completion time 8.36% on average of the considered FL system.","sentences":["Federated learning (FL) has found many successes in wireless networks; however, the implementation of FL has been hindered by the energy limitation of mobile devices (MDs) and the availability of training data at MDs.","How to integrate wireless power transfer and mobile crowdsensing towards sustainable FL solutions is a research topic entirely missing from the open literature.","This work for the first time investigates a resource allocation problem in collaborative sensing-assisted sustainable FL (S2FL) networks with the goal of minimizing the total completion time.","We investigate a practical harvesting-sensing-training-transmitting protocol in which energy-limited MDs first harvest energy from RF signals, use it to gain a reward for user participation, sense the training data from the environment, train the local models at MDs, and transmit the model updates to the server.","The total completion time minimization problem of jointly optimizing power transfer, transmit power allocation, data sensing, bandwidth allocation, local model training, and data transmission is complicated due to the non-convex objective function, highly non-convex constraints, and strongly coupled variables.","We propose a computationally-efficient path-following algorithm to obtain the optimal solution via the decomposition technique.","In particular, inner convex approximations are developed for the resource allocation subproblem, and the subproblems are performed alternatively in an iterative fashion.","Simulation results are provided to evaluate the effectiveness of the proposed S2FL algorithm in reducing the completion time up to 21.45% in comparison with other benchmark schemes.","Further, we investigate an extension of our work from frequency division multiple access (FDMA) to non-orthogonal multiple access (NOMA) and show that NOMA can speed up the total completion time 8.36% on average of the considered FL system."],"url":"http://arxiv.org/abs/2308.04953v1"}
{"created":"2023-08-09 13:38:52","title":"Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation","abstract":"Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic Segmentation (FSS) to simultaneously segment unseen classes and seen classes during evaluation. Previous works leverage additional branch or prototypical aggregation to eliminate the constrained setting of FSS. However, representation division and embedding prejudice, which heavily results in poor performance of GFSS, have not been synthetical considered. We address the aforementioned problems by jointing the prototypical kernel learning and open-set foreground perception. Specifically, a group of learnable kernels is proposed to perform segmentation with each kernel in charge of a stuff class. Then, we explore to merge the prototypical learning to the update of base-class kernels, which is consistent with the prototype knowledge aggregation of few-shot novel classes. In addition, a foreground contextual perception module cooperating with conditional bias based inference is adopted to perform class-agnostic as well as open-set foreground detection, thus to mitigate the embedding prejudice and prevent novel targets from being misclassified as background. Moreover, we also adjust our method to the Class Incremental Few-shot Semantic Segmentation (CIFSS) which takes the knowledge of novel classes in a incremental stream. Extensive experiments on PASCAL-5i and COCO-20i datasets demonstrate that our method performs better than previous state-of-the-art.","sentences":["Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic Segmentation (FSS) to simultaneously segment unseen classes and seen classes during evaluation.","Previous works leverage additional branch or prototypical aggregation to eliminate the constrained setting of FSS.","However, representation division and embedding prejudice, which heavily results in poor performance of GFSS, have not been synthetical considered.","We address the aforementioned problems by jointing the prototypical kernel learning and open-set foreground perception.","Specifically, a group of learnable kernels is proposed to perform segmentation with each kernel in charge of a stuff class.","Then, we explore to merge the prototypical learning to the update of base-class kernels, which is consistent with the prototype knowledge aggregation of few-shot novel classes.","In addition, a foreground contextual perception module cooperating with conditional bias based inference is adopted to perform class-agnostic as well as open-set foreground detection, thus to mitigate the embedding prejudice and prevent novel targets from being misclassified as background.","Moreover, we also adjust our method to the Class Incremental Few-shot Semantic Segmentation (CIFSS) which takes the knowledge of novel classes in a incremental stream.","Extensive experiments on PASCAL-5i and COCO-20i datasets demonstrate that our method performs better than previous state-of-the-art."],"url":"http://arxiv.org/abs/2308.04952v1"}
{"created":"2023-08-09 13:33:27","title":"Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection","abstract":"Fake news is fake material in a news media format but is not processed properly by news agencies. The fake material can provoke or defame significant entities or individuals or potentially even for the personal interests of the creators, causing problems for society. Distinguishing fake news and real news is challenging due to limited of domain knowledge and time constraints. According to the survey, the top three areas most exposed to hoaxes and misinformation by residents are in Banten, DKI Jakarta and West Java. The model of transformers is referring to an approach in the field of artificial intelligence (AI) in natural language processing utilizing the deep learning architectures. Transformers exercise a powerful attention mechanism to process text in parallel and produce rich and contextual word representations. A previous study indicates a superior performance of a transformer model known as BERT over and above non transformer approach. However, some studies suggest the performance can be improved with the use of improved BERT models known as ALBERT and RoBERTa. However, the modified BERT models are not well explored for detecting fake news in Bahasa Indonesia. In this research, we explore those transformer models and found that ALBERT outperformed other models with 87.6% accuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch) respectively. Source code available at: https://github.com/Shafna81/fakenewsdetection.git","sentences":["Fake news is fake material in a news media format but is not processed properly by news agencies.","The fake material can provoke or defame significant entities or individuals or potentially even for the personal interests of the creators, causing problems for society.","Distinguishing fake news and real news is challenging due to limited of domain knowledge and time constraints.","According to the survey, the top three areas most exposed to hoaxes and misinformation by residents are in Banten, DKI Jakarta and West Java.","The model of transformers is referring to an approach in the field of artificial intelligence (AI) in natural language processing utilizing the deep learning architectures.","Transformers exercise a powerful attention mechanism to process text in parallel and produce rich and contextual word representations.","A previous study indicates a superior performance of a transformer model known as BERT over and above non transformer approach.","However, some studies suggest the performance can be improved with the use of improved BERT models known as ALBERT and RoBERTa.","However, the modified BERT models are not well explored for detecting fake news in Bahasa Indonesia.","In this research, we explore those transformer models and found that ALBERT outperformed other models with 87.6% accuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch) respectively.","Source code available at: https://github.com/Shafna81/fakenewsdetection.git"],"url":"http://arxiv.org/abs/2308.04950v1"}
{"created":"2023-08-09 13:32:10","title":"Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation","abstract":"End-to-end weakly supervised semantic segmentation aims at optimizing a segmentation model in a single-stage training process based on only image annotations. Existing methods adopt an online-trained classification branch to provide pseudo annotations for supervising the segmentation branch. However, this strategy makes the classification branch dominate the whole concurrent training process, hindering these two branches from assisting each other. In our work, we treat these two branches equally by viewing them as diverse ways to generate the segmentation map, and add interactions on both their supervision and operation to achieve mutual promotion. For this purpose, a bidirectional supervision mechanism is elaborated to force the consistency between the outputs of these two branches. Thus, the segmentation branch can also give feedback to the classification branch to enhance the quality of localization seeds. Moreover, our method also designs interaction operations between these two branches to exchange their knowledge to assist each other. Experiments indicate our work outperforms existing end-to-end weakly supervised segmentation methods.","sentences":["End-to-end weakly supervised semantic segmentation aims at optimizing a segmentation model in a single-stage training process based on only image annotations.","Existing methods adopt an online-trained classification branch to provide pseudo annotations for supervising the segmentation branch.","However, this strategy makes the classification branch dominate the whole concurrent training process, hindering these two branches from assisting each other.","In our work, we treat these two branches equally by viewing them as diverse ways to generate the segmentation map, and add interactions on both their supervision and operation to achieve mutual promotion.","For this purpose, a bidirectional supervision mechanism is elaborated to force the consistency between the outputs of these two branches.","Thus, the segmentation branch can also give feedback to the classification branch to enhance the quality of localization seeds.","Moreover, our method also designs interaction operations between these two branches to exchange their knowledge to assist each other.","Experiments indicate our work outperforms existing end-to-end weakly supervised segmentation methods."],"url":"http://arxiv.org/abs/2308.04949v1"}
{"created":"2023-08-09 13:32:06","title":"Extrapolating Large Language Models to Non-English by Aligning Languages","abstract":"Due to the unbalanced training data distribution, the language ability of large language models (LLMs) is often biased towards English. In this paper, we propose to empower pre-trained LLMs on non-English languages by building semantic alignment across languages. We perform instruction-tuning on LLaMA with both translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMA). Experiment results on cross-lingual benchmark XQUAD and MLQA show that x-LLaMA models outperform the English instruction-tuned counterpart (Alpaca) by 42.50% on average on six non-English languages. Further experiments on Chinese benchmark C-Eval show that x-LLaMA achieves significant improvement on Chinese humanities tasks, outperforming Alpaca by 8.2%. We also discover that incorporating non-English text on the target side of translation data is particularly effective for boosting non-English ability. Besides, we find that semantic alignment within LLM can be further strengthened as translation task data scales up and we present the formulation of the underlying scaling law. Evaluation results on translation dataset Flores-101 show that \\method outperforms previous LLaMA-based models in all evaluated directions. Code and data will be available at: https://github.com/OwenNJU/x-LLM.","sentences":["Due to the unbalanced training data distribution, the language ability of large language models (LLMs) is often biased towards English.","In this paper, we propose to empower pre-trained LLMs on non-English languages by building semantic alignment across languages.","We perform instruction-tuning on LLaMA with both translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMA).","Experiment results on cross-lingual benchmark XQUAD and MLQA show that x-LLaMA models outperform the English instruction-tuned counterpart (Alpaca) by 42.50% on average on six non-English languages.","Further experiments on Chinese benchmark C-Eval show that x-LLaMA achieves significant improvement on Chinese humanities tasks, outperforming Alpaca by 8.2%.","We also discover that incorporating non-English text on the target side of translation data is particularly effective for boosting non-English ability.","Besides, we find that semantic alignment within LLM can be further strengthened as translation task data scales up and we present the formulation of the underlying scaling law.","Evaluation results on translation dataset Flores-101 show that \\method outperforms previous LLaMA-based models in all evaluated directions.","Code and data will be available at: https://github.com/OwenNJU/x-LLM."],"url":"http://arxiv.org/abs/2308.04948v1"}
{"created":"2023-08-09 13:24:55","title":"SelectNAdapt: Support Set Selection for Few-Shot Domain Adaptation","abstract":"Generalisation of deep neural networks becomes vulnerable when distribution shifts are encountered between train (source) and test (target) domain data. Few-shot domain adaptation mitigates this issue by adapting deep neural networks pre-trained on the source domain to the target domain using a randomly selected and annotated support set from the target domain. This paper argues that randomly selecting the support set can be further improved for effectively adapting the pre-trained source models to the target domain. Alternatively, we propose SelectNAdapt, an algorithm to curate the selection of the target domain samples, which are then annotated and included in the support set. In particular, for the K-shot adaptation problem, we first leverage self-supervision to learn features of the target domain data. Then, we propose a per-class clustering scheme of the learned target domain features and select K representative target samples using a distance-based scoring function. Finally, we bring our selection setup towards a practical ground by relying on pseudo-labels for clustering semantically similar target domain samples. Our experiments show promising results on three few-shot domain adaptation benchmarks for image recognition compared to related approaches and the standard random selection.","sentences":["Generalisation of deep neural networks becomes vulnerable when distribution shifts are encountered between train (source) and test (target) domain data.","Few-shot domain adaptation mitigates this issue by adapting deep neural networks pre-trained on the source domain to the target domain using a randomly selected and annotated support set from the target domain.","This paper argues that randomly selecting the support set can be further improved for effectively adapting the pre-trained source models to the target domain.","Alternatively, we propose SelectNAdapt, an algorithm to curate the selection of the target domain samples, which are then annotated and included in the support set.","In particular, for the K-shot adaptation problem, we first leverage self-supervision to learn features of the target domain data.","Then, we propose a per-class clustering scheme of the learned target domain features and select K representative target samples using a distance-based scoring function.","Finally, we bring our selection setup towards a practical ground by relying on pseudo-labels for clustering semantically similar target domain samples.","Our experiments show promising results on three few-shot domain adaptation benchmarks for image recognition compared to related approaches and the standard random selection."],"url":"http://arxiv.org/abs/2308.04946v1"}
{"created":"2023-08-09 13:22:37","title":"LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking","abstract":"The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework. Initially developed to evaluate Arabic NLP tasks using OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task and model, regardless of language. The framework also features zero- and few-shot learning settings. A new custom dataset can be added in less than 10 minutes, and users can use their own model API keys to evaluate the task at hand. The developed framework has been already tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We plan to open-source the framework for the community (https://github.com/qcri/LLMeBench/). A video demonstrating the framework is available online (https://youtu.be/FkQn4UjYA0s).","sentences":["The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages.","Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users.","In this study, we introduce the LLMeBench framework.","Initially developed to evaluate Arabic NLP tasks using OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task and model, regardless of language.","The framework also features zero- and few-shot learning settings.","A new custom dataset can be added in less than 10 minutes, and users can use their own model API keys to evaluate the task at hand.","The developed framework has been already tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points.","We plan to open-source the framework for the community (https://github.com/qcri/LLMeBench/).","A video demonstrating the framework is available online (https://youtu.be/FkQn4UjYA0s)."],"url":"http://arxiv.org/abs/2308.04945v1"}
{"created":"2023-08-09 13:19:28","title":"Gaussian Image Anomaly Detection with Greedy Eigencomponent Selection","abstract":"Anomaly detection (AD) in images, identifying significant deviations from normality, is a critical issue in computer vision. This paper introduces a novel approach to dimensionality reduction for AD using pre-trained convolutional neural network (CNN) that incorporate EfficientNet models. We investigate the importance of component selection and propose two types of tree search approaches, both employing a greedy strategy, for optimal eigencomponent selection. Our study conducts three main experiments to evaluate the effectiveness of our approach. The first experiment explores the influence of test set performance on component choice, the second experiment examines the performance when we train on one anomaly type and evaluate on all other types, and the third experiment investigates the impact of using a minimum number of images for training and selecting them based on anomaly types. Our approach aims to find the optimal subset of components that deliver the highest performance score, instead of focusing solely on the proportion of variance explained by each component and also understand the components behaviour in different settings. Our results indicate that the proposed method surpasses both Principal Component Analysis (PCA) and Negated Principal Component Analysis (NPCA) in terms of detection accuracy, even when using fewer components. Thus, our approach provides a promising alternative to conventional dimensionality reduction techniques in AD, and holds potential to enhance the efficiency and effectiveness of AD systems.","sentences":["Anomaly detection (AD) in images, identifying significant deviations from normality, is a critical issue in computer vision.","This paper introduces a novel approach to dimensionality reduction for AD using pre-trained convolutional neural network (CNN) that incorporate EfficientNet models.","We investigate the importance of component selection and propose two types of tree search approaches, both employing a greedy strategy, for optimal eigencomponent selection.","Our study conducts three main experiments to evaluate the effectiveness of our approach.","The first experiment explores the influence of test set performance on component choice, the second experiment examines the performance when we train on one anomaly type and evaluate on all other types, and the third experiment investigates the impact of using a minimum number of images for training and selecting them based on anomaly types.","Our approach aims to find the optimal subset of components that deliver the highest performance score, instead of focusing solely on the proportion of variance explained by each component and also understand the components behaviour in different settings.","Our results indicate that the proposed method surpasses both Principal Component Analysis (PCA) and Negated Principal Component Analysis (NPCA) in terms of detection accuracy, even when using fewer components.","Thus, our approach provides a promising alternative to conventional dimensionality reduction techniques in AD, and holds potential to enhance the efficiency and effectiveness of AD systems."],"url":"http://arxiv.org/abs/2308.04944v1"}
{"created":"2023-08-09 13:18:41","title":"Differentially Private Graph Neural Network with Importance-Grained Noise Adaption","abstract":"Graph Neural Networks (GNNs) with differential privacy have been proposed to preserve graph privacy when nodes represent personal and sensitive information. However, the existing methods ignore that nodes with different importance may yield diverse privacy demands, which may lead to over-protect some nodes and decrease model utility. In this paper, we study the problem of importance-grained privacy, where nodes contain personal data that need to be kept private but are critical for training a GNN. We propose NAP-GNN, a node-importance-grained privacy-preserving GNN algorithm with privacy guarantees based on adaptive differential privacy to safeguard node information. First, we propose a Topology-based Node Importance Estimation (TNIE) method to infer unknown node importance with neighborhood and centrality awareness. Second, an adaptive private aggregation method is proposed to perturb neighborhood aggregation from node-importance-grain. Third, we propose to privately train a graph learning algorithm on perturbed aggregations in adaptive residual connection mode over multi-layers convolution for node-wise tasks. Theoretically analysis shows that NAP-GNN satisfies privacy guarantees. Empirical experiments over real-world graph datasets show that NAP-GNN achieves a better trade-off between privacy and accuracy.","sentences":["Graph Neural Networks (GNNs) with differential privacy have been proposed to preserve graph privacy when nodes represent personal and sensitive information.","However, the existing methods ignore that nodes with different importance may yield diverse privacy demands, which may lead to over-protect some nodes and decrease model utility.","In this paper, we study the problem of importance-grained privacy, where nodes contain personal data that need to be kept private but are critical for training a GNN.","We propose NAP-GNN, a node-importance-grained privacy-preserving GNN algorithm with privacy guarantees based on adaptive differential privacy to safeguard node information.","First, we propose a Topology-based Node Importance Estimation (TNIE) method to infer unknown node importance with neighborhood and centrality awareness.","Second, an adaptive private aggregation method is proposed to perturb neighborhood aggregation from node-importance-grain.","Third, we propose to privately train a graph learning algorithm on perturbed aggregations in adaptive residual connection mode over multi-layers convolution for node-wise tasks.","Theoretically analysis shows that NAP-GNN satisfies privacy guarantees.","Empirical experiments over real-world graph datasets show that NAP-GNN achieves a better trade-off between privacy and accuracy."],"url":"http://arxiv.org/abs/2308.04943v1"}
{"created":"2023-08-09 13:17:21","title":"Semantic Communications for Artificial Intelligence Generated Content (AIGC) Toward Effective Content Creation","abstract":"Artificial Intelligence Generated Content (AIGC) Services have significant potential in digital content creation. The distinctive abilities of AIGC, such as content generation based on minimal input, hold huge potential, especially when integrating with semantic communication (SemCom). In this paper, a novel comprehensive conceptual model for the integration of AIGC and SemCom is developed. Particularly, a content generation level is introduced on top of the semantic level that provides a clear outline of how AIGC and SemCom interact with each other to produce meaningful and effective content. Moreover, a novel framework that employs AIGC technology is proposed as an encoder and decoder for semantic information, considering the joint optimization of semantic extraction and evaluation metrics tailored to AIGC services. The framework can adapt to different types of content generated, the required quality, and the semantic information utilized. By employing a Deep Q Network (DQN), a case study is presented that provides useful insights into the feasibility of the optimization problem and its convergence characteristics.","sentences":["Artificial Intelligence Generated Content (AIGC)","Services have significant potential in digital content creation.","The distinctive abilities of AIGC, such as content generation based on minimal input, hold huge potential, especially when integrating with semantic communication (SemCom).","In this paper, a novel comprehensive conceptual model for the integration of AIGC and SemCom is developed.","Particularly, a content generation level is introduced on top of the semantic level that provides a clear outline of how AIGC and SemCom interact with each other to produce meaningful and effective content.","Moreover, a novel framework that employs AIGC technology is proposed as an encoder and decoder for semantic information, considering the joint optimization of semantic extraction and evaluation metrics tailored to AIGC services.","The framework can adapt to different types of content generated, the required quality, and the semantic information utilized.","By employing a Deep Q Network (DQN), a case study is presented that provides useful insights into the feasibility of the optimization problem and its convergence characteristics."],"url":"http://arxiv.org/abs/2308.04942v1"}
{"created":"2023-08-09 13:13:19","title":"An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning","abstract":"Communication is crucial in multi-agent reinforcement learning when agents are not able to observe the full state of the environment. The most common approach to allow learned communication between agents is the use of a differentiable communication channel that allows gradients to flow between agents as a form of feedback. However, this is challenging when we want to use discrete messages to reduce the message size, since gradients cannot flow through a discrete communication channel. Previous work proposed methods to deal with this problem. However, these methods are tested in different communication learning architectures and environments, making it hard to compare them. In this paper, we compare several state-of-the-art discretization methods as well as a novel approach. We do this comparison in the context of communication learning using gradients from other agents and perform tests on several environments. In addition, we present COMA-DIAL, a communication learning approach based on DIAL and COMA extended with learning rate scaling and adapted exploration. Using COMA-DIAL allows us to perform experiments on more complex environments. Our results show that the novel ST-DRU method, proposed in this paper, achieves the best results out of all discretization methods across the different environments. It achieves the best or close to the best performance in each of the experiments and is the only method that does not fail on any of the tested environments.","sentences":["Communication is crucial in multi-agent reinforcement learning when agents are not able to observe the full state of the environment.","The most common approach to allow learned communication between agents is the use of a differentiable communication channel that allows gradients to flow between agents as a form of feedback.","However, this is challenging when we want to use discrete messages to reduce the message size, since gradients cannot flow through a discrete communication channel.","Previous work proposed methods to deal with this problem.","However, these methods are tested in different communication learning architectures and environments, making it hard to compare them.","In this paper, we compare several state-of-the-art discretization methods as well as a novel approach.","We do this comparison in the context of communication learning using gradients from other agents and perform tests on several environments.","In addition, we present COMA-DIAL, a communication learning approach based on DIAL and COMA extended with learning rate scaling and adapted exploration.","Using COMA-DIAL allows us to perform experiments on more complex environments.","Our results show that the novel ST-DRU method, proposed in this paper, achieves the best results out of all discretization methods across the different environments.","It achieves the best or close to the best performance in each of the experiments and is the only method that does not fail on any of the tested environments."],"url":"http://arxiv.org/abs/2308.04938v1"}
{"created":"2023-08-09 13:09:07","title":"JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition","abstract":"We propose JEDI, a multi-dataset semi-supervised learning method, which efficiently combines knowledge from multiple experts, learned on different datasets, to train and improve the performance of individual, per dataset, student models. Our approach achieves this by addressing two important problems in current machine learning research: generalization across datasets and limitations of supervised training due to scarcity of labeled data. We start with an arbitrary number of experts, pretrained on their own specific dataset, which form the initial set of student models. The teachers are immediately derived by concatenating the feature representations from the penultimate layers of the students. We then train all models in a student-teacher semi-supervised learning scenario until convergence. In our efficient approach, student-teacher training is carried out jointly and end-to-end, showing that both students and teachers improve their generalization capacity during training. We validate our approach on four video action recognition datasets. By simultaneously considering all datasets within a unified semi-supervised setting, we demonstrate significant improvements over the initial experts.","sentences":["We propose JEDI, a multi-dataset semi-supervised learning method, which efficiently combines knowledge from multiple experts, learned on different datasets, to train and improve the performance of individual, per dataset, student models.","Our approach achieves this by addressing two important problems in current machine learning research: generalization across datasets and limitations of supervised training due to scarcity of labeled data.","We start with an arbitrary number of experts, pretrained on their own specific dataset, which form the initial set of student models.","The teachers are immediately derived by concatenating the feature representations from the penultimate layers of the students.","We then train all models in a student-teacher semi-supervised learning scenario until convergence.","In our efficient approach, student-teacher training is carried out jointly and end-to-end, showing that both students and teachers improve their generalization capacity during training.","We validate our approach on four video action recognition datasets.","By simultaneously considering all datasets within a unified semi-supervised setting, we demonstrate significant improvements over the initial experts."],"url":"http://arxiv.org/abs/2308.04934v1"}
{"created":"2023-08-09 13:06:13","title":"You Are How You Walk: Quantifying Privacy Risks in Step Count Data","abstract":"Wearable devices have gained huge popularity in today's world. These devices collect large-scale health data from their users, such as heart rate and step count data, that is privacy sensitive, however it has not yet received the necessary attention in the academia. In this paper, we perform the first systematic study on quantifying privacy risks stemming from step count data. In particular, we propose two attacks including attribute inference for gender, age and education and temporal linkability. We demonstrate the severity of the privacy attacks by performing extensive evaluation on a real life dataset and derive key insights. We believe our results can serve as a step stone for deriving a privacy-preserving ecosystem for wearable devices in the future.","sentences":["Wearable devices have gained huge popularity in today's world.","These devices collect large-scale health data from their users, such as heart rate and step count data, that is privacy sensitive, however it has not yet received the necessary attention in the academia.","In this paper, we perform the first systematic study on quantifying privacy risks stemming from step count data.","In particular, we propose two attacks including attribute inference for gender, age and education and temporal linkability.","We demonstrate the severity of the privacy attacks by performing extensive evaluation on a real life dataset and derive key insights.","We believe our results can serve as a step stone for deriving a privacy-preserving ecosystem for wearable devices in the future."],"url":"http://arxiv.org/abs/2308.04933v1"}
{"created":"2023-08-09 12:55:51","title":"A/B Testing: A Systematic Literature Review","abstract":"In A/B testing two variants of a piece of software are compared in the field from an end user's point of view, enabling data-driven decision making. While widely used in practice, no comprehensive study has been conducted on the state-of-the-art in A/B testing. This paper reports the results of a systematic literature review that analyzed 141 primary studies. The results shows that the main targets of A/B testing are algorithms and visual elements. Single classic A/B tests are the dominating type of tests. Stakeholders have three main roles in the design of A/B tests: concept designer, experiment architect, and setup technician. The primary types of data collected during the execution of A/B tests are product/system data and user-centric data. The dominating use of the test results are feature selection, feature rollout, and continued feature development. Stakeholders have two main roles during A/B test execution: experiment coordinator and experiment assessor. The main reported open problems are enhancement of proposed approaches and their usability. Interesting lines for future research include: strengthen the adoption of statistical methods in A/B testing, improving the process of A/B testing, and enhancing the automation of A/B testing.","sentences":["In A/B testing two variants of a piece of software are compared in the field from an end user's point of view, enabling data-driven decision making.","While widely used in practice, no comprehensive study has been conducted on the state-of-the-art in A/B testing.","This paper reports the results of a systematic literature review that analyzed 141 primary studies.","The results shows that the main targets of A/B testing are algorithms and visual elements.","Single classic A/B tests are the dominating type of tests.","Stakeholders have three main roles in the design of A/B tests: concept designer, experiment architect, and setup technician.","The primary types of data collected during the execution of A/B tests are product/system data and user-centric data.","The dominating use of the test results are feature selection, feature rollout, and continued feature development.","Stakeholders have two main roles during A/B test execution: experiment coordinator and experiment assessor.","The main reported open problems are enhancement of proposed approaches and their usability.","Interesting lines for future research include: strengthen the adoption of statistical methods in A/B testing, improving the process of A/B testing, and enhancing the automation of A/B testing."],"url":"http://arxiv.org/abs/2308.04929v1"}
{"created":"2023-08-09 12:54:27","title":"GeodesicPSIM: Predicting the Quality of Static Mesh with Texture Map via Geodesic Patch Similarity","abstract":"Static meshes with texture maps have attracted considerable attention in both industrial manufacturing and academic research, leading to an urgent requirement for effective and robust objective quality evaluation. However, current model-based static mesh quality metrics have obvious limitations: most of them only consider geometry information, while color information is ignored, and they have strict constraints for the meshes' geometrical topology. Other metrics, such as image-based and point-based metrics, are easily influenced by the prepossessing algorithms, e.g., projection and sampling, hampering their ability to perform at their best. In this paper, we propose Geodesic Patch Similarity (GeodesicPSIM), a novel model-based metric to accurately predict human perception quality for static meshes. After selecting a group keypoints, 1-hop geodesic patches are constructed based on both the reference and distorted meshes cleaned by an effective mesh cleaning algorithm. A two-step patch cropping algorithm and a patch texture mapping module refine the size of 1-hop geodesic patches and build the relationship between the mesh geometry and color information, resulting in the generation of 1-hop textured geodesic patches. Three types of features are extracted to quantify the distortion: patch color smoothness, patch discrete mean curvature, and patch pixel color average and variance. To the best of our knowledge, GeodesicPSIM is the first model-based metric especially designed for static meshes with texture maps. GeodesicPSIM provides state-of-the-art performance in comparison with image-based, point-based, and video-based metrics on a newly created and challenging database. We also prove the robustness of GeodesicPSIM by introducing different settings of hyperparameters. Ablation studies also exhibit the effectiveness of three proposed features and the patch cropping algorithm.","sentences":["Static meshes with texture maps have attracted considerable attention in both industrial manufacturing and academic research, leading to an urgent requirement for effective and robust objective quality evaluation.","However, current model-based static mesh quality metrics have obvious limitations: most of them only consider geometry information, while color information is ignored, and they have strict constraints for the meshes' geometrical topology.","Other metrics, such as image-based and point-based metrics, are easily influenced by the prepossessing algorithms, e.g., projection and sampling, hampering their ability to perform at their best.","In this paper, we propose Geodesic Patch Similarity (GeodesicPSIM), a novel model-based metric to accurately predict human perception quality for static meshes.","After selecting a group keypoints, 1-hop geodesic patches are constructed based on both the reference and distorted meshes cleaned by an effective mesh cleaning algorithm.","A two-step patch cropping algorithm and a patch texture mapping module refine the size of 1-hop geodesic patches and build the relationship between the mesh geometry and color information, resulting in the generation of 1-hop textured geodesic patches.","Three types of features are extracted to quantify the distortion: patch color smoothness, patch discrete mean curvature, and patch pixel color average and variance.","To the best of our knowledge, GeodesicPSIM is the first model-based metric especially designed for static meshes with texture maps.","GeodesicPSIM provides state-of-the-art performance in comparison with image-based, point-based, and video-based metrics on a newly created and challenging database.","We also prove the robustness of GeodesicPSIM by introducing different settings of hyperparameters.","Ablation studies also exhibit the effectiveness of three proposed features and the patch cropping algorithm."],"url":"http://arxiv.org/abs/2308.04928v1"}
{"created":"2023-08-09 12:27:49","title":"Service Reservation and Pricing for Green Metaverses: A Stackelberg Game Approach","abstract":"Metaverse enables users to communicate, collaborate and socialize with each other through their digital avatars. Due to the spatio-temporal characteristics, co-located users are served well by performing their software components in a collaborative manner such that a Metaverse service provider (MSP) eliminates redundant data transmission and processing, ultimately reducing the total energy consumption. The energyefficient service provision is crucial for enabling the green and sustainable Metaverse. In this article, we take an augmented reality (AR) application as an example to achieve this goal. Moreover, we study an economic issue on how the users reserve offloading services from the MSP and how the MSP determines an optimal charging price since each user is rational to decide whether to accept the offloading service by taking into account the monetary cost. A single-leader multi-follower Stackelberg game is formulated between the MSP and users while each user optimizes an offloading probability to minimize the weighted sum of time, energy consumption and monetary cost. Numerical results show that our scheme achieves energy savings and satisfies individual rationality simultaneously compared with the conventional schemes. Finally, we identify and discuss open directions on how several emerging technologies are combined with the sustainable green Metaverse.","sentences":["Metaverse enables users to communicate, collaborate and socialize with each other through their digital avatars.","Due to the spatio-temporal characteristics, co-located users are served well by performing their software components in a collaborative manner such that a Metaverse service provider (MSP) eliminates redundant data transmission and processing, ultimately reducing the total energy consumption.","The energyefficient service provision is crucial for enabling the green and sustainable Metaverse.","In this article, we take an augmented reality (AR) application as an example to achieve this goal.","Moreover, we study an economic issue on how the users reserve offloading services from the MSP and how the MSP determines an optimal charging price since each user is rational to decide whether to accept the offloading service by taking into account the monetary cost.","A single-leader multi-follower Stackelberg game is formulated between the MSP and users while each user optimizes an offloading probability to minimize the weighted sum of time, energy consumption and monetary cost.","Numerical results show that our scheme achieves energy savings and satisfies individual rationality simultaneously compared with the conventional schemes.","Finally, we identify and discuss open directions on how several emerging technologies are combined with the sustainable green Metaverse."],"url":"http://arxiv.org/abs/2308.04914v1"}
{"created":"2023-08-09 12:26:37","title":"LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following","abstract":"E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales. The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario. However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission. This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks. Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&A. These tasks enable the models to comprehensively understand precise e-commerce authoring knowledge by interleaving features covering typical service aspects of customers, sellers, and platforms. The GPT-3.5 is introduced as a teacher model, which expands the seed instructions to form a training set for the LLaMA-E models with various scales. The experimental results show that the proposed LLaMA-E models achieve state-of-the-art results in quantitative and qualitative evaluations, also exhibiting the advantage in zero-shot scenes. To the best of our knowledge, this study is the first to serve the LLMs to specific e-commerce authoring scenarios.","sentences":["E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales.","The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario.","However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers.","Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission.","This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks.","Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&A.","These tasks enable the models to comprehensively understand precise e-commerce authoring knowledge by interleaving features covering typical service aspects of customers, sellers, and platforms.","The GPT-3.5 is introduced as a teacher model, which expands the seed instructions to form a training set for the LLaMA-E models with various scales.","The experimental results show that the proposed LLaMA-E models achieve state-of-the-art results in quantitative and qualitative evaluations, also exhibiting the advantage in zero-shot scenes.","To the best of our knowledge, this study is the first to serve the LLMs to specific e-commerce authoring scenarios."],"url":"http://arxiv.org/abs/2308.04913v1"}
{"created":"2023-08-09 12:23:41","title":"Cross-view Semantic Alignment for Livestreaming Product Recognition","abstract":"Live commerce is the act of selling products online through live streaming. The customer's diverse demands for online products introduce more challenges to Livestreaming Product Recognition. Previous works have primarily focused on fashion clothing data or utilize single-modal input, which does not reflect the real-world scenario where multimodal data from various categories are present. In this paper, we present LPR4M, a large-scale multimodal dataset that covers 34 categories, comprises 3 modalities (image, video, and text), and is 50? larger than the largest publicly available dataset. LPR4M contains diverse videos and noise modality pairs while exhibiting a long-tailed distribution, resembling real-world problems. Moreover, a cRoss-vIew semantiC alignmEnt (RICE) model is proposed to learn discriminative instance features from the image and video views of the products. This is achieved through instance-level contrastive learning and cross-view patch-level feature propagation. A novel Patch Feature Reconstruction loss is proposed to penalize the semantic misalignment between cross-view patches. Extensive experiments demonstrate the effectiveness of RICE and provide insights into the importance of dataset diversity and expressivity. The dataset and code are available at https://github.com/adxcreative/RICE","sentences":["Live commerce is the act of selling products online through live streaming.","The customer's diverse demands for online products introduce more challenges to Livestreaming Product Recognition.","Previous works have primarily focused on fashion clothing data or utilize single-modal input, which does not reflect the real-world scenario where multimodal data from various categories are present.","In this paper, we present LPR4M, a large-scale multimodal dataset that covers 34 categories, comprises 3 modalities (image, video, and text), and is 50?","larger than the largest publicly available dataset.","LPR4M contains diverse videos and noise modality pairs while exhibiting a long-tailed distribution, resembling real-world problems.","Moreover, a cRoss-vIew semantiC alignmEnt (RICE) model is proposed to learn discriminative instance features from the image and video views of the products.","This is achieved through instance-level contrastive learning and cross-view patch-level feature propagation.","A novel Patch Feature Reconstruction loss is proposed to penalize the semantic misalignment between cross-view patches.","Extensive experiments demonstrate the effectiveness of RICE and provide insights into the importance of dataset diversity and expressivity.","The dataset and code are available at https://github.com/adxcreative/RICE"],"url":"http://arxiv.org/abs/2308.04912v1"}
{"created":"2023-08-09 12:22:49","title":"SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation","abstract":"Medical image analysis using deep learning is often challenged by limited labeled data and high annotation costs. Fine-tuning the entire network in label-limited scenarios can lead to overfitting and suboptimal performance. Recently, prompt tuning has emerged as a more promising technique that introduces a few additional tunable parameters as prompts to a task-agnostic pre-trained model, and updates only these parameters using supervision from limited labeled data while keeping the pre-trained model unchanged. However, previous work has overlooked the importance of selective labeling in downstream tasks, which aims to select the most valuable downstream samples for annotation to achieve the best performance with minimum annotation cost. To address this, we propose a framework that combines selective labeling with prompt tuning (SLPT) to boost performance in limited labels. Specifically, we introduce a feature-aware prompt updater to guide prompt tuning and a TandEm Selective LAbeling (TESLA) strategy. TESLA includes unsupervised diversity selection and supervised selection using prompt-based uncertainty. In addition, we propose a diversified visual prompt tuning strategy to provide multi-prompt-based discrepant predictions for TESLA. We evaluate our method on liver tumor segmentation and achieve state-of-the-art performance, outperforming traditional fine-tuning with only 6% of tunable parameters, also achieving 94% of full-data performance by labeling only 5% of the data.","sentences":["Medical image analysis using deep learning is often challenged by limited labeled data and high annotation costs.","Fine-tuning the entire network in label-limited scenarios can lead to overfitting and suboptimal performance.","Recently, prompt tuning has emerged as a more promising technique that introduces a few additional tunable parameters as prompts to a task-agnostic pre-trained model, and updates only these parameters using supervision from limited labeled data while keeping the pre-trained model unchanged.","However, previous work has overlooked the importance of selective labeling in downstream tasks, which aims to select the most valuable downstream samples for annotation to achieve the best performance with minimum annotation cost.","To address this, we propose a framework that combines selective labeling with prompt tuning (SLPT) to boost performance in limited labels.","Specifically, we introduce a feature-aware prompt updater to guide prompt tuning and a TandEm Selective LAbeling (TESLA) strategy.","TESLA includes unsupervised diversity selection and supervised selection using prompt-based uncertainty.","In addition, we propose a diversified visual prompt tuning strategy to provide multi-prompt-based discrepant predictions for TESLA.","We evaluate our method on liver tumor segmentation and achieve state-of-the-art performance, outperforming traditional fine-tuning with only 6% of tunable parameters, also achieving 94% of full-data performance by labeling only 5% of the data."],"url":"http://arxiv.org/abs/2308.04911v1"}
{"created":"2023-08-09 12:18:01","title":"Ehrenfeucht-Fra\u00efss\u00e9 Games in Semiring Semantics","abstract":"Ehrenfeucht-Fra\\\"iss\\'e games provide a fundamental method for proving elementary equivalence (and equivalence up to a certain quantifier rank) of relational structures. We investigate the soundness and completeness of this method in the more general context of semiring semantics. Motivated originally by provenance analysis of database queries, semiring semantics evaluates logical statements not just by true or false, but by values in some commutative semiring; this can provide much more detailed information, for instance concerning the combinations of atomic facts that imply the truth of a statement, or practical information about evaluation costs, confidence scores, access levels or the number of successful evaluation strategies. There is a wide variety of different semirings that are relevant for provenance analysis, and the applicability of classical logical methods in semiring semantics may strongly depend on the algebraic properties of the underlying semiring.   While Ehrenfeucht-Fra\\\"iss\\'e games are sound and complete for logical equivalences in classical semantics, and thus on the Boolean semiring, this is in general not the case for other semirings. We provide a detailed analysis of the soundness and completeness of model comparison games on specific semirings, not just for classical Ehrenfeucht-Fra\\\"iss\\'e games but also for other variants based on bijections or counting.   Finally we propose a new kind of games, called homomorphism games, which are based on the fact that there exist certain rather simple semiring interpretations that are locally very different and can be separated even in a one-move game, but which can be proved to be elementarily equivalent via separating sets of homomorphisms into the Boolean semiring. We prove that these homomorphism games provide a sound and complete method for logical equivalences on finite and infinite lattice semirings.","sentences":["Ehrenfeucht-Fra\\\"iss\\'e games provide a fundamental method for proving elementary equivalence (and equivalence up to a certain quantifier rank) of relational structures.","We investigate the soundness and completeness of this method in the more general context of semiring semantics.","Motivated originally by provenance analysis of database queries, semiring semantics evaluates logical statements not just by true or false, but by values in some commutative semiring; this can provide much more detailed information, for instance concerning the combinations of atomic facts that imply the truth of a statement, or practical information about evaluation costs, confidence scores, access levels or the number of successful evaluation strategies.","There is a wide variety of different semirings that are relevant for provenance analysis, and the applicability of classical logical methods in semiring semantics may strongly depend on the algebraic properties of the underlying semiring.   ","While Ehrenfeucht-Fra\\\"iss\\'e games are sound and complete for logical equivalences in classical semantics, and thus on the Boolean semiring, this is in general not the case for other semirings.","We provide a detailed analysis of the soundness and completeness of model comparison games on specific semirings, not just for classical Ehrenfeucht-Fra\\\"iss\\'e games but also for other variants based on bijections or counting.   ","Finally we propose a new kind of games, called homomorphism games, which are based on the fact that there exist certain rather simple semiring interpretations that are locally very different and can be separated even in a one-move game, but which can be proved to be elementarily equivalent via separating sets of homomorphisms into the Boolean semiring.","We prove that these homomorphism games provide a sound and complete method for logical equivalences on finite and infinite lattice semirings."],"url":"http://arxiv.org/abs/2308.04910v1"}
{"created":"2023-08-09 12:16:10","title":"Adversarial Deep Reinforcement Learning for Cyber Security in Software Defined Networks","abstract":"This paper focuses on the impact of leveraging autonomous offensive approaches in Deep Reinforcement Learning (DRL) to train more robust agents by exploring the impact of applying adversarial learning to DRL for autonomous security in Software Defined Networks (SDN). Two algorithms, Double Deep Q-Networks (DDQN) and Neural Episodic Control to Deep Q-Network (NEC2DQN or N2D), are compared. NEC2DQN was proposed in 2018 and is a new member of the deep q-network (DQN) family of algorithms. The attacker has full observability of the environment and access to a causative attack that uses state manipulation in an attempt to poison the learning process. The implementation of the attack is done under a white-box setting, in which the attacker has access to the defender's model and experiences. Two games are played; in the first game, DDQN is a defender and N2D is an attacker, and in second game, the roles are reversed. The games are played twice; first, without an active causative attack and secondly, with an active causative attack. For execution, three sets of game results are recorded in which a single set consists of 10 game runs. The before and after results are then compared in order to see if there was actually an improvement or degradation. The results show that with minute parameter changes made to the algorithms, there was growth in the attacker's role, since it is able to win games. Implementation of the adversarial learning by the introduction of the causative attack showed the algorithms are still able to defend the network according to their strengths.","sentences":["This paper focuses on the impact of leveraging autonomous offensive approaches in Deep Reinforcement Learning (DRL) to train more robust agents by exploring the impact of applying adversarial learning to DRL for autonomous security in Software Defined Networks (SDN).","Two algorithms, Double Deep Q-Networks (DDQN) and Neural Episodic Control to Deep Q-Network (NEC2DQN or N2D), are compared.","NEC2DQN was proposed in 2018 and is a new member of the deep q-network (DQN) family of algorithms.","The attacker has full observability of the environment and access to a causative attack that uses state manipulation in an attempt to poison the learning process.","The implementation of the attack is done under a white-box setting, in which the attacker has access to the defender's model and experiences.","Two games are played; in the first game, DDQN is a defender and N2D is an attacker, and in second game, the roles are reversed.","The games are played twice; first, without an active causative attack and secondly, with an active causative attack.","For execution, three sets of game results are recorded in which a single set consists of 10 game runs.","The before and after results are then compared in order to see if there was actually an improvement or degradation.","The results show that with minute parameter changes made to the algorithms, there was growth in the attacker's role, since it is able to win games.","Implementation of the adversarial learning by the introduction of the causative attack showed the algorithms are still able to defend the network according to their strengths."],"url":"http://arxiv.org/abs/2308.04909v1"}
{"created":"2023-08-09 12:04:41","title":"GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters","abstract":"Congestion Control (CC) plays a fundamental role in optimizing traffic in Data Center Networks (DCN). Currently, DCNs mainly implement two main CC protocols: DCTCP and DCQCN. Both protocols -- and their main variants -- are based on Explicit Congestion Notification (ECN), where intermediate switches mark packets when they detect congestion. The ECN configuration is thus a crucial aspect on the performance of CC protocols. Nowadays, network experts set static ECN parameters carefully selected to optimize the average network performance. However, today's high-speed DCNs experience quick and abrupt changes that severely change the network state (e.g., dynamic traffic workloads, incast events, failures). This leads to under-utilization and sub-optimal performance. This paper presents GraphCC, a novel Machine Learning-based framework for in-network CC optimization. Our distributed solution relies on a novel combination of Multi-agent Reinforcement Learning (MARL) and Graph Neural Networks (GNN), and it is compatible with widely deployed ECN-based CC protocols. GraphCC deploys distributed agents on switches that communicate with their neighbors to cooperate and optimize the global ECN configuration. In our evaluation, we test the performance of GraphCC under a wide variety of scenarios, focusing on the capability of this solution to adapt to new scenarios unseen during training (e.g., new traffic workloads, failures, upgrades). We compare GraphCC with a state-of-the-art MARL-based solution for ECN tuning -- ACC -- and observe that our proposed solution outperforms the state-of-the-art baseline in all of the evaluation scenarios, showing improvements up to $20\\%$ in Flow Completion Time as well as significant reductions in buffer occupancy ($38.0-85.7\\%$).","sentences":["Congestion Control (CC) plays a fundamental role in optimizing traffic in Data Center Networks (DCN).","Currently, DCNs mainly implement two main CC protocols: DCTCP and DCQCN.","Both protocols -- and their main variants -- are based on Explicit Congestion Notification (ECN), where intermediate switches mark packets when they detect congestion.","The ECN configuration is thus a crucial aspect on the performance of CC protocols.","Nowadays, network experts set static ECN parameters carefully selected to optimize the average network performance.","However, today's high-speed DCNs experience quick and abrupt changes that severely change the network state (e.g., dynamic traffic workloads, incast events, failures).","This leads to under-utilization and sub-optimal performance.","This paper presents GraphCC, a novel Machine Learning-based framework for in-network CC optimization.","Our distributed solution relies on a novel combination of Multi-agent Reinforcement Learning (MARL) and Graph Neural Networks (GNN), and it is compatible with widely deployed ECN-based CC protocols.","GraphCC deploys distributed agents on switches that communicate with their neighbors to cooperate and optimize the global ECN configuration.","In our evaluation, we test the performance of GraphCC under a wide variety of scenarios, focusing on the capability of this solution to adapt to new scenarios unseen during training (e.g., new traffic workloads, failures, upgrades).","We compare GraphCC with a state-of-the-art MARL-based solution for ECN tuning -- ACC -- and observe that our proposed solution outperforms the state-of-the-art baseline in all of the evaluation scenarios, showing improvements up to $20\\%$ in Flow Completion Time as well as significant reductions in buffer occupancy ($38.0-85.7\\%$)."],"url":"http://arxiv.org/abs/2308.04905v1"}
{"created":"2023-08-09 12:04:36","title":"StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability","abstract":"Video shakiness is an unpleasant distortion of User Generated Content (UGC) videos, which is usually caused by the unstable hold of cameras. In recent years, many video stabilization algorithms have been proposed, yet no specific and accurate metric enables comprehensively evaluating the stability of videos. Indeed, most existing quality assessment models evaluate video quality as a whole without specifically taking the subjective experience of video stability into consideration. Therefore, these models cannot measure the video stability explicitly and precisely when severe shakes are present. In addition, there is no large-scale video database in public that includes various degrees of shaky videos with the corresponding subjective scores available, which hinders the development of Video Quality Assessment for Stability (VQA-S). To this end, we build a new database named StableDB that contains 1,952 diversely-shaky UGC videos, where each video has a Mean Opinion Score (MOS) on the degree of video stability rated by 34 subjects. Moreover, we elaborately design a novel VQA-S model named StableVQA, which consists of three feature extractors to acquire the optical flow, semantic, and blur features respectively, and a regression layer to predict the final stability score. Extensive experiments demonstrate that the StableVQA achieves a higher correlation with subjective opinions than the existing VQA-S models and generic VQA models. The database and codes are available at https://github.com/QMME/StableVQA.","sentences":["Video shakiness is an unpleasant distortion of User Generated Content (UGC) videos, which is usually caused by the unstable hold of cameras.","In recent years, many video stabilization algorithms have been proposed, yet no specific and accurate metric enables comprehensively evaluating the stability of videos.","Indeed, most existing quality assessment models evaluate video quality as a whole without specifically taking the subjective experience of video stability into consideration.","Therefore, these models cannot measure the video stability explicitly and precisely when severe shakes are present.","In addition, there is no large-scale video database in public that includes various degrees of shaky videos with the corresponding subjective scores available, which hinders the development of Video Quality Assessment for Stability (VQA-S).","To this end, we build a new database named StableDB that contains 1,952 diversely-shaky UGC videos, where each video has a Mean Opinion Score (MOS) on the degree of video stability rated by 34 subjects.","Moreover, we elaborately design a novel VQA-S model named StableVQA, which consists of three feature extractors to acquire the optical flow, semantic, and blur features respectively, and a regression layer to predict the final stability score.","Extensive experiments demonstrate that the StableVQA achieves a higher correlation with subjective opinions than the existing VQA-S models and generic VQA models.","The database and codes are available at https://github.com/QMME/StableVQA."],"url":"http://arxiv.org/abs/2308.04904v1"}
{"created":"2023-08-09 12:03:12","title":"Towards true discovery of the differential equations","abstract":"Differential equation discovery, a machine learning subfield, is used to develop interpretable models, particularly in nature-related applications. By expertly incorporating the general parametric form of the equation of motion and appropriate differential terms, algorithms can autonomously uncover equations from data. This paper explores the prerequisites and tools for independent equation discovery without expert input, eliminating the need for equation form assumptions. We focus on addressing the challenge of assessing the adequacy of discovered equations when the correct equation is unknown, with the aim of providing insights for reliable equation discovery without prior knowledge of the equation form.","sentences":["Differential equation discovery, a machine learning subfield, is used to develop interpretable models, particularly in nature-related applications.","By expertly incorporating the general parametric form of the equation of motion and appropriate differential terms, algorithms can autonomously uncover equations from data.","This paper explores the prerequisites and tools for independent equation discovery without expert input, eliminating the need for equation form assumptions.","We focus on addressing the challenge of assessing the adequacy of discovered equations when the correct equation is unknown, with the aim of providing insights for reliable equation discovery without prior knowledge of the equation form."],"url":"http://arxiv.org/abs/2308.04901v1"}
