{"created":"2023-07-27 17:59:59","title":"To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation","abstract":"The goal of Online Domain Adaptation for semantic segmentation is to handle unforeseeable domain changes that occur during deployment, like sudden weather events. However, the high computational costs associated with brute-force adaptation make this paradigm unfeasible for real-world applications. In this paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training framework for real-time domain adaptation. Our approach includes a hardware-aware back-propagation orchestration agent (HAMT) and a dedicated domain-shift detector that enables active control over when and how the model is adapted (LT). Thanks to these advancements, our approach is capable of performing semantic segmentation while simultaneously adapting at more than 29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and speed trade-off is demonstrated on OnDA and SHIFT benchmarks through experimental results.","sentences":["The goal of Online Domain Adaptation for semantic segmentation is to handle unforeseeable domain changes that occur during deployment, like sudden weather events.","However, the high computational costs associated with brute-force adaptation make this paradigm unfeasible for real-world applications.","In this paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training framework for real-time domain adaptation.","Our approach includes a hardware-aware back-propagation orchestration agent (HAMT) and a dedicated domain-shift detector that enables active control over when and how the model is adapted (LT).","Thanks to these advancements, our approach is capable of performing semantic segmentation while simultaneously adapting at more than 29FPS on a single consumer-grade GPU.","Our framework's encouraging accuracy and speed trade-off is demonstrated on OnDA and SHIFT benchmarks through experimental results."],"url":"http://arxiv.org/abs/2307.15063v1"}
{"created":"2023-07-27 17:59:59","title":"Self-Supervised Visual Acoustic Matching","abstract":"Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments.","sentences":["Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment.","Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples.","We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference.","Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio.","Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments."],"url":"http://arxiv.org/abs/2307.15064v1"}
{"created":"2023-07-27 17:59:56","title":"The RoboDepth Challenge: Methods and Advancements Towards Robust Depth Estimation","abstract":"Accurate depth estimation under out-of-distribution (OoD) scenarios, such as adverse weather conditions, sensor failure, and noise contamination, is desirable for safety-critical applications. Existing depth estimation systems, however, suffer inevitably from real-world corruptions and perturbations and are struggled to provide reliable depth predictions under such cases. In this paper, we summarize the winning solutions from the RoboDepth Challenge -- an academic competition designed to facilitate and advance robust OoD depth estimation. This challenge was developed based on the newly established KITTI-C and NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimation, respectively. Out of more than two hundred participants, nine unique and top-performing solutions have appeared, with novel designs ranging from the following aspects: spatial- and frequency-domain augmentations, masked image modeling, image restoration and super-resolution, adversarial training, diffusion-based noise suppression, vision-language pre-training, learned model ensembling, and hierarchical feature enhancement. Extensive experimental analyses along with insightful observations are drawn to better understand the rationale behind each design. We hope this challenge could lay a solid foundation for future research on robust and reliable depth estimation and beyond. The datasets, competition toolkit, workshop recordings, and source code from the winning teams are publicly available on the challenge website.","sentences":["Accurate depth estimation under out-of-distribution (OoD) scenarios, such as adverse weather conditions, sensor failure, and noise contamination, is desirable for safety-critical applications.","Existing depth estimation systems, however, suffer inevitably from real-world corruptions and perturbations and are struggled to provide reliable depth predictions under such cases.","In this paper, we summarize the winning solutions from the RoboDepth Challenge -- an academic competition designed to facilitate and advance robust OoD depth estimation.","This challenge was developed based on the newly established KITTI-C and NYUDepth2-C benchmarks.","We hosted two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimation, respectively.","Out of more than two hundred participants, nine unique and top-performing solutions have appeared, with novel designs ranging from the following aspects: spatial- and frequency-domain augmentations, masked image modeling, image restoration and super-resolution, adversarial training, diffusion-based noise suppression, vision-language pre-training, learned model ensembling, and hierarchical feature enhancement.","Extensive experimental analyses along with insightful observations are drawn to better understand the rationale behind each design.","We hope this challenge could lay a solid foundation for future research on robust and reliable depth estimation and beyond.","The datasets, competition toolkit, workshop recordings, and source code from the winning teams are publicly available on the challenge website."],"url":"http://arxiv.org/abs/2307.15061v1"}
{"created":"2023-07-27 17:59:52","title":"MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous Driving","abstract":"Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is widely recognized that realistic sensor simulation will play a critical role in solving remaining corner cases by simulating them. To this end, we propose an autonomous driving simulator based upon neural radiance fields (NeRFs). Compared with existing works, ours has three notable features: (1) Instance-aware. Our simulator models the foreground instances and background environments separately with independent networks so that the static (e.g., size and appearance) and dynamic (e.g., trajectory) properties of instances can be controlled separately. (2) Modular. Our simulator allows flexible switching between different modern NeRF-related backbones, sampling strategies, input modalities, etc. We expect this modular design to boost academic progress and industrial deployment of NeRF-based autonomous driving simulation. (3) Realistic. Our simulator set new state-of-the-art photo-realism results given the best module selection. Our simulator will be open-sourced while most of our counterparts are not. Project page: https://open-air-sun.github.io/mars/.","sentences":["Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is widely recognized that realistic sensor simulation will play a critical role in solving remaining corner cases by simulating them.","To this end, we propose an autonomous driving simulator based upon neural radiance fields (NeRFs).","Compared with existing works, ours has three notable features: (1) Instance-aware.","Our simulator models the foreground instances and background environments separately with independent networks so that the static (e.g., size and appearance) and dynamic (e.g., trajectory) properties of instances can be controlled separately.","(2) Modular.","Our simulator allows flexible switching between different modern NeRF-related backbones, sampling strategies, input modalities, etc.","We expect this modular design to boost academic progress and industrial deployment of NeRF-based autonomous driving simulation.","(3) Realistic.","Our simulator set new state-of-the-art photo-realism results given the best module selection.","Our simulator will be open-sourced while most of our counterparts are not.","Project page: https://open-air-sun.github.io/mars/."],"url":"http://arxiv.org/abs/2307.15058v1"}
{"created":"2023-07-27 17:58:56","title":"IPv6 Hitlists at Scale: Be Careful What You Wish For","abstract":"Today's network measurements rely heavily on Internet-wide scanning, employing tools like ZMap that are capable of quickly iterating over the entire IPv4 address space. Unfortunately, IPv6's vast address space poses an existential threat for Internet-wide scans and traditional network measurement techniques. To address this reality, efforts are underway to develop ``hitlists'' of known-active IPv6 addresses to reduce the search space for would-be scanners. As a result, there is an inexorable push for constructing as large and complete a hitlist as possible.   This paper asks: what are the potential benefits and harms when IPv6 hitlists grow larger? To answer this question, we obtain the largest IPv6 active-address list to date: 7.9 billion addresses, 898 times larger than the current state-of-the-art hitlist. Although our list is not comprehensive, it is a significant step forward and provides a glimpse into the type of analyses possible with more complete hitlists.   We compare our dataset to prior IPv6 hitlists and show both benefits and dangers. The benefits include improved insight into client devices (prior datasets consist primarily of routers), outage detection, IPv6 roll-out, previously unknown aliased networks, and address assignment strategies. The dangers, unfortunately, are severe: we expose widespread instances of addresses that permit user tracking and device geolocation, and a dearth of firewalls in home networks. We discuss ethics and security guidelines to ensure a safe path towards more complete hitlists.","sentences":["Today's network measurements rely heavily on Internet-wide scanning, employing tools like ZMap that are capable of quickly iterating over the entire IPv4 address space.","Unfortunately, IPv6's vast address space poses an existential threat for Internet-wide scans and traditional network measurement techniques.","To address this reality, efforts are underway to develop ``hitlists'' of known-active IPv6 addresses to reduce the search space for would-be scanners.","As a result, there is an inexorable push for constructing as large and complete a hitlist as possible.   ","This paper asks: what are the potential benefits and harms when IPv6 hitlists grow larger?","To answer this question, we obtain the largest IPv6 active-address list to date: 7.9 billion addresses, 898 times larger than the current state-of-the-art hitlist.","Although our list is not comprehensive, it is a significant step forward and provides a glimpse into the type of analyses possible with more complete hitlists.   ","We compare our dataset to prior IPv6 hitlists and show both benefits and dangers.","The benefits include improved insight into client devices (prior datasets consist primarily of routers), outage detection, IPv6 roll-out, previously unknown aliased networks, and address assignment strategies.","The dangers, unfortunately, are severe: we expose widespread instances of addresses that permit user tracking and device geolocation, and a dearth of firewalls in home networks.","We discuss ethics and security guidelines to ensure a safe path towards more complete hitlists."],"url":"http://arxiv.org/abs/2307.15057v1"}
{"created":"2023-07-27 17:58:11","title":"PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking","abstract":"We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms. Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion. Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos. We create combinatorial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric effects. Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work. We show that existing methods can be trained from scratch in our dataset and outperform the published variants. Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks. Our data and code are publicly available at: https://pointodyssey.com","sentences":["We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms.","Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion.","Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos.","We create combinatorial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric effects.","Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work.","We show that existing methods can be trained from scratch in our dataset and outperform the published variants.","Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks.","Our data and code are publicly available at: https://pointodyssey.com"],"url":"http://arxiv.org/abs/2307.15055v1"}
{"created":"2023-07-27 17:57:57","title":"A Geometric Notion of Causal Probing","abstract":"Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the value of the conceptual component of a representation. Empirically, we find that R-LACE (Ravfogel et al., 2022) returns a one-dimensional subspace containing roughly half of total concept information under our framework. Our causal controlled intervention shows that, for at least one model, the subspace returned by R-LACE can be used to manipulate the concept value of the generated word with precision.","sentences":["Large language models rely on real-valued representations of text to make their predictions.","These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender.","A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space.","We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space.","We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently.","We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace.","Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the value of the conceptual component of a representation.","Empirically, we find that R-LACE (Ravfogel et al., 2022) returns a one-dimensional subspace containing roughly half of total concept information under our framework.","Our causal controlled intervention shows that, for at least one model, the subspace returned by R-LACE can be used to manipulate the concept value of the generated word with precision."],"url":"http://arxiv.org/abs/2307.15054v1"}
{"created":"2023-07-27 17:57:42","title":"On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation","abstract":"Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.   Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR. Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order. Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated. This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited.","sentences":["Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment.","Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval.","(Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.   ","Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment.","We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR.","Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order.","Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated.","This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited."],"url":"http://arxiv.org/abs/2307.15053v1"}
{"created":"2023-07-27 17:57:06","title":"Learning Depth Estimation for Transparent and Mirror Surfaces","abstract":"Inferring the depth of transparent or mirror (ToM) surfaces represents a hard challenge for either sensors, algorithms, or deep networks. We propose a simple pipeline for learning to estimate depth properly for such surfaces with neural networks, without requiring any ground-truth annotation. We unveil how to obtain reliable pseudo labels by in-painting ToM objects in images and processing them with a monocular depth estimation model. These labels can be used to fine-tune existing monocular or stereo networks, to let them learn how to deal with ToM surfaces. Experimental results on the Booster dataset show the dramatic improvements enabled by our remarkably simple proposal.","sentences":["Inferring the depth of transparent or mirror (ToM) surfaces represents a hard challenge for either sensors, algorithms, or deep networks.","We propose a simple pipeline for learning to estimate depth properly for such surfaces with neural networks, without requiring any ground-truth annotation.","We unveil how to obtain reliable pseudo labels by in-painting ToM objects in images and processing them with a monocular depth estimation model.","These labels can be used to fine-tune existing monocular or stereo networks, to let them learn how to deal with ToM surfaces.","Experimental results on the Booster dataset show the dramatic improvements enabled by our remarkably simple proposal."],"url":"http://arxiv.org/abs/2307.15052v1"}
{"created":"2023-07-27 17:56:56","title":"Matching Patients to Clinical Trials with Large Language Models","abstract":"Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scores prove effective in ranking clinical trials and exclude ineligible candidates. Our error analysis suggests that current LLMs still make some mistakes due to limited medical knowledge and domain-specific context understanding. Nonetheless, we believe the explanatory capabilities of LLMs are highly valuable. Future research is warranted on how such AI assistants can be integrated into the routine trial matching workflow in real-world settings to improve its efficiency.","sentences":["Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment.","In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection.","Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes.","We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials.","The experimental results demonstrate several key findings:","First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations.","Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations.","Third, these scores prove effective in ranking clinical trials and exclude ineligible candidates.","Our error analysis suggests that current LLMs still make some mistakes due to limited medical knowledge and domain-specific context understanding.","Nonetheless, we believe the explanatory capabilities of LLMs are highly valuable.","Future research is warranted on how such AI assistants can be integrated into the routine trial matching workflow in real-world settings to improve its efficiency."],"url":"http://arxiv.org/abs/2307.15051v1"}
{"created":"2023-07-27 17:56:05","title":"Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models","abstract":"Prompt tuning and adapter tuning have shown great potential in transferring pre-trained vision-language models (VLMs) to various downstream tasks. In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through a learnable selection. Inspired by neural pathways, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage. To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen. When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data. Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives. It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters. Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them. Project page can be found here (https://wuw2019.github.io/RMT/).","sentences":["Prompt tuning and adapter tuning have shown great potential in transferring pre-trained vision-language models (VLMs) to various downstream tasks.","In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through a learnable selection.","Inspired by neural pathways, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage.","To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen.","When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data.","Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives.","It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters.","Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them.","Project page can be found here (https://wuw2019.github.io/RMT/)."],"url":"http://arxiv.org/abs/2307.15049v1"}
{"created":"2023-07-27 17:51:52","title":"A Transformer-based Approach for Arabic Offline Handwritten Text Recognition","abstract":"Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains. In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text. Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation. However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks. Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy. To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and speed. Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallelizable and less complex. We employ pre-trained Transformers for both image understanding and language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-art approaches for recognizing offline Arabic handwritten text.","sentences":["Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains.","In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text.","Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation.","However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks.","Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy.","To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and speed.","Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallelizable and less complex.","We employ pre-trained Transformers for both image understanding and language modeling.","Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-art approaches for recognizing offline Arabic handwritten text."],"url":"http://arxiv.org/abs/2307.15045v1"}
{"created":"2023-07-27 17:49:12","title":"Universal and Transferable Adversarial Attacks on Aligned Language Models","abstract":"Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.","sentences":["Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation.","While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice.","In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors.","Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer).","However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   ","Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs.","Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B).","When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others.","In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information.","Code is available at github.com/llm-attacks/llm-attacks."],"url":"http://arxiv.org/abs/2307.15043v1"}
{"created":"2023-07-27 17:48:44","title":"TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis","abstract":"The gradual nature of a diffusion process that synthesizes samples in small increments constitutes a key ingredient of Denoising Diffusion Probabilistic Models (DDPM), which have presented unprecedented quality in image synthesis and been recently explored in the motion domain. In this work, we propose to adapt the gradual diffusion concept (operating along a diffusion time-axis) into the temporal-axis of the motion sequence. Our key idea is to extend the DDPM framework to support temporally varying denoising, thereby entangling the two axes. Using our special formulation, we iteratively denoise a motion buffer that contains a set of increasingly-noised poses, which auto-regressively produces an arbitrarily long stream of frames. With a stationary diffusion time-axis, in each diffusion step we increment only the temporal-axis of the motion such that the framework produces a new, clean frame which is removed from the beginning of the buffer, followed by a newly drawn noise vector that is appended to it. This new mechanism paves the way towards a new framework for long-term motion synthesis with applications to character animation and other domains.","sentences":["The gradual nature of a diffusion process that synthesizes samples in small increments constitutes a key ingredient of Denoising Diffusion Probabilistic Models (DDPM), which have presented unprecedented quality in image synthesis and been recently explored in the motion domain.","In this work, we propose to adapt the gradual diffusion concept (operating along a diffusion time-axis) into the temporal-axis of the motion sequence.","Our key idea is to extend the DDPM framework to support temporally varying denoising, thereby entangling the two axes.","Using our special formulation, we iteratively denoise a motion buffer that contains a set of increasingly-noised poses, which auto-regressively produces an arbitrarily long stream of frames.","With a stationary diffusion time-axis, in each diffusion step we increment only the temporal-axis of the motion such that the framework produces a new, clean frame which is removed from the beginning of the buffer, followed by a newly drawn noise vector that is appended to it.","This new mechanism paves the way towards a new framework for long-term motion synthesis with applications to character animation and other domains."],"url":"http://arxiv.org/abs/2307.15042v1"}
{"created":"2023-07-27 17:46:17","title":"A Sparse Quantized Hopfield Network for Online-Continual Memory","abstract":"An important difference between brains and deep neural networks is the way they learn. Nervous systems learn online where a stream of noisy data points are presented in a non-independent, identically distributed (non-i.i.d.) way. Further, synaptic plasticity in the brain depends only on information local to synapses. Deep networks, on the other hand, typically use non-local learning algorithms and are trained in an offline, non-noisy, i.i.d. setting. Understanding how neural networks learn under the same constraints as the brain is an open problem for neuroscience and neuromorphic computing. A standard approach to this problem has yet to be established. In this paper, we propose that discrete graphical models that learn via an online maximum a posteriori learning algorithm could provide such an approach. We implement this kind of model in a novel neural network called the Sparse Quantized Hopfield Network (SQHN). We show that SQHNs outperform state-of-the-art neural networks on associative memory tasks, outperform these models in online, non-i.i.d. settings, learn efficiently with noisy inputs, and are better than baselines on a novel episodic memory task.","sentences":["An important difference between brains and deep neural networks is the way they learn.","Nervous systems learn online where a stream of noisy data points are presented in a non-independent, identically distributed (non-i.i.d.) way.","Further, synaptic plasticity in the brain depends only on information local to synapses.","Deep networks, on the other hand, typically use non-local learning algorithms and are trained in an offline, non-noisy, i.i.d. setting.","Understanding how neural networks learn under the same constraints as the brain is an open problem for neuroscience and neuromorphic computing.","A standard approach to this problem has yet to be established.","In this paper, we propose that discrete graphical models that learn via an online maximum a posteriori learning algorithm could provide such an approach.","We implement this kind of model in a novel neural network called the Sparse Quantized Hopfield Network (SQHN).","We show that SQHNs outperform state-of-the-art neural networks on associative memory tasks, outperform these models in online, non-i.i.d. settings, learn efficiently with noisy inputs, and are better than baselines on a novel episodic memory task."],"url":"http://arxiv.org/abs/2307.15040v1"}
{"created":"2023-07-27 17:44:24","title":"Autocalibrating Gaze Tracking: A Demonstration through Gaze Typing","abstract":"Miscalibration of gaze tracking devices and the resulting need for repeat calibration are a significant barrier to use. As devices miscalibrate, people tend to auto-correct by gazing at neighboring targets, which makes it difficult to detect miscalibration from eye signals. To address this problem, we provide a novel and simple insight for autocalibrating eye trackers during gaze typing: the eyes are used as both input (i.e. typing) and output (i.e. reading) signals, but auto-correction by users only occurs when eye gaze is functioning as input. Thus, output eye gaze signals during reading can help systems detect the miscalibration offset and enable autocalibration. To demonstrate the potential for this type of approach, we designed and built an auto-calibration system for gaze typing and ran a user study with 15 able-bodied participants. Results from our user study suggest that such an implicit approach to autocalibration can significantly improve typing speed and overall user experience for gaze typing interfaces. Insights from our work are applicable to a broad set of gaze tracking technologies and may help create more seamless user experiences in a variety of domains.","sentences":["Miscalibration of gaze tracking devices and the resulting need for repeat calibration are a significant barrier to use.","As devices miscalibrate, people tend to auto-correct by gazing at neighboring targets, which makes it difficult to detect miscalibration from eye signals.","To address this problem, we provide a novel and simple insight for autocalibrating eye trackers during gaze typing: the eyes are used as both input (i.e. typing) and output (i.e. reading) signals, but auto-correction by users only occurs when eye gaze is functioning as input.","Thus, output eye gaze signals during reading can help systems detect the miscalibration offset and enable autocalibration.","To demonstrate the potential for this type of approach, we designed and built an auto-calibration system for gaze typing and ran a user study with 15 able-bodied participants.","Results from our user study suggest that such an implicit approach to autocalibration can significantly improve typing speed and overall user experience for gaze typing interfaces.","Insights from our work are applicable to a broad set of gaze tracking technologies and may help create more seamless user experiences in a variety of domains."],"url":"http://arxiv.org/abs/2307.15039v1"}
{"created":"2023-07-27 17:43:16","title":"3-Coloring $C_4$ or $C_3$-free Diameter Two Graphs","abstract":"The question of whether 3-Coloring can be solved in polynomial-time for the diameter two graphs is a well-known open problem in the area of algorithmic graph theory. We study the problem restricted to graph classes that avoid cycles of given lengths as induced subgraphs. Martin et. al. [CIAC 2021] showed that the problem is polynomial-time solvable for $C_5$-free or $C_6$-free graphs, and, $(C_4,C_s)$-free graphs where $s \\in \\{3,7,8,9\\}$. We extend their result proving that it is polynomial-time solvable for $(C_4,C_s)$-free graphs, for any constant $s$, and for $(C_3,C_7)$-free graphs. Our results also hold for the more general problem List 3-Colouring.","sentences":["The question of whether 3-Coloring can be solved in polynomial-time for the diameter two graphs is a well-known open problem in the area of algorithmic graph theory.","We study the problem restricted to graph classes that avoid cycles of given lengths as induced subgraphs.","Martin et.","al.","[CIAC 2021] showed that the problem is polynomial-time solvable for $C_5$-free or $C_6$-free graphs, and, $(C_4,C_s)$-free graphs where $s \\in \\{3,7,8,9\\}$.","We extend their result proving that it is polynomial-time solvable for $(C_4,C_s)$-free graphs, for any constant $s$, and for $(C_3,C_7)$-free graphs.","Our results also hold for the more general problem List 3-Colouring."],"url":"http://arxiv.org/abs/2307.15036v1"}
{"created":"2023-07-27 17:42:06","title":"Speeding up Fourier Neural Operators via Mixed Precision","abstract":"The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memory usage (up to 34%), with little or no reduction in accuracy, on the Navier-Stokes and Darcy flow equations. Combined with the recently proposed tensorized FNO (Kossaifi et al., 2023), the resulting model has far better performance while also being significantly faster than the original FNO.","sentences":["The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators.","For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks.","While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued)","Fourier domain and in function spaces.","On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision.","In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memory usage (up to 34%), with little or no reduction in accuracy, on the Navier-Stokes and Darcy flow equations.","Combined with the recently proposed tensorized FNO (Kossaifi et al., 2023), the resulting model has far better performance while also being significantly faster than the original FNO."],"url":"http://arxiv.org/abs/2307.15034v1"}
{"created":"2023-07-27 17:41:36","title":"Diverse Inpainting and Editing with GAN Inversion","abstract":"Recent inversion methods have shown that real images can be inverted into StyleGAN's latent space and numerous edits can be achieved on those images thanks to the semantically rich feature representations of well-trained GAN models. However, extensive research has also shown that image inversion is challenging due to the trade-off between high-fidelity reconstruction and editability. In this paper, we tackle an even more difficult task, inverting erased images into GAN's latent space for realistic inpaintings and editings. Furthermore, by augmenting inverted latent codes with different latent samples, we achieve diverse inpaintings. Specifically, we propose to learn an encoder and mixing network to combine encoded features from erased images with StyleGAN's mapped features from random samples. To encourage the mixing network to utilize both inputs, we train the networks with generated data via a novel set-up. We also utilize higher-rate features to prevent color inconsistencies between the inpainted and unerased parts. We run extensive experiments and compare our method with state-of-the-art inversion and inpainting methods. Qualitative metrics and visual comparisons show significant improvements.","sentences":["Recent inversion methods have shown that real images can be inverted into StyleGAN's latent space and numerous edits can be achieved on those images thanks to the semantically rich feature representations of well-trained GAN models.","However, extensive research has also shown that image inversion is challenging due to the trade-off between high-fidelity reconstruction and editability.","In this paper, we tackle an even more difficult task, inverting erased images into GAN's latent space for realistic inpaintings and editings.","Furthermore, by augmenting inverted latent codes with different latent samples, we achieve diverse inpaintings.","Specifically, we propose to learn an encoder and mixing network to combine encoded features from erased images with StyleGAN's mapped features from random samples.","To encourage the mixing network to utilize both inputs, we train the networks with generated data via a novel set-up.","We also utilize higher-rate features to prevent color inconsistencies between the inpainted and unerased parts.","We run extensive experiments and compare our method with state-of-the-art inversion and inpainting methods.","Qualitative metrics and visual comparisons show significant improvements."],"url":"http://arxiv.org/abs/2307.15033v1"}
{"created":"2023-07-27 17:37:56","title":"Adaptive Segmentation Network for Scene Text Detection","abstract":"Inspired by deep convolution segmentation algorithms, scene text detectors break the performance ceiling of datasets steadily. However, these methods often encounter threshold selection bottlenecks and have poor performance on text instances with extreme aspect ratios. In this paper, we propose to automatically learn the discriminate segmentation threshold, which distinguishes text pixels from background pixels for segmentation-based scene text detectors and then further reduces the time-consuming manual parameter adjustment. Besides, we design a Global-information Enhanced Feature Pyramid Network (GE-FPN) for capturing text instances with macro size and extreme aspect ratios. Following the GE-FPN, we introduce a cascade optimization structure to further refine the text instances. Finally, together with the proposed threshold learning strategy and text detection structure, we design an Adaptive Segmentation Network (ASNet) for scene text detection. Extensive experiments are carried out to demonstrate that the proposed ASNet can achieve the state-of-the-art performance on four text detection benchmarks, i.e., ICDAR 2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500. The ablation experiments also verify the effectiveness of our contributions.","sentences":["Inspired by deep convolution segmentation algorithms, scene text detectors break the performance ceiling of datasets steadily.","However, these methods often encounter threshold selection bottlenecks and have poor performance on text instances with extreme aspect ratios.","In this paper, we propose to automatically learn the discriminate segmentation threshold, which distinguishes text pixels from background pixels for segmentation-based scene text detectors and then further reduces the time-consuming manual parameter adjustment.","Besides, we design a Global-information Enhanced Feature Pyramid Network (GE-FPN) for capturing text instances with macro size and extreme aspect ratios.","Following the GE-FPN, we introduce a cascade optimization structure to further refine the text instances.","Finally, together with the proposed threshold learning strategy and text detection structure, we design an Adaptive Segmentation Network (ASNet) for scene text detection.","Extensive experiments are carried out to demonstrate that the proposed ASNet can achieve the state-of-the-art performance on four text detection benchmarks, i.e., ICDAR 2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500.","The ablation experiments also verify the effectiveness of our contributions."],"url":"http://arxiv.org/abs/2307.15029v1"}
{"created":"2023-07-27 17:35:18","title":"Measuring Centralization of Online Platforms Through Size and Interconnection of Communities","abstract":"Decentralized architecture offers a robust and flexible structure for online platforms, since centralized moderation and computation can be easy to disrupt with targeted attacks. However, a platform offering a decentralized architecture does not guarantee that users will use it in a decentralized way, and measuring the centralization of socio-technical networks is not an easy task. In this paper we introduce a method of characterizing community influence in terms of how many edges between communities would be disrupted by a community's removal. Our approach provides a careful definition of \"centralization\" appropriate in bipartite user-community socio-technical networks, and demonstrates the inadequacy of more trivial methods for interrogating centralization such as examining the distribution of community sizes. We use this method to compare the structure of multiple socio-technical platforms -- Mastodon, git code hosting servers, BitChute, Usenet, and Voat -- and find a range of structures, from interconnected but decentralized git servers to an effectively centralized use of Mastodon servers, as well as multiscale hybrid network structures of disconnected Voat subverses. As the ecosystem of socio-technical platforms diversifies, it becomes critical to not solely focus on the underlying technologies but also consider the structure of how users interact through the technical infrastructure.","sentences":["Decentralized architecture offers a robust and flexible structure for online platforms, since centralized moderation and computation can be easy to disrupt with targeted attacks.","However, a platform offering a decentralized architecture does not guarantee that users will use it in a decentralized way, and measuring the centralization of socio-technical networks is not an easy task.","In this paper we introduce a method of characterizing community influence in terms of how many edges between communities would be disrupted by a community's removal.","Our approach provides a careful definition of \"centralization\" appropriate in bipartite user-community socio-technical networks, and demonstrates the inadequacy of more trivial methods for interrogating centralization such as examining the distribution of community sizes.","We use this method to compare the structure of multiple socio-technical platforms -- Mastodon, git code hosting servers, BitChute, Usenet, and Voat -- and find a range of structures, from interconnected but decentralized git servers to an effectively centralized use of Mastodon servers, as well as multiscale hybrid network structures of disconnected Voat subverses.","As the ecosystem of socio-technical platforms diversifies, it becomes critical to not solely focus on the underlying technologies but also consider the structure of how users interact through the technical infrastructure."],"url":"http://arxiv.org/abs/2307.15027v1"}
{"created":"2023-07-27 17:28:32","title":"Revealing the Impact of Beamforming in ISAC","abstract":"This letter proposes advanced beamforming design and analyzes its influence on the sensing and communications (S&C) performance for a multiple-antenna integrated S&C (ISAC) system with a single communication user and a single target. Novel closed-form beamformers are derived for three typical scenarios, including the sensing-centric design, communications-centric design, and Pareto optimal design. Regarding each scenario, the outage probability, ergodic communication rate (CR), and sensing rate (SR) are analyzed to derive the diversity orders and high signal-to-noise ratio slopes. Numerical results are provided to demonstrate that i) beamforming design can affect the high-SNR power offset and diversity order but does not influence the high-SNR slope; ii) ISAC exhibits larger high-SNR slopes and a more extensive SR-CR region than conventional frequency-division S&C (FDSAC) techniques.","sentences":["This letter proposes advanced beamforming design and analyzes its influence on the sensing and communications (S&C) performance for a multiple-antenna integrated S&C (ISAC) system with a single communication user and a single target.","Novel closed-form beamformers are derived for three typical scenarios, including the sensing-centric design, communications-centric design, and Pareto optimal design.","Regarding each scenario, the outage probability, ergodic communication rate (CR), and sensing rate (SR) are analyzed to derive the diversity orders and high signal-to-noise ratio slopes.","Numerical results are provided to demonstrate that i) beamforming design can affect the high-SNR power offset and diversity order but does not influence the high-SNR slope; ii) ISAC exhibits larger high-SNR slopes and a more extensive SR-CR region than conventional frequency-division S&C (FDSAC) techniques."],"url":"http://arxiv.org/abs/2307.15023v1"}
{"created":"2023-07-27 17:24:09","title":"SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark","abstract":"Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications. We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user preferences. We also demonstrate that GPT-4 is a reliable judge to automatically evaluate human preferences on open-ended questions in a Chinese context. Our benchmark will be released at https://www.CLUEbenchmarks.com","sentences":["Large language models (LLMs) have shown the potential to be integrated into human daily lives.","Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios.","However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications.","We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE.","SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE).","Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones.","At the same time, they can complement each other to predict actual user preferences.","We also demonstrate that GPT-4 is a reliable judge to automatically evaluate human preferences on open-ended questions in a Chinese context.","Our benchmark will be released at https://www.CLUEbenchmarks.com"],"url":"http://arxiv.org/abs/2307.15020v1"}
{"created":"2023-07-27 17:22:41","title":"Self-Supervised Graph Transformer for Deepfake Detection","abstract":"Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.","sentences":["Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset.","However, their performance deteriorates significantly when presented with unseen samples.","As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance.","Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur.","Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability.","The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision.","To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations.","The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches."],"url":"http://arxiv.org/abs/2307.15019v1"}
{"created":"2023-07-27 17:19:37","title":"Samplable Anonymous Aggregation for Private Federated Data Analysis","abstract":"We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails. Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.","sentences":["We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data.","Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails.","Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system."],"url":"http://arxiv.org/abs/2307.15017v1"}
{"created":"2023-07-27 17:19:32","title":"How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges","abstract":"Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments. We expect that this empirical study will prove valuable in advancing future models, leading to enhanced capabilities in comprehending and interpreting fine-grained visual data. Our project is released on https://github.com/htqin/GoogleBard-VisUnderstand","sentences":["Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI.","Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations.","Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions.","This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding.","Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance.","Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments.","We expect that this empirical study will prove valuable in advancing future models, leading to enhanced capabilities in comprehending and interpreting fine-grained visual data.","Our project is released on https://github.com/htqin/GoogleBard-VisUnderstand"],"url":"http://arxiv.org/abs/2307.15016v1"}
{"created":"2023-07-27 17:06:02","title":"Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability","abstract":"With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge the gap between the aforementioned strategies by proposing Verifiability Tuning (VerT), a method that transforms black-box models into models that naturally yield faithful and verifiable feature attributions. We begin by introducing a formal theoretical framework to understand verifiability and show that attributions produced by standard models cannot be verified. We then leverage this framework to propose a method to build verifiable models and feature attributions out of fully trained black-box models. Finally, we perform extensive experiments on semi-synthetic and real-world datasets, and show that VerT produces models that (1) yield explanations that are correct and verifiable and (2) are faithful to the original black-box models they are meant to explain.","sentences":["With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour.","To this end, two broad strategies have been outlined in prior literature to explain models.","Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them.","Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model.","Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power.","In this work, we aim to bridge the gap between the aforementioned strategies by proposing Verifiability Tuning (VerT), a method that transforms black-box models into models that naturally yield faithful and verifiable feature attributions.","We begin by introducing a formal theoretical framework to understand verifiability and show that attributions produced by standard models cannot be verified.","We then leverage this framework to propose a method to build verifiable models and feature attributions out of fully trained black-box models.","Finally, we perform extensive experiments on semi-synthetic and real-world datasets, and show that VerT produces models that (1) yield explanations that are correct and verifiable and (2) are faithful to the original black-box models they are meant to explain."],"url":"http://arxiv.org/abs/2307.15007v1"}
{"created":"2023-07-27 17:04:05","title":"FLiCR: A Fast and Lightweight LiDAR Point Cloud Compression Based on Lossy RI","abstract":"Light detection and ranging (LiDAR) sensors are becoming available on modern mobile devices and provide a 3D sensing capability. This new capability is beneficial for perceptions in various use cases, but it is challenging for resource-constrained mobile devices to use the perceptions in real-time because of their high computational complexity. In this context, edge computing can be used to enable LiDAR online perceptions, but offloading the perceptions on the edge server requires a low-latency, lightweight, and efficient compression due to the large volume of LiDAR point clouds data.   This paper presents FLiCR, a fast and lightweight LiDAR point cloud compression method for enabling edge-assisted online perceptions. FLiCR is based on range images (RI) as an intermediate representation (IR), and dictionary coding for compressing RIs. FLiCR achieves its benefits by leveraging lossy RIs, and we show the efficiency of bytestream compression is largely improved with quantization and subsampling. In addition, we identify the limitation of current quality metrics for presenting the entropy of a point cloud, and introduce a new metric that reflects both point-wise and entropy-wise qualities for lossy IRs. The evaluation results show FLiCR is more suitable for edge-assisted real-time perceptions than the existing LiDAR compressions, and we demonstrate the effectiveness of our compression and metric with the evaluations on 3D object detection and LiDAR SLAM.","sentences":["Light detection and ranging (LiDAR) sensors are becoming available on modern mobile devices and provide a 3D sensing capability.","This new capability is beneficial for perceptions in various use cases, but it is challenging for resource-constrained mobile devices to use the perceptions in real-time because of their high computational complexity.","In this context, edge computing can be used to enable LiDAR online perceptions, but offloading the perceptions on the edge server requires a low-latency, lightweight, and efficient compression due to the large volume of LiDAR point clouds data.   ","This paper presents FLiCR, a fast and lightweight LiDAR point cloud compression method for enabling edge-assisted online perceptions.","FLiCR is based on range images (RI) as an intermediate representation (IR), and dictionary coding for compressing RIs.","FLiCR achieves its benefits by leveraging lossy RIs, and we show the efficiency of bytestream compression is largely improved with quantization and subsampling.","In addition, we identify the limitation of current quality metrics for presenting the entropy of a point cloud, and introduce a new metric that reflects both point-wise and entropy-wise qualities for lossy IRs.","The evaluation results show FLiCR is more suitable for edge-assisted real-time perceptions than the existing LiDAR compressions, and we demonstrate the effectiveness of our compression and metric with the evaluations on 3D object detection and LiDAR SLAM."],"url":"http://arxiv.org/abs/2307.15005v1"}
{"created":"2023-07-27 16:57:32","title":"Gzip versus bag-of-words for text classification with KNN","abstract":"The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.","sentences":["The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention.","In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary.","Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient."],"url":"http://arxiv.org/abs/2307.15002v1"}
{"created":"2023-07-27 16:45:33","title":"Scaling TransNormer to 175 Billion Parameters","abstract":"We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20%. Furthermore, we have developed a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. Scalability is at the heart of our model's design, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, all while maintaining outstanding performance metrics. Rigorous validation of our model design is achieved through a series of comprehensive experiments on our self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure data quality and relevance, we implement a new self-cleaning strategy to filter our collected data. Our pre-trained models will be released to foster community advancements in efficient LLMs.","sentences":["We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency.","TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization.","Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens.","Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times.","To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20%.","Furthermore, we have developed a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages.","Scalability is at the heart of our model's design, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, all while maintaining outstanding performance metrics.","Rigorous validation of our model design is achieved through a series of comprehensive experiments on our self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion tokens.","To ensure data quality and relevance, we implement a new self-cleaning strategy to filter our collected data.","Our pre-trained models will be released to foster community advancements in efficient LLMs."],"url":"http://arxiv.org/abs/2307.14995v1"}
{"created":"2023-07-27 16:40:14","title":"Thinker: Learning to Plan and Act","abstract":"We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. The algorithm's generality opens a new research direction on how a world model can be used in reinforcement learning and how planning can be seamlessly integrated into an agent's decision-making process.","sentences":["We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model.","The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model.","These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment.","This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization.","We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively.","Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions.","The algorithm's generality opens a new research direction on how a world model can be used in reinforcement learning and how planning can be seamlessly integrated into an agent's decision-making process."],"url":"http://arxiv.org/abs/2307.14993v1"}
{"created":"2023-07-27 16:37:30","title":"Multilingual Code Co-Evolution Using Large Language Models","abstract":"Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.","sentences":["Many software projects implement APIs and algorithms in multiple programming languages.","Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages.","In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value.","Translating each time the entire codebase from one language to another is not the way developers work.","In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs).","We design and implement the first LLM, dubbed Codeditor, to tackle this task.","Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages.","To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#).","Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics.","Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance."],"url":"http://arxiv.org/abs/2307.14991v1"}
{"created":"2023-07-27 16:30:27","title":"Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs","abstract":"Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexity proportional to the fraction of the modified inputs. Our experiments with adapting the OPT-125M pre-trained language model demonstrate comparable accuracy on document classification while requiring 12.1X (median) fewer operations for processing sequences of atomic edits.","sentences":["Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs.","For example, an AI writing assistant is required to update its suggestions in real time as a document is edited.","Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization.","Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change.","However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse.","To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values.","We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexity proportional to the fraction of the modified inputs.","Our experiments with adapting the OPT-125M pre-trained language model demonstrate comparable accuracy on document classification while requiring 12.1X (median) fewer operations for processing sequences of atomic edits."],"url":"http://arxiv.org/abs/2307.14988v1"}
{"created":"2023-07-27 16:24:56","title":"S$^3$: Social-network Simulation System with Large Language Model-Empowered Agents","abstract":"Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.","sentences":["Social network simulation plays a crucial role in addressing various challenges within social science.","It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others.","In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem).","Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network.","Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors.","By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions.","We conduct an evaluation encompassing two levels of simulation, employing real-world social network data.","Encouragingly, the results demonstrate promising accuracy.","This work represents an initial step in the realm of social network simulation empowered by LLM-based agents.","We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science."],"url":"http://arxiv.org/abs/2307.14984v1"}
{"created":"2023-07-27 16:19:12","title":"MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation","abstract":"Simulating camera sensors is a crucial task in autonomous driving. Although neural radiance fields are exceptional at synthesizing photorealistic views in driving simulations, they still fail in generating extrapolated views. This paper proposes to incorporate map priors into neural radiance fields to synthesize out-of-trajectory driving views with semantic road consistency. The key insight is that map information can be utilized as a prior to guide the training of the radiance fields with uncertainty. Specifically, we utilize the coarse ground surface as uncertain information to supervise the density field and warp depth with uncertainty from unknown camera poses to ensure multi-view consistency. Experimental results demonstrate that our approach can produce semantic consistency in deviated views for vehicle camera simulation.","sentences":["Simulating camera sensors is a crucial task in autonomous driving.","Although neural radiance fields are exceptional at synthesizing photorealistic views in driving simulations, they still fail in generating extrapolated views.","This paper proposes to incorporate map priors into neural radiance fields to synthesize out-of-trajectory driving views with semantic road consistency.","The key insight is that map information can be utilized as a prior to guide the training of the radiance fields with uncertainty.","Specifically, we utilize the coarse ground surface as uncertain information to supervise the density field and warp depth with uncertainty from unknown camera poses to ensure multi-view consistency.","Experimental results demonstrate that our approach can produce semantic consistency in deviated views for vehicle camera simulation."],"url":"http://arxiv.org/abs/2307.14981v1"}
{"created":"2023-07-27 16:19:04","title":"Aligning rTWT with 802.1Qbv: a Network Calculus Approach","abstract":"Industry 4.0 applications impose the challenging demand of delivering packets with bounded latencies via a wireless network. This is further complicated if the network is not dedicated to the time critical application. In this paper we use network calculus analysis to derive closed form expressions of latency bounds for time critical traffic when 802.11 Target Wake Time (TWT) and 802.1Qbv work together in a shared 802.11 network.","sentences":["Industry 4.0 applications impose the challenging demand of delivering packets with bounded latencies via a wireless network.","This is further complicated if the network is not dedicated to the time critical application.","In this paper we use network calculus analysis to derive closed form expressions of latency bounds for time critical traffic when 802.11 Target Wake Time (TWT) and 802.1Qbv work together in a shared 802.11 network."],"url":"http://arxiv.org/abs/2307.14980v1"}
{"created":"2023-07-27 16:07:03","title":"Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models","abstract":"With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNetPart segmentation tasks. Code is available at https://github.com/wangzy22/TAP.","sentences":["With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision.","However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training.","In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model.","We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme.","Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud.","Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods.","Our method is also effective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNetPart segmentation tasks.","Code is available at https://github.com/wangzy22/TAP."],"url":"http://arxiv.org/abs/2307.14971v1"}
{"created":"2023-07-27 15:52:18","title":"Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification","abstract":"In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \\url{https://github.com/xmed-lab/Fed-MAS}.","sentences":["In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images.","Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners.","In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks.","Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements.","Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization.","Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model.","Our code is available at \\url{https://github.com/xmed-lab/Fed-MAS}."],"url":"http://arxiv.org/abs/2307.14959v1"}
{"created":"2023-07-27 15:48:13","title":"The Effect of Third Party Implementations on Reproducibility","abstract":"Reproducibility of recommender systems research has come under scrutiny during recent years. Along with works focusing on repeating experiments with certain algorithms, the research community has also started discussing various aspects of evaluation and how these affect reproducibility. We add a novel angle to this discussion by examining how unofficial third-party implementations could benefit or hinder reproducibility. Besides giving a general overview, we thoroughly examine six third-party implementations of a popular recommender algorithm and compare them to the official version on five public datasets. In the light of our alarming findings we aim to draw the attention of the research community to this neglected aspect of reproducibility.","sentences":["Reproducibility of recommender systems research has come under scrutiny during recent years.","Along with works focusing on repeating experiments with certain algorithms, the research community has also started discussing various aspects of evaluation and how these affect reproducibility.","We add a novel angle to this discussion by examining how unofficial third-party implementations could benefit or hinder reproducibility.","Besides giving a general overview, we thoroughly examine six third-party implementations of a popular recommender algorithm and compare them to the official version on five public datasets.","In the light of our alarming findings we aim to draw the attention of the research community to this neglected aspect of reproducibility."],"url":"http://arxiv.org/abs/2307.14956v1"}
{"created":"2023-07-27 15:46:59","title":"Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space","abstract":"This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification performance. Finally, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to the target domain.","sentences":["This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain.","We propose a novel MSDA framework based on dictionary learning and optimal transport.","We interpret each domain in MSDA as an empirical distribution.","As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions.","We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.","Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions.","We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification performance.","Finally, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to the target domain."],"url":"http://arxiv.org/abs/2307.14953v1"}
{"created":"2023-07-27 15:46:46","title":"Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning","abstract":"As the network scale increases, existing fully distributed solutions start to lag behind the real-world challenges such as (1) slow information propagation, (2) network communication failures, and (3) external adversarial attacks. In this paper, we focus on hierarchical system architecture and address the problem of non-Bayesian learning over networks that are vulnerable to communication failures and adversarial attacks. On network communication, we consider packet-dropping link failures.   We first propose a hierarchical robust push-sum algorithm that can achieve average consensus despite frequent packet-dropping link failures. We provide a sparse information fusion rule between the parameter server and arbitrarily selected network representatives. Then, interleaving the consensus update step with a dual averaging update with Kullback-Leibler (KL) divergence as the proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.   On external adversarial attacks, we consider Byzantine attacks in which the compromised agents can send maliciously calibrated messages to others (including both the agents and the parameter server). To avoid the curse of dimensionality of Byzantine consensus, we solve the non-Bayesian learning problem via running multiple dynamics, each of which only involves Byzantine consensus with scalar inputs. To facilitate resilient information propagation across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at the parameter server.","sentences":["As the network scale increases, existing fully distributed solutions start to lag behind the real-world challenges such as (1) slow information propagation, (2) network communication failures, and (3) external adversarial attacks.","In this paper, we focus on hierarchical system architecture and address the problem of non-Bayesian learning over networks that are vulnerable to communication failures and adversarial attacks.","On network communication, we consider packet-dropping link failures.   ","We first propose a hierarchical robust push-sum algorithm that can achieve average consensus despite frequent packet-dropping link failures.","We provide a sparse information fusion rule between the parameter server and arbitrarily selected network representatives.","Then, interleaving the consensus update step with a dual averaging update with Kullback-Leibler (KL) divergence as the proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.   ","On external adversarial attacks, we consider Byzantine attacks in which the compromised agents can send maliciously calibrated messages to others (including both the agents and the parameter server).","To avoid the curse of dimensionality of Byzantine consensus, we solve the non-Bayesian learning problem via running multiple dynamics, each of which only involves Byzantine consensus with scalar inputs.","To facilitate resilient information propagation across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at the parameter server."],"url":"http://arxiv.org/abs/2307.14952v1"}
{"created":"2023-07-27 15:46:41","title":"Widespread Flaws in Offline Evaluation of Recommender Systems","abstract":"Even though offline evaluation is just an imperfect proxy of online performance -- due to the interactive nature of recommenders -- it will probably remain the primary way of evaluation in recommender systems research for the foreseeable future, since the proprietary nature of production recommenders prevents independent validation of A/B test setups and verification of online results. Therefore, it is imperative that offline evaluation setups are as realistic and as flawless as they can be. Unfortunately, evaluation flaws are quite common in recommender systems research nowadays, due to later works copying flawed evaluation setups from their predecessors without questioning their validity. In the hope of improving the quality of offline evaluation of recommender systems, we discuss four of these widespread flaws and why researchers should avoid them.","sentences":["Even though offline evaluation is just an imperfect proxy of online performance -- due to the interactive nature of recommenders -- it will probably remain the primary way of evaluation in recommender systems research for the foreseeable future, since the proprietary nature of production recommenders prevents independent validation of A/B test setups and verification of online results.","Therefore, it is imperative that offline evaluation setups are as realistic and as flawless as they can be.","Unfortunately, evaluation flaws are quite common in recommender systems research nowadays, due to later works copying flawed evaluation setups from their predecessors without questioning their validity.","In the hope of improving the quality of offline evaluation of recommender systems, we discuss four of these widespread flaws and why researchers should avoid them."],"url":"http://arxiv.org/abs/2307.14951v1"}
{"created":"2023-07-27 15:43:41","title":"Visual Analysis of Displacement Processes in Porous Media using Spatio-Temporal Flow Graphs","abstract":"We developed a new approach comprised of different visualizations for the comparative spatio-temporal analysis of displacement processes in porous media. We aim to analyze and compare ensemble datasets from experiments to gain insight into the influence of different parameters on fluid flow. To capture the displacement of a defending fluid by an invading fluid, we first condense an input image series to a single time map. From this map, we generate a spatio-temporal flow graph covering the whole process. This graph is further simplified to only reflect topological changes in the movement of the invading fluid. Our interactive tools allow the visual analysis of these processes by visualizing the graph structure and the context of the experimental setup, as well as by providing charts for multiple metrics. We apply our approach to analyze and compare ensemble datasets jointly with domain experts, where we vary either fluid properties or the solid structure of the porous medium. We finally report the generated insights from the domain experts and discuss our contribution's advantages, generality, and limitations.","sentences":["We developed a new approach comprised of different visualizations for the comparative spatio-temporal analysis of displacement processes in porous media.","We aim to analyze and compare ensemble datasets from experiments to gain insight into the influence of different parameters on fluid flow.","To capture the displacement of a defending fluid by an invading fluid, we first condense an input image series to a single time map.","From this map, we generate a spatio-temporal flow graph covering the whole process.","This graph is further simplified to only reflect topological changes in the movement of the invading fluid.","Our interactive tools allow the visual analysis of these processes by visualizing the graph structure and the context of the experimental setup, as well as by providing charts for multiple metrics.","We apply our approach to analyze and compare ensemble datasets jointly with domain experts, where we vary either fluid properties or the solid structure of the porous medium.","We finally report the generated insights from the domain experts and discuss our contribution's advantages, generality, and limitations."],"url":"http://arxiv.org/abs/2307.14949v1"}
{"created":"2023-07-27 15:32:02","title":"A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs","abstract":"The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \\emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural ODEs in modelling constrained natural systems. Moreover, the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.","sentences":["The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs).","However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems.","In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems.","The proposed self-adaptive penalty function can dynamically adjust the penalty parameters.","The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models.","We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion.","The numerical experiments and a comparison with other penalty Neural ODE approaches and \\emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural ODEs in modelling constrained natural systems.","Moreover, the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions."],"url":"http://arxiv.org/abs/2307.14940v1"}
{"created":"2023-07-27 15:28:29","title":"PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback","abstract":"Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.","sentences":["Large Language Models for Code (Code LLM) are flourishing.","New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task.","Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc.","In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation.","Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.","Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs."],"url":"http://arxiv.org/abs/2307.14936v1"}
{"created":"2023-07-27 15:26:26","title":"Solving Data Quality Problems with Desbordante: a Demo","abstract":"Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.   However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.   Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the same time, as we are going to demonstrate in this presentation, complex statistics can be efficiently used to solve many classic data quality problems.   Desbordante is an open-source data profiler that aims to close this gap. It is built with emphasis on industrial application: it is efficient, scalable, resilient to crashes, and provides explanations. Furthermore, it provides seamless Python integration by offloading various costly operations to the C++ core, not only mining.   In this demonstration, we show several scenarios that allow end users to solve different data quality problems. Namely, we showcase typo detection, data deduplication, and data anomaly detection scenarios.","sentences":["Data profiling is an essential process in modern data-driven industries.","One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.   ","However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists.","This creates a significant barrier to the adoption of these tools in the industry.","Moreover, existing systems were not created with industrial-grade workloads in mind.","Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found.","It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.   ","Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public.","At the same time, as we are going to demonstrate in this presentation, complex statistics can be efficiently used to solve many classic data quality problems.   ","Desbordante is an open-source data profiler that aims to close this gap.","It is built with emphasis on industrial application: it is efficient, scalable, resilient to crashes, and provides explanations.","Furthermore, it provides seamless Python integration by offloading various costly operations to the C++ core, not only mining.   ","In this demonstration, we show several scenarios that allow end users to solve different data quality problems.","Namely, we showcase typo detection, data deduplication, and data anomaly detection scenarios."],"url":"http://arxiv.org/abs/2307.14935v1"}
{"created":"2023-07-27 15:19:49","title":"Evaluating Regular Path Queries on Compressed Adjacency Matrices","abstract":"Regular Path Queries (RPQs), which are essentially regular expressions to be matched against the labels of paths in labeled graphs, are at the core of graph database query languages like SPARQL. A way to solve RPQs is to translate them into a sequence of operations on the adjacency matrices of each label. We design and implement a Boolean algebra on sparse matrix representations and, as an application, use them to handle RPQs. Our baseline representation uses the same space as the previously most compact index for RPQs and excels in handling the hardest types of queries. Our more succinct structure, based on $k^2$-trees, is 4 times smaller and still solves complex RPQs in reasonable time.","sentences":["Regular Path Queries (RPQs), which are essentially regular expressions to be matched against the labels of paths in labeled graphs, are at the core of graph database query languages like SPARQL.","A way to solve RPQs is to translate them into a sequence of operations on the adjacency matrices of each label.","We design and implement a Boolean algebra on sparse matrix representations and, as an application, use them to handle RPQs.","Our baseline representation uses the same space as the previously most compact index for RPQs and excels in handling the hardest types of queries.","Our more succinct structure, based on $k^2$-trees, is 4 times smaller and still solves complex RPQs in reasonable time."],"url":"http://arxiv.org/abs/2307.14930v1"}
{"created":"2023-07-27 15:18:50","title":"Graph-based Polyphonic Multitrack Music Generation","abstract":"Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generate appealing short and long musical sequences and to realistically interpolate between them, producing music that is tonally and rhythmically consistent. Finally, the visualization of the embeddings shows that the model is able to organize its latent space in accordance with known musical concepts.","sentences":["Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships.","Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation.","This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music.","By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times.","This opens the door to a new form of human-computer interaction in the context of music co-creation.","After training the model on existing MIDI datasets, the experiments show that the model is able to generate appealing short and long musical sequences and to realistically interpolate between them, producing music that is tonally and rhythmically consistent.","Finally, the visualization of the embeddings shows that the model is able to organize its latent space in accordance with known musical concepts."],"url":"http://arxiv.org/abs/2307.14928v1"}
{"created":"2023-07-27 15:16:40","title":"Cascaded Code Distributed Computing With Low Complexity and Improved Flexibility","abstract":"Coded distributed computing, proposed by Li et al., offers significant potential for reducing the communication load in MapReduce computing systems. In the setting of the \\emph{cascaded} coded distributed computing that consisting of $K$ nodes, $N$ input files, and $Q$ output functions, the objective is to compute each output function through $s\\geq 1$ nodes with a computation load $r\\geq 1$, enabling the application of coding techniques during the Shuffle phase to achieve minimum communication load. However, for most existing coded distributed computing schemes, a major limitation lies in their demand for splitting the original data into an exponentially growing number of input files in terms of $N/\\binom{K}{r} \\in\\mathbb{N}$ and requiring an exponentially large number of output functions $Q/\\binom{K}{s} \\in\\mathbb{N}$, which imposes stringent requirements for implementation and results in significant coding complexity when $K$ is large. In this paper, we focus on the cascaded case of $K/s\\in\\mathbb{N} $, deliberately designing the strategy of input files store and output functions assignment based on a grouping method, such that a low-complexity two-round Shuffle phase is available. The main advantages of our proposed scheme contains: 1) the communication load is quilt close to or surprisingly better than the optimal state-of-the-art scheme proposed by Li et al.; 2) our scheme requires significantly less number of input files and output functions; 3) all the operations are implemented over the minimum binary field $\\mathbb{F}_2$.","sentences":["Coded distributed computing, proposed by Li et al., offers significant potential for reducing the communication load in MapReduce computing systems.","In the setting of the \\emph{cascaded} coded distributed computing that consisting of $K$ nodes, $N$ input files, and $Q$ output functions, the objective is to compute each output function through $s\\geq 1$ nodes with a computation load $r\\geq 1$, enabling the application of coding techniques during the Shuffle phase to achieve minimum communication load.","However, for most existing coded distributed computing schemes, a major limitation lies in their demand for splitting the original data into an exponentially growing number of input files in terms of $N/\\binom{K}{r} \\in\\mathbb{N}$ and requiring an exponentially large number of output functions $Q/\\binom{K}{s} \\in\\mathbb{N}$, which imposes stringent requirements for implementation and results in significant coding complexity when $K$ is large.","In this paper, we focus on the cascaded case of $K/s\\in\\mathbb{N} $, deliberately designing the strategy of input files store and output functions assignment based on a grouping method, such that a low-complexity two-round Shuffle phase is available.","The main advantages of our proposed scheme contains: 1) the communication load is quilt close to or surprisingly better than the optimal state-of-the-art scheme proposed by Li et al.; 2) our scheme requires significantly less number of input files and output functions; 3) all the operations are implemented over the minimum binary field $\\mathbb{F}_2$."],"url":"http://arxiv.org/abs/2307.14927v1"}
{"created":"2023-07-27 15:03:13","title":"Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems","abstract":"Performance Benchmarking of HPC systems is an ongoing effort that seeks to provide information that will allow for increased performance and improve the job schedulers that manage these systems. We develop a benchmarking tool that utilizes machine learning models and gathers performance data on GPU-accelerated nodes while they perform material segmentation analysis. The benchmark uses a ML model that has been converted from Caffe to PyTorch using the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while Vulcanite has faster model times in a large number of benchmarks, and it is also more subject to some environmental factors that can cause performances slower than Onyx. In contrast the model times from Onyx are consistent across benchmarks.","sentences":["Performance Benchmarking of HPC systems is an ongoing effort that seeks to provide information that will allow for increased performance and improve the job schedulers that manage these systems.","We develop a benchmarking tool that utilizes machine learning models and gathers performance data on GPU-accelerated nodes while they perform material segmentation analysis.","The benchmark uses a ML model that has been converted from Caffe to PyTorch using the MMdnn toolkit and the MINC-2500 dataset.","Performance data is gathered on two ERDC DSRC systems, Onyx and Vulcanite.","The data reveals that while Vulcanite has faster model times in a large number of benchmarks, and it is also more subject to some environmental factors that can cause performances slower than Onyx.","In contrast the model times from Onyx are consistent across benchmarks."],"url":"http://arxiv.org/abs/2307.14921v1"}
{"created":"2023-07-27 15:00:54","title":"GET3D--: Learning GET3D from Unconstrained Image Collections","abstract":"The demand for efficient 3D model generation techniques has grown exponentially, as manual creation of 3D models is time-consuming and requires specialized expertise. While generative models have shown potential in creating 3D textured shapes from 2D images, their applicability in 3D industries is limited due to the lack of a well-defined camera distribution in real-world scenarios, resulting in low-quality shapes. To overcome this limitation, we propose GET3D--, the first method that directly generates textured 3D shapes from 2D images with unknown pose and scale. GET3D-- comprises a 3D shape generator and a learnable camera sampler that captures the 6D external changes on the camera. In addition, We propose a novel training schedule to stably optimize both the shape generator and camera sampler in a unified framework. By controlling external variations using the learnable camera sampler, our method can generate aligned shapes with clear textures. Extensive experiments demonstrate the efficacy of GET3D--, which precisely fits the 6D camera pose distribution and generates high-quality shapes on both synthetic and realistic unconstrained datasets.","sentences":["The demand for efficient 3D model generation techniques has grown exponentially, as manual creation of 3D models is time-consuming and requires specialized expertise.","While generative models have shown potential in creating 3D textured shapes from 2D images, their applicability in 3D industries is limited due to the lack of a well-defined camera distribution in real-world scenarios, resulting in low-quality shapes.","To overcome this limitation, we propose GET3D--, the first method that directly generates textured 3D shapes from 2D images with unknown pose and scale.","GET3D-- comprises a 3D shape generator and a learnable camera sampler that captures the 6D external changes on the camera.","In addition, We propose a novel training schedule to stably optimize both the shape generator and camera sampler in a unified framework.","By controlling external variations using the learnable camera sampler, our method can generate aligned shapes with clear textures.","Extensive experiments demonstrate the efficacy of GET3D--, which precisely fits the 6D camera pose distribution and generates high-quality shapes on both synthetic and realistic unconstrained datasets."],"url":"http://arxiv.org/abs/2307.14918v1"}
{"created":"2023-07-27 15:00:31","title":"NSA: Naturalistic Support Artifact to Boost Network Confidence","abstract":"Visual AI systems are vulnerable to natural and synthetic physical corruption in the real-world. Such corruption often arises unexpectedly and alters the model's performance. In recent years, the primary focus has been on adversarial attacks. However, natural corruptions (e.g., snow, fog, dust) are an omnipresent threat to visual AI systems and should be considered equally important. Many existing works propose interesting solutions to train robust models against natural corruption. These works either leverage image augmentations, which come with the additional cost of model training, or place suspicious patches in the scene to design unadversarial examples. In this work, we propose the idea of naturalistic support artifacts (NSA) for robust prediction. The NSAs are shown to be beneficial in scenarios where model parameters are inaccessible and adding artifacts in the scene is feasible. The NSAs are natural looking objects generated through artifact training using DC-GAN to have high visual fidelity in the scene. We test against natural corruptions on the Imagenette dataset and observe the improvement in prediction confidence score by four times. We also demonstrate NSA's capability to increase adversarial accuracy by 8\\% on average. Lastly, we qualitatively analyze NSAs using saliency maps to understand how they help improve prediction confidence.","sentences":["Visual AI systems are vulnerable to natural and synthetic physical corruption in the real-world.","Such corruption often arises unexpectedly and alters the model's performance.","In recent years, the primary focus has been on adversarial attacks.","However, natural corruptions (e.g., snow, fog, dust) are an omnipresent threat to visual AI systems and should be considered equally important.","Many existing works propose interesting solutions to train robust models against natural corruption.","These works either leverage image augmentations, which come with the additional cost of model training, or place suspicious patches in the scene to design unadversarial examples.","In this work, we propose the idea of naturalistic support artifacts (NSA) for robust prediction.","The NSAs are shown to be beneficial in scenarios where model parameters are inaccessible and adding artifacts in the scene is feasible.","The NSAs are natural looking objects generated through artifact training using DC-GAN to have high visual fidelity in the scene.","We test against natural corruptions on the Imagenette dataset and observe the improvement in prediction confidence score by four times.","We also demonstrate NSA's capability to increase adversarial accuracy by 8\\% on average.","Lastly, we qualitatively analyze NSAs using saliency maps to understand how they help improve prediction confidence."],"url":"http://arxiv.org/abs/2307.14917v1"}
{"created":"2023-07-27 14:56:06","title":"ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection","abstract":"The task of multi-author writing style detection aims at finding any positions of writing style change in a given text document. We formulate the task as a natural language inference problem where two consecutive paragraphs are paired. Our approach focuses on transitions between paragraphs while truncating input tokens for the task. As backbone models, we employ different Transformer-based encoders with warmup phase during training. We submit the model version that outperforms baselines and other proposed model versions in our experiments. For the easy and medium setups, we submit transition-focused natural language inference based on DeBERTa with warmup training, and the same model without transition for the hard setup.","sentences":["The task of multi-author writing style detection aims at finding any positions of writing style change in a given text document.","We formulate the task as a natural language inference problem where two consecutive paragraphs are paired.","Our approach focuses on transitions between paragraphs while truncating input tokens for the task.","As backbone models, we employ different Transformer-based encoders with warmup phase during training.","We submit the model version that outperforms baselines and other proposed model versions in our experiments.","For the easy and medium setups, we submit transition-focused natural language inference based on DeBERTa with warmup training, and the same model without transition for the hard setup."],"url":"http://arxiv.org/abs/2307.14913v1"}
{"created":"2023-07-27 14:55:10","title":"ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection","abstract":"Fanfiction, a popular form of creative writing set within established fictional universes, has gained a substantial online following. However, ensuring the well-being and safety of participants has become a critical concern in this community. The detection of triggering content, material that may cause emotional distress or trauma to readers, poses a significant challenge. In this paper, we describe our approach for the Trigger Detection shared task at PAN CLEF 2023, where we want to detect multiple triggering content in a given Fanfiction document. For this, we build a hierarchical model that uses recurrence over Transformer-based language models. In our approach, we first split long documents into smaller sized segments and use them to fine-tune a Transformer model. Then, we extract feature embeddings from the fine-tuned Transformer model, which are used as input in the training of multiple LSTM models for trigger detection in a multi-label setting. Our model achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the validation set, which are higher than the baseline results shared at PAN CLEF 2023.","sentences":["Fanfiction, a popular form of creative writing set within established fictional universes, has gained a substantial online following.","However, ensuring the well-being and safety of participants has become a critical concern in this community.","The detection of triggering content, material that may cause emotional distress or trauma to readers, poses a significant challenge.","In this paper, we describe our approach for the Trigger Detection shared task at PAN CLEF 2023, where we want to detect multiple triggering content in a given Fanfiction document.","For this, we build a hierarchical model that uses recurrence over Transformer-based language models.","In our approach, we first split long documents into smaller sized segments and use them to fine-tune a Transformer model.","Then, we extract feature embeddings from the fine-tuned Transformer model, which are used as input in the training of multiple LSTM models for trigger detection in a multi-label setting.","Our model achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the validation set, which are higher than the baseline results shared at PAN CLEF 2023."],"url":"http://arxiv.org/abs/2307.14912v1"}
{"created":"2023-07-27 14:52:56","title":"Low-Latency Massive Access with Multicast Wake Up Radio","abstract":"The use of Wake-Up Radio (WUR) in Internet of Things (IoT) networks can significantly improve their energy efficiency: battery-powered sensors can remain in a low-power (sleep) mode while listening for wake-up messages using their WUR and reactivate only when polled, saving energy. However, polling-based Time Division Multiple Access (TDMA) may significantly increase data transmission delay if packets are generated sporadically, as nodes with no information still need to be polled. In this paper, we examine the effect of multicast polling for WUR-enabled wireless nodes. The idea is to assign nodes to multicast groups so that all nodes in the same group can be solicited by a multicast polling message. This may cause collisions, which can be solved by requesting retransmissions from the involved nodes. We analyze the performance of different multicast polling and retransmission strategies, showing that the optimal approach can significantly reduce the delay over TDMA and ALOHA in low-traffic scenarios while keeping good energy efficiency.","sentences":["The use of Wake-Up Radio (WUR) in Internet of Things (IoT) networks can significantly improve their energy efficiency: battery-powered sensors can remain in a low-power (sleep) mode while listening for wake-up messages using their WUR and reactivate only when polled, saving energy.","However, polling-based Time Division Multiple Access (TDMA) may significantly increase data transmission delay if packets are generated sporadically, as nodes with no information still need to be polled.","In this paper, we examine the effect of multicast polling for WUR-enabled wireless nodes.","The idea is to assign nodes to multicast groups so that all nodes in the same group can be solicited by a multicast polling message.","This may cause collisions, which can be solved by requesting retransmissions from the involved nodes.","We analyze the performance of different multicast polling and retransmission strategies, showing that the optimal approach can significantly reduce the delay over TDMA and ALOHA in low-traffic scenarios while keeping good energy efficiency."],"url":"http://arxiv.org/abs/2307.14910v1"}
{"created":"2023-07-27 14:49:52","title":"Targeted Static Analysis for OCaml C Stubs: eliminating gremlins from the code","abstract":"Migration to OCaml 5 requires updating a lot of C bindings due to the removal of naked pointer support. Writing OCaml user-defined primitives in C is a necessity, but is unsafe and error-prone. It does not benefit from either OCaml's or C's type checking, and existing C static analysers are not aware of the OCaml GC safety rules, and cannot infer them from existing macros alone.The alternative is automatically generating C stubs, which requires correctly managing value lifetimes. Having a static analyser for OCaml to C interfaces is useful outside the OCaml 5 porting effort too.   After some motivating examples of real bugs in C bindings a static analyser is presented that finds these known classes of bugs. The tool works on the OCaml abstract parse and typed trees, and generates a header file and a caller model. Together with a simplified model of the OCaml runtime this is used as input to a static analysis framework, Goblint. An analysis is developed that tracks dereferences of OCaml values, and together with the existing framework reports incorrect dereferences. An example is shown how to extend the analysis to cover more safety properties.   The tools and runtime models are generic and could be reused with other static analysis tools.","sentences":["Migration to OCaml 5 requires updating a lot of C bindings due to the removal of naked pointer support.","Writing OCaml user-defined primitives in C is a necessity, but is unsafe and error-prone.","It does not benefit from either OCaml's or C's type checking, and existing C static analysers are not aware of the OCaml GC safety rules, and cannot infer them from existing macros alone.","The alternative is automatically generating C stubs, which requires correctly managing value lifetimes.","Having a static analyser for OCaml to C interfaces is useful outside the OCaml 5 porting effort too.   ","After some motivating examples of real bugs in C bindings a static analyser is presented that finds these known classes of bugs.","The tool works on the OCaml abstract parse and typed trees, and generates a header file and a caller model.","Together with a simplified model of the OCaml runtime this is used as input to a static analysis framework, Goblint.","An analysis is developed that tracks dereferences of OCaml values, and together with the existing framework reports incorrect dereferences.","An example is shown how to extend the analysis to cover more safety properties.   ","The tools and runtime models are generic and could be reused with other static analysis tools."],"url":"http://arxiv.org/abs/2307.14909v1"}
{"created":"2023-07-27 14:47:38","title":"Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions","abstract":"This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.","sentences":["This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling.","Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy.","Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec.","A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings.","For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset."],"url":"http://arxiv.org/abs/2307.14906v1"}
{"created":"2023-07-27 14:46:09","title":"CodeLens: An Interactive Tool for Visualizing Code Representations","abstract":"Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information. Visualizing code representations can further enable human experts to gain an intuitive insight into the code. Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations. In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them. CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG). By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code. The Web-based interface of CodeLens is available at http://www.codelens.org. The demonstration video can be found at http://www.codelens.org/demo.","sentences":["Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information.","Visualizing code representations can further enable human experts to gain an intuitive insight into the code.","Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations.","In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them.","CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG).","By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code.","The Web-based interface of CodeLens is available at http://www.codelens.org.","The demonstration video can be found at http://www.codelens.org/demo."],"url":"http://arxiv.org/abs/2307.14902v1"}
{"created":"2023-07-27 14:44:56","title":"Text-guided Foundation Model Adaptation for Pathological Image Classification","abstract":"The recent surge of foundation models in computer vision and natural language processing opens up perspectives in utilizing multi-modal clinical data to train large models with strong generalizability. Yet pathological image datasets often lack biomedical text annotation and enrichment. Guiding data-efficient image diagnosis from the use of biomedical text knowledge becomes a substantial interest. In this paper, we propose to Connect Image and Text Embeddings (CITE) to enhance pathological image classification. CITE injects text insights gained from language models pre-trained with a broad range of biomedical texts, leading to adapt foundation models towards pathological image understanding. Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce. CITE offers insights into leveraging in-domain text knowledge to reinforce data-efficient pathological image classification. Code is available at https://github.com/Yunkun-Zhang/CITE.","sentences":["The recent surge of foundation models in computer vision and natural language processing opens up perspectives in utilizing multi-modal clinical data to train large models with strong generalizability.","Yet pathological image datasets often lack biomedical text annotation and enrichment.","Guiding data-efficient image diagnosis from the use of biomedical text knowledge becomes a substantial interest.","In this paper, we propose to Connect Image and Text Embeddings (CITE) to enhance pathological image classification.","CITE injects text insights gained from language models pre-trained with a broad range of biomedical texts, leading to adapt foundation models towards pathological image understanding.","Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce.","CITE offers insights into leveraging in-domain text knowledge to reinforce data-efficient pathological image classification.","Code is available at https://github.com/Yunkun-Zhang/CITE."],"url":"http://arxiv.org/abs/2307.14901v1"}
{"created":"2023-07-27 14:42:16","title":"Retrieval-based Text Selection for Addressing Class-Imbalanced Data in Classification","abstract":"This paper addresses the problem of selecting of a set of texts for annotation in text classification using retrieval methods when there are limits on the number of annotations due to constraints on human resources. An additional challenge addressed is dealing with binary categories that have a small number of positive instances, reflecting severe class imbalance. In our situation, where annotation occurs over a long time period, the selection of texts to be annotated can be made in batches, with previous annotations guiding the choice of the next set. To address these challenges, the paper proposes leveraging SHAP to construct a quality set of queries for Elasticsearch and semantic search, to try to identify optimal sets of texts for annotation that will help with class imbalance. The approach is tested on sets of cue texts describing possible future events, constructed by participants involved in studies aimed to help with the management of obesity and diabetes. We introduce an effective method for selecting a small set of texts for annotation and building high-quality classifiers. We integrate vector search, semantic search, and machine learning classifiers to yield a good solution. Our experiments demonstrate improved F1 scores for the minority classes in binary classification.","sentences":["This paper addresses the problem of selecting of a set of texts for annotation in text classification using retrieval methods when there are limits on the number of annotations due to constraints on human resources.","An additional challenge addressed is dealing with binary categories that have a small number of positive instances, reflecting severe class imbalance.","In our situation, where annotation occurs over a long time period, the selection of texts to be annotated can be made in batches, with previous annotations guiding the choice of the next set.","To address these challenges, the paper proposes leveraging SHAP to construct a quality set of queries for Elasticsearch and semantic search, to try to identify optimal sets of texts for annotation that will help with class imbalance.","The approach is tested on sets of cue texts describing possible future events, constructed by participants involved in studies aimed to help with the management of obesity and diabetes.","We introduce an effective method for selecting a small set of texts for annotation and building high-quality classifiers.","We integrate vector search, semantic search, and machine learning classifiers to yield a good solution.","Our experiments demonstrate improved F1 scores for the minority classes in binary classification."],"url":"http://arxiv.org/abs/2307.14899v1"}
{"created":"2023-07-27 14:38:32","title":"Mixture of Self-Supervised Learning","abstract":"Self-supervised learning is popular method because of its ability to learn features in images without using its labels and is able to overcome limited labeled datasets used in supervised learning. Self-supervised learning works by using a pretext task which will be trained on the model before being applied to a specific task. There are some examples of pretext tasks used in self-supervised learning in the field of image recognition, namely rotation prediction, solving jigsaw puzzles, and predicting relative positions on image. Previous studies have only used one type of transformation as a pretext task. This raises the question of how it affects if more than one pretext task is used and to use a gating network to combine all pretext tasks. Therefore, we propose the Gated Self-Supervised Learning method to improve image classification which use more than one transformation as pretext task and uses the Mixture of Expert architecture as a gating network in combining each pretext task so that the model automatically can study and focus more on the most useful augmentations for classification. We test performance of the proposed method in several scenarios, namely CIFAR imbalance dataset classification, adversarial perturbations, Tiny-Imagenet dataset classification, and semi-supervised learning. Moreover, there are Grad-CAM and T-SNE analysis that are used to see the proposed method for identifying important features that influence image classification and representing data for each class and separating different classes properly. Our code is in https://github.com/aristorenaldo/G-SSL","sentences":["Self-supervised learning is popular method because of its ability to learn features in images without using its labels and is able to overcome limited labeled datasets used in supervised learning.","Self-supervised learning works by using a pretext task which will be trained on the model before being applied to a specific task.","There are some examples of pretext tasks used in self-supervised learning in the field of image recognition, namely rotation prediction, solving jigsaw puzzles, and predicting relative positions on image.","Previous studies have only used one type of transformation as a pretext task.","This raises the question of how it affects if more than one pretext task is used and to use a gating network to combine all pretext tasks.","Therefore, we propose the Gated Self-Supervised Learning method to improve image classification which use more than one transformation as pretext task and uses the Mixture of Expert architecture as a gating network in combining each pretext task so that the model automatically can study and focus more on the most useful augmentations for classification.","We test performance of the proposed method in several scenarios, namely CIFAR imbalance dataset classification, adversarial perturbations, Tiny-Imagenet dataset classification, and semi-supervised learning.","Moreover, there are Grad-CAM and T-SNE analysis that are used to see the proposed method for identifying important features that influence image classification and representing data for each class and separating different classes properly.","Our code is in https://github.com/aristorenaldo/G-SSL"],"url":"http://arxiv.org/abs/2307.14897v1"}
{"created":"2023-07-27 14:35:42","title":"Base-based Model Checking for Multi-Agent Only Believing (long version)","abstract":"We present a novel semantics for the language of multi-agent only believing exploiting belief bases, and show how to use it for automatically checking formulas of this language and of its dynamic extension with private belief expansion operators. We provide a PSPACE algorithm for model checking relying on a reduction to QBF and alternative dedicated algorithm relying on the exploration of the state space. We present an implementation of the QBF-based algorithm and some experimental results on computation time in a concrete example.","sentences":["We present a novel semantics for the language of multi-agent only believing exploiting belief bases, and show how to use it for automatically checking formulas of this language and of its dynamic extension with private belief expansion operators.","We provide a PSPACE algorithm for model checking relying on a reduction to QBF and alternative dedicated algorithm relying on the exploration of the state space.","We present an implementation of the QBF-based algorithm and some experimental results on computation time in a concrete example."],"url":"http://arxiv.org/abs/2307.14893v1"}
{"created":"2023-07-27 14:28:50","title":"Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving","abstract":"Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous vehicles (AVs) to make informed decisions and respond proactively in critical road scenarios. Promising results of 3D HPE have been gained in several domains such as human-computer interaction, robotics, sports and medical analytics, often based on data collected in well-controlled laboratory environments. Nevertheless, the transfer of 3D HPE methods to AVs has received limited research attention, due to the challenges posed by obtaining accurate 3D pose annotations and the limited suitability of data from other domains.   We present a simple yet efficient weakly supervised approach for 3D HPE in the AV context by employing a high-level sensor fusion between camera and LiDAR data. The weakly supervised setting enables training on the target datasets without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor and pseudo labels generated from LiDAR to image projections. Our approach outperforms state-of-the-art results by up to $\\sim$ 13% on the Waymo Open Dataset in the weakly supervised setting and achieves state-of-the-art results in the supervised setting.","sentences":["Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous vehicles (AVs) to make informed decisions and respond proactively in critical road scenarios.","Promising results of 3D HPE have been gained in several domains such as human-computer interaction, robotics, sports and medical analytics, often based on data collected in well-controlled laboratory environments.","Nevertheless, the transfer of 3D HPE methods to AVs has received limited research attention, due to the challenges posed by obtaining accurate 3D pose annotations and the limited suitability of data from other domains.   ","We present a simple yet efficient weakly supervised approach for 3D HPE in the AV context by employing a high-level sensor fusion between camera and LiDAR data.","The weakly supervised setting enables training on the target datasets without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor and pseudo labels generated from LiDAR to image projections.","Our approach outperforms state-of-the-art results by up to $\\sim$ 13% on the Waymo Open Dataset in the weakly supervised setting and achieves state-of-the-art results in the supervised setting."],"url":"http://arxiv.org/abs/2307.14889v1"}
{"created":"2023-07-27 14:12:27","title":"Knot Theory and Error-Correcting Codes","abstract":"This paper builds a novel bridge between algebraic coding theory and mathematical knot theory, with applications in both directions. We give methods to construct error-correcting codes starting from the colorings of a knot, describing through a series of results how the properties of the knot translate into code parameters. We show that knots can be used to obtain error-correcting codes with prescribed parameters and an efficient decoding algorithm.","sentences":["This paper builds a novel bridge between algebraic coding theory and mathematical knot theory, with applications in both directions.","We give methods to construct error-correcting codes starting from the colorings of a knot, describing through a series of results how the properties of the knot translate into code parameters.","We show that knots can be used to obtain error-correcting codes with prescribed parameters and an efficient decoding algorithm."],"url":"http://arxiv.org/abs/2307.14882v1"}
{"created":"2023-07-27 14:10:00","title":"Don't Shoot the Messenger: Localization Prevention of Satellite Internet Users","abstract":"Satellite Internet plays an increasingly important role in geopolitical conflicts. This notion was affirmed in the Ukrainian conflict escalating at the beginning of 2022, with the large-scale deployment of the Starlink satellite Internet service which consequently demonstrated the strategic importance of a free flow of information. Aside from military use, many citizens publish sensitive information on social media platforms to influence the public narrative. However, the use of satellite communication has proven to be dangerous, as the signals can be monitored by other satellites and used to triangulate the source on the ground. Unfortunately, the targeted killings of journalists have shown this threat to be effective. While the increasing deployment of satellite Internet systems gives citizens an unprecedented mouthpiece in conflicts, protecting them against localization is an unaddressed problem.   To address this threat, we present AnonSat, a novel scheme to protect satellite Internet users from triangulation. AnonSat works with cheap off-the-shelf devices, leveraging long-range wireless communication to span a local network among satellite base stations. This allows rerouting users' communication to other satellite base stations, some distance away from each user, thus, preventing their localization. AnonSat is designed for easy deployment and usability, which we demonstrate with a prototype implementation. Our large-scale network simulations using real-world data sets show the effectiveness of AnonSat in various practical settings.","sentences":["Satellite Internet plays an increasingly important role in geopolitical conflicts.","This notion was affirmed in the Ukrainian conflict escalating at the beginning of 2022, with the large-scale deployment of the Starlink satellite Internet service which consequently demonstrated the strategic importance of a free flow of information.","Aside from military use, many citizens publish sensitive information on social media platforms to influence the public narrative.","However, the use of satellite communication has proven to be dangerous, as the signals can be monitored by other satellites and used to triangulate the source on the ground.","Unfortunately, the targeted killings of journalists have shown this threat to be effective.","While the increasing deployment of satellite Internet systems gives citizens an unprecedented mouthpiece in conflicts, protecting them against localization is an unaddressed problem.   ","To address this threat, we present AnonSat, a novel scheme to protect satellite Internet users from triangulation.","AnonSat works with cheap off-the-shelf devices, leveraging long-range wireless communication to span a local network among satellite base stations.","This allows rerouting users' communication to other satellite base stations, some distance away from each user, thus, preventing their localization.","AnonSat is designed for easy deployment and usability, which we demonstrate with a prototype implementation.","Our large-scale network simulations using real-world data sets show the effectiveness of AnonSat in various practical settings."],"url":"http://arxiv.org/abs/2307.14879v1"}
{"created":"2023-07-27 14:09:59","title":"MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities","abstract":"The Entity Set Expansion (ESE) task aims to expand a handful of seed entities with new entities belonging to the same semantic class. Conventional ESE methods are based on mono-modality (i.e., literal modality), which struggle to deal with complex entities in the real world such as: (1) Negative entities with fine-grained semantic differences. (2) Synonymous entities. (3) Polysemous entities. (4) Long-tailed entities. These challenges prompt us to propose Multi-modal Entity Set Expansion (MESE), where models integrate information from multiple modalities to represent entities. Intuitively, the benefits of multi-modal information for ESE are threefold: (1) Different modalities can provide complementary information. (2) Multi-modal information provides a unified signal via common visual properties for the same semantic class or entity. (3) Multi-modal information offers robust alignment signal for synonymous entities. To assess the performance of model in MESE and facilitate further research, we constructed the MESED dataset which is the first multi-modal dataset for ESE with large-scale and elaborate manual calibration. A powerful multi-modal model MultiExpan is proposed which is pre-trained on four multimodal pre-training tasks. The extensive experiments and analyses on MESED demonstrate the high quality of the dataset and the effectiveness of our MultiExpan, as well as pointing the direction for future research.","sentences":["The Entity Set Expansion (ESE) task aims to expand a handful of seed entities with new entities belonging to the same semantic class.","Conventional ESE methods are based on mono-modality (i.e., literal modality), which struggle to deal with complex entities in the real world such as: (1) Negative entities with fine-grained semantic differences.","(2) Synonymous entities.","(3) Polysemous entities.","(4) Long-tailed entities.","These challenges prompt us to propose Multi-modal Entity Set Expansion (MESE), where models integrate information from multiple modalities to represent entities.","Intuitively, the benefits of multi-modal information for ESE are threefold: (1) Different modalities can provide complementary information.","(2) Multi-modal information provides a unified signal via common visual properties for the same semantic class or entity.","(3) Multi-modal information offers robust alignment signal for synonymous entities.","To assess the performance of model in MESE and facilitate further research, we constructed the MESED dataset which is the first multi-modal dataset for ESE with large-scale and elaborate manual calibration.","A powerful multi-modal model MultiExpan is proposed which is pre-trained on four multimodal pre-training tasks.","The extensive experiments and analyses on MESED demonstrate the high quality of the dataset and the effectiveness of our MultiExpan, as well as pointing the direction for future research."],"url":"http://arxiv.org/abs/2307.14878v1"}
{"created":"2023-07-27 14:06:36","title":"Single machine rescheduling for new orders: properties and complexity results","abstract":"Rescheduling problems arise in a variety of situations where a previously planned schedule needs to be adjusted to deal with unforeseen events. A common problem is the arrival of new orders, i.e. jobs, which have to be integrated into the schedule of the so-called old jobs. The maximum and total absolute time deviations of the completion times of these jobs are modeled as a disruption constraint to limit the change in the original schedule. Disruption constraints affect the shape of an optimal schedule, particularly with respect to the sequencing of old jobs and the insertion of idle time. We therefore give a classification into idle and no-idle problems for a set of single-machine rescheduling problems with different objective functions. We then prove the complexity of five rescheduling problems that have been left open in the literature.","sentences":["Rescheduling problems arise in a variety of situations where a previously planned schedule needs to be adjusted to deal with unforeseen events.","A common problem is the arrival of new orders, i.e. jobs, which have to be integrated into the schedule of the so-called old jobs.","The maximum and total absolute time deviations of the completion times of these jobs are modeled as a disruption constraint to limit the change in the original schedule.","Disruption constraints affect the shape of an optimal schedule, particularly with respect to the sequencing of old jobs and the insertion of idle time.","We therefore give a classification into idle and no-idle problems for a set of single-machine rescheduling problems with different objective functions.","We then prove the complexity of five rescheduling problems that have been left open in the literature."],"url":"http://arxiv.org/abs/2307.14876v1"}
{"created":"2023-07-27 14:01:32","title":"Correlation decay up to the sampling threshold in the local lemma regime","abstract":"We study the decay of correlation between locally constrained independent random variables in the local lemma regimes. the distribution defined by constraint satisfaction problems (CSPs) in the local lemma regime. For atomically constrained independent random variables of sufficiently large domains, we show that a decay of correlation property holds up to the local lemma condition $pD^{2+o(1)}\\lesssim 1$, asymptotically matching the sampling threshold for constraint satisfaction solutions [BGG+19,GGW22]. This provides evidence for the conjectured $pD^2\\lesssim 1$ threshold for the \"sampling Lov\\'{a}sz local lemma\".   We use a recursively-constructed coupling to bound the correlation decay. Our approach completely dispenses with the \"freezing\" paradigm originated from Beck [Bec91], which was commonly used to deal with the non-self-reducibility of the local lemma regimes, and hence can bypass the current technical barriers due to the use of $\\{2,3\\}$-trees.","sentences":["We study the decay of correlation between locally constrained independent random variables in the local lemma regimes.","the distribution defined by constraint satisfaction problems (CSPs) in the local lemma regime.","For atomically constrained independent random variables of sufficiently large domains, we show that a decay of correlation property holds up to the local lemma condition $pD^{2+o(1)}\\lesssim 1$, asymptotically matching the sampling threshold for constraint satisfaction solutions","[BGG+19,GGW22].","This provides evidence for the conjectured $pD^2\\lesssim 1$ threshold for the \"sampling Lov\\'{a}sz local lemma\".   ","We use a recursively-constructed coupling to bound the correlation decay.","Our approach completely dispenses with the \"freezing\" paradigm originated from Beck [Bec91], which was commonly used to deal with the non-self-reducibility of the local lemma regimes, and hence can bypass the current technical barriers due to the use of $\\{2,3\\}$-trees."],"url":"http://arxiv.org/abs/2307.14872v1"}
{"created":"2023-07-27 13:56:57","title":"Conditional Handover Modelling for Increased Contention Free Resource Use in 5G-Advanced","abstract":"This paper elaborates on Conditional Handover (CHO) modelling, aimed at maximizing the use of contention free random access (CFRA) during mobility. This is a desirable behavior as CFRA increases the chance of fast and successful handover. In CHO this may be especially challenging as the time between the preparation and the actual cell change can be significantly longer in comparison to non-conditional handover. Thus, new means to mitigate this issue need to be defined. We present the scheme where beam-specific measurement reporting can lead to CFRA resource updating prior to CHO execution. We have run system level simulations to confirm that the proposed solution increases the ratio of CFRA attempts during CHO. In the best-case scenario, we observe a gain exceeding 13%. We also show how the average delay of completing the handover is reduced. To provide the entire perspective, we present at what expense these gains can be achieved by analyzing the increased signaling overhead for updating the random access resources. The study has been conducted for various network settings and considering higher frequency ranges at which the user communicates with the network. Finally, we provide an outlook on future extensions of the investigated solution.","sentences":["This paper elaborates on Conditional Handover (CHO) modelling, aimed at maximizing the use of contention free random access (CFRA) during mobility.","This is a desirable behavior as CFRA increases the chance of fast and successful handover.","In CHO this may be especially challenging as the time between the preparation and the actual cell change can be significantly longer in comparison to non-conditional handover.","Thus, new means to mitigate this issue need to be defined.","We present the scheme where beam-specific measurement reporting can lead to CFRA resource updating prior to CHO execution.","We have run system level simulations to confirm that the proposed solution increases the ratio of CFRA attempts during CHO.","In the best-case scenario, we observe a gain exceeding 13%.","We also show how the average delay of completing the handover is reduced.","To provide the entire perspective, we present at what expense these gains can be achieved by analyzing the increased signaling overhead for updating the random access resources.","The study has been conducted for various network settings and considering higher frequency ranges at which the user communicates with the network.","Finally, we provide an outlook on future extensions of the investigated solution."],"url":"http://arxiv.org/abs/2307.14870v1"}
{"created":"2023-07-27 13:52:42","title":"Sample Less, Learn More: Efficient Action Recognition via Frame Feature Restoration","abstract":"Training an effective video action recognition model poses significant computational challenges, particularly under limited resource budgets. Current methods primarily aim to either reduce model size or utilize pre-trained models, limiting their adaptability to various backbone architectures. This paper investigates the issue of over-sampled frames, a prevalent problem in many approaches yet it has received relatively little attention. Despite the use of fewer frames being a potential solution, this approach often results in a substantial decline in performance. To address this issue, we propose a novel method to restore the intermediate features for two sparsely sampled and adjacent video frames. This feature restoration technique brings a negligible increase in computational requirements compared to resource-intensive image encoders, such as ViT. To evaluate the effectiveness of our method, we conduct extensive experiments on four public datasets, including Kinetics-400, ActivityNet, UCF-101, and HMDB-51. With the integration of our method, the efficiency of three commonly used baselines has been improved by over 50%, with a mere 0.5% reduction in recognition accuracy. In addition, our method also surprisingly helps improve the generalization ability of the models under zero-shot settings.","sentences":["Training an effective video action recognition model poses significant computational challenges, particularly under limited resource budgets.","Current methods primarily aim to either reduce model size or utilize pre-trained models, limiting their adaptability to various backbone architectures.","This paper investigates the issue of over-sampled frames, a prevalent problem in many approaches yet it has received relatively little attention.","Despite the use of fewer frames being a potential solution, this approach often results in a substantial decline in performance.","To address this issue, we propose a novel method to restore the intermediate features for two sparsely sampled and adjacent video frames.","This feature restoration technique brings a negligible increase in computational requirements compared to resource-intensive image encoders, such as ViT. To evaluate the effectiveness of our method, we conduct extensive experiments on four public datasets, including Kinetics-400, ActivityNet, UCF-101, and HMDB-51.","With the integration of our method, the efficiency of three commonly used baselines has been improved by over 50%, with a mere 0.5% reduction in recognition accuracy.","In addition, our method also surprisingly helps improve the generalization ability of the models under zero-shot settings."],"url":"http://arxiv.org/abs/2307.14866v1"}
{"created":"2023-07-27 13:49:27","title":"IML-ViT: Image Manipulation Localization by Vision Transformer","abstract":"Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between the manipulated and authentic regions, which needs to compare differences between these two areas explicitly. With the self-attention mechanism, naturally, the Transformer is the best candidate. Besides, artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border. Therefore, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision. We term this simple but effective ViT paradigm as the IML-ViT, which has great potential to become a new benchmark for IML. Extensive experiments on five benchmark datasets verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at \\url{https://github.com/SunnyHaze/IML-ViT}","sentences":["Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML).","But what makes a good IML model?","The answer lies in the way to capture artifacts.","Exploiting artifacts requires the model to extract non-semantic discrepancies between the manipulated and authentic regions, which needs to compare differences between these two areas explicitly.","With the self-attention mechanism, naturally, the Transformer is the best candidate.","Besides, artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border.","Therefore, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision.","We term this simple but effective ViT paradigm as the IML-ViT, which has great potential to become a new benchmark for IML.","Extensive experiments on five benchmark datasets verified our model outperforms the state-of-the-art manipulation localization methods.","Code and models are available at \\url{https://github.com/SunnyHaze/IML-ViT}"],"url":"http://arxiv.org/abs/2307.14863v1"}
{"created":"2023-07-27 13:45:41","title":"Quantum Computer Simulations at Warp Speed: Assessing the Impact of GPU Acceleration","abstract":"Quantum computer simulators are crucial for the development of quantum computing. In this work, we investigate the suitability and performance impact of GPU and multi-GPU systems on a widely used simulation tool - the state vector simulator Qiskit Aer. In particular, we evaluate the performance of both Qiskit's default Nvidia Thrust backend and the recent Nvidia cuQuantum backend on Nvidia A100 GPUs. We provide a benchmark suite of representative quantum applications for characterization. For simulations with a large number of qubits, the two GPU backends can provide up to 14x speedup over the CPU backend, with Nvidia cuQuantum providing further 1.5-3x speedup over the default Thrust backend. Our evaluation on a single GPU identifies the most important functions in Nvidia Thrust and cuQuantum for different quantum applications and their compute and memory bottlenecks. We also evaluate the gate fusion and cache-blocking optimizations on different quantum applications. Finally, we evaluate large-number qubit quantum applications on multi-GPU and identify data movement between host and GPU as the limiting factor for the performance.","sentences":["Quantum computer simulators are crucial for the development of quantum computing.","In this work, we investigate the suitability and performance impact of GPU and multi-GPU systems on a widely used simulation tool - the state vector simulator Qiskit Aer.","In particular, we evaluate the performance of both Qiskit's default Nvidia Thrust backend and the recent Nvidia cuQuantum backend on Nvidia A100 GPUs.","We provide a benchmark suite of representative quantum applications for characterization.","For simulations with a large number of qubits, the two GPU backends can provide up to 14x speedup over the CPU backend, with Nvidia cuQuantum providing further 1.5-3x speedup over the default Thrust backend.","Our evaluation on a single GPU identifies the most important functions in Nvidia Thrust and cuQuantum for different quantum applications and their compute and memory bottlenecks.","We also evaluate the gate fusion and cache-blocking optimizations on different quantum applications.","Finally, we evaluate large-number qubit quantum applications on multi-GPU and identify data movement between host and GPU as the limiting factor for the performance."],"url":"http://arxiv.org/abs/2307.14860v1"}
{"created":"2023-07-27 13:44:51","title":"Comparative Evaluation of Digital and Analog Chest Radiographs to Identify Tuberculosis using Deep Learning Model","abstract":"Purpose:Chest X-ray (CXR) is an essential tool and one of the most prescribed imaging to detect pulmonary abnormalities, with a yearly estimate of over 2 billion imaging performed worldwide. However, the accurate and timely diagnosis of TB remains an unmet goal. The prevalence of TB is highest in low-middle-income countries, and the requirement of a portable, automated, and reliable solution is required. In this study, we compared the performance of DL-based devices on digital and analog CXR. The evaluated DL-based device can be used in resource-constraint settings. Methods: A total of 10,000 CXR DICOMs(.dcm) and printed photos of the films acquired with three different cellular phones - Samsung S8, iPhone 8, and iPhone XS along with their radiological report were retrospectively collected from various sites across India from April 2020 to March 2021. Results: 10,000 chest X-rays were utilized to evaluate the DL-based device in identifying radiological signs of TB. The AUC of qXR for detecting signs of tuberculosis on the original DICOMs dataset was 0.928 with a sensitivity of 0.841 at a specificity of 0.806. At an optimal threshold, the difference in the AUC of three cellular smartphones with the original DICOMs is 0.024 (2.55%), 0.048 (5.10%), and 0.038 (1.91%). The minimum difference demonstrates the robustness of the DL-based device in identifying radiological signs of TB in both digital and analog CXR.","sentences":["Purpose:Chest X-ray (CXR) is an essential tool and one of the most prescribed imaging to detect pulmonary abnormalities, with a yearly estimate of over 2 billion imaging performed worldwide.","However, the accurate and timely diagnosis of TB remains an unmet goal.","The prevalence of TB is highest in low-middle-income countries, and the requirement of a portable, automated, and reliable solution is required.","In this study, we compared the performance of DL-based devices on digital and analog CXR.","The evaluated DL-based device can be used in resource-constraint settings.","Methods: A total of 10,000 CXR DICOMs(.dcm) and printed photos of the films acquired with three different cellular phones - Samsung S8, iPhone 8, and iPhone XS along with their radiological report were retrospectively collected from various sites across India from April 2020 to March 2021.","Results: 10,000 chest X-rays were utilized to evaluate the DL-based device in identifying radiological signs of TB.","The AUC of qXR for detecting signs of tuberculosis on the original DICOMs dataset was 0.928 with a sensitivity of 0.841 at a specificity of 0.806.","At an optimal threshold, the difference in the AUC of three cellular smartphones with the original DICOMs is 0.024 (2.55%), 0.048 (5.10%), and 0.038 (1.91%).","The minimum difference demonstrates the robustness of the DL-based device in identifying radiological signs of TB in both digital and analog CXR."],"url":"http://arxiv.org/abs/2307.14859v1"}
{"created":"2023-07-27 13:37:06","title":"Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners","abstract":"In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings. We posit that, with the right configuration and prompt design, seq2seq models can be highly effective few-shot learners for a wide spectrum of applications.","sentences":["In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates.","Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation.","Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks.","Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach.","Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings.","We posit that, with the right configuration and prompt design, seq2seq models can be highly effective few-shot learners for a wide spectrum of applications."],"url":"http://arxiv.org/abs/2307.14856v1"}
