{"created":"2023-10-05 17:59:56","title":"Improved Baselines with Visual Instruction Tuning","abstract":"Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.","sentences":["Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning.","In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient.","With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks.","Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node.","We hope this can make state-of-the-art LMM research more accessible.","Code and model will be publicly available."],"url":"http://arxiv.org/abs/2310.03744v1"}
{"created":"2023-10-05 17:59:55","title":"The Un-Kidnappable Robot: Acoustic Localization of Sneaking People","abstract":"How easy is it to sneak up on a robot? We examine whether we can detect people using only the incidental sounds they produce as they move, even when they try to be quiet. We collect a robotic dataset of high-quality 4-channel audio paired with 360 degree RGB data of people moving in different indoor settings. We train models that predict if there is a moving person nearby and their location using only audio. We implement our method on a robot, allowing it to track a single person moving quietly with only passive audio sensing. For demonstration videos, see our project page: https://sites.google.com/view/unkidnappable-robot","sentences":["How easy is it to sneak up on a robot?","We examine whether we can detect people using only the incidental sounds they produce as they move, even when they try to be quiet.","We collect a robotic dataset of high-quality 4-channel audio paired with 360 degree RGB data of people moving in different indoor settings.","We train models that predict if there is a moving person nearby and their location using only audio.","We implement our method on a robot, allowing it to track a single person moving quietly with only passive audio sensing.","For demonstration videos, see our project page: https://sites.google.com/view/unkidnappable-robot"],"url":"http://arxiv.org/abs/2310.03743v1"}
{"created":"2023-10-05 17:59:52","title":"A High-Performance Design, Implementation, Deployment, and Evaluation of The Slim Fly Network","abstract":"Novel low-diameter network topologies such as Slim Fly (SF) offer significant cost and power advantages over the established Fat Tree, Clos, or Dragonfly. To spearhead the adoption of low-diameter networks, we design, implement, deploy, and evaluate the first real-world SF installation. We focus on deployment, management, and operational aspects of our test cluster with 200 servers and carefully analyze performance. We demonstrate techniques for simple cabling and cabling validation as well as a novel high-performance routing architecture for InfiniBand-based low-diameter topologies. Our real-world benchmarks show SF's strong performance for many modern workloads such as deep neural network training, graph analytics, or linear algebra kernels. SF outperforms non-blocking Fat Trees in scalability while offering comparable or better performance and lower cost for large network sizes. Our work can facilitate deploying SF while the associated (open-source) routing architecture is fully portable and applicable to accelerate any low-diameter interconnect.","sentences":["Novel low-diameter network topologies such as Slim Fly (SF) offer significant cost and power advantages over the established Fat Tree, Clos, or Dragonfly.","To spearhead the adoption of low-diameter networks, we design, implement, deploy, and evaluate the first real-world SF installation.","We focus on deployment, management, and operational aspects of our test cluster with 200 servers and carefully analyze performance.","We demonstrate techniques for simple cabling and cabling validation as well as a novel high-performance routing architecture for InfiniBand-based low-diameter topologies.","Our real-world benchmarks show SF's strong performance for many modern workloads such as deep neural network training, graph analytics, or linear algebra kernels.","SF outperforms non-blocking Fat Trees in scalability while offering comparable or better performance and lower cost for large network sizes.","Our work can facilitate deploying SF while the associated (open-source) routing architecture is fully portable and applicable to accelerate any low-diameter interconnect."],"url":"http://arxiv.org/abs/2310.03742v1"}
{"created":"2023-10-05 17:59:45","title":"ContactGen: Generative Contact Modeling for Grasp Generation","abstract":"This paper presents a novel object-centric contact representation ContactGen for hand-object interaction. The ContactGen comprises three components: a contact map indicates the contact location, a part map represents the contact hand part, and a direction map tells the contact direction within each part. Given an input object, we propose a conditional generative model to predict ContactGen and adopt model-based optimization to predict diverse and geometrically feasible grasps. Experimental results demonstrate our method can generate high-fidelity and diverse human grasps for various objects. Project page: https://stevenlsw.github.io/contactgen/","sentences":["This paper presents a novel object-centric contact representation ContactGen for hand-object interaction.","The ContactGen comprises three components: a contact map indicates the contact location, a part map represents the contact hand part, and a direction map tells the contact direction within each part.","Given an input object, we propose a conditional generative model to predict ContactGen and adopt model-based optimization to predict diverse and geometrically feasible grasps.","Experimental results demonstrate our method can generate high-fidelity and diverse human grasps for various objects.","Project page: https://stevenlsw.github.io/contactgen/"],"url":"http://arxiv.org/abs/2310.03740v1"}
{"created":"2023-10-05 17:59:18","title":"Aligning Text-to-Image Diffusion Models with Reward Backpropagation","abstract":"Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets. Due to their unsupervised training, controlling their behavior in downstream tasks, such as maximizing human-perceived image quality, image-text alignment, or ethical image generation, is difficult. Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators. In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end-to-end backpropagation of the reward gradient through the denoising process. While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable. We test AlignProp in finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations. We show AlignProp achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest. Code and Visualization results are available at https://align-prop.github.io/.","sentences":["Text-to-image diffusion models have recently emerged at the forefront of image generation, powered by very large-scale unsupervised or weakly supervised text-to-image training datasets.","Due to their unsupervised training, controlling their behavior in downstream tasks, such as maximizing human-perceived image quality, image-text alignment, or ethical image generation, is difficult.","Recent works finetune diffusion models to downstream reward functions using vanilla reinforcement learning, notorious for the high variance of the gradient estimators.","In this paper, we propose AlignProp, a method that aligns diffusion models to downstream reward functions using end-to-end backpropagation of the reward gradient through the denoising process.","While naive implementation of such backpropagation would require prohibitive memory resources for storing the partial derivatives of modern text-to-image models, AlignProp finetunes low-rank adapter weight modules and uses gradient checkpointing, to render its memory usage viable.","We test AlignProp in finetuning diffusion models to various objectives, such as image-text semantic alignment, aesthetics, compressibility and controllability of the number of objects present, as well as their combinations.","We show AlignProp achieves higher rewards in fewer training steps than alternatives, while being conceptually simpler, making it a straightforward choice for optimizing diffusion models for differentiable reward functions of interest.","Code and Visualization results are available at https://align-prop.github.io/."],"url":"http://arxiv.org/abs/2310.03739v1"}
{"created":"2023-10-05 17:58:32","title":"Stylist: Style-Driven Feature Ranking for Robust Novelty Detection","abstract":"Novelty detection aims at finding samples that differ in some form from the distribution of seen samples. But not all changes are created equal. Data can suffer a multitude of distribution shifts, and we might want to detect only some types of relevant changes. Similar to works in out-of-distribution generalization, we propose to use the formalization of separating into semantic or content changes, that are relevant to our task, and style changes, that are irrelevant. Within this formalization, we define the robust novelty detection as the task of finding semantic changes while being robust to style distributional shifts. Leveraging pretrained, large-scale model representations, we introduce Stylist, a novel method that focuses on dropping environment-biased features. First, we compute a per-feature score based on the feature distribution distances between environments. Next, we show that our selection manages to remove features responsible for spurious correlations and improve novelty detection performance. For evaluation, we adapt domain generalization datasets to our task and analyze the methods behaviors. We additionally built a large synthetic dataset where we have control over the spurious correlations degree. We prove that our selection mechanism improves novelty detection algorithms across multiple datasets, containing both stylistic and content shifts.","sentences":["Novelty detection aims at finding samples that differ in some form from the distribution of seen samples.","But not all changes are created equal.","Data can suffer a multitude of distribution shifts, and we might want to detect only some types of relevant changes.","Similar to works in out-of-distribution generalization, we propose to use the formalization of separating into semantic or content changes, that are relevant to our task, and style changes, that are irrelevant.","Within this formalization, we define the robust novelty detection as the task of finding semantic changes while being robust to style distributional shifts.","Leveraging pretrained, large-scale model representations, we introduce Stylist, a novel method that focuses on dropping environment-biased features.","First, we compute a per-feature score based on the feature distribution distances between environments.","Next, we show that our selection manages to remove features responsible for spurious correlations and improve novelty detection performance.","For evaluation, we adapt domain generalization datasets to our task and analyze the methods behaviors.","We additionally built a large synthetic dataset where we have control over the spurious correlations degree.","We prove that our selection mechanism improves novelty detection algorithms across multiple datasets, containing both stylistic and content shifts."],"url":"http://arxiv.org/abs/2310.03738v1"}
{"created":"2023-10-05 17:57:42","title":"Recovering Single-Crossing Preferences From Approval Ballots","abstract":"An electorate with fully-ranked innate preferences casts approval votes over a finite set of alternatives. As a result, only partial information about the true preferences is revealed to the voting authorities. In an effort to understand the nature of the true preferences given only partial information, one might ask whether the unknown innate preferences could possibly be single-crossing. The existence of a polynomial time algorithm to determine this has been asked as an outstanding problem in the works of Elkind and Lackner. We hereby give a polynomial time algorithm determining a single-crossing collection of fully-ranked preferences that could have induced the elicited approval ballots, or reporting the nonexistence thereof. Moreover, we consider the problem of identifying negative instances with a set of forbidden sub-ballots, showing that any such characterization requires infinitely many forbidden configurations.","sentences":["An electorate with fully-ranked innate preferences casts approval votes over a finite set of alternatives.","As a result, only partial information about the true preferences is revealed to the voting authorities.","In an effort to understand the nature of the true preferences given only partial information, one might ask whether the unknown innate preferences could possibly be single-crossing.","The existence of a polynomial time algorithm to determine this has been asked as an outstanding problem in the works of Elkind and Lackner.","We hereby give a polynomial time algorithm determining a single-crossing collection of fully-ranked preferences that could have induced the elicited approval ballots, or reporting the nonexistence thereof.","Moreover, we consider the problem of identifying negative instances with a set of forbidden sub-ballots, showing that any such characterization requires infinitely many forbidden configurations."],"url":"http://arxiv.org/abs/2310.03736v1"}
{"created":"2023-10-05 17:55:19","title":"Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency","abstract":"Current vision-language generative models rely on expansive corpora of paired image-text data to attain optimal performance and generalization capabilities. However, automatically collecting such data (e.g. via large-scale web scraping) leads to low quality and poor image-text correlation, while human annotation is more accurate but requires significant manual effort and expense. We introduce $\\textbf{ITIT}$ ($\\textbf{I}$n$\\textbf{T}$egrating $\\textbf{I}$mage $\\textbf{T}$ext): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data. ITIT is comprised of a joint image-text encoder with disjoint image and text decoders that enable bidirectional image-to-text and text-to-image generation in a single framework. During training, ITIT leverages a small set of paired image-text data to ensure its output matches the input reasonably well in both directions. Simultaneously, the model is also trained on much larger datasets containing only images or texts. This is achieved by enforcing cycle consistency between the original unpaired samples and the cycle-generated counterparts. For instance, it generates a caption for a given input image and then uses the caption to create an output image, and enforces similarity between the input and output images. Our experiments show that ITIT with unpaired datasets exhibits similar scaling behavior as using high-quality paired data. We demonstrate image generation and captioning performance on par with state-of-the-art text-to-image and image-to-text models with orders of magnitude fewer (only 3M) paired image-text data.","sentences":["Current vision-language generative models rely on expansive corpora of paired image-text data to attain optimal performance and generalization capabilities.","However, automatically collecting such data (e.g. via large-scale web scraping) leads to low quality and poor image-text correlation, while human annotation is more accurate but requires significant manual effort and expense.","We introduce $\\textbf{ITIT}$ ($\\textbf{I}$n$\\textbf{T}$egrating $\\textbf{I}$mage $\\textbf{T}$ext): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data.","ITIT is comprised of a joint image-text encoder with disjoint image and text decoders that enable bidirectional image-to-text and text-to-image generation in a single framework.","During training, ITIT leverages a small set of paired image-text data to ensure its output matches the input reasonably well in both directions.","Simultaneously, the model is also trained on much larger datasets containing only images or texts.","This is achieved by enforcing cycle consistency between the original unpaired samples and the cycle-generated counterparts.","For instance, it generates a caption for a given input image and then uses the caption to create an output image, and enforces similarity between the input and output images.","Our experiments show that ITIT with unpaired datasets exhibits similar scaling behavior as using high-quality paired data.","We demonstrate image generation and captioning performance on par with state-of-the-art text-to-image and image-to-text models with orders of magnitude fewer (only 3M) paired image-text data."],"url":"http://arxiv.org/abs/2310.03734v1"}
{"created":"2023-10-05 17:52:09","title":"MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning","abstract":"The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.","sentences":["The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output.","In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities.","We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct.","Each solution interleaves natural language, code, and execution results.","We also introduce a customized supervised fine-tuning and inference approach.","This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems.","Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives.","Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset.","The dataset and models will be released at https://github.com/mathllm/MathCoder."],"url":"http://arxiv.org/abs/2310.03731v1"}
{"created":"2023-10-05 17:46:31","title":"Stochastic interpolants with data-dependent couplings","abstract":"Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to \\textit{couple} the base and the target densities. This enables us to incorporate information about class labels or continuous embeddings to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting.","sentences":["Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities.","Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic.","In this work, using the framework of stochastic interpolants, we formalize how to \\textit{couple} the base and the target densities.","This enables us to incorporate information about class labels or continuous embeddings to construct dynamical transport maps that serve as conditional generative models.","We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting.","We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting."],"url":"http://arxiv.org/abs/2310.03725v1"}
{"created":"2023-10-05 17:44:37","title":"Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer","abstract":"Recent research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation. In this work, we show that this type of approach can be further improved with multilingual training. We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages.","sentences":["Recent research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation.","In this work, we show that this type of approach can be further improved with multilingual training.","We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages."],"url":"http://arxiv.org/abs/2310.03724v1"}
{"created":"2023-10-05 17:40:09","title":"HeaP: Hierarchical Policies for Web Actions using LLMs","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website interactions, and show that it is able to outperform prior works using orders of magnitude less data.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings.","However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces.","We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy.","These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies.","We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies.","We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website interactions, and show that it is able to outperform prior works using orders of magnitude less data."],"url":"http://arxiv.org/abs/2310.03720v1"}
{"created":"2023-10-05 17:39:02","title":"Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning","abstract":"Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint thresholds data-efficiently. This makes our approach suitable for real-world dynamic applications.","sentences":["Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints.","Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area.","In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability.","To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization.","Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint thresholds data-efficiently.","This makes our approach suitable for real-world dynamic applications."],"url":"http://arxiv.org/abs/2310.03718v1"}
{"created":"2023-10-05 17:38:28","title":"A Long Way to Go: Investigating Length Correlations in RLHF","abstract":"Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models. Open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more \"helpful\" for tasks like web question answering, summarization, and multi-turn dialogue. When optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements in these settings. First, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. We then explore interventions during both RL and reward model learning to see if we can achieve the same downstream improvements as RLHF without increasing length. While our interventions mitigate length increases, they aren't uniformly effective across settings. Furthermore, we find that even running RLHF with a reward based solely on length can reproduce most of the downstream improvements over the initial policy model, showing that reward models in these settings have a long way to go.","sentences":["Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models.","Open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more \"helpful\" for tasks like web question answering, summarization, and multi-turn dialogue.","When optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs.","This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements in these settings.","First, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness.","Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths.","We then explore interventions during both RL and reward model learning to see if we can achieve the same downstream improvements as RLHF without increasing length.","While our interventions mitigate length increases, they aren't uniformly effective across settings.","Furthermore, we find that even running RLHF with a reward based solely on length can reproduce most of the downstream improvements over the initial policy model, showing that reward models in these settings have a long way to go."],"url":"http://arxiv.org/abs/2310.03716v1"}
{"created":"2023-10-05 17:37:58","title":"Artificial Intelligence Index Report 2023","abstract":"Welcome to the sixth edition of the AI Index Report. This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records, a study of the environmental impact of AI systems, and more. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI.","sentences":["Welcome to the sixth edition of the AI Index Report.","This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records, a study of the environmental impact of AI systems, and more.","The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence.","Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI.","The report aims to be the world's most credible and authoritative source for data and insights about AI."],"url":"http://arxiv.org/abs/2310.03715v1"}
{"created":"2023-10-05 17:37:25","title":"DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines","abstract":"The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded \"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy","sentences":["The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks.","Unfortunately, existing LM pipelines are typically implemented using hard-coded \"prompt templates\", i.e. lengthy strings discovered via trial and error.","Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules.","DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques.","We design a compiler that will optimize any DSPy pipeline to maximize a given metric.","We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops.","Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively).","On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5.","DSPy is available at https://github.com/stanfordnlp/dspy"],"url":"http://arxiv.org/abs/2310.03714v1"}
{"created":"2023-10-05 17:36:16","title":"Agent Instructs Large Language Models to be General Zero-Shot Reasoners","abstract":"We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.","sentences":["We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks.","Specifically, we build an autonomous agent to instruct the reasoning process of large language models.","We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks.","We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning.","We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate.","For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5","Turbo (17.0%).","Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%.","With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%."],"url":"http://arxiv.org/abs/2310.03710v1"}
{"created":"2023-10-05 17:35:26","title":"Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization","abstract":"Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MODPO objective are analytically the exact solutions of the original MORLHF objective. Empirical results in safety alignment and long-form question answering confirm that MODPO matches or outperforms existing methods, efficiently producing a Pareto-optimal set of LMs that cater to diverse preferences with 3 times less computational resources compared with MORLHF.","sentences":["Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences.","Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty).","LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings.","Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives.","In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives.","Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings.","With a simple cross-entropy loss, the LMs optimized against the MODPO objective are analytically the exact solutions of the original MORLHF objective.","Empirical results in safety alignment and long-form question answering confirm that MODPO matches or outperforms existing methods, efficiently producing a Pareto-optimal set of LMs that cater to diverse preferences with 3 times less computational resources compared with MORLHF."],"url":"http://arxiv.org/abs/2310.03708v1"}
{"created":"2023-10-05 17:34:47","title":"OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks","abstract":"Evasion Attacks (EA) are used to test the robustness of trained neural networks by distorting input data to misguide the model into incorrect classifications. Creating these attacks is a challenging task, especially with the ever-increasing complexity of models and datasets. In this work, we introduce a self-supervised, computationally economical method for generating adversarial examples, designed for the unseen black-box setting. Adapting techniques from representation learning, our method generates on-manifold EAs that are encouraged to resemble the data distribution. These attacks are comparable in effectiveness compared to the state-of-the-art when attacking the model trained on, but are significantly more effective when attacking unseen models, as the attacks are more related to the data rather than the model itself. Our experiments consistently demonstrate the method is effective across various models, unseen data categories, and even defended models, suggesting a significant role for on-manifold EAs when targeting unseen models.","sentences":["Evasion Attacks (EA) are used to test the robustness of trained neural networks by distorting input data to misguide the model into incorrect classifications.","Creating these attacks is a challenging task, especially with the ever-increasing complexity of models and datasets.","In this work, we introduce a self-supervised, computationally economical method for generating adversarial examples, designed for the unseen black-box setting.","Adapting techniques from representation learning, our method generates on-manifold EAs that are encouraged to resemble the data distribution.","These attacks are comparable in effectiveness compared to the state-of-the-art when attacking the model trained on, but are significantly more effective when attacking unseen models, as the attacks are more related to the data rather than the model itself.","Our experiments consistently demonstrate the method is effective across various models, unseen data categories, and even defended models, suggesting a significant role for on-manifold EAs when targeting unseen models."],"url":"http://arxiv.org/abs/2310.03707v1"}
{"created":"2023-10-05 17:24:36","title":"Drag View: Generalizable Novel View Synthesis with Unposed Imagery","abstract":"We introduce DragView, a novel and interactive framework for generating novel views of unseen scenes. DragView initializes the new view from a single source image, and the rendering is supported by a sparse set of unposed multi-view images, all seamlessly executed within a single feed-forward pass. Our approach begins with users dragging a source view through a local relative coordinate system. Pixel-aligned features are obtained by projecting the sampled 3D points along the target ray onto the source view. We then incorporate a view-dependent modulation layer to effectively handle occlusion during the projection. Additionally, we broaden the epipolar attention mechanism to encompass all source pixels, facilitating the aggregation of initialized coordinate-aligned point features from other unposed views. Finally, we employ another transformer to decode ray features into final pixel intensities. Crucially, our framework does not rely on either 2D prior models or the explicit estimation of camera poses. During testing, DragView showcases the capability to generalize to new scenes unseen during training, also utilizing only unposed support images, enabling the generation of photo-realistic new views characterized by flexible camera trajectories. In our experiments, we conduct a comprehensive comparison of the performance of DragView with recent scene representation networks operating under pose-free conditions, as well as with generalizable NeRFs subject to noisy test camera poses. DragView consistently demonstrates its superior performance in view synthesis quality, while also being more user-friendly. Project page: https://zhiwenfan.github.io/DragView/.","sentences":["We introduce DragView, a novel and interactive framework for generating novel views of unseen scenes.","DragView initializes the new view from a single source image, and the rendering is supported by a sparse set of unposed multi-view images, all seamlessly executed within a single feed-forward pass.","Our approach begins with users dragging a source view through a local relative coordinate system.","Pixel-aligned features are obtained by projecting the sampled 3D points along the target ray onto the source view.","We then incorporate a view-dependent modulation layer to effectively handle occlusion during the projection.","Additionally, we broaden the epipolar attention mechanism to encompass all source pixels, facilitating the aggregation of initialized coordinate-aligned point features from other unposed views.","Finally, we employ another transformer to decode ray features into final pixel intensities.","Crucially, our framework does not rely on either 2D prior models or the explicit estimation of camera poses.","During testing, DragView showcases the capability to generalize to new scenes unseen during training, also utilizing only unposed support images, enabling the generation of photo-realistic new views characterized by flexible camera trajectories.","In our experiments, we conduct a comprehensive comparison of the performance of DragView with recent scene representation networks operating under pose-free conditions, as well as with generalizable NeRFs subject to noisy test camera poses.","DragView consistently demonstrates its superior performance in view synthesis quality, while also being more user-friendly.","Project page: https://zhiwenfan.github.io/DragView/."],"url":"http://arxiv.org/abs/2310.03704v1"}
{"created":"2023-10-05 17:23:09","title":"Robust Analysis of Auction Equilibria","abstract":"Equilibria in auctions can be very difficult to analyze, beyond the symmetric environments where revenue equivalence renders the analysis straightforward. This paper takes a robust approach to evaluating the equilibria of auctions. Rather than identify the equilibria of an auction under specific environmental conditions, it considers worst-case analysis, where an auction is evaluated according to the worst environment and worst equilibrium in that environment. It identifies a non-equilibrium property of auctions that governs whether or not their worst-case equilibria are good for welfare and revenue. This property is easy to analyze, can be refined from data, and composes across markets where multiple auctions are run simultaneously.","sentences":["Equilibria in auctions can be very difficult to analyze, beyond the symmetric environments where revenue equivalence renders the analysis straightforward.","This paper takes a robust approach to evaluating the equilibria of auctions.","Rather than identify the equilibria of an auction under specific environmental conditions, it considers worst-case analysis, where an auction is evaluated according to the worst environment and worst equilibrium in that environment.","It identifies a non-equilibrium property of auctions that governs whether or not their worst-case equilibria are good for welfare and revenue.","This property is easy to analyze, can be refined from data, and composes across markets where multiple auctions are run simultaneously."],"url":"http://arxiv.org/abs/2310.03702v1"}
{"created":"2023-10-05 17:18:13","title":"BrickStARt: Enabling In-situ Design and Tangible Exploration for Personal Fabrication using Mixed Reality","abstract":"3D printers enable end-users to design and fabricate unique physical artifacts but maintain an increased entry barrier and friction. End users must design tangible artifacts through intangible media away from the main problem space (ex-situ) and transfer spatial requirements to an abstract software environment. To allow users to evaluate dimensions, balance, or fit early and in-situ, we developed BrickStARt, a design tool using tangible construction blocks paired with a mixed-reality headset. Users assemble a physical block model at the envisioned location of the fabricated artifact. Designs can be tested tangibly, refined, and digitally post-processed, remaining continuously in-situ. We implemented BrickStARt using a Magic Leap headset and present walkthroughs, highlighting novel interactions for 3D design. In a user study (n=16), first-time 3D modelers succeeded more often using BrickStARt than Tinkercad. Our results suggest that BrickStARt provides an accessible and explorative process while facilitating quick, tangible design iterations that allow users to detect physics-related issues (e.g., clearance) early on.","sentences":["3D printers enable end-users to design and fabricate unique physical artifacts but maintain an increased entry barrier and friction.","End users must design tangible artifacts through intangible media away from the main problem space (ex-situ) and transfer spatial requirements to an abstract software environment.","To allow users to evaluate dimensions, balance, or fit early and in-situ, we developed BrickStARt, a design tool using tangible construction blocks paired with a mixed-reality headset.","Users assemble a physical block model at the envisioned location of the fabricated artifact.","Designs can be tested tangibly, refined, and digitally post-processed, remaining continuously in-situ.","We implemented BrickStARt using a Magic Leap headset and present walkthroughs, highlighting novel interactions for 3D design.","In a user study (n=16), first-time 3D modelers succeeded more often using BrickStARt than Tinkercad.","Our results suggest that BrickStARt provides an accessible and explorative process while facilitating quick, tangible design iterations that allow users to detect physics-related issues (e.g., clearance) early on."],"url":"http://arxiv.org/abs/2310.03700v1"}
{"created":"2023-10-05 17:12:38","title":"Multimarginal generative modeling with stochastic interpolants","abstract":"Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure. Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework. The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted. The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption. In addition, the multimarginal perspective enables an efficient algorithm for reducing the dynamical transport cost in the ordinary two-marginal setting. We demonstrate these capacities with several numerical examples.","sentences":["Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals.","The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals.","We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure.","Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework.","The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted.","The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption.","In addition, the multimarginal perspective enables an efficient algorithm for reducing the dynamical transport cost in the ordinary two-marginal setting.","We demonstrate these capacities with several numerical examples."],"url":"http://arxiv.org/abs/2310.03695v1"}
{"created":"2023-10-05 17:12:17","title":"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!","abstract":"Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.","sentences":["Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning.","Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5","Turbo on custom datasets also encourage this practice.","But, what are the safety costs associated with such custom fine-tuning?","We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users.","Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples.","For instance, we jailbreak GPT-3.5","Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions.","Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent.","These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning.","We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs."],"url":"http://arxiv.org/abs/2310.03693v1"}
{"created":"2023-10-05 17:11:51","title":"DirectGPT: A Direct Manipulation Interface to Interact with Large Language Models","abstract":"We characterize and demonstrate how the principles of direct manipulation can improve interaction with large language models. This includes: continuous representation of generated objects of interest; reuse of prompt syntax in a toolbar of commands; manipulable outputs to compose or control the effect of prompts; and undo mechanisms. This idea is exemplified in DirectGPT, a user interface layer on top of ChatGPT that works by transforming direct manipulation actions to engineered prompts. A study shows participants were 50% faster and relied on 50% fewer and 72% shorter prompts to edit text, code, and vector images compared to baseline ChatGPT. Our work contributes a validated approach to integrate LLMs into traditional software using direct manipulation.","sentences":["We characterize and demonstrate how the principles of direct manipulation can improve interaction with large language models.","This includes: continuous representation of generated objects of interest; reuse of prompt syntax in a toolbar of commands; manipulable outputs to compose or control the effect of prompts; and undo mechanisms.","This idea is exemplified in DirectGPT, a user interface layer on top of ChatGPT that works by transforming direct manipulation actions to engineered prompts.","A study shows participants were 50% faster and relied on 50% fewer and 72% shorter prompts to edit text, code, and vector images compared to baseline ChatGPT.","Our work contributes a validated approach to integrate LLMs into traditional software using direct manipulation."],"url":"http://arxiv.org/abs/2310.03691v1"}
{"created":"2023-10-05 17:07:25","title":"Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries","abstract":"Due to limited resources and fast economic growth, designing optimal transportation road networks with traffic simulation and validation in a cost-effective manner is vital for developing countries, where extensive manual testing is expensive and often infeasible. Current rule-based road design generators lack diversity, a key feature for design robustness. Generative Flow Networks (GFlowNets) learn stochastic policies to sample from an unnormalized reward distribution, thus generating high-quality solutions while preserving their diversity. In this work, we formulate the problem of linking incident roads to the circular junction of a roundabout by a Markov decision process, and we leverage GFlowNets as the Junction-Art road generator. We compare our method with related methods and our empirical results show that our method achieves better diversity while preserving a high validity score.","sentences":["Due to limited resources and fast economic growth, designing optimal transportation road networks with traffic simulation and validation in a cost-effective manner is vital for developing countries, where extensive manual testing is expensive and often infeasible.","Current rule-based road design generators lack diversity, a key feature for design robustness.","Generative Flow Networks (GFlowNets) learn stochastic policies to sample from an unnormalized reward distribution, thus generating high-quality solutions while preserving their diversity.","In this work, we formulate the problem of linking incident roads to the circular junction of a roundabout by a Markov decision process, and we leverage GFlowNets as the Junction-Art road generator.","We compare our method with related methods and our empirical results show that our method achieves better diversity while preserving a high validity score."],"url":"http://arxiv.org/abs/2310.03687v1"}
{"created":"2023-10-05 17:04:59","title":"DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers","abstract":"In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity. Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens. Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models. The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols. We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation. The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of models.","sentences":["In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity.","Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens.","Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models.","The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols.","We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation.","The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of models."],"url":"http://arxiv.org/abs/2310.03686v1"}
{"created":"2023-10-05 17:01:53","title":"SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks","abstract":"Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.","sentences":["Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content.","To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs.","Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs.","SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation.","Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM."],"url":"http://arxiv.org/abs/2310.03684v1"}
{"created":"2023-10-05 16:52:59","title":"Hadamard Domain Training with Integers for Class Incremental Quantized Learning","abstract":"Continual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks. For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms. Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference. However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels. In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators. We demonstrate the effectiveness of our technique on several human activity recognition datasets and CIFAR100 in a class incremental learning setting. We achieve less than 0.5% and 3% accuracy degradation while we quantize all matrix multiplications inputs down to 4-bits with 8-bit accumulators.","sentences":["Continual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks.","For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms.","Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference.","However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels.","In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications.","We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators.","We demonstrate the effectiveness of our technique on several human activity recognition datasets and CIFAR100 in a class incremental learning setting.","We achieve less than 0.5% and 3% accuracy degradation while we quantize all matrix multiplications inputs down to 4-bits with 8-bit accumulators."],"url":"http://arxiv.org/abs/2310.03675v1"}
{"created":"2023-10-05 16:52:59","title":"PV-OSIMr: A Lowest Order Complexity Algorithm for Computing the Delassus Matrix","abstract":"We present PV-OSIMr, an efficient algorithm for computing the Delassus matrix (also known as the inverse operational space inertia matrix) for a kinematic tree, with the lowest order computational complexity known in literature. PV-OSIMr is derived by optimizing the Popov-Vereshchagin (PV) solver computations using the compositionality of the force and motion propagators. It has a computational complexity of O(n + m^2 ) compared to O(n + m^2d) of the original PV-OSIM algorithm and O(n+md+m^2 ) of the extended force propagator algorithm (EFPA), where n is the number of joints, m is the number of constraints and d is the depth of the kinematic tree. Since Delassus matrix computation requires constructing an m x m sized matrix and must consider all the n joints at least once, the asymptotic computational complexity of PV-OSIMr is optimal. We further benchmark our algorithm and find it to be often more efficient than the PV-OSIM and EFPA in practice.","sentences":["We present PV-OSIMr, an efficient algorithm for computing the Delassus matrix (also known as the inverse operational space inertia matrix) for a kinematic tree, with the lowest order computational complexity known in literature.","PV-OSIMr is derived by optimizing the Popov-Vereshchagin (PV) solver computations using the compositionality of the force and motion propagators.","It has a computational complexity of O(n + m^2 ) compared to O(n + m^2d) of the original PV-OSIM algorithm and O(n+md+m^2 ) of the extended force propagator algorithm (EFPA), where n is the number of joints, m is the number of constraints and d is the depth of the kinematic tree.","Since Delassus matrix computation requires constructing an m x m sized matrix and must consider all the n joints at least once, the asymptotic computational complexity of PV-OSIMr is optimal.","We further benchmark our algorithm and find it to be often more efficient than the PV-OSIM and EFPA in practice."],"url":"http://arxiv.org/abs/2310.03676v1"}
{"created":"2023-10-05 16:43:28","title":"LumiNet: The Bright Side of Perceptual Knowledge Distillation","abstract":"In knowledge distillation research, feature-based methods have dominated due to their ability to effectively tap into extensive teacher models. In contrast, logit-based approaches are considered to be less adept at extracting hidden 'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel knowledge-transfer algorithm designed to enhance logit-based distillation. We introduce a perception matrix that aims to recalibrate logits through adjustments based on the model's representation capability. By meticulously analyzing intra-class dynamics, LumiNet reconstructs more granular inter-class relationships, enabling the student model to learn a richer breadth of knowledge. Both teacher and student models are mapped onto this refined matrix, with the student's goal being to minimize representational discrepancies. Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO) attests to LumiNet's efficacy, revealing its competitive edge over leading feature-based methods. Moreover, in exploring the realm of transfer learning, we assess how effectively the student model, trained using our method, adapts to downstream tasks. Notably, when applied to Tiny ImageNet, the transferred features exhibit remarkable performance, further underscoring LumiNet's versatility and robustness in diverse settings. With LumiNet, we hope to steer the research discourse towards a renewed interest in the latent capabilities of logit-based knowledge distillation.","sentences":["In knowledge distillation research, feature-based methods have dominated due to their ability to effectively tap into extensive teacher models.","In contrast, logit-based approaches are considered to be less adept at extracting hidden 'dark knowledge' from teachers.","To bridge this gap, we present LumiNet, a novel knowledge-transfer algorithm designed to enhance logit-based distillation.","We introduce a perception matrix that aims to recalibrate logits through adjustments based on the model's representation capability.","By meticulously analyzing intra-class dynamics, LumiNet reconstructs more granular inter-class relationships, enabling the student model to learn a richer breadth of knowledge.","Both teacher and student models are mapped onto this refined matrix, with the student's goal being to minimize representational discrepancies.","Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO) attests to LumiNet's efficacy, revealing its competitive edge over leading feature-based methods.","Moreover, in exploring the realm of transfer learning, we assess how effectively the student model, trained using our method, adapts to downstream tasks.","Notably, when applied to Tiny ImageNet, the transferred features exhibit remarkable performance, further underscoring LumiNet's versatility and robustness in diverse settings.","With LumiNet, we hope to steer the research discourse towards a renewed interest in the latent capabilities of logit-based knowledge distillation."],"url":"http://arxiv.org/abs/2310.03669v1"}
{"created":"2023-10-05 16:43:13","title":"GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction","abstract":"Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results.","sentences":["Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks.","However, they have been less successful in Information Extraction (IE), lagging behind task-specific models.","Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans.","Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box.","In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines.","Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction.","The ablation study shows that detailed guidelines is key for good results."],"url":"http://arxiv.org/abs/2310.03668v1"}
{"created":"2023-10-05 16:43:12","title":"Enhancing Exfiltration Path Analysis Using Reinforcement Learning","abstract":"Building on previous work using reinforcement learning (RL) focused on identification of exfiltration paths, this work expands the methodology to include protocol and payload considerations. The former approach to exfiltration path discovery, where reward and state are associated specifically with the determination of optimal paths, are presented with these additional realistic characteristics to account for nuances in adversarial behavior. The paths generated are enhanced by including communication payload and protocol into the Markov decision process (MDP) in order to more realistically emulate attributes of network based exfiltration events. The proposed method will help emulate complex adversarial considerations such as the size of a payload being exported over time or the protocol on which it occurs, as is the case where threat actors steal data over long periods of time using system native ports or protocols to avoid detection. As such, practitioners will be able to improve identification of expected adversary behavior under various payload and protocol assumptions more comprehensively.","sentences":["Building on previous work using reinforcement learning (RL) focused on identification of exfiltration paths, this work expands the methodology to include protocol and payload considerations.","The former approach to exfiltration path discovery, where reward and state are associated specifically with the determination of optimal paths, are presented with these additional realistic characteristics to account for nuances in adversarial behavior.","The paths generated are enhanced by including communication payload and protocol into the Markov decision process (MDP) in order to more realistically emulate attributes of network based exfiltration events.","The proposed method will help emulate complex adversarial considerations such as the size of a payload being exported over time or the protocol on which it occurs, as is the case where threat actors steal data over long periods of time using system native ports or protocols to avoid detection.","As such, practitioners will be able to improve identification of expected adversary behavior under various payload and protocol assumptions more comprehensively."],"url":"http://arxiv.org/abs/2310.03667v1"}
{"created":"2023-10-05 16:43:04","title":"MapperGPT: Large Language Models for Linking and Mapping Entities","abstract":"Aligning terminological resources, including ontologies, controlled vocabularies, taxonomies, and value sets is a critical part of data integration in many domains such as healthcare, chemistry, and biomedical research. Entity mapping is the process of determining correspondences between entities across these resources, such as gene identifiers, disease concepts, or chemical entity identifiers. Many tools have been developed to compute such mappings based on common structural features and lexical information such as labels and synonyms. Lexical approaches in particular often provide very high recall, but low precision, due to lexical ambiguity. As a consequence of this, mapping efforts often resort to a labor intensive manual mapping refinement through a human curator.   Large Language Models (LLMs), such as the ones employed by ChatGPT, have generalizable abilities to perform a wide range of tasks, including question-answering and information extraction. Here we present MapperGPT, an approach that uses LLMs to review and refine mapping relationships as a post-processing step, in concert with existing high-recall methods that are based on lexical and structural heuristics.   We evaluated MapperGPT on a series of alignment tasks from different domains, including anatomy, developmental biology, and renal diseases. We devised a collection of tasks that are designed to be particularly challenging for lexical methods. We show that when used in combination with high-recall methods, MapperGPT can provide a substantial improvement in accuracy, beating state-of-the-art (SOTA) methods such as LogMap.","sentences":["Aligning terminological resources, including ontologies, controlled vocabularies, taxonomies, and value sets is a critical part of data integration in many domains such as healthcare, chemistry, and biomedical research.","Entity mapping is the process of determining correspondences between entities across these resources, such as gene identifiers, disease concepts, or chemical entity identifiers.","Many tools have been developed to compute such mappings based on common structural features and lexical information such as labels and synonyms.","Lexical approaches in particular often provide very high recall, but low precision, due to lexical ambiguity.","As a consequence of this, mapping efforts often resort to a labor intensive manual mapping refinement through a human curator.   ","Large Language Models (LLMs), such as the ones employed by ChatGPT, have generalizable abilities to perform a wide range of tasks, including question-answering and information extraction.","Here we present MapperGPT, an approach that uses LLMs to review and refine mapping relationships as a post-processing step, in concert with existing high-recall methods that are based on lexical and structural heuristics.   ","We evaluated MapperGPT on a series of alignment tasks from different domains, including anatomy, developmental biology, and renal diseases.","We devised a collection of tasks that are designed to be particularly challenging for lexical methods.","We show that when used in combination with high-recall methods, MapperGPT can provide a substantial improvement in accuracy, beating state-of-the-art (SOTA) methods such as LogMap."],"url":"http://arxiv.org/abs/2310.03666v1"}
{"created":"2023-10-05 16:40:38","title":"POLYLLA: Polygonal/Polyhedral meshing algorithm based on terminal-edge regions and terminal-face regions","abstract":"Polylla is a polygonal mesh algorithm that generates meshes with arbitrarily shaped polygons using the concept of terminal-edge regions. Until now, Polylla has been limited to 2D meshes, but in this work, we extend Polylla to 3D volumetric meshes. We present two versions of Polylla 3D. The first version generates terminal-edge regions, converts them into polyhedra, and repairs polyhedra that are joined by only an edge. This version differs from the original Polylla algorithm in that it does not have the same phases as the 2D version. In the second version, we define two new concepts: longest-face propagation path and terminal-face regions. We use these concepts to create an almost direct extension of the 2D Polylla mesh with the same three phases: label phase, traversal phase, and repair phase.","sentences":["Polylla is a polygonal mesh algorithm that generates meshes with arbitrarily shaped polygons using the concept of terminal-edge regions.","Until now, Polylla has been limited to 2D meshes, but in this work, we extend Polylla to 3D volumetric meshes.","We present two versions of Polylla 3D.","The first version generates terminal-edge regions, converts them into polyhedra, and repairs polyhedra that are joined by only an edge.","This version differs from the original Polylla algorithm in that it does not have the same phases as the 2D version.","In the second version, we define two new concepts: longest-face propagation path and terminal-face regions.","We use these concepts to create an almost direct extension of the 2D Polylla mesh with the same three phases: label phase, traversal phase, and repair phase."],"url":"http://arxiv.org/abs/2310.03665v1"}
{"created":"2023-10-05 16:39:14","title":"Robustness-Guided Image Synthesis for Data-Free Quantization","abstract":"Quantization has emerged as a promising direction for model compression. Recently, data-free quantization has been widely studied as a promising method to avoid privacy concerns, which synthesizes images as an alternative to real training data. Existing methods use classification loss to ensure the reliability of the synthesized images. Unfortunately, even if these images are well-classified by the pre-trained model, they still suffer from low semantics and homogenization issues. Intuitively, these low-semantic images are sensitive to perturbations, and the pre-trained model tends to have inconsistent output when the generator synthesizes an image with poor semantics. To this end, we propose Robustness-Guided Image Synthesis (RIS), a simple but effective method to enrich the semantics of synthetic images and improve image diversity, further boosting the performance of downstream data-free compression tasks. Concretely, we first introduce perturbations on input and model weight, then define the inconsistency metrics at feature and prediction levels before and after perturbations. On the basis of inconsistency on two levels, we design a robustness optimization objective to enhance the semantics of synthetic images. Moreover, we also make our approach diversity-aware by forcing the generator to synthesize images with small correlations in the label space. With RIS, we achieve state-of-the-art performance for various settings on data-free quantization and can be extended to other data-free compression tasks.","sentences":["Quantization has emerged as a promising direction for model compression.","Recently, data-free quantization has been widely studied as a promising method to avoid privacy concerns, which synthesizes images as an alternative to real training data.","Existing methods use classification loss to ensure the reliability of the synthesized images.","Unfortunately, even if these images are well-classified by the pre-trained model, they still suffer from low semantics and homogenization issues.","Intuitively, these low-semantic images are sensitive to perturbations, and the pre-trained model tends to have inconsistent output when the generator synthesizes an image with poor semantics.","To this end, we propose Robustness-Guided Image Synthesis (RIS), a simple but effective method to enrich the semantics of synthetic images and improve image diversity, further boosting the performance of downstream data-free compression tasks.","Concretely, we first introduce perturbations on input and model weight, then define the inconsistency metrics at feature and prediction levels before and after perturbations.","On the basis of inconsistency on two levels, we design a robustness optimization objective to enhance the semantics of synthetic images.","Moreover, we also make our approach diversity-aware by forcing the generator to synthesize images with small correlations in the label space.","With RIS, we achieve state-of-the-art performance for various settings on data-free quantization and can be extended to other data-free compression tasks."],"url":"http://arxiv.org/abs/2310.03661v1"}
{"created":"2023-10-05 16:37:29","title":"Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures","abstract":"Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities. However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations. Autonomous LLM-powered multi-agent systems represent a strategic response to these challenges. Such systems strive for autonomously tackling user-prompted goals by decomposing them into manageable tasks and orchestrating their execution and result synthesis through a collective of specialized intelligent agents. Equipped with LLM-powered reasoning capabilities, these agents harness the cognitive synergy of collaborating with their peers, enhanced by leveraging contextual resources such as tools and datasets. While these architectures hold promising potential in amplifying AI capabilities, striking the right balance between different levels of autonomy and alignment remains the crucial challenge for their effective operation. This paper proposes a comprehensive multi-dimensional taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems balance the dynamic interplay between autonomy and alignment across various aspects inherent to architectural viewpoints such as goal-driven task management, agent composition, multi-agent collaboration, and context interaction. It also includes a domain-ontology model specifying fundamental architectural concepts. Our taxonomy aims to empower researchers, engineers, and AI practitioners to systematically analyze the architectural dynamics and balancing strategies employed by these increasingly prevalent AI systems. The exploratory taxonomic classification of selected representative LLM-powered multi-agent systems illustrates its practical utility and reveals potential for future research and development.","sentences":["Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities.","However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations.","Autonomous LLM-powered multi-agent systems represent a strategic response to these challenges.","Such systems strive for autonomously tackling user-prompted goals by decomposing them into manageable tasks and orchestrating their execution and result synthesis through a collective of specialized intelligent agents.","Equipped with LLM-powered reasoning capabilities, these agents harness the cognitive synergy of collaborating with their peers, enhanced by leveraging contextual resources such as tools and datasets.","While these architectures hold promising potential in amplifying AI capabilities, striking the right balance between different levels of autonomy and alignment remains the crucial challenge for their effective operation.","This paper proposes a comprehensive multi-dimensional taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems balance the dynamic interplay between autonomy and alignment across various aspects inherent to architectural viewpoints such as goal-driven task management, agent composition, multi-agent collaboration, and context interaction.","It also includes a domain-ontology model specifying fundamental architectural concepts.","Our taxonomy aims to empower researchers, engineers, and AI practitioners to systematically analyze the architectural dynamics and balancing strategies employed by these increasingly prevalent AI systems.","The exploratory taxonomic classification of selected representative LLM-powered multi-agent systems illustrates its practical utility and reveals potential for future research and development."],"url":"http://arxiv.org/abs/2310.03659v1"}
{"created":"2023-10-05 16:35:27","title":"Visual inspection for illicit items in X-ray images using Deep Learning","abstract":"Automated detection of contraband items in X-ray images can significantly increase public safety, by enhancing the productivity and alleviating the mental load of security officers in airports, subways, customs/post offices, etc. The large volume and high throughput of passengers, mailed parcels, etc., during rush hours practically make it a Big Data problem. Modern computer vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of undertaking this task even under resource-constrained and embedded execution scenarios, e.g., as is the case with fast, single-stage object detectors. However, no comparative experimental assessment of the various relevant DNN components/methods has been performed under a common evaluation protocol, which means that reliable cross-method comparisons are missing. This paper presents exactly such a comparative assessment, utilizing a public relevant dataset and a well-defined methodology for selecting the specific DNN components/modules that are being evaluated. The results indicate the superiority of Transformer detectors, the obsolete nature of auxiliary neural modules that have been developed in the past few years for security applications and the efficiency of the CSP-DarkNet backbone CNN.","sentences":["Automated detection of contraband items in X-ray images can significantly increase public safety, by enhancing the productivity and alleviating the mental load of security officers in airports, subways, customs/post offices, etc.","The large volume and high throughput of passengers, mailed parcels, etc., during rush hours practically make it a Big Data problem.","Modern computer vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of undertaking this task even under resource-constrained and embedded execution scenarios, e.g., as is the case with fast, single-stage object detectors.","However, no comparative experimental assessment of the various relevant DNN components/methods has been performed under a common evaluation protocol, which means that reliable cross-method comparisons are missing.","This paper presents exactly such a comparative assessment, utilizing a public relevant dataset and a well-defined methodology for selecting the specific DNN components/modules that are being evaluated.","The results indicate the superiority of Transformer detectors, the obsolete nature of auxiliary neural modules that have been developed in the past few years for security applications and the efficiency of the CSP-DarkNet backbone CNN."],"url":"http://arxiv.org/abs/2310.03658v1"}
{"created":"2023-10-05 16:33:08","title":"Strategic Evaluation: Subjects, Evaluators, and Society","abstract":"A broad current application of algorithms is in formal and quantitative measures of murky concepts -- like merit -- to make decisions. When people strategically respond to these sorts of evaluations in order to gain favorable decision outcomes, their behavior can be subjected to moral judgments. They may be described as 'gaming the system' or 'cheating,' or (in other cases) investing 'honest effort' or 'improving.' Machine learning literature on strategic behavior has tried to describe these dynamics by emphasizing the efforts expended by decision subjects hoping to obtain a more favorable assessment -- some works offer ways to preempt or prevent such manipulations, some differentiate 'gaming' from 'improvement' behavior, while others aim to measure the effort burden or disparate effects of classification systems. We begin from a different starting point: that the design of an evaluation itself can be understood as furthering goals held by the evaluator which may be misaligned with broader societal goals. To develop the idea that evaluation represents a strategic interaction in which both the evaluator and the subject of their evaluation are operating out of self-interest, we put forward a model that represents the process of evaluation using three interacting agents: a decision subject, an evaluator, and society, representing a bundle of values and oversight mechanisms. We highlight our model's applicability to a number of social systems where one or two players strategically undermine the others' interests to advance their own. Treating evaluators as themselves strategic allows us to re-cast the scrutiny directed at decision subjects, towards the incentives that underpin institutional designs of evaluations. The moral standing of strategic behaviors often depend on the moral standing of the evaluations and incentives that provoke such behaviors.","sentences":["A broad current application of algorithms is in formal and quantitative measures of murky concepts -- like merit -- to make decisions.","When people strategically respond to these sorts of evaluations in order to gain favorable decision outcomes, their behavior can be subjected to moral judgments.","They may be described as 'gaming the system' or 'cheating,' or (in other cases) investing 'honest effort' or 'improving.'","Machine learning literature on strategic behavior has tried to describe these dynamics by emphasizing the efforts expended by decision subjects hoping to obtain a more favorable assessment -- some works offer ways to preempt or prevent such manipulations, some differentiate 'gaming' from 'improvement' behavior, while others aim to measure the effort burden or disparate effects of classification systems.","We begin from a different starting point: that the design of an evaluation itself can be understood as furthering goals held by the evaluator which may be misaligned with broader societal goals.","To develop the idea that evaluation represents a strategic interaction in which both the evaluator and the subject of their evaluation are operating out of self-interest, we put forward a model that represents the process of evaluation using three interacting agents: a decision subject, an evaluator, and society, representing a bundle of values and oversight mechanisms.","We highlight our model's applicability to a number of social systems where one or two players strategically undermine the others' interests to advance their own.","Treating evaluators as themselves strategic allows us to re-cast the scrutiny directed at decision subjects, towards the incentives that underpin institutional designs of evaluations.","The moral standing of strategic behaviors often depend on the moral standing of the evaluations and incentives that provoke such behaviors."],"url":"http://arxiv.org/abs/2310.03655v1"}
{"created":"2023-10-05 16:28:58","title":"Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics","abstract":"Data-driven constitutive modeling with neural networks has received increased interest in recent years due to its ability to easily incorporate physical and mechanistic constraints and to overcome the challenging and time-consuming task of formulating phenomenological constitutive laws that can accurately capture the observed material response. However, even though neural network-based constitutive laws have been shown to generalize proficiently, the generated representations are not easily interpretable due to their high number of trainable parameters. Sparse regression approaches exist that allow to obtaining interpretable expressions, but the user is tasked with creating a library of model forms which by construction limits their expressiveness to the functional forms provided in the libraries. In this work, we propose to train regularized physics-augmented neural network-based constitutive models utilizing a smoothed version of $L^{0}$-regularization. This aims to maintain the trustworthiness inherited by the physical constraints, but also enables interpretability which has not been possible thus far on any type of machine learning-based constitutive model where model forms were not assumed a-priory but were actually discovered. During the training process, the network simultaneously fits the training data and penalizes the number of active parameters, while also ensuring constitutive constraints such as thermodynamic consistency. We show that the method can reliably obtain interpretable and trustworthy constitutive models for compressible and incompressible hyperelasticity, yield functions, and hardening models for elastoplasticity, for synthetic and experimental data.","sentences":["Data-driven constitutive modeling with neural networks has received increased interest in recent years due to its ability to easily incorporate physical and mechanistic constraints and to overcome the challenging and time-consuming task of formulating phenomenological constitutive laws that can accurately capture the observed material response.","However, even though neural network-based constitutive laws have been shown to generalize proficiently, the generated representations are not easily interpretable due to their high number of trainable parameters.","Sparse regression approaches exist that allow to obtaining interpretable expressions, but the user is tasked with creating a library of model forms which by construction limits their expressiveness to the functional forms provided in the libraries.","In this work, we propose to train regularized physics-augmented neural network-based constitutive models utilizing a smoothed version of $L^{0}$-regularization.","This aims to maintain the trustworthiness inherited by the physical constraints, but also enables interpretability which has not been possible thus far on any type of machine learning-based constitutive model where model forms were not assumed a-priory but were actually discovered.","During the training process, the network simultaneously fits the training data and penalizes the number of active parameters, while also ensuring constitutive constraints such as thermodynamic consistency.","We show that the method can reliably obtain interpretable and trustworthy constitutive models for compressible and incompressible hyperelasticity, yield functions, and hardening models for elastoplasticity, for synthetic and experimental data."],"url":"http://arxiv.org/abs/2310.03652v1"}
{"created":"2023-10-05 16:21:42","title":"Rethinking Fairness for Human-AI Collaboration","abstract":"Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions. However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration. Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy. As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern. We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern. We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy. However, we show that it may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy; thus, if our goal is to improve the equity and accuracy of human-AI collaboration, it may not be desirable to enforce traditional fairness constraints.","sentences":["Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions.","However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration.","Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy.","As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern.","We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern.","We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy.","However, we show that it may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy; thus, if our goal is to improve the equity and accuracy of human-AI collaboration, it may not be desirable to enforce traditional fairness constraints."],"url":"http://arxiv.org/abs/2310.03647v1"}
{"created":"2023-10-05 16:21:36","title":"TRAM: Bridging Trust Regions and Sharpness Aware Minimization","abstract":"By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based optimization methods on cross-domain language modeling and cross-lingual transfer, where robustness to domain transfer and representation generality are critical for success. TRAM establishes a new standard in training generalizable models with minimal additional computation.","sentences":["By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer.","Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup.","To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training.","We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces.","We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure.","We find that TRAM outperforms both sharpness-aware and trust region-based optimization methods on cross-domain language modeling and cross-lingual transfer, where robustness to domain transfer and representation generality are critical for success.","TRAM establishes a new standard in training generalizable models with minimal additional computation."],"url":"http://arxiv.org/abs/2310.03646v1"}
{"created":"2023-10-05 16:13:29","title":"Distributional PAC-Learning from Nisan's Natural Proofs","abstract":"(Abridged) Carmosino et al. (2016) demonstrated that natural proofs of circuit lower bounds for \\Lambda imply efficient algorithms for learning \\Lambda-circuits, but only over the uniform distribution, with membership queries, and provided \\AC^0[p] \\subseteq \\Lambda. We consider whether this implication can be generalized to \\Lambda \\not\\supseteq \\AC^0[p], and to learning algorithms in Valiant's PAC model, which use only random examples and learn over arbitrary example distributions. We give results of both positive and negative flavor.   On the negative side, we observe that if, for every circuit class \\Lambda, the implication from natural proofs for \\Lambda to learning \\Lambda-circuits in Valiant's PAC model holds, then there is a polynomial time solution to O(n^{1.5})-uSVP (unique Shortest Vector Problem), and polynomial time quantum solutions to O(n^{1.5})-SVP (Shortest Vector Problem) and O(n^{1.5})-SIVP (Shortest Independent Vector Problem). This indicates that whether natural proofs for \\Lambda imply efficient learning algorithms for \\Lambda in Valiant's PAC model may depend on \\Lambda.   On the positive side, our main result is that specific natural proofs arising from a type of communication complexity argument (e.g., Nisan (1993), for depth-2 majority circuits) imply PAC-learning algorithms in a new distributional variant of Valiant's model. Our distributional PAC model is stronger than the average-case prediction model of Blum et al (1993) and the heuristic PAC model of Nanashima (2021), and has several important properties which make it of independent interest, such as being boosting-friendly. The main applications of our result are new distributional PAC-learning algorithms for depth-2 majority circuits, polytopes and DNFs over natural target distributions, as well as the nonexistence of encoded-input weak PRFs that can be evaluated by depth-2 majority circuits.","sentences":["(Abridged) Carmosino et al.","(2016) demonstrated that natural proofs of circuit lower bounds for \\Lambda imply efficient algorithms for learning \\Lambda-circuits, but only over the uniform distribution, with membership queries, and provided \\AC^0[p]","\\subseteq \\Lambda.","We consider whether this implication can be generalized to \\Lambda \\not\\supseteq \\AC^0[p], and to learning algorithms in Valiant's PAC model, which use only random examples and learn over arbitrary example distributions.","We give results of both positive and negative flavor.   ","On the negative side, we observe that if, for every circuit class \\Lambda, the implication from natural proofs for \\Lambda to learning \\Lambda-circuits in Valiant's PAC model holds, then there is a polynomial time solution to O(n^{1.5})-uSVP (unique Shortest Vector Problem), and polynomial time quantum solutions to O(n^{1.5})-SVP (Shortest Vector Problem) and O(n^{1.5})-SIVP (Shortest Independent Vector Problem).","This indicates that whether natural proofs for \\Lambda imply efficient learning algorithms for \\Lambda in Valiant's PAC model may depend on \\Lambda.   ","On the positive side, our main result is that specific natural proofs arising from a type of communication complexity argument (e.g., Nisan (1993), for depth-2 majority circuits) imply PAC-learning algorithms in a new distributional variant of Valiant's model.","Our distributional PAC model is stronger than the average-case prediction model of Blum et al (1993) and the heuristic PAC model of Nanashima (2021), and has several important properties which make it of independent interest, such as being boosting-friendly.","The main applications of our result are new distributional PAC-learning algorithms for depth-2 majority circuits, polytopes and DNFs over natural target distributions, as well as the nonexistence of encoded-input weak PRFs that can be evaluated by depth-2 majority circuits."],"url":"http://arxiv.org/abs/2310.03641v1"}
{"created":"2023-10-05 16:11:14","title":"Evaluating Self-Supervised Speech Representations for Indigenous American Languages","abstract":"The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.","sentences":["The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data.","However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English.","Few models consider other languages, and even fewer consider indigenous ones.","In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language.","We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR.","Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data."],"url":"http://arxiv.org/abs/2310.03639v1"}
{"created":"2023-10-05 16:10:14","title":"Solving Degree Bounds For Iterated Polynomial Systems","abstract":"For Arithmetization-Oriented ciphers and hash functions Gr\\\"obner basis attacks are generally considered as the most competitive attack vector. Unfortunately, the complexity of Gr\\\"obner basis algorithms is only understood for special cases, and it is needless to say that these cases do not apply to most cryptographic polynomial systems. Therefore, cryptographers have to resort to experiments, extrapolations and hypotheses to assess the security of their designs. One established measure to quantify the complexity of linear algebra-based Gr\\\"obner basis algorithms is the so-called solving degree. Caminata \\& Gorla revealed that under a certain genericity condition on a polynomial system the solving degree is always upper bounded by the Castelnuovo-Mumford regularity and henceforth by the Macaulay bound, which only takes the degrees and number of variables of the input polynomials into account. In this paper we extend their framework to iterated polynomial systems, the standard polynomial model for symmetric ciphers and hash functions. In particular, we prove solving degree bounds for various attacks on MiMC, Feistel-MiMC, Feistel-MiMC-Hash, Hades and GMiMC. Our bounds fall in line with the hypothesized complexity of Gr\\\"obner basis attacks on these designs, and to the best of our knowledge this is the first time that a mathematical proof for these complexities is provided.   Moreover, by studying polynomials with degree falls we can prove lower bounds on the Castelnuovo-Mumford regularity for attacks on MiMC, Feistel-MiMC and Feistel-MiMC-Hash provided that only a few solutions of the corresponding iterated polynomial system originate from the base field. Hence, regularity-based solving degree estimations can never surpass a certain threshold, a desirable property for cryptographic polynomial systems.","sentences":["For Arithmetization-Oriented ciphers and hash functions Gr\\\"obner basis attacks are generally considered as the most competitive attack vector.","Unfortunately, the complexity of Gr\\\"obner basis algorithms is only understood for special cases, and it is needless to say that these cases do not apply to most cryptographic polynomial systems.","Therefore, cryptographers have to resort to experiments, extrapolations and hypotheses to assess the security of their designs.","One established measure to quantify the complexity of linear algebra-based Gr\\\"obner basis algorithms is the so-called solving degree.","Caminata \\& Gorla revealed that under a certain genericity condition on a polynomial system the solving degree is always upper bounded by the Castelnuovo-Mumford regularity and henceforth by the Macaulay bound, which only takes the degrees and number of variables of the input polynomials into account.","In this paper we extend their framework to iterated polynomial systems, the standard polynomial model for symmetric ciphers and hash functions.","In particular, we prove solving degree bounds for various attacks on MiMC, Feistel-MiMC, Feistel-MiMC-Hash, Hades and GMiMC.","Our bounds fall in line with the hypothesized complexity of Gr\\\"obner basis attacks on these designs, and to the best of our knowledge this is the first time that a mathematical proof for these complexities is provided.   ","Moreover, by studying polynomials with degree falls we can prove lower bounds on the Castelnuovo-Mumford regularity for attacks on MiMC, Feistel-MiMC and Feistel-MiMC-Hash provided that only a few solutions of the corresponding iterated polynomial system originate from the base field.","Hence, regularity-based solving degree estimations can never surpass a certain threshold, a desirable property for cryptographic polynomial systems."],"url":"http://arxiv.org/abs/2310.03637v1"}
{"created":"2023-10-05 16:09:48","title":"CLEVRER-Humans: Describing Physical and Causal Events the Human Way","abstract":"Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting the great challenges set forth by our benchmark.","sentences":["Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world.","However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships.","This design brings up two issues.","First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments.","To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels.","We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models.","We convert the collected CEGs into questions and answers to be consistent with prior work.","Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting the great challenges set forth by our benchmark."],"url":"http://arxiv.org/abs/2310.03635v1"}
{"created":"2023-10-05 16:08:45","title":"When a random tape is not enough: lower bounds for a problem in adversarially robust streaming","abstract":"Adversarially robust streaming algorithms are required to process a stream of elements and produce correct outputs, even when each stream element can be chosen depending on earlier algorithm outputs. As with classic streaming algorithms, which must only be correct for the worst-case fixed stream, adversarially robust algorithms with access to randomness can use significantly less space than deterministic algorithms. We prove that for the Missing Item Finding problem in streaming, the space complexity also significantly depends on how adversarially robust algorithms are permitted to use randomness. (In contrast, the space complexity of classic streaming algorithms does not depend as strongly on the way randomness is used.)   For Missing Item Finding on streams of length $r$ with elements in $\\{1,...n\\}$, and $\\le 1/\\text{poly}(n)$ error, we show that when $r = O(2^{\\sqrt{\\log n}})$, \"random seed\" adversarially robust algorithms, which only use randomness at initialization, require $r^{\\Omega(1)}$ bits of space, while \"random tape\" adversarially robust algorithms, which may make random decisions at any time, may use $O(\\text{polylog}(r))$ random bits. When $r = \\Theta(\\sqrt{n})$, \"random tape\" adversarially robust algorithms need $r^{\\Omega(1)}$ space, while \"random oracle\" adversarially robust algorithms, which can read from a long random string for free, may use $O(\\text{polylog}(r))$ space. The space lower bound for the \"random seed\" case follows, by a reduction given in prior work, from a lower bound for pseudo-deterministic streaming algorithms given in this paper.","sentences":["Adversarially robust streaming algorithms are required to process a stream of elements and produce correct outputs, even when each stream element can be chosen depending on earlier algorithm outputs.","As with classic streaming algorithms, which must only be correct for the worst-case fixed stream, adversarially robust algorithms with access to randomness can use significantly less space than deterministic algorithms.","We prove that for the Missing Item Finding problem in streaming, the space complexity also significantly depends on how adversarially robust algorithms are permitted to use randomness.","(In contrast, the space complexity of classic streaming algorithms does not depend as strongly on the way randomness is used.)   ","For Missing Item Finding on streams of length $r$ with elements in $\\{1,...n\\}$, and $\\le 1/\\text{poly}(n)$ error, we show that when $r = O(2^{\\sqrt{\\log n}})$, \"random seed\" adversarially robust algorithms, which only use randomness at initialization, require $r^{\\Omega(1)}$ bits of space, while \"random tape\" adversarially robust algorithms, which may make random decisions at any time, may use $O(\\text{polylog}(r))$ random bits.","When $r = \\Theta(\\sqrt{n})$, \"random tape\" adversarially robust algorithms need $r^{\\Omega(1)}$ space, while \"random oracle\" adversarially robust algorithms, which can read from a long random string for free, may use $O(\\text{polylog}(r))$ space.","The space lower bound for the \"random seed\" case follows, by a reduction given in prior work, from a lower bound for pseudo-deterministic streaming algorithms given in this paper."],"url":"http://arxiv.org/abs/2310.03634v1"}
{"created":"2023-10-05 16:03:25","title":"Wasserstein Distortion: Unifying Fidelity and Realism","abstract":"We introduce a distortion measure for images, Wasserstein distortion, that simultaneously generalizes pixel-level fidelity on the one hand and realism on the other. We show how Wasserstein distortion reduces mathematically to a pure fidelity constraint or a pure realism constraint under different parameter choices. Pairs of images that are close under Wasserstein distortion illustrate its utility. In particular, we generate random textures that have high fidelity to a reference texture in one location of the image and smoothly transition to an independent realization of the texture as one moves away from this point. Connections between Wasserstein distortion and models of the human visual system are noted.","sentences":["We introduce a distortion measure for images, Wasserstein distortion, that simultaneously generalizes pixel-level fidelity on the one hand and realism on the other.","We show how Wasserstein distortion reduces mathematically to a pure fidelity constraint or a pure realism constraint under different parameter choices.","Pairs of images that are close under Wasserstein distortion illustrate its utility.","In particular, we generate random textures that have high fidelity to a reference texture in one location of the image and smoothly transition to an independent realization of the texture as one moves away from this point.","Connections between Wasserstein distortion and models of the human visual system are noted."],"url":"http://arxiv.org/abs/2310.03629v1"}
{"created":"2023-10-05 16:03:07","title":"Belief Expansion in Subset Models","abstract":"Subset models provide a new semantics for justifcation logic. The main idea of subset models is that evidence terms are interpreted as sets of possible worlds. A term then justifies a formula if that formula is true in each world of the interpretation of the term. In this paper, we introduce a belief expansion operator for subset models. We study the main properties of the resulting logic as well as the differences to a previous (symbolic) approach to belief expansion in justification logic.","sentences":["Subset models provide a new semantics for justifcation logic.","The main idea of subset models is that evidence terms are interpreted as sets of possible worlds.","A term then justifies a formula if that formula is true in each world of the interpretation of the term.","In this paper, we introduce a belief expansion operator for subset models.","We study the main properties of the resulting logic as well as the differences to a previous (symbolic) approach to belief expansion in justification logic."],"url":"http://arxiv.org/abs/2310.03627v1"}
{"created":"2023-10-05 16:01:29","title":"High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning","abstract":"A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of the robot's workspace dimension. We demonstrate the capabilities of this model on a motion planning task as an exemplary downstream application.","sentences":["A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models.","In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents.","In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations.","This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge.","To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs).","In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of the robot's workspace dimension.","We demonstrate the capabilities of this model on a motion planning task as an exemplary downstream application."],"url":"http://arxiv.org/abs/2310.03624v1"}
{"created":"2023-10-05 15:58:45","title":"PeaTMOSS: Mining Pre-Trained Models in Open-Source Software","abstract":"Developing and training deep learning models is expensive, so software engineers have begun to reuse pre-trained deep learning models (PTMs) and fine-tune them for downstream tasks. Despite the wide-spread use of PTMs, we know little about the corresponding software engineering behaviors and challenges.   To enable the study of software engineering with PTMs, we present the PeaTMOSS dataset: Pre-Trained Models in Open-Source Software. PeaTMOSS has three parts: a snapshot of (1) 281,638 PTMs, (2) 27,270 open-source software repositories that use PTMs, and (3) a mapping between PTMs and the projects that use them. We challenge PeaTMOSS miners to discover software engineering practices around PTMs. A demo and link to the full dataset are available at: https://github.com/PurdueDualityLab/PeaTMOSS-Demos.","sentences":["Developing and training deep learning models is expensive, so software engineers have begun to reuse pre-trained deep learning models (PTMs) and fine-tune them for downstream tasks.","Despite the wide-spread use of PTMs, we know little about the corresponding software engineering behaviors and challenges.   ","To enable the study of software engineering with PTMs, we present the PeaTMOSS dataset: Pre-Trained Models in Open-Source Software.","PeaTMOSS has three parts: a snapshot of (1) 281,638 PTMs, (2) 27,270 open-source software repositories that use PTMs, and (3) a mapping between PTMs and the projects that use them.","We challenge PeaTMOSS miners to discover software engineering practices around PTMs.","A demo and link to the full dataset are available at: https://github.com/PurdueDualityLab/PeaTMOSS-Demos."],"url":"http://arxiv.org/abs/2310.03620v1"}
{"created":"2023-10-05 15:51:36","title":"CLASSify: A Web-Based Tool for Machine Learning","abstract":"Machine learning classification problems are widespread in bioinformatics, but the technical knowledge required to perform model training, optimization, and inference can prevent researchers from utilizing this technology. This article presents an automated tool for machine learning classification problems to simplify the process of training models and producing results while providing informative visualizations and insights into the data. This tool supports both binary and multiclass classification problems, and it provides access to a variety of models and methods. Synthetic data can be generated within the interface to fill missing values, balance class labels, or generate entirely new datasets. It also provides support for feature evaluation and generates explainability scores to indicate which features influence the output the most. We present CLASSify, an open-source tool for simplifying the user experience of solving classification problems without the need for knowledge of machine learning.","sentences":["Machine learning classification problems are widespread in bioinformatics, but the technical knowledge required to perform model training, optimization, and inference can prevent researchers from utilizing this technology.","This article presents an automated tool for machine learning classification problems to simplify the process of training models and producing results while providing informative visualizations and insights into the data.","This tool supports both binary and multiclass classification problems, and it provides access to a variety of models and methods.","Synthetic data can be generated within the interface to fill missing values, balance class labels, or generate entirely new datasets.","It also provides support for feature evaluation and generates explainability scores to indicate which features influence the output the most.","We present CLASSify, an open-source tool for simplifying the user experience of solving classification problems without the need for knowledge of machine learning."],"url":"http://arxiv.org/abs/2310.03618v1"}
{"created":"2023-10-05 15:49:48","title":"SoK: Decentralized Sequencers for Rollups","abstract":"Rollups have emerged as a promising solution to enhance blockchain scalability, offering increased throughput, reduced latency, and lower transaction fees. However, they currently rely on a centralized sequencer to determine transaction ordering, compromising the decentralization principle of blockchain systems. Recognizing this, there is a clear need for decentralized sequencers in rollups. However, designing such a system is intricate. This paper presents a comprehensive exploration of decentralized sequencers in rollups, formulating their ideal properties, dissecting their core components, and synthesizing community insights. Our findings emphasize the imperative for an adept sequencer design, harmonizing with the overarching goals of the blockchain ecosystem, and setting a trajectory for subsequent research endeavors.","sentences":["Rollups have emerged as a promising solution to enhance blockchain scalability, offering increased throughput, reduced latency, and lower transaction fees.","However, they currently rely on a centralized sequencer to determine transaction ordering, compromising the decentralization principle of blockchain systems.","Recognizing this, there is a clear need for decentralized sequencers in rollups.","However, designing such a system is intricate.","This paper presents a comprehensive exploration of decentralized sequencers in rollups, formulating their ideal properties, dissecting their core components, and synthesizing community insights.","Our findings emphasize the imperative for an adept sequencer design, harmonizing with the overarching goals of the blockchain ecosystem, and setting a trajectory for subsequent research endeavors."],"url":"http://arxiv.org/abs/2310.03616v1"}
{"created":"2023-10-05 15:49:44","title":"Animatable Virtual Humans: Learning pose-dependent human representations in UV space for interactive performance synthesis","abstract":"We propose a novel representation of virtual humans for highly realistic real-time animation and rendering in 3D applications. We learn pose dependent appearance and geometry from highly accurate dynamic mesh sequences obtained from state-of-the-art multiview-video reconstruction. Learning pose-dependent appearance and geometry from mesh sequences poses significant challenges, as it requires the network to learn the intricate shape and articulated motion of a human body. However, statistical body models like SMPL provide valuable a-priori knowledge which we leverage in order to constrain the dimension of the search space enabling more efficient and targeted learning and define pose-dependency. Instead of directly learning absolute pose-dependent geometry, we learn the difference between the observed geometry and the fitted SMPL model. This allows us to encode both pose-dependent appearance and geometry in the consistent UV space of the SMPL model. This approach not only ensures a high level of realism but also facilitates streamlined processing and rendering of virtual humans in real-time scenarios.","sentences":["We propose a novel representation of virtual humans for highly realistic real-time animation and rendering in 3D applications.","We learn pose dependent appearance and geometry from highly accurate dynamic mesh sequences obtained from state-of-the-art multiview-video reconstruction.","Learning pose-dependent appearance and geometry from mesh sequences poses significant challenges, as it requires the network to learn the intricate shape and articulated motion of a human body.","However, statistical body models like SMPL provide valuable a-priori knowledge which we leverage in order to constrain the dimension of the search space enabling more efficient and targeted learning and define pose-dependency.","Instead of directly learning absolute pose-dependent geometry, we learn the difference between the observed geometry and the fitted SMPL model.","This allows us to encode both pose-dependent appearance and geometry in the consistent UV space of the SMPL model.","This approach not only ensures a high level of realism but also facilitates streamlined processing and rendering of virtual humans in real-time scenarios."],"url":"http://arxiv.org/abs/2310.03615v1"}
{"created":"2023-10-05 15:49:04","title":"Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally","abstract":"Deep Neural Networks (DNNs) have been the driving force behind many of the recent advances in machine learning. However, research has shown that DNNs are vulnerable to adversarial examples -- input samples that have been perturbed to force DNN-based models to make errors. As a result, Adversarial Machine Learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social AI applications. The emergence of new AI technologies that leverage Large Language Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing anti-social applications at scale. AdvML for Social Good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent pro-social applications. Regulators, practitioners, and researchers should collaborate to encourage the development of pro-social applications and hinder the development of anti-social ones. In this work, we provide the first comprehensive review of the emerging field of AdvML4G. This paper encompasses a taxonomy that highlights the emergence of AdvML4G, a discussion of the differences and similarities between AdvML4G and AdvML, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the works that utilize AdvML4G as an auxiliary tool for innovating pro-social applications. Finally, we elaborate upon various challenges and open research issues that require significant attention from the research community.","sentences":["Deep Neural Networks (DNNs) have been the driving force behind many of the recent advances in machine learning.","However, research has shown that DNNs are vulnerable to adversarial examples -- input samples that have been perturbed to force DNN-based models to make errors.","As a result, Adversarial Machine Learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities.","In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social AI applications.","The emergence of new AI technologies that leverage Large Language Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing anti-social applications at scale.","AdvML for Social Good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent pro-social applications.","Regulators, practitioners, and researchers should collaborate to encourage the development of pro-social applications and hinder the development of anti-social ones.","In this work, we provide the first comprehensive review of the emerging field of AdvML4G.","This paper encompasses a taxonomy that highlights the emergence of AdvML4G, a discussion of the differences and similarities between AdvML4G and AdvML, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the works that utilize AdvML4G as an auxiliary tool for innovating pro-social applications.","Finally, we elaborate upon various challenges and open research issues that require significant attention from the research community."],"url":"http://arxiv.org/abs/2310.03614v1"}
{"created":"2023-10-05 15:48:41","title":"Solving a Class of Non-Convex Minimax Optimization in Federated Learning","abstract":"The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\\emph{i.e.} single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample complexity of $O(\\kappa^{3} N^{-1}\\varepsilon^{-3})$ and the best-known communication complexity of $O(\\kappa^{2}\\varepsilon^{-2})$. FedSGDA-M is the first algorithm to match the best sample complexity $O(\\varepsilon^{-3})$ achieved by the single-machine method under the nonconvex-strongly-concave setting. Extensive experimental results on fair classification and AUROC maximization show the efficiency of our algorithms.","sentences":["The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization.","To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity.","Many optimization algorithms for minimax problems have been developed in the centralized setting (\\emph{i.e.} single-machine).","Nonetheless, the algorithm for minimax problems under FL is still underexplored.","In this paper, we study a class of federated nonconvex minimax optimization problems.","We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems.","For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample complexity of $O(\\kappa^{3} N^{-1}\\varepsilon^{-3})$ and the best-known communication complexity of $O(\\kappa^{2}\\varepsilon^{-2})$. FedSGDA-M is the first algorithm to match the best sample complexity $O(\\varepsilon^{-3})$ achieved by the single-machine method under the nonconvex-strongly-concave setting.","Extensive experimental results on fair classification and AUROC maximization show the efficiency of our algorithms."],"url":"http://arxiv.org/abs/2310.03613v1"}
{"created":"2023-10-05 15:45:53","title":"GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data","abstract":"Detecting and discovering new gene interactions based on known gene expressions and gene interaction data presents a significant challenge. Various statistical and deep learning methods have attempted to tackle this challenge by leveraging the topological structure of gene interactions and gene expression patterns to predict novel gene interactions. In contrast, some approaches have focused exclusively on utilizing gene expression profiles. In this context, we introduce GENER, a parallel-layer deep learning network designed exclusively for the identification of gene-gene relationships using gene expression data. We conducted two training experiments and compared the performance of our network with that of existing statistical and deep learning approaches. Notably, our model achieved an average AUROC score of 0.834 on the combined BioGRID&DREAM5 dataset, outperforming competing methods in predicting gene-gene interactions.","sentences":["Detecting and discovering new gene interactions based on known gene expressions and gene interaction data presents a significant challenge.","Various statistical and deep learning methods have attempted to tackle this challenge by leveraging the topological structure of gene interactions and gene expression patterns to predict novel gene interactions.","In contrast, some approaches have focused exclusively on utilizing gene expression profiles.","In this context, we introduce GENER, a parallel-layer deep learning network designed exclusively for the identification of gene-gene relationships using gene expression data.","We conducted two training experiments and compared the performance of our network with that of existing statistical and deep learning approaches.","Notably, our model achieved an average AUROC score of 0.834 on the combined BioGRID&DREAM5 dataset, outperforming competing methods in predicting gene-gene interactions."],"url":"http://arxiv.org/abs/2310.03611v1"}
{"created":"2023-10-05 15:36:47","title":"Comparing Time-Series Analysis Approaches Utilized in Research Papers to Forecast COVID-19 Cases in Africa: A Literature Review","abstract":"This literature review aimed to compare various time-series analysis approaches utilized in forecasting COVID-19 cases in Africa. The study involved a methodical search for English-language research papers published between January 2020 and July 2023, focusing specifically on papers that utilized time-series analysis approaches on COVID-19 datasets in Africa. A variety of databases including PubMed, Google Scholar, Scopus, and Web of Science were utilized for this process. The research papers underwent an evaluation process to extract relevant information regarding the implementation and performance of the time-series analysis models. The study highlighted the different methodologies employed, evaluating their effectiveness and limitations in forecasting the spread of the virus. The result of this review could contribute deeper insights into the field, and future research should consider these insights to improve time series analysis models and explore the integration of different approaches for enhanced public health decision-making.","sentences":["This literature review aimed to compare various time-series analysis approaches utilized in forecasting COVID-19 cases in Africa.","The study involved a methodical search for English-language research papers published between January 2020 and July 2023, focusing specifically on papers that utilized time-series analysis approaches on COVID-19 datasets in Africa.","A variety of databases including PubMed, Google Scholar, Scopus, and Web of Science were utilized for this process.","The research papers underwent an evaluation process to extract relevant information regarding the implementation and performance of the time-series analysis models.","The study highlighted the different methodologies employed, evaluating their effectiveness and limitations in forecasting the spread of the virus.","The result of this review could contribute deeper insights into the field, and future research should consider these insights to improve time series analysis models and explore the integration of different approaches for enhanced public health decision-making."],"url":"http://arxiv.org/abs/2310.03606v1"}
{"created":"2023-10-05 15:36:35","title":"FASER: Binary Code Similarity Search through the use of Intermediate Representations","abstract":"Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transformers with the use of intermediate representations to create a model capable of cross architecture function search without the need for manual feature engineering, pre-training or a dynamic analysis step. We compare our approach against a series of baseline approaches for two tasks; A general function search task and a targeted vulnerability search task. Our approach demonstrates strong performance across both tasks, performing better than all baseline approaches.","sentences":["Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research.","Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals.","The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach.","One data source which has received less attention is binary intermediate representations.","Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage.","Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transformers with the use of intermediate representations to create a model capable of cross architecture function search without the need for manual feature engineering, pre-training or a dynamic analysis step.","We compare our approach against a series of baseline approaches for two tasks; A general function search task and a targeted vulnerability search task.","Our approach demonstrates strong performance across both tasks, performing better than all baseline approaches."],"url":"http://arxiv.org/abs/2310.03605v1"}
{"created":"2023-10-05 15:29:52","title":"Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints","abstract":"Text-driven 3D indoor scene generation could be useful for gaming, film industry, and AR/VR applications. However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room. To address these problems, we present Ctrl-Room, which is able to generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items. Our key insight is to separate the modeling of layouts and appearance. %how to model the room that takes into account both scene texture and geometry at the same time. To this end, Our proposed method consists of two stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The `Layout Generation Stage' trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. Next, the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt. In this way, we achieve a high-quality 3D room with convincing layouts and lively textures. Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, without expensive editing-specific training. Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, view-consistent, and editable 3D rooms from natural language prompts.","sentences":["Text-driven 3D indoor scene generation could be useful for gaming, film industry, and AR/VR applications.","However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room.","To address these problems, we present Ctrl-Room, which is able to generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt.","Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items.","Our key insight is to separate the modeling of layouts and appearance.","%how to model the room that takes into account both scene texture and geometry at the same time.","To this end, Our proposed method consists of two stages, a `Layout Generation Stage' and an `Appearance Generation Stage'.","The `Layout Generation Stage' trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization.","Next, the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt.","In this way, we achieve a high-quality 3D room with convincing layouts and lively textures.","Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, without expensive editing-specific training.","Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, view-consistent, and editable 3D rooms from natural language prompts."],"url":"http://arxiv.org/abs/2310.03602v1"}
{"created":"2023-10-05 15:21:10","title":"Divide, Conquer and Verify: Improving Symbolic Execution Performance","abstract":"Symbolic Execution is a formal method that can be used to verify the behavior of computer programs and detect software vulnerabilities. Compared to other testing methods such as fuzzing, Symbolic Execution has the advantage of providing formal guarantees about the program. However, despite advances in performance in recent years, Symbolic Execution is too slow to be applied to real-world software. This is primarily caused by the \\emph{path explosion problem} as well as by the computational complexity of SMT solving. In this paper, we present a divide-and-conquer approach for symbolic execution by executing individual slices and later combining the side effects. This way, the overall problem size is kept small, reducing the impact of computational complexity on large problems.","sentences":["Symbolic Execution is a formal method that can be used to verify the behavior of computer programs and detect software vulnerabilities.","Compared to other testing methods such as fuzzing, Symbolic Execution has the advantage of providing formal guarantees about the program.","However, despite advances in performance in recent years, Symbolic Execution is too slow to be applied to real-world software.","This is primarily caused by the \\emph{path explosion problem} as well as by the computational complexity of SMT solving.","In this paper, we present a divide-and-conquer approach for symbolic execution by executing individual slices and later combining the side effects.","This way, the overall problem size is kept small, reducing the impact of computational complexity on large problems."],"url":"http://arxiv.org/abs/2310.03598v1"}
{"created":"2023-10-05 15:14:00","title":"TimeGPT-1","abstract":"In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.","sentences":["In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training.","We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity.","Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis.","We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning."],"url":"http://arxiv.org/abs/2310.03589v1"}
{"created":"2023-10-05 15:11:10","title":"A Suspended Aerial Manipulation Avatar for Physical Interaction in Unstructured Environments","abstract":"This paper presents a floating robot capable of performing physically interactive tasks in unstructured environments with human-like dexterity under human supervision. The robot consists of a humanoid torso attached to a hexacopter. A two-degree-of-freedom head and two five-degree-of-freedom arms equipped with softhands provide the requisite dexterity to allow human operators to carry out various tasks. A robust tendon-driven structure is purposefully designed for the arms, considerably reducing the impact of arm inertia on the floating base in motion. In addition, tendons provide flexibility to the joints, which enhances the robustness of the arm preventing damage in interaction with the environment. To increase the payload of the aerial system and the battery life, we use the concept of Suspended Aerial Manipulation, i.e., the flying humanoid can be connected with a tether to a structure, e.g., a larger airborne carrier or a supporting crane. Importantly, to maximize portability and applicability, we adopt a modular approach exploiting commercial components for the drone hardware and autopilot, while developing a whole-body outer control loop to stabilize the robot attitude, compensating for the tether force and for the humanoid head and arm motions. The humanoid can be controlled by a remote operator, thus effectively realizing a Suspended Aerial Manipulation Avatar. The proposed system is validated through experiments in indoor scenarios reproducing post-disaster tasks.","sentences":["This paper presents a floating robot capable of performing physically interactive tasks in unstructured environments with human-like dexterity under human supervision.","The robot consists of a humanoid torso attached to a hexacopter.","A two-degree-of-freedom head and two five-degree-of-freedom arms equipped with softhands provide the requisite dexterity to allow human operators to carry out various tasks.","A robust tendon-driven structure is purposefully designed for the arms, considerably reducing the impact of arm inertia on the floating base in motion.","In addition, tendons provide flexibility to the joints, which enhances the robustness of the arm preventing damage in interaction with the environment.","To increase the payload of the aerial system and the battery life, we use the concept of Suspended Aerial Manipulation, i.e., the flying humanoid can be connected with a tether to a structure, e.g., a larger airborne carrier or a supporting crane.","Importantly, to maximize portability and applicability, we adopt a modular approach exploiting commercial components for the drone hardware and autopilot, while developing a whole-body outer control loop to stabilize the robot attitude, compensating for the tether force and for the humanoid head and arm motions.","The humanoid can be controlled by a remote operator, thus effectively realizing a Suspended Aerial Manipulation Avatar.","The proposed system is validated through experiments in indoor scenarios reproducing post-disaster tasks."],"url":"http://arxiv.org/abs/2310.03586v1"}
{"created":"2023-10-05 15:08:37","title":"Smoothing Methods for Automatic Differentiation Across Conditional Branches","abstract":"Programs involving discontinuities introduced by control flow constructs such as conditional branches pose challenges to mathematical optimization methods that assume a degree of smoothness in the objective function's response surface. Smooth interpretation (SI) is a form of abstract interpretation that approximates the convolution of a program's output with a Gaussian kernel, thus smoothing its output in a principled manner. Here, we combine SI with automatic differentiation (AD) to efficiently compute gradients of smoothed programs. In contrast to AD across a regular program execution, these gradients also capture the effects of alternative control flow paths. The combination of SI with AD enables the direct gradient-based parameter synthesis for branching programs, allowing for instance the calibration of simulation models or their combination with neural network models in machine learning pipelines. We detail the effects of the approximations made for tractability in SI and propose a novel Monte Carlo estimator that avoids the underlying assumptions by estimating the smoothed programs' gradients through a combination of AD and sampling. Using DiscoGrad, our tool for automatically translating simple C++ programs to a smooth differentiable form, we perform an extensive evaluation. We compare the combination of SI with AD and our Monte Carlo estimator to existing gradient-free and stochastic methods on four non-trivial and originally discontinuous problems ranging from classical simulation-based optimization to neural network-driven control. While the optimization progress with the SI-based estimator depends on the complexity of the programs' control flow, our Monte Carlo estimator is competitive in all problems, exhibiting the fastest convergence by a substantial margin in our highest-dimensional problem.","sentences":["Programs involving discontinuities introduced by control flow constructs such as conditional branches pose challenges to mathematical optimization methods that assume a degree of smoothness in the objective function's response surface.","Smooth interpretation (SI) is a form of abstract interpretation that approximates the convolution of a program's output with a Gaussian kernel, thus smoothing its output in a principled manner.","Here, we combine SI with automatic differentiation (AD) to efficiently compute gradients of smoothed programs.","In contrast to AD across a regular program execution, these gradients also capture the effects of alternative control flow paths.","The combination of SI with AD enables the direct gradient-based parameter synthesis for branching programs, allowing for instance the calibration of simulation models or their combination with neural network models in machine learning pipelines.","We detail the effects of the approximations made for tractability in SI and propose a novel Monte Carlo estimator that avoids the underlying assumptions by estimating the smoothed programs' gradients through a combination of AD and sampling.","Using DiscoGrad, our tool for automatically translating simple C++ programs to a smooth differentiable form, we perform an extensive evaluation.","We compare the combination of SI with AD and our Monte Carlo estimator to existing gradient-free and stochastic methods on four non-trivial and originally discontinuous problems ranging from classical simulation-based optimization to neural network-driven control.","While the optimization progress with the SI-based estimator depends on the complexity of the programs' control flow, our Monte Carlo estimator is competitive in all problems, exhibiting the fastest convergence by a substantial margin in our highest-dimensional problem."],"url":"http://arxiv.org/abs/2310.03585v1"}
{"created":"2023-10-05 15:05:16","title":"CyMed: A Framework for Testing Cybersecurity of Connected Medical Devices","abstract":"Connected Medical Devices (CMDs) have a large impact on patients as they allow them to lead a more normal life. Any malfunction could not only remove the health benefits the CMDs provide, they could also cause further harm to the patient. Due to this, there are many safety regulations which must be adhered to prior to a CMD entering the market. However, while many detailed safety regulations exist, there are a fundamental lack of cybersecurity frameworks applicable to CMDs. While there are recent regulations which aim to enforce cybersecurity practices, they are vague and do not contain the concrete steps necessary to implement cybersecurity. This paper aims to fill that gap by describing a framework, CyMed, to be used by vendors and ens-users, which contains concrete measures to improve the resilience of CMDs against cyber attack. The CyMed framework is subsequently evaluated based on practical tests as well as expert interviews.","sentences":["Connected Medical Devices (CMDs) have a large impact on patients as they allow them to lead a more normal life.","Any malfunction could not only remove the health benefits the CMDs provide, they could also cause further harm to the patient.","Due to this, there are many safety regulations which must be adhered to prior to a CMD entering the market.","However, while many detailed safety regulations exist, there are a fundamental lack of cybersecurity frameworks applicable to CMDs.","While there are recent regulations which aim to enforce cybersecurity practices, they are vague and do not contain the concrete steps necessary to implement cybersecurity.","This paper aims to fill that gap by describing a framework, CyMed, to be used by vendors and ens-users, which contains concrete measures to improve the resilience of CMDs against cyber attack.","The CyMed framework is subsequently evaluated based on practical tests as well as expert interviews."],"url":"http://arxiv.org/abs/2310.03583v1"}
{"created":"2023-10-05 15:01:31","title":"Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End","abstract":"Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the real quadruped robot ANYmal running in real-time (<10 ms CPU inference). In a quantitative comparison with existing heuristic-based locally reactive planners, our policy increases the success rate over 30% when facing perception failures. Project Page: https://bit.ly/45NBTuh.","sentences":["Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures.","Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization.","In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot.","Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end.","To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions.","We validate our approach in simulation and on the real quadruped robot ANYmal running in real-time (<10 ms CPU inference).","In a quantitative comparison with existing heuristic-based locally reactive planners, our policy increases the success rate over 30% when facing perception failures.","Project Page: https://bit.ly/45NBTuh."],"url":"http://arxiv.org/abs/2310.03581v1"}
{"created":"2023-10-05 14:59:23","title":"Open RAN for 5G Supply Chain Diversification: The BEACON-5G Approach and Key Achievements","abstract":"Open RAN brings multi-vendor diversity and interoperability to mobile/cellular networks. It is becoming part of governmental strategies for diversifying telecoms supply chains. This paper describes the approach and key achievements of the BEACON-5G project, jointly funded by the UK government and industry. The BEACON-5G project aims at developing a competitive edge for 5G Open RAN and contributing toward its maturity. It addresses some of the key challenges in this respect and provides various innovations for system integration, network slicing, marketplace integration, cyber security, and white-box RAN. It also conducts real-world technology trials for urban use-cases. The paper also captures some of the key lessons learned during delivery, the main outcomes, and highlights potential impact on the wider UK 5G diversification strategy.","sentences":["Open RAN brings multi-vendor diversity and interoperability to mobile/cellular networks.","It is becoming part of governmental strategies for diversifying telecoms supply chains.","This paper describes the approach and key achievements of the BEACON-5G project, jointly funded by the UK government and industry.","The BEACON-5G project aims at developing a competitive edge for 5G Open RAN and contributing toward its maturity.","It addresses some of the key challenges in this respect and provides various innovations for system integration, network slicing, marketplace integration, cyber security, and white-box RAN.","It also conducts real-world technology trials for urban use-cases.","The paper also captures some of the key lessons learned during delivery, the main outcomes, and highlights potential impact on the wider UK 5G diversification strategy."],"url":"http://arxiv.org/abs/2310.03580v1"}
{"created":"2023-10-05 14:59:19","title":"Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems","abstract":"Understanding causal relationships within Gene Regulatory Networks (GRNs) is essential for unraveling the gene interactions in cellular processes. However, causal discovery in GRNs is a challenging problem for multiple reasons including the existence of cyclic feedback loops and uncertainty that yields diverse possible causal structures. Previous works in this area either ignore cyclic dynamics (assume acyclic structure) or struggle with scalability. We introduce Swift-DynGFN as a novel framework that enhances causal structure learning in GRNs while addressing scalability concerns. Specifically, Swift-DynGFN exploits gene-wise independence to boost parallelization and to lower computational cost. Experiments on real single-cell RNA velocity and synthetic GRN datasets showcase the advancement in learning causal structure in GRNs and scalability in larger systems.","sentences":["Understanding causal relationships within Gene Regulatory Networks (GRNs) is essential for unraveling the gene interactions in cellular processes.","However, causal discovery in GRNs is a challenging problem for multiple reasons including the existence of cyclic feedback loops and uncertainty that yields diverse possible causal structures.","Previous works in this area either ignore cyclic dynamics (assume acyclic structure) or struggle with scalability.","We introduce Swift-DynGFN as a novel framework that enhances causal structure learning in GRNs while addressing scalability concerns.","Specifically, Swift-DynGFN exploits gene-wise independence to boost parallelization and to lower computational cost.","Experiments on real single-cell RNA velocity and synthetic GRN datasets showcase the advancement in learning causal structure in GRNs and scalability in larger systems."],"url":"http://arxiv.org/abs/2310.03579v1"}
{"created":"2023-10-05 14:59:18","title":"Targeted Adversarial Attacks on Generalizable Neural Radiance Fields","abstract":"Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for 3D scene representation and rendering. These data-driven models can learn to synthesize high-quality images from sparse 2D observations, enabling realistic and interactive scene reconstructions. However, the growing usage of NeRFs in critical applications such as augmented reality, robotics, and virtual environments could be threatened by adversarial attacks.   In this paper we present how generalizable NeRFs can be attacked by both low-intensity adversarial attacks and adversarial patches, where the later could be robust enough to be used in real world applications. We also demonstrate targeted attacks, where a specific, predefined output scene is generated by these attack with success.","sentences":["Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for 3D scene representation and rendering.","These data-driven models can learn to synthesize high-quality images from sparse 2D observations, enabling realistic and interactive scene reconstructions.","However, the growing usage of NeRFs in critical applications such as augmented reality, robotics, and virtual environments could be threatened by adversarial attacks.   ","In this paper we present how generalizable NeRFs can be attacked by both low-intensity adversarial attacks and adversarial patches, where the later could be robust enough to be used in real world applications.","We also demonstrate targeted attacks, where a specific, predefined output scene is generated by these attack with success."],"url":"http://arxiv.org/abs/2310.03578v1"}
{"created":"2023-10-05 14:46:22","title":"A note on a gap in the proof of the minimum distance for Projective Reed-Muller Codes","abstract":"The note clarifies a gap in the proof of the minimum distance for Projective Reed-Muller Codes. The gap was identified by S.Ghorpade and R.Ludhani in a recent article. Here the original thoughts are explained and the gap closed.","sentences":["The note clarifies a gap in the proof of the minimum distance for Projective Reed-Muller Codes.","The gap was identified by S.Ghorpade and R.Ludhani in a recent article.","Here the original thoughts are explained and the gap closed."],"url":"http://arxiv.org/abs/2310.03574v1"}
{"created":"2023-10-05 14:43:16","title":"Residual Multi-Fidelity Neural Network Computing","abstract":"In this work, we consider the general problem of constructing a neural network surrogate model using multi-fidelity information. Given an inexpensive low-fidelity and an expensive high-fidelity computational model, we present a residual multi-fidelity computational framework that formulates the correlation between models as a residual function, a possibly non-linear mapping between 1) the shared input space of the models together with the low-fidelity model output and 2) the discrepancy between the two model outputs. To accomplish this, we train two neural networks to work in concert. The first network learns the residual function on a small set of high-fidelity and low-fidelity data. Once trained, this network is used to generate additional synthetic high-fidelity data, which is used in the training of a second network. This second network, once trained, acts as our surrogate for the high-fidelity quantity of interest. We present three numerical examples to demonstrate the power of the proposed framework. In particular, we show that dramatic savings in computational cost may be achieved when the output predictions are desired to be accurate within small tolerances.","sentences":["In this work, we consider the general problem of constructing a neural network surrogate model using multi-fidelity information.","Given an inexpensive low-fidelity and an expensive high-fidelity computational model, we present a residual multi-fidelity computational framework that formulates the correlation between models as a residual function, a possibly non-linear mapping between 1) the shared input space of the models together with the low-fidelity model output and 2) the discrepancy between the two model outputs.","To accomplish this, we train two neural networks to work in concert.","The first network learns the residual function on a small set of high-fidelity and low-fidelity data.","Once trained, this network is used to generate additional synthetic high-fidelity data, which is used in the training of a second network.","This second network, once trained, acts as our surrogate for the high-fidelity quantity of interest.","We present three numerical examples to demonstrate the power of the proposed framework.","In particular, we show that dramatic savings in computational cost may be achieved when the output predictions are desired to be accurate within small tolerances."],"url":"http://arxiv.org/abs/2310.03572v1"}
{"created":"2023-10-05 14:33:38","title":"Reverse-Mode AD of Reduce-by-Index and Scan in Futhark","abstract":"We present and evaluate the Futhark implementation of reverse-mode automatic differentiation (AD) for the basic blocks of parallel programming: reduce, prefix sum (scan), and reduce by index. We first present derivations of general-case algorithms and then discuss several specializations that result in efficient differentiation of most cases of practical interest. We report an experiment that evaluates the performance of the differentiated code in the context of GPU execution and highlights the impact of the proposed specializations as well as the strengths and weaknesses of differentiating at high level vs. low level (i.e., ``differentiating the memory'').","sentences":["We present and evaluate the Futhark implementation of reverse-mode automatic differentiation (AD) for the basic blocks of parallel programming: reduce, prefix sum (scan), and reduce by index.","We first present derivations of general-case algorithms and then discuss several specializations that result in efficient differentiation of most cases of practical interest.","We report an experiment that evaluates the performance of the differentiated code in the context of GPU execution and highlights the impact of the proposed specializations as well as the strengths and weaknesses of differentiating at high level vs. low level (i.e., ``differentiating the memory'')."],"url":"http://arxiv.org/abs/2310.03568v1"}
{"created":"2023-10-05 14:30:52","title":"SimLOD: Simultaneous LOD Generation and Rendering","abstract":"About: We propose an incremental LOD generation approach for point clouds that allows us to simultaneously load points from disk, update an octree-based level-of-detail representation, and render the intermediate results in real time while additional points are still being loaded from disk. LOD construction and rendering are both implemented in CUDA and share the GPU's processing power, but each incremental update is lightweight enough to leave enough time to maintain real-time frame rates.   Background: LOD construction is typically implemented as a preprocessing step that requires users to wait before they are able to view the results in real time. This approach allows users to view intermediate results right away.   Results: Our approach is able to stream points from an SSD and update the octree on the GPU at rates of up to 580 million points per second (~9.3GB/s from a PCIe 5.0 SSD) on an RTX 4090. Depending on the data set, our approach spends an average of about 1 to 2 ms to incrementally insert 1 million points into the octree, allowing us to insert several million points per frame into the LOD structure and render the intermediate results within the same frame.   Discussion/Limitations: We aim to provide near-instant, real-time visualization of large data sets without preprocessing. Out-of-core processing of arbitrarily large data sets and color-filtering for higher-quality LODs are subject to future work.","sentences":["About: We propose an incremental LOD generation approach for point clouds that allows us to simultaneously load points from disk, update an octree-based level-of-detail representation, and render the intermediate results in real time while additional points are still being loaded from disk.","LOD construction and rendering are both implemented in CUDA and share the GPU's processing power, but each incremental update is lightweight enough to leave enough time to maintain real-time frame rates.   ","Background: LOD construction is typically implemented as a preprocessing step that requires users to wait before they are able to view the results in real time.","This approach allows users to view intermediate results right away.   ","Results:","Our approach is able to stream points from an SSD and update the octree on the GPU at rates of up to 580 million points per second (~9.3GB/s from a PCIe 5.0 SSD) on an RTX 4090.","Depending on the data set, our approach spends an average of about 1 to 2 ms to incrementally insert 1 million points into the octree, allowing us to insert several million points per frame into the LOD structure and render the intermediate results within the same frame.   ","Discussion/Limitations: We aim to provide near-instant, real-time visualization of large data sets without preprocessing.","Out-of-core processing of arbitrarily large data sets and color-filtering for higher-quality LODs are subject to future work."],"url":"http://arxiv.org/abs/2310.03567v1"}
{"created":"2023-10-05 14:27:06","title":"BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance Fields","abstract":"We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which defines the image pose estimation problem as a NeRF based iterative linear optimization. NeRFs are novel neural space representation models that can synthesize photorealistic novel views of real-world scenes or objects. Our contributions are as follows: we extend the localization optimization objective with a depth-based loss function, we introduce a multi-image based loss function where a sequence of images with known relative poses are used without increasing the computational complexity, we omit hierarchical sampling during volumetric rendering, meaning only the coarse model is used for pose estimation, and we how that by extending the sampling interval convergence can be achieved even or higher initial pose estimate errors. With the proposed modifications the convergence speed is significantly improved, and the basin of convergence is substantially extended.","sentences":["We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which defines the image pose estimation problem as a NeRF based iterative linear optimization.","NeRFs are novel neural space representation models that can synthesize photorealistic novel views of real-world scenes or objects.","Our contributions are as follows: we extend the localization optimization objective with a depth-based loss function, we introduce a multi-image based loss function where a sequence of images with known relative poses are used without increasing the computational complexity, we omit hierarchical sampling during volumetric rendering, meaning only the coarse model is used for pose estimation, and we how that by extending the sampling interval convergence can be achieved even or higher initial pose estimate errors.","With the proposed modifications the convergence speed is significantly improved, and the basin of convergence is substantially extended."],"url":"http://arxiv.org/abs/2310.03563v1"}
{"created":"2023-10-05 14:18:40","title":"Redefining Digital Health Interfaces with Large Language Models","abstract":"Digital health tools have the potential to significantly improve the delivery of healthcare services. However, their use remains comparatively limited due, in part, to challenges surrounding usability and trust. Recently, Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare. Directly applying LLMs in clinical settings is not straightforward, with LLMs susceptible to providing inconsistent or nonsensical answers. We demonstrate how LLMs can utilize external tools to provide a novel interface between clinicians and digital technologies. This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLM in clinical settings such as hallucinations. We illustrate our approach with examples from cardiovascular disease and diabetes risk prediction, highlighting the benefit compared to traditional interfaces for digital tools.","sentences":["Digital health tools have the potential to significantly improve the delivery of healthcare services.","However, their use remains comparatively limited due, in part, to challenges surrounding usability and trust.","Recently, Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare.","Directly applying LLMs in clinical settings is not straightforward, with LLMs susceptible to providing inconsistent or nonsensical answers.","We demonstrate how LLMs can utilize external tools to provide a novel interface between clinicians and digital technologies.","This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLM in clinical settings such as hallucinations.","We illustrate our approach with examples from cardiovascular disease and diabetes risk prediction, highlighting the benefit compared to traditional interfaces for digital tools."],"url":"http://arxiv.org/abs/2310.03560v1"}
{"created":"2023-10-05 14:08:44","title":"Mobility Segregation Dynamics and Residual Isolation During Pandemic Interventions","abstract":"External shocks embody an unexpected and disruptive impact on the regular life of people. This was the case during the COVID-19 outbreak that rapidly led to changes in the typical mobility patterns in urban areas. In response, people reorganised their daily errands throughout space. However, these changes might not have been the same across socioeconomic classes leading to possibile additional detrimental effects on inequality due to the pandemic. In this paper we study the reorganisation of mobility segregation networks due to external shocks and show that the diversity of visited places in terms of locations and socioeconomic status is affected by the enforcement of mobility restriction during pandemic. We use the case of COVID-19 as a natural experiment in several cities to observe not only the effect of external shocks but also its mid-term consequences and residual effects. We build on anonymised and privacy-preserved mobility data in four cities: Bogota, Jakarta, London, and New York. We couple mobility data with socioeconomic information to capture inequalities in mobility among different socioeconomic groups and see how it changes dynamically before, during, and after different lockdown periods. We find that the first lockdowns induced considerable increases in mobility segregation in each city, while loosening mobility restrictions did not necessarily diminished isolation between different socioeconomic groups, as mobility mixing has not recovered fully to its pre-pandemic level even weeks after the interruption of interventions. Our results suggest that a one fits-all policy does not equally affect the way people adjust their mobility, which calls for socioeconomically informed intervention policies in the future.","sentences":["External shocks embody an unexpected and disruptive impact on the regular life of people.","This was the case during the COVID-19 outbreak that rapidly led to changes in the typical mobility patterns in urban areas.","In response, people reorganised their daily errands throughout space.","However, these changes might not have been the same across socioeconomic classes leading to possibile additional detrimental effects on inequality due to the pandemic.","In this paper we study the reorganisation of mobility segregation networks due to external shocks and show that the diversity of visited places in terms of locations and socioeconomic status is affected by the enforcement of mobility restriction during pandemic.","We use the case of COVID-19 as a natural experiment in several cities to observe not only the effect of external shocks but also its mid-term consequences and residual effects.","We build on anonymised and privacy-preserved mobility data in four cities: Bogota, Jakarta, London, and New York.","We couple mobility data with socioeconomic information to capture inequalities in mobility among different socioeconomic groups and see how it changes dynamically before, during, and after different lockdown periods.","We find that the first lockdowns induced considerable increases in mobility segregation in each city, while loosening mobility restrictions did not necessarily diminished isolation between different socioeconomic groups, as mobility mixing has not recovered fully to its pre-pandemic level even weeks after the interruption of interventions.","Our results suggest that a one fits-all policy does not equally affect the way people adjust their mobility, which calls for socioeconomically informed intervention policies in the future."],"url":"http://arxiv.org/abs/2310.03557v1"}
{"created":"2023-10-05 14:06:04","title":"Digital Twin-Empowered Smart Attack Detection System for 6G Edge of Things Networks","abstract":"As global Internet of Things (IoT) devices connectivity surges, a significant portion gravitates towards the Edge of Things (EoT) network. This shift prompts businesses to deploy infrastructure closer to end-users, enhancing accessibility. However, the growing EoT network expands the attack surface, necessitating robust and proactive security measures. Traditional solutions fall short against dynamic EoT threats, highlighting the need for proactive and intelligent systems. We introduce a digital twin-empowered smart attack detection system for 6G EoT networks. Leveraging digital twin and edge computing, it monitors and simulates physical assets in real time, enhancing security. An online learning module in the proposed system optimizes the network performance. Our system excels in proactive threat detection, ensuring 6G EoT network security. The performance evaluations demonstrate its effectiveness, robustness, and adaptability using real datasets.","sentences":["As global Internet of Things (IoT) devices connectivity surges, a significant portion gravitates towards the Edge of Things (EoT) network.","This shift prompts businesses to deploy infrastructure closer to end-users, enhancing accessibility.","However, the growing EoT network expands the attack surface, necessitating robust and proactive security measures.","Traditional solutions fall short against dynamic EoT threats, highlighting the need for proactive and intelligent systems.","We introduce a digital twin-empowered smart attack detection system for 6G EoT networks.","Leveraging digital twin and edge computing, it monitors and simulates physical assets in real time, enhancing security.","An online learning module in the proposed system optimizes the network performance.","Our system excels in proactive threat detection, ensuring 6G EoT network security.","The performance evaluations demonstrate its effectiveness, robustness, and adaptability using real datasets."],"url":"http://arxiv.org/abs/2310.03554v1"}
{"created":"2023-10-05 13:57:24","title":"Distribution-free risk assessment of regression-based machine learning algorithms","abstract":"Machine learning algorithms have grown in sophistication over the years and are increasingly deployed for real-life applications. However, when using machine learning techniques in practical settings, particularly in high-risk applications such as medicine and engineering, obtaining the failure probability of the predictive model is critical. We refer to this problem as the risk-assessment task. We focus on regression algorithms and the risk-assessment task of computing the probability of the true label lying inside an interval defined around the model's prediction. We solve the risk-assessment problem using the conformal prediction approach, which provides prediction intervals that are guaranteed to contain the true label with a given probability. Using this coverage property, we prove that our approximated failure probability is conservative in the sense that it is not lower than the true failure probability of the ML algorithm. We conduct extensive experiments to empirically study the accuracy of the proposed method for problems with and without covariate shift. Our analysis focuses on different modeling regimes, dataset sizes, and conformal prediction methodologies.","sentences":["Machine learning algorithms have grown in sophistication over the years and are increasingly deployed for real-life applications.","However, when using machine learning techniques in practical settings, particularly in high-risk applications such as medicine and engineering, obtaining the failure probability of the predictive model is critical.","We refer to this problem as the risk-assessment task.","We focus on regression algorithms and the risk-assessment task of computing the probability of the true label lying inside an interval defined around the model's prediction.","We solve the risk-assessment problem using the conformal prediction approach, which provides prediction intervals that are guaranteed to contain the true label with a given probability.","Using this coverage property, we prove that our approximated failure probability is conservative in the sense that it is not lower than the true failure probability of the ML algorithm.","We conduct extensive experiments to empirically study the accuracy of the proposed method for problems with and without covariate shift.","Our analysis focuses on different modeling regimes, dataset sizes, and conformal prediction methodologies."],"url":"http://arxiv.org/abs/2310.03545v1"}
