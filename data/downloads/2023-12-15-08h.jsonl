{"created":"2023-12-14 18:59:59","title":"LIME: Localized Image Editing via Attention Regularization in Diffusion Models","abstract":"Diffusion models (DMs) have gained prominence due to their ability to generate high-quality, varied images, with recent advancements in text-to-image generation. The research focus is now shifting towards the controllability of DMs. A significant challenge within this domain is localized editing, where specific areas of an image are modified without affecting the rest of the content. This paper introduces LIME for localized image editing in diffusion models that do not require user-specified regions of interest (RoI) or additional text input. Our method employs features from pre-trained methods and a simple clustering technique to obtain precise semantic segmentation maps. Then, by leveraging cross-attention maps, it refines these segments for localized edits. Finally, we propose a novel cross-attention regularization technique that penalizes unrelated cross-attention scores in the RoI during the denoising steps, ensuring localized edits. Our approach, without re-training and fine-tuning, consistently improves the performance of existing methods in various editing benchmarks.","sentences":["Diffusion models (DMs) have gained prominence due to their ability to generate high-quality, varied images, with recent advancements in text-to-image generation.","The research focus is now shifting towards the controllability of DMs.","A significant challenge within this domain is localized editing, where specific areas of an image are modified without affecting the rest of the content.","This paper introduces LIME for localized image editing in diffusion models that do not require user-specified regions of interest (RoI) or additional text input.","Our method employs features from pre-trained methods and a simple clustering technique to obtain precise semantic segmentation maps.","Then, by leveraging cross-attention maps, it refines these segments for localized edits.","Finally, we propose a novel cross-attention regularization technique that penalizes unrelated cross-attention scores in the RoI during the denoising steps, ensuring localized edits.","Our approach, without re-training and fine-tuning, consistently improves the performance of existing methods in various editing benchmarks."],"url":"http://arxiv.org/abs/2312.09256v1"}
{"created":"2023-12-14 18:59:58","title":"Revisiting Depth Completion from a Stereo Matching Perspective for Cross-domain Generalization","abstract":"This paper proposes a new framework for depth completion robust against domain-shifting issues. It exploits the generalization capability of modern stereo networks to face depth completion, by processing fictitious stereo pairs obtained through a virtual pattern projection paradigm. Any stereo network or traditional stereo matcher can be seamlessly plugged into our framework, allowing for the deployment of a virtual stereo setup that is future-proof against advancement in the stereo field. Exhaustive experiments on cross-domain generalization support our claims. Hence, we argue that our framework can help depth completion to reach new deployment scenarios.","sentences":["This paper proposes a new framework for depth completion robust against domain-shifting issues.","It exploits the generalization capability of modern stereo networks to face depth completion, by processing fictitious stereo pairs obtained through a virtual pattern projection paradigm.","Any stereo network or traditional stereo matcher can be seamlessly plugged into our framework, allowing for the deployment of a virtual stereo setup that is future-proof against advancement in the stereo field.","Exhaustive experiments on cross-domain generalization support our claims.","Hence, we argue that our framework can help depth completion to reach new deployment scenarios."],"url":"http://arxiv.org/abs/2312.09254v1"}
{"created":"2023-12-14 18:59:43","title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation","abstract":"In this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data. VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text. To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly. In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model. Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction). Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more. Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts. We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance. The source code and model weights shall be released.","sentences":["In this work, we introduce Vision-Language Generative Pre-trained Transformer (VL-GPT), a transformer model proficient at concurrently perceiving and generating visual and linguistic data.","VL-GPT achieves a unified pre-training approach for both image and text modalities by employing a straightforward auto-regressive objective, thereby enabling the model to process image and text as seamlessly as a language model processes text.","To accomplish this, we initially propose a novel image tokenizer-detokenizer framework for visual data, specifically designed to transform raw images into a sequence of continuous embeddings and reconstruct them accordingly.","In combination with the existing text tokenizer and detokenizer, this framework allows for the encoding of interleaved image-text data into a multimodal sequence, which can subsequently be fed into the transformer model.","Consequently, VL-GPT can perform large-scale pre-training on multimodal corpora utilizing a unified auto-regressive objective (i.e., next-token prediction).","Upon completion of pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance across a diverse range of vision and language understanding and generation tasks, including image captioning, visual question answering, text-to-image generation, and more.","Additionally, the pre-trained model retrains in-context learning capabilities when provided with multimodal prompts.","We further conduct instruction tuning on our VL-GPT, highlighting its exceptional potential for multimodal assistance.","The source code and model weights shall be released."],"url":"http://arxiv.org/abs/2312.09251v1"}
{"created":"2023-12-14 18:59:43","title":"FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection","abstract":"Recently introduced ControlNet has the ability to steer the text-driven image generation process with geometric input such as human 2D pose, or edge features. While ControlNet provides control over the geometric form of the instances in the generated image, it lacks the capability to dictate the visual appearance of each instance. We present FineControlNet to provide fine control over each instance's appearance while maintaining the precise pose control capability. Specifically, we develop and demonstrate FineControlNet with geometric control via human pose images and appearance control via instance-level text prompts. The spatial alignment of instance-specific text prompts and 2D poses in latent space enables the fine control capabilities of FineControlNet. We evaluate the performance of FineControlNet with rigorous comparison against state-of-the-art pose-conditioned text-to-image diffusion models. FineControlNet achieves superior performance in generating images that follow the user-provided instance-specific text prompts and poses compared with existing methods. Project webpage: https://samsunglabs.github.io/FineControlNet-project-page","sentences":["Recently introduced ControlNet has the ability to steer the text-driven image generation process with geometric input such as human 2D pose, or edge features.","While ControlNet provides control over the geometric form of the instances in the generated image, it lacks the capability to dictate the visual appearance of each instance.","We present FineControlNet to provide fine control over each instance's appearance while maintaining the precise pose control capability.","Specifically, we develop and demonstrate FineControlNet with geometric control via human pose images and appearance control via instance-level text prompts.","The spatial alignment of instance-specific text prompts and 2D poses in latent space enables the fine control capabilities of FineControlNet.","We evaluate the performance of FineControlNet with rigorous comparison against state-of-the-art pose-conditioned text-to-image diffusion models.","FineControlNet achieves superior performance in generating images that follow the user-provided instance-specific text prompts and poses compared with existing methods.","Project webpage: https://samsunglabs.github.io/FineControlNet-project-page"],"url":"http://arxiv.org/abs/2312.09252v1"}
{"created":"2023-12-14 18:59:36","title":"Single Mesh Diffusion Models with Field Latents for Texture Generation","abstract":"We introduce a framework for intrinsic latent diffusion models operating directly on the surfaces of 3D shapes, with the goal of synthesizing high-quality textures. Our approach is underpinned by two contributions: field latents, a latent representation encoding textures as discrete vector fields on the mesh vertices, and field latent diffusion models, which learn to denoise a diffusion process in the learned latent space on the surface. We consider a single-textured-mesh paradigm, where our models are trained to generate variations of a given texture on a mesh. We show the synthesized textures are of superior fidelity compared those from existing single-textured-mesh generative models. Our models can also be adapted for user-controlled editing tasks such as inpainting and label-guided generation. The efficacy of our approach is due in part to the equivariance of our proposed framework under isometries, allowing our models to seamlessly reproduce details across locally similar regions and opening the door to a notion of generative texture transfer.","sentences":["We introduce a framework for intrinsic latent diffusion models operating directly on the surfaces of 3D shapes, with the goal of synthesizing high-quality textures.","Our approach is underpinned by two contributions: field latents, a latent representation encoding textures as discrete vector fields on the mesh vertices, and field latent diffusion models, which learn to denoise a diffusion process in the learned latent space on the surface.","We consider a single-textured-mesh paradigm, where our models are trained to generate variations of a given texture on a mesh.","We show the synthesized textures are of superior fidelity compared those from existing single-textured-mesh generative models.","Our models can also be adapted for user-controlled editing tasks such as inpainting and label-guided generation.","The efficacy of our approach is due in part to the equivariance of our proposed framework under isometries, allowing our models to seamlessly reproduce details across locally similar regions and opening the door to a notion of generative texture transfer."],"url":"http://arxiv.org/abs/2312.09250v1"}
{"created":"2023-12-14 18:59:32","title":"ZeroRF: Fast Sparse View 360\u00b0 Reconstruction with Zero Pretraining","abstract":"We present ZeroRF, a novel per-scene optimization method addressing the challenge of sparse view 360{\\deg} reconstruction in neural field representations. Current breakthroughs like Neural Radiance Fields (NeRF) have demonstrated high-fidelity image synthesis but struggle with sparse input views. Existing methods, such as Generalizable NeRFs and per-scene optimization approaches, face limitations in data dependency, computational cost, and generalization across diverse scenarios. To overcome these challenges, we propose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into a factorized NeRF representation. Unlike traditional methods, ZeroRF parametrizes feature grids with a neural network generator, enabling efficient sparse view 360{\\deg} reconstruction without any pretraining or additional regularization. Extensive experiments showcase ZeroRF's versatility and superiority in terms of both quality and speed, achieving state-of-the-art results on benchmark datasets. ZeroRF's significance extends to applications in 3D content generation and editing. Project page: https://sarahweiii.github.io/zerorf/","sentences":["We present ZeroRF, a novel per-scene optimization method addressing the challenge of sparse view 360{\\deg} reconstruction in neural field representations.","Current breakthroughs like Neural Radiance Fields (NeRF) have demonstrated high-fidelity image synthesis but struggle with sparse input views.","Existing methods, such as Generalizable NeRFs and per-scene optimization approaches, face limitations in data dependency, computational cost, and generalization across diverse scenarios.","To overcome these challenges, we propose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into a factorized NeRF representation.","Unlike traditional methods, ZeroRF parametrizes feature grids with a neural network generator, enabling efficient sparse view 360{\\deg} reconstruction without any pretraining or additional regularization.","Extensive experiments showcase ZeroRF's versatility and superiority in terms of both quality and speed, achieving state-of-the-art results on benchmark datasets.","ZeroRF's significance extends to applications in 3D content generation and editing.","Project page: https://sarahweiii.github.io/zerorf/"],"url":"http://arxiv.org/abs/2312.09249v1"}
{"created":"2023-12-14 18:59:06","title":"SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds","abstract":"We propose a novel feed-forward 3D editing framework called Shap-Editor. Prior research on editing 3D objects primarily concentrated on editing individual objects by leveraging off-the-shelf 2D image editing networks. This is achieved via a process called distillation, which transfers knowledge from the 2D network to 3D assets. Distillation necessitates at least tens of minutes per asset to attain satisfactory editing results, and is thus not very practical. In contrast, we ask whether 3D editing can be carried out directly by a feed-forward network, eschewing test-time optimisation. In particular, we hypothesise that editing can be greatly simplified by first encoding 3D objects in a suitable latent space. We validate this hypothesis by building upon the latent space of Shap-E. We demonstrate that direct 3D editing in this space is possible and efficient by building a feed-forward editor network that only requires approximately one second per edit. Our experiments show that Shap-Editor generalises well to both in-distribution and out-of-distribution 3D assets with different prompts, exhibiting comparable performance with methods that carry out test-time optimisation for each edited instance.","sentences":["We propose a novel feed-forward 3D editing framework called Shap-Editor.","Prior research on editing 3D objects primarily concentrated on editing individual objects by leveraging off-the-shelf 2D image editing networks.","This is achieved via a process called distillation, which transfers knowledge from the 2D network to 3D assets.","Distillation necessitates at least tens of minutes per asset to attain satisfactory editing results, and is thus not very practical.","In contrast, we ask whether 3D editing can be carried out directly by a feed-forward network, eschewing test-time optimisation.","In particular, we hypothesise that editing can be greatly simplified by first encoding 3D objects in a suitable latent space.","We validate this hypothesis by building upon the latent space of Shap-E. We demonstrate that direct 3D editing in this space is possible and efficient by building a feed-forward editor network that only requires approximately one second per edit.","Our experiments show that Shap-Editor generalises well to both in-distribution and out-of-distribution 3D assets with different prompts, exhibiting comparable performance with methods that carry out test-time optimisation for each edited instance."],"url":"http://arxiv.org/abs/2312.09246v1"}
{"created":"2023-12-14 18:59:05","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving","abstract":"Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multi-modal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that our model achieves 76.1 driving score on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points under the same settings, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs. Code and models shall be released at https://github.com/OpenGVLab/DriveMLM.","sentences":["Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities.","In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD).","We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators.","To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module.","(2) We employ a multi-modal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Apollo for close-loop driving.","(3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation.","We conduct extensive experiments and show that our model achieves 76.1 driving score on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points under the same settings, demonstrating the effectiveness of our model.","We hope this work can serve as a baseline for autonomous driving with LLMs.","Code and models shall be released at https://github.com/OpenGVLab/DriveMLM."],"url":"http://arxiv.org/abs/2312.09245v1"}
{"created":"2023-12-14 18:59:04","title":"Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking","abstract":"Reward models play a key role in aligning language model applications towards human preferences. However, this setup creates an incentive for the language model to exploit errors in the reward model to achieve high estimated reward, a phenomenon often termed \\emph{reward hacking}. A natural mitigation is to train an ensemble of reward models, aggregating over model outputs to obtain a more robust reward estimate. We explore the application of reward ensembles to alignment at both training time (through reinforcement learning) and inference time (through reranking). First, we show that reward models are \\emph{underspecified}: reward models that perform similarly in-distribution can yield very different rewards when used in alignment, due to distribution shift. Second, underspecification results in overoptimization, where alignment to one reward model does not improve reward as measured by another reward model trained on the same data. Third, overoptimization is mitigated by the use of reward ensembles, and ensembles that vary by their \\emph{pretraining} seeds lead to better generalization than ensembles that differ only by their \\emph{fine-tuning} seeds, with both outperforming individual reward models. However, even pretrain reward ensembles do not eliminate reward hacking: we show several qualitative reward hacking phenomena that are not mitigated by ensembling because all reward models in the ensemble exhibit similar error patterns.","sentences":["Reward models play a key role in aligning language model applications towards human preferences.","However, this setup creates an incentive for the language model to exploit errors in the reward model to achieve high estimated reward, a phenomenon often termed \\emph{reward hacking}.","A natural mitigation is to train an ensemble of reward models, aggregating over model outputs to obtain a more robust reward estimate.","We explore the application of reward ensembles to alignment at both training time (through reinforcement learning) and inference time (through reranking).","First, we show that reward models are \\emph{underspecified}: reward models that perform similarly in-distribution can yield very different rewards when used in alignment, due to distribution shift.","Second, underspecification results in overoptimization, where alignment to one reward model does not improve reward as measured by another reward model trained on the same data.","Third, overoptimization is mitigated by the use of reward ensembles, and ensembles that vary by their \\emph{pretraining} seeds lead to better generalization than ensembles that differ only by their \\emph{fine-tuning} seeds, with both outperforming individual reward models.","However, even pretrain reward ensembles do not eliminate reward hacking: we show several qualitative reward hacking phenomena that are not mitigated by ensembling because all reward models in the ensemble exhibit similar error patterns."],"url":"http://arxiv.org/abs/2312.09244v1"}
{"created":"2023-12-14 18:58:52","title":"OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural Radiance Fields","abstract":"As a fundamental task of vision-based perception, 3D occupancy prediction reconstructs 3D structures of surrounding environments. It provides detailed information for autonomous driving planning and navigation. However, most existing methods heavily rely on the LiDAR point clouds to generate occupancy ground truth, which is not available in the vision-based system. In this paper, we propose an OccNeRF method for self-supervised multi-camera occupancy prediction. Different from bounded 3D occupancy labels, we need to consider unbounded scenes with raw image supervision. To solve the issue, we parameterize the reconstructed occupancy fields and reorganize the sampling strategy. The neural rendering is adopted to convert occupancy fields to multi-camera depth maps, supervised by multi-frame photometric consistency. Moreover, for semantic occupancy prediction, we design several strategies to polish the prompts and filter the outputs of a pretrained open-vocabulary 2D segmentation model. Extensive experiments for both self-supervised depth estimation and semantic occupancy prediction tasks on nuScenes dataset demonstrate the effectiveness of our method.","sentences":["As a fundamental task of vision-based perception, 3D occupancy prediction reconstructs 3D structures of surrounding environments.","It provides detailed information for autonomous driving planning and navigation.","However, most existing methods heavily rely on the LiDAR point clouds to generate occupancy ground truth, which is not available in the vision-based system.","In this paper, we propose an OccNeRF method for self-supervised multi-camera occupancy prediction.","Different from bounded 3D occupancy labels, we need to consider unbounded scenes with raw image supervision.","To solve the issue, we parameterize the reconstructed occupancy fields and reorganize the sampling strategy.","The neural rendering is adopted to convert occupancy fields to multi-camera depth maps, supervised by multi-frame photometric consistency.","Moreover, for semantic occupancy prediction, we design several strategies to polish the prompts and filter the outputs of a pretrained open-vocabulary 2D segmentation model.","Extensive experiments for both self-supervised depth estimation and semantic occupancy prediction tasks on nuScenes dataset demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2312.09243v1"}
{"created":"2023-12-14 18:58:47","title":"Text2Immersion: Generative Immersive Scene with 3D Gaussians","abstract":"We introduce Text2Immersion, an elegant method for producing high-quality 3D immersive scenes from text prompts. Our proposed pipeline initiates by progressively generating a Gaussian cloud using pre-trained 2D diffusion and depth estimation models. This is followed by a refining stage on the Gaussian cloud, interpolating and refining it to enhance the details of the generated scene. Distinct from prevalent methods that focus on single object or indoor scenes, or employ zoom-out trajectories, our approach generates diverse scenes with various objects, even extending to the creation of imaginary scenes. Consequently, Text2Immersion can have wide-ranging implications for various applications such as virtual reality, game development, and automated content creation. Extensive evaluations demonstrate that our system surpasses other methods in rendering quality and diversity, further progressing towards text-driven 3D scene generation. We will make the source code publicly accessible at the project page.","sentences":["We introduce Text2Immersion, an elegant method for producing high-quality 3D immersive scenes from text prompts.","Our proposed pipeline initiates by progressively generating a Gaussian cloud using pre-trained 2D diffusion and depth estimation models.","This is followed by a refining stage on the Gaussian cloud, interpolating and refining it to enhance the details of the generated scene.","Distinct from prevalent methods that focus on single object or indoor scenes, or employ zoom-out trajectories, our approach generates diverse scenes with various objects, even extending to the creation of imaginary scenes.","Consequently, Text2Immersion can have wide-ranging implications for various applications such as virtual reality, game development, and automated content creation.","Extensive evaluations demonstrate that our system surpasses other methods in rendering quality and diversity, further progressing towards text-driven 3D scene generation.","We will make the source code publicly accessible at the project page."],"url":"http://arxiv.org/abs/2312.09242v1"}
{"created":"2023-12-14 18:58:28","title":"TinyGSM: achieving >80% on GSM8k with small language models","abstract":"Small-scale models offer various computational advantages, and yet to which extent size is critical for problem-solving abilities remains an open question. Specifically for solving grade school math, the smallest model size so far required to break the 80\\% barrier on the GSM8K benchmark remains to be 34B. Our work studies how high-quality datasets may be the key for small language models to acquire mathematical reasoning. We introduce \\texttt{TinyGSM}, a synthetic dataset of 12.3M grade school math problems paired with Python solutions, generated fully by GPT-3.5. After finetuning on \\texttt{TinyGSM}, we find that a duo of a 1.3B generation model and a 1.3B verifier model can achieve 81.5\\% accuracy, outperforming existing models that are orders of magnitude larger. This also rivals the performance of the GPT-3.5 ``teacher'' model (77.4\\%), from which our model's training data is generated. Our approach is simple and has two key components: 1) the high-quality dataset \\texttt{TinyGSM}, 2) the use of a verifier, which selects the final outputs from multiple candidate generations.","sentences":["Small-scale models offer various computational advantages, and yet to which extent size is critical for problem-solving abilities remains an open question.","Specifically for solving grade school math, the smallest model size so far required to break the 80\\% barrier on the GSM8K benchmark remains to be 34B. Our work studies how high-quality datasets may be the key for small language models to acquire mathematical reasoning.","We introduce \\texttt{TinyGSM}, a synthetic dataset of 12.3M grade school math problems paired with Python solutions, generated fully by GPT-3.5.","After finetuning on \\texttt{TinyGSM}, we find that a duo of a 1.3B generation model and a 1.3B verifier model can achieve 81.5\\% accuracy, outperforming existing models that are orders of magnitude larger.","This also rivals the performance of the GPT-3.5 ``teacher'' model (77.4\\%), from which our model's training data is generated.","Our approach is simple and has two key components: 1) the high-quality dataset \\texttt{TinyGSM}, 2) the use of a verifier, which selects the final outputs from multiple candidate generations."],"url":"http://arxiv.org/abs/2312.09241v1"}
{"created":"2023-12-14 18:58:12","title":"Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft","abstract":"Traditional reinforcement-learning-based agents rely on sparse rewards that often only use binary values to indicate task completion or failure. The challenge in exploration efficiency makes it difficult to effectively learn complex tasks in Minecraft. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax and semantic errors. Further, the Trajectory Analyzer summarizes possible failure causes and provides refinement suggestions according to collected trajectories. In the next round, Reward Designer will take further refine and iterate the dense reward function based on feedback. Experiments demonstrate a significant improvement in the success rate and learning efficiency of our agents in complex tasks in Minecraft, such as obtaining diamond with the efficient ability to avoid lava, and efficiently explore trees and animals that are sparse on the plains biome.","sentences":["Traditional reinforcement-learning-based agents rely on sparse rewards that often only use binary values to indicate task completion or failure.","The challenge in exploration efficiency makes it difficult to effectively learn complex tasks in Minecraft.","To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency.","Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer.","Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs.","Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax and semantic errors.","Further, the Trajectory Analyzer summarizes possible failure causes and provides refinement suggestions according to collected trajectories.","In the next round, Reward Designer will take further refine and iterate the dense reward function based on feedback.","Experiments demonstrate a significant improvement in the success rate and learning efficiency of our agents in complex tasks in Minecraft, such as obtaining diamond with the efficient ability to avoid lava, and efficiently explore trees and animals that are sparse on the plains biome."],"url":"http://arxiv.org/abs/2312.09238v1"}
{"created":"2023-12-14 18:57:58","title":"Pixel Aligned Language Models","abstract":"Large language models have achieved great success in recent years, so as their variants in vision. Existing vision-language models can describe images in natural languages, answer visual-related questions, or perform complex reasoning about the image. However, it is yet unclear how localization tasks, such as word grounding or referring localization, can be performed using large language models. In this work, we aim to develop a vision-language model that can take locations, for example, a set of points or boxes, as either inputs or outputs. When taking locations as inputs, the model performs location-conditioned captioning, which generates captions for the indicated object or region. When generating locations as outputs, our model regresses pixel coordinates for each output word generated by the language model, and thus performs dense word grounding. Our model is pre-trained on the Localized Narrative dataset, which contains pixel-word-aligned captioning from human attention. We show our model can be applied to various location-aware vision-language tasks, including referring localization, location-conditioned captioning, and dense object captioning, archiving state-of-the-art performance on RefCOCO and Visual Genome. Project page: https://jerryxu.net/PixelLLM .","sentences":["Large language models have achieved great success in recent years, so as their variants in vision.","Existing vision-language models can describe images in natural languages, answer visual-related questions, or perform complex reasoning about the image.","However, it is yet unclear how localization tasks, such as word grounding or referring localization, can be performed using large language models.","In this work, we aim to develop a vision-language model that can take locations, for example, a set of points or boxes, as either inputs or outputs.","When taking locations as inputs, the model performs location-conditioned captioning, which generates captions for the indicated object or region.","When generating locations as outputs, our model regresses pixel coordinates for each output word generated by the language model, and thus performs dense word grounding.","Our model is pre-trained on the Localized Narrative dataset, which contains pixel-word-aligned captioning from human attention.","We show our model can be applied to various location-aware vision-language tasks, including referring localization, location-conditioned captioning, and dense object captioning, archiving state-of-the-art performance on RefCOCO and Visual Genome.","Project page: https://jerryxu.net/PixelLLM ."],"url":"http://arxiv.org/abs/2312.09237v1"}
{"created":"2023-12-14 18:57:56","title":"A framework for conditional diffusion modelling with applications in motif scaffolding for protein design","abstract":"Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision. Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases. In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature. However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches. In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform. This new perspective allows us to draw connections between existing methods and propose a new variation on existing conditional training protocols. We illustrate the effectiveness of this new protocol in both, image outpainting and motif scaffolding and find that it outperforms standard methods.","sentences":["Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision.","Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases.","In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature.","However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches.","In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform.","This new perspective allows us to draw connections between existing methods and propose a new variation on existing conditional training protocols.","We illustrate the effectiveness of this new protocol in both, image outpainting and motif scaffolding and find that it outperforms standard methods."],"url":"http://arxiv.org/abs/2312.09236v1"}
{"created":"2023-12-14 18:57:16","title":"Let's do the time-warp-attend: Learning topological invariants of dynamical systems","abstract":"Dynamical systems across the sciences, from electrical circuits to ecological networks, undergo qualitative and often catastrophic changes in behavior, called bifurcations, when their underlying parameters cross a threshold. Existing methods predict oncoming catastrophes in individual systems but are primarily time-series-based and struggle both to categorize qualitative dynamical regimes across diverse systems and to generalize to real data. To address this challenge, we propose a data-driven, physically-informed deep-learning framework for classifying dynamical regimes and characterizing bifurcation boundaries based on the extraction of topologically invariant features. We focus on the paradigmatic case of the supercritical Hopf bifurcation, which is used to model periodic dynamics across a wide range of applications. Our convolutional attention method is trained with data augmentations that encourage the learning of topological invariants which can be used to detect bifurcation boundaries in unseen systems and to design models of biological systems like oscillatory gene regulatory networks. We further demonstrate our method's use in analyzing real data by recovering distinct proliferation and differentiation dynamics along pancreatic endocrinogenesis trajectory in gene expression space based on single-cell data. Our method provides valuable insights into the qualitative, long-term behavior of a wide range of dynamical systems, and can detect bifurcations or catastrophic transitions in large-scale physical and biological systems.","sentences":["Dynamical systems across the sciences, from electrical circuits to ecological networks, undergo qualitative and often catastrophic changes in behavior, called bifurcations, when their underlying parameters cross a threshold.","Existing methods predict oncoming catastrophes in individual systems but are primarily time-series-based and struggle both to categorize qualitative dynamical regimes across diverse systems and to generalize to real data.","To address this challenge, we propose a data-driven, physically-informed deep-learning framework for classifying dynamical regimes and characterizing bifurcation boundaries based on the extraction of topologically invariant features.","We focus on the paradigmatic case of the supercritical Hopf bifurcation, which is used to model periodic dynamics across a wide range of applications.","Our convolutional attention method is trained with data augmentations that encourage the learning of topological invariants which can be used to detect bifurcation boundaries in unseen systems and to design models of biological systems like oscillatory gene regulatory networks.","We further demonstrate our method's use in analyzing real data by recovering distinct proliferation and differentiation dynamics along pancreatic endocrinogenesis trajectory in gene expression space based on single-cell data.","Our method provides valuable insights into the qualitative, long-term behavior of a wide range of dynamical systems, and can detect bifurcations or catastrophic transitions in large-scale physical and biological systems."],"url":"http://arxiv.org/abs/2312.09234v1"}
{"created":"2023-12-14 18:56:54","title":"DVQI: A Multi-task, Hardware-integrated Artificial Intelligence System for Automated Visual Inspection in Electronics Manufacturing","abstract":"As electronics manufacturers continue to face pressure to increase production efficiency amid difficulties with supply chains and labour shortages, many printed circuit board assembly (PCBA) manufacturers have begun to invest in automation and technological innovations to remain competitive. One such method is to leverage artificial intelligence (AI) to greatly augment existing manufacturing processes. In this paper, we present the DarwinAI Visual Quality Inspection (DVQI) system, a hardware-integration artificial intelligence system for the automated inspection of printed circuit board assembly defects in an electronics manufacturing environment. The DVQI system enables multi-task inspection via minimal programming and setup for manufacturing engineers while improving cycle time relative to manual inspection. We also present a case study of the deployed DVQI system's performance and impact for a top electronics manufacturer.","sentences":["As electronics manufacturers continue to face pressure to increase production efficiency amid difficulties with supply chains and labour shortages, many printed circuit board assembly (PCBA) manufacturers have begun to invest in automation and technological innovations to remain competitive.","One such method is to leverage artificial intelligence (AI) to greatly augment existing manufacturing processes.","In this paper, we present the DarwinAI Visual Quality Inspection (DVQI) system, a hardware-integration artificial intelligence system for the automated inspection of printed circuit board assembly defects in an electronics manufacturing environment.","The DVQI system enables multi-task inspection via minimal programming and setup for manufacturing engineers while improving cycle time relative to manual inspection.","We also present a case study of the deployed DVQI system's performance and impact for a top electronics manufacturer."],"url":"http://arxiv.org/abs/2312.09232v1"}
{"created":"2023-12-14 18:56:07","title":"Reliability in Semantic Segmentation: Can We Use Synthetic Data?","abstract":"Assessing the reliability of perception models to covariate shifts and out-of-distribution (OOD) detection is crucial for safety-critical applications such as autonomous vehicles. By nature of the task, however, the relevant data is difficult to collect and annotate. In this paper, we challenge cutting-edge generative models to automatically synthesize data for assessing reliability in semantic segmentation. By fine-tuning Stable Diffusion, we perform zero-shot generation of synthetic data in OOD domains or inpainted with OOD objects. Synthetic data is employed to provide an initial assessment of pretrained segmenters, thereby offering insights into their performance when confronted with real edge cases. Through extensive experiments, we demonstrate a high correlation between the performance on synthetic data and the performance on real OOD data, showing the validity approach. Furthermore, we illustrate how synthetic data can be utilized to enhance the calibration and OOD detection capabilities of segmenters.","sentences":["Assessing the reliability of perception models to covariate shifts and out-of-distribution (OOD) detection is crucial for safety-critical applications such as autonomous vehicles.","By nature of the task, however, the relevant data is difficult to collect and annotate.","In this paper, we challenge cutting-edge generative models to automatically synthesize data for assessing reliability in semantic segmentation.","By fine-tuning Stable Diffusion, we perform zero-shot generation of synthetic data in OOD domains or inpainted with OOD objects.","Synthetic data is employed to provide an initial assessment of pretrained segmenters, thereby offering insights into their performance when confronted with real edge cases.","Through extensive experiments, we demonstrate a high correlation between the performance on synthetic data and the performance on real OOD data, showing the validity approach.","Furthermore, we illustrate how synthetic data can be utilized to enhance the calibration and OOD detection capabilities of segmenters."],"url":"http://arxiv.org/abs/2312.09231v1"}
{"created":"2023-12-14 18:55:47","title":"Successor Heads: Recurring, Interpretable Attention Heads In The Wild","abstract":"In this work we present successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days. For example, successor heads increment 'Monday' into 'Tuesday'. We explain the successor head behavior with an approach rooted in mechanistic interpretability, the field that aims to explain how models complete tasks in human-understandable terms. Existing research in this area has found interpretable language model components in small toy models. However, results in toy models have not yet led to insights that explain the internals of frontier models and little is currently understood about the internal operations of large language models. In this paper, we analyze the behavior of successor heads in large language models (LLMs) and find that they implement abstract representations that are common to different architectures. They form in LLMs with as few as 31 million parameters, and at least as many as 12 billion parameters, such as GPT-2, Pythia, and Llama-2. We find a set of 'mod-10 features' that underlie how successor heads increment in LLMs across different architectures and sizes. We perform vector arithmetic with these features to edit head behavior and provide insights into numeric representations within LLMs. Additionally, we study the behavior of successor heads on natural language data, identifying interpretable polysemanticity in a Pythia successor head.","sentences":["In this work we present successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days.","For example, successor heads increment 'Monday' into 'Tuesday'.","We explain the successor head behavior with an approach rooted in mechanistic interpretability, the field that aims to explain how models complete tasks in human-understandable terms.","Existing research in this area has found interpretable language model components in small toy models.","However, results in toy models have not yet led to insights that explain the internals of frontier models and little is currently understood about the internal operations of large language models.","In this paper, we analyze the behavior of successor heads in large language models (LLMs) and find that they implement abstract representations that are common to different architectures.","They form in LLMs with as few as 31 million parameters, and at least as many as 12 billion parameters, such as GPT-2, Pythia, and Llama-2.","We find a set of 'mod-10 features' that underlie how successor heads increment in LLMs across different architectures and sizes.","We perform vector arithmetic with these features to edit head behavior and provide insights into numeric representations within LLMs.","Additionally, we study the behavior of successor heads on natural language data, identifying interpretable polysemanticity in a Pythia successor head."],"url":"http://arxiv.org/abs/2312.09230v1"}
{"created":"2023-12-14 18:54:32","title":"3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting","abstract":"We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.","sentences":["We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS).","Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time.","Recently, the community has explored fast grid structures for efficient training of clothed avatars.","Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS.","In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS).","Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses.","Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively."],"url":"http://arxiv.org/abs/2312.09228v1"}
{"created":"2023-12-14 18:52:52","title":"Mosaic-SDF for 3D Generative Models","abstract":"Current diffusion or flow-based generative models for 3D shapes divide to two: distilling pre-trained 2D image diffusion models, and training directly on 3D shapes. When training a diffusion or flow models on 3D shapes a crucial design choice is the shape representation. An effective shape representation needs to adhere three design principles: it should allow an efficient conversion of large 3D datasets to the representation form; it should provide a good tradeoff of approximation power versus number of parameters; and it should have a simple tensorial form that is compatible with existing powerful neural architectures. While standard 3D shape representations such as volumetric grids and point clouds do not adhere to all these principles simultaneously, we advocate in this paper a new representation that does. We introduce Mosaic-SDF (M-SDF): a simple 3D shape representation that approximates the Signed Distance Function (SDF) of a given shape by using a set of local grids spread near the shape's boundary. The M-SDF representation is fast to compute for each shape individually making it readily parallelizable; it is parameter efficient as it only covers the space around the shape's boundary; and it has a simple matrix form, compatible with Transformer-based architectures. We demonstrate the efficacy of the M-SDF representation by using it to train a 3D generative flow model including class-conditioned generation with the 3D Warehouse dataset, and text-to-3D generation using a dataset of about 600k caption-shape pairs.","sentences":["Current diffusion or flow-based generative models for 3D shapes divide to two: distilling pre-trained 2D image diffusion models, and training directly on 3D shapes.","When training a diffusion or flow models on 3D shapes a crucial design choice is the shape representation.","An effective shape representation needs to adhere three design principles: it should allow an efficient conversion of large 3D datasets to the representation form; it should provide a good tradeoff of approximation power versus number of parameters; and it should have a simple tensorial form that is compatible with existing powerful neural architectures.","While standard 3D shape representations such as volumetric grids and point clouds do not adhere to all these principles simultaneously, we advocate in this paper a new representation that does.","We introduce Mosaic-SDF (M-SDF): a simple 3D shape representation that approximates the Signed Distance Function (SDF) of a given shape by using a set of local grids spread near the shape's boundary.","The M-SDF representation is fast to compute for each shape individually making it readily parallelizable; it is parameter efficient as it only covers the space around the shape's boundary; and it has a simple matrix form, compatible with Transformer-based architectures.","We demonstrate the efficacy of the M-SDF representation by using it to train a 3D generative flow model including class-conditioned generation with the 3D Warehouse dataset, and text-to-3D generation using a dataset of about 600k caption-shape pairs."],"url":"http://arxiv.org/abs/2312.09222v1"}
{"created":"2023-12-14 18:49:30","title":"NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning","abstract":"Reasoning with knowledge graphs (KGs) has primarily focused on triple-shaped facts. Recent advancements have been explored to enhance the semantics of these facts by incorporating more potent representations, such as hyper-relational facts. However, these approaches are limited to \\emph{atomic facts}, which describe a single piece of information. This paper extends beyond \\emph{atomic facts} and delves into \\emph{nested facts}, represented by quoted triples where subjects and objects are triples themselves (e.g., ((\\emph{BarackObama}, \\emph{holds\\_position}, \\emph{President}), \\emph{succeed\\_by}, (\\emph{DonaldTrump}, \\emph{holds\\_position}, \\emph{President}))). These nested facts enable the expression of complex semantics like \\emph{situations} over time and \\emph{logical patterns} over entities and relations. In response, we introduce NestE, a novel KG embedding approach that captures the semantics of both atomic and nested factual knowledge. NestE represents each atomic fact as a $1\\times3$ matrix, and each nested relation is modeled as a $3\\times3$ matrix that rotates the $1\\times3$ atomic fact matrix through matrix multiplication. Each element of the matrix is represented as a complex number in the generalized 4D hypercomplex space, including (spherical) quaternions, hyperbolic quaternions, and split-quaternions. Through thorough analysis, we demonstrate the embedding's efficacy in capturing diverse logical patterns over nested facts, surpassing the confines of first-order logic-like expressions. Our experimental results showcase NestE's significant performance gains over current baselines in triple prediction and conditional link prediction. The code and pre-trained models are open available at https://github.com/xiongbo010/NestE.","sentences":["Reasoning with knowledge graphs (KGs) has primarily focused on triple-shaped facts.","Recent advancements have been explored to enhance the semantics of these facts by incorporating more potent representations, such as hyper-relational facts.","However, these approaches are limited to \\emph{atomic facts}, which describe a single piece of information.","This paper extends beyond \\emph{atomic facts} and delves into \\emph{nested facts}, represented by quoted triples where subjects and objects are triples themselves (e.g., ((\\emph{BarackObama}, \\emph{holds\\_position}, \\emph{President}), \\emph{succeed\\_by}, (\\emph{DonaldTrump}, \\emph{holds\\_position}, \\emph{President}))).","These nested facts enable the expression of complex semantics like \\emph{situations} over time and \\emph{logical patterns} over entities and relations.","In response, we introduce NestE, a novel KG embedding approach that captures the semantics of both atomic and nested factual knowledge.","NestE represents each atomic fact as a $1\\times3$ matrix, and each nested relation is modeled as a $3\\times3$ matrix that rotates the $1\\times3$ atomic fact matrix through matrix multiplication.","Each element of the matrix is represented as a complex number in the generalized 4D hypercomplex space, including (spherical) quaternions, hyperbolic quaternions, and split-quaternions.","Through thorough analysis, we demonstrate the embedding's efficacy in capturing diverse logical patterns over nested facts, surpassing the confines of first-order logic-like expressions.","Our experimental results showcase NestE's significant performance gains over current baselines in triple prediction and conditional link prediction.","The code and pre-trained models are open available at https://github.com/xiongbo010/NestE."],"url":"http://arxiv.org/abs/2312.09219v1"}
{"created":"2023-12-14 18:41:32","title":"Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models","abstract":"Low-precision fine-tuning of language models has gained prominence as a cost-effective and energy-efficient approach to deploying large-scale models in various applications. However, this approach is susceptible to the existence of outlier values in activation. The outlier values in the activation can negatively affect the performance of fine-tuning language models in the low-precision regime since they affect the scaling factor and thus make representing smaller values harder. This paper investigates techniques for mitigating outlier activation in low-precision integer fine-tuning of the language models. Our proposed novel approach enables us to represent the outlier activation values in 8-bit integers instead of floating-point (FP16) values. The benefit of using integers for outlier values is that it enables us to use operator tiling to avoid performing 16-bit integer matrix multiplication to address this problem effectively. We provide theoretical analysis and supporting experiments to demonstrate the effectiveness of our approach in improving the robustness and performance of low-precision fine-tuned language models.","sentences":["Low-precision fine-tuning of language models has gained prominence as a cost-effective and energy-efficient approach to deploying large-scale models in various applications.","However, this approach is susceptible to the existence of outlier values in activation.","The outlier values in the activation can negatively affect the performance of fine-tuning language models in the low-precision regime since they affect the scaling factor and thus make representing smaller values harder.","This paper investigates techniques for mitigating outlier activation in low-precision integer fine-tuning of the language models.","Our proposed novel approach enables us to represent the outlier activation values in 8-bit integers instead of floating-point (FP16) values.","The benefit of using integers for outlier values is that it enables us to use operator tiling to avoid performing 16-bit integer matrix multiplication to address this problem effectively.","We provide theoretical analysis and supporting experiments to demonstrate the effectiveness of our approach in improving the robustness and performance of low-precision fine-tuned language models."],"url":"http://arxiv.org/abs/2312.09211v1"}
{"created":"2023-12-14 18:38:02","title":"WikiMuTe: A web-sourced dataset of semantic descriptions for music audio","abstract":"Multi-modal deep learning techniques for matching free-form text with music have shown promising results in the field of Music Information Retrieval (MIR). Prior work is often based on large proprietary data while publicly available datasets are few and small in size. In this study, we present WikiMuTe, a new and open dataset containing rich semantic descriptions of music. The data is sourced from Wikipedia's rich catalogue of articles covering musical works. Using a dedicated text-mining pipeline, we extract both long and short-form descriptions covering a wide range of topics related to music content such as genre, style, mood, instrumentation, and tempo. To show the use of this data, we train a model that jointly learns text and audio representations and performs cross-modal retrieval. The model is evaluated on two tasks: tag-based music retrieval and music auto-tagging. The results show that while our approach has state-of-the-art performance on multiple tasks, but still observe a difference in performance depending on the data used for training.","sentences":["Multi-modal deep learning techniques for matching free-form text with music have shown promising results in the field of Music Information Retrieval (MIR).","Prior work is often based on large proprietary data while publicly available datasets are few and small in size.","In this study, we present WikiMuTe, a new and open dataset containing rich semantic descriptions of music.","The data is sourced from Wikipedia's rich catalogue of articles covering musical works.","Using a dedicated text-mining pipeline, we extract both long and short-form descriptions covering a wide range of topics related to music content such as genre, style, mood, instrumentation, and tempo.","To show the use of this data, we train a model that jointly learns text and audio representations and performs cross-modal retrieval.","The model is evaluated on two tasks: tag-based music retrieval and music auto-tagging.","The results show that while our approach has state-of-the-art performance on multiple tasks, but still observe a difference in performance depending on the data used for training."],"url":"http://arxiv.org/abs/2312.09207v1"}
{"created":"2023-12-14 18:34:06","title":"Measurement in the Age of LLMs: An Application to Ideological Scaling","abstract":"Much of social science is centered around terms like ``ideology'' or ``power'', which generally elude precise definition, and whose contextual meanings are trapped in surrounding language. This paper explores the use of large language models (LLMs) to flexibly navigate the conceptual clutter inherent to social scientific measurement tasks. We rely on LLMs' remarkable linguistic fluency to elicit ideological scales of both legislators and text, which accord closely to established methods and our own judgement. A key aspect of our approach is that we elicit such scores directly, instructing the LLM to furnish numeric scores itself. This approach affords a great deal of flexibility, which we showcase through a variety of different case studies. Our results suggest that LLMs can be used to characterize highly subtle and diffuse manifestations of political ideology in text.","sentences":["Much of social science is centered around terms like ``ideology'' or ``power'', which generally elude precise definition, and whose contextual meanings are trapped in surrounding language.","This paper explores the use of large language models (LLMs) to flexibly navigate the conceptual clutter inherent to social scientific measurement tasks.","We rely on LLMs' remarkable linguistic fluency to elicit ideological scales of both legislators and text, which accord closely to established methods and our own judgement.","A key aspect of our approach is that we elicit such scores directly, instructing the LLM to furnish numeric scores itself.","This approach affords a great deal of flexibility, which we showcase through a variety of different case studies.","Our results suggest that LLMs can be used to characterize highly subtle and diffuse manifestations of political ideology in text."],"url":"http://arxiv.org/abs/2312.09203v1"}
{"created":"2023-12-14 18:20:59","title":"Weaving Pathways for Justice with GPT: LLM-driven automated drafting of interactive legal applications","abstract":"Can generative AI help us speed up the authoring of tools to help self-represented litigants?   In this paper, we describe 3 approaches to automating the completion of court forms: a generative AI approach that uses GPT-3 to iteratively prompt the user to answer questions, a constrained template-driven approach that uses GPT-4-turbo to generate a draft of questions that are subject to human review, and a hybrid method. We use the open source Docassemble platform in all 3 experiments, together with a tool created at Suffolk University Law School called the Assembly Line Weaver. We conclude that the hybrid model of constrained automated drafting with human review is best suited to the task of authoring guided interviews.","sentences":["Can generative AI help us speed up the authoring of tools to help self-represented litigants?   ","In this paper, we describe 3 approaches to automating the completion of court forms: a generative AI approach that uses GPT-3 to iteratively prompt the user to answer questions, a constrained template-driven approach that uses GPT-4-turbo to generate a draft of questions that are subject to human review, and a hybrid method.","We use the open source Docassemble platform in all 3 experiments, together with a tool created at Suffolk University Law School called the Assembly Line Weaver.","We conclude that the hybrid model of constrained automated drafting with human review is best suited to the task of authoring guided interviews."],"url":"http://arxiv.org/abs/2312.09198v1"}
{"created":"2023-12-14 18:18:34","title":"DIRECT: Deep Active Learning under Imbalance and Label Noise","abstract":"Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. In this work, we propose a novel algorithm that first identifies the class separation threshold and then annotate the most uncertain examples from the minority classes, close to the separation threshold. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. Compared to existing algorithms, our algorithm saves more than 15\\% of the annotation budget compared to state-of-art active learning algorithm and more than 90\\% of annotation budget compared to random sampling.","sentences":["Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes.","With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation.","In this work, we propose a novel algorithm that first identifies the class separation threshold and then annotate the most uncertain examples from the minority classes, close to the separation threshold.","Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise.","Compared to existing algorithms, our algorithm saves more than 15\\% of the annotation budget compared to state-of-art active learning algorithm and more than 90\\% of annotation budget compared to random sampling."],"url":"http://arxiv.org/abs/2312.09196v1"}
{"created":"2023-12-14 18:14:11","title":"Fast Sampling via De-randomization for Discrete Diffusion Models","abstract":"Diffusion models have emerged as powerful tools for high-quality data generation, such as image generation. Despite its success in continuous spaces, discrete diffusion models, which apply to domains such as texts and natural languages, remain under-studied and often suffer from slow generation speed. In this paper, we propose a novel de-randomized diffusion process, which leads to an accelerated algorithm for discrete diffusion models. Our technique significantly reduces the number of function evaluations (i.e., calls to the neural network), making the sampling process much faster. Furthermore, we introduce a continuous-time (i.e., infinite-step) sampling algorithm that can provide even better sample qualities than its discrete-time (finite-step) counterpart. Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality over existing methods for discrete diffusion models.","sentences":["Diffusion models have emerged as powerful tools for high-quality data generation, such as image generation.","Despite its success in continuous spaces, discrete diffusion models, which apply to domains such as texts and natural languages, remain under-studied and often suffer from slow generation speed.","In this paper, we propose a novel de-randomized diffusion process, which leads to an accelerated algorithm for discrete diffusion models.","Our technique significantly reduces the number of function evaluations (i.e., calls to the neural network), making the sampling process much faster.","Furthermore, we introduce a continuous-time (i.e., infinite-step) sampling algorithm that can provide even better sample qualities than its discrete-time (finite-step) counterpart.","Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality over existing methods for discrete diffusion models."],"url":"http://arxiv.org/abs/2312.09193v1"}
{"created":"2023-12-14 18:11:34","title":"Efficient Online Learning of Contact Force Models for Connector Insertion","abstract":"Contact-rich manipulation tasks with stiff frictional elements like connector insertion are difficult to model with rigid-body simulators. In this work, we propose a new approach for modeling these environments by learning a quasi-static contact force model instead of a full simulator. Using a feature vector that contains information about the configuration and control, we find a linear mapping adequately captures the relationship between this feature vector and the sensed contact forces. A novel Linear Model Learning (LML) algorithm is used to solve for the globally optimal mapping in real time without any matrix inversions, resulting in an algorithm that runs in nearly constant time on a GPU as the model size increases. We validate the proposed approach for connector insertion both in simulation and hardware experiments, where the learned model is combined with an optimization-based controller to achieve smooth insertions in the presence of misalignments and uncertainty. Our website featuring videos, code, and more materials is available at https://model-based-plugging.github.io/.","sentences":["Contact-rich manipulation tasks with stiff frictional elements like connector insertion are difficult to model with rigid-body simulators.","In this work, we propose a new approach for modeling these environments by learning a quasi-static contact force model instead of a full simulator.","Using a feature vector that contains information about the configuration and control, we find a linear mapping adequately captures the relationship between this feature vector and the sensed contact forces.","A novel Linear Model Learning (LML) algorithm is used to solve for the globally optimal mapping in real time without any matrix inversions, resulting in an algorithm that runs in nearly constant time on a GPU as the model size increases.","We validate the proposed approach for connector insertion both in simulation and hardware experiments, where the learned model is combined with an optimization-based controller to achieve smooth insertions in the presence of misalignments and uncertainty.","Our website featuring videos, code, and more materials is available at https://model-based-plugging.github.io/."],"url":"http://arxiv.org/abs/2312.09190v1"}
{"created":"2023-12-14 18:06:17","title":"Vision-Language Models as a Source of Rewards","abstract":"Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.","sentences":["Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning.","A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals.","We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents.","We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals.","We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents."],"url":"http://arxiv.org/abs/2312.09187v1"}
{"created":"2023-12-14 17:48:09","title":"Improving Efficiency of Diffusion Models via Multi-Stage Framework and Tailored Multi-Decoder Architectures","abstract":"Diffusion models, emerging as powerful deep generative tools, excel in various applications. They operate through a two-steps process: introducing noise into training samples and then employing a model to convert random noise into new samples (e.g., images). However, their remarkable generative performance is hindered by slow training and sampling. This is due to the necessity of tracking extensive forward and reverse diffusion trajectories, and employing a large model with numerous parameters across multiple timesteps (i.e., noise levels). To tackle these challenges, we present a multi-stage framework inspired by our empirical findings. These observations indicate the advantages of employing distinct parameters tailored to each timestep while retaining universal parameters shared across all time steps. Our approach involves segmenting the time interval into multiple stages where we employ custom multi-decoder U-net architecture that blends time-dependent models with a universally shared encoder. Our framework enables the efficient distribution of computational resources and mitigates inter-stage interference, which substantially improves training efficiency. Extensive numerical experiments affirm the effectiveness of our framework, showcasing significant training and sampling efficiency enhancements on three state-of-the-art diffusion models, including large-scale latent diffusion models. Furthermore, our ablation studies illustrate the impact of two important components in our framework: (i) a novel timestep clustering algorithm for stage division, and (ii) an innovative multi-decoder U-net architecture, seamlessly integrating universal and customized hyperparameters.","sentences":["Diffusion models, emerging as powerful deep generative tools, excel in various applications.","They operate through a two-steps process: introducing noise into training samples and then employing a model to convert random noise into new samples (e.g., images).","However, their remarkable generative performance is hindered by slow training and sampling.","This is due to the necessity of tracking extensive forward and reverse diffusion trajectories, and employing a large model with numerous parameters across multiple timesteps (i.e., noise levels).","To tackle these challenges, we present a multi-stage framework inspired by our empirical findings.","These observations indicate the advantages of employing distinct parameters tailored to each timestep while retaining universal parameters shared across all time steps.","Our approach involves segmenting the time interval into multiple stages where we employ custom multi-decoder U-net architecture that blends time-dependent models with a universally shared encoder.","Our framework enables the efficient distribution of computational resources and mitigates inter-stage interference, which substantially improves training efficiency.","Extensive numerical experiments affirm the effectiveness of our framework, showcasing significant training and sampling efficiency enhancements on three state-of-the-art diffusion models, including large-scale latent diffusion models.","Furthermore, our ablation studies illustrate the impact of two important components in our framework: (i) a novel timestep clustering algorithm for stage division, and (ii) an innovative multi-decoder U-net architecture, seamlessly integrating universal and customized hyperparameters."],"url":"http://arxiv.org/abs/2312.09181v1"}
{"created":"2023-12-14 17:39:58","title":"Safe Motion Planning for Quadruped Robots Using Density Functions","abstract":"This paper presents a motion planning algorithm for quadruped locomotion based on density functions. We decompose the locomotion problem into a high-level density planner and a model predictive controller (MPC). Due to density functions having a physical interpretation through the notion of occupancy, it is intuitive to represent the environment with safety constraints. Hence, there is an ease of use to constructing the planning problem with density. The proposed method uses a simplified model of the robot into an integrator system, where the high-level plan is in a feedback form formulated through an analytically constructed density function. We then use the MPC to optimize the reference trajectory, in which a low-level PID controller is used to obtain the torque level control. The overall framework is implemented in simulation, demonstrating our feedback density planner for legged locomotion. The implementation of work is available at \\url{https://github.com/AndrewZheng-1011/legged_planner}","sentences":["This paper presents a motion planning algorithm for quadruped locomotion based on density functions.","We decompose the locomotion problem into a high-level density planner and a model predictive controller (MPC).","Due to density functions having a physical interpretation through the notion of occupancy, it is intuitive to represent the environment with safety constraints.","Hence, there is an ease of use to constructing the planning problem with density.","The proposed method uses a simplified model of the robot into an integrator system, where the high-level plan is in a feedback form formulated through an analytically constructed density function.","We then use the MPC to optimize the reference trajectory, in which a low-level PID controller is used to obtain the torque level control.","The overall framework is implemented in simulation, demonstrating our feedback density planner for legged locomotion.","The implementation of work is available at \\url{https://github.com/AndrewZheng-1011/legged_planner}"],"url":"http://arxiv.org/abs/2312.09173v1"}
{"created":"2023-12-14 17:34:53","title":"DiffusionLight: Light Probes for Free by Painting a Chrome Ball","abstract":"We present a simple yet effective technique to estimate lighting in a single input image. Current techniques rely heavily on HDR panorama datasets to train neural networks to regress an input with limited field-of-view to a full environment map. However, these approaches often struggle with real-world, uncontrolled settings due to the limited diversity and size of their datasets. To address this problem, we leverage diffusion models trained on billions of standard images to render a chrome ball into the input image. Despite its simplicity, this task remains challenging: the diffusion models often insert incorrect or inconsistent objects and cannot readily generate images in HDR format. Our research uncovers a surprising relationship between the appearance of chrome balls and the initial diffusion noise map, which we utilize to consistently generate high-quality chrome balls. We further fine-tune an LDR difusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure bracketing for HDR light estimation. Our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios.","sentences":["We present a simple yet effective technique to estimate lighting in a single input image.","Current techniques rely heavily on HDR panorama datasets to train neural networks to regress an input with limited field-of-view to a full environment map.","However, these approaches often struggle with real-world, uncontrolled settings due to the limited diversity and size of their datasets.","To address this problem, we leverage diffusion models trained on billions of standard images to render a chrome ball into the input image.","Despite its simplicity, this task remains challenging: the diffusion models often insert incorrect or inconsistent objects and cannot readily generate images in HDR format.","Our research uncovers a surprising relationship between the appearance of chrome balls and the initial diffusion noise map, which we utilize to consistently generate high-quality chrome balls.","We further fine-tune an LDR difusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure bracketing for HDR light estimation.","Our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios."],"url":"http://arxiv.org/abs/2312.09168v1"}
{"created":"2023-12-14 17:33:42","title":"Maximizing Nash Social Welfare under Two-Sided Preferences","abstract":"The maximum Nash social welfare (NSW) -- which maximizes the geometric mean of agents' utilities -- is a fundamental solution concept with remarkable fairness and efficiency guarantees. The computational aspects of NSW have been extensively studied for one-sided preferences where a set of agents have preferences over a set of resources. Our work deviates from this trend and studies NSW maximization for two-sided preferences, wherein a set of workers and firms, each having a cardinal valuation function, are matched with each other. We provide a systematic study of the computational complexity of maximizing NSW for many-to-one matchings under two-sided preferences. Our main negative result is that maximizing NSW is NP-hard even in a highly restricted setting where each firm has capacity 2, all valuations are in the range {0,1,2}, and each agent positively values at most three other agents. In search of positive results, we develop approximation algorithms as well as parameterized algorithms in terms of natural parameters such as the number of workers, the number of firms, and the firms' capacities. We also provide algorithms for restricted domains such as symmetric binary valuations and bounded degree instances.","sentences":["The maximum Nash social welfare (NSW) -- which maximizes the geometric mean of agents' utilities -- is a fundamental solution concept with remarkable fairness and efficiency guarantees.","The computational aspects of NSW have been extensively studied for one-sided preferences where a set of agents have preferences over a set of resources.","Our work deviates from this trend and studies NSW maximization for two-sided preferences, wherein a set of workers and firms, each having a cardinal valuation function, are matched with each other.","We provide a systematic study of the computational complexity of maximizing NSW for many-to-one matchings under two-sided preferences.","Our main negative result is that maximizing NSW is NP-hard even in a highly restricted setting where each firm has capacity 2, all valuations are in the range {0,1,2}, and each agent positively values at most three other agents.","In search of positive results, we develop approximation algorithms as well as parameterized algorithms in terms of natural parameters such as the number of workers, the number of firms, and the firms' capacities.","We also provide algorithms for restricted domains such as symmetric binary valuations and bounded degree instances."],"url":"http://arxiv.org/abs/2312.09167v1"}
{"created":"2023-12-14 17:31:38","title":"Approximation Algorithms for Preference Aggregation Using CP-Nets","abstract":"This paper studies the design and analysis of approximation algorithms for aggregating preferences over combinatorial domains, represented using Conditional Preference Networks (CP-nets). Its focus is on aggregating preferences over so-called \\emph{swaps}, for which optimal solutions in general are already known to be of exponential size. We first analyze a trivial 2-approximation algorithm that simply outputs the best of the given input preferences, and establish a structural condition under which the approximation ratio of this algorithm is improved to $4/3$. We then propose a polynomial-time approximation algorithm whose outputs are provably no worse than those of the trivial algorithm, but often substantially better. A family of problem instances is presented for which our improved algorithm produces optimal solutions, while, for any $\\varepsilon$, the trivial algorithm can\\emph{not}\\/ attain a $(2-\\varepsilon)$-approximation. These results may lead to the first polynomial-time approximation algorithm that solves the CP-net aggregation problem for swaps with an approximation ratio substantially better than $2$.","sentences":["This paper studies the design and analysis of approximation algorithms for aggregating preferences over combinatorial domains, represented using Conditional Preference Networks (CP-nets).","Its focus is on aggregating preferences over so-called \\emph{swaps}, for which optimal solutions in general are already known to be of exponential size.","We first analyze a trivial 2-approximation algorithm that simply outputs the best of the given input preferences, and establish a structural condition under which the approximation ratio of this algorithm is improved to $4/3$. We then propose a polynomial-time approximation algorithm whose outputs are provably no worse than those of the trivial algorithm, but often substantially better.","A family of problem instances is presented for which our improved algorithm produces optimal solutions, while, for any $\\varepsilon$, the trivial algorithm can\\emph{not}\\/ attain a $(2-\\varepsilon)$-approximation.","These results may lead to the first polynomial-time approximation algorithm that solves the CP-net aggregation problem for swaps with an approximation ratio substantially better than $2$."],"url":"http://arxiv.org/abs/2312.09162v1"}
{"created":"2023-12-14 17:29:26","title":"WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views","abstract":"We present the Wildland-fire Infrared Thermal (WIT-UAS) dataset for long-wave infrared sensing of crew and vehicle assets amidst prescribed wildland fire environments. While such a dataset is crucial for safety monitoring in wildland fire applications, to the authors' awareness, no such dataset focusing on assets near fire is publicly available. Presumably, this is due to the barrier to entry of collaborating with fire management personnel. We present two related data subsets: WIT-UAS-ROS consists of full ROS bag files containing sensor and robot data of UAS flight over the fire, and WIT-UAS-Image contains hand-labeled long-wave infrared (LWIR) images extracted from WIT-UAS-ROS. Our dataset is the first to focus on asset detection in a wildland fire environment. We show that thermal detection models trained without fire data frequently detect false positives by classifying fire as people. By adding our dataset to training, we show that the false positive rate is reduced significantly. Yet asset detection in wildland fire environments is still significantly more challenging than detection in urban environments, due to dense obscuring trees, greater heat variation, and overbearing thermal signal of the fire. We publicize this dataset to encourage the community to study more advanced models to tackle this challenging environment. The dataset, code and pretrained models are available at \\url{https://github.com/castacks/WIT-UAS-Dataset}.","sentences":["We present the Wildland-fire Infrared Thermal (WIT-UAS) dataset for long-wave infrared sensing of crew and vehicle assets amidst prescribed wildland fire environments.","While such a dataset is crucial for safety monitoring in wildland fire applications, to the authors' awareness, no such dataset focusing on assets near fire is publicly available.","Presumably, this is due to the barrier to entry of collaborating with fire management personnel.","We present two related data subsets: WIT-UAS-ROS consists of full ROS bag files containing sensor and robot data of UAS flight over the fire, and WIT-UAS-Image contains hand-labeled long-wave infrared (LWIR) images extracted from WIT-UAS-ROS.","Our dataset is the first to focus on asset detection in a wildland fire environment.","We show that thermal detection models trained without fire data frequently detect false positives by classifying fire as people.","By adding our dataset to training, we show that the false positive rate is reduced significantly.","Yet asset detection in wildland fire environments is still significantly more challenging than detection in urban environments, due to dense obscuring trees, greater heat variation, and overbearing thermal signal of the fire.","We publicize this dataset to encourage the community to study more advanced models to tackle this challenging environment.","The dataset, code and pretrained models are available at \\url{https://github.com/castacks/WIT-UAS-Dataset}."],"url":"http://arxiv.org/abs/2312.09159v1"}
{"created":"2023-12-14 17:29:26","title":"Architecture Singularity Distance Computations for Linear Pentapods","abstract":"The kinematic/robotic community is not only interested in measuring the closeness of a given robot configuration to its next singular one but also in a geometric meaningful index evaluating how far the robot design is away from being architecturally singular. Such an architecture singularity distance, which can be used by engineers as a criterion within the design process, is presented for a certain class of parallel manipulators of Stewart-Gough type; namely so-called linear pentapods. Geometrically the architecture singular designs are well-understood and can be subclassified into several cases, which allows to solve the optimization problem of computing the closest architecture singular design to a given linear pentapod with algorithms from numerical algebraic geometry.","sentences":["The kinematic/robotic community is not only interested in measuring the closeness of a given robot configuration to its next singular one but also in a geometric meaningful index evaluating how far the robot design is away from being architecturally singular.","Such an architecture singularity distance, which can be used by engineers as a criterion within the design process, is presented for a certain class of parallel manipulators of Stewart-Gough type; namely so-called linear pentapods.","Geometrically the architecture singular designs are well-understood and can be subclassified into several cases, which allows to solve the optimization problem of computing the closest architecture singular design to a given linear pentapod with algorithms from numerical algebraic geometry."],"url":"http://arxiv.org/abs/2312.09160v1"}
{"created":"2023-12-14 17:26:00","title":"General Object Foundation Model for Images and Videos at Scale","abstract":"We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos. Through a unified framework, GLEE accomplishes detection, segmentation, tracking, grounding, and identification of arbitrary objects in the open world scenario for various object perception tasks. Adopting a cohesive learning strategy, GLEE acquires knowledge from diverse data sources with varying supervision levels to formulate general object representations, excelling in zero-shot transfer to new data and tasks. Specifically, we employ an image encoder, text encoder, and visual prompter to handle multi-modal inputs, enabling to simultaneously solve various object-centric downstream tasks while maintaining state-of-the-art performance. Demonstrated through extensive training on over five million images from diverse benchmarks, GLEE exhibits remarkable versatility and improved generalization performance, efficiently tackling downstream tasks without the need for task-specific adaptation. By integrating large volumes of automatically labeled data, we further enhance its zero-shot generalization capabilities. Additionally, GLEE is capable of being integrated into Large Language Models, serving as a foundational model to provide universal object-level information for multi-modal tasks. We hope that the versatility and universality of our method will mark a significant step in the development of efficient visual foundation models for AGI systems. The model and code will be released at https://glee-vision.github.io .","sentences":["We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos.","Through a unified framework, GLEE accomplishes detection, segmentation, tracking, grounding, and identification of arbitrary objects in the open world scenario for various object perception tasks.","Adopting a cohesive learning strategy, GLEE acquires knowledge from diverse data sources with varying supervision levels to formulate general object representations, excelling in zero-shot transfer to new data and tasks.","Specifically, we employ an image encoder, text encoder, and visual prompter to handle multi-modal inputs, enabling to simultaneously solve various object-centric downstream tasks while maintaining state-of-the-art performance.","Demonstrated through extensive training on over five million images from diverse benchmarks, GLEE exhibits remarkable versatility and improved generalization performance, efficiently tackling downstream tasks without the need for task-specific adaptation.","By integrating large volumes of automatically labeled data, we further enhance its zero-shot generalization capabilities.","Additionally, GLEE is capable of being integrated into Large Language Models, serving as a foundational model to provide universal object-level information for multi-modal tasks.","We hope that the versatility and universality of our method will mark a significant step in the development of efficient visual foundation models for AGI systems.","The model and code will be released at https://glee-vision.github.io ."],"url":"http://arxiv.org/abs/2312.09158v1"}
{"created":"2023-12-14 17:23:16","title":"CMG-Net: Robust Normal Estimation for Point Clouds via Chamfer Normal Distance and Multi-scale Geometry","abstract":"This work presents an accurate and robust method for estimating normals from point clouds. In contrast to predecessor approaches that minimize the deviations between the annotated and the predicted normals directly, leading to direction inconsistency, we first propose a new metric termed Chamfer Normal Distance to address this issue. This not only mitigates the challenge but also facilitates network training and substantially enhances the network robustness against noise. Subsequently, we devise an innovative architecture that encompasses Multi-scale Local Feature Aggregation and Hierarchical Geometric Information Fusion. This design empowers the network to capture intricate geometric details more effectively and alleviate the ambiguity in scale selection. Extensive experiments demonstrate that our method achieves the state-of-the-art performance on both synthetic and real-world datasets, particularly in scenarios contaminated by noise. Our implementation is available at https://github.com/YingruiWoo/CMG-Net_Pytorch.","sentences":["This work presents an accurate and robust method for estimating normals from point clouds.","In contrast to predecessor approaches that minimize the deviations between the annotated and the predicted normals directly, leading to direction inconsistency, we first propose a new metric termed Chamfer Normal Distance to address this issue.","This not only mitigates the challenge but also facilitates network training and substantially enhances the network robustness against noise.","Subsequently, we devise an innovative architecture that encompasses Multi-scale Local Feature Aggregation and Hierarchical Geometric Information Fusion.","This design empowers the network to capture intricate geometric details more effectively and alleviate the ambiguity in scale selection.","Extensive experiments demonstrate that our method achieves the state-of-the-art performance on both synthetic and real-world datasets, particularly in scenarios contaminated by noise.","Our implementation is available at https://github.com/YingruiWoo/CMG-Net_Pytorch."],"url":"http://arxiv.org/abs/2312.09154v1"}
{"created":"2023-12-14 17:22:22","title":"Evaluating Augmented Reality Communication: How Can We Teach Procedural Skill in AR?","abstract":"Augmented reality (AR) has great potential for use in healthcare applications, especially remote medical training and supervision. In this paper, we analyze the usage of an AR communication system to teach a medical procedure, the placement of a central venous catheter (CVC) under ultrasound guidance. We examine various AR communication and collaboration components, including gestural communication, volumetric information, annotations, augmented objects, and augmented screens. We compare how teaching in AR differs from teaching through videoconferencing-based communication. Our results include a detailed medical training steps analysis in which we compare how verbal and visual communication differs between video and AR training. We identify procedural steps in which medical experts give visual instructions utilizing AR components. We examine the change in AR usage and interaction over time and recognize patterns between users. Moreover, AR design recommendations are given based on post-training interviews.","sentences":["Augmented reality (AR) has great potential for use in healthcare applications, especially remote medical training and supervision.","In this paper, we analyze the usage of an AR communication system to teach a medical procedure, the placement of a central venous catheter (CVC) under ultrasound guidance.","We examine various AR communication and collaboration components, including gestural communication, volumetric information, annotations, augmented objects, and augmented screens.","We compare how teaching in AR differs from teaching through videoconferencing-based communication.","Our results include a detailed medical training steps analysis in which we compare how verbal and visual communication differs between video and AR training.","We identify procedural steps in which medical experts give visual instructions utilizing AR components.","We examine the change in AR usage and interaction over time and recognize patterns between users.","Moreover, AR design recommendations are given based on post-training interviews."],"url":"http://arxiv.org/abs/2312.09152v1"}
{"created":"2023-12-14 17:21:15","title":"CPR Emergency Assistance Through Mixed Reality Communication","abstract":"We design and evaluate a mixed reality real-time communication system for remote assistance during CPR emergencies. Our system allows an expert to guide a first responder, remotely, on how to give first aid. RGBD cameras capture a volumetric view of the local scene including the patient, the first responder, and the environment. The volumetric capture is augmented onto the remote expert's view to spatially guide the first responder using visual and verbal instructions. We evaluate the mixed reality communication system in a research study in which participants face a simulated emergency. The first responder moves the patient to the recovery position and performs chest compressions as well as mouth-to-mask ventilation. Our study compares mixed reality against videoconferencing-based assistance using CPR performance measures, cognitive workload surveys, and semi-structured interviews. We find that more visual communication including gestures and objects is used by the remote expert when assisting in mixed reality compared to videoconferencing. Moreover, the performance and the workload of the first responder during simulation do not differ significantly between the two technologies.","sentences":["We design and evaluate a mixed reality real-time communication system for remote assistance during CPR emergencies.","Our system allows an expert to guide a first responder, remotely, on how to give first aid.","RGBD cameras capture a volumetric view of the local scene including the patient, the first responder, and the environment.","The volumetric capture is augmented onto the remote expert's view to spatially guide the first responder using visual and verbal instructions.","We evaluate the mixed reality communication system in a research study in which participants face a simulated emergency.","The first responder moves the patient to the recovery position and performs chest compressions as well as mouth-to-mask ventilation.","Our study compares mixed reality against videoconferencing-based assistance using CPR performance measures, cognitive workload surveys, and semi-structured interviews.","We find that more visual communication including gestures and objects is used by the remote expert when assisting in mixed reality compared to videoconferencing.","Moreover, the performance and the workload of the first responder during simulation do not differ significantly between the two technologies."],"url":"http://arxiv.org/abs/2312.09150v1"}
{"created":"2023-12-14 17:18:44","title":"Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting","abstract":"Uncertainty estimation is crucial for machine learning models to detect out-of-distribution (OOD) inputs. However, the conventional discriminative deep learning classifiers produce uncalibrated closed-set predictions for OOD data. A more robust classifiers with the uncertainty estimation typically require a potentially unavailable OOD dataset for outlier exposure training, or a considerable amount of additional memory and compute to build ensemble models. In this work, we improve on uncertainty estimation without extra OOD data or additional inference costs using an alternative Split-Ensemble method. Specifically, we propose a novel subtask-splitting ensemble training objective, where a common multiclass classification task is split into several complementary subtasks. Then, each subtask's training data can be considered as OOD to the other subtasks. Diverse submodels can therefore be trained on each subtask with OOD-aware objectives. The subtask-splitting objective enables us to share low-level features across submodels to avoid parameter and computational overheads. In particular, we build a tree-like Split-Ensemble architecture by performing iterative splitting and pruning from a shared backbone model, where each branch serves as a submodel corresponding to a subtask. This leads to improved accuracy and uncertainty estimation across submodels under a fixed ensemble computation budget. Empirical study with ResNet-18 backbone shows Split-Ensemble, without additional computation cost, improves accuracy over a single model by 0.8%, 1.8%, and 25.5% on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively. OOD detection for the same backbone and in-distribution datasets surpasses a single model baseline by, correspondingly, 2.2%, 8.1%, and 29.6% mean AUROC. Codes will be publicly available at https://antonioo-c.github.io/projects/split-ensemble","sentences":["Uncertainty estimation is crucial for machine learning models to detect out-of-distribution (OOD) inputs.","However, the conventional discriminative deep learning classifiers produce uncalibrated closed-set predictions for OOD data.","A more robust classifiers with the uncertainty estimation typically require a potentially unavailable OOD dataset for outlier exposure training, or a considerable amount of additional memory and compute to build ensemble models.","In this work, we improve on uncertainty estimation without extra OOD data or additional inference costs using an alternative Split-Ensemble method.","Specifically, we propose a novel subtask-splitting ensemble training objective, where a common multiclass classification task is split into several complementary subtasks.","Then, each subtask's training data can be considered as OOD to the other subtasks.","Diverse submodels can therefore be trained on each subtask with OOD-aware objectives.","The subtask-splitting objective enables us to share low-level features across submodels to avoid parameter and computational overheads.","In particular, we build a tree-like Split-Ensemble architecture by performing iterative splitting and pruning from a shared backbone model, where each branch serves as a submodel corresponding to a subtask.","This leads to improved accuracy and uncertainty estimation across submodels under a fixed ensemble computation budget.","Empirical study with ResNet-18 backbone shows Split-Ensemble, without additional computation cost, improves accuracy over a single model by 0.8%, 1.8%, and 25.5% on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively.","OOD detection for the same backbone and in-distribution datasets surpasses a single model baseline by, correspondingly, 2.2%, 8.1%, and 29.6% mean AUROC.","Codes will be publicly available at https://antonioo-c.github.io/projects/split-ensemble"],"url":"http://arxiv.org/abs/2312.09148v1"}
{"created":"2023-12-14 17:18:34","title":"Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers","abstract":"Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques. Please see our project page at https://zouzx.github.io/TriplaneGaussian/.","sentences":["Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models.","Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain.","Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times.","In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference.","Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation.","This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations.","The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point.","This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature.","Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting.","Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets.","The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques.","Please see our project page at https://zouzx.github.io/TriplaneGaussian/."],"url":"http://arxiv.org/abs/2312.09147v1"}
{"created":"2023-12-14 17:13:07","title":"F1-EV Score: Measuring the Likelihood of Estimating a Good Decision Threshold for Semi-Supervised Anomaly Detection","abstract":"Anomalous sound detection (ASD) systems are usually compared by using threshold-independent performance measures such as AUC-ROC. However, for practical applications a decision threshold is needed to decide whether a given test sample is normal or anomalous. Estimating such a threshold is highly non-trivial in a semi-supervised setting where only normal training samples are available. In this work, F1-EV a novel threshold-independent performance measure for ASD systems that also includes the likelihood of estimating a good decision threshold is proposed and motivated using specific toy examples. In experimental evaluations, multiple performance measures are evaluated for all systems submitted to the ASD task of the DCASE Challenge 2023. It is shown that F1-EV is strongly correlated with AUC-ROC while having a significantly stronger correlation with the F1-score obtained with estimated and optimal decision thresholds than AUC-ROC.","sentences":["Anomalous sound detection (ASD) systems are usually compared by using threshold-independent performance measures such as AUC-ROC.","However, for practical applications a decision threshold is needed to decide whether a given test sample is normal or anomalous.","Estimating such a threshold is highly non-trivial in a semi-supervised setting where only normal training samples are available.","In this work, F1-EV a novel threshold-independent performance measure for ASD systems that also includes the likelihood of estimating a good decision threshold is proposed and motivated using specific toy examples.","In experimental evaluations, multiple performance measures are evaluated for all systems submitted to the ASD task of the DCASE Challenge 2023.","It is shown that F1-EV is strongly correlated with AUC-ROC while having a significantly stronger correlation with the F1-score obtained with estimated and optimal decision thresholds than AUC-ROC."],"url":"http://arxiv.org/abs/2312.09143v1"}
{"created":"2023-12-14 17:10:09","title":"Class-Wise Buffer Management for Incremental Object Detection: An Effective Buffer Training Strategy","abstract":"Class incremental learning aims to solve a problem that arises when continuously adding unseen class instances to an existing model This approach has been extensively studied in the context of image classification; however its applicability to object detection is not well established yet. Existing frameworks using replay methods mainly collect replay data without considering the model being trained and tend to rely on randomness or the number of labels of each sample. Also, despite the effectiveness of the replay, it was not yet optimized for the object detection task. In this paper, we introduce an effective buffer training strategy (eBTS) that creates the optimized replay buffer on object detection. Our approach incorporates guarantee minimum and hierarchical sampling to establish the buffer customized to the trained model. %These methods can facilitate effective retrieval of prior knowledge. Furthermore, we use the circular experience replay training to optimally utilize the accumulated buffer data. Experiments on the MS COCO dataset demonstrate that our eBTS achieves state-of-the-art performance compared to the existing replay schemes.","sentences":["Class incremental learning aims to solve a problem that arises when continuously adding unseen class instances to an existing model This approach has been extensively studied in the context of image classification; however its applicability to object detection is not well established yet.","Existing frameworks using replay methods mainly collect replay data without considering the model being trained and tend to rely on randomness or the number of labels of each sample.","Also, despite the effectiveness of the replay, it was not yet optimized for the object detection task.","In this paper, we introduce an effective buffer training strategy (eBTS) that creates the optimized replay buffer on object detection.","Our approach incorporates guarantee minimum and hierarchical sampling to establish the buffer customized to the trained model.","%These methods can facilitate effective retrieval of prior knowledge.","Furthermore, we use the circular experience replay training to optimally utilize the accumulated buffer data.","Experiments on the MS COCO dataset demonstrate that our eBTS achieves state-of-the-art performance compared to the existing replay schemes."],"url":"http://arxiv.org/abs/2312.09139v1"}
{"created":"2023-12-14 17:09:57","title":"Living Scenes: Multi-object Relocalization and Reconstruction in Changing 3D Environments","abstract":"Research into dynamic 3D scene understanding has primarily focused on short-term change tracking from dense observations, while little attention has been paid to long-term changes with sparse observations. We address this gap with MoRE, a novel approach for multi-object relocalization and reconstruction in evolving environments. We view these environments as \"living scenes\" and consider the problem of transforming scans taken at different points in time into a 3D reconstruction of the object instances, whose accuracy and completeness increase over time. At the core of our method lies an SE(3)-equivariant representation in a single encoder-decoder network, trained on synthetic data. This representation enables us to seamlessly tackle instance matching, registration, and reconstruction. We also introduce a joint optimization algorithm that facilitates the accumulation of point clouds originating from the same instance across multiple scans taken at different points in time. We validate our method on synthetic and real-world data and demonstrate state-of-the-art performance in both end-to-end performance and individual subtasks.","sentences":["Research into dynamic 3D scene understanding has primarily focused on short-term change tracking from dense observations, while little attention has been paid to long-term changes with sparse observations.","We address this gap with MoRE, a novel approach for multi-object relocalization and reconstruction in evolving environments.","We view these environments as \"living scenes\" and consider the problem of transforming scans taken at different points in time into a 3D reconstruction of the object instances, whose accuracy and completeness increase over time.","At the core of our method lies an SE(3)-equivariant representation in a single encoder-decoder network, trained on synthetic data.","This representation enables us to seamlessly tackle instance matching, registration, and reconstruction.","We also introduce a joint optimization algorithm that facilitates the accumulation of point clouds originating from the same instance across multiple scans taken at different points in time.","We validate our method on synthetic and real-world data and demonstrate state-of-the-art performance in both end-to-end performance and individual subtasks."],"url":"http://arxiv.org/abs/2312.09138v1"}
{"created":"2023-12-14 17:01:02","title":"Tokenize Anything via Prompting","abstract":"We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, e.g., SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 150.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of perception tasks. Code and models are available at https://github.com/baaivision/tokenize-anything.","sentences":["We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything.","Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting.","To achieve this, we train a generalizable model with massive segmentation masks, e.g., SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters.","Specifically, we construct a promptable image decoder by adding a semantic token to each mask token.","The semantic token is responsible for learning the semantic priors in a predefined concept space.","Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities.","For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 150.7 on the Visual Genome region captioning task.","We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of perception tasks.","Code and models are available at https://github.com/baaivision/tokenize-anything."],"url":"http://arxiv.org/abs/2312.09128v1"}
{"created":"2023-12-14 16:59:49","title":"Towards Trustworthy AI Software Development Assistance","abstract":"It is expected that in the near future, AI software development assistants will play an important role in the software industry. However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code. We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants. In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness. The LLM will make use of graph-based code representations for advanced semantic comprehension. We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations. Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for correctness and security) hold for the generated code.","sentences":["It is expected that in the near future, AI software development assistants will play an important role in the software industry.","However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code.","We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants.","In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness.","The LLM will make use of graph-based code representations for advanced semantic comprehension.","We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations.","Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for correctness and security) hold for the generated code."],"url":"http://arxiv.org/abs/2312.09126v1"}
{"created":"2023-12-14 16:58:55","title":"Puppy: A Publicly Verifiable Watermarking Protocol","abstract":"In this paper, we propose Puppy, the first formally defined framework for converting any symmetric watermarking into a publicly verifiable one. Puppy allows anyone to verify a watermark any number of times with the help of an untrusted third party, without requiring owner presence during detection. We formally define and prove security of Puppy using the ideal/real-world simulation paradigm and construct two practical and secure instances: (1) Puppy-TEE that uses Trusted Execution Environments (TEEs), and (2) Puppy-2PC that relies on two-party computation (2PC) based on garbled circuits. We then convert four current symmetric watermarking schemes into publicly verifiable ones and run extensive experiments using Puppy-TEE and Puppy-2PC. Evaluation results show that, while Puppy-TEE incurs some overhead, its total latency is on the order of milliseconds for three out of four watermarking schemes. Although the overhead of Puppy-2PC is higher (on the order of seconds), it is viable for settings that lack a TEE or where strong trust assumptions about a TEE need to be avoided. We further optimize the solution to increase its scalability and resilience to denial of service attacks via memoization.","sentences":["In this paper, we propose Puppy, the first formally defined framework for converting any symmetric watermarking into a publicly verifiable one.","Puppy allows anyone to verify a watermark any number of times with the help of an untrusted third party, without requiring owner presence during detection.","We formally define and prove security of Puppy using the ideal/real-world simulation paradigm and construct two practical and secure instances: (1) Puppy-TEE that uses Trusted Execution Environments (TEEs), and (2) Puppy-2PC that relies on two-party computation (2PC) based on garbled circuits.","We then convert four current symmetric watermarking schemes into publicly verifiable ones and run extensive experiments using Puppy-TEE and Puppy-2PC.","Evaluation results show that, while Puppy-TEE incurs some overhead, its total latency is on the order of milliseconds for three out of four watermarking schemes.","Although the overhead of Puppy-2PC is higher (on the order of seconds), it is viable for settings that lack a TEE or where strong trust assumptions about a TEE need to be avoided.","We further optimize the solution to increase its scalability and resilience to denial of service attacks via memoization."],"url":"http://arxiv.org/abs/2312.09125v1"}
{"created":"2023-12-14 16:58:18","title":"MRL-PoS: A Multi-agent Reinforcement Learning based Proof of Stake Consensus Algorithm for Blockchain","abstract":"The core of a blockchain network is its consensus algorithm. Starting with the Proof-of-Work, there have been various versions of consensus algorithms, such as Proof-of-Stake (PoS), Proof-of-Authority (PoA), and Practical Byzantine Fault Tolerance (PBFT). Each of these algorithms focuses on different aspects to ensure efficient and reliable processing of transactions. Blockchain operates in a decentralized manner where there is no central authority and the network is composed of diverse users. This openness creates the potential for malicious nodes to disrupt the network in various ways. Therefore, it is crucial to embed a mechanism within the blockchain network to constantly monitor, identify, and eliminate these malicious nodes. However, there is no one-size-fits-all mechanism to identify all malicious nodes. Hence, the dynamic adaptability of the blockchain network is important to maintain security and reliability at all times. This paper introduces MRL-PoS, a Proof-of-Stake consensus algorithm based on multi-agent reinforcement learning. MRL-PoS employs reinforcement learning for dynamically adjusting to the behavior of all users. It incorporates a system of rewards and penalties to eliminate malicious nodes and incentivize honest ones. Additionally, MRL-PoS has the capability to learn and respond to new malicious tactics by continually training its agents.","sentences":["The core of a blockchain network is its consensus algorithm.","Starting with the Proof-of-Work, there have been various versions of consensus algorithms, such as Proof-of-Stake (PoS), Proof-of-Authority (PoA), and Practical Byzantine Fault Tolerance (PBFT).","Each of these algorithms focuses on different aspects to ensure efficient and reliable processing of transactions.","Blockchain operates in a decentralized manner where there is no central authority and the network is composed of diverse users.","This openness creates the potential for malicious nodes to disrupt the network in various ways.","Therefore, it is crucial to embed a mechanism within the blockchain network to constantly monitor, identify, and eliminate these malicious nodes.","However, there is no one-size-fits-all mechanism to identify all malicious nodes.","Hence, the dynamic adaptability of the blockchain network is important to maintain security and reliability at all times.","This paper introduces MRL-PoS, a Proof-of-Stake consensus algorithm based on multi-agent reinforcement learning.","MRL-PoS employs reinforcement learning for dynamically adjusting to the behavior of all users.","It incorporates a system of rewards and penalties to eliminate malicious nodes and incentivize honest ones.","Additionally, MRL-PoS has the capability to learn and respond to new malicious tactics by continually training its agents."],"url":"http://arxiv.org/abs/2312.09123v1"}
{"created":"2023-12-14 16:54:37","title":"Less is more -- the Dispatcher/ Executor principle for multi-task Reinforcement Learning","abstract":"Humans instinctively know how to neglect details when it comes to solve complex decision making problems in environments with unforeseeable variations. This abstraction process seems to be a vital property for most biological systems and helps to 'abstract away' unnecessary details and boost generalisation. In this work we introduce the dispatcher/ executor principle for the design of multi-task Reinforcement Learning controllers. It suggests to partition the controller in two entities, one that understands the task (the dispatcher) and one that computes the controls for the specific device (the executor) - and to connect these two by a strongly regularizing communication channel. The core rationale behind this position paper is that changes in structure and design principles can improve generalisation properties and drastically enforce data-efficiency. It is in some sense a 'yes, and ...' response to the current trend of using large neural networks trained on vast amounts of data and bet on emerging generalisation properties. While we agree on the power of scaling - in the sense of Sutton's 'bitter lesson' - we will give some evidence, that considering structure and adding design principles can be a valuable and critical component in particular when data is not abundant and infinite, but is a precious resource.","sentences":["Humans instinctively know how to neglect details when it comes to solve complex decision making problems in environments with unforeseeable variations.","This abstraction process seems to be a vital property for most biological systems and helps to 'abstract away' unnecessary details and boost generalisation.","In this work we introduce the dispatcher/ executor principle for the design of multi-task Reinforcement Learning controllers.","It suggests to partition the controller in two entities, one that understands the task (the dispatcher) and one that computes the controls for the specific device (the executor) - and to connect these two by a strongly regularizing communication channel.","The core rationale behind this position paper is that changes in structure and design principles can improve generalisation properties and drastically enforce data-efficiency.","It is in some sense a 'yes, and ...' response to the current trend of using large neural networks trained on vast amounts of data and bet on emerging generalisation properties.","While we agree on the power of scaling - in the sense of Sutton's 'bitter lesson' - we will give some evidence, that considering structure and adding design principles can be a valuable and critical component in particular when data is not abundant and infinite, but is a precious resource."],"url":"http://arxiv.org/abs/2312.09120v1"}
{"created":"2023-12-14 16:53:55","title":"Stability in Online Coalition Formation","abstract":"Coalition formation is concerned with the question of how to partition a set of agents into disjoint coalitions according to their preferences. Deviating from most of the previous work, we consider an online variant of the problem, where agents arrive in sequence and whenever an agent arrives, they have to be assigned to a coalition immediately and irrevocably. The scarce existing literature on online coalition formation has focused on the objective of maximizing social welfare, a demanding requirement, even in the offline setting. Instead, we seek to achieve stable coalition structures in an online setting, and focus on stability concepts based on deviations by single agents. We present a comprehensive picture in additively separable hedonic games, leading to dichotomies, where positive results are obtained by deterministic algorithms and negative results even hold for randomized algorithms.","sentences":["Coalition formation is concerned with the question of how to partition a set of agents into disjoint coalitions according to their preferences.","Deviating from most of the previous work, we consider an online variant of the problem, where agents arrive in sequence and whenever an agent arrives, they have to be assigned to a coalition immediately and irrevocably.","The scarce existing literature on online coalition formation has focused on the objective of maximizing social welfare, a demanding requirement, even in the offline setting.","Instead, we seek to achieve stable coalition structures in an online setting, and focus on stability concepts based on deviations by single agents.","We present a comprehensive picture in additively separable hedonic games, leading to dichotomies, where positive results are obtained by deterministic algorithms and negative results even hold for randomized algorithms."],"url":"http://arxiv.org/abs/2312.09119v1"}
{"created":"2023-12-14 16:50:42","title":"LayerZero","abstract":"In this paper, we present the first instrinsically secure and semantically universal omnichain interoperability protocol: LayerZero. Utilizing an immutable endpoint, append-only verification modules, and fully-configurable verification infrastructure, LayerZero provides the security, configurability, and extensibility necessary to achieve omnichain interoperability. LayerZero enforces strict application-exclusive ownership of protocol security and cost through its novel trust-minimized modular security framework which is designed to universally support all blockchains and use cases. Omnichain applications (OApps) built on the LayerZero protocol achieve frictionless blockchain-agnostic interoperation through LayerZero's universal network semantics.","sentences":["In this paper, we present the first instrinsically secure and semantically universal omnichain interoperability protocol: LayerZero.","Utilizing an immutable endpoint, append-only verification modules, and fully-configurable verification infrastructure, LayerZero provides the security, configurability, and extensibility necessary to achieve omnichain interoperability.","LayerZero enforces strict application-exclusive ownership of protocol security and cost through its novel trust-minimized modular security framework which is designed to universally support all blockchains and use cases.","Omnichain applications (OApps) built on the LayerZero protocol achieve frictionless blockchain-agnostic interoperation through LayerZero's universal network semantics."],"url":"http://arxiv.org/abs/2312.09118v1"}
{"created":"2023-12-14 16:45:36","title":"VideoLCM: Video Latent Consistency Model","abstract":"Consistency models have demonstrated powerful capability in efficient image generation and allowed synthesis within a few sampling steps, alleviating the high computational cost in diffusion models. However, the consistency model in the more challenging and resource-consuming video generation is still less explored. In this report, we present the VideoLCM framework to fill this gap, which leverages the concept of consistency models from image generation to efficiently synthesize videos with minimal steps while maintaining high quality. VideoLCM builds upon existing latent video diffusion models and incorporates consistency distillation techniques for training the latent consistency model. Experimental results reveal the effectiveness of our VideoLCM in terms of computational efficiency, fidelity and temporal consistency. Notably, VideoLCM achieves high-fidelity and smooth video synthesis with only four sampling steps, showcasing the potential for real-time synthesis. We hope that VideoLCM can serve as a simple yet effective baseline for subsequent research. The source code and models will be publicly available.","sentences":["Consistency models have demonstrated powerful capability in efficient image generation and allowed synthesis within a few sampling steps, alleviating the high computational cost in diffusion models.","However, the consistency model in the more challenging and resource-consuming video generation is still less explored.","In this report, we present the VideoLCM framework to fill this gap, which leverages the concept of consistency models from image generation to efficiently synthesize videos with minimal steps while maintaining high quality.","VideoLCM builds upon existing latent video diffusion models and incorporates consistency distillation techniques for training the latent consistency model.","Experimental results reveal the effectiveness of our VideoLCM in terms of computational efficiency, fidelity and temporal consistency.","Notably, VideoLCM achieves high-fidelity and smooth video synthesis with only four sampling steps, showcasing the potential for real-time synthesis.","We hope that VideoLCM can serve as a simple yet effective baseline for subsequent research.","The source code and models will be publicly available."],"url":"http://arxiv.org/abs/2312.09109v1"}
{"created":"2023-12-14 16:44:38","title":"Greedy Shapley Client Selection for Communication-Efficient Federated Learning","abstract":"The standard client selection algorithms for Federated Learning (FL) are often unbiased and involve uniform random sampling of clients. This has been proven sub-optimal for fast convergence under practical settings characterized by significant heterogeneity in data distribution and computing and communication resources across clients. For applications having timing constraints due to limited communication opportunities, the client selection strategy is critical to complete model training within the fixed budget of communication rounds. To address this, we develop a biased client selection strategy, GreedyFed that identifies and greedily selects the most contributing clients in each communication round. This method builds on a fast approximation algorithm for the Shapley Value at the parameter server (PS), making the computation tractable for real-world applications with many clients. Compared to various client selection strategies on several real-world datasets, GreedyFed demonstrates fast and stable convergence with high accuracy under timing constraints and a higher degree of heterogeneity in data distribution, systems constraints, and privacy requirements.","sentences":["The standard client selection algorithms for Federated Learning (FL) are often unbiased and involve uniform random sampling of clients.","This has been proven sub-optimal for fast convergence under practical settings characterized by significant heterogeneity in data distribution and computing and communication resources across clients.","For applications having timing constraints due to limited communication opportunities, the client selection strategy is critical to complete model training within the fixed budget of communication rounds.","To address this, we develop a biased client selection strategy, GreedyFed that identifies and greedily selects the most contributing clients in each communication round.","This method builds on a fast approximation algorithm for the Shapley Value at the parameter server (PS), making the computation tractable for real-world applications with many clients.","Compared to various client selection strategies on several real-world datasets, GreedyFed demonstrates fast and stable convergence with high accuracy under timing constraints and a higher degree of heterogeneity in data distribution, systems constraints, and privacy requirements."],"url":"http://arxiv.org/abs/2312.09108v1"}
{"created":"2023-12-14 16:44:25","title":"A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata","abstract":"While scientists increasingly recognize the importance of metadata in describing their data, spreadsheets remain the preferred tool for supplying this information despite their limitations in ensuring compliance and quality. Various tools have been developed to address these limitations, but they suffer from their own shortcomings, such as steep learning curves and limited customization. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata while providing rigorous compliance and quality control. Our approach employs several key strategies, including customizable templates for defining metadata, integral support for the use of controlled terminologies when defining these templates, and an interactive Web-based tool that allows users to rapidly identify and fix errors in the spreadsheet-based metadata they supply. We demonstrate how this approach is being deployed in a biomedical consortium to define and collect metadata about scientific experiments.","sentences":["While scientists increasingly recognize the importance of metadata in describing their data, spreadsheets remain the preferred tool for supplying this information despite their limitations in ensuring compliance and quality.","Various tools have been developed to address these limitations, but they suffer from their own shortcomings, such as steep learning curves and limited customization.","In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata while providing rigorous compliance and quality control.","Our approach employs several key strategies, including customizable templates for defining metadata, integral support for the use of controlled terminologies when defining these templates, and an interactive Web-based tool that allows users to rapidly identify and fix errors in the spreadsheet-based metadata they supply.","We demonstrate how this approach is being deployed in a biomedical consortium to define and collect metadata about scientific experiments."],"url":"http://arxiv.org/abs/2312.09107v1"}
{"created":"2023-12-14 16:26:46","title":"ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance Field","abstract":"Neural Radiance Fields (NeRF) have demonstrated impressive potential in synthesizing novel views from dense input, however, their effectiveness is challenged when dealing with sparse input. Existing approaches that incorporate additional depth or semantic supervision can alleviate this issue to an extent. However, the process of supervision collection is not only costly but also potentially inaccurate, leading to poor performance and generalization ability in diverse scenarios. In our work, we introduce a novel model: the Collaborative Neural Radiance Fields (ColNeRF) designed to work with sparse input. The collaboration in ColNeRF includes both the cooperation between sparse input images and the cooperation between the output of the neural radiation field. Through this, we construct a novel collaborative module that aligns information from various views and meanwhile imposes self-supervised constraints to ensure multi-view consistency in both geometry and appearance. A Collaborative Cross-View Volume Integration module (CCVI) is proposed to capture complex occlusions and implicitly infer the spatial location of objects. Moreover, we introduce self-supervision of target rays projected in multiple directions to ensure geometric and color consistency in adjacent regions. Benefiting from the collaboration at the input and output ends, ColNeRF is capable of capturing richer and more generalized scene representation, thereby facilitating higher-quality results of the novel view synthesis. Extensive experiments demonstrate that ColNeRF outperforms state-of-the-art sparse input generalizable NeRF methods. Furthermore, our approach exhibits superiority in fine-tuning towards adapting to new scenes, achieving competitive performance compared to per-scene optimized NeRF-based methods while significantly reducing computational costs. Our code is available at: https://github.com/eezkni/ColNeRF.","sentences":["Neural Radiance Fields (NeRF) have demonstrated impressive potential in synthesizing novel views from dense input, however, their effectiveness is challenged when dealing with sparse input.","Existing approaches that incorporate additional depth or semantic supervision can alleviate this issue to an extent.","However, the process of supervision collection is not only costly but also potentially inaccurate, leading to poor performance and generalization ability in diverse scenarios.","In our work, we introduce a novel model: the Collaborative Neural Radiance Fields (ColNeRF) designed to work with sparse input.","The collaboration in ColNeRF includes both the cooperation between sparse input images and the cooperation between the output of the neural radiation field.","Through this, we construct a novel collaborative module that aligns information from various views and meanwhile imposes self-supervised constraints to ensure multi-view consistency in both geometry and appearance.","A Collaborative Cross-View Volume Integration module (CCVI) is proposed to capture complex occlusions and implicitly infer the spatial location of objects.","Moreover, we introduce self-supervision of target rays projected in multiple directions to ensure geometric and color consistency in adjacent regions.","Benefiting from the collaboration at the input and output ends, ColNeRF is capable of capturing richer and more generalized scene representation, thereby facilitating higher-quality results of the novel view synthesis.","Extensive experiments demonstrate that ColNeRF outperforms state-of-the-art sparse input generalizable NeRF methods.","Furthermore, our approach exhibits superiority in fine-tuning towards adapting to new scenes, achieving competitive performance compared to per-scene optimized NeRF-based methods while significantly reducing computational costs.","Our code is available at: https://github.com/eezkni/ColNeRF."],"url":"http://arxiv.org/abs/2312.09095v1"}
{"created":"2023-12-14 16:26:29","title":"Agent Attention: On the Integration of Softmax and Linear Attention","abstract":"The attention module is the key component in Transformers. While the global attention mechanism offers high expressiveness, its excessive computational cost restricts its applicability in various scenarios. In this paper, we propose a novel attention paradigm, Agent Attention, to strike a favorable balance between computational efficiency and representation power. Specifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$, introduces an additional set of agent tokens $A$ into the conventional attention module. The agent tokens first act as the agent for the query tokens $Q$ to aggregate information from $K$ and $V$, and then broadcast the information back to $Q$. Given the number of agent tokens can be designed to be much smaller than the number of query tokens, the agent attention is significantly more efficient than the widely adopted Softmax attention, while preserving global context modelling capability. Interestingly, we show that the proposed agent attention is equivalent to a generalized form of linear attention. Therefore, agent attention seamlessly integrates the powerful Softmax attention and the highly efficient linear attention. Extensive experiments demonstrate the effectiveness of agent attention with various vision Transformers and across diverse vision tasks, including image classification, object detection, semantic segmentation and image generation. Notably, agent attention has shown remarkable performance in high-resolution scenarios, owning to its linear attention nature. For instance, when applied to Stable Diffusion, our agent attention accelerates generation and substantially enhances image generation quality without any additional training. Code is available at https://github.com/LeapLabTHU/Agent-Attention.","sentences":["The attention module is the key component in Transformers.","While the global attention mechanism offers high expressiveness, its excessive computational cost restricts its applicability in various scenarios.","In this paper, we propose a novel attention paradigm, Agent Attention, to strike a favorable balance between computational efficiency and representation power.","Specifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$, introduces an additional set of agent tokens $A$ into the conventional attention module.","The agent tokens first act as the agent for the query tokens $Q$ to aggregate information from $K$ and $V$, and then broadcast the information back to $Q$. Given the number of agent tokens can be designed to be much smaller than the number of query tokens, the agent attention is significantly more efficient than the widely adopted Softmax attention, while preserving global context modelling capability.","Interestingly, we show that the proposed agent attention is equivalent to a generalized form of linear attention.","Therefore, agent attention seamlessly integrates the powerful Softmax attention and the highly efficient linear attention.","Extensive experiments demonstrate the effectiveness of agent attention with various vision Transformers and across diverse vision tasks, including image classification, object detection, semantic segmentation and image generation.","Notably, agent attention has shown remarkable performance in high-resolution scenarios, owning to its linear attention nature.","For instance, when applied to Stable Diffusion, our agent attention accelerates generation and substantially enhances image generation quality without any additional training.","Code is available at https://github.com/LeapLabTHU/Agent-Attention."],"url":"http://arxiv.org/abs/2312.08874v1"}
{"created":"2023-12-14 16:25:06","title":"Hopf Arborescent Links, Minor Theory, and Decidability of the Genus Defect","abstract":"While the problem of computing the genus of a knot is now fairly well understood, no algorithm is known for its four-dimensional variants, both in the smooth and in the topological locally flat category. In this article, we investigate a class of knots and links called Hopf arborescent links, which are obtained as the boundaries of some iterated plumbings of Hopf bands. We show that for such links, computing the genus defects, which measure how much the four-dimensional genera differ from the classical genus, is decidable. Our proof is non-constructive, and is obtained by proving that Seifert surfaces of Hopf arborescent links under a relation of minors defined by containment of their Seifert surfaces form a well-quasi-order.","sentences":["While the problem of computing the genus of a knot is now fairly well understood, no algorithm is known for its four-dimensional variants, both in the smooth and in the topological locally flat category.","In this article, we investigate a class of knots and links called Hopf arborescent links, which are obtained as the boundaries of some iterated plumbings of Hopf bands.","We show that for such links, computing the genus defects, which measure how much the four-dimensional genera differ from the classical genus, is decidable.","Our proof is non-constructive, and is obtained by proving that Seifert surfaces of Hopf arborescent links under a relation of minors defined by containment of their Seifert surfaces form a well-quasi-order."],"url":"http://arxiv.org/abs/2312.09094v1"}
{"created":"2023-12-14 16:24:09","title":"Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption","abstract":"The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered methodology, entangling the aspects of illumination and material reflectance into emission solely from 3D points. This simplified rendering approach presents challenges in accurately modeling images captured under adverse lighting conditions, such as low light or over-exposure. Motivated by the ancient Greek emission theory that posits visual perception as a result of rays emanating from the eyes, we slightly refine the conventional NeRF framework to train NeRF under challenging light conditions and generate normal-light condition novel views unsupervised. We introduce the concept of a \"Concealing Field,\" which assigns transmittance values to the surrounding air to account for illumination effects. In dark scenarios, we assume that object emissions maintain a standard lighting level but are attenuated as they traverse the air during the rendering process. Concealing Field thus compel NeRF to learn reasonable density and colour estimations for objects even in dimly lit situations. Similarly, the Concealing Field can mitigate over-exposed emissions during the rendering stage. Furthermore, we present a comprehensive multi-view dataset captured under challenging illumination conditions for evaluation. Our code and dataset available at https://github.com/cuiziteng/Aleth-NeRF","sentences":["The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered methodology, entangling the aspects of illumination and material reflectance into emission solely from 3D points.","This simplified rendering approach presents challenges in accurately modeling images captured under adverse lighting conditions, such as low light or over-exposure.","Motivated by the ancient Greek emission theory that posits visual perception as a result of rays emanating from the eyes, we slightly refine the conventional NeRF framework to train NeRF under challenging light conditions and generate normal-light condition novel views unsupervised.","We introduce the concept of a \"Concealing Field,\" which assigns transmittance values to the surrounding air to account for illumination effects.","In dark scenarios, we assume that object emissions maintain a standard lighting level but are attenuated as they traverse the air during the rendering process.","Concealing Field thus compel NeRF to learn reasonable density and colour estimations for objects even in dimly lit situations.","Similarly, the Concealing Field can mitigate over-exposed emissions during the rendering stage.","Furthermore, we present a comprehensive multi-view dataset captured under challenging illumination conditions for evaluation.","Our code and dataset available at https://github.com/cuiziteng/Aleth-NeRF"],"url":"http://arxiv.org/abs/2312.09093v1"}
{"created":"2023-12-14 16:19:00","title":"A Comprehensive Trusted Runtime for WebAssembly with Intel SGX","abstract":"In real-world scenarios, trusted execution environments (TEEs) frequently host applications that lack the trust of the infrastructure provider, as well as data owners who have specifically outsourced their data for remote processing. We present Twine, a trusted runtime for running WebAssembly-compiled applications within TEEs, establishing a two-way sandbox. Twine leverages memory safety guarantees of WebAssembly (Wasm) and abstracts the complexity of TEEs, empowering the execution of legacy and language-agnostic applications. It extends the standard WebAssembly system interface (WASI), providing controlled OS services, focusing on I/O. Additionally, through built-in TEE mechanisms, Twine delivers attestation capabilities to ensure the integrity of the runtime and the OS services supplied to the application. We evaluate its performance using general-purpose benchmarks and real-world applications, showing it compares on par with state-of-the-art solutions. A case study involving fintech company Credora reveals that Twine can be deployed in production with reasonable performance trade-offs, ranging from a 0.7x slowdown to a 1.17x speedup compared to native run time. Finally, we identify performance improvement through library optimisation, showcasing one such adjustment that leads up to 4.1x speedup. Twine is open-source and has been upstreamed into the original Wasm runtime, WAMR.","sentences":["In real-world scenarios, trusted execution environments (TEEs) frequently host applications that lack the trust of the infrastructure provider, as well as data owners who have specifically outsourced their data for remote processing.","We present Twine, a trusted runtime for running WebAssembly-compiled applications within TEEs, establishing a two-way sandbox.","Twine leverages memory safety guarantees of WebAssembly (Wasm) and abstracts the complexity of TEEs, empowering the execution of legacy and language-agnostic applications.","It extends the standard WebAssembly system interface (WASI), providing controlled OS services, focusing on I/O. Additionally, through built-in TEE mechanisms, Twine delivers attestation capabilities to ensure the integrity of the runtime and the OS services supplied to the application.","We evaluate its performance using general-purpose benchmarks and real-world applications, showing it compares on par with state-of-the-art solutions.","A case study involving fintech company Credora reveals that Twine can be deployed in production with reasonable performance trade-offs, ranging from a 0.7x slowdown to a 1.17x speedup compared to native run time.","Finally, we identify performance improvement through library optimisation, showcasing one such adjustment that leads up to 4.1x speedup.","Twine is open-source and has been upstreamed into the original Wasm runtime, WAMR."],"url":"http://arxiv.org/abs/2312.09087v1"}
{"created":"2023-12-14 16:17:20","title":"COMBHelper: A Neural Approach to Reduce Search Space for Graph Combinatorial Problems","abstract":"Combinatorial Optimization (CO) problems over graphs appear routinely in many applications such as in optimizing traffic, viral marketing in social networks, and matching for job allocation. Due to their combinatorial nature, these problems are often NP-hard. Existing approximation algorithms and heuristics rely on the search space to find the solutions and become time-consuming when this space is large. In this paper, we design a neural method called COMBHelper to reduce this space and thus improve the efficiency of the traditional CO algorithms based on node selection. Specifically, it employs a Graph Neural Network (GNN) to identify promising nodes for the solution set. This pruned search space is then fed to the traditional CO algorithms. COMBHelper also uses a Knowledge Distillation (KD) module and a problem-specific boosting module to bring further efficiency and efficacy. Our extensive experiments show that the traditional CO algorithms with COMBHelper are at least 2 times faster than their original versions.","sentences":["Combinatorial Optimization (CO) problems over graphs appear routinely in many applications such as in optimizing traffic, viral marketing in social networks, and matching for job allocation.","Due to their combinatorial nature, these problems are often NP-hard.","Existing approximation algorithms and heuristics rely on the search space to find the solutions and become time-consuming when this space is large.","In this paper, we design a neural method called COMBHelper to reduce this space and thus improve the efficiency of the traditional CO algorithms based on node selection.","Specifically, it employs a Graph Neural Network (GNN) to identify promising nodes for the solution set.","This pruned search space is then fed to the traditional CO algorithms.","COMBHelper also uses a Knowledge Distillation (KD) module and a problem-specific boosting module to bring further efficiency and efficacy.","Our extensive experiments show that the traditional CO algorithms with COMBHelper are at least 2 times faster than their original versions."],"url":"http://arxiv.org/abs/2312.09086v1"}
{"created":"2023-12-14 16:16:50","title":"The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation","abstract":"Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.","sentences":["Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation.","Existing research mainly studied this susceptibility behavior in a single-turn setting.","However, belief can change during a multi-turn conversation, especially a persuasive one.","Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly.","We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation.","Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue.","Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies."],"url":"http://arxiv.org/abs/2312.09085v1"}
{"created":"2023-12-14 16:16:35","title":"Language Modeling on a SpiNNaker 2 Neuromorphic Chip","abstract":"As large language models continue to scale in size rapidly, so too does the computational power required to run them. Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly. However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling. As a result, language modeling on neuromorphic devices has seemed a distant prospect. In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip - based on a recently published event-based architecture called the EGRU. SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance. This implementation marks the first time a neuromorphic language model matches LSTMs, setting the stage for taking task performance to the level of large language models. We also demonstrate results on a gesture recognition task based on inputs from a DVS camera. Overall, our results showcase the feasibility of this neuro-inspired neural network in hardware, highlighting significant gains versus conventional hardware in energy efficiency for the common use case of single batch inference.","sentences":["As large language models continue to scale in size rapidly, so too does the computational power required to run them.","Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly.","However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling.","As a result, language modeling on neuromorphic devices has seemed a distant prospect.","In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip - based on a recently published event-based architecture called the EGRU.","SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance.","This implementation marks the first time a neuromorphic language model matches LSTMs, setting the stage for taking task performance to the level of large language models.","We also demonstrate results on a gesture recognition task based on inputs from a DVS camera.","Overall, our results showcase the feasibility of this neuro-inspired neural network in hardware, highlighting significant gains versus conventional hardware in energy efficiency for the common use case of single batch inference."],"url":"http://arxiv.org/abs/2312.09084v1"}
{"created":"2023-12-14 16:15:31","title":"Learned Fusion: 3D Object Detection using Calibration-Free Transformer Feature Fusion","abstract":"The state of the art in 3D object detection using sensor fusion heavily relies on calibration quality, which is difficult to maintain in large scale deployment outside a lab environment. We present the first calibration-free approach for 3D object detection. Thus, eliminating the need for complex and costly calibration procedures. Our approach uses transformers to map the features between multiple views of different sensors at multiple abstraction levels. In an extensive evaluation for object detection, we not only show that our approach outperforms single modal setups by 14.1% in BEV mAP, but also that the transformer indeed learns mapping. By showing calibration is not necessary for sensor fusion, we hope to motivate other researchers following the direction of calibration-free fusion. Additionally, resulting approaches have a substantial resilience against rotation and translation changes.","sentences":["The state of the art in 3D object detection using sensor fusion heavily relies on calibration quality, which is difficult to maintain in large scale deployment outside a lab environment.","We present the first calibration-free approach for 3D object detection.","Thus, eliminating the need for complex and costly calibration procedures.","Our approach uses transformers to map the features between multiple views of different sensors at multiple abstraction levels.","In an extensive evaluation for object detection, we not only show that our approach outperforms single modal setups by 14.1% in BEV mAP, but also that the transformer indeed learns mapping.","By showing calibration is not necessary for sensor fusion, we hope to motivate other researchers following the direction of calibration-free fusion.","Additionally, resulting approaches have a substantial resilience against rotation and translation changes."],"url":"http://arxiv.org/abs/2312.09082v1"}
{"created":"2023-12-14 16:12:22","title":"Coevolutionary Algorithm for Building Robust Decision Trees under Minimax Regret","abstract":"In recent years, there has been growing interest in developing robust machine learning (ML) models that can withstand adversarial attacks, including one of the most widely adopted, efficient, and interpretable ML algorithms-decision trees (DTs). This paper proposes a novel coevolutionary algorithm (CoEvoRDT) designed to create robust DTs capable of handling noisy high-dimensional data in adversarial contexts. Motivated by the limitations of traditional DT algorithms, we leverage adaptive coevolution to allow DTs to evolve and learn from interactions with perturbed input data. CoEvoRDT alternately evolves competing populations of DTs and perturbed features, enabling construction of DTs with desired properties. CoEvoRDT is easily adaptable to various target metrics, allowing the use of tailored robustness criteria such as minimax regret. Furthermore, CoEvoRDT has potential to improve the results of other state-of-the-art methods by incorporating their outcomes (DTs they produce) into the initial population and optimize them in the process of coevolution. Inspired by the game theory, CoEvoRDT utilizes mixed Nash equilibrium to enhance convergence. The method is tested on 20 popular datasets and shows superior performance compared to 4 state-of-the-art algorithms. It outperformed all competing methods on 13 datasets with adversarial accuracy metrics, and on all 20 considered datasets with minimax regret. Strong experimental results and flexibility in choosing the error measure make CoEvoRDT a promising approach for constructing robust DTs in real-world applications.","sentences":["In recent years, there has been growing interest in developing robust machine learning (ML) models that can withstand adversarial attacks, including one of the most widely adopted, efficient, and interpretable ML algorithms-decision trees (DTs).","This paper proposes a novel coevolutionary algorithm (CoEvoRDT) designed to create robust DTs capable of handling noisy high-dimensional data in adversarial contexts.","Motivated by the limitations of traditional DT algorithms, we leverage adaptive coevolution to allow DTs to evolve and learn from interactions with perturbed input data.","CoEvoRDT alternately evolves competing populations of DTs and perturbed features, enabling construction of DTs with desired properties.","CoEvoRDT is easily adaptable to various target metrics, allowing the use of tailored robustness criteria such as minimax regret.","Furthermore, CoEvoRDT has potential to improve the results of other state-of-the-art methods by incorporating their outcomes (DTs they produce) into the initial population and optimize them in the process of coevolution.","Inspired by the game theory, CoEvoRDT utilizes mixed Nash equilibrium to enhance convergence.","The method is tested on 20 popular datasets and shows superior performance compared to 4 state-of-the-art algorithms.","It outperformed all competing methods on 13 datasets with adversarial accuracy metrics, and on all 20 considered datasets with minimax regret.","Strong experimental results and flexibility in choosing the error measure make CoEvoRDT a promising approach for constructing robust DTs in real-world applications."],"url":"http://arxiv.org/abs/2312.09078v1"}
{"created":"2023-12-14 16:12:16","title":"Entropy Regularization and Faster Decremental Matching in General Graphs","abstract":"We provide an algorithm that maintains, against an adaptive adversary, a $(1-\\varepsilon)$-approximate maximum matching in $n$-node $m$-edge general (not necessarily bipartite) undirected graph undergoing edge deletions with high probability with (amortized) $O(\\mathrm{poly}(\\varepsilon^{-1}, \\log n))$ time per update. We also obtain the same update time for maintaining a fractional approximate weighted matching (and hence an approximation to the value of the maximum weight matching) and an integral approximate weighted matching in dense graphs. Our unweighted result improves upon the prior state-of-the-art which includes a $\\mathrm{poly}(\\log{n}) \\cdot 2^{O(1/\\varepsilon^2)}$ update time [Assadi-Bernstein-Dudeja 2022] and an $O(\\sqrt{m} \\varepsilon^{-2})$ update time [Gupta-Peng 2013], and our weighted result improves upon the $O(\\sqrt{m}\\varepsilon^{-O(1/\\varepsilon)}\\log{n})$ update time due to [Gupta-Peng 2013].   To obtain our results, we generalize a recent optimization approach to dynamic algorithms from [Jambulapati-Jin-Sidford-Tian 2022]. We show that repeatedly solving entropy-regularized optimization problems yields a lazy updating scheme for fractional decremental problems with a near-optimal number of updates. To apply this framework we develop optimization methods compatible with it and new dynamic rounding algorithms for the matching polytope.","sentences":["We provide an algorithm that maintains, against an adaptive adversary, a $(1-\\varepsilon)$-approximate maximum matching in $n$-node $m$-edge general (not necessarily bipartite) undirected graph undergoing edge deletions with high probability with (amortized) $O(\\mathrm{poly}(\\varepsilon^{-1}, \\log n))$ time per update.","We also obtain the same update time for maintaining a fractional approximate weighted matching (and hence an approximation to the value of the maximum weight matching) and an integral approximate weighted matching in dense graphs.","Our unweighted result improves upon the prior state-of-the-art which includes a $\\mathrm{poly}(\\log{n})","\\cdot 2^{O(1/\\varepsilon^2)}$ update time [Assadi-Bernstein-Dudeja 2022] and an $O(\\sqrt{m} \\varepsilon^{-2})$ update time","[Gupta-Peng 2013], and our weighted result improves upon the $O(\\sqrt{m}\\varepsilon^{-O(1/\\varepsilon)}\\log{n})$ update time due to [Gupta-Peng 2013].   ","To obtain our results, we generalize a recent optimization approach to dynamic algorithms from [Jambulapati-Jin-Sidford-Tian 2022].","We show that repeatedly solving entropy-regularized optimization problems yields a lazy updating scheme for fractional decremental problems with a near-optimal number of updates.","To apply this framework we develop optimization methods compatible with it and new dynamic rounding algorithms for the matching polytope."],"url":"http://arxiv.org/abs/2312.09077v1"}
{"created":"2023-12-14 16:11:42","title":"ProSGNeRF: Progressive Dynamic Neural Scene Graph with Frequency Modulated Auto-Encoder in Urban Scenes","abstract":"Implicit neural representation has demonstrated promising results in view synthesis for large and complex scenes. However, existing approaches either fail to capture the fast-moving objects or need to build the scene graph without camera ego-motions, leading to low-quality synthesized views of the scene. We aim to jointly solve the view synthesis problem of large-scale urban scenes and fast-moving vehicles, which is more practical and challenging. To this end, we first leverage a graph structure to learn the local scene representations of dynamic objects and the background. Then, we design a progressive scheme that dynamically allocates a new local scene graph trained with frames within a temporal window, allowing us to scale up the representation to an arbitrarily large scene. Besides, the training views of urban scenes are relatively sparse, which leads to a significant decline in reconstruction accuracy for dynamic objects. Therefore, we design a frequency auto-encoder network to encode the latent code and regularize the frequency range of objects, which can enhance the representation of dynamic objects and address the issue of sparse image inputs. Additionally, we employ lidar point projection to maintain geometry consistency in large-scale urban scenes. Experimental results demonstrate that our method achieves state-of-the-art view synthesis accuracy, object manipulation, and scene roaming ability. The code will be open-sourced upon paper acceptance.","sentences":["Implicit neural representation has demonstrated promising results in view synthesis for large and complex scenes.","However, existing approaches either fail to capture the fast-moving objects or need to build the scene graph without camera ego-motions, leading to low-quality synthesized views of the scene.","We aim to jointly solve the view synthesis problem of large-scale urban scenes and fast-moving vehicles, which is more practical and challenging.","To this end, we first leverage a graph structure to learn the local scene representations of dynamic objects and the background.","Then, we design a progressive scheme that dynamically allocates a new local scene graph trained with frames within a temporal window, allowing us to scale up the representation to an arbitrarily large scene.","Besides, the training views of urban scenes are relatively sparse, which leads to a significant decline in reconstruction accuracy for dynamic objects.","Therefore, we design a frequency auto-encoder network to encode the latent code and regularize the frequency range of objects, which can enhance the representation of dynamic objects and address the issue of sparse image inputs.","Additionally, we employ lidar point projection to maintain geometry consistency in large-scale urban scenes.","Experimental results demonstrate that our method achieves state-of-the-art view synthesis accuracy, object manipulation, and scene roaming ability.","The code will be open-sourced upon paper acceptance."],"url":"http://arxiv.org/abs/2312.09076v1"}
{"created":"2023-12-14 16:10:56","title":"Towards Verifiable Text Generation with Evolving Memory and Self-Reflection","abstract":"Large Language Models (LLMs) face several challenges, including the tendency to produce incorrect outputs, known as hallucination. An effective solution is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the dilemma between the precision and scope in document retrieval, and the intricate reasoning required to discern the relationship between the claim and citations. In this paper, we present VTG, an innovative approach for Verifiable Text Generation with evolving memory and self-reflection. VTG maintains evolving long short-term memory to retain both valuable documents and up-to-date documents. Active retrieval and diverse query generation are utilized to enhance both the precision and scope of the retrieved documents. Furthermore, VTG features a two-tier verifier and an evidence finder, enabling rethinking and reflection on the relationship between the claim and citations. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms existing baselines.","sentences":["Large Language Models (LLMs) face several challenges, including the tendency to produce incorrect outputs, known as hallucination.","An effective solution is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification.","However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the dilemma between the precision and scope in document retrieval, and the intricate reasoning required to discern the relationship between the claim and citations.","In this paper, we present VTG, an innovative approach for Verifiable Text Generation with evolving memory and self-reflection.","VTG maintains evolving long short-term memory to retain both valuable documents and up-to-date documents.","Active retrieval and diverse query generation are utilized to enhance both the precision and scope of the retrieved documents.","Furthermore, VTG features a two-tier verifier and an evidence finder, enabling rethinking and reflection on the relationship between the claim and citations.","We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms existing baselines."],"url":"http://arxiv.org/abs/2312.09075v1"}
{"created":"2023-12-14 16:08:10","title":"Optimal Motion Planning using Finite Fourier Series in a Learning-based Collision Field","abstract":"This paper utilizes finite Fourier series to represent a time-continuous motion and proposes a novel planning method that adjusts the motion harmonics of each manipulator joint. Primarily, we sum the potential energy for collision detection and the kinetic energy up to calculate the Hamiltonian of the manipulator motion harmonics. Though the adaptive interior-point method is designed to modify the harmonics in its finite frequency domain, we still encounter the local minima due to the non-convexity of the collision field. In this way, we learn the collision field through a support vector machine with a Gaussian kernel, which is highly convex. The learning-based collision field is applied for Hamiltonian, and the experiment results show our method's high reliability and efficiency.","sentences":["This paper utilizes finite Fourier series to represent a time-continuous motion and proposes a novel planning method that adjusts the motion harmonics of each manipulator joint.","Primarily, we sum the potential energy for collision detection and the kinetic energy up to calculate the Hamiltonian of the manipulator motion harmonics.","Though the adaptive interior-point method is designed to modify the harmonics in its finite frequency domain, we still encounter the local minima due to the non-convexity of the collision field.","In this way, we learn the collision field through a support vector machine with a Gaussian kernel, which is highly convex.","The learning-based collision field is applied for Hamiltonian, and the experiment results show our method's high reliability and efficiency."],"url":"http://arxiv.org/abs/2312.09073v1"}
{"created":"2023-12-14 16:04:34","title":"PI3D: Efficient Text-to-3D Generation with Pseudo-Image Diffusion","abstract":"In this paper, we introduce PI3D, a novel and efficient framework that utilizes the pre-trained text-to-image diffusion models to generate high-quality 3D shapes in minutes. On the one hand, it fine-tunes a pre-trained 2D diffusion model into a 3D diffusion model, enabling both 3D generative capabilities and generalization derived from the 2D model. On the other, it utilizes score distillation sampling of 2D diffusion models to quickly improve the quality of the sampled 3D shapes. PI3D enables the migration of knowledge from image to triplane generation by treating it as a set of pseudo-images. We adapt the modules in the pre-training model to enable hybrid training using pseudo and real images, which has proved to be a well-established strategy for improving generalizability. The efficiency of PI3D is highlighted by its ability to sample diverse 3D models in seconds and refine them in minutes. The experimental results confirm the advantages of PI3D over existing methods based on either 3D diffusion models or lifting 2D diffusion models in terms of fast generation of 3D consistent and high-quality models. The proposed PI3D stands as a promising advancement in the field of text-to-3D generation, and we hope it will inspire more research into 3D generation leveraging the knowledge in both 2D and 3D data.","sentences":["In this paper, we introduce PI3D, a novel and efficient framework that utilizes the pre-trained text-to-image diffusion models to generate high-quality 3D shapes in minutes.","On the one hand, it fine-tunes a pre-trained 2D diffusion model into a 3D diffusion model, enabling both 3D generative capabilities and generalization derived from the 2D model.","On the other, it utilizes score distillation sampling of 2D diffusion models to quickly improve the quality of the sampled 3D shapes.","PI3D enables the migration of knowledge from image to triplane generation by treating it as a set of pseudo-images.","We adapt the modules in the pre-training model to enable hybrid training using pseudo and real images, which has proved to be a well-established strategy for improving generalizability.","The efficiency of PI3D is highlighted by its ability to sample diverse 3D models in seconds and refine them in minutes.","The experimental results confirm the advantages of PI3D over existing methods based on either 3D diffusion models or lifting 2D diffusion models in terms of fast generation of 3D consistent and high-quality models.","The proposed PI3D stands as a promising advancement in the field of text-to-3D generation, and we hope it will inspire more research into 3D generation leveraging the knowledge in both 2D and 3D data."],"url":"http://arxiv.org/abs/2312.09069v1"}
{"created":"2023-12-14 16:04:14","title":"CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset with High-Quality Labels","abstract":"Online learning is a rapidly growing industry due to its convenience. However, a major challenge in online learning is whether students are as engaged as they are in face-to-face classes. An engagement recognition system can significantly improve the learning experience in online classes. Current challenges in engagement detection involve poor label quality in the dataset, intra-class variation, and extreme data imbalance. To address these problems, we present the CMOSE dataset, which contains a large number of data in different engagement levels and high-quality labels generated according to the psychological advice. We demonstrate the advantage of transferability by analyzing the model performance on other engagement datasets. We also developed a training mechanism, MocoRank, to handle the intra-class variation, the ordinal relationship between different classes, and the data imbalance problem. MocoRank outperforms prior engagement detection losses, achieving a 1.32% enhancement in overall accuracy and 5.05% improvement in average accuracy. We further demonstrate the effectiveness of multi-modality by conducting ablation studies on features such as pre-trained video features, high-level facial features, and audio features.","sentences":["Online learning is a rapidly growing industry due to its convenience.","However, a major challenge in online learning is whether students are as engaged as they are in face-to-face classes.","An engagement recognition system can significantly improve the learning experience in online classes.","Current challenges in engagement detection involve poor label quality in the dataset, intra-class variation, and extreme data imbalance.","To address these problems, we present the CMOSE dataset, which contains a large number of data in different engagement levels and high-quality labels generated according to the psychological advice.","We demonstrate the advantage of transferability by analyzing the model performance on other engagement datasets.","We also developed a training mechanism, MocoRank, to handle the intra-class variation, the ordinal relationship between different classes, and the data imbalance problem.","MocoRank outperforms prior engagement detection losses, achieving a 1.32% enhancement in overall accuracy and 5.05% improvement in average accuracy.","We further demonstrate the effectiveness of multi-modality by conducting ablation studies on features such as pre-trained video features, high-level facial features, and audio features."],"url":"http://arxiv.org/abs/2312.09066v1"}
{"created":"2023-12-14 16:04:14","title":"Holodeck: Language Guided Generation of 3D Embodied AI Environments","abstract":"3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope. To mitigate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fully automatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as \"apartment for a researcher with a cat\" and \"office of a professor who is a fan of Star Wars\". Holodeck leverages a large language model (GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents.","sentences":["3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope.","To mitigate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fully automatedly.","Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as \"apartment for a researcher with a cat\" and \"office of a professor who is a fan of Star Wars\".","Holodeck leverages a large language model (GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects.","To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints.","Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types.","We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents."],"url":"http://arxiv.org/abs/2312.09067v1"}
{"created":"2023-12-14 15:55:53","title":"Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery","abstract":"The substantial success of Vision Transformer (ViT) in computer vision tasks is largely attributed to the architecture design. This underscores the necessity of efficient architecture search for designing better ViTs automatically. As training-based architecture search methods are computationally intensive, there is a growing interest in training-free methods that use zero-cost proxies to score ViTs. However, existing training-free approaches require expert knowledge to manually design specific zero-cost proxies. Moreover, these zero-cost proxies exhibit limitations to generalize across diverse domains. In this paper, we introduce Auto-Prox, an automatic proxy discovery framework, to address the problem. First, we build the ViT-Bench-101, which involves different ViT candidates and their actual performance on multiple datasets. Utilizing ViT-Bench-101, we can evaluate zero-cost proxies based on their score-accuracy correlation. Then, we represent zero-cost proxies with computation graphs and organize the zero-cost proxy search space with ViT statistics and primitive operations. To discover generic zero-cost proxies, we propose a joint correlation metric to evolve and mutate different zero-cost proxy candidates. We introduce an elitism-preserve strategy for search efficiency to achieve a better trade-off between exploitation and exploration. Based on the discovered zero-cost proxy, we conduct a ViT architecture search in a training-free manner. Extensive experiments demonstrate that our method generalizes well to different datasets and achieves state-of-the-art results both in ranking correlation and final accuracy. Codes can be found at https://github.com/lilujunai/Auto-Prox-AAAI24.","sentences":["The substantial success of Vision Transformer (ViT) in computer vision tasks is largely attributed to the architecture design.","This underscores the necessity of efficient architecture search for designing better ViTs automatically.","As training-based architecture search methods are computationally intensive, there is a growing interest in training-free methods that use zero-cost proxies to score ViTs.","However, existing training-free approaches require expert knowledge to manually design specific zero-cost proxies.","Moreover, these zero-cost proxies exhibit limitations to generalize across diverse domains.","In this paper, we introduce Auto-Prox, an automatic proxy discovery framework, to address the problem.","First, we build the ViT-Bench-101, which involves different ViT candidates and their actual performance on multiple datasets.","Utilizing ViT-Bench-101, we can evaluate zero-cost proxies based on their score-accuracy correlation.","Then, we represent zero-cost proxies with computation graphs and organize the zero-cost proxy search space with ViT statistics and primitive operations.","To discover generic zero-cost proxies, we propose a joint correlation metric to evolve and mutate different zero-cost proxy candidates.","We introduce an elitism-preserve strategy for search efficiency to achieve a better trade-off between exploitation and exploration.","Based on the discovered zero-cost proxy, we conduct a ViT architecture search in a training-free manner.","Extensive experiments demonstrate that our method generalizes well to different datasets and achieves state-of-the-art results both in ranking correlation and final accuracy.","Codes can be found at https://github.com/lilujunai/Auto-Prox-AAAI24."],"url":"http://arxiv.org/abs/2312.09059v1"}
{"created":"2023-12-14 15:54:55","title":"Learning Coalition Structures with Games","abstract":"Coalitions naturally exist in many real-world systems involving multiple decision makers such as ridesharing, security, and online ad auctions, but the coalition structure among the agents is often unknown. We propose and study an important yet previously overseen problem -- Coalition Structure Learning (CSL), where we aim to carefully design a series of games for the agents and infer the underlying coalition structure by observing their interactions in those games. We establish a lower bound on the sample complexity -- defined as the number of games needed to learn the structure -- of any algorithms for CSL and propose the Iterative Grouping (IG) algorithm for designing normal-form games to achieve the lower bound. We show that IG can be extended to other succinct games such as congestion games and graphical games. Moreover, we solve CSL in a more restrictive and practical setting: auctions. We show a variant of IG to solve CSL in the auction setting even if we cannot design the bidder valuations. Finally, we conduct experiments to evaluate IG in the auction setting and the results align with our theoretical analysis.","sentences":["Coalitions naturally exist in many real-world systems involving multiple decision makers such as ridesharing, security, and online ad auctions, but the coalition structure among the agents is often unknown.","We propose and study an important yet previously overseen problem -- Coalition Structure Learning (CSL), where we aim to carefully design a series of games for the agents and infer the underlying coalition structure by observing their interactions in those games.","We establish a lower bound on the sample complexity -- defined as the number of games needed to learn the structure -- of any algorithms for CSL and propose the Iterative Grouping (IG) algorithm for designing normal-form games to achieve the lower bound.","We show that IG can be extended to other succinct games such as congestion games and graphical games.","Moreover, we solve CSL in a more restrictive and practical setting: auctions.","We show a variant of IG to solve CSL in the auction setting even if we cannot design the bidder valuations.","Finally, we conduct experiments to evaluate IG in the auction setting and the results align with our theoretical analysis."],"url":"http://arxiv.org/abs/2312.09058v1"}
{"created":"2023-12-14 15:54:52","title":"On the Difficulty of Defending Contrastive Learning against Backdoor Attacks","abstract":"Recent studies have shown that contrastive learning, like supervised learning, is highly vulnerable to backdoor attacks wherein malicious functions are injected into target models, only to be activated by specific triggers. However, thus far it remains under-explored how contrastive backdoor attacks fundamentally differ from their supervised counterparts, which impedes the development of effective defenses against the emerging threat.   This work represents a solid step toward answering this critical question. Specifically, we define TRL, a unified framework that encompasses both supervised and contrastive backdoor attacks. Through the lens of TRL, we uncover that the two types of attacks operate through distinctive mechanisms: in supervised attacks, the learning of benign and backdoor tasks tends to occur independently, while in contrastive attacks, the two tasks are deeply intertwined both in their representations and throughout their learning processes. This distinction leads to the disparate learning dynamics and feature distributions of supervised and contrastive attacks. More importantly, we reveal that the specificities of contrastive backdoor attacks entail important implications from a defense perspective: existing defenses for supervised attacks are often inadequate and not easily retrofitted to contrastive attacks. We also explore several alternative defenses and discuss their potential challenges. Our findings highlight the need for defenses tailored to the specificities of contrastive backdoor attacks, pointing to promising directions for future research.","sentences":["Recent studies have shown that contrastive learning, like supervised learning, is highly vulnerable to backdoor attacks wherein malicious functions are injected into target models, only to be activated by specific triggers.","However, thus far it remains under-explored how contrastive backdoor attacks fundamentally differ from their supervised counterparts, which impedes the development of effective defenses against the emerging threat.   ","This work represents a solid step toward answering this critical question.","Specifically, we define TRL, a unified framework that encompasses both supervised and contrastive backdoor attacks.","Through the lens of TRL, we uncover that the two types of attacks operate through distinctive mechanisms: in supervised attacks, the learning of benign and backdoor tasks tends to occur independently, while in contrastive attacks, the two tasks are deeply intertwined both in their representations and throughout their learning processes.","This distinction leads to the disparate learning dynamics and feature distributions of supervised and contrastive attacks.","More importantly, we reveal that the specificities of contrastive backdoor attacks entail important implications from a defense perspective: existing defenses for supervised attacks are often inadequate and not easily retrofitted to contrastive attacks.","We also explore several alternative defenses and discuss their potential challenges.","Our findings highlight the need for defenses tailored to the specificities of contrastive backdoor attacks, pointing to promising directions for future research."],"url":"http://arxiv.org/abs/2312.09057v1"}
