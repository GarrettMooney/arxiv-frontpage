{"created":"2023-11-28 18:59:58","title":"Mission-driven Exploration for Accelerated Deep Reinforcement Learning with Temporal Logic Task Specifications","abstract":"This paper addresses the problem of designing optimal control policies for mobile robots with mission and safety requirements specified using Linear Temporal Logic (LTL). We consider robots with unknown stochastic dynamics operating in environments with unknown geometric structure. The robots are equipped with sensors allowing them to detect obstacles. Our goal is to synthesize a control policy that maximizes the probability of satisfying an LTL-encoded task in the presence of motion and environmental uncertainty. Several deep reinforcement learning (DRL) algorithms have been proposed recently to address similar problems. A common limitation in related works is that of slow learning performance. In order to address this issue, we propose a novel DRL algorithm, which has the capability to learn control policies at a notably faster rate compared to similar methods. Its sample efficiency is due to a mission-driven exploration strategy that prioritizes exploration towards directions that may contribute to mission accomplishment. Identifying these directions relies on an automaton representation of the LTL task as well as a learned neural network that (partially) models the unknown system dynamics. We provide comparative experiments demonstrating the efficiency of our algorithm on robot navigation tasks in unknown environments.","sentences":["This paper addresses the problem of designing optimal control policies for mobile robots with mission and safety requirements specified using Linear Temporal Logic (LTL).","We consider robots with unknown stochastic dynamics operating in environments with unknown geometric structure.","The robots are equipped with sensors allowing them to detect obstacles.","Our goal is to synthesize a control policy that maximizes the probability of satisfying an LTL-encoded task in the presence of motion and environmental uncertainty.","Several deep reinforcement learning (DRL) algorithms have been proposed recently to address similar problems.","A common limitation in related works is that of slow learning performance.","In order to address this issue, we propose a novel DRL algorithm, which has the capability to learn control policies at a notably faster rate compared to similar methods.","Its sample efficiency is due to a mission-driven exploration strategy that prioritizes exploration towards directions that may contribute to mission accomplishment.","Identifying these directions relies on an automaton representation of the LTL task as well as a learned neural network that (partially) models the unknown system dynamics.","We provide comparative experiments demonstrating the efficiency of our algorithm on robot navigation tasks in unknown environments."],"url":"http://arxiv.org/abs/2311.17059v1"}
{"created":"2023-11-28 18:59:58","title":"Material Palette: Extraction of Materials from a Single Image","abstract":"In this paper, we propose a method to extract physically-based rendering (PBR) materials from a single real-world image. We do so in two steps: first, we map regions of the image to material concepts using a diffusion model, which allows the sampling of texture images resembling each material in the scene. Second, we benefit from a separate network to decompose the generated textures into Spatially Varying BRDFs (SVBRDFs), providing us with materials ready to be used in rendering applications. Our approach builds on existing synthetic material libraries with SVBRDF ground truth, but also exploits a diffusion-generated RGB texture dataset to allow generalization to new samples using unsupervised domain adaptation (UDA). Our contributions are thoroughly evaluated on synthetic and real-world datasets. We further demonstrate the applicability of our method for editing 3D scenes with materials estimated from real photographs. The code and models will be made open-source. Project page: https://astra-vision.github.io/MaterialPalette/","sentences":["In this paper, we propose a method to extract physically-based rendering (PBR) materials from a single real-world image.","We do so in two steps: first, we map regions of the image to material concepts using a diffusion model, which allows the sampling of texture images resembling each material in the scene.","Second, we benefit from a separate network to decompose the generated textures into Spatially Varying BRDFs (SVBRDFs), providing us with materials ready to be used in rendering applications.","Our approach builds on existing synthetic material libraries with SVBRDF ground truth, but also exploits a diffusion-generated RGB texture dataset to allow generalization to new samples using unsupervised domain adaptation (UDA).","Our contributions are thoroughly evaluated on synthetic and real-world datasets.","We further demonstrate the applicability of our method for editing 3D scenes with materials estimated from real photographs.","The code and models will be made open-source.","Project page: https://astra-vision.github.io/MaterialPalette/"],"url":"http://arxiv.org/abs/2311.17060v1"}
{"created":"2023-11-28 18:59:58","title":"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting","abstract":"Realistic 3D human generation from text prompts is a desirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian","sentences":["Realistic 3D human generation from text prompts is a desirable yet challenging task.","Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time.","In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance.","Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.","Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry.","The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process.","2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue.","The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness.","Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.","Project Page: https://alvinliu0.github.io/projects/HumanGaussian"],"url":"http://arxiv.org/abs/2311.17061v1"}
{"created":"2023-11-28 18:59:57","title":"Panoptic Video Scene Graph Generation","abstract":"Towards building comprehensive real-world visual perception systems, we propose and study a new problem called panoptic scene graph generation (PVSG). PVSG relates to the existing video scene graph generation (VidSGG) problem, which focuses on temporal interactions between humans and objects grounded with bounding boxes in videos. However, the limitation of bounding boxes in detecting non-rigid objects and backgrounds often causes VidSGG to miss key details crucial for comprehensive video understanding. In contrast, PVSG requires nodes in scene graphs to be grounded by more precise, pixel-level segmentation masks, which facilitate holistic scene understanding. To advance research in this new area, we contribute the PVSG dataset, which consists of 400 videos (289 third-person + 111 egocentric videos) with a total of 150K frames labeled with panoptic segmentation masks as well as fine, temporal scene graphs. We also provide a variety of baseline methods and share useful design practices for future work.","sentences":["Towards building comprehensive real-world visual perception systems, we propose and study a new problem called panoptic scene graph generation (PVSG).","PVSG relates to the existing video scene graph generation (VidSGG) problem, which focuses on temporal interactions between humans and objects grounded with bounding boxes in videos.","However, the limitation of bounding boxes in detecting non-rigid objects and backgrounds often causes VidSGG to miss key details crucial for comprehensive video understanding.","In contrast, PVSG requires nodes in scene graphs to be grounded by more precise, pixel-level segmentation masks, which facilitate holistic scene understanding.","To advance research in this new area, we contribute the PVSG dataset, which consists of 400 videos (289 third-person + 111 egocentric videos) with a total of 150K frames labeled with panoptic segmentation masks as well as fine, temporal scene graphs.","We also provide a variety of baseline methods and share useful design practices for future work."],"url":"http://arxiv.org/abs/2311.17058v1"}
{"created":"2023-11-28 18:59:52","title":"ReMoS: Reactive 3D Motion Synthesis for Two-Person Interactions","abstract":"Current approaches for 3D human motion synthesis can generate high-quality 3D animations of digital humans performing a wide variety of actions and gestures. However, there is still a notable technological gap in addressing the complex dynamics of multi-human interactions within this paradigm. In this work, we introduce ReMoS, a denoising diffusion-based probabilistic model for reactive motion synthesis that explores two-person interactions. Given the motion of one person, we synthesize the reactive motion of the second person to complete the interactions between the two. In addition to synthesizing the full-body motions, we also synthesize plausible hand interactions. We show the performance of ReMoS under a wide range of challenging two-person scenarios including pair-dancing, Ninjutsu, kickboxing, and acrobatics, where one person's movements have complex and diverse influences on the motions of the other. We further propose the ReMoCap dataset for two-person interactions consisting of full-body and hand motions. We evaluate our approach through multiple quantitative metrics, qualitative visualizations, and a user study. Our results are usable in interactive applications while also providing an adequate amount of control for animators.","sentences":["Current approaches for 3D human motion synthesis can generate high-quality 3D animations of digital humans performing a wide variety of actions and gestures.","However, there is still a notable technological gap in addressing the complex dynamics of multi-human interactions within this paradigm.","In this work, we introduce ReMoS, a denoising diffusion-based probabilistic model for reactive motion synthesis that explores two-person interactions.","Given the motion of one person, we synthesize the reactive motion of the second person to complete the interactions between the two.","In addition to synthesizing the full-body motions, we also synthesize plausible hand interactions.","We show the performance of ReMoS under a wide range of challenging two-person scenarios including pair-dancing, Ninjutsu, kickboxing, and acrobatics, where one person's movements have complex and diverse influences on the motions of the other.","We further propose the ReMoCap dataset for two-person interactions consisting of full-body and hand motions.","We evaluate our approach through multiple quantitative metrics, qualitative visualizations, and a user study.","Our results are usable in interactive applications while also providing an adequate amount of control for animators."],"url":"http://arxiv.org/abs/2311.17057v1"}
{"created":"2023-11-28 18:59:51","title":"Self-Supervised Motion Magnification by Backpropagating Through Optical Flow","abstract":"This paper presents a simple, self-supervised method for magnifying subtle motions in video: given an input video and a magnification factor, we manipulate the video such that its new optical flow is scaled by the desired amount. To train our model, we propose a loss function that estimates the optical flow of the generated video and penalizes how far if deviates from the given magnification factor. Thus, training involves differentiating through a pretrained optical flow network. Since our model is self-supervised, we can further improve its performance through test-time adaptation, by finetuning it on the input video. It can also be easily extended to magnify the motions of only user-selected objects. Our approach avoids the need for synthetic magnification datasets that have been used to train prior learning-based approaches. Instead, it leverages the existing capabilities of off-the-shelf motion estimators. We demonstrate the effectiveness of our method through evaluations of both visual quality and quantitative metrics on a range of real-world and synthetic videos, and we show our method works for both supervised and unsupervised optical flow methods.","sentences":["This paper presents a simple, self-supervised method for magnifying subtle motions in video: given an input video and a magnification factor, we manipulate the video such that its new optical flow is scaled by the desired amount.","To train our model, we propose a loss function that estimates the optical flow of the generated video and penalizes how far if deviates from the given magnification factor.","Thus, training involves differentiating through a pretrained optical flow network.","Since our model is self-supervised, we can further improve its performance through test-time adaptation, by finetuning it on the input video.","It can also be easily extended to magnify the motions of only user-selected objects.","Our approach avoids the need for synthetic magnification datasets that have been used to train prior learning-based approaches.","Instead, it leverages the existing capabilities of off-the-shelf motion estimators.","We demonstrate the effectiveness of our method through evaluations of both visual quality and quantitative metrics on a range of real-world and synthetic videos, and we show our method works for both supervised and unsupervised optical flow methods."],"url":"http://arxiv.org/abs/2311.17056v1"}
{"created":"2023-11-28 18:59:50","title":"Rethinking Directional Integration in Neural Radiance Fields","abstract":"Recent works use the Neural radiance field (NeRF) to perform multi-view 3D reconstruction, providing a significant leap in rendering photorealistic scenes. However, despite its efficacy, NeRF exhibits limited capability of learning view-dependent effects compared to light field rendering or image-based view synthesis. To that end, we introduce a modification to the NeRF rendering equation which is as simple as a few lines of code change for any NeRF variations, while greatly improving the rendering quality of view-dependent effects. By swapping the integration operator and the direction decoder network, we only integrate the positional features along the ray and move the directional terms out of the integration, resulting in a disentanglement of the view-dependent and independent components. The modified equation is equivalent to the classical volumetric rendering in ideal cases on object surfaces with Dirac densities. Furthermore, we prove that with the errors caused by network approximation and numerical integration, our rendering equation exhibits better convergence properties with lower error accumulations compared to the classical NeRF. We also show that the modified equation can be interpreted as light field rendering with learned ray embeddings. Experiments on different NeRF variations show consistent improvements in the quality of view-dependent effects with our simple modification.","sentences":["Recent works use the Neural radiance field (NeRF) to perform multi-view 3D reconstruction, providing a significant leap in rendering photorealistic scenes.","However, despite its efficacy, NeRF exhibits limited capability of learning view-dependent effects compared to light field rendering or image-based view synthesis.","To that end, we introduce a modification to the NeRF rendering equation which is as simple as a few lines of code change for any NeRF variations, while greatly improving the rendering quality of view-dependent effects.","By swapping the integration operator and the direction decoder network, we only integrate the positional features along the ray and move the directional terms out of the integration, resulting in a disentanglement of the view-dependent and independent components.","The modified equation is equivalent to the classical volumetric rendering in ideal cases on object surfaces with Dirac densities.","Furthermore, we prove that with the errors caused by network approximation and numerical integration, our rendering equation exhibits better convergence properties with lower error accumulations compared to the classical NeRF.","We also show that the modified equation can be interpreted as light field rendering with learned ray embeddings.","Experiments on different NeRF variations show consistent improvements in the quality of view-dependent effects with our simple modification."],"url":"http://arxiv.org/abs/2311.16504v1"}
{"created":"2023-11-28 18:59:46","title":"No Representation Rules Them All in Category Discovery","abstract":"In this paper we tackle the problem of Generalized Category Discovery (GCD). Specifically, given a dataset with labelled and unlabelled images, the task is to cluster all images in the unlabelled subset, whether or not they belong to the labelled categories. Our first contribution is to recognize that most existing GCD benchmarks only contain labels for a single clustering of the data, making it difficult to ascertain whether models are using the available labels to solve the GCD task, or simply solving an unsupervised clustering problem. As such, we present a synthetic dataset, named 'Clevr-4', for category discovery. Clevr-4 contains four equally valid partitions of the data, i.e based on object shape, texture, color or count. To solve the task, models are required to extrapolate the taxonomy specified by the labelled set, rather than simply latching onto a single natural grouping of the data. We use this dataset to demonstrate the limitations of unsupervised clustering in the GCD setting, showing that even very strong unsupervised models fail on Clevr-4. We further use Clevr-4 to examine the weaknesses of existing GCD algorithms, and propose a new method which addresses these shortcomings, leveraging consistent findings from the representation learning literature to do so. Our simple solution, which is based on 'mean teachers' and termed $\\mu$GCD, substantially outperforms implemented baselines on Clevr-4. Finally, when we transfer these findings to real data on the challenging Semantic Shift Benchmark (SSB), we find that $\\mu$GCD outperforms all prior work, setting a new state-of-the-art. For the project webpage, see https://www.robots.ox.ac.uk/~vgg/data/clevr4/","sentences":["In this paper we tackle the problem of Generalized Category Discovery (GCD).","Specifically, given a dataset with labelled and unlabelled images, the task is to cluster all images in the unlabelled subset, whether or not they belong to the labelled categories.","Our first contribution is to recognize that most existing GCD benchmarks only contain labels for a single clustering of the data, making it difficult to ascertain whether models are using the available labels to solve the GCD task, or simply solving an unsupervised clustering problem.","As such, we present a synthetic dataset, named 'Clevr-4', for category discovery.","Clevr-4 contains four equally valid partitions of the data, i.e based on object shape, texture, color or count.","To solve the task, models are required to extrapolate the taxonomy specified by the labelled set, rather than simply latching onto a single natural grouping of the data.","We use this dataset to demonstrate the limitations of unsupervised clustering in the GCD setting, showing that even very strong unsupervised models fail on Clevr-4.","We further use Clevr-4 to examine the weaknesses of existing GCD algorithms, and propose a new method which addresses these shortcomings, leveraging consistent findings from the representation learning literature to do so.","Our simple solution, which is based on 'mean teachers' and termed $\\mu$GCD, substantially outperforms implemented baselines on Clevr-4.","Finally, when we transfer these findings to real data on the challenging Semantic Shift Benchmark (SSB), we find that $\\mu$GCD outperforms all prior work, setting a new state-of-the-art.","For the project webpage, see https://www.robots.ox.ac.uk/~vgg/data/clevr4/"],"url":"http://arxiv.org/abs/2311.17055v1"}
{"created":"2023-11-28 18:58:48","title":"DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models","abstract":"Nature evolves creatures with a high complexity of morphological and behavioral intelligence, meanwhile computational methods lag in approaching that diversity and efficacy. Co-optimization of artificial creatures' morphology and control in silico shows promise for applications in physical soft robotics and virtual character creation; such approaches, however, require developing new learning algorithms that can reason about function atop pure structure. In this paper, we present DiffuseBot, a physics-augmented diffusion model that generates soft robot morphologies capable of excelling in a wide spectrum of tasks. DiffuseBot bridges the gap between virtually generated content and physical utility by (i) augmenting the diffusion process with a physical dynamical simulation which provides a certificate of performance, and (ii) introducing a co-design procedure that jointly optimizes physical design and control by leveraging information about physical sensitivities from differentiable simulation. We showcase a range of simulated and fabricated robots along with their capabilities. Check our website at https://diffusebot.github.io/","sentences":["Nature evolves creatures with a high complexity of morphological and behavioral intelligence, meanwhile computational methods lag in approaching that diversity and efficacy.","Co-optimization of artificial creatures' morphology and control in silico shows promise for applications in physical soft robotics and virtual character creation; such approaches, however, require developing new learning algorithms that can reason about function atop pure structure.","In this paper, we present DiffuseBot, a physics-augmented diffusion model that generates soft robot morphologies capable of excelling in a wide spectrum of tasks.","DiffuseBot bridges the gap between virtually generated content and physical utility by (i) augmenting the diffusion process with a physical dynamical simulation which provides a certificate of performance, and (ii) introducing a co-design procedure that jointly optimizes physical design and control by leveraging information about physical sensitivities from differentiable simulation.","We showcase a range of simulated and fabricated robots along with their capabilities.","Check our website at https://diffusebot.github.io/"],"url":"http://arxiv.org/abs/2311.17053v1"}
{"created":"2023-11-28 18:56:01","title":"Surf-D: High-Quality Surface Generation for Arbitrary Topologies using Diffusion Models","abstract":"In this paper, we present Surf-D, a novel method for generating high-quality 3D shapes as Surfaces with arbitrary topologies using Diffusion models. Specifically, we adopt Unsigned Distance Field (UDF) as the surface representation, as it excels in handling arbitrary topologies, enabling the generation of complex shapes. While the prior methods explored shape generation with different representations, they suffer from limited topologies and geometry details. Moreover, it's non-trivial to directly extend prior diffusion models to UDF because they lack spatial continuity due to the discrete volume structure. However, UDF requires accurate gradients for mesh extraction and learning. To tackle the issues, we first leverage a point-based auto-encoder to learn a compact latent space, which supports gradient querying for any input point through differentiation to effectively capture intricate geometry at a high resolution. Since the learning difficulty for various shapes can differ, a curriculum learning strategy is employed to efficiently embed various surfaces, enhancing the whole embedding process. With pretrained shape latent space, we employ a latent diffusion model to acquire the distribution of various shapes. Our approach demonstrates superior performance in shape generation across multiple modalities and conducts extensive experiments in unconditional generation, category conditional generation, 3D reconstruction from images, and text-to-shape tasks.","sentences":["In this paper, we present Surf-D, a novel method for generating high-quality 3D shapes as Surfaces with arbitrary topologies using Diffusion models.","Specifically, we adopt Unsigned Distance Field (UDF) as the surface representation, as it excels in handling arbitrary topologies, enabling the generation of complex shapes.","While the prior methods explored shape generation with different representations, they suffer from limited topologies and geometry details.","Moreover, it's non-trivial to directly extend prior diffusion models to UDF because they lack spatial continuity due to the discrete volume structure.","However, UDF requires accurate gradients for mesh extraction and learning.","To tackle the issues, we first leverage a point-based auto-encoder to learn a compact latent space, which supports gradient querying for any input point through differentiation to effectively capture intricate geometry at a high resolution.","Since the learning difficulty for various shapes can differ, a curriculum learning strategy is employed to efficiently embed various surfaces, enhancing the whole embedding process.","With pretrained shape latent space, we employ a latent diffusion model to acquire the distribution of various shapes.","Our approach demonstrates superior performance in shape generation across multiple modalities and conducts extensive experiments in unconditional generation, category conditional generation, 3D reconstruction from images, and text-to-shape tasks."],"url":"http://arxiv.org/abs/2311.17050v1"}
{"created":"2023-11-28 18:55:42","title":"MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training","abstract":"Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$ improved learning efficiency when compared with non-reinforced CLIP training.","sentences":["Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks.","However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices.","In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training.","The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models.","Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset.","MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets.","Our MobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to previous best CLIP model based on ViT-B/16.","We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best.","Moreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$ improved learning efficiency when compared with non-reinforced CLIP training."],"url":"http://arxiv.org/abs/2311.17049v1"}
{"created":"2023-11-28 18:55:37","title":"Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions","abstract":"Zero-shot referring expression comprehension aims at localizing bounding boxes in an image corresponding to the provided textual prompts, which requires: (i) a fine-grained disentanglement of complex visual scene and textual context, and (ii) a capacity to understand relationships among disentangled entities. Unfortunately, existing large vision-language alignment (VLA) models, e.g., CLIP, struggle with both aspects so cannot be directly used for this task. To mitigate this gap, we leverage large foundation models to disentangle both images and texts into triplets in the format of (subject, predicate, object). After that, grounding is accomplished by calculating the structural similarity matrix between visual and textual triplets with a VLA model, and subsequently propagate it to an instance-level similarity matrix. Furthermore, to equip VLA models with the ability of relationship understanding, we design a triplet-matching objective to fine-tune the VLA models on a collection of curated dataset containing abundant entity relationships. Experiments demonstrate that our visual grounding performance increase of up to 19.5% over the SOTA zero-shot model on RefCOCO/+/g. On the more challenging Who's Waldo dataset, our zero-shot approach achieves comparable accuracy to the fully supervised model.","sentences":["Zero-shot referring expression comprehension aims at localizing bounding boxes in an image corresponding to the provided textual prompts, which requires: (i) a fine-grained disentanglement of complex visual scene and textual context, and (ii) a capacity to understand relationships among disentangled entities.","Unfortunately, existing large vision-language alignment (VLA) models, e.g., CLIP, struggle with both aspects so cannot be directly used for this task.","To mitigate this gap, we leverage large foundation models to disentangle both images and texts into triplets in the format of (subject, predicate, object).","After that, grounding is accomplished by calculating the structural similarity matrix between visual and textual triplets with a VLA model, and subsequently propagate it to an instance-level similarity matrix.","Furthermore, to equip VLA models with the ability of relationship understanding, we design a triplet-matching objective to fine-tune the VLA models on a collection of curated dataset containing abundant entity relationships.","Experiments demonstrate that our visual grounding performance increase of up to 19.5% over the SOTA zero-shot model on RefCOCO/+/g.","On the more challenging Who's Waldo dataset, our zero-shot approach achieves comparable accuracy to the fully supervised model."],"url":"http://arxiv.org/abs/2311.17048v1"}
{"created":"2023-11-28 18:53:43","title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models","abstract":"In this work, we present a novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID. Current VLMs, while proficient in tasks like image captioning and visual question answering, face computational burdens when processing long videos due to the excessive visual tokens. LLaMA-VID addresses this issue by representing each frame with two distinct tokens, namely context token and content token. The context token encodes the overall image context based on user input, whereas the content token encapsulates visual cues in each frame. This dual-token strategy significantly reduces the overload of long videos while preserving critical information. Generally, LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. It is proved to surpass previous methods on most of video- or image-based benchmarks. Code is available https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID","sentences":["In this work, we present a novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID.","Current VLMs, while proficient in tasks like image captioning and visual question answering, face computational burdens when processing long videos due to the excessive visual tokens.","LLaMA-VID addresses this issue by representing each frame with two distinct tokens, namely context token and content token.","The context token encodes the overall image context based on user input, whereas the content token encapsulates visual cues in each frame.","This dual-token strategy significantly reduces the overload of long videos while preserving critical information.","Generally, LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token.","It is proved to surpass previous methods on most of video- or image-based benchmarks.","Code is available https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID"],"url":"http://arxiv.org/abs/2311.17043v1"}
{"created":"2023-11-28 18:53:24","title":"Adversarial Diffusion Distillation","abstract":"We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality. We use score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps. ADD is the first method to unlock single-step, real-time image synthesis with foundation models. Code and weights available under https://github.com/Stability-AI/generative-models and https://huggingface.co/stabilityai/ .","sentences":["We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality.","We use score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps.","Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps.","ADD is the first method to unlock single-step, real-time image synthesis with foundation models.","Code and weights available under https://github.com/Stability-AI/generative-models and https://huggingface.co/stabilityai/ ."],"url":"http://arxiv.org/abs/2311.17042v1"}
{"created":"2023-11-28 18:53:06","title":"Efficient In-Context Learning in Vision-Language Models for Egocentric Videos","abstract":"Recent advancements in text-only large language models (LLMs) have highlighted the benefit of in-context learning for adapting to new tasks with a few demonstrations. However, extending in-context learning to large vision-language models (VLMs) using a huge amount of naturalistic vision-language data has shown limited success, particularly for egocentric videos, due to high data collection costs. We propose a novel training method $\\mathbb{E}$fficient $\\mathbb{I}$n-context $\\mathbb{L}$earning on $\\mathbb{E}$gocentric $\\mathbb{V}$ideos ($\\mathbb{EILEV}$), which elicits in-context learning in VLMs for egocentric videos without requiring massive, naturalistic egocentric video datasets. $\\mathbb{EILEV}$ involves architectural and training data adaptations to allow the model to process contexts interleaved with video clips and narrations, sampling of in-context examples with clusters of similar verbs and nouns, use of data with skewed marginal distributions with a long tail of infrequent verbs and nouns, as well as homonyms and synonyms. Our evaluations show that $\\mathbb{EILEV}$-trained models outperform larger VLMs trained on a huge amount of naturalistic data in in-context learning. Furthermore, they can generalize to not only out-of-distribution, but also novel, rare egocentric videos and texts via in-context learning, demonstrating potential for applications requiring cost-effective training, and rapid post-deployment adaptability. Our code and demo are available at \\url{https://github.com/yukw777/EILEV}.","sentences":["Recent advancements in text-only large language models (LLMs) have highlighted the benefit of in-context learning for adapting to new tasks with a few demonstrations.","However, extending in-context learning to large vision-language models (VLMs) using a huge amount of naturalistic vision-language data has shown limited success, particularly for egocentric videos, due to high data collection costs.","We propose a novel training method $\\mathbb{E}$fficient $\\mathbb{I}$n-context $\\mathbb{L}$earning on $\\mathbb{E}$gocentric $\\mathbb{V}$ideos ($\\mathbb{EILEV}$), which elicits in-context learning in VLMs for egocentric videos without requiring massive, naturalistic egocentric video datasets.","$\\mathbb{EILEV}$ involves architectural and training data adaptations to allow the model to process contexts interleaved with video clips and narrations, sampling of in-context examples with clusters of similar verbs and nouns, use of data with skewed marginal distributions with a long tail of infrequent verbs and nouns, as well as homonyms and synonyms.","Our evaluations show that $\\mathbb{EILEV}$-trained models outperform larger VLMs trained on a huge amount of naturalistic data in in-context learning.","Furthermore, they can generalize to not only out-of-distribution, but also novel, rare egocentric videos and texts via in-context learning, demonstrating potential for applications requiring cost-effective training, and rapid post-deployment adaptability.","Our code and demo are available at \\url{https://github.com/yukw777/EILEV}."],"url":"http://arxiv.org/abs/2311.17041v1"}
{"created":"2023-11-28 18:52:38","title":"Rumors with Changing Credibility","abstract":"Randomized rumor spreading processes diffuse information on an undirected graph and have been widely studied. In this work, we present a generic framework for analyzing a broad class of such processes on regular graphs. Our analysis is protocol-agnostic, as it only requires the expected proportion of newly informed vertices in each round to be bounded, and a natural negative correlation property.   This framework allows us to analyze various protocols, including PUSH, PULL, and PUSH-PULL, thereby extending prior research. Unlike previous work, our framework accommodates message failures at any time $t\\geq 0$ with a probability of $1-q(t)$, where the credibility $q(t)$ is any function of time. This enables us to model real-world scenarios in which the transmissibility of rumors may fluctuate, as seen in the spread of ``fake news'' and viruses. Additionally, our framework is sufficiently broad to cover dynamic graphs.","sentences":["Randomized rumor spreading processes diffuse information on an undirected graph and have been widely studied.","In this work, we present a generic framework for analyzing a broad class of such processes on regular graphs.","Our analysis is protocol-agnostic, as it only requires the expected proportion of newly informed vertices in each round to be bounded, and a natural negative correlation property.   ","This framework allows us to analyze various protocols, including PUSH, PULL, and PUSH-PULL, thereby extending prior research.","Unlike previous work, our framework accommodates message failures at any time $t\\geq 0$ with a probability of $1-q(t)$, where the credibility $q(t)$ is any function of time.","This enables us to model real-world scenarios in which the transmissibility of rumors may fluctuate, as seen in the spread of ``fake news'' and viruses.","Additionally, our framework is sufficiently broad to cover dynamic graphs."],"url":"http://arxiv.org/abs/2311.17040v1"}
{"created":"2023-11-28 18:49:17","title":"Extension of Minimax for Algorithmic Lower Bounds","abstract":"This paper considers the use of Yao's Minimax Theorem in robust algorithm design, e.g., for online algorithms, where the algorithm designer aims to minimize the ratio of the algorithm's performance to the optimal performance. When applying Minimax, the objective becomes the expectation of these ratios. The main result of the paper is that it is equally valid for the objective (after applying Minimax) to be the ratio of the expectations.","sentences":["This paper considers the use of Yao's Minimax Theorem in robust algorithm design, e.g., for online algorithms, where the algorithm designer aims to minimize the ratio of the algorithm's performance to the optimal performance.","When applying Minimax, the objective becomes the expectation of these ratios.","The main result of the paper is that it is equally valid for the objective (after applying Minimax) to be the ratio of the expectations."],"url":"http://arxiv.org/abs/2311.17038v1"}
{"created":"2023-11-28 18:49:14","title":"Concurrent Stochastic Lossy Channel Games","abstract":"Concurrent stochastic games are an important formalism for the rational verification of probabilistic multi-agent systems, which involves verifying whether a temporal logic property is satisfied in some or all game-theoretic equilibria of such systems. In this work, we study the rational verification of probabilistic multi-agent systems where agents can cooperate by communicating over unbounded lossy channels. To model such systems, we present concurrent stochastic lossy channel games (CSLCG) and employ an equilibrium concept from cooperative game theory known as the core, which is the most fundamental and widely studied cooperative equilibrium concept. Our main contribution is twofold. First, we show that the rational verification problem is undecidable for systems whose agents have almost-sure LTL objectives. Second, we provide a decidable fragment of such a class of objectives that subsumes almost-sure reachability and safety. Our techniques involve reductions to solving infinite-state zero-sum games with conjunctions of qualitative objectives. To the best of our knowledge, our result represents the first decidability result on the rational verification of stochastic multi-agent systems on infinite arenas.","sentences":["Concurrent stochastic games are an important formalism for the rational verification of probabilistic multi-agent systems, which involves verifying whether a temporal logic property is satisfied in some or all game-theoretic equilibria of such systems.","In this work, we study the rational verification of probabilistic multi-agent systems where agents can cooperate by communicating over unbounded lossy channels.","To model such systems, we present concurrent stochastic lossy channel games (CSLCG) and employ an equilibrium concept from cooperative game theory known as the core, which is the most fundamental and widely studied cooperative equilibrium concept.","Our main contribution is twofold.","First, we show that the rational verification problem is undecidable for systems whose agents have almost-sure LTL objectives.","Second, we provide a decidable fragment of such a class of objectives that subsumes almost-sure reachability and safety.","Our techniques involve reductions to solving infinite-state zero-sum games with conjunctions of qualitative objectives.","To the best of our knowledge, our result represents the first decidability result on the rational verification of stochastic multi-agent systems on infinite arenas."],"url":"http://arxiv.org/abs/2311.17037v1"}
{"created":"2023-11-28 18:47:03","title":"Scalable Extraction of Training Data from (Production) Language Models","abstract":"This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.","sentences":["This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset.","We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT.","Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly.","Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization."],"url":"http://arxiv.org/abs/2311.17035v1"}
{"created":"2023-11-28 18:45:13","title":"Telling Left from Right: Identifying Geometry-Aware Semantic Correspondence","abstract":"While pre-trained large-scale vision models have shown significant promise for semantic correspondence, their features often struggle to grasp the geometry and orientation of instances. This paper identifies the importance of being geometry-aware for semantic correspondence and reveals a limitation of the features of current foundation models under simple post-processing. We show that incorporating this information can markedly enhance semantic correspondence performance with simple but effective solutions in both zero-shot and supervised settings. We also construct a new challenging benchmark for semantic correspondence built from an existing animal pose estimation dataset, for both pre-training validating models. Our method achieves a PCK@0.10 score of 64.2 (zero-shot) and 85.6 (supervised) on the challenging SPair-71k dataset, outperforming the state-of-the-art by 4.3p and 11.0p absolute gains, respectively. Our code and datasets will be publicly available.","sentences":["While pre-trained large-scale vision models have shown significant promise for semantic correspondence, their features often struggle to grasp the geometry and orientation of instances.","This paper identifies the importance of being geometry-aware for semantic correspondence and reveals a limitation of the features of current foundation models under simple post-processing.","We show that incorporating this information can markedly enhance semantic correspondence performance with simple but effective solutions in both zero-shot and supervised settings.","We also construct a new challenging benchmark for semantic correspondence built from an existing animal pose estimation dataset, for both pre-training validating models.","Our method achieves a PCK@0.10 score of 64.2 (zero-shot) and 85.6 (supervised) on the challenging SPair-71k dataset, outperforming the state-of-the-art by 4.3p and 11.0p absolute gains, respectively.","Our code and datasets will be publicly available."],"url":"http://arxiv.org/abs/2311.17034v1"}
{"created":"2023-11-28 18:32:19","title":"Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching","abstract":"Mechanistic interpretability aims to understand model behaviors in terms of specific, interpretable features, often hypothesized to manifest as low-dimensional subspaces of activations. Specifically, recent studies have explored subspace interventions (such as activation patching) as a way to simultaneously manipulate model behavior and attribute the features behind it to given subspaces.   In this work, we demonstrate that these two aims diverge, potentially leading to an illusory sense of interpretability. Counterintuitively, even if a subspace intervention makes the model's output behave as if the value of a feature was changed, this effect may be achieved by activating a dormant parallel pathway leveraging another subspace that is causally disconnected from model outputs. We demonstrate this phenomenon in a distilled mathematical example, in two real-world domains (the indirect object identification task and factual recall), and present evidence for its prevalence in practice. In the context of factual recall, we further show a link to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localization.   However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (indirect object identification) where prior manual circuit analysis informs an understanding of the location of a feature. We explore the additional evidence needed to argue that a patched subspace is faithful.","sentences":["Mechanistic interpretability aims to understand model behaviors in terms of specific, interpretable features, often hypothesized to manifest as low-dimensional subspaces of activations.","Specifically, recent studies have explored subspace interventions (such as activation patching) as a way to simultaneously manipulate model behavior and attribute the features behind it to given subspaces.   ","In this work, we demonstrate that these two aims diverge, potentially leading to an illusory sense of interpretability.","Counterintuitively, even if a subspace intervention makes the model's output behave as if the value of a feature was changed, this effect may be achieved by activating a dormant parallel pathway leveraging another subspace that is causally disconnected from model outputs.","We demonstrate this phenomenon in a distilled mathematical example, in two real-world domains (the indirect object identification task and factual recall), and present evidence for its prevalence in practice.","In the context of factual recall, we further show a link to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localization.   ","However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability.","To contextualize our findings, we also show what a success case looks like in a task (indirect object identification) where prior manual circuit analysis informs an understanding of the location of a feature.","We explore the additional evidence needed to argue that a patched subspace is faithful."],"url":"http://arxiv.org/abs/2311.17030v1"}
{"created":"2023-11-28 18:28:03","title":"When the Few Outweigh the Many: Illicit Content Recognition with Few-Shot Learning","abstract":"The anonymity and untraceability benefits of the Dark web account for the exponentially-increased potential of its popularity while creating a suitable womb for many illicit activities, to date. Hence, in collaboration with cybersecurity and law enforcement agencies, research has provided approaches for recognizing and classifying illicit activities with most exploiting textual dark web markets' content recognition; few such approaches use images that originated from dark web content. This paper investigates this alternative technique for recognizing illegal activities from images. In particular, we investigate label-agnostic learning techniques like One-Shot and Few-Shot learning featuring the use Siamese neural networks, a state-of-the-art approach in the field. Our solution manages to handle small-scale datasets with promising accuracy. In particular, Siamese neural networks reach 90.9% on 20-Shot experiments over a 10-class dataset; this leads us to conclude that such models are a promising and cheaper alternative to the definition of automated law-enforcing machinery over the dark web.","sentences":["The anonymity and untraceability benefits of the Dark web account for the exponentially-increased potential of its popularity while creating a suitable womb for many illicit activities, to date.","Hence, in collaboration with cybersecurity and law enforcement agencies, research has provided approaches for recognizing and classifying illicit activities with most exploiting textual dark web markets' content recognition; few such approaches use images that originated from dark web content.","This paper investigates this alternative technique for recognizing illegal activities from images.","In particular, we investigate label-agnostic learning techniques like One-Shot and Few-Shot learning featuring the use Siamese neural networks, a state-of-the-art approach in the field.","Our solution manages to handle small-scale datasets with promising accuracy.","In particular, Siamese neural networks reach 90.9% on 20-Shot experiments over a 10-class dataset; this leads us to conclude that such models are a promising and cheaper alternative to the definition of automated law-enforcing machinery over the dark web."],"url":"http://arxiv.org/abs/2311.17026v1"}
{"created":"2023-11-28 18:27:15","title":"Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features","abstract":"We present Diff3F as a simple, robust, and class-agnostic feature descriptor that can be computed for untextured input shapes (meshes or point clouds). Our method distills diffusion features from image foundational models onto input shapes. Specifically, we use the input shapes to produce depth and normal maps as guidance for conditional image synthesis, and in the process produce (diffusion) features in 2D that we subsequently lift and aggregate on the original surface. Our key observation is that even if the conditional image generations obtained from multi-view rendering of the input shapes are inconsistent, the associated image features are robust and can be directly aggregated across views. This produces semantic features on the input shapes, without requiring additional data or training. We perform extensive experiments on multiple benchmarks (SHREC'19, SHREC'20, and TOSCA) and demonstrate that our features, being semantic instead of geometric, produce reliable correspondence across both isometeric and non-isometrically related shape families.","sentences":["We present Diff3F as a simple, robust, and class-agnostic feature descriptor that can be computed for untextured input shapes (meshes or point clouds).","Our method distills diffusion features from image foundational models onto input shapes.","Specifically, we use the input shapes to produce depth and normal maps as guidance for conditional image synthesis, and in the process produce (diffusion) features in 2D that we subsequently lift and aggregate on the original surface.","Our key observation is that even if the conditional image generations obtained from multi-view rendering of the input shapes are inconsistent, the associated image features are robust and can be directly aggregated across views.","This produces semantic features on the input shapes, without requiring additional data or training.","We perform extensive experiments on multiple benchmarks (SHREC'19, SHREC'20, and TOSCA) and demonstrate that our features, being semantic instead of geometric, produce reliable correspondence across both isometeric and non-isometrically related shape families."],"url":"http://arxiv.org/abs/2311.17024v1"}
{"created":"2023-11-28 18:20:59","title":"Message Recovery Attack in NTRU through VFK Lattices","abstract":"In the present paper, we implement a message recovery attack to all variants of the NTRU cryptosystem. Our approach involves a reduction from the NTRU-lattice to a Voronoi First Kind lattice, enabling the application of a polynomial CVP exact algorithm crucial for executing the Message Recovery. The efficacy of our attack relies on a specific oracle that permits us to approximate an unknown quantity. Furthermore, we outline the mathematical conditions under which the attack is successful. Finally, we delve into a well-established polynomial algorithm for CVP on VFK lattices and its implementation, shedding light on its efficacy in our attack. Subsequently, we present comprehensive experimental results on the NTRU-HPS and the NTRU-Prime variants of the NIST submissions and propose a method that could indicate the resistance of the NTRU cryptosystem to our attack.","sentences":["In the present paper, we implement a message recovery attack to all variants of the NTRU cryptosystem.","Our approach involves a reduction from the NTRU-lattice to a Voronoi First Kind lattice, enabling the application of a polynomial CVP exact algorithm crucial for executing the Message Recovery.","The efficacy of our attack relies on a specific oracle that permits us to approximate an unknown quantity.","Furthermore, we outline the mathematical conditions under which the attack is successful.","Finally, we delve into a well-established polynomial algorithm for CVP on VFK lattices and its implementation, shedding light on its efficacy in our attack.","Subsequently, we present comprehensive experimental results on the NTRU-HPS and the NTRU-Prime variants of the NIST submissions and propose a method that could indicate the resistance of the NTRU cryptosystem to our attack."],"url":"http://arxiv.org/abs/2311.17022v1"}
{"created":"2023-11-28 18:17:28","title":"Homogeneous Algebraic Complexity Theory and Algebraic Formulas","abstract":"We study algebraic complexity classes and their complete polynomials under \\emph{homogeneous linear} projections, not just under the usual affine linear projections that were originally introduced by Valiant in 1979. These reductions are weaker yet more natural from a geometric complexity theory (GCT) standpoint, because the corresponding orbit closure formulations do not require the padding of polynomials. We give the \\emph{first} complete polynomials for VF, the class of sequences of polynomials that admit small algebraic formulas, under homogeneous linear projections: The sum of the entries of the non-commutative elementary symmetric polynomial in 3 by 3 matrices of homogeneous linear forms.   Even simpler variants of the elementary symmetric polynomial are hard for the topological closure of a large subclass of VF: the sum of the entries of the non-commutative elementary symmetric polynomial in 2 by 2 matrices of homogeneous linear forms, and homogeneous variants of the continuant polynomial (Bringmann, Ikenmeyer, Zuiddam, JACM '18). This requires a careful study of circuits with arity-3 product gates.","sentences":["We study algebraic complexity classes and their complete polynomials under \\emph{homogeneous linear} projections, not just under the usual affine linear projections that were originally introduced by Valiant in 1979.","These reductions are weaker yet more natural from a geometric complexity theory (GCT) standpoint, because the corresponding orbit closure formulations do not require the padding of polynomials.","We give the \\emph{first} complete polynomials for VF, the class of sequences of polynomials that admit small algebraic formulas, under homogeneous linear projections: The sum of the entries of the non-commutative elementary symmetric polynomial in 3 by 3 matrices of homogeneous linear forms.   ","Even simpler variants of the elementary symmetric polynomial are hard for the topological closure of a large subclass of VF: the sum of the entries of the non-commutative elementary symmetric polynomial in 2 by 2 matrices of homogeneous linear forms, and homogeneous variants of the continuant polynomial (Bringmann, Ikenmeyer, Zuiddam, JACM '18).","This requires a careful study of circuits with arity-3 product gates."],"url":"http://arxiv.org/abs/2311.17019v1"}
{"created":"2023-11-28 18:11:24","title":"Foundational Moral Values for AI Alignment","abstract":"Solving the AI alignment problem requires having clear, defensible values towards which AI systems can align. Currently, targets for alignment remain underspecified and do not seem to be built from a philosophically robust structure. We begin the discussion of this problem by presenting five core, foundational values, drawn from moral philosophy and built on the requisites for human existence: survival, sustainable intergenerational existence, society, education, and truth. We show that these values not only provide a clearer direction for technical alignment work, but also serve as a framework to highlight threats and opportunities from AI systems to both obtain and sustain these values.","sentences":["Solving the AI alignment problem requires having clear, defensible values towards which AI systems can align.","Currently, targets for alignment remain underspecified and do not seem to be built from a philosophically robust structure.","We begin the discussion of this problem by presenting five core, foundational values, drawn from moral philosophy and built on the requisites for human existence: survival, sustainable intergenerational existence, society, education, and truth.","We show that these values not only provide a clearer direction for technical alignment work, but also serve as a framework to highlight threats and opportunities from AI systems to both obtain and sustain these values."],"url":"http://arxiv.org/abs/2311.17017v1"}
{"created":"2023-11-28 18:06:30","title":"Counter-terrorism in cyber-physical spaces: Best practices and technologies from the state of the art","abstract":"Context: The demand for protection and security of physical spaces and urban areas increased with the escalation of terroristic attacks in recent years. We envision with the proposed cyber-physical systems and spaces, a city that would indeed become a smarter urbanistic object, proactively providing alerts and being protective against any threat. Objectives: This survey intend to provide a systematic multivocal literature survey comprised of an updated, comprehensive and timely overview of state of the art in counter-terrorism cyber-physical systems, hence aimed at the protection of cyber-physical spaces. Hence, provide guidelines to law enforcement agencies and practitioners providing a description of technologies and best practices for the protection of public spaces. Methods: We analyzed 112 papers collected from different online sources, both from the academic field and from websites and blogs ranging from 2004 till mid-2022. Results: a) There is no one single bullet-proof solution available for the protection of public spaces. b) From our analysis we found three major active fields for the protection of public spaces: Information Technologies, Architectural approaches, Organizational field. c) While the academic suggest best practices and methodologies for the protection of urban areas, the market did not provide any type of implementation of such suggested approaches, which shows a lack of fertilization between academia and industry. Conclusion: The overall analysis has led us to state that there is no one single solution available, conversely, multiple methods and techniques can be put in place to guarantee safety and security in public spaces. The techniques range from architectural design to rethink the design of public spaces keeping security into account in continuity, to emerging technologies such as AI and predictive surveillance.","sentences":["Context: The demand for protection and security of physical spaces and urban areas increased with the escalation of terroristic attacks in recent years.","We envision with the proposed cyber-physical systems and spaces, a city that would indeed become a smarter urbanistic object, proactively providing alerts and being protective against any threat.","Objectives:","This survey intend to provide a systematic multivocal literature survey comprised of an updated, comprehensive and timely overview of state of the art in counter-terrorism cyber-physical systems, hence aimed at the protection of cyber-physical spaces.","Hence, provide guidelines to law enforcement agencies and practitioners providing a description of technologies and best practices for the protection of public spaces.","Methods: We analyzed 112 papers collected from different online sources, both from the academic field and from websites and blogs ranging from 2004 till mid-2022.","Results: a)","There is no one single bullet-proof solution available for the protection of public spaces.","b)","From our analysis we found three major active fields for the protection of public spaces: Information Technologies, Architectural approaches, Organizational field.","c)","While the academic suggest best practices and methodologies for the protection of urban areas, the market did not provide any type of implementation of such suggested approaches, which shows a lack of fertilization between academia and industry.","Conclusion: The overall analysis has led us to state that there is no one single solution available, conversely, multiple methods and techniques can be put in place to guarantee safety and security in public spaces.","The techniques range from architectural design to rethink the design of public spaces keeping security into account in continuity, to emerging technologies such as AI and predictive surveillance."],"url":"http://arxiv.org/abs/2311.17012v1"}
{"created":"2023-11-28 18:04:17","title":"Node Connectivity Augmentation of Highly Connected Graphs","abstract":"Node-connectivity augmentation is a fundamental network design problem. We are given a $k$-node connected graph $G$ together with an additional set of links, and the goal is to add a cheap subset of links to $G$ to make it $(k+1)$-node connected.   In this work, we characterize completely the computational complexity status of the problem, by showing hardness for all values of $k$ which were not addressed previously in the literature.   We then focus on $k$-node connectivity augmentation for $k=n-4$, which corresponds to the highest value of $k$ for which the problem is NP-hard. We improve over the previously best known approximation bounds for this problem, by developing a $\\frac{3}{2}$-approximation algorithm for the weighted setting, and a $\\frac{4}{3}$-approximation algorithm for the unweighted setting.","sentences":["Node-connectivity augmentation is a fundamental network design problem.","We are given a $k$-node connected graph $G$ together with an additional set of links, and the goal is to add a cheap subset of links to $G$ to make it $(k+1)$-node connected.   ","In this work, we characterize completely the computational complexity status of the problem, by showing hardness for all values of $k$ which were not addressed previously in the literature.   ","We then focus on $k$-node connectivity augmentation for $k=n-4$, which corresponds to the highest value of $k$ for which the problem is NP-hard.","We improve over the previously best known approximation bounds for this problem, by developing a $\\frac{3}{2}$-approximation algorithm for the weighted setting, and a $\\frac{4}{3}$-approximation algorithm for the unweighted setting."],"url":"http://arxiv.org/abs/2311.17010v1"}
{"created":"2023-11-28 18:03:27","title":"Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer","abstract":"We present a new method for text-driven motion transfer - synthesizing a video that complies with an input text prompt describing the target objects and scene while maintaining an input video's motion and scene layout. Prior methods are confined to transferring motion across two subjects within the same or closely related object categories and are applicable for limited domains (e.g., humans). In this work, we consider a significantly more challenging setting in which the target and source objects differ drastically in shape and fine-grained motion characteristics (e.g., translating a jumping dog into a dolphin). To this end, we leverage a pre-trained and fixed text-to-video diffusion model, which provides us with generative and motion priors. The pillar of our method is a new space-time feature loss derived directly from the model. This loss guides the generation process to preserve the overall motion of the input video while complying with the target object in terms of shape and fine-grained motion traits.","sentences":["We present a new method for text-driven motion transfer - synthesizing a video that complies with an input text prompt describing the target objects and scene while maintaining an input video's motion and scene layout.","Prior methods are confined to transferring motion across two subjects within the same or closely related object categories and are applicable for limited domains (e.g., humans).","In this work, we consider a significantly more challenging setting in which the target and source objects differ drastically in shape and fine-grained motion characteristics (e.g., translating a jumping dog into a dolphin).","To this end, we leverage a pre-trained and fixed text-to-video diffusion model, which provides us with generative and motion priors.","The pillar of our method is a new space-time feature loss derived directly from the model.","This loss guides the generation process to preserve the overall motion of the input video while complying with the target object in terms of shape and fine-grained motion traits."],"url":"http://arxiv.org/abs/2311.17009v1"}
{"created":"2023-11-28 18:02:06","title":"Computational Hypergraph Discovery, a Gaussian Process framework for connecting the dots","abstract":"Most scientific challenges can be framed into one of the following three levels of complexity of function approximation. Type 1: Approximate an unknown function given input/output data. Type 2: Consider a collection of variables and functions, some of which are unknown, indexed by the nodes and hyperedges of a hypergraph (a generalized graph where edges can connect more than two vertices). Given partial observations of the variables of the hypergraph (satisfying the functional dependencies imposed by its structure), approximate all the unobserved variables and unknown functions. Type 3: Expanding on Type 2, if the hypergraph structure itself is unknown, use partial observations of the variables of the hypergraph to discover its structure and approximate its unknown functions. While most Computational Science and Engineering and Scientific Machine Learning challenges can be framed as Type 1 and Type 2 problems, many scientific problems can only be categorized as Type 3. Despite their prevalence, these Type 3 challenges have been largely overlooked due to their inherent complexity. Although Gaussian Process (GP) methods are sometimes perceived as well-founded but old technology limited to Type 1 curve fitting, their scope has recently been expanded to Type 2 problems. In this paper, we introduce an interpretable GP framework for Type 3 problems, targeting the data-driven discovery and completion of computational hypergraphs. Our approach is based on a kernel generalization of Row Echelon Form reduction from linear systems to nonlinear ones and variance-based analysis. Here, variables are linked via GPs and those contributing to the highest data variance unveil the hypergraph's structure. We illustrate the scope and efficiency of the proposed approach with applications to (algebraic) equation discovery, network discovery (gene pathways, chemical, and mechanical) and raw data analysis.","sentences":["Most scientific challenges can be framed into one of the following three levels of complexity of function approximation.","Type 1: Approximate an unknown function given input/output data.","Type 2: Consider a collection of variables and functions, some of which are unknown, indexed by the nodes and hyperedges of a hypergraph (a generalized graph where edges can connect more than two vertices).","Given partial observations of the variables of the hypergraph (satisfying the functional dependencies imposed by its structure), approximate all the unobserved variables and unknown functions.","Type 3: Expanding on Type 2, if the hypergraph structure itself is unknown, use partial observations of the variables of the hypergraph to discover its structure and approximate its unknown functions.","While most Computational Science and Engineering and Scientific Machine Learning challenges can be framed as Type 1 and Type 2 problems, many scientific problems can only be categorized as Type 3.","Despite their prevalence, these Type 3 challenges have been largely overlooked due to their inherent complexity.","Although Gaussian Process (GP) methods are sometimes perceived as well-founded but old technology limited to Type 1 curve fitting, their scope has recently been expanded to Type 2 problems.","In this paper, we introduce an interpretable GP framework for Type 3 problems, targeting the data-driven discovery and completion of computational hypergraphs.","Our approach is based on a kernel generalization of Row Echelon Form reduction from linear systems to nonlinear ones and variance-based analysis.","Here, variables are linked via GPs and those contributing to the highest data variance unveil the hypergraph's structure.","We illustrate the scope and efficiency of the proposed approach with applications to (algebraic) equation discovery, network discovery (gene pathways, chemical, and mechanical) and raw data analysis."],"url":"http://arxiv.org/abs/2311.17007v1"}
{"created":"2023-11-28 18:02:06","title":"An Investigation of Time Reversal Symmetry in Reinforcement Learning","abstract":"One of the fundamental challenges associated with reinforcement learning (RL) is that collecting sufficient data can be both time-consuming and expensive. In this paper, we formalize a concept of time reversal symmetry in a Markov decision process (MDP), which builds upon the established structure of dynamically reversible Markov chains (DRMCs) and time-reversibility in classical physics. Specifically, we investigate the utility of this concept in reducing the sample complexity of reinforcement learning. We observe that utilizing the structure of time reversal in an MDP allows every environment transition experienced by an agent to be transformed into a feasible reverse-time transition, effectively doubling the number of experiences in the environment. To test the usefulness of this newly synthesized data, we develop a novel approach called time symmetric data augmentation (TSDA) and investigate its application in both proprioceptive and pixel-based state within the realm of off-policy, model-free RL. Empirical evaluations showcase how these synthetic transitions can enhance the sample efficiency of RL agents in time reversible scenarios without friction or contact. We also test this method in more realistic environments where these assumptions are not globally satisfied. We find that TSDA can significantly degrade sample efficiency and policy performance, but can also improve sample efficiency under the right conditions. Ultimately we conclude that time symmetry shows promise in enhancing the sample efficiency of reinforcement learning and provide guidance when the environment and reward structures are of an appropriate form for TSDA to be employed effectively.","sentences":["One of the fundamental challenges associated with reinforcement learning (RL) is that collecting sufficient data can be both time-consuming and expensive.","In this paper, we formalize a concept of time reversal symmetry in a Markov decision process (MDP), which builds upon the established structure of dynamically reversible Markov chains (DRMCs) and time-reversibility in classical physics.","Specifically, we investigate the utility of this concept in reducing the sample complexity of reinforcement learning.","We observe that utilizing the structure of time reversal in an MDP allows every environment transition experienced by an agent to be transformed into a feasible reverse-time transition, effectively doubling the number of experiences in the environment.","To test the usefulness of this newly synthesized data, we develop a novel approach called time symmetric data augmentation (TSDA) and investigate its application in both proprioceptive and pixel-based state within the realm of off-policy, model-free RL.","Empirical evaluations showcase how these synthetic transitions can enhance the sample efficiency of RL agents in time reversible scenarios without friction or contact.","We also test this method in more realistic environments where these assumptions are not globally satisfied.","We find that TSDA can significantly degrade sample efficiency and policy performance, but can also improve sample efficiency under the right conditions.","Ultimately we conclude that time symmetry shows promise in enhancing the sample efficiency of reinforcement learning and provide guidance when the environment and reward structures are of an appropriate form for TSDA to be employed effectively."],"url":"http://arxiv.org/abs/2311.17008v1"}
{"created":"2023-11-28 17:59:49","title":"On the Impact of Sampling on Deep Sequential State Estimation","abstract":"State inference and parameter learning in sequential models can be successfully performed with approximation techniques that maximize the evidence lower bound to the marginal log-likelihood of the data distribution. These methods may be referred to as Dynamical Variational Autoencoders, and our specific focus lies on the deep Kalman filter. It has been shown that the ELBO objective can oversimplify data representations, potentially compromising estimation quality. Tighter Monte Carlo objectives have been proposed in the literature to enhance generative modeling performance. For instance, the IWAE objective uses importance weights to reduce the variance of marginal log-likelihood estimates. In this paper, importance sampling is applied to the DKF framework for learning deep Markov models, resulting in the IW-DKF, which shows an improvement in terms of log-likelihood estimates and KL divergence between the variational distribution and the transition model. The framework using the sampled DKF update rule is also accommodated to address sequential state and parameter estimation when working with highly non-linear physics-based models. An experiment with the 3-space Lorenz attractor shows an enhanced generative modeling performance and also a decrease in RMSE when estimating the model parameters and latent states, indicating that tighter MCOs lead to improved state inference performance.","sentences":["State inference and parameter learning in sequential models can be successfully performed with approximation techniques that maximize the evidence lower bound to the marginal log-likelihood of the data distribution.","These methods may be referred to as Dynamical Variational Autoencoders, and our specific focus lies on the deep Kalman filter.","It has been shown that the ELBO objective can oversimplify data representations, potentially compromising estimation quality.","Tighter Monte Carlo objectives have been proposed in the literature to enhance generative modeling performance.","For instance, the IWAE objective uses importance weights to reduce the variance of marginal log-likelihood estimates.","In this paper, importance sampling is applied to the DKF framework for learning deep Markov models, resulting in the IW-DKF, which shows an improvement in terms of log-likelihood estimates and KL divergence between the variational distribution and the transition model.","The framework using the sampled DKF update rule is also accommodated to address sequential state and parameter estimation when working with highly non-linear physics-based models.","An experiment with the 3-space Lorenz attractor shows an enhanced generative modeling performance and also a decrease in RMSE when estimating the model parameters and latent states, indicating that tighter MCOs lead to improved state inference performance."],"url":"http://arxiv.org/abs/2311.17006v1"}
{"created":"2023-11-28 17:59:04","title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark","abstract":"With the rapid development of Multi-modal Large Language Models (MLLMs), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models. However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks. To alleviate this issue, we introduce a comprehensive Multi-modal Video understanding Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot be effectively solved with a single frame. Specifically, we first introduce a novel static-to-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition. Then, guided by the task definition, we automatically convert public video annotations into multiple-choice QA to evaluate each task. On one hand, such a distinct paradigm allows us to build MVBench efficiently, without much manual intervention. On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of LLMs. Moreover, we further develop a robust video MLLM baseline, i.e., VideoChat2, by progressive multi-modal training with diverse instruction-tuning data. The extensive results on our MVBench reveal that, the existing MLLMs are far from satisfactory in temporal understanding, while our VideoChat2 largely surpasses these leading models by over 15% on MVBench. All models and data are available at https://github.com/OpenGVLab/Ask-Anything.","sentences":["With the rapid development of Multi-modal Large Language Models (MLLMs), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models.","However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks.","To alleviate this issue, we introduce a comprehensive Multi-modal Video understanding Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot be effectively solved with a single frame.","Specifically, we first introduce a novel static-to-dynamic method to define these temporal-related tasks.","By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition.","Then, guided by the task definition, we automatically convert public video annotations into multiple-choice QA to evaluate each task.","On one hand, such a distinct paradigm allows us to build MVBench efficiently, without much manual intervention.","On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of LLMs.","Moreover, we further develop a robust video MLLM baseline, i.e., VideoChat2, by progressive multi-modal training with diverse instruction-tuning data.","The extensive results on our MVBench reveal that, the existing MLLMs are far from satisfactory in temporal understanding, while our VideoChat2 largely surpasses these leading models by over 15% on MVBench.","All models and data are available at https://github.com/OpenGVLab/Ask-Anything."],"url":"http://arxiv.org/abs/2311.17005v1"}
{"created":"2023-11-28 17:57:44","title":"Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following","abstract":"Existing text-to-image (T2I) diffusion models usually struggle in interpreting complex prompts, especially those with quantity, object-attribute binding, and multi-subject descriptions. In this work, we introduce a semantic panel as the middleware in decoding texts to images, supporting the generator to better follow instructions. The panel is obtained through arranging the visual concepts parsed from the input text by the aid of large language models, and then injected into the denoising network as a detailed control signal to complement the text condition. To facilitate text-to-panel learning, we come up with a carefully designed semantic formatting protocol, accompanied by a fully-automatic data preparation pipeline. Thanks to such a design, our approach, which we call Ranni, manages to enhance a pre-trained T2I generator regarding its textual controllability. More importantly, the introduction of the generative middleware brings a more convenient form of interaction (i.e., directly adjusting the elements in the panel or using language instructions) and further allows users to finely customize their generation, based on which we develop a practical system and showcase its potential in continuous generation and chatting-based editing.","sentences":["Existing text-to-image (T2I) diffusion models usually struggle in interpreting complex prompts, especially those with quantity, object-attribute binding, and multi-subject descriptions.","In this work, we introduce a semantic panel as the middleware in decoding texts to images, supporting the generator to better follow instructions.","The panel is obtained through arranging the visual concepts parsed from the input text by the aid of large language models, and then injected into the denoising network as a detailed control signal to complement the text condition.","To facilitate text-to-panel learning, we come up with a carefully designed semantic formatting protocol, accompanied by a fully-automatic data preparation pipeline.","Thanks to such a design, our approach, which we call Ranni, manages to enhance a pre-trained T2I generator regarding its textual controllability.","More importantly, the introduction of the generative middleware brings a more convenient form of interaction (i.e., directly adjusting the elements in the panel or using language instructions) and further allows users to finely customize their generation, based on which we develop a practical system and showcase its potential in continuous generation and chatting-based editing."],"url":"http://arxiv.org/abs/2311.17002v1"}
{"created":"2023-11-28 17:57:24","title":"New Approximation Bounds for Small-Set Vertex Expansion","abstract":"The vertex expansion of the graph is a fundamental graph parameter. Given a graph $G=(V,E)$ and a parameter $\\delta \\in (0,1/2]$, its $\\delta$-Small-Set Vertex Expansion (SSVE) is defined as \\[ \\min_{S : |S| = \\delta |V|} \\frac{|{\\partial^V(S)}|}{ \\min \\{ |S|, |S^c| \\} } \\] where $\\partial^V(S)$ is the vertex boundary of a set $S$. The SSVE~problem, in addition to being of independent interest as a natural graph partitioning problem, is also of interest due to its connections to the Strong Unique Games problem. We give a randomized algorithm running in time $n^{{\\sf poly}(1/\\delta)}$, which outputs a set $S$ of size $\\Theta(\\delta n)$, having vertex expansion at most \\[ \\max\\left(O(\\sqrt{\\phi^* \\log d \\log (1/\\delta)}) , \\tilde{O}(d\\log^2(1/\\delta)) \\cdot \\phi^* \\right), \\] where $d$ is the largest vertex degree of the graph, and $\\phi^*$ is the optimal $\\delta$-SSVE. The previous best-known guarantees for this were the bi-criteria bounds of $\\tilde{O}(1/\\delta)\\sqrt{\\phi^* \\log d}$ and $\\tilde{O}(1/\\delta)\\phi^* \\sqrt{\\log n}$ due to Louis-Makarychev [TOC'16].   Our algorithm uses the basic SDP relaxation of the problem augmented with ${\\rm poly}(1/\\delta)$ rounds of the Lasserre/SoS hierarchy. Our rounding algorithm is a combination of the rounding algorithms of Raghavendra-Tan [SODA'12] and Austrin-Benabbas-Georgiou [SODA'13]. A key component of our analysis is novel Gaussian rounding lemma for hyperedges which might be of independent interest.","sentences":["The vertex expansion of the graph is a fundamental graph parameter.","Given a graph $G=(V,E)$ and a parameter $\\delta \\in (0,1/2]$, its $\\delta$-Small-Set Vertex Expansion (SSVE) is defined as \\[ \\min_{S : |S| = \\delta |V|} \\frac{|{\\partial^V(S)}|}{ \\min \\{ |S|, |S^c| \\} } \\] where $\\partial^V(S)$ is the vertex boundary of a set $S$. The SSVE~problem, in addition to being of independent interest as a natural graph partitioning problem, is also of interest due to its connections to the Strong Unique Games problem.","We give a randomized algorithm running in time $n^{{\\sf poly}(1/\\delta)}$, which outputs a set $S$ of size $\\Theta(\\delta n)$, having vertex expansion at most \\[ \\max\\left(O(\\sqrt{\\phi^* \\log d \\log (1/\\delta)}) , \\tilde{O}(d\\log^2(1/\\delta))","\\cdot \\phi^* \\right), \\] where $d$ is the largest vertex degree of the graph, and $\\phi^*$ is the optimal $\\delta$-SSVE.","The previous best-known guarantees for this were the bi-criteria bounds of $\\tilde{O}(1/\\delta)\\sqrt{\\phi^* \\log d}$ and $\\tilde{O}(1/\\delta)\\phi^* \\sqrt{\\log n}$ due to Louis-Makarychev","[TOC'16].   ","Our algorithm uses the basic SDP relaxation of the problem augmented with ${\\rm poly}(1/\\delta)$ rounds of the Lasserre/SoS hierarchy.","Our rounding algorithm is a combination of the rounding algorithms of Raghavendra-Tan","[SODA'12] and Austrin-Benabbas-Georgiou","[SODA'13].","A key component of our analysis is novel Gaussian rounding lemma for hyperedges which might be of independent interest."],"url":"http://arxiv.org/abs/2311.17001v1"}
{"created":"2023-11-28 17:48:18","title":"Goal-conditioned Offline Planning from Curious Exploration","abstract":"Curiosity has established itself as a powerful exploration strategy in deep reinforcement learning. Notably, leveraging expected future novelty as intrinsic motivation has been shown to efficiently generate exploratory trajectories, as well as a robust dynamics model. We consider the challenge of extracting goal-conditioned behavior from the products of such unsupervised exploration techniques, without any additional environment interaction. We find that conventional goal-conditioned reinforcement learning approaches for extracting a value function and policy fall short in this difficult offline setting. By analyzing the geometry of optimal goal-conditioned value functions, we relate this issue to a specific class of estimation artifacts in learned values. In order to mitigate their occurrence, we propose to combine model-based planning over learned value landscapes with a graph-based value aggregation scheme. We show how this combination can correct both local and global artifacts, obtaining significant improvements in zero-shot goal-reaching performance across diverse simulated environments.","sentences":["Curiosity has established itself as a powerful exploration strategy in deep reinforcement learning.","Notably, leveraging expected future novelty as intrinsic motivation has been shown to efficiently generate exploratory trajectories, as well as a robust dynamics model.","We consider the challenge of extracting goal-conditioned behavior from the products of such unsupervised exploration techniques, without any additional environment interaction.","We find that conventional goal-conditioned reinforcement learning approaches for extracting a value function and policy fall short in this difficult offline setting.","By analyzing the geometry of optimal goal-conditioned value functions, we relate this issue to a specific class of estimation artifacts in learned values.","In order to mitigate their occurrence, we propose to combine model-based planning over learned value landscapes with a graph-based value aggregation scheme.","We show how this combination can correct both local and global artifacts, obtaining significant improvements in zero-shot goal-reaching performance across diverse simulated environments."],"url":"http://arxiv.org/abs/2311.16996v1"}
{"created":"2023-11-28 17:46:57","title":"Nested Integrals and Rationalizing Transformations","abstract":"A brief overview of some computer algebra methods for computations with nested integrals is given. The focus is on nested integrals over integrands involving square roots. Rewrite rules for conversion to and from associated nested sums are discussed. We also include a short discussion comparing the holonomic systems approach and the differential field approach. For simplification to rational integrands, we give a comprehensive list of univariate rationalizing transformations, including transformations tuned to map the interval $[0,1]$ bijectively to itself.","sentences":["A brief overview of some computer algebra methods for computations with nested integrals is given.","The focus is on nested integrals over integrands involving square roots.","Rewrite rules for conversion to and from associated nested sums are discussed.","We also include a short discussion comparing the holonomic systems approach and the differential field approach.","For simplification to rational integrands, we give a comprehensive list of univariate rationalizing transformations, including transformations tuned to map the interval $[0,1]$ bijectively to itself."],"url":"http://arxiv.org/abs/2311.16992v1"}
{"created":"2023-11-28 17:44:51","title":"ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?","abstract":"Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.","sentences":["Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce.","Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks.","Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs.","While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks.","This has crucial implications not only on research but also on business.","In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT."],"url":"http://arxiv.org/abs/2311.16989v1"}
{"created":"2023-11-28 17:25:34","title":"Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models","abstract":"Subject-verb agreement in the presence of an attractor noun located between the main noun and the verb elicits complex behavior: judgments of grammaticality are modulated by the grammatical features of the attractor. For example, in the sentence \"The girl near the boys likes climbing\", the attractor (boys) disagrees in grammatical number with the verb (likes), creating a locally implausible transition probability. Here, we parametrically modulate the distance between the attractor and the verb while keeping the length of the sentence equal. We evaluate the performance of both humans and two artificial neural network models: both make more mistakes when the attractor is closer to the verb, but neural networks get close to the chance level while humans are mostly able to overcome the attractor interference. Additionally, we report a linear effect of attractor distance on reaction times. We hypothesize that a possible reason for the proximity effect is the calculation of transition probabilities between adjacent words. Nevertheless, classical models of attraction such as the cue-based model might suffice to explain this phenomenon, thus paving the way for new research. Data and analyses available at https://osf.io/d4g6k","sentences":["Subject-verb agreement in the presence of an attractor noun located between the main noun and the verb elicits complex behavior:","judgments of grammaticality are modulated by the grammatical features of the attractor.","For example, in the sentence \"The girl near the boys likes climbing\", the attractor (boys) disagrees in grammatical number with the verb (likes), creating a locally implausible transition probability.","Here, we parametrically modulate the distance between the attractor and the verb while keeping the length of the sentence equal.","We evaluate the performance of both humans and two artificial neural network models: both make more mistakes when the attractor is closer to the verb, but neural networks get close to the chance level while humans are mostly able to overcome the attractor interference.","Additionally, we report a linear effect of attractor distance on reaction times.","We hypothesize that a possible reason for the proximity effect is the calculation of transition probabilities between adjacent words.","Nevertheless, classical models of attraction such as the cue-based model might suffice to explain this phenomenon, thus paving the way for new research.","Data and analyses available at https://osf.io/d4g6k"],"url":"http://arxiv.org/abs/2311.16978v1"}
{"created":"2023-11-28 17:25:16","title":"Bidirectional Reactive Programming for Machine Learning","abstract":"Reactive languages are dedicated to the programming of systems which interact continuously and concurrently with their environment. Values take the form of unbounded streams modeling the (discrete) passing of time or the sequence of concurrent interactions. While conventional reactivity models recurrences forward in time, we introduce a symmetric reactive construct enabling backward recurrences. Constraints on the latter allow to make the implementation practical. Machine Learning (ML) systems provide numerous motivations for all of this: we demonstrate that reverse-mode automatic differentiation, backpropagation, batch normalization, bidirectional recurrent neural networks, training and reinforcement learning algorithms, are all naturally captured as bidirectional reactive programs.","sentences":["Reactive languages are dedicated to the programming of systems which interact continuously and concurrently with their environment.","Values take the form of unbounded streams modeling the (discrete) passing of time or the sequence of concurrent interactions.","While conventional reactivity models recurrences forward in time, we introduce a symmetric reactive construct enabling backward recurrences.","Constraints on the latter allow to make the implementation practical.","Machine Learning (ML) systems provide numerous motivations for all of this: we demonstrate that reverse-mode automatic differentiation, backpropagation, batch normalization, bidirectional recurrent neural networks, training and reinforcement learning algorithms, are all naturally captured as bidirectional reactive programs."],"url":"http://arxiv.org/abs/2311.16977v1"}
{"created":"2023-11-28 17:22:17","title":"COLE: A Hierarchical Generation Framework for Graphic Design","abstract":"Graphic design, which has been evolving since the 15th century, plays a crucial role in advertising. The creation of high-quality designs demands creativity, innovation, and lateral thinking. This intricate task involves understanding the objective, crafting visual elements such as the background, decoration, font, color, and shape, formulating diverse professional layouts, and adhering to fundamental visual design principles. In this paper, we introduce COLE, a hierarchical generation framework designed to comprehensively address these challenges. This COLE system can transform a straightforward intention prompt into a high-quality graphic design, while also supporting flexible editing based on user input. Examples of such input might include directives like ``design a poster for Hisaishi's concert.'' The key insight is to dissect the complex task of text-to-design generation into a hierarchy of simpler sub-tasks, each addressed by specialized models working collaboratively. The results from these models are then consolidated to produce a cohesive final output. Our hierarchical task decomposition can streamline the complex process and significantly enhance generation reliability. Our COLE system consists of multiple fine-tuned Large Language Models (LLMs), Large Multimodal Models (LMMs), and Diffusion Models (DMs), each specifically tailored for a design-aware text or image generation task. Furthermore, we construct the DESIGNERINTENTION benchmark to highlight the superiority of our COLE over existing methods in generating high-quality graphic designs from user intent. We perceive our COLE as an important step towards addressing more complex visual design generation tasks in the future.","sentences":["Graphic design, which has been evolving since the 15th century, plays a crucial role in advertising.","The creation of high-quality designs demands creativity, innovation, and lateral thinking.","This intricate task involves understanding the objective, crafting visual elements such as the background, decoration, font, color, and shape, formulating diverse professional layouts, and adhering to fundamental visual design principles.","In this paper, we introduce COLE, a hierarchical generation framework designed to comprehensively address these challenges.","This COLE system can transform a straightforward intention prompt into a high-quality graphic design, while also supporting flexible editing based on user input.","Examples of such input might include directives like ``design a poster for Hisaishi's concert.''","The key insight is to dissect the complex task of text-to-design generation into a hierarchy of simpler sub-tasks, each addressed by specialized models working collaboratively.","The results from these models are then consolidated to produce a cohesive final output.","Our hierarchical task decomposition can streamline the complex process and significantly enhance generation reliability.","Our COLE system consists of multiple fine-tuned Large Language Models (LLMs), Large Multimodal Models (LMMs), and Diffusion Models (DMs), each specifically tailored for a design-aware text or image generation task.","Furthermore, we construct the DESIGNERINTENTION benchmark to highlight the superiority of our COLE over existing methods in generating high-quality graphic designs from user intent.","We perceive our COLE as an important step towards addressing more complex visual design generation tasks in the future."],"url":"http://arxiv.org/abs/2311.16974v1"}
{"created":"2023-11-28 17:12:06","title":"Natural Language Processing Through Transfer Learning: A Case Study on Sentiment Analysis","abstract":"Artificial intelligence and machine learning have significantly bolstered the technological world. This paper explores the potential of transfer learning in natural language processing focusing mainly on sentiment analysis. The models trained on the big data can also be used where data are scarce. The claim is that, compared to training models from scratch, transfer learning, using pre-trained BERT models, can increase sentiment classification accuracy. The study adopts a sophisticated experimental design that uses the IMDb dataset of sentimentally labelled movie reviews. Pre-processing includes tokenization and encoding of text data, making it suitable for NLP models. The dataset is used on a BERT based model, measuring its performance using accuracy. The result comes out to be 100 per cent accurate. Although the complete accuracy could appear impressive, it might be the result of overfitting or a lack of generalization. Further analysis is required to ensure the model's ability to handle diverse and unseen data. The findings underscore the effectiveness of transfer learning in NLP, showcasing its potential to excel in sentiment analysis tasks. However, the research calls for a cautious interpretation of perfect accuracy and emphasizes the need for additional measures to validate the model's generalization.","sentences":["Artificial intelligence and machine learning have significantly bolstered the technological world.","This paper explores the potential of transfer learning in natural language processing focusing mainly on sentiment analysis.","The models trained on the big data can also be used where data are scarce.","The claim is that, compared to training models from scratch, transfer learning, using pre-trained BERT models, can increase sentiment classification accuracy.","The study adopts a sophisticated experimental design that uses the IMDb dataset of sentimentally labelled movie reviews.","Pre-processing includes tokenization and encoding of text data, making it suitable for NLP models.","The dataset is used on a BERT based model, measuring its performance using accuracy.","The result comes out to be 100 per cent accurate.","Although the complete accuracy could appear impressive, it might be the result of overfitting or a lack of generalization.","Further analysis is required to ensure the model's ability to handle diverse and unseen data.","The findings underscore the effectiveness of transfer learning in NLP, showcasing its potential to excel in sentiment analysis tasks.","However, the research calls for a cautious interpretation of perfect accuracy and emphasizes the need for additional measures to validate the model's generalization."],"url":"http://arxiv.org/abs/2311.16965v1"}
{"created":"2023-11-28 17:06:28","title":"HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion","abstract":"Generating a 3D human model from a single reference image is challenging because it requires inferring textures and geometries in invisible views while maintaining consistency with the reference image. Previous methods utilizing 3D generative models are limited by the availability of 3D training data. Optimization-based methods that lift text-to-image diffusion models to 3D generation often fail to preserve the texture details of the reference image, resulting in inconsistent appearances in different views. In this paper, we propose HumanRef, a 3D human generation framework from a single-view input. To ensure the generated 3D model is photorealistic and consistent with the input image, HumanRef introduces a novel method called reference-guided score distillation sampling (Ref-SDS), which effectively incorporates image guidance into the generation process. Furthermore, we introduce region-aware attention to Ref-SDS, ensuring accurate correspondence between different body regions. Experimental results demonstrate that HumanRef outperforms state-of-the-art methods in generating 3D clothed humans with fine geometry, photorealistic textures, and view-consistent appearances.","sentences":["Generating a 3D human model from a single reference image is challenging because it requires inferring textures and geometries in invisible views while maintaining consistency with the reference image.","Previous methods utilizing 3D generative models are limited by the availability of 3D training data.","Optimization-based methods that lift text-to-image diffusion models to 3D generation often fail to preserve the texture details of the reference image, resulting in inconsistent appearances in different views.","In this paper, we propose HumanRef, a 3D human generation framework from a single-view input.","To ensure the generated 3D model is photorealistic and consistent with the input image, HumanRef introduces a novel method called reference-guided score distillation sampling (Ref-SDS), which effectively incorporates image guidance into the generation process.","Furthermore, we introduce region-aware attention to Ref-SDS, ensuring accurate correspondence between different body regions.","Experimental results demonstrate that HumanRef outperforms state-of-the-art methods in generating 3D clothed humans with fine geometry, photorealistic textures, and view-consistent appearances."],"url":"http://arxiv.org/abs/2311.16961v1"}
{"created":"2023-11-28 17:05:32","title":"Principal-Agent Problem with Third Party: Information Design from Social Planner's Perspective","abstract":"We study the principal-agent problem with a third party that we call social planner, whose responsibility is to reconcile the conflicts of interest between the two players and induce socially optimal outcome in terms of some given social utility function. The social planner owns no contractual power but manages to control the information flow between the principal and the agent. We design a simple workflow with two stages for the social planner. In the first stage, the problem is reformulated as an optimization problem whose solution is the optimal utility profile. In the second stage, we investigate information design and show that binary-signal information structure suffices to induce the socially optimal outcome determined in the first stage. The result shows that information plays a key role in social planning in the principal-agent model.","sentences":["We study the principal-agent problem with a third party that we call social planner, whose responsibility is to reconcile the conflicts of interest between the two players and induce socially optimal outcome in terms of some given social utility function.","The social planner owns no contractual power but manages to control the information flow between the principal and the agent.","We design a simple workflow with two stages for the social planner.","In the first stage, the problem is reformulated as an optimization problem whose solution is the optimal utility profile.","In the second stage, we investigate information design and show that binary-signal information structure suffices to induce the socially optimal outcome determined in the first stage.","The result shows that information plays a key role in social planning in the principal-agent model."],"url":"http://arxiv.org/abs/2311.16959v1"}
{"created":"2023-11-28 17:05:25","title":"From Simulations to Reality: Enhancing Multi-Robot Exploration for Urban Search and Rescue","abstract":"In this study, we present a novel hybrid algorithm, combining Levy Flight (LF) and Particle Swarm Optimization (PSO) (LF-PSO), tailored for efficient multi-robot exploration in unknown environments with limited communication and no global positioning information. The research addresses the growing interest in employing multiple autonomous robots for exploration tasks, particularly in scenarios such as Urban Search and Rescue (USAR) operations. Multiple robots offer advantages like increased task coverage, robustness, flexibility, and scalability. However, existing approaches often make assumptions such as search area, robot positioning, communication restrictions, and target information that may not hold in real-world situations. The hybrid algorithm leverages LF, known for its effectiveness in large space exploration with sparse targets, and incorporates inter-robot repulsion as a social component through PSO. This combination enhances area exploration efficiency. We redefine the local best and global best positions to suit scenarios without continuous target information. Experimental simulations in a controlled environment demonstrate the algorithm's effectiveness, showcasing improved area coverage compared to traditional methods. In the process of refining our approach and testing it in complex, obstacle-rich environments, the presented work holds promise for enhancing multi-robot exploration in scenarios with limited information and communication capabilities.","sentences":["In this study, we present a novel hybrid algorithm, combining Levy Flight (LF) and Particle Swarm Optimization (PSO) (LF-PSO), tailored for efficient multi-robot exploration in unknown environments with limited communication and no global positioning information.","The research addresses the growing interest in employing multiple autonomous robots for exploration tasks, particularly in scenarios such as Urban Search and Rescue (USAR) operations.","Multiple robots offer advantages like increased task coverage, robustness, flexibility, and scalability.","However, existing approaches often make assumptions such as search area, robot positioning, communication restrictions, and target information that may not hold in real-world situations.","The hybrid algorithm leverages LF, known for its effectiveness in large space exploration with sparse targets, and incorporates inter-robot repulsion as a social component through PSO.","This combination enhances area exploration efficiency.","We redefine the local best and global best positions to suit scenarios without continuous target information.","Experimental simulations in a controlled environment demonstrate the algorithm's effectiveness, showcasing improved area coverage compared to traditional methods.","In the process of refining our approach and testing it in complex, obstacle-rich environments, the presented work holds promise for enhancing multi-robot exploration in scenarios with limited information and communication capabilities."],"url":"http://arxiv.org/abs/2311.16958v1"}
{"created":"2023-11-28 17:04:27","title":"Space-Efficient Data Structures for Polyominoes and Bar Graphs","abstract":"We provide a compact data structure for representing polyominoes that supports neighborhood and visibility queries. Neighborhood queries concern reporting adjacent cells to a given cell, and visibility queries determine whether a straight line can be drawn within the polyomino that connects two specified cells. For an arbitrary small $\\epsilon >0$, our data structure can encode a polyomino with $n$ cells in $(3+\\epsilon)n + o(n)$ bits while supporting all queries in constant time. The space complexity can be improved to $3n+o(n)$, while supporting neighborhood queries in $\\mathcal{O}(1)$ and visibility queries in $\\mathcal{O}(t(n))$ for any arbitrary $t(n) \\in \\omega(1)$. Previous attempts at enumerating polyominoes have indicated that at least $2.00091n - o(n)$ bits are required to differentiate between distinct polyominoes, which shows our data structure is compact.   In addition, we introduce a succinct data structure tailored for bar graphs, a specific subclass of polyominoes resembling histograms. We demonstrate that a bar graph comprising $n$ cells can be encoded using only $n + o(n)$ bits, enabling constant-time query processing. Meanwhile, $n-1$ bits are necessary to represent any bar graph, proving our data structure is succinct.","sentences":["We provide a compact data structure for representing polyominoes that supports neighborhood and visibility queries.","Neighborhood queries concern reporting adjacent cells to a given cell, and visibility queries determine whether a straight line can be drawn within the polyomino that connects two specified cells.","For an arbitrary small $\\epsilon >0$, our data structure can encode a polyomino with $n$ cells in $(3+\\epsilon)n + o(n)$ bits while supporting all queries in constant time.","The space complexity can be improved to $3n+o(n)$, while supporting neighborhood queries in $\\mathcal{O}(1)$ and visibility queries in $\\mathcal{O}(t(n))$ for any arbitrary $t(n) \\in \\omega(1)$. Previous attempts at enumerating polyominoes have indicated that at least $2.00091n - o(n)$ bits are required to differentiate between distinct polyominoes, which shows our data structure is compact.   ","In addition, we introduce a succinct data structure tailored for bar graphs, a specific subclass of polyominoes resembling histograms.","We demonstrate that a bar graph comprising $n$ cells can be encoded using only $n + o(n)$ bits, enabling constant-time query processing.","Meanwhile, $n-1$ bits are necessary to represent any bar graph, proving our data structure is succinct."],"url":"http://arxiv.org/abs/2311.16957v1"}
{"created":"2023-11-28 16:59:42","title":"Local certification of geometric graph classes","abstract":"The goal of local certification is to locally convince the vertices of a graph $G$ that $G$ satisfies a given property. A prover assigns short certificates to the vertices of the graph, then the vertices are allowed to check their certificates and the certificates of their neighbors, and based only on this local view, they must decide whether $G$ satisfies the given property. If the graph indeed satisfies the property, all vertices must accept the instance, and otherwise at least one vertex must reject the instance (for any possible assignment of certificates). The goal is to minimize to size of the certificates.   In this paper we study the local certification of geometric and topological graph classes. While it is known that in $n$-vertex graphs, planarity can be certified locally with certificates of size $O(\\log n)$, we show that several closely related graph classes require certificates of size $\\Omega(n)$. This includes penny graphs, unit-distance graphs, (induced) subgraphs of the square grid, 1-planar graphs, and unit-square graphs. For unit-disk graphs we obtain a lower bound of $\\Omega(n^{1-\\delta})$ for any $\\delta>0$ on the size of the certificates. All our results are tight up to a $n^{o(1)}$ factor, and give the first known examples of hereditary (and even monotone) graph classes for which the certificates must have polynomial size. The lower bounds are obtained by proving rigidity properties of the considered graphs, which might be of independent interest.","sentences":["The goal of local certification is to locally convince the vertices of a graph $G$ that $G$ satisfies a given property.","A prover assigns short certificates to the vertices of the graph, then the vertices are allowed to check their certificates and the certificates of their neighbors, and based only on this local view, they must decide whether $G$ satisfies the given property.","If the graph indeed satisfies the property, all vertices must accept the instance, and otherwise at least one vertex must reject the instance (for any possible assignment of certificates).","The goal is to minimize to size of the certificates.   ","In this paper we study the local certification of geometric and topological graph classes.","While it is known that in $n$-vertex graphs, planarity can be certified locally with certificates of size $O(\\log n)$, we show that several closely related graph classes require certificates of size $\\Omega(n)$. This includes penny graphs, unit-distance graphs, (induced) subgraphs of the square grid, 1-planar graphs, and unit-square graphs.","For unit-disk graphs we obtain a lower bound of $\\Omega(n^{1-\\delta})$ for any $\\delta>0$ on the size of the certificates.","All our results are tight up to a $n^{o(1)}$ factor, and give the first known examples of hereditary (and even monotone) graph classes for which the certificates must have polynomial size.","The lower bounds are obtained by proving rigidity properties of the considered graphs, which might be of independent interest."],"url":"http://arxiv.org/abs/2311.16953v1"}
{"created":"2023-11-28 16:50:57","title":"End-to-end Reinforcement Learning for Time-Optimal Quadcopter Flight","abstract":"Aggressive time-optimal control of quadcopters poses a significant challenge in the field of robotics. The state-of-the-art approach leverages reinforcement learning (RL) to train optimal neural policies. However, a critical hurdle is the sim-to-real gap, often addressed by employing a robust inner loop controller -an abstraction that, in theory, constrains the optimality of the trained controller, necessitating margins to counter potential disturbances. In contrast, our novel approach introduces high-speed quadcopter control using end-to-end RL (E2E) that gives direct motor commands. To bridge the reality gap, we incorporate a learned residual model and an adaptive method that can compensate for modeling errors in thrust and moments. We compare our E2E approach against a state-of-the-art network that commands thrust and body rates to an INDI inner loop controller, both in simulated and real-world flight. E2E showcases a significant 1.39-second advantage in simulation and a 0.17-second edge in real-world testing, highlighting end-to-end reinforcement learning's potential. The performance drop observed from simulation to reality shows potential for further improvement, including refining strategies to address the reality gap or exploring offline reinforcement learning with real flight data.","sentences":["Aggressive time-optimal control of quadcopters poses a significant challenge in the field of robotics.","The state-of-the-art approach leverages reinforcement learning (RL) to train optimal neural policies.","However, a critical hurdle is the sim-to-real gap, often addressed by employing a robust inner loop controller -an abstraction that, in theory, constrains the optimality of the trained controller, necessitating margins to counter potential disturbances.","In contrast, our novel approach introduces high-speed quadcopter control using end-to-end RL (E2E) that gives direct motor commands.","To bridge the reality gap, we incorporate a learned residual model and an adaptive method that can compensate for modeling errors in thrust and moments.","We compare our E2E approach against a state-of-the-art network that commands thrust and body rates to an INDI inner loop controller, both in simulated and real-world flight.","E2E showcases a significant 1.39-second advantage in simulation and a 0.17-second edge in real-world testing, highlighting end-to-end reinforcement learning's potential.","The performance drop observed from simulation to reality shows potential for further improvement, including refining strategies to address the reality gap or exploring offline reinforcement learning with real flight data."],"url":"http://arxiv.org/abs/2311.16948v1"}
{"created":"2023-11-28 16:47:59","title":"UC-NeRF: Neural Radiance Field for Under-Calibrated multi-view cameras in autonomous driving","abstract":"Multi-camera setups find widespread use across various applications, such as autonomous driving, as they greatly expand sensing capabilities. Despite the fast development of Neural radiance field (NeRF) techniques and their wide applications in both indoor and outdoor scenes, applying NeRF to multi-camera systems remains very challenging. This is primarily due to the inherent under-calibration issues in multi-camera setup, including inconsistent imaging effects stemming from separately calibrated image signal processing units in diverse cameras, and system errors arising from mechanical vibrations during driving that affect relative camera poses. In this paper, we present UC-NeRF, a novel method tailored for novel view synthesis in under-calibrated multi-view camera systems. Firstly, we propose a layer-based color correction to rectify the color inconsistency in different image regions. Second, we propose virtual warping to generate more viewpoint-diverse but color-consistent virtual views for color correction and 3D recovery. Finally, a spatiotemporally constrained pose refinement is designed for more robust and accurate pose calibration in multi-camera systems. Our method not only achieves state-of-the-art performance of novel view synthesis in multi-camera setups, but also effectively facilitates depth estimation in large-scale outdoor scenes with the synthesized novel views.","sentences":["Multi-camera setups find widespread use across various applications, such as autonomous driving, as they greatly expand sensing capabilities.","Despite the fast development of Neural radiance field (NeRF) techniques and their wide applications in both indoor and outdoor scenes, applying NeRF to multi-camera systems remains very challenging.","This is primarily due to the inherent under-calibration issues in multi-camera setup, including inconsistent imaging effects stemming from separately calibrated image signal processing units in diverse cameras, and system errors arising from mechanical vibrations during driving that affect relative camera poses.","In this paper, we present UC-NeRF, a novel method tailored for novel view synthesis in under-calibrated multi-view camera systems.","Firstly, we propose a layer-based color correction to rectify the color inconsistency in different image regions.","Second, we propose virtual warping to generate more viewpoint-diverse but color-consistent virtual views for color correction and 3D recovery.","Finally, a spatiotemporally constrained pose refinement is designed for more robust and accurate pose calibration in multi-camera systems.","Our method not only achieves state-of-the-art performance of novel view synthesis in multi-camera setups, but also effectively facilitates depth estimation in large-scale outdoor scenes with the synthesized novel views."],"url":"http://arxiv.org/abs/2311.16945v1"}
{"created":"2023-11-28 16:46:44","title":"Image segmentation with traveling waves in an exactly solvable recurrent neural network","abstract":"We study image segmentation using spatiotemporal dynamics in a recurrent neural network where the state of each unit is given by a complex number. We show that this network generates sophisticated spatiotemporal dynamics that can effectively divide an image into groups according to a scene's structural characteristics. Using an exact solution of the recurrent network's dynamics, we present a precise description of the mechanism underlying object segmentation in this network, providing a clear mathematical interpretation of how the network performs this task. We then demonstrate a simple algorithm for object segmentation that generalizes across inputs ranging from simple geometric objects in grayscale images to natural images. Object segmentation across all images is accomplished with one recurrent neural network that has a single, fixed set of weights. This demonstrates the expressive potential of recurrent neural networks when constructed using a mathematical approach that brings together their structure, dynamics, and computation.","sentences":["We study image segmentation using spatiotemporal dynamics in a recurrent neural network where the state of each unit is given by a complex number.","We show that this network generates sophisticated spatiotemporal dynamics that can effectively divide an image into groups according to a scene's structural characteristics.","Using an exact solution of the recurrent network's dynamics, we present a precise description of the mechanism underlying object segmentation in this network, providing a clear mathematical interpretation of how the network performs this task.","We then demonstrate a simple algorithm for object segmentation that generalizes across inputs ranging from simple geometric objects in grayscale images to natural images.","Object segmentation across all images is accomplished with one recurrent neural network that has a single, fixed set of weights.","This demonstrates the expressive potential of recurrent neural networks when constructed using a mathematical approach that brings together their structure, dynamics, and computation."],"url":"http://arxiv.org/abs/2311.16943v1"}
{"created":"2023-11-28 16:46:14","title":"Debiasing Multimodal Models via Causal Information Minimization","abstract":"Most existing debiasing methods for multimodal models, including causal intervention and inference methods, utilize approximate heuristics to represent the biases, such as shallow features from early stages of training or unimodal features for multimodal tasks like VQA, etc., which may not be accurate. In this paper, we study bias arising from confounders in a causal graph for multimodal data and examine a novel approach that leverages causally-motivated information minimization to learn the confounder representations. Robust predictive features contain diverse information that helps a model generalize to out-of-distribution data. Hence, minimizing the information content of features obtained from a pretrained biased model helps learn the simplest predictive features that capture the underlying data distribution. We treat these features as confounder representations and use them via methods motivated by causal theory to remove bias from models. We find that the learned confounder representations indeed capture dataset biases, and the proposed debiasing methods improve out-of-distribution (OOD) performance on multiple multimodal datasets without sacrificing in-distribution performance. Additionally, we introduce a novel metric to quantify the sufficiency of spurious features in models' predictions that further demonstrates the effectiveness of our proposed methods. Our code is available at: https://github.com/Vaidehi99/CausalInfoMin","sentences":["Most existing debiasing methods for multimodal models, including causal intervention and inference methods, utilize approximate heuristics to represent the biases, such as shallow features from early stages of training or unimodal features for multimodal tasks like VQA, etc., which may not be accurate.","In this paper, we study bias arising from confounders in a causal graph for multimodal data and examine a novel approach that leverages causally-motivated information minimization to learn the confounder representations.","Robust predictive features contain diverse information that helps a model generalize to out-of-distribution data.","Hence, minimizing the information content of features obtained from a pretrained biased model helps learn the simplest predictive features that capture the underlying data distribution.","We treat these features as confounder representations and use them via methods motivated by causal theory to remove bias from models.","We find that the learned confounder representations indeed capture dataset biases, and the proposed debiasing methods improve out-of-distribution (OOD) performance on multiple multimodal datasets without sacrificing in-distribution performance.","Additionally, we introduce a novel metric to quantify the sufficiency of spurious features in models' predictions that further demonstrates the effectiveness of our proposed methods.","Our code is available at: https://github.com/Vaidehi99/CausalInfoMin"],"url":"http://arxiv.org/abs/2311.16941v1"}
{"created":"2023-11-28 16:43:17","title":"FP-Fed: Privacy-Preserving Federated Detection of Browser Fingerprinting","abstract":"Browser fingerprinting often provides an attractive alternative to third-party cookies for tracking users across the web. In fact, the increasing restrictions on third-party cookies placed by common web browsers and recent regulations like the GDPR may accelerate the transition. To counter browser fingerprinting, previous work proposed several techniques to detect its prevalence and severity. However, these rely on 1) centralized web crawls and/or 2) computationally intensive operations to extract and process signals (e.g., information-flow and static analysis). To address these limitations, we present FP-Fed, the first distributed system for browser fingerprinting detection. Using FP-Fed, users can collaboratively train on-device models based on their real browsing patterns, without sharing their training data with a central entity, by relying on Differentially Private Federated Learning (DP-FL). To demonstrate its feasibility and effectiveness, we evaluate FP-Fed's performance on a set of 18.3k popular websites with different privacy levels, numbers of participants, and features extracted from the scripts. Our experiments show that FP-Fed achieves reasonably high detection performance and can perform both training and inference efficiently, on-device, by only relying on runtime signals extracted from the execution trace, without requiring any resource-intensive operation.","sentences":["Browser fingerprinting often provides an attractive alternative to third-party cookies for tracking users across the web.","In fact, the increasing restrictions on third-party cookies placed by common web browsers and recent regulations like the GDPR may accelerate the transition.","To counter browser fingerprinting, previous work proposed several techniques to detect its prevalence and severity.","However, these rely on 1) centralized web crawls and/or 2) computationally intensive operations to extract and process signals (e.g., information-flow and static analysis).","To address these limitations, we present FP-Fed, the first distributed system for browser fingerprinting detection.","Using FP-Fed, users can collaboratively train on-device models based on their real browsing patterns, without sharing their training data with a central entity, by relying on Differentially Private Federated Learning (DP-FL).","To demonstrate its feasibility and effectiveness, we evaluate FP-Fed's performance on a set of 18.3k popular websites with different privacy levels, numbers of participants, and features extracted from the scripts.","Our experiments show that FP-Fed achieves reasonably high detection performance and can perform both training and inference efficiently, on-device, by only relying on runtime signals extracted from the execution trace, without requiring any resource-intensive operation."],"url":"http://arxiv.org/abs/2311.16940v1"}
{"created":"2023-11-28 16:39:49","title":"The Sky's the Limit: Re-lightable Outdoor Scenes via a Sky-pixel Constrained Illumination Prior and Outside-In Visibility","abstract":"Inverse rendering of outdoor scenes from unconstrained image collections is a challenging task, particularly illumination/albedo ambiguities and occlusion of the illumination environment (shadowing) caused by geometry. However, there are many cues in an image that can aid in the disentanglement of geometry, albedo and shadows. We exploit the fact that any sky pixel provides a direct measurement of distant lighting in the corresponding direction and, via a neural illumination prior, a statistical cue as to the remaining illumination environment. We also introduce a novel `outside-in' method for computing differentiable sky visibility based on a neural directional distance function. This is efficient and can be trained in parallel with the neural scene representation, allowing gradients from appearance loss to flow from shadows to influence estimation of illumination and geometry. Our method estimates high-quality albedo, geometry, illumination and sky visibility, achieving state-of-the-art results on the NeRF-OSR relighting benchmark. Our code and models can be found https://github.com/JADGardner/neusky","sentences":["Inverse rendering of outdoor scenes from unconstrained image collections is a challenging task, particularly illumination/albedo ambiguities and occlusion of the illumination environment (shadowing) caused by geometry.","However, there are many cues in an image that can aid in the disentanglement of geometry, albedo and shadows.","We exploit the fact that any sky pixel provides a direct measurement of distant lighting in the corresponding direction and, via a neural illumination prior, a statistical cue as to the remaining illumination environment.","We also introduce a novel `outside-in' method for computing differentiable sky visibility based on a neural directional distance function.","This is efficient and can be trained in parallel with the neural scene representation, allowing gradients from appearance loss to flow from shadows to influence estimation of illumination and geometry.","Our method estimates high-quality albedo, geometry, illumination and sky visibility, achieving state-of-the-art results on the NeRF-OSR relighting benchmark.","Our code and models can be found https://github.com/JADGardner/neusky"],"url":"http://arxiv.org/abs/2311.16937v1"}
{"created":"2023-11-28 16:33:08","title":"SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models","abstract":"The development of text-to-video (T2V), i.e., generating videos with a given text prompt, has been significantly advanced in recent years. However, relying solely on text prompts often results in ambiguous frame composition due to spatial uncertainty. The research community thus leverages the dense structure signals, e.g., per-frame depth/edge sequences, to enhance controllability, whose collection accordingly increases the burden of inference. In this work, we present SparseCtrl to enable flexible structure control with temporally sparse signals, requiring only one or a few inputs, as shown in Figure 1. It incorporates an additional condition encoder to process these sparse signals while leaving the pre-trained T2V model untouched. The proposed approach is compatible with various modalities, including sketches, depth maps, and RGB images, providing more practical control for video generation and promoting applications such as storyboarding, depth rendering, keyframe animation, and interpolation. Extensive experiments demonstrate the generalization of SparseCtrl on both original and personalized T2V generators. Codes and models will be publicly available at https://guoyww.github.io/projects/SparseCtrl .","sentences":["The development of text-to-video (T2V), i.e., generating videos with a given text prompt, has been significantly advanced in recent years.","However, relying solely on text prompts often results in ambiguous frame composition due to spatial uncertainty.","The research community thus leverages the dense structure signals, e.g., per-frame depth/edge sequences, to enhance controllability, whose collection accordingly increases the burden of inference.","In this work, we present SparseCtrl to enable flexible structure control with temporally sparse signals, requiring only one or a few inputs, as shown in Figure 1.","It incorporates an additional condition encoder to process these sparse signals while leaving the pre-trained T2V model untouched.","The proposed approach is compatible with various modalities, including sketches, depth maps, and RGB images, providing more practical control for video generation and promoting applications such as storyboarding, depth rendering, keyframe animation, and interpolation.","Extensive experiments demonstrate the generalization of SparseCtrl on both original and personalized T2V generators.","Codes and models will be publicly available at https://guoyww.github.io/projects/SparseCtrl ."],"url":"http://arxiv.org/abs/2311.16933v1"}
{"created":"2023-11-28 16:31:27","title":"LLaFS: When Large-Language Models Meet Few-Shot Segmentation","abstract":"This paper proposes LLaFS, the first attempt to leverage large language models (LLMs) in few-shot segmentation. In contrast to the conventional few-shot segmentation methods that only rely on the limited and biased information from the annotated support images, LLaFS leverages the vast prior knowledge gained by LLM as an effective supplement and directly uses the LLM to segment images in a few-shot manner. To enable the text-based LLM to handle image-related tasks, we carefully design an input instruction that allows the LLM to produce segmentation results represented as polygons, and propose a region-attribute table to simulate the human visual mechanism and provide multi-modal guidance. We also synthesize pseudo samples and use curriculum learning for pretraining to augment data and achieve better optimization. LLaFS achieves state-of-the-art results on multiple datasets, showing the potential of using LLMs for few-shot computer vision tasks. Code will be available at https://github.com/lanyunzhu99/LLaFS.","sentences":["This paper proposes LLaFS, the first attempt to leverage large language models (LLMs) in few-shot segmentation.","In contrast to the conventional few-shot segmentation methods that only rely on the limited and biased information from the annotated support images, LLaFS leverages the vast prior knowledge gained by LLM as an effective supplement and directly uses the LLM to segment images in a few-shot manner.","To enable the text-based LLM to handle image-related tasks, we carefully design an input instruction that allows the LLM to produce segmentation results represented as polygons, and propose a region-attribute table to simulate the human visual mechanism and provide multi-modal guidance.","We also synthesize pseudo samples and use curriculum learning for pretraining to augment data and achieve better optimization.","LLaFS achieves state-of-the-art results on multiple datasets, showing the potential of using LLMs for few-shot computer vision tasks.","Code will be available at https://github.com/lanyunzhu99/LLaFS."],"url":"http://arxiv.org/abs/2311.16926v1"}
{"created":"2023-11-28 16:27:24","title":"Super-Resolution through StyleGAN Regularized Latent Search: A Realism-Fidelity Trade-off","abstract":"This paper addresses the problem of super-resolution: constructing a highly resolved (HR) image from a low resolved (LR) one. Recent unsupervised approaches search the latent space of a StyleGAN pre-trained on HR images, for the image that best downscales to the input LR image. However, they tend to produce out-of-domain images and fail to accurately reconstruct HR images that are far from the original domain. Our contribution is twofold. Firstly, we introduce a new regularizer to constrain the search in the latent space, ensuring that the inverted code lies in the original image manifold. Secondly, we further enhanced the reconstruction through expanding the image prior around the optimal latent code. Our results show that the proposed approach recovers realistic high-quality images for large magnification factors. Furthermore, for low magnification factors, it can still reconstruct details that the generator could not have produced otherwise. Altogether, our approach achieves a good trade-off between fidelity and realism for the super-resolution task.","sentences":["This paper addresses the problem of super-resolution: constructing a highly resolved (HR) image from a low resolved (LR) one.","Recent unsupervised approaches search the latent space of a StyleGAN pre-trained on HR images, for the image that best downscales to the input LR image.","However, they tend to produce out-of-domain images and fail to accurately reconstruct HR images that are far from the original domain.","Our contribution is twofold.","Firstly, we introduce a new regularizer to constrain the search in the latent space, ensuring that the inverted code lies in the original image manifold.","Secondly, we further enhanced the reconstruction through expanding the image prior around the optimal latent code.","Our results show that the proposed approach recovers realistic high-quality images for large magnification factors.","Furthermore, for low magnification factors, it can still reconstruct details that the generator could not have produced otherwise.","Altogether, our approach achieves a good trade-off between fidelity and realism for the super-resolution task."],"url":"http://arxiv.org/abs/2311.16923v1"}
{"created":"2023-11-28 16:26:35","title":"Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding","abstract":"Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.","sentences":["Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned.","Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images.","To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs.","The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations.","This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs.","Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families.","Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability."],"url":"http://arxiv.org/abs/2311.16922v1"}
{"created":"2023-11-28 16:22:33","title":"RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D","abstract":"Lifting 2D diffusion for 3D generation is a challenging problem due to the lack of geometric prior and the complex entanglement of materials and lighting in natural images. Existing methods have shown promise by first creating the geometry through score-distillation sampling (SDS) applied to rendered surface normals, followed by appearance modeling. However, relying on a 2D RGB diffusion model to optimize surface normals is suboptimal due to the distribution discrepancy between natural images and normals maps, leading to instability in optimization. In this paper, recognizing that the normal and depth information effectively describe scene geometry and be automatically estimated from images, we propose to learn a generalizable Normal-Depth diffusion model for 3D generation. We achieve this by training on the large-scale LAION dataset together with the generalizable image-to-depth and normal prior models. In an attempt to alleviate the mixed illumination effects in the generated materials, we introduce an albedo diffusion model to impose data-driven constraints on the albedo component. Our experiments show that when integrated into existing text-to-3D pipelines, our models significantly enhance the detail richness, achieving state-of-the-art results. Our project page is https://lingtengqiu.github.io/RichDreamer/.","sentences":["Lifting 2D diffusion for 3D generation is a challenging problem due to the lack of geometric prior and the complex entanglement of materials and lighting in natural images.","Existing methods have shown promise by first creating the geometry through score-distillation sampling (SDS) applied to rendered surface normals, followed by appearance modeling.","However, relying on a 2D RGB diffusion model to optimize surface normals is suboptimal due to the distribution discrepancy between natural images and normals maps, leading to instability in optimization.","In this paper, recognizing that the normal and depth information effectively describe scene geometry and be automatically estimated from images, we propose to learn a generalizable Normal-Depth diffusion model for 3D generation.","We achieve this by training on the large-scale LAION dataset together with the generalizable image-to-depth and normal prior models.","In an attempt to alleviate the mixed illumination effects in the generated materials, we introduce an albedo diffusion model to impose data-driven constraints on the albedo component.","Our experiments show that when integrated into existing text-to-3D pipelines, our models significantly enhance the detail richness, achieving state-of-the-art results.","Our project page is https://lingtengqiu.github.io/RichDreamer/."],"url":"http://arxiv.org/abs/2311.16918v1"}
{"created":"2023-11-28 16:20:33","title":"UGG: Unified Generative Grasping","abstract":"Dexterous grasping aims to produce diverse grasping postures with a high grasping success rate. Regression-based methods that directly predict grasping parameters given the object may achieve a high success rate but often lack diversity. Generation-based methods that generate grasping postures conditioned on the object can often produce diverse grasping, but they are insufficient for high grasping success due to lack of discriminative information. To mitigate, we introduce a unified diffusion-based dexterous grasp generation model, dubbed the name UGG, which operates within the object point cloud and hand parameter spaces. Our all-transformer architecture unifies the information from the object, the hand, and the contacts, introducing a novel representation of contact points for improved contact modeling. The flexibility and quality of our model enable the integration of a lightweight discriminator, benefiting from simulated discriminative data, which pushes for a high success rate while preserving high diversity. Beyond grasp generation, our model can also generate objects based on hand information, offering valuable insights into object design and studying how the generative model perceives objects. Our model achieves state-of-the-art dexterous grasping on the large-scale DexGraspNet dataset while facilitating human-centric object design, marking a significant advancement in dexterous grasping research. Our project page is https://jiaxin-lu.github.io/ugg/ .","sentences":["Dexterous grasping aims to produce diverse grasping postures with a high grasping success rate.","Regression-based methods that directly predict grasping parameters given the object may achieve a high success rate but often lack diversity.","Generation-based methods that generate grasping postures conditioned on the object can often produce diverse grasping, but they are insufficient for high grasping success due to lack of discriminative information.","To mitigate, we introduce a unified diffusion-based dexterous grasp generation model, dubbed the name UGG, which operates within the object point cloud and hand parameter spaces.","Our all-transformer architecture unifies the information from the object, the hand, and the contacts, introducing a novel representation of contact points for improved contact modeling.","The flexibility and quality of our model enable the integration of a lightweight discriminator, benefiting from simulated discriminative data, which pushes for a high success rate while preserving high diversity.","Beyond grasp generation, our model can also generate objects based on hand information, offering valuable insights into object design and studying how the generative model perceives objects.","Our model achieves state-of-the-art dexterous grasping on the large-scale DexGraspNet dataset while facilitating human-centric object design, marking a significant advancement in dexterous grasping research.","Our project page is https://jiaxin-lu.github.io/ugg/ ."],"url":"http://arxiv.org/abs/2311.16917v1"}
{"created":"2023-11-28 16:19:30","title":"Stein Variational Belief Propagation for Multi-Robot Coordination","abstract":"Decentralized coordination for multi-robot systems involves planning in challenging, high-dimensional spaces. The planning problem is particularly challenging in the presence of obstacles and different sources of uncertainty such as inaccurate dynamic models and sensor noise. In this paper, we introduce Stein Variational Belief Propagation (SVBP), a novel algorithm for performing inference over nonparametric marginal distributions of nodes in a graph. We apply SVBP to multi-robot coordination by modelling a robot swarm as a graphical model and performing inference for each robot. We demonstrate our algorithm on a simulated multi-robot perception task, and on a multi-robot planning task within a Model-Predictive Control (MPC) framework, on both simulated and real-world mobile robots. Our experiments show that SVBP represents multi-modal distributions better than sampling-based or Gaussian baselines, resulting in improved performance on perception and planning tasks. Furthermore, we show that SVBP's ability to represent diverse trajectories for decentralized multi-robot planning makes it less prone to deadlock scenarios than leading baselines.","sentences":["Decentralized coordination for multi-robot systems involves planning in challenging, high-dimensional spaces.","The planning problem is particularly challenging in the presence of obstacles and different sources of uncertainty such as inaccurate dynamic models and sensor noise.","In this paper, we introduce Stein Variational Belief Propagation (SVBP), a novel algorithm for performing inference over nonparametric marginal distributions of nodes in a graph.","We apply SVBP to multi-robot coordination by modelling a robot swarm as a graphical model and performing inference for each robot.","We demonstrate our algorithm on a simulated multi-robot perception task, and on a multi-robot planning task within a Model-Predictive Control (MPC) framework, on both simulated and real-world mobile robots.","Our experiments show that SVBP represents multi-modal distributions better than sampling-based or Gaussian baselines, resulting in improved performance on perception and planning tasks.","Furthermore, we show that SVBP's ability to represent diverse trajectories for decentralized multi-robot planning makes it less prone to deadlock scenarios than leading baselines."],"url":"http://arxiv.org/abs/2311.16916v1"}
{"created":"2023-11-28 16:16:10","title":"Brain-ID: Learning Robust Feature Representations for Brain Imaging","abstract":"Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography, yet they struggle to generalize in uncalibrated modalities -- notoriously magnetic resonance imaging (MRI), where performance is highly sensitive to the differences in MR contrast, resolution, and orientation between the training and testing data. This prevents broad applicability to the diverse clinical acquisition protocols in the real world. We introduce Brain-ID, a robust feature representation learning strategy for brain imaging, which is contrast-agnostic, and robust to the brain anatomy of each subject regardless of the appearance of acquired images (i.e., deformation, contrast, resolution, orientation, artifacts, etc). Brain-ID is trained entirely on synthetic data, and easily adapts to downstream tasks with our proposed simple one-layer solution. We validate the robustness of Brain-ID features, and evaluate their performance in a variety of downstream applications, including both contrast-independent (anatomy reconstruction/contrast synthesis, brain segmentation), and contrast-dependent (super-resolution, bias field estimation) tasks. Extensive experiments on 6 public datasets demonstrate that Brain-ID achieves state-of-the-art performance in all tasks, and more importantly, preserves its performance when only limited training data is available.","sentences":["Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography, yet they struggle to generalize in uncalibrated modalities -- notoriously magnetic resonance imaging (MRI), where performance is highly sensitive to the differences in MR contrast, resolution, and orientation between the training and testing data.","This prevents broad applicability to the diverse clinical acquisition protocols in the real world.","We introduce Brain-ID, a robust feature representation learning strategy for brain imaging, which is contrast-agnostic, and robust to the brain anatomy of each subject regardless of the appearance of acquired images (i.e., deformation, contrast, resolution, orientation, artifacts, etc).","Brain-ID is trained entirely on synthetic data, and easily adapts to downstream tasks with our proposed simple one-layer solution.","We validate the robustness of Brain-ID features, and evaluate their performance in a variety of downstream applications, including both contrast-independent (anatomy reconstruction/contrast synthesis, brain segmentation), and contrast-dependent (super-resolution, bias field estimation) tasks.","Extensive experiments on 6 public datasets demonstrate that Brain-ID achieves state-of-the-art performance in all tasks, and more importantly, preserves its performance when only limited training data is available."],"url":"http://arxiv.org/abs/2311.16914v1"}
{"created":"2023-11-28 16:15:50","title":"Which Quantum Circuit Mutants Shall Be Used? An Empirical Evaluation of Quantum Circuit Mutations","abstract":"As a new research area, quantum software testing lacks systematic testing benchmarks to assess testing techniques' effectiveness. Recently, some open-source benchmarks and mutation analysis tools have emerged. However, there is insufficient evidence on how various quantum circuit characteristics (e.g., circuit depth, number of quantum gates), algorithms (e.g., Quantum Approximate Optimization Algorithm), and mutation characteristics (e.g., mutation operators) affect the most mutant detection in quantum circuits. Studying such relations is important to systematically design faulty benchmarks with varied attributes (e.g., the difficulty in detecting a seeded fault) to facilitate assessing the cost-effectiveness of quantum software testing techniques efficiently. To this end, we present a large-scale empirical evaluation with more than 700K faulty benchmarks (quantum circuits) generated by mutating 382 real-world quantum circuits. Based on the results, we provide valuable insights for researchers to define systematic quantum mutation analysis techniques. We also provide a tool to recommend mutants to users based on chosen characteristics (e.g., a quantum algorithm type) and the required difficulty of killing mutants. Finally, we also provide faulty benchmarks that can already be used to assess the cost-effectiveness of quantum software testing techniques.","sentences":["As a new research area, quantum software testing lacks systematic testing benchmarks to assess testing techniques' effectiveness.","Recently, some open-source benchmarks and mutation analysis tools have emerged.","However, there is insufficient evidence on how various quantum circuit characteristics (e.g., circuit depth, number of quantum gates), algorithms (e.g., Quantum Approximate Optimization Algorithm), and mutation characteristics (e.g., mutation operators) affect the most mutant detection in quantum circuits.","Studying such relations is important to systematically design faulty benchmarks with varied attributes (e.g., the difficulty in detecting a seeded fault) to facilitate assessing the cost-effectiveness of quantum software testing techniques efficiently.","To this end, we present a large-scale empirical evaluation with more than 700K faulty benchmarks (quantum circuits) generated by mutating 382 real-world quantum circuits.","Based on the results, we provide valuable insights for researchers to define systematic quantum mutation analysis techniques.","We also provide a tool to recommend mutants to users based on chosen characteristics (e.g., a quantum algorithm type) and the required difficulty of killing mutants.","Finally, we also provide faulty benchmarks that can already be used to assess the cost-effectiveness of quantum software testing techniques."],"url":"http://arxiv.org/abs/2311.16913v1"}
{"created":"2023-11-28 16:15:30","title":"Continuous optimization methods for the graph isomorphism problem","abstract":"The graph isomorphism problem looks deceptively simple, but although polynomial-time algorithms exist for certain types of graphs such as planar graphs and graphs with bounded degree or eigenvalue multiplicity, its complexity class is still unknown. Information about potential isomorphisms between two graphs is contained in the eigenvalues and eigenvectors of their adjacency matrices. However, symmetries of graphs often lead to repeated eigenvalues so that associated eigenvectors are determined only up to basis rotations, which complicates graph isomorphism testing. We consider orthogonal and doubly stochastic relaxations of the graph isomorphism problem, analyze the geometric properties of the resulting solution spaces, and show that their complexity increases significantly if repeated eigenvalues exist. By restricting the search space to suitable subspaces, we derive an efficient Frank-Wolfe based continuous optimization approach for detecting isomorphisms. We illustrate the efficacy of the algorithm with the aid of various highly symmetric graphs.","sentences":["The graph isomorphism problem looks deceptively simple, but although polynomial-time algorithms exist for certain types of graphs such as planar graphs and graphs with bounded degree or eigenvalue multiplicity, its complexity class is still unknown.","Information about potential isomorphisms between two graphs is contained in the eigenvalues and eigenvectors of their adjacency matrices.","However, symmetries of graphs often lead to repeated eigenvalues so that associated eigenvectors are determined only up to basis rotations, which complicates graph isomorphism testing.","We consider orthogonal and doubly stochastic relaxations of the graph isomorphism problem, analyze the geometric properties of the resulting solution spaces, and show that their complexity increases significantly if repeated eigenvalues exist.","By restricting the search space to suitable subspaces, we derive an efficient Frank-Wolfe based continuous optimization approach for detecting isomorphisms.","We illustrate the efficacy of the algorithm with the aid of various highly symmetric graphs."],"url":"http://arxiv.org/abs/2311.16912v1"}
{"created":"2023-11-28 16:08:42","title":"Analyzing the Influence of Language Model-Generated Responses in Mitigating Hate Speech on Social Media Directed at Ukrainian Refugees in Poland","abstract":"In the context of escalating hate speech and polarization on social media, this study investigates the potential of employing responses generated by Large Language Models (LLM), complemented with pertinent verified knowledge links, to counteract such trends. Through extensive A/B testing involving the posting of 753 automatically generated responses, the goal was to minimize the propagation of hate speech directed at Ukrainian refugees in Poland.   The results indicate that deploying LLM-generated responses as replies to harmful tweets effectively diminishes user engagement, as measured by likes/impressions. When we respond to an original tweet, i.e., which is not a reply, we reduce the engagement of users by over 20\\% without increasing the number of impressions. On the other hand, our responses increase the ratio of the number of replies to a harmful tweet to impressions, especially if the harmful tweet is not original. Additionally, the study examines how generated responses influence the overall sentiment of tweets in the discussion, revealing that our intervention does not significantly alter the mean sentiment.   This paper suggests the implementation of an automatic moderation system to combat hate speech on social media and provides an in-depth analysis of the A/B experiment, covering methodology, data collection, and statistical outcomes. Ethical considerations and challenges are also discussed, offering guidance for the development of discourse moderation systems leveraging the capabilities of generative AI.","sentences":["In the context of escalating hate speech and polarization on social media, this study investigates the potential of employing responses generated by Large Language Models (LLM), complemented with pertinent verified knowledge links, to counteract such trends.","Through extensive A/B testing involving the posting of 753 automatically generated responses, the goal was to minimize the propagation of hate speech directed at Ukrainian refugees in Poland.   ","The results indicate that deploying LLM-generated responses as replies to harmful tweets effectively diminishes user engagement, as measured by likes/impressions.","When we respond to an original tweet, i.e., which is not a reply, we reduce the engagement of users by over 20\\% without increasing the number of impressions.","On the other hand, our responses increase the ratio of the number of replies to a harmful tweet to impressions, especially if the harmful tweet is not original.","Additionally, the study examines how generated responses influence the overall sentiment of tweets in the discussion, revealing that our intervention does not significantly alter the mean sentiment.   ","This paper suggests the implementation of an automatic moderation system to combat hate speech on social media and provides an in-depth analysis of the A/B experiment, covering methodology, data collection, and statistical outcomes.","Ethical considerations and challenges are also discussed, offering guidance for the development of discourse moderation systems leveraging the capabilities of generative AI."],"url":"http://arxiv.org/abs/2311.16905v1"}
{"created":"2023-11-28 15:58:13","title":"Lane-Keeping Control of Autonomous Vehicles Through a Soft-Constrained Iterative LQR","abstract":"The accurate prediction of smooth steering inputs is crucial for autonomous vehicle applications because control actions with jitter might cause the vehicle system to become unstable. To address this problem in automobile lane-keeping control without the use of additional smoothing algorithms, we developed a soft-constrained iterative linear-quadratic regulator (soft-CILQR) algorithm by integrating CILQR algorithm and a model predictive control (MPC) constraint relaxation method. We incorporated slack variables into the state and control barrier functions of the soft-CILQR solver to soften the constraints in the optimization process so that stabilizing control inputs can be calculated in a relatively simple manner. Two types of automotive lane-keeping experiments were conducted with a linear system dynamics model to test the performance of the proposed soft-CILQR algorithm and to compare its performance with that of the CILQR algorithm: numerical simulations and experiments involving challenging vision-based maneuvers. In the numerical simulations, the soft-CILQR and CILQR solvers managed to drive the system toward the reference state asymptotically; however, the soft-CILQR solver obtained smooth steering input trajectories more easily than did the CILQR solver under conditions involving additive disturbances. In the experiments with visual inputs, the soft-CILQR controller outperformed the CILQR controller in terms of tracking accuracy and steering smoothness during the driving of an ego vehicle on TORCS.","sentences":["The accurate prediction of smooth steering inputs is crucial for autonomous vehicle applications because control actions with jitter might cause the vehicle system to become unstable.","To address this problem in automobile lane-keeping control without the use of additional smoothing algorithms, we developed a soft-constrained iterative linear-quadratic regulator (soft-CILQR)","algorithm by integrating CILQR algorithm and a model predictive control (MPC) constraint relaxation method.","We incorporated slack variables into the state and control barrier functions of the soft-CILQR solver to soften the constraints in the optimization process so that stabilizing control inputs can be calculated in a relatively simple manner.","Two types of automotive lane-keeping experiments were conducted with a linear system dynamics model to test the performance of the proposed soft-CILQR algorithm and to compare its performance with that of the CILQR algorithm: numerical simulations and experiments involving challenging vision-based maneuvers.","In the numerical simulations, the soft-CILQR and CILQR solvers managed to drive the system toward the reference state asymptotically; however, the soft-CILQR solver obtained smooth steering input trajectories more easily than did the CILQR solver under conditions involving additive disturbances.","In the experiments with visual inputs, the soft-CILQR controller outperformed the CILQR controller in terms of tracking accuracy and steering smoothness during the driving of an ego vehicle on TORCS."],"url":"http://arxiv.org/abs/2311.16900v1"}
{"created":"2023-11-28 15:46:12","title":"Dendrogram distance: an evaluation metric for generative networks using hierarchical clustering","abstract":"We present a novel metric for generative modeling evaluation, focusing primarily on generative networks. The method uses dendrograms to represent real and fake data, allowing for the divergence between training and generated samples to be computed. This metric focus on mode collapse, targeting generators that are not able to capture all modes in the training set. To evaluate the proposed method it is introduced a validation scheme based on sampling from real datasets, therefore the metric is evaluated in a controlled environment and proves to be competitive with other state-of-the-art approaches.","sentences":["We present a novel metric for generative modeling evaluation, focusing primarily on generative networks.","The method uses dendrograms to represent real and fake data, allowing for the divergence between training and generated samples to be computed.","This metric focus on mode collapse, targeting generators that are not able to capture all modes in the training set.","To evaluate the proposed method it is introduced a validation scheme based on sampling from real datasets, therefore the metric is evaluated in a controlled environment and proves to be competitive with other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2311.16894v1"}
{"created":"2023-11-28 15:43:46","title":"Enhancing Item-level Bundle Representation for Bundle Recommendation","abstract":"Bundle recommendation approaches offer users a set of related items on a particular topic. The current state-of-the-art (SOTA) method utilizes contrastive learning to learn representations at both the bundle and item levels. However, due to the inherent difference between the bundle-level and item-level preferences, the item-level representations may not receive sufficient information from the bundle affiliations to make accurate predictions. In this paper, we propose a novel approach EBRec, short of Enhanced Bundle Recommendation, which incorporates two enhanced modules to explore inherent item-level bundle representations. First, we propose to incorporate the bundle-user-item (B-U-I) high-order correlations to explore more collaborative information, thus to enhance the previous bundle representation that solely relies on the bundle-item affiliation information. Second, we further enhance the B-U-I correlations by augmenting the observed user-item interactions with interactions generated from pre-trained models, thus improving the item-level bundle representations. We conduct extensive experiments on three public datasets, and the results justify the effectiveness of our approach as well as the two core modules. Codes and datasets are available at https://github.com/answermycode/EBRec.","sentences":["Bundle recommendation approaches offer users a set of related items on a particular topic.","The current state-of-the-art (SOTA) method utilizes contrastive learning to learn representations at both the bundle and item levels.","However, due to the inherent difference between the bundle-level and item-level preferences, the item-level representations may not receive sufficient information from the bundle affiliations to make accurate predictions.","In this paper, we propose a novel approach EBRec, short of Enhanced Bundle Recommendation, which incorporates two enhanced modules to explore inherent item-level bundle representations.","First, we propose to incorporate the bundle-user-item (B-U-I) high-order correlations to explore more collaborative information, thus to enhance the previous bundle representation that solely relies on the bundle-item affiliation information.","Second, we further enhance the B-U-I correlations by augmenting the observed user-item interactions with interactions generated from pre-trained models, thus improving the item-level bundle representations.","We conduct extensive experiments on three public datasets, and the results justify the effectiveness of our approach as well as the two core modules.","Codes and datasets are available at https://github.com/answermycode/EBRec."],"url":"http://arxiv.org/abs/2311.16892v1"}
{"created":"2023-11-28 15:38:47","title":"Technological Challenges of Ambient Serious Games in Higher Education","abstract":"Naturally, university courses should be designed to attract students, engaging them to achieve learning goals. Toward this end, the use of Serious Games has been proposed in the literature. To address positive effects, such as content memorability and attendance rates, we propose Ambient Serious Games as games embedded in a computer-enriched environment, which is only partially perceived mentally by players. In this paper, we describe five technological key challenges that must be overcome to seamlessly and beneficially integrate an Ambient Serious Game into teaching. These challenges, derived from a scenario, focus on the technological provision and conduct of such games based on a software platform. They include (1) the integration of physical smart learning objects in heterogeneous environments under dynamic constraints, (2) the representation of abstract subject matter using smart learning objects, (3) the guided or automatic connection of all involved components, (4) the explanation of the components, their interaction, as well as the serious game itself, and (5) feedback on the game state.","sentences":["Naturally, university courses should be designed to attract students, engaging them to achieve learning goals.","Toward this end, the use of Serious Games has been proposed in the literature.","To address positive effects, such as content memorability and attendance rates, we propose Ambient Serious Games as games embedded in a computer-enriched environment, which is only partially perceived mentally by players.","In this paper, we describe five technological key challenges that must be overcome to seamlessly and beneficially integrate an Ambient Serious Game into teaching.","These challenges, derived from a scenario, focus on the technological provision and conduct of such games based on a software platform.","They include (1) the integration of physical smart learning objects in heterogeneous environments under dynamic constraints, (2) the representation of abstract subject matter using smart learning objects, (3) the guided or automatic connection of all involved components, (4) the explanation of the components, their interaction, as well as the serious game itself, and (5) feedback on the game state."],"url":"http://arxiv.org/abs/2311.16888v1"}
{"created":"2023-11-28 15:31:31","title":"Compressing the Backward Pass of Large-Scale Neural Architectures by Structured Activation Pruning","abstract":"The rise of Deep Neural Networks (DNNs) has led to an increase in model size and complexity, straining the memory capacity of GPUs. Sparsity in DNNs, characterized as structural or ephemeral, has gained attention as a solution. This work focuses on ephemeral sparsity, aiming to reduce memory consumption during training. It emphasizes the significance of activations, an often overlooked component, and their role in memory usage. This work employs structured pruning in Block Sparse Compressed Row (BSR) format in combination with a magnitude-based criterion to efficiently prune activations. We furthermore introduce efficient block-sparse operators for GPUs and showcase their effectiveness, as well as the superior compression offered by block sparsity. We report the effectiveness of activation pruning by evaluating training speed, accuracy, and memory usage of large-scale neural architectures on the example of ResMLP on image classification tasks. As a result, we observe a memory reduction of up to 32\\% while maintaining accuracy. Ultimately, our approach aims to democratize large-scale model training, reduce GPU requirements, and address ecological concerns.","sentences":["The rise of Deep Neural Networks (DNNs) has led to an increase in model size and complexity, straining the memory capacity of GPUs.","Sparsity in DNNs, characterized as structural or ephemeral, has gained attention as a solution.","This work focuses on ephemeral sparsity, aiming to reduce memory consumption during training.","It emphasizes the significance of activations, an often overlooked component, and their role in memory usage.","This work employs structured pruning in Block Sparse Compressed Row (BSR) format in combination with a magnitude-based criterion to efficiently prune activations.","We furthermore introduce efficient block-sparse operators for GPUs and showcase their effectiveness, as well as the superior compression offered by block sparsity.","We report the effectiveness of activation pruning by evaluating training speed, accuracy, and memory usage of large-scale neural architectures on the example of ResMLP on image classification tasks.","As a result, we observe a memory reduction of up to 32\\% while maintaining accuracy.","Ultimately, our approach aims to democratize large-scale model training, reduce GPU requirements, and address ecological concerns."],"url":"http://arxiv.org/abs/2311.16883v1"}
{"created":"2023-11-28 15:31:11","title":"Optimisation-Based Multi-Modal Semantic Image Editing","abstract":"Image editing affords increased control over the aesthetics and content of generated images. Pre-existing works focus predominantly on text-based instructions to achieve desired image modifications, which limit edit precision and accuracy. In this work, we propose an inference-time editing optimisation, designed to extend beyond textual edits to accommodate multiple editing instruction types (e.g. spatial layout-based; pose, scribbles, edge maps). We propose to disentangle the editing task into two competing subtasks: successful local image modifications and global content consistency preservation, where subtasks are guided through two dedicated loss functions. By allowing to adjust the influence of each loss function, we build a flexible editing solution that can be adjusted to user preferences. We evaluate our method using text, pose and scribble edit conditions, and highlight our ability to achieve complex edits, through both qualitative and quantitative experiments.","sentences":["Image editing affords increased control over the aesthetics and content of generated images.","Pre-existing works focus predominantly on text-based instructions to achieve desired image modifications, which limit edit precision and accuracy.","In this work, we propose an inference-time editing optimisation, designed to extend beyond textual edits to accommodate multiple editing instruction types (e.g. spatial layout-based; pose, scribbles, edge maps).","We propose to disentangle the editing task into two competing subtasks: successful local image modifications and global content consistency preservation, where subtasks are guided through two dedicated loss functions.","By allowing to adjust the influence of each loss function, we build a flexible editing solution that can be adjusted to user preferences.","We evaluate our method using text, pose and scribble edit conditions, and highlight our ability to achieve complex edits, through both qualitative and quantitative experiments."],"url":"http://arxiv.org/abs/2311.16882v1"}
{"created":"2023-11-28 15:28:12","title":"Temporal Importance Factor for Loss Functions for CTR Prediction","abstract":"Click-through rate (CTR) prediction is an important task for the companies to recommend products which better match user preferences. User behavior in digital advertising is dynamic and changes over time. It is crucial for the companies to capture the most recent trends to provide more accurate recommendations for users. In CTR prediction, most models use binary cross-entropy loss function. However, it does not focus on the data distribution shifts occurring over time. To address this problem, we propose a factor for the loss functions by utilizing the sequential nature of user-item interactions. This approach aims to focus on the most recent samples by penalizing them more through the loss function without forgetting the long-term information. Our solution is model-agnostic, and the temporal importance factor can be used with different loss functions. Offline experiments in both public and company datasets show that the temporal importance factor for loss functions outperforms the baseline loss functions considered.","sentences":["Click-through rate (CTR) prediction is an important task for the companies to recommend products which better match user preferences.","User behavior in digital advertising is dynamic and changes over time.","It is crucial for the companies to capture the most recent trends to provide more accurate recommendations for users.","In CTR prediction, most models use binary cross-entropy loss function.","However, it does not focus on the data distribution shifts occurring over time.","To address this problem, we propose a factor for the loss functions by utilizing the sequential nature of user-item interactions.","This approach aims to focus on the most recent samples by penalizing them more through the loss function without forgetting the long-term information.","Our solution is model-agnostic, and the temporal importance factor can be used with different loss functions.","Offline experiments in both public and company datasets show that the temporal importance factor for loss functions outperforms the baseline loss functions considered."],"url":"http://arxiv.org/abs/2311.16878v1"}
{"created":"2023-11-28 15:26:09","title":"Imputation using training labels and classification via label imputation","abstract":"Missing data is a common problem in practical settings. Various imputation methods have been developed to deal with missing data. However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. This allows imputing the label and the input at the same time. Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. Experiments show promising results in terms of accuracy.","sentences":["Missing data is a common problem in practical settings.","Various imputation methods have been developed to deal with missing data.","However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label.","In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input.","In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation.","This allows imputing the label and the input at the same time.","Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data.","Experiments show promising results in terms of accuracy."],"url":"http://arxiv.org/abs/2311.16877v1"}
{"created":"2023-11-28 15:25:14","title":"Digital Twin-Enhanced Deep Reinforcement Learning for Resource Management in Networks Slicing","abstract":"Network slicing-based communication systems can dynamically and efficiently allocate resources for diversified services. However, due to the limitation of the network interface on channel access and the complexity of the resource allocation, it is challenging to achieve an acceptable solution in the practical system without precise prior knowledge of the dynamics probability model of the service requests. Existing work attempts to solve this problem using deep reinforcement learning (DRL), however, such methods usually require a lot of interaction with the real environment in order to achieve good results. In this paper, a framework consisting of a digital twin and reinforcement learning agents is present to handle the issue. Specifically, we propose to use the historical data and the neural networks to build a digital twin model to simulate the state variation law of the real environment. Then, we use the data generated by the network slicing environment to calibrate the digital twin so that it is in sync with the real environment. Finally, DRL for slice optimization optimizes its own performance in this virtual pre-verification environment. We conducted an exhaustive verification of the proposed digital twin framework to confirm its scalability. Specifically, we propose to use loss landscapes to visualize the generalization of DRL solutions. We explore a distillation-based optimization scheme for lightweight slicing strategies. In addition, we also extend the framework to offline reinforcement learning, where solutions can be used to obtain intelligent decisions based solely on historical data. Numerical simulation experiments show that the proposed digital twin can significantly improve the performance of the slice optimization strategy.","sentences":["Network slicing-based communication systems can dynamically and efficiently allocate resources for diversified services.","However, due to the limitation of the network interface on channel access and the complexity of the resource allocation, it is challenging to achieve an acceptable solution in the practical system without precise prior knowledge of the dynamics probability model of the service requests.","Existing work attempts to solve this problem using deep reinforcement learning (DRL), however, such methods usually require a lot of interaction with the real environment in order to achieve good results.","In this paper, a framework consisting of a digital twin and reinforcement learning agents is present to handle the issue.","Specifically, we propose to use the historical data and the neural networks to build a digital twin model to simulate the state variation law of the real environment.","Then, we use the data generated by the network slicing environment to calibrate the digital twin so that it is in sync with the real environment.","Finally, DRL for slice optimization optimizes its own performance in this virtual pre-verification environment.","We conducted an exhaustive verification of the proposed digital twin framework to confirm its scalability.","Specifically, we propose to use loss landscapes to visualize the generalization of DRL solutions.","We explore a distillation-based optimization scheme for lightweight slicing strategies.","In addition, we also extend the framework to offline reinforcement learning, where solutions can be used to obtain intelligent decisions based solely on historical data.","Numerical simulation experiments show that the proposed digital twin can significantly improve the performance of the slice optimization strategy."],"url":"http://arxiv.org/abs/2311.16876v1"}
{"created":"2023-11-28 15:24:02","title":"A unified weighting framework for evaluating nearest neighbour classification","abstract":"We present the first comprehensive and large-scale evaluation of classical (NN), fuzzy (FNN) and fuzzy rough (FRNN) nearest neighbour classification. We show that existing proposals for nearest neighbour weighting can be standardised in the form of kernel functions, applied to the distance values and/or ranks of the nearest neighbours of a test instance. Furthermore, we identify three commonly used distance functions and four scaling measures. We systematically evaluate these choices on a collection of 85 real-life classification datasets. We find that NN, FNN and FRNN all perform best with Boscovich distance. NN and FRNN perform best with a combination of Samworth rank- and distance weights and scaling by the mean absolute deviation around the median ($r_1$), the standard deviaton ($r_2$) or the interquartile range ($r_{\\infty}^*$), while FNN performs best with only Samworth distance-weights and $r_1$- or $r_2$-scaling. We also introduce a new kernel based on fuzzy Yager negation, and show that NN achieves comparable performance with Yager distance-weights, which are simpler to implement than a combination of Samworth distance- and rank-weights. Finally, we demonstrate that FRNN generally outperforms NN, which in turns performs systematically better than FNN.","sentences":["We present the first comprehensive and large-scale evaluation of classical (NN), fuzzy (FNN) and fuzzy rough (FRNN) nearest neighbour classification.","We show that existing proposals for nearest neighbour weighting can be standardised in the form of kernel functions, applied to the distance values and/or ranks of the nearest neighbours of a test instance.","Furthermore, we identify three commonly used distance functions and four scaling measures.","We systematically evaluate these choices on a collection of 85 real-life classification datasets.","We find that NN, FNN and FRNN all perform best with Boscovich distance.","NN and FRNN perform best with a combination of Samworth rank- and distance weights and scaling by the mean absolute deviation around the median ($r_1$), the standard deviaton ($r_2$) or the interquartile range ($r_{\\infty}^*$), while FNN performs best with only Samworth distance-weights and $r_1$- or $r_2$-scaling.","We also introduce a new kernel based on fuzzy Yager negation, and show that NN achieves comparable performance with Yager distance-weights, which are simpler to implement than a combination of Samworth distance- and rank-weights.","Finally, we demonstrate that FRNN generally outperforms NN, which in turns performs systematically better than FNN."],"url":"http://arxiv.org/abs/2311.16872v1"}
{"created":"2023-11-28 15:12:47","title":"The Falcon Series of Open Language Models","abstract":"We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.","sentences":["We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data.","The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run.","Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1.","It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large.","We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon.","Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect.","We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models."],"url":"http://arxiv.org/abs/2311.16867v1"}
{"created":"2023-11-28 15:12:11","title":"A Benchmark for Evaluating Machine Translation Metrics on Dialects Without Standard Orthography","abstract":"For sensible progress in natural language processing, it is important that we are aware of the limitations of the evaluation metrics we use. In this work, we evaluate how robust metrics are to non-standardized dialects, i.e. spelling differences in language varieties that do not have a standard orthography. To investigate this, we collect a dataset of human translations and human judgments for automatic machine translations from English to two Swiss German dialects. We further create a challenge set for dialect variation and benchmark existing metrics' performances. Our results show that existing metrics cannot reliably evaluate Swiss German text generation outputs, especially on segment level. We propose initial design adaptations that increase robustness in the face of non-standardized dialects, although there remains much room for further improvement. The dataset, code, and models are available here: https://github.com/textshuttle/dialect_eval","sentences":["For sensible progress in natural language processing, it is important that we are aware of the limitations of the evaluation metrics we use.","In this work, we evaluate how robust metrics are to non-standardized dialects, i.e. spelling differences in language varieties that do not have a standard orthography.","To investigate this, we collect a dataset of human translations and human judgments for automatic machine translations from English to two Swiss German dialects.","We further create a challenge set for dialect variation and benchmark existing metrics' performances.","Our results show that existing metrics cannot reliably evaluate Swiss German text generation outputs, especially on segment level.","We propose initial design adaptations that increase robustness in the face of non-standardized dialects, although there remains much room for further improvement.","The dataset, code, and models are available here: https://github.com/textshuttle/dialect_eval"],"url":"http://arxiv.org/abs/2311.16865v1"}
{"created":"2023-11-28 15:09:36","title":"Power Hungry Processing: Watts Driving the Cost of AI Deployment?","abstract":"Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of \"generality\" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.","sentences":["Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology.","However, this ambition of \"generality\" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit.","In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks).","We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models.","We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters.","We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions.","All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis."],"url":"http://arxiv.org/abs/2311.16863v1"}
{"created":"2023-11-28 15:07:25","title":"Data-efficient operator learning for solving high Mach number fluid flow problems","abstract":"We consider the problem of using SciML to predict solutions of high Mach fluid flows over irregular geometries. In this setting, data is limited, and so it is desirable for models to perform well in the low-data setting. We show that Neural Basis Functions (NBF), which learns a basis of behavior modes from the data and then uses this basis to make predictions, is more effective than a basis-unaware baseline model. In addition, we identify continuing challenges in the space of predicting solutions for this type of problem.","sentences":["We consider the problem of using SciML to predict solutions of high Mach fluid flows over irregular geometries.","In this setting, data is limited, and so it is desirable for models to perform well in the low-data setting.","We show that Neural Basis Functions (NBF), which learns a basis of behavior modes from the data and then uses this basis to make predictions, is more effective than a basis-unaware baseline model.","In addition, we identify continuing challenges in the space of predicting solutions for this type of problem."],"url":"http://arxiv.org/abs/2311.16860v1"}
