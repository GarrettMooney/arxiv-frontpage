{"created":"2023-06-20","title":"The Cultivated Practices of Text-to-Image Generation","abstract":"Humankind is entering a novel creative era in which anybody can synthesize digital information using generative artificial intelligence (AI). Text-to-image generation, in particular, has become vastly popular and millions of practitioners produce AI-generated images and AI art online. This chapter first gives an overview of the key developments that enabled a healthy co-creative online ecosystem around text-to-image generation to rapidly emerge, followed by a high-level description of key elements in this ecosystem. A particular focus is placed on prompt engineering, a creative practice that has been embraced by the AI art community. It is then argued that the emerging co-creative ecosystem constitutes an intelligent system on its own - a system that both supports human creativity, but also potentially entraps future generations and limits future development efforts in AI. The chapter discusses the potential risks and dangers of cultivating this co-creative ecosystem, such as the bias inherent in today's training data, potential quality degradation in future image generation systems due to synthetic data becoming common place, and the potential long-term effects of text-to-image generation on people's imagination, ambitions, and development.","sentences":["Humankind is entering a novel creative era in which anybody can synthesize digital information using generative artificial intelligence (AI).","Text-to-image generation, in particular, has become vastly popular and millions of practitioners produce AI-generated images and AI art online.","This chapter first gives an overview of the key developments that enabled a healthy co-creative online ecosystem around text-to-image generation to rapidly emerge, followed by a high-level description of key elements in this ecosystem.","A particular focus is placed on prompt engineering, a creative practice that has been embraced by the AI art community.","It is then argued that the emerging co-creative ecosystem constitutes an intelligent system on its own - a system that both supports human creativity, but also potentially entraps future generations and limits future development efforts in AI.","The chapter discusses the potential risks and dangers of cultivating this co-creative ecosystem, such as the bias inherent in today's training data, potential quality degradation in future image generation systems due to synthetic data becoming common place, and the potential long-term effects of text-to-image generation on people's imagination, ambitions, and development."],"url":"http://arxiv.org/abs/2306.11393v1"}
{"created":"2023-06-20","title":"RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks","abstract":"Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs actions containing the movement and position transitions. We set four natural language understanding tasks with progressive reasoning levels and evaluate the robot's ability to understand natural language instructions in two modes of adsorption and grasping. In addition, we also conduct a comprehensive analysis and comparison of the differences and advantages of 10 different LLMs in instruction understanding and generation quality. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation. Project website: https://necolizer.github.io/RM-PRT/ .","sentences":["Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities.","This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions.","To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis.","Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation.","We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs actions containing the movement and position transitions.","We set four natural language understanding tasks with progressive reasoning levels and evaluate the robot's ability to understand natural language instructions in two modes of adsorption and grasping.","In addition, we also conduct a comprehensive analysis and comparison of the differences and advantages of 10 different LLMs in instruction understanding and generation quality.","We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.","Project website: https://necolizer.github.io/RM-PRT/ ."],"url":"http://arxiv.org/abs/2306.11335v1"}
{"created":"2023-06-20","title":"ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis","abstract":"We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature. This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging. Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself. All of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy. We deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles. This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%. Furthermore, with the dataset built by text mining, we constructed a machine-learning model with over 86% accuracy in predicting MOF experimental crystallization outcomes and preliminarily identifying important factors in MOF crystallization. We also developed a reliable data-grounded MOF chatbot to answer questions on chemical reactions and synthesis procedures. Given that the process of using ChatGPT reliably mines and tabulates diverse MOF synthesis information in a unified format, while using only narrative language requiring no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be very useful across various other chemistry sub-disciplines.","sentences":["We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature.","This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging.","Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself.","All of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy.","We deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles.","This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%.","Furthermore, with the dataset built by text mining, we constructed a machine-learning model with over 86% accuracy in predicting MOF experimental crystallization outcomes and preliminarily identifying important factors in MOF crystallization.","We also developed a reliable data-grounded MOF chatbot to answer questions on chemical reactions and synthesis procedures.","Given that the process of using ChatGPT reliably mines and tabulates diverse MOF synthesis information in a unified format, while using only narrative language requiring no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be very useful across various other chemistry sub-disciplines."],"url":"http://arxiv.org/abs/2306.11296v1"}
{"created":"2023-06-20","title":"FAIR: A Causal Framework for Accurately Inferring Judgments Reversals","abstract":"Artificial intelligence researchers have made significant advances in legal intelligence in recent years. However, the existing studies have not focused on the important value embedded in judgments reversals, which limits the improvement of the efficiency of legal intelligence. In this paper, we propose a causal Framework for Accurately Inferring case Reversals (FAIR), which models the problem of judgments reversals based on real Chinese judgments. We mine the causes of judgments reversals by causal inference methods and inject the obtained causal relationships into the neural network as a priori knowledge. And then, our framework is validated on a challenging dataset as a legal judgment prediction task. The experimental results show that our framework can tap the most critical factors in judgments reversal, and the obtained causal relationships can effectively improve the neural network's performance. In addition, we discuss the generalization ability of large language models for legal intelligence tasks using ChatGPT as an example. Our experiment has found that the generalization ability of large language models still has defects, and mining causal relationships can effectively improve the accuracy and explain ability of model predictions.","sentences":["Artificial intelligence researchers have made significant advances in legal intelligence in recent years.","However, the existing studies have not focused on the important value embedded in judgments reversals, which limits the improvement of the efficiency of legal intelligence.","In this paper, we propose a causal Framework for Accurately Inferring case Reversals (FAIR), which models the problem of judgments reversals based on real Chinese judgments.","We mine the causes of judgments reversals by causal inference methods and inject the obtained causal relationships into the neural network as a priori knowledge.","And then, our framework is validated on a challenging dataset as a legal judgment prediction task.","The experimental results show that our framework can tap the most critical factors in judgments reversal, and the obtained causal relationships can effectively improve the neural network's performance.","In addition, we discuss the generalization ability of large language models for legal intelligence tasks using ChatGPT as an example.","Our experiment has found that the generalization ability of large language models still has defects, and mining causal relationships can effectively improve the accuracy and explain ability of model predictions."],"url":"http://arxiv.org/abs/2306.11585v1"}
{"created":"2023-06-20","title":"TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models","abstract":"Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities. It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs. Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms. It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible.","sentences":["Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities.","It is crucial to prioritize human-centered principles when utilizing these models.","Safeguarding the ethical and moral compliance of LLMs is of utmost importance.","However, individual ethical issues have not been well studied on the latest LLMs.","Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT.","TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment.","Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms.","It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups.","Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks.","Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible."],"url":"http://arxiv.org/abs/2306.11507v1"}
{"created":"2023-06-20","title":"ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling","abstract":"Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities. Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases. However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs' factual reasoning ability, opening up new avenues for LLM research.","sentences":["Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities.","Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases.","However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents.","To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries.","This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications.","Inspired by existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs).","KGLLM provides a solution to enhance LLMs' factual reasoning ability, opening up new avenues for LLM research."],"url":"http://arxiv.org/abs/2306.11489v1"}
{"created":"2023-06-20","title":"RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks","abstract":"Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs actions containing the movement and position transitions. We set four natural language understanding tasks with progressive reasoning levels and evaluate the robot's ability to understand natural language instructions in two modes of adsorption and grasping. In addition, we also conduct a comprehensive analysis and comparison of the differences and advantages of 10 different LLMs in instruction understanding and generation quality. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation. Project website: https://necolizer.github.io/RM-PRT/ .","sentences":["Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities.","This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions.","To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis.","Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation.","We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs actions containing the movement and position transitions.","We set four natural language understanding tasks with progressive reasoning levels and evaluate the robot's ability to understand natural language instructions in two modes of adsorption and grasping.","In addition, we also conduct a comprehensive analysis and comparison of the differences and advantages of 10 different LLMs in instruction understanding and generation quality.","We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.","Project website: https://necolizer.github.io/RM-PRT/ ."],"url":"http://arxiv.org/abs/2306.11335v1"}
{"created":"2023-06-20","title":"ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis","abstract":"We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature. This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging. Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself. All of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy. We deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles. This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%. Furthermore, with the dataset built by text mining, we constructed a machine-learning model with over 86% accuracy in predicting MOF experimental crystallization outcomes and preliminarily identifying important factors in MOF crystallization. We also developed a reliable data-grounded MOF chatbot to answer questions on chemical reactions and synthesis procedures. Given that the process of using ChatGPT reliably mines and tabulates diverse MOF synthesis information in a unified format, while using only narrative language requiring no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be very useful across various other chemistry sub-disciplines.","sentences":["We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature.","This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging.","Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself.","All of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy.","We deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles.","This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%.","Furthermore, with the dataset built by text mining, we constructed a machine-learning model with over 86% accuracy in predicting MOF experimental crystallization outcomes and preliminarily identifying important factors in MOF crystallization.","We also developed a reliable data-grounded MOF chatbot to answer questions on chemical reactions and synthesis procedures.","Given that the process of using ChatGPT reliably mines and tabulates diverse MOF synthesis information in a unified format, while using only narrative language requiring no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be very useful across various other chemistry sub-disciplines."],"url":"http://arxiv.org/abs/2306.11296v1"}
{"created":"2023-06-20","title":"Learning Profitable NFT Image Diffusions via Multiple Visual-Policy Guided Reinforcement Learning","abstract":"We study the task of generating profitable Non-Fungible Token (NFT) images from user-input texts. Recent advances in diffusion models have shown great potential for image generation. However, existing works can fall short in generating visually-pleasing and highly-profitable NFT images, mainly due to the lack of 1) plentiful and fine-grained visual attribute prompts for an NFT image, and 2) effective optimization metrics for generating high-quality NFT images. To solve these challenges, we propose a Diffusion-based generation framework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) for NFT images. The proposed framework consists of a large language model (LLM), a diffusion-based image generator, and a series of visual rewards by design. First, the LLM enhances a basic human input (such as \"panda\") by generating more comprehensive NFT-style prompts that include specific visual attributes, such as \"panda with Ninja style and green background.\" Second, the diffusion-based image generator is fine-tuned using a large-scale NFT dataset to capture fine-grained image styles and accessory compositions of popular NFT elements. Third, we further propose to utilize multiple visual-policies as optimization goals, including visual rarity levels, visual aesthetic scores, and CLIP-based text-image relevances. This design ensures that our proposed Diffusion-MVP is capable of minting NFT images with high visual quality and market value. To facilitate this research, we have collected the largest publicly available NFT image dataset to date, consisting of 1.5 million high-quality images with corresponding texts and market values. Extensive experiments including objective evaluations and user studies demonstrate that our framework can generate NFT images showing more visually engaging elements and higher market value, compared with SOTA approaches.","sentences":["We study the task of generating profitable Non-Fungible Token (NFT) images from user-input texts.","Recent advances in diffusion models have shown great potential for image generation.","However, existing works can fall short in generating visually-pleasing and highly-profitable NFT images, mainly due to the lack of 1) plentiful and fine-grained visual attribute prompts for an NFT image, and 2) effective optimization metrics for generating high-quality NFT images.","To solve these challenges, we propose a Diffusion-based generation framework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) for NFT images.","The proposed framework consists of a large language model (LLM), a diffusion-based image generator, and a series of visual rewards by design.","First, the LLM enhances a basic human input (such as \"panda\") by generating more comprehensive NFT-style prompts that include specific visual attributes, such as \"panda with Ninja style and green background.\"","Second, the diffusion-based image generator is fine-tuned using a large-scale NFT dataset to capture fine-grained image styles and accessory compositions of popular NFT elements.","Third, we further propose to utilize multiple visual-policies as optimization goals, including visual rarity levels, visual aesthetic scores, and CLIP-based text-image relevances.","This design ensures that our proposed Diffusion-MVP is capable of minting NFT images with high visual quality and market value.","To facilitate this research, we have collected the largest publicly available NFT image dataset to date, consisting of 1.5 million high-quality images with corresponding texts and market values.","Extensive experiments including objective evaluations and user studies demonstrate that our framework can generate NFT images showing more visually engaging elements and higher market value, compared with SOTA approaches."],"url":"http://arxiv.org/abs/2306.11731v1"}
{"created":"2023-06-20","title":"Dense Video Object Captioning from Disjoint Supervision","abstract":"We propose a new task and model for dense video object captioning -- detecting, tracking, and captioning trajectories of all objects in a video. This task unifies spatial and temporal understanding of the video, and requires fine-grained language description. Our model for dense video object captioning is trained end-to-end and consists of different modules for spatial localization, tracking, and captioning. As such, we can train our model with a mixture of disjoint tasks, and leverage diverse, large-scale datasets which supervise different parts of our model. This results in noteworthy zero-shot performance. Moreover, by finetuning a model from this initialization, we can further improve our performance, surpassing strong image-based baselines by a significant margin. Although we are not aware of other work performing this task, we are able to repurpose existing video grounding datasets for our task, namely VidSTG and VLN. We show our task is more general than grounding, and models trained on our task can directly be applied to grounding by finding the bounding box with the maximum likelihood of generating the query sentence. Our model outperforms dedicated, state-of-the-art models for spatial grounding on both VidSTG and VLN.","sentences":["We propose a new task and model for dense video object captioning -- detecting, tracking, and captioning trajectories of all objects in a video.","This task unifies spatial and temporal understanding of the video, and requires fine-grained language description.","Our model for dense video object captioning is trained end-to-end and consists of different modules for spatial localization, tracking, and captioning.","As such, we can train our model with a mixture of disjoint tasks, and leverage diverse, large-scale datasets which supervise different parts of our model.","This results in noteworthy zero-shot performance.","Moreover, by finetuning a model from this initialization, we can further improve our performance, surpassing strong image-based baselines by a significant margin.","Although we are not aware of other work performing this task, we are able to repurpose existing video grounding datasets for our task, namely VidSTG and VLN.","We show our task is more general than grounding, and models trained on our task can directly be applied to grounding by finding the bounding box with the maximum likelihood of generating the query sentence.","Our model outperforms dedicated, state-of-the-art models for spatial grounding on both VidSTG and VLN."],"url":"http://arxiv.org/abs/2306.11729v1"}
{"created":"2023-06-20","title":"How can objects help action recognition?","abstract":"Current state-of-the-art video models process a video clip as a long sequence of spatio-temporal tokens. However, they do not explicitly model objects, their interactions across the video, and instead process all the tokens in the video. In this paper, we investigate how we can use knowledge of objects to design better video models, namely to process fewer tokens and to improve recognition accuracy. This is in contrast to prior works which either drop tokens at the cost of accuracy, or increase accuracy whilst also increasing the computation required. First, we propose an object-guided token sampling strategy that enables us to retain a small fraction of the input tokens with minimal impact on accuracy. And second, we propose an object-aware attention module that enriches our feature representation with object information and improves overall accuracy. Our resulting framework achieves better performance when using fewer tokens than strong baselines. In particular, we match our baseline with 30%, 40%, and 60% of the input tokens on SomethingElse, Something-something v2, and Epic-Kitchens, respectively. When we use our model to process the same number of tokens as our baseline, we improve by 0.6 to 4.2 points on these datasets.","sentences":["Current state-of-the-art video models process a video clip as a long sequence of spatio-temporal tokens.","However, they do not explicitly model objects, their interactions across the video, and instead process all the tokens in the video.","In this paper, we investigate how we can use knowledge of objects to design better video models, namely to process fewer tokens and to improve recognition accuracy.","This is in contrast to prior works which either drop tokens at the cost of accuracy, or increase accuracy whilst also increasing the computation required.","First, we propose an object-guided token sampling strategy that enables us to retain a small fraction of the input tokens with minimal impact on accuracy.","And second, we propose an object-aware attention module that enriches our feature representation with object information and improves overall accuracy.","Our resulting framework achieves better performance when using fewer tokens than strong baselines.","In particular, we match our baseline with 30%, 40%, and 60% of the input tokens on SomethingElse, Something-something v2, and Epic-Kitchens, respectively.","When we use our model to process the same number of tokens as our baseline, we improve by 0.6 to 4.2 points on these datasets."],"url":"http://arxiv.org/abs/2306.11726v1"}
{"created":"2023-06-20","title":"Meta-Analysis of Transfer Learning for Segmentation of Brain Lesions","abstract":"A major challenge in stroke research and stroke recovery predictions is the determination of a stroke lesion's extent and its impact on relevant brain systems. Manual segmentation of stroke lesions from 3D magnetic resonance (MR) imaging volumes, the current gold standard, is not only very time-consuming, but its accuracy highly depends on the operator's experience. As a result, there is a need for a fully automated segmentation method that can efficiently and objectively measure lesion extent and the impact of each lesion to predict impairment and recovery potential which might be beneficial for clinical, translational, and research settings. We have implemented and tested a fully automatic method for stroke lesion segmentation which was developed using eight different 2D-model architectures trained via transfer learning (TL) and mixed data approaches. Additionally, the final prediction was made using a novel ensemble method involving stacking and agreement window. Our novel method was evaluated in a novel in-house dataset containing 22 T1w brain MR images, which were challenging in various perspectives, but mostly because they included T1w MR images from the subacute (which typically less well defined T1 lesions) and chronic stroke phase (which typically means well defined T1-lesions). Cross-validation results indicate that our new method can efficiently and automatically segment lesions fast and with high accuracy compared to ground truth. In addition to segmentation, we provide lesion volume and weighted lesion load of relevant brain systems based on the lesions' overlap with a canonical structural motor system that stretches from the cortical motor region to the lowest end of the brain stem.","sentences":["A major challenge in stroke research and stroke recovery predictions is the determination of a stroke lesion's extent and its impact on relevant brain systems.","Manual segmentation of stroke lesions from 3D magnetic resonance (MR) imaging volumes, the current gold standard, is not only very time-consuming, but its accuracy highly depends on the operator's experience.","As a result, there is a need for a fully automated segmentation method that can efficiently and objectively measure lesion extent and the impact of each lesion to predict impairment and recovery potential which might be beneficial for clinical, translational, and research settings.","We have implemented and tested a fully automatic method for stroke lesion segmentation which was developed using eight different 2D-model architectures trained via transfer learning (TL) and mixed data approaches.","Additionally, the final prediction was made using a novel ensemble method involving stacking and agreement window.","Our novel method was evaluated in a novel in-house dataset containing 22 T1w brain MR images, which were challenging in various perspectives, but mostly because they included T1w MR images from the subacute (which typically less well defined T1 lesions) and chronic stroke phase (which typically means well defined T1-lesions).","Cross-validation results indicate that our new method can efficiently and automatically segment lesions fast and with high accuracy compared to ground truth.","In addition to segmentation, we provide lesion volume and weighted lesion load of relevant brain systems based on the lesions' overlap with a canonical structural motor system that stretches from the cortical motor region to the lowest end of the brain stem."],"url":"http://arxiv.org/abs/2306.11714v1"}
{"created":"2023-06-20","title":"Data-Driven but Privacy-Conscious: Pedestrian Dataset De-identification via Full-Body Person Synthesis","abstract":"The advent of data-driven technology solutions is accompanied by an increasing concern with data privacy. This is of particular importance for human-centered image recognition tasks, such as pedestrian detection, re-identification, and tracking. To highlight the importance of privacy issues and motivate future research, we motivate and introduce the Pedestrian Dataset De-Identification (PDI) task. PDI evaluates the degree of de-identification and downstream task training performance for a given de-identification method. As a first baseline, we propose IncogniMOT, a two-stage full-body de-identification pipeline based on image synthesis via generative adversarial networks. The first stage replaces target pedestrians with synthetic identities. To improve downstream task performance, we then apply stage two, which blends and adapts the synthetic image parts into the data. To demonstrate the effectiveness of IncogniMOT, we generate a fully de-identified version of the MOT17 pedestrian tracking dataset and analyze its application as training data for pedestrian re-identification, detection, and tracking models. Furthermore, we show how our data is able to narrow the synthetic-to-real performance gap in a privacy-conscious manner.","sentences":["The advent of data-driven technology solutions is accompanied by an increasing concern with data privacy.","This is of particular importance for human-centered image recognition tasks, such as pedestrian detection, re-identification, and tracking.","To highlight the importance of privacy issues and motivate future research, we motivate and introduce the Pedestrian Dataset De-Identification (PDI) task.","PDI evaluates the degree of de-identification and downstream task training performance for a given de-identification method.","As a first baseline, we propose IncogniMOT, a two-stage full-body de-identification pipeline based on image synthesis via generative adversarial networks.","The first stage replaces target pedestrians with synthetic identities.","To improve downstream task performance, we then apply stage two, which blends and adapts the synthetic image parts into the data.","To demonstrate the effectiveness of IncogniMOT, we generate a fully de-identified version of the MOT17 pedestrian tracking dataset and analyze its application as training data for pedestrian re-identification, detection, and tracking models.","Furthermore, we show how our data is able to narrow the synthetic-to-real performance gap in a privacy-conscious manner."],"url":"http://arxiv.org/abs/2306.11710v1"}
{"created":"2023-06-20","title":"Lingua Manga: A Generic Large Language Model Centric System for Data Curation","abstract":"Data curation is a wide-ranging area which contains many critical but time-consuming data processing tasks. However, the diversity of such tasks makes it challenging to develop a general-purpose data curation system. To address this issue, we present Lingua Manga, a user-friendly and versatile system that utilizes pre-trained large language models. Lingua Manga offers automatic optimization for achieving high performance and label efficiency while facilitating flexible and rapid development. Through three example applications with distinct objectives and users of varying levels of technical proficiency, we demonstrate that Lingua Manga can effectively assist both skilled programmers and low-code or even no-code users in addressing data curation challenges.","sentences":["Data curation is a wide-ranging area which contains many critical but time-consuming data processing tasks.","However, the diversity of such tasks makes it challenging to develop a general-purpose data curation system.","To address this issue, we present Lingua Manga, a user-friendly and versatile system that utilizes pre-trained large language models.","Lingua Manga offers automatic optimization for achieving high performance and label efficiency while facilitating flexible and rapid development.","Through three example applications with distinct objectives and users of varying levels of technical proficiency, we demonstrate that Lingua Manga can effectively assist both skilled programmers and low-code or even no-code users in addressing data curation challenges."],"url":"http://arxiv.org/abs/2306.11702v1"}
{"created":"2023-06-20","title":"GenPlot: Increasing the Scale and Diversity of Chart Derendering Data","abstract":"Vertical bars, horizontal bars, dot, scatter, and line plots provide a diverse set of visualizations to represent data. To understand these plots, one must be able to recognize textual components, locate data points in a plot, and process diverse visual contexts to extract information. In recent works such as Pix2Struct, Matcha, and Deplot, OCR-free chart-to-text translation has achieved state-of-the-art results on visual language tasks. These results outline the importance of chart-derendering as a pre-training objective, yet existing datasets provide a fixed set of training examples. In this paper, we propose GenPlot; a plot generator that can generate billions of additional plots for chart-derendering using synthetic data.","sentences":["Vertical bars, horizontal bars, dot, scatter, and line plots provide a diverse set of visualizations to represent data.","To understand these plots, one must be able to recognize textual components, locate data points in a plot, and process diverse visual contexts to extract information.","In recent works such as Pix2Struct, Matcha, and Deplot, OCR-free chart-to-text translation has achieved state-of-the-art results on visual language tasks.","These results outline the importance of chart-derendering as a pre-training objective, yet existing datasets provide a fixed set of training examples.","In this paper, we propose GenPlot; a plot generator that can generate billions of additional plots for chart-derendering using synthetic data."],"url":"http://arxiv.org/abs/2306.11699v1"}
{"created":"2023-06-20","title":"Individual Treatment Effects in Extreme Regimes","abstract":"Understanding individual treatment effects in extreme regimes is important for characterizing risks associated with different interventions. This is hindered by the fact that extreme regime data may be hard to collect, as it is scarcely observed in practice. In addressing this issue, we propose a new framework for estimating the individual treatment effect in extreme regimes (ITE$_2$). Specifically, we quantify this effect by the changes in the tail decay rates of potential outcomes in the presence or absence of the treatment. Subsequently, we establish conditions under which ITE$_2$ may be calculated and develop algorithms for its computation. We demonstrate the efficacy of our proposed method on various synthetic and semi-synthetic datasets.","sentences":["Understanding individual treatment effects in extreme regimes is important for characterizing risks associated with different interventions.","This is hindered by the fact that extreme regime data may be hard to collect, as it is scarcely observed in practice.","In addressing this issue, we propose a new framework for estimating the individual treatment effect in extreme regimes (ITE$_2$).","Specifically, we quantify this effect by the changes in the tail decay rates of potential outcomes in the presence or absence of the treatment.","Subsequently, we establish conditions under which ITE$_2$ may be calculated and develop algorithms for its computation.","We demonstrate the efficacy of our proposed method on various synthetic and semi-synthetic datasets."],"url":"http://arxiv.org/abs/2306.11697v1"}
{"created":"2023-06-20","title":"RoTaR: Efficient Row-Based Table Representation Learning via Teacher-Student Training","abstract":"We propose RoTaR, a row-based table representation learning method, to address the efficiency and scalability issues faced by existing table representation learning methods. The key idea of RoTaR is to generate query-agnostic row representations that could be re-used via query-specific aggregation. In addition to the row-based architecture, we introduce several techniques: cell-aware position embedding, teacher-student training paradigm, and selective backward to improve the performance of RoTaR model.","sentences":["We propose RoTaR, a row-based table representation learning method, to address the efficiency and scalability issues faced by existing table representation learning methods.","The key idea of RoTaR is to generate query-agnostic row representations that could be re-used via query-specific aggregation.","In addition to the row-based architecture, we introduce several techniques: cell-aware position embedding, teacher-student training paradigm, and selective backward to improve the performance of RoTaR model."],"url":"http://arxiv.org/abs/2306.11696v1"}
{"created":"2023-06-20","title":"Statistical Tests for Replacing Human Decision Makers with Algorithms","abstract":"This paper proposes a statistical framework with which artificial intelligence can improve human decision making. The performance of each human decision maker is first benchmarked against machine predictions; we then replace the decisions made by a subset of the decision makers with the recommendation from the proposed artificial intelligence algorithm. Using a large nationwide dataset of pregnancy outcomes and doctor diagnoses from prepregnancy checkups of reproductive age couples, we experimented with both a heuristic frequentist approach and a Bayesian posterior loss function approach with an application to abnormal birth detection. We find that our algorithm on a test dataset results in a higher overall true positive rate and a lower false positive rate than the diagnoses made by doctors only. We also find that the diagnoses of doctors from rural areas are more frequently replaceable, suggesting that artificial intelligence assisted decision making tends to improve precision more in less developed regions.","sentences":["This paper proposes a statistical framework with which artificial intelligence can improve human decision making.","The performance of each human decision maker is first benchmarked against machine predictions; we then replace the decisions made by a subset of the decision makers with the recommendation from the proposed artificial intelligence algorithm.","Using a large nationwide dataset of pregnancy outcomes and doctor diagnoses from prepregnancy checkups of reproductive age couples, we experimented with both a heuristic frequentist approach and a Bayesian posterior loss function approach with an application to abnormal birth detection.","We find that our algorithm on a test dataset results in a higher overall true positive rate and a lower false positive rate than the diagnoses made by doctors only.","We also find that the diagnoses of doctors from rural areas are more frequently replaceable, suggesting that artificial intelligence assisted decision making tends to improve precision more in less developed regions."],"url":"http://arxiv.org/abs/2306.11689v1"}
{"created":"2023-06-20","title":"A primal-dual data-driven method for computational optical imaging with a photonic lantern","abstract":"Optical fibres aim to image in-vivo biological processes. In this context, high spatial resolution and stability to fibre movements are key to enable decision-making processes (e.g., for microendoscopy). Recently, a single-pixel imaging technique based on a multicore fibre photonic lantern has been designed, named computational optical imaging using a lantern (COIL). A proximal algorithm based on a sparsity prior, dubbed SARA-COIL, has been further proposed to enable image reconstructions for high resolution COIL microendoscopy. In this work, we develop a data-driven approach for COIL. We replace the sparsity prior in the proximal algorithm by a learned denoiser, leading to a plug-and-play (PnP) algorithm. We use recent results in learning theory to train a network with desirable Lipschitz properties. We show that the resulting primal-dual PnP algorithm converges to a solution to a monotone inclusion problem. Our simulations highlight that the proposed data-driven approach improves the reconstruction quality over variational SARA-COIL method on both simulated and real data.","sentences":["Optical fibres aim to image in-vivo biological processes.","In this context, high spatial resolution and stability to fibre movements are key to enable decision-making processes (e.g., for microendoscopy).","Recently, a single-pixel imaging technique based on a multicore fibre photonic lantern has been designed, named computational optical imaging using a lantern (COIL).","A proximal algorithm based on a sparsity prior, dubbed SARA-COIL, has been further proposed to enable image reconstructions for high resolution COIL microendoscopy.","In this work, we develop a data-driven approach for COIL.","We replace the sparsity prior in the proximal algorithm by a learned denoiser, leading to a plug-and-play (PnP) algorithm.","We use recent results in learning theory to train a network with desirable Lipschitz properties.","We show that the resulting primal-dual PnP algorithm converges to a solution to a monotone inclusion problem.","Our simulations highlight that the proposed data-driven approach improves the reconstruction quality over variational SARA-COIL method on both simulated and real data."],"url":"http://arxiv.org/abs/2306.11679v1"}
{"created":"2023-06-20","title":"GIO: Gradient Information Optimization for Training Dataset Selection","abstract":"It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to new datasets and domains.","sentences":["It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance.","We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution.","GIO begins from a natural, information-theoretic objective that is intractable in practice.","Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation.","In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets.","These findings are robust to different representation models and hyperparameters for GIO itself.","GIO is task- and domain-agnostic and can be applied out-of-the-box to new datasets and domains."],"url":"http://arxiv.org/abs/2306.11670v1"}
{"created":"2023-06-20","title":"FedNoisy: Federated Noisy Label Learning Benchmark","abstract":"Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federated noisy label learning. We hope this benchmark can facilitate idea verification in federated learning with noisy labels. \\texttt{FedNoisy} is available at \\codeword{https://github.com/SMILELab-FL/FedNoisy}.","sentences":["Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients.","But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels.","Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings.","However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings.","In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings.","Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future.","We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federated noisy label learning.","We hope this benchmark can facilitate idea verification in federated learning with noisy labels.","\\texttt{FedNoisy} is available at \\codeword{https://github.com/SMILELab-FL/FedNoisy}."],"url":"http://arxiv.org/abs/2306.11650v1"}
{"created":"2023-06-20","title":"Textbooks Are All You Need","abstract":"We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.","sentences":["We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens).","Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP.","It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval."],"url":"http://arxiv.org/abs/2306.11644v1"}
{"created":"2023-06-20","title":"A low-cost integrated hyperspectral imaging sensor with full temporal and spatial resolution at VIS-NIR wide range","abstract":"Hyperspectral imaging provides optical information with high-dimensional spatial-temporal-spectral data cubes revealing intrinsic matter characteristics. It has been widely applied in various intelligent inspection applications. The existing hyperspectral imaging systems employ individual optical prism or grating to separate different spectra, which require complicated optical design that takes a heavy load for integration. The emerging multispectral imaging sensor integrates narrow-band filters on each pixel element, wasting most light throughput and trapped in the rigorous tradeoff between spatial and spectral resolution. In this work, we report an on-chip computational hyperspectral imaging framework with full spatial and temporal resolution. By integrating different broadband filtering materials on the imaging sensor, the target spectral information is non-uniformly and intrinsically coupled on each pixel with high light throughput. Using intelligent reconstruction algorithms, multi-channel images can be recovered from each frame, realizing real-time hyperspectral imaging. Following such a framework, we for the first time fabricated a broadband (400-1700 nm) hyperspectral imaging sensor with an average light throughput of 71.8$\\%$ (enabling 32.5 dB peak signal-to-noise ratio of spectral reconstruction on ColorChecker Classic chart) and 89 wavelength channels (10 nm interval within 400-1000 nm and 25 nm interval within 1000-1700 nm). The average spectral resolution achieves 2.65 nm at 400-1000 nm and 8.59 nm at 1000-1700 nm. The demonstrated spatial resolution is 2048x2048 pixels at 47 fps, with $\\sim$3.43 arc minute resolving ability at a 39-degree field of view. We employed the prototype sensor to collect a large-scale hyperspectral image dataset (210 scenes over 8 categories), and demonstrated the sensor's wide application potentials on a series of experiments, including chlorophyll and sugar quantification for agriculture growth, blood oxygen and water quality monitoring for human health, and face recognition for social security. The integrated hyperspectral imaging sensor is only 33.9 grams in weight without an imaging lens, and can be assembled on various resource-limited platforms such as unmanned aerial vehicles and nanosatellites, or equipped with off-the-shelf optical systems such as a microscope for direct real-time hyperspectral imaging. The technique integrates innovations from multiple fields of material, integrated circuits, computer science, and optics. It transforms the general challenge of high-dimensional imaging from one that is coupled to the physical limitations of high-cost optics manufacture and complex system design to one that is solvable through agile computation.","sentences":["Hyperspectral imaging provides optical information with high-dimensional spatial-temporal-spectral data cubes revealing intrinsic matter characteristics.","It has been widely applied in various intelligent inspection applications.","The existing hyperspectral imaging systems employ individual optical prism or grating to separate different spectra, which require complicated optical design that takes a heavy load for integration.","The emerging multispectral imaging sensor integrates narrow-band filters on each pixel element, wasting most light throughput and trapped in the rigorous tradeoff between spatial and spectral resolution.","In this work, we report an on-chip computational hyperspectral imaging framework with full spatial and temporal resolution.","By integrating different broadband filtering materials on the imaging sensor, the target spectral information is non-uniformly and intrinsically coupled on each pixel with high light throughput.","Using intelligent reconstruction algorithms, multi-channel images can be recovered from each frame, realizing real-time hyperspectral imaging.","Following such a framework, we for the first time fabricated a broadband (400-1700 nm) hyperspectral imaging sensor with an average light throughput of 71.8$\\%$ (enabling 32.5 dB peak signal-to-noise ratio of spectral reconstruction on ColorChecker Classic chart) and 89 wavelength channels (10 nm interval within 400-1000 nm and 25 nm interval within 1000-1700 nm).","The average spectral resolution achieves 2.65 nm at 400-1000 nm and 8.59 nm at 1000-1700 nm.","The demonstrated spatial resolution is 2048x2048 pixels at 47 fps, with $\\sim$3.43 arc minute resolving ability at a 39-degree field of view.","We employed the prototype sensor to collect a large-scale hyperspectral image dataset (210 scenes over 8 categories), and demonstrated the sensor's wide application potentials on a series of experiments, including chlorophyll and sugar quantification for agriculture growth, blood oxygen and water quality monitoring for human health, and face recognition for social security.","The integrated hyperspectral imaging sensor is only 33.9 grams in weight without an imaging lens, and can be assembled on various resource-limited platforms such as unmanned aerial vehicles and nanosatellites, or equipped with off-the-shelf optical systems such as a microscope for direct real-time hyperspectral imaging.","The technique integrates innovations from multiple fields of material, integrated circuits, computer science, and optics.","It transforms the general challenge of high-dimensional imaging from one that is coupled to the physical limitations of high-cost optics manufacture and complex system design to one that is solvable through agile computation."],"url":"http://arxiv.org/abs/2306.11583v1"}
{"created":"2023-06-20","title":"CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality","abstract":"There are three problems existing in the popular data-to-text datasets. First, the large-scale datasets either contain noise or lack real application scenarios. Second, the datasets close to real applications are relatively small in size. Last, current datasets bias in the English language while leaving other languages underexplored. To alleviate these limitations, in this paper, we present CATS, a pragmatic Chinese answer-to-sequence dataset with large scale and high quality. The dataset aims to generate textual descriptions for the answer in the practical TableQA system. Further, to bridge the structural gap between the input SQL and table and establish better semantic alignments, we propose a Unified Graph Transformation approach to establish a joint encoding space for the two hybrid knowledge resources and convert this task to a graph-to-text problem. The experiment results demonstrate the effectiveness of our proposed method. Further analysis on CATS attests to both the high quality and challenges of the dataset.","sentences":["There are three problems existing in the popular data-to-text datasets.","First, the large-scale datasets either contain noise or lack real application scenarios.","Second, the datasets close to real applications are relatively small in size.","Last, current datasets bias in the English language while leaving other languages underexplored.","To alleviate these limitations, in this paper, we present CATS, a pragmatic Chinese answer-to-sequence dataset with large scale and high quality.","The dataset aims to generate textual descriptions for the answer in the practical TableQA system.","Further, to bridge the structural gap between the input SQL and table and establish better semantic alignments, we propose a Unified Graph Transformation approach to establish a joint encoding space for the two hybrid knowledge resources and convert this task to a graph-to-text problem.","The experiment results demonstrate the effectiveness of our proposed method.","Further analysis on CATS attests to both the high quality and challenges of the dataset."],"url":"http://arxiv.org/abs/2306.11477v1"}
{"created":"2023-06-20","title":"The Cultivated Practices of Text-to-Image Generation","abstract":"Humankind is entering a novel creative era in which anybody can synthesize digital information using generative artificial intelligence (AI). Text-to-image generation, in particular, has become vastly popular and millions of practitioners produce AI-generated images and AI art online. This chapter first gives an overview of the key developments that enabled a healthy co-creative online ecosystem around text-to-image generation to rapidly emerge, followed by a high-level description of key elements in this ecosystem. A particular focus is placed on prompt engineering, a creative practice that has been embraced by the AI art community. It is then argued that the emerging co-creative ecosystem constitutes an intelligent system on its own - a system that both supports human creativity, but also potentially entraps future generations and limits future development efforts in AI. The chapter discusses the potential risks and dangers of cultivating this co-creative ecosystem, such as the bias inherent in today's training data, potential quality degradation in future image generation systems due to synthetic data becoming common place, and the potential long-term effects of text-to-image generation on people's imagination, ambitions, and development.","sentences":["Humankind is entering a novel creative era in which anybody can synthesize digital information using generative artificial intelligence (AI).","Text-to-image generation, in particular, has become vastly popular and millions of practitioners produce AI-generated images and AI art online.","This chapter first gives an overview of the key developments that enabled a healthy co-creative online ecosystem around text-to-image generation to rapidly emerge, followed by a high-level description of key elements in this ecosystem.","A particular focus is placed on prompt engineering, a creative practice that has been embraced by the AI art community.","It is then argued that the emerging co-creative ecosystem constitutes an intelligent system on its own - a system that both supports human creativity, but also potentially entraps future generations and limits future development efforts in AI.","The chapter discusses the potential risks and dangers of cultivating this co-creative ecosystem, such as the bias inherent in today's training data, potential quality degradation in future image generation systems due to synthetic data becoming common place, and the potential long-term effects of text-to-image generation on people's imagination, ambitions, and development."],"url":"http://arxiv.org/abs/2306.11393v1"}
{"created":"2023-06-20","title":"Fingerprinting and Building Large Reproducible Datasets","abstract":"Obtaining a relevant dataset is central to conducting empirical studies in software engineering. However, in the context of mining software repositories, the lack of appropriate tooling for large scale mining tasks hinders the creation of new datasets. Moreover, limitations related to data sources that change over time (e.g., code bases) and the lack of documentation of extraction processes make it difficult to reproduce datasets over time. This threatens the quality and reproducibility of empirical studies.   In this paper, we propose a tool-supported approach facilitating the creation of large tailored datasets while ensuring their reproducibility. We leveraged all the sources feeding the Software Heritage append-only archive which are accessible through a unified programming interface to outline a reproducible and generic extraction process. We propose a way to define a unique fingerprint to characterize a dataset which, when provided to the extraction process, ensures that the same dataset will be extracted.   We demonstrate the feasibility of our approach by implementing a prototype. We show how it can help reduce the limitations researchers face when creating or reproducing datasets.","sentences":["Obtaining a relevant dataset is central to conducting empirical studies in software engineering.","However, in the context of mining software repositories, the lack of appropriate tooling for large scale mining tasks hinders the creation of new datasets.","Moreover, limitations related to data sources that change over time (e.g., code bases) and the lack of documentation of extraction processes make it difficult to reproduce datasets over time.","This threatens the quality and reproducibility of empirical studies.   ","In this paper, we propose a tool-supported approach facilitating the creation of large tailored datasets while ensuring their reproducibility.","We leveraged all the sources feeding the Software Heritage append-only archive which are accessible through a unified programming interface to outline a reproducible and generic extraction process.","We propose a way to define a unique fingerprint to characterize a dataset which, when provided to the extraction process, ensures that the same dataset will be extracted.   ","We demonstrate the feasibility of our approach by implementing a prototype.","We show how it can help reduce the limitations researchers face when creating or reproducing datasets."],"url":"http://arxiv.org/abs/2306.11391v1"}
{"created":"2023-06-20","title":"Masked Diffusion Models are Fast Learners","abstract":"Diffusion models have emerged as the de-facto technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90%) of the input image and employing masked score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing this masked learning process in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ 256x256 in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to DDPM. Moreover, our masked pre-training technique is universally applicable to various diffusion models that directly generate images in the pixel space and facilitates learning pre-trained models with excellent generalizability: a diffusion model pre-trained on VGGFace2 attains a 46% quality improvement through fine-tuning with merely 10% local data. Our code is available at https://github.com/jiachenlei/maskdm.","sentences":["Diffusion models have emerged as the de-facto technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community.","We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks.","Our approach centers on masking a high proportion (e.g., up to 90%) of the input image and employing masked score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge.","By utilizing this masked learning process in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ 256x256 in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to DDPM.","Moreover, our masked pre-training technique is universally applicable to various diffusion models that directly generate images in the pixel space and facilitates learning pre-trained models with excellent generalizability: a diffusion model pre-trained on VGGFace2 attains a 46% quality improvement through fine-tuning with merely 10% local data.","Our code is available at https://github.com/jiachenlei/maskdm."],"url":"http://arxiv.org/abs/2306.11363v1"}
{"created":"2023-06-20","title":"GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization","abstract":"Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to 'hallucinations', low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023's 'Reality Check' theme, we present GUMSum, a small but carefully crafted dataset of English summaries in 12 written and spoken genres for evaluation of abstractive summarization. Summaries are highly constrained, focusing on substitutive potential, factuality, and faithfulness. We present guidelines and evaluate human agreement as well as subjective judgments on recent system outputs, comparing general-domain untuned approaches, a fine-tuned one, and a prompt-based approach, to human performance. Results show that while GPT3 achieves impressive scores, it still underperforms humans, with varying quality across genres. Human judgments reveal different types of errors in supervised, prompted, and human-generated summaries, shedding light on the challenges of producing a good summary.","sentences":["Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to 'hallucinations', low performance on non-news genres, and outputs which are not exactly summaries.","Targeting ACL 2023's 'Reality Check' theme, we present GUMSum, a small but carefully crafted dataset of English summaries in 12 written and spoken genres for evaluation of abstractive summarization.","Summaries are highly constrained, focusing on substitutive potential, factuality, and faithfulness.","We present guidelines and evaluate human agreement as well as subjective judgments on recent system outputs, comparing general-domain untuned approaches, a fine-tuned one, and a prompt-based approach, to human performance.","Results show that while GPT3 achieves impressive scores, it still underperforms humans, with varying quality across genres.","Human judgments reveal different types of errors in supervised, prompted, and human-generated summaries, shedding light on the challenges of producing a good summary."],"url":"http://arxiv.org/abs/2306.11256v1"}
