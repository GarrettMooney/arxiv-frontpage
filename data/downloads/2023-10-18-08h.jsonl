{"created":"2023-10-17 17:59:46","title":"VeRA: Vector-based Random Matrix Adaptation","abstract":"Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which reduces the number of trainable parameters by 10x compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, and show its application in instruction-following with just 1.4M parameters using the Llama2 7B model.","sentences":["Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models.","In this work, we present Vector-based Random Matrix Adaptation (VeRA), which reduces the number of trainable parameters by 10x compared to LoRA, yet maintains the same performance.","It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead.","We demonstrate its effectiveness on the GLUE and E2E benchmarks, and show its application in instruction-following with just 1.4M parameters using the Llama2 7B model."],"url":"http://arxiv.org/abs/2310.11454v1"}
{"created":"2023-10-17 17:59:15","title":"BitNet: Scaling 1-bit Transformers for Large Language Models","abstract":"The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.","sentences":["The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption.","In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models.","Specifically, we introduce BitLinear as a drop-in replacement of the nn.","Linear layer in order to train 1-bit weights from scratch.","Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines.","Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits."],"url":"http://arxiv.org/abs/2310.11453v1"}
{"created":"2023-10-17 17:58:34","title":"Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective","abstract":"Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge (encompassing detection, editing, and merging), there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. We release code and data at \\url{https://github.com/maszhongming/ParaKnowTransfer}.","sentences":["Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora.","While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge (encompassing detection, editing, and merging), there remains an ambiguous understanding regarding their transferability across models with varying scales.","In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective.","To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs.","Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models.","Evaluations across four benchmarks validate the efficacy of our proposed method.","Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales.","We release code and data at \\url{https://github.com/maszhongming/ParaKnowTransfer}."],"url":"http://arxiv.org/abs/2310.11451v1"}
{"created":"2023-10-17 17:58:19","title":"Explaining Deep Neural Networks for Bearing Fault Detection with Vibration Concepts","abstract":"Concept-based explanation methods, such as Concept Activation Vectors, are potent means to quantify how abstract or high-level characteristics of input data influence the predictions of complex deep neural networks. However, applying them to industrial prediction problems is challenging as it is not immediately clear how to define and access appropriate concepts for individual use cases and specific data types. In this work, we investigate how to leverage established concept-based explanation techniques in the context of bearing fault detection with deep neural networks trained on vibration signals. Since bearings are prevalent in almost every rotating equipment, ensuring the reliability of intransparent fault detection models is crucial to prevent costly repairs and downtimes of industrial machinery. Our evaluations demonstrate that explaining opaque models in terms of vibration concepts enables human-comprehensible and intuitive insights about their inner workings, but the underlying assumptions need to be carefully validated first.","sentences":["Concept-based explanation methods, such as Concept Activation Vectors, are potent means to quantify how abstract or high-level characteristics of input data influence the predictions of complex deep neural networks.","However, applying them to industrial prediction problems is challenging as it is not immediately clear how to define and access appropriate concepts for individual use cases and specific data types.","In this work, we investigate how to leverage established concept-based explanation techniques in the context of bearing fault detection with deep neural networks trained on vibration signals.","Since bearings are prevalent in almost every rotating equipment, ensuring the reliability of intransparent fault detection models is crucial to prevent costly repairs and downtimes of industrial machinery.","Our evaluations demonstrate that explaining opaque models in terms of vibration concepts enables human-comprehensible and intuitive insights about their inner workings, but the underlying assumptions need to be carefully validated first."],"url":"http://arxiv.org/abs/2310.11450v1"}
{"created":"2023-10-17 17:58:00","title":"DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis","abstract":"Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics. Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved. To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model. At the core, we represent the light field around the human with a deformable two-surface parameterization, which enables fast and accurate inference of the human appearance. This allows perceptual supervision on the full image compared to previous approaches that could only supervise individual pixels or small patches due to their slow runtime. Our carefully designed human representation and supervision strategy leads to state-of-the-art synthesis results and inference time. The video results and code are available at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS.","sentences":["Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics.","Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved.","To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model.","At the core, we represent the light field around the human with a deformable two-surface parameterization, which enables fast and accurate inference of the human appearance.","This allows perceptual supervision on the full image compared to previous approaches that could only supervise individual pixels or small patches due to their slow runtime.","Our carefully designed human representation and supervision strategy leads to state-of-the-art synthesis results and inference time.","The video results and code are available at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS."],"url":"http://arxiv.org/abs/2310.11449v1"}
{"created":"2023-10-17 17:57:38","title":"4K4D: Real-Time 4D View Synthesis at 4K Resolution","abstract":"This paper targets high-fidelity and real-time view synthesis of dynamic 3D scenes at 4K resolution. Recently, some methods on dynamic view synthesis have shown impressive rendering quality. However, their speed is still limited when rendering high-resolution images. To overcome this problem, we propose 4K4D, a 4D point cloud representation that supports hardware rasterization and enables unprecedented rendering speed. Our representation is built on a 4D feature grid so that the points are naturally regularized and can be robustly optimized. In addition, we design a novel hybrid appearance model that significantly boosts the rendering quality while preserving efficiency. Moreover, we develop a differentiable depth peeling algorithm to effectively learn the proposed model from RGB videos. Experiments show that our representation can be rendered at over 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on the ENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30x faster than previous methods and achieves the state-of-the-art rendering quality. We will release the code for reproducibility.","sentences":["This paper targets high-fidelity and real-time view synthesis of dynamic 3D scenes at 4K resolution.","Recently, some methods on dynamic view synthesis have shown impressive rendering quality.","However, their speed is still limited when rendering high-resolution images.","To overcome this problem, we propose 4K4D, a 4D point cloud representation that supports hardware rasterization and enables unprecedented rendering speed.","Our representation is built on a 4D feature grid so that the points are naturally regularized and can be robustly optimized.","In addition, we design a novel hybrid appearance model that significantly boosts the rendering quality while preserving efficiency.","Moreover, we develop a differentiable depth peeling algorithm to effectively learn the proposed model from RGB videos.","Experiments show that our representation can be rendered at over 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on the ENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30x faster than previous methods and achieves the state-of-the-art rendering quality.","We will release the code for reproducibility."],"url":"http://arxiv.org/abs/2310.11448v1"}
{"created":"2023-10-17 17:57:31","title":"Nondango is NP-Complete","abstract":"Nondango is a pencil puzzle consisting of a rectangular grid partitioned into regions, with some cells containing a white circle. The player has to color some circles black such that every region contains exactly one black circle, and there are no three consecutive circles (horizontally, vertically, or diagonally) with the same color. In this paper, we prove that deciding the solvability of a given Nondango puzzle is NP-complete.","sentences":["Nondango is a pencil puzzle consisting of a rectangular grid partitioned into regions, with some cells containing a white circle.","The player has to color some circles black such that every region contains exactly one black circle, and there are no three consecutive circles (horizontally, vertically, or diagonally) with the same color.","In this paper, we prove that deciding the solvability of a given Nondango puzzle is NP-complete."],"url":"http://arxiv.org/abs/2310.11447v1"}
{"created":"2023-10-17 17:56:18","title":"Functional Invariants to Watermark Large Transformers","abstract":"The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to protect the integrity of large models.","sentences":["The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance.","Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance.","However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost.","This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks).","They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling.","This enables to watermark models without any change in their outputs and remains stealthy.","Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to protect the integrity of large models."],"url":"http://arxiv.org/abs/2310.11446v1"}
{"created":"2023-10-17 17:52:27","title":"Trusted Provenance of Automated, Collaborative and Adaptive Data Processing Pipelines","abstract":"To benefit from the abundance of data and the insights it brings data processing pipelines are being used in many areas of research and development in both industry and academia. One approach to automating data processing pipelines is the workflow technology, as it also supports collaborative, trial-and-error experimentation with the pipeline architecture in different application domains. In addition to the necessary flexibility that such pipelines need to possess, in collaborative settings cross-organisational interactions are plagued by lack of trust. While capturing provenance information related to the pipeline execution and the processed data is a first step towards enabling trusted collaborations, the current solutions do not allow for provenance of the change in the processing pipelines, where the subject of change can be made on any aspect of the workflow implementing the pipeline and on the data used while the pipeline is being executed. Therefore in this work we provide a solution architecture and a proof of concept implementation of a service, called Provenance Holder, which enable provenance of collaborative, adaptive data processing pipelines in a trusted manner. We also contribute a definition of a set of properties of such a service and identify future research directions.","sentences":["To benefit from the abundance of data and the insights it brings data processing pipelines are being used in many areas of research and development in both industry and academia.","One approach to automating data processing pipelines is the workflow technology, as it also supports collaborative, trial-and-error experimentation with the pipeline architecture in different application domains.","In addition to the necessary flexibility that such pipelines need to possess, in collaborative settings cross-organisational interactions are plagued by lack of trust.","While capturing provenance information related to the pipeline execution and the processed data is a first step towards enabling trusted collaborations, the current solutions do not allow for provenance of the change in the processing pipelines, where the subject of change can be made on any aspect of the workflow implementing the pipeline and on the data used while the pipeline is being executed.","Therefore in this work we provide a solution architecture and a proof of concept implementation of a service, called Provenance Holder, which enable provenance of collaborative, adaptive data processing pipelines in a trusted manner.","We also contribute a definition of a set of properties of such a service and identify future research directions."],"url":"http://arxiv.org/abs/2310.11442v1"}
{"created":"2023-10-17 17:51:31","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V","abstract":"We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting.","sentences":["We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes.","Using the marked image as input, GPT-4V can answer the questions that require visual grounding.","We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks.","For example, our experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting."],"url":"http://arxiv.org/abs/2310.11441v1"}
{"created":"2023-10-17 17:50:46","title":"EvalCrafter: Benchmarking and Evaluating Large Video Generation Models","abstract":"The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services are released for generating high-visual quality videos. However, these methods often use a few academic metrics, for example, FVD or IS, to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a new framework and pipeline to exhaustively evaluate the performance of the generated videos. To achieve this, we first conduct a new prompt list for text-to-video generation by analyzing the real-world prompt list with the help of the large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmarks, in terms of visual qualities, content qualities, motion qualities, and text-caption alignment with around 18 objective metrics. To obtain the final leaderboard of the models, we also fit a series of coefficients to align the objective metrics to the users' opinions. Based on the proposed opinion alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.","sentences":["The vision and language generative models have been overgrown in recent years.","For video generation, various open-sourced models and public-available services are released for generating high-visual quality videos.","However, these methods often use a few academic metrics, for example, FVD or IS, to evaluate the performance.","We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities.","Thus, we propose a new framework and pipeline to exhaustively evaluate the performance of the generated videos.","To achieve this, we first conduct a new prompt list for text-to-video generation by analyzing the real-world prompt list with the help of the large language model.","Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmarks, in terms of visual qualities, content qualities, motion qualities, and text-caption alignment with around 18 objective metrics.","To obtain the final leaderboard of the models, we also fit a series of coefficients to align the objective metrics to the users' opinions.","Based on the proposed opinion alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method."],"url":"http://arxiv.org/abs/2310.11440v1"}
{"created":"2023-10-17 17:50:22","title":"Understanding deep neural networks through the lens of their non-linearity","abstract":"The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.","sentences":["The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity.","Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this.","While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem.","In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications.","Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms.","We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications."],"url":"http://arxiv.org/abs/2310.11439v1"}
{"created":"2023-10-17 17:46:16","title":"Sadness, Anger, or Anxiety: Twitter Users' Emotional Responses to Toxicity in Public Conversations","abstract":"Cyberbullying and online harassment have serious negative psychological and emotional consequences for the victims, such as decreased life satisfaction, suicidal ideation, self-harming behaviors, depression, anxiety, and others. Most of the prior works assessed people's emotional responses via questionnaires, while social media platforms contain data that could provide valuable insights into users' emotions in real online discussions. Therefore, this data-driven study investigates the effect of toxicity on Twitter users' emotions and other factors associated with expressing anger, anxiety, and sadness in terms of account identifiability, activity, conversation structure, and conversation topic. To achieve this goal, we identified toxic replies in the large dataset consisting of 79,799 random Twitter conversations and obtained the emotions expressed in these conversations. Then, we performed propensity score matching and analyzed causal associations between toxicity and users' emotions. In general, we found that users receiving toxic replies are more likely to express emotions of anger, sadness, and anxiety compared to users who did not receive toxic replies. Finally, analysis results indicate that the conversation topic and users' account characteristics are likely to affect their emotional responses to toxicity. Our findings provide a better understanding of toxic replies' consequences on users' emotional states, which can potentially lead to developing personalized moderation methods that will help users emotionally cope with toxicity on social media.","sentences":["Cyberbullying and online harassment have serious negative psychological and emotional consequences for the victims, such as decreased life satisfaction, suicidal ideation, self-harming behaviors, depression, anxiety, and others.","Most of the prior works assessed people's emotional responses via questionnaires, while social media platforms contain data that could provide valuable insights into users' emotions in real online discussions.","Therefore, this data-driven study investigates the effect of toxicity on Twitter users' emotions and other factors associated with expressing anger, anxiety, and sadness in terms of account identifiability, activity, conversation structure, and conversation topic.","To achieve this goal, we identified toxic replies in the large dataset consisting of 79,799 random Twitter conversations and obtained the emotions expressed in these conversations.","Then, we performed propensity score matching and analyzed causal associations between toxicity and users' emotions.","In general, we found that users receiving toxic replies are more likely to express emotions of anger, sadness, and anxiety compared to users who did not receive toxic replies.","Finally, analysis results indicate that the conversation topic and users' account characteristics are likely to affect their emotional responses to toxicity.","Our findings provide a better understanding of toxic replies' consequences on users' emotional states, which can potentially lead to developing personalized moderation methods that will help users emotionally cope with toxicity on social media."],"url":"http://arxiv.org/abs/2310.11436v1"}
{"created":"2023-10-17 17:40:21","title":"An Empirical Study of Translation Hypothesis Ensembling with Large Language Models","abstract":"Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature.","sentences":["Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output.","In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation.","We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca.","We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding).","Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature."],"url":"http://arxiv.org/abs/2310.11430v1"}
{"created":"2023-10-17 17:39:40","title":"Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression","abstract":"This work studies training instabilities of behavior cloning with deep neural networks. We observe that minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss. We empirically disentangle the statistical and computational causes of these oscillations, and find them to stem from the chaotic propagation of minibatch SGD noise through unstable closed-loop dynamics. While SGD noise is benign in the single-step action prediction objective, it results in catastrophic error accumulation over long horizons, an effect we term gradient variance amplification (GVA). We show that many standard mitigation techniques do not alleviate GVA, but find an exponential moving average (EMA) of iterates to be surprisingly effective at doing so. We illustrate the generality of this phenomenon by showing the existence of GVA and its amelioration by EMA in both continuous control and autoregressive language generation. Finally, we provide theoretical vignettes that highlight the benefits of EMA in alleviating GVA and shed light on the extent to which classical convex models can help in understanding the benefits of iterate averaging in deep learning.","sentences":["This work studies training instabilities of behavior cloning with deep neural networks.","We observe that minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss.","We empirically disentangle the statistical and computational causes of these oscillations, and find them to stem from the chaotic propagation of minibatch SGD noise through unstable closed-loop dynamics.","While SGD noise is benign in the single-step action prediction objective, it results in catastrophic error accumulation over long horizons, an effect we term gradient variance amplification (GVA).","We show that many standard mitigation techniques do not alleviate GVA, but find an exponential moving average (EMA) of iterates to be surprisingly effective at doing so.","We illustrate the generality of this phenomenon by showing the existence of GVA and its amelioration by EMA in both continuous control and autoregressive language generation.","Finally, we provide theoretical vignettes that highlight the benefits of EMA in alleviating GVA and shed light on the extent to which classical convex models can help in understanding the benefits of iterate averaging in deep learning."],"url":"http://arxiv.org/abs/2310.11428v1"}
{"created":"2023-10-17 17:37:30","title":"Underwater and Surface Aquatic Locomotion of Soft Biomimetic Robot Based on Bending Rolled Dielectric Elastomer Actuators","abstract":"All-around, real-time navigation and sensing across the water environments by miniature soft robotics are promising, for their merits of small size, high agility and good compliance to the unstructured surroundings. In this paper, we propose and demonstrate a mantas-like soft aquatic robot which propels itself by flapping-fins using rolled dielectric elastomer actuators (DEAs) with bending motions. This robot exhibits fast-moving capabilities of swimming at 57mm/s or 1.25 body length per second (BL/s), skating on water surface at 64 mm/s (1.36 BL/s) and vertical ascending at 38mm/s (0.82 BL/s) at 1300 V, 17 Hz of the power supply. These results show the feasibility of adopting rolled DEAs for mesoscale aquatic robots with high motion performance in various water-related scenarios.","sentences":["All-around, real-time navigation and sensing across the water environments by miniature soft robotics are promising, for their merits of small size, high agility and good compliance to the unstructured surroundings.","In this paper, we propose and demonstrate a mantas-like soft aquatic robot which propels itself by flapping-fins using rolled dielectric elastomer actuators (DEAs) with bending motions.","This robot exhibits fast-moving capabilities of swimming at 57mm/s or 1.25 body length per second (BL/s), skating on water surface at 64 mm/s (1.36 BL/s) and vertical ascending at 38mm/s (0.82 BL/s) at 1300 V, 17 Hz of the power supply.","These results show the feasibility of adopting rolled DEAs for mesoscale aquatic robots with high motion performance in various water-related scenarios."],"url":"http://arxiv.org/abs/2310.11426v1"}
{"created":"2023-10-17 17:28:03","title":"Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching","abstract":"We propose a novel unsupervised learning approach for non-rigid 3D shape matching. Our approach improves upon recent state-of-the art deep functional map methods and can be applied to a broad range of different challenging scenarios. Previous deep functional map methods mainly focus on feature extraction and aim exclusively at obtaining more expressive features for functional map computation. However, the importance of the functional map computation itself is often neglected and the relationship between the functional map and point-wise map is underexplored. In this paper, we systematically investigate the coupling relationship between the functional map from the functional map solver and the point-wise map based on feature similarity. To this end, we propose a self-adaptive functional map solver to adjust the functional map regularisation for different shape matching scenarios, together with a vertex-wise contrastive loss to obtain more discriminative features. Using different challenging datasets (including non-isometry, topological noise and partiality), we demonstrate that our method substantially outperforms previous state-of-the-art methods.","sentences":["We propose a novel unsupervised learning approach for non-rigid 3D shape matching.","Our approach improves upon recent state-of-the art deep functional map methods and can be applied to a broad range of different challenging scenarios.","Previous deep functional map methods mainly focus on feature extraction and aim exclusively at obtaining more expressive features for functional map computation.","However, the importance of the functional map computation itself is often neglected and the relationship between the functional map and point-wise map is underexplored.","In this paper, we systematically investigate the coupling relationship between the functional map from the functional map solver and the point-wise map based on feature similarity.","To this end, we propose a self-adaptive functional map solver to adjust the functional map regularisation for different shape matching scenarios, together with a vertex-wise contrastive loss to obtain more discriminative features.","Using different challenging datasets (including non-isometry, topological noise and partiality), we demonstrate that our method substantially outperforms previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2310.11420v1"}
{"created":"2023-10-17 17:25:31","title":"VcT: Visual change Transformer for Remote Sensing Image Change Detection","abstract":"Existing visual change detectors usually adopt CNNs or Transformers for feature representation learning and focus on learning effective representation for the changed regions between images. Although good performance can be obtained by enhancing the features of the change regions, however, these works are still limited mainly due to the ignorance of mining the unchanged background context information. It is known that one main challenge for change detection is how to obtain the consistent representations for two images involving different variations, such as spatial variation, sunlight intensity, etc. In this work, we demonstrate that carefully mining the common background information provides an important cue to learn the consistent representations for the two images which thus obviously facilitates the visual change detection problem. Based on this observation, we propose a novel Visual change Transformer (VcT) model for visual change detection problem. To be specific, a shared backbone network is first used to extract the feature maps for the given image pair. Then, each pixel of feature map is regarded as a graph node and the graph neural network is proposed to model the structured information for coarse change map prediction. Top-K reliable tokens can be mined from the map and refined by using the clustering algorithm. Then, these reliable tokens are enhanced by first utilizing self/cross-attention schemes and then interacting with original features via an anchor-primary attention learning module. Finally, the prediction head is proposed to get a more accurate change map. Extensive experiments on multiple benchmark datasets validated the effectiveness of our proposed VcT model.","sentences":["Existing visual change detectors usually adopt CNNs or Transformers for feature representation learning and focus on learning effective representation for the changed regions between images.","Although good performance can be obtained by enhancing the features of the change regions, however, these works are still limited mainly due to the ignorance of mining the unchanged background context information.","It is known that one main challenge for change detection is how to obtain the consistent representations for two images involving different variations, such as spatial variation, sunlight intensity, etc.","In this work, we demonstrate that carefully mining the common background information provides an important cue to learn the consistent representations for the two images which thus obviously facilitates the visual change detection problem.","Based on this observation, we propose a novel Visual change Transformer (VcT) model for visual change detection problem.","To be specific, a shared backbone network is first used to extract the feature maps for the given image pair.","Then, each pixel of feature map is regarded as a graph node and the graph neural network is proposed to model the structured information for coarse change map prediction.","Top-K reliable tokens can be mined from the map and refined by using the clustering algorithm.","Then, these reliable tokens are enhanced by first utilizing self/cross-attention schemes and then interacting with original features via an anchor-primary attention learning module.","Finally, the prediction head is proposed to get a more accurate change map.","Extensive experiments on multiple benchmark datasets validated the effectiveness of our proposed VcT model."],"url":"http://arxiv.org/abs/2310.11417v1"}
{"created":"2023-10-17 17:15:41","title":"Evaluating LLMs for Privilege-Escalation Scenarios","abstract":"Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs). We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. We create an automated Linux privilege-escalation benchmark utilizing local virtual machines. We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark. We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs. We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them with both stochastic parrots as well as with human hackers.","sentences":["Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks.","One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs).","We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation.","We create an automated Linux privilege-escalation benchmark utilizing local virtual machines.","We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark.","We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs.","We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them with both stochastic parrots as well as with human hackers."],"url":"http://arxiv.org/abs/2310.11409v1"}
{"created":"2023-10-17 17:14:07","title":"Group-blind optimal transport to group parity and its constrained variants","abstract":"Fairness holds a pivotal role in the realm of machine learning, particularly when it comes to addressing groups categorised by sensitive attributes, e.g., gender, race. Prevailing algorithms in fair learning predominantly hinge on accessibility or estimations of these sensitive attributes, at least in the training process. We design a single group-blind projection map that aligns the feature distributions of both groups in the source data, achieving (demographic) group parity, without requiring values of the protected attribute for individual samples in the computation of the map, as well as its use. Instead, our approach utilises the feature distributions of the privileged and unprivileged groups in a boarder population and the essential assumption that the source data are unbiased representation of the population. We present numerical results on synthetic data and real data.","sentences":["Fairness holds a pivotal role in the realm of machine learning, particularly when it comes to addressing groups categorised by sensitive attributes, e.g., gender, race.","Prevailing algorithms in fair learning predominantly hinge on accessibility or estimations of these sensitive attributes, at least in the training process.","We design a single group-blind projection map that aligns the feature distributions of both groups in the source data, achieving (demographic) group parity, without requiring values of the protected attribute for individual samples in the computation of the map, as well as its use.","Instead, our approach utilises the feature distributions of the privileged and unprivileged groups in a boarder population and the essential assumption that the source data are unbiased representation of the population.","We present numerical results on synthetic data and real data."],"url":"http://arxiv.org/abs/2310.11407v1"}
{"created":"2023-10-17 17:14:01","title":"GreenNFV: Energy-Efficient Network Function Virtualization with Service Level Agreement Constraints","abstract":"Network Function Virtualization (NFV) platforms consume significant energy, introducing high operational costs in edge and data centers. This paper presents a novel framework called GreenNFV that optimizes resource usage for network function chains using deep reinforcement learning. GreenNFV optimizes resource parameters such as CPU sharing ratio, CPU frequency scaling, last-level cache (LLC) allocation, DMA buffer size, and packet batch size. GreenNFV learns the resource scheduling model from the benchmark experiments and takes Service Level Agreements (SLAs) into account to optimize resource usage models based on the different throughput and energy consumption requirements. Our evaluation shows that GreenNFV models achieve high transfer throughput and low energy consumption while satisfying various SLA constraints. Specifically, GreenNFV with Throughput SLA can achieve $4.4\\times$ higher throughput and $1.5\\times$ better energy efficiency over the baseline settings, whereas GreenNFV with Energy SLA can achieve $3\\times$ higher throughput while reducing energy consumption by 50%.","sentences":["Network Function Virtualization (NFV) platforms consume significant energy, introducing high operational costs in edge and data centers.","This paper presents a novel framework called GreenNFV that optimizes resource usage for network function chains using deep reinforcement learning.","GreenNFV optimizes resource parameters such as CPU sharing ratio, CPU frequency scaling, last-level cache (LLC) allocation, DMA buffer size, and packet batch size.","GreenNFV learns the resource scheduling model from the benchmark experiments and takes Service Level Agreements (SLAs) into account to optimize resource usage models based on the different throughput and energy consumption requirements.","Our evaluation shows that GreenNFV models achieve high transfer throughput and low energy consumption while satisfying various SLA constraints.","Specifically, GreenNFV with Throughput SLA can achieve $4.4\\times$ higher throughput and $1.5\\times$ better energy efficiency over the baseline settings, whereas GreenNFV with Energy SLA can achieve $3\\times$ higher throughput while reducing energy consumption by 50%."],"url":"http://arxiv.org/abs/2310.11406v1"}
{"created":"2023-10-17 17:13:13","title":"On Coherence-based Predictors for Dense Query Performance Prediction","abstract":"Query Performance Prediction (QPP) estimates the effectiveness of a search engine's results in response to a query without relevance judgments. Traditionally, post-retrieval predictors have focused upon either the distribution of the retrieval scores, or the coherence of the top-ranked documents using traditional bag-of-words index representations. More recently, BERT-based models using dense embedded document representations have been used to create new predictors, but mostly applied to predict the performance of rankings created by BM25. Instead, we aim to predict the effectiveness of rankings created by single-representation dense retrieval models (ANCE & TCT-ColBERT). Therefore, we propose a number of variants of existing unsupervised coherence-based predictors that employ neural embedding representations. In our experiments on the TREC Deep Learning Track datasets, we demonstrate improved accuracy upon dense retrieval (up to 92% compared to sparse variants for TCT-ColBERT and 188% for ANCE). Going deeper, we select the most representative and best performing predictors to study the importance of differences among predictors and query types on query performance. Using existing distribution-based evaluation QPP measures and a particular type of linear mixed models, we find that query types further significantly influence query performance (and are up to 35% responsible for the unstable performance of QPP predictors), and that this sensitivity is unique to dense retrieval models. Our approach introduces a new setting for obtaining richer information on query differences in dense QPP that can explain potential unstable performance of existing predictors and outlines the unique characteristics of different query types on dense retrieval models.","sentences":["Query Performance Prediction (QPP) estimates the effectiveness of a search engine's results in response to a query without relevance judgments.","Traditionally, post-retrieval predictors have focused upon either the distribution of the retrieval scores, or the coherence of the top-ranked documents using traditional bag-of-words index representations.","More recently, BERT-based models using dense embedded document representations have been used to create new predictors, but mostly applied to predict the performance of rankings created by BM25.","Instead, we aim to predict the effectiveness of rankings created by single-representation dense retrieval models (ANCE & TCT-ColBERT).","Therefore, we propose a number of variants of existing unsupervised coherence-based predictors that employ neural embedding representations.","In our experiments on the TREC Deep Learning Track datasets, we demonstrate improved accuracy upon dense retrieval (up to 92% compared to sparse variants for TCT-ColBERT and 188% for ANCE).","Going deeper, we select the most representative and best performing predictors to study the importance of differences among predictors and query types on query performance.","Using existing distribution-based evaluation QPP measures and a particular type of linear mixed models, we find that query types further significantly influence query performance (and are up to 35% responsible for the unstable performance of QPP predictors), and that this sensitivity is unique to dense retrieval models.","Our approach introduces a new setting for obtaining richer information on query differences in dense QPP that can explain potential unstable performance of existing predictors and outlines the unique characteristics of different query types on dense retrieval models."],"url":"http://arxiv.org/abs/2310.11405v1"}
{"created":"2023-10-17 17:10:56","title":"Enhancing Group Fairness in Online Settings Using Oblique Decision Forests","abstract":"Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of oblique decision trees, to make fair decisions in online settings. The hierarchical tree structure of Aranyani enables parameter isolation and allows us to efficiently compute the fairness gradients using aggregate statistics of previous decisions, eliminating the need for additional storage and forward/backward passes. We also present an efficient framework to train Aranyani and theoretically analyze several of its properties. We conduct empirical evaluations on 5 publicly available benchmarks (including vision and language datasets) to show that Aranyani achieves a better accuracy-fairness trade-off compared to baseline approaches.","sentences":["Fairness, especially group fairness, is an important consideration in the context of machine learning systems.","The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process.","However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges.","In particular, group fairness objectives are defined using expectations of predictions across different demographic groups.","In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step.","In this paper, we propose Aranyani, an ensemble of oblique decision trees, to make fair decisions in online settings.","The hierarchical tree structure of Aranyani enables parameter isolation and allows us to efficiently compute the fairness gradients using aggregate statistics of previous decisions, eliminating the need for additional storage and forward/backward passes.","We also present an efficient framework to train Aranyani and theoretically analyze several of its properties.","We conduct empirical evaluations on 5 publicly available benchmarks (including vision and language datasets) to show that Aranyani achieves a better accuracy-fairness trade-off compared to baseline approaches."],"url":"http://arxiv.org/abs/2310.11401v1"}
{"created":"2023-10-17 17:06:26","title":"Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks","abstract":"In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision. Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances. This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation. Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach. The experimental results unveil a significant enhancement in BLEU scores with our method. Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable reduction in model perplexity compared to its original counterpart. These experimental outcomes not only validate the efficacy of our method but also reveal the immense potential in optimizing the self-attention mechanism through neural network-based QKV computation, paving the way for future research and practical applications. The source code and implementation details for our proposed method can be accessed at https://github.com/ocislyjrti/NeuralAttention.","sentences":["In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision.","Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances.","This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation.","Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach.","The experimental results unveil a significant enhancement in BLEU scores with our method.","Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable reduction in model perplexity compared to its original counterpart.","These experimental outcomes not only validate the efficacy of our method but also reveal the immense potential in optimizing the self-attention mechanism through neural network-based QKV computation, paving the way for future research and practical applications.","The source code and implementation details for our proposed method can be accessed at https://github.com/ocislyjrti/NeuralAttention."],"url":"http://arxiv.org/abs/2310.11398v1"}
{"created":"2023-10-17 17:03:00","title":"Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning","abstract":"Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.","sentences":["Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences.","However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges.","Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated.","In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security).","Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses."],"url":"http://arxiv.org/abs/2310.11397v1"}
{"created":"2023-10-17 16:45:47","title":"Towards Automatic Satellite Images Captions Generation Using Large Language Models","abstract":"Automatic image captioning is a promising technique for conveying visual information using natural language. It can benefit various tasks in satellite remote sensing, such as environmental monitoring, resource management, disaster management, etc. However, one of the main challenges in this domain is the lack of large-scale image-caption datasets, as they require a lot of human expertise and effort to create. Recent research on large language models (LLMs) has demonstrated their impressive performance in natural language understanding and generation tasks. Nonetheless, most of them cannot handle images (GPT-3.5, Falcon, Claude, etc.), while conventional captioning models pre-trained on general ground-view images often fail to produce detailed and accurate captions for aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to automatically collect captions for remote sensing images by guiding LLMs to describe their object annotations. We also present a benchmark model that adapts the pre-trained generative image2text model (GIT) to generate high-quality captions for remote-sensing images. Our evaluation demonstrates the effectiveness of our approach for collecting captions for remote sensing images.","sentences":["Automatic image captioning is a promising technique for conveying visual information using natural language.","It can benefit various tasks in satellite remote sensing, such as environmental monitoring, resource management, disaster management, etc.","However, one of the main challenges in this domain is the lack of large-scale image-caption datasets, as they require a lot of human expertise and effort to create.","Recent research on large language models (LLMs) has demonstrated their impressive performance in natural language understanding and generation tasks.","Nonetheless, most of them cannot handle images (GPT-3.5, Falcon, Claude, etc.), while conventional captioning models pre-trained on general ground-view images often fail to produce detailed and accurate captions for aerial images (BLIP, GIT, CM3, CM3Leon, etc.).","To address this problem, we propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to automatically collect captions for remote sensing images by guiding LLMs to describe their object annotations.","We also present a benchmark model that adapts the pre-trained generative image2text model (GIT) to generate high-quality captions for remote-sensing images.","Our evaluation demonstrates the effectiveness of our approach for collecting captions for remote sensing images."],"url":"http://arxiv.org/abs/2310.11392v1"}
{"created":"2023-10-17 16:35:39","title":"VaR\\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds","abstract":"We tackle the problem of estimating the Value-at-Risk (VaR) and the Conditional Value-at-Risk (CVaR) of the infinite-horizon discounted cost within a Markov cost process. First, we derive a minimax lower bound of $\\Omega(1/\\sqrt{n})$ that holds both in an expected and in a probabilistic sense. Then, using a finite-horizon truncation scheme, we derive an upper bound for the error in CVaR estimation, which matches our lower bound up to constant factors. Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, e.g., spectral risk measures, utility-based shortfall risk. To the best of our knowledge, our work is the first to provide lower and upper bounds on the estimation error for any risk measure within Markovian settings. We remark that our lower bounds also extend to the infinite-horizon discounted costs' mean. Even in that case, our result $\\Omega(1/\\sqrt{n}) $ improves upon the existing result $\\Omega(1/n)$[13].","sentences":["We tackle the problem of estimating the Value-at-Risk (VaR) and the Conditional Value-at-Risk (CVaR) of the infinite-horizon discounted cost within a Markov cost process.","First, we derive a minimax lower bound of $\\Omega(1/\\sqrt{n})$ that holds both in an expected and in a probabilistic sense.","Then, using a finite-horizon truncation scheme, we derive an upper bound for the error in CVaR estimation, which matches our lower bound up to constant factors.","Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, e.g., spectral risk measures, utility-based shortfall risk.","To the best of our knowledge, our work is the first to provide lower and upper bounds on the estimation error for any risk measure within Markovian settings.","We remark that our lower bounds also extend to the infinite-horizon discounted costs' mean.","Even in that case, our result $\\Omega(1/\\sqrt{n}) $ improves upon the existing result $\\Omega(1/n)$[13]."],"url":"http://arxiv.org/abs/2310.11389v1"}
{"created":"2023-10-17 16:35:03","title":"Towards Operationalizing Social Bonding in Human-Robot Dyads","abstract":"With momentum increasing in the use of social robots as long-term assistive and collaborative partners, humans developing social bonds with these artificial agents appears to be inevitable. In human-human dyads, social bonding plays a powerful role in regulating behaviours, emotions, and even health. If this is to extend to human-robot dyads, the phenomenology of such relationships (including their emergence and stability) must be better understood. In this paper, we discuss potential approaches towards operationalizing the phenomenon of social bonding between human-robot dyads. We will discuss a number of biobehavioural proxies of social bonding, moving away from existing approaches that use subjective, psychological measures, and instead grounding our approach in some of the evolutionary, neurobiological and physiological correlates of social bond formation in natural systems: (a) reductions in physiological stress (the ''social buffering'' phenomenon), (b) narrowing of spatial proximity between dyads, and (c) inter-dyad behavioural synchrony. We provide relevant evolutionary support for each proposed component, with suggestions and considerations for how they can be recorded in (real-time) human-robot interaction scenarios. With this, we aim to inspire more robust operationalisation of ''social bonding'' between human and artificial (robotic) agents.","sentences":["With momentum increasing in the use of social robots as long-term assistive and collaborative partners, humans developing social bonds with these artificial agents appears to be inevitable.","In human-human dyads, social bonding plays a powerful role in regulating behaviours, emotions, and even health.","If this is to extend to human-robot dyads, the phenomenology of such relationships (including their emergence and stability) must be better understood.","In this paper, we discuss potential approaches towards operationalizing the phenomenon of social bonding between human-robot dyads.","We will discuss a number of biobehavioural proxies of social bonding, moving away from existing approaches that use subjective, psychological measures, and instead grounding our approach in some of the evolutionary, neurobiological and physiological correlates of social bond formation in natural systems: (a) reductions in physiological stress (the ''social buffering'' phenomenon), (b) narrowing of spatial proximity between dyads, and (c) inter-dyad behavioural synchrony.","We provide relevant evolutionary support for each proposed component, with suggestions and considerations for how they can be recorded in (real-time) human-robot interaction scenarios.","With this, we aim to inspire more robust operationalisation of ''social bonding'' between human and artificial (robotic) agents."],"url":"http://arxiv.org/abs/2310.11386v1"}
{"created":"2023-10-17 16:32:38","title":"A voxel-level approach to brain age prediction: A method to assess regional brain aging","abstract":"Brain aging is a regional phenomenon, a facet that remains relatively under-explored within the realm of brain age prediction research using machine learning methods. Voxel-level predictions can provide localized brain age estimates that can provide granular insights into the regional aging processes. This is essential to understand the differences in aging trajectories in healthy versus diseased subjects. In this work, a deep learning-based multitask model is proposed for voxel-level brain age prediction from T1-weighted magnetic resonance images. The proposed model outperforms the models existing in the literature and yields valuable clinical insights when applied to both healthy and diseased populations. Regional analysis is performed on the voxel-level brain age predictions to understand aging trajectories of known anatomical regions in the brain and show that there exist disparities in regional aging trajectories of healthy subjects compared to ones with underlying neurological disorders such as Dementia and more specifically, Alzheimer's disease. Our code is available at https://github.com/nehagianchandani/Voxel-level-brain-age-prediction.","sentences":["Brain aging is a regional phenomenon, a facet that remains relatively under-explored within the realm of brain age prediction research using machine learning methods.","Voxel-level predictions can provide localized brain age estimates that can provide granular insights into the regional aging processes.","This is essential to understand the differences in aging trajectories in healthy versus diseased subjects.","In this work, a deep learning-based multitask model is proposed for voxel-level brain age prediction from T1-weighted magnetic resonance images.","The proposed model outperforms the models existing in the literature and yields valuable clinical insights when applied to both healthy and diseased populations.","Regional analysis is performed on the voxel-level brain age predictions to understand aging trajectories of known anatomical regions in the brain and show that there exist disparities in regional aging trajectories of healthy subjects compared to ones with underlying neurological disorders such as Dementia and more specifically, Alzheimer's disease.","Our code is available at https://github.com/nehagianchandani/Voxel-level-brain-age-prediction."],"url":"http://arxiv.org/abs/2310.11385v1"}
{"created":"2023-10-17 16:22:18","title":"Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles","abstract":"Voice-based interfaces rely on a wake-up word mechanism to initiate communication with devices. However, achieving a robust, energy-efficient, and fast detection remains a challenge. This paper addresses these real production needs by enhancing data with temporal alignments and using detection based on two phases with multi-resolution. It employs two models: a lightweight on-device model for real-time processing of the audio stream and a verification model on the server-side, which is an ensemble of heterogeneous architectures that refine detection. This scheme allows the optimization of two operating points. To protect privacy, audio features are sent to the cloud instead of raw audio. The study investigated different parametric configurations for feature extraction to select one for on-device detection and another for the verification model. Furthermore, thirteen different audio classifiers were compared in terms of performance and inference time. The proposed ensemble outperforms our stronger classifier in every noise condition.","sentences":["Voice-based interfaces rely on a wake-up word mechanism to initiate communication with devices.","However, achieving a robust, energy-efficient, and fast detection remains a challenge.","This paper addresses these real production needs by enhancing data with temporal alignments and using detection based on two phases with multi-resolution.","It employs two models: a lightweight on-device model for real-time processing of the audio stream and a verification model on the server-side, which is an ensemble of heterogeneous architectures that refine detection.","This scheme allows the optimization of two operating points.","To protect privacy, audio features are sent to the cloud instead of raw audio.","The study investigated different parametric configurations for feature extraction to select one for on-device detection and another for the verification model.","Furthermore, thirteen different audio classifiers were compared in terms of performance and inference time.","The proposed ensemble outperforms our stronger classifier in every noise condition."],"url":"http://arxiv.org/abs/2310.11379v1"}
{"created":"2023-10-17 16:21:28","title":"Faster Algorithms for Generalized Mean Densest Subgraph Problem","abstract":"The densest subgraph of a large graph usually refers to some subgraph with the highest average degree, which has been extended to the family of $p$-means dense subgraph objectives by~\\citet{veldt2021generalized}. The $p$-mean densest subgraph problem seeks a subgraph with the highest average $p$-th-power degree, whereas the standard densest subgraph problem seeks a subgraph with a simple highest average degree. It was shown that the standard peeling algorithm can perform arbitrarily poorly on generalized objective when $p>1$ but uncertain when $0<p<1$. In this paper, we are the first to show that a standard peeling algorithm can still yield $2^{1/p}$-approximation for the case $0<p < 1$. (Veldt 2021) proposed a new generalized peeling algorithm (GENPEEL), which for $p \\geq 1$ has an approximation guarantee ratio $(p+1)^{1/p}$, and time complexity $O(mn)$, where $m$ and $n$ denote the number of edges and nodes in graph respectively. In terms of algorithmic contributions, we propose a new and faster generalized peeling algorithm (called GENPEEL++ in this paper), which for $p \\in [1, +\\infty)$ has an approximation guarantee ratio $(2(p+1))^{1/p}$, and time complexity $O(m(\\log n))$, where $m$ and $n$ denote the number of edges and nodes in graph, respectively. This approximation ratio converges to 1 as $p \\rightarrow \\infty$.","sentences":["The densest subgraph of a large graph usually refers to some subgraph with the highest average degree, which has been extended to the family of $p$-means dense subgraph objectives by~\\citet{veldt2021generalized}.","The $p$-mean densest subgraph problem seeks a subgraph with the highest average $p$-th-power degree, whereas the standard densest subgraph problem seeks a subgraph with a simple highest average degree.","It was shown that the standard peeling algorithm can perform arbitrarily poorly on generalized objective when $p>1$ but uncertain when $0<p<1$. In this paper, we are the first to show that a standard peeling algorithm can still yield $2^{1/p}$-approximation for the case $0<p < 1$. (Veldt 2021) proposed a new generalized peeling algorithm (GENPEEL), which for $p \\geq 1$ has an approximation guarantee ratio $(p+1)^{1/p}$, and time complexity $O(mn)$, where $m$ and $n$ denote the number of edges and nodes in graph respectively.","In terms of algorithmic contributions, we propose a new and faster generalized peeling algorithm (called GENPEEL++ in this paper), which for $p \\in [1, +\\infty)$ has an approximation guarantee ratio $(2(p+1))^{1/p}$, and time complexity $O(m(\\log n))$, where $m$ and $n$ denote the number of edges and nodes in graph, respectively.","This approximation ratio converges to 1 as $p \\rightarrow \\infty$."],"url":"http://arxiv.org/abs/2310.11377v1"}
{"created":"2023-10-17 16:15:34","title":"DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations","abstract":"Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP. Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typical trained without leveraging multi-modal information. To overcome these limitations, we propose DialogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues. The visual information is considered as the supplementary knowledge to construct high-quality instructions. We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datasets and compare the results against the SOTA baselines and other SOTA LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers.","sentences":["Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP.","Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain.","As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision.","Another limitation of LLMs is that they are typical trained without leveraging multi-modal information.","To overcome these limitations, we propose DialogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.","The visual information is considered as the supplementary knowledge to construct high-quality instructions.","We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datasets and compare the results against the SOTA baselines and other SOTA LLMs.","Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers."],"url":"http://arxiv.org/abs/2310.11374v1"}
{"created":"2023-10-17 16:15:28","title":"A Two-Layer Blockchain Sharding Protocol Leveraging Safety and Liveness for Enhanced Performance","abstract":"Sharding is essential for improving blockchain scalability. Existing protocols overlook diverse adversarial attacks, limiting transaction throughput. This paper presents Reticulum, a groundbreaking sharding protocol addressing this issue, boosting blockchain scalability.   Reticulum employs a two-phase approach, adapting transaction throughput based on runtime adversarial attacks. It comprises \"control\" and \"process\" shards in two layers. Process shards contain at least one trustworthy node, while control shards have a majority of trusted nodes. In the first phase, transactions are written to blocks and voted on by nodes in process shards. Unanimously accepted blocks are confirmed. In the second phase, blocks without unanimous acceptance are voted on by control shards. Blocks are accepted if the majority votes in favor, eliminating first-phase opponents and silent voters. Reticulum uses unanimous voting in the first phase, involving fewer nodes, enabling more parallel process shards. Control shards finalize decisions and resolve disputes.   Experiments confirm Reticulum's innovative design, providing high transaction throughput and robustness against various network attacks, outperforming existing sharding protocols for blockchain networks.","sentences":["Sharding is essential for improving blockchain scalability.","Existing protocols overlook diverse adversarial attacks, limiting transaction throughput.","This paper presents Reticulum, a groundbreaking sharding protocol addressing this issue, boosting blockchain scalability.   ","Reticulum employs a two-phase approach, adapting transaction throughput based on runtime adversarial attacks.","It comprises \"control\" and \"process\" shards in two layers.","Process shards contain at least one trustworthy node, while control shards have a majority of trusted nodes.","In the first phase, transactions are written to blocks and voted on by nodes in process shards.","Unanimously accepted blocks are confirmed.","In the second phase, blocks without unanimous acceptance are voted on by control shards.","Blocks are accepted if the majority votes in favor, eliminating first-phase opponents and silent voters.","Reticulum uses unanimous voting in the first phase, involving fewer nodes, enabling more parallel process shards.","Control shards finalize decisions and resolve disputes.   ","Experiments confirm Reticulum's innovative design, providing high transaction throughput and robustness against various network attacks, outperforming existing sharding protocols for blockchain networks."],"url":"http://arxiv.org/abs/2310.11373v1"}
{"created":"2023-10-17 16:14:06","title":"3D Force and Contact Estimation for a Soft-Bubble Visuotactile Sensor Using FEM","abstract":"Soft-bubble tactile sensors have the potential to capture dense contact and force information across a large contact surface. However, it is difficult to extract contact forces directly from observing the bubble surface because local contacts change the global surface shape significantly due to membrane mechanics and air pressure. This paper presents a model-based method of reconstructing dense contact forces from the bubble sensor's internal RGBD camera and air pressure sensor. We present a finite element model of the force response of the bubble sensor that uses a linear plane stress approximation that only requires calibrating 3 variables. Our method is shown to reconstruct normal and shear forces significantly more accurately than the state-of-the-art, with comparable accuracy for detecting the contact patch, and with very little calibration data.","sentences":["Soft-bubble tactile sensors have the potential to capture dense contact and force information across a large contact surface.","However, it is difficult to extract contact forces directly from observing the bubble surface because local contacts change the global surface shape significantly due to membrane mechanics and air pressure.","This paper presents a model-based method of reconstructing dense contact forces from the bubble sensor's internal RGBD camera and air pressure sensor.","We present a finite element model of the force response of the bubble sensor that uses a linear plane stress approximation that only requires calibrating 3 variables.","Our method is shown to reconstruct normal and shear forces significantly more accurately than the state-of-the-art, with comparable accuracy for detecting the contact patch, and with very little calibration data."],"url":"http://arxiv.org/abs/2310.11372v1"}
{"created":"2023-10-17 16:13:07","title":"Improving Operator Situation Awareness when Working with AI Recommender Systems","abstract":"AI recommender systems are sought for decision support by providing suggestions to operators responsible for making final decisions. However, these systems are typically considered black boxes, and are often presented without any context or insight into the underlying algorithm. As a result, recommender systems can lead to miscalibrated user reliance and decreased situation awareness. Recent work has focused on improving the transparency of recommender systems in various ways such as improving the recommender's analysis and visualization of the figures of merit, providing explanations for the recommender's decision, as well as improving user training or calibrating user trust. In this paper, we introduce an alternative transparency technique of structuring the order in which contextual information and the recommender's decision are shown to the human operator. This technique is designed to improve the operator's situation awareness and therefore the shared situation awareness between the operator and the recommender system. This paper presents the results of a two-phase between-subjects study in which participants and a recommender system jointly make a high-stakes decision. We varied the amount of contextual information the participant had, the assessment technique of the figures of merit, and the reliability of the recommender system. We found that providing contextual information upfront improves the team's shared situation awareness by improving the human decision maker's initial and final judgment, as well as their ability to discern the recommender's error boundary. Additionally, this technique accurately calibrated the human operator's trust in the recommender. This work proposes and validates a way to provide model-agnostic transparency into AI systems that can support the human decision maker and lead to improved team performance.","sentences":["AI recommender systems are sought for decision support by providing suggestions to operators responsible for making final decisions.","However, these systems are typically considered black boxes, and are often presented without any context or insight into the underlying algorithm.","As a result, recommender systems can lead to miscalibrated user reliance and decreased situation awareness.","Recent work has focused on improving the transparency of recommender systems in various ways such as improving the recommender's analysis and visualization of the figures of merit, providing explanations for the recommender's decision, as well as improving user training or calibrating user trust.","In this paper, we introduce an alternative transparency technique of structuring the order in which contextual information and the recommender's decision are shown to the human operator.","This technique is designed to improve the operator's situation awareness and therefore the shared situation awareness between the operator and the recommender system.","This paper presents the results of a two-phase between-subjects study in which participants and a recommender system jointly make a high-stakes decision.","We varied the amount of contextual information the participant had, the assessment technique of the figures of merit, and the reliability of the recommender system.","We found that providing contextual information upfront improves the team's shared situation awareness by improving the human decision maker's initial and final judgment, as well as their ability to discern the recommender's error boundary.","Additionally, this technique accurately calibrated the human operator's trust in the recommender.","This work proposes and validates a way to provide model-agnostic transparency into AI systems that can support the human decision maker and lead to improved team performance."],"url":"http://arxiv.org/abs/2310.11370v1"}
{"created":"2023-10-17 16:05:52","title":"VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights","abstract":"Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives. Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited performance. Our dataset poses unique challenges offering significant room for improvement regarding performance, explainability, and robustness.","sentences":["Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need.","This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection.","However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it.","To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale.","We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives.","Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts.","Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited performance.","Our dataset poses unique challenges offering significant room for improvement regarding performance, explainability, and robustness."],"url":"http://arxiv.org/abs/2310.11368v1"}
{"created":"2023-10-17 16:05:10","title":"Towards the Characterization of Terminal Cut Functions: a Condition for Laminar Families","abstract":"We study the following characterization problem. Given a set $T$ of terminals and a $(2^{|T|}-2)$-dimensional vector $\\pi$ whose coordinates are indexed by proper subsets of $T$, is there a graph $G$ that contains $T$, such that for all subsets $\\emptyset\\subsetneq S\\subsetneq T$, $\\pi_S$ equals the value of the min-cut in $G$ separating $S$ from $T\\setminus S$? The only known necessary conditions are submodularity and a special class of linear inequalities given by Chaudhuri, Subrahmanyam, Wagner and Zaroliagis.   Our main result is a new class of linear inequalities concerning laminar families, that generalize all previous ones. Using our new class of inequalities, we can generalize Karger's approximate min-cut counting result to graphs with terminals.","sentences":["We study the following characterization problem.","Given a set $T$ of terminals and a $(2^{|T|}-2)$-dimensional vector $\\pi$ whose coordinates are indexed by proper subsets of $T$, is there a graph $G$ that contains $T$, such that for all subsets $\\emptyset\\subsetneq S\\subsetneq T$, $\\pi_S$ equals the value of the min-cut in $G$ separating $S$ from $T\\setminus S$?","The only known necessary conditions are submodularity and a special class of linear inequalities given by Chaudhuri, Subrahmanyam, Wagner and Zaroliagis.   ","Our main result is a new class of linear inequalities concerning laminar families, that generalize all previous ones.","Using our new class of inequalities, we can generalize Karger's approximate min-cut counting result to graphs with terminals."],"url":"http://arxiv.org/abs/2310.11367v1"}
{"created":"2023-10-17 16:04:33","title":"Lie Group Decompositions for Equivariant Neural Networks","abstract":"Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective. Further limitations are encountered when $G$ is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \\text{GL}^{+}(n, \\mathbb{R})$ and $G = \\text{SL}(n, \\mathbb{R})$, as well as their representation as affine transformations $\\mathbb{R}^{n} \\rtimes G$. Invariant integration as well as a global parametrization is realized by decomposing the `larger` groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals.","sentences":["Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime.","Much work has focused on the case where the symmetry group employed is compact or abelian, or both.","Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps.","The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective.","Further limitations are encountered when $G$ is neither compact nor abelian.","Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \\text{GL}^{+}(n, \\mathbb{R})$ and $G = \\text{SL}(n, \\mathbb{R})$, as well as their representation as affine transformations $\\mathbb{R}^{n} \\rtimes G$. Invariant integration as well as a global parametrization is realized by decomposing the `larger` groups into subgroups and submanifolds which can be handled individually.","Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations.","We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals."],"url":"http://arxiv.org/abs/2310.11366v1"}
{"created":"2023-10-17 16:02:07","title":"High-Fidelity Noise Reduction with Differentiable Signal Processing","abstract":"Noise reduction techniques based on deep learning have demonstrated impressive performance in enhancing the overall quality of recorded speech. While these approaches are highly performant, their application in audio engineering can be limited due to a number of factors. These include operation only on speech without support for music, lack of real-time capability, lack of interpretable control parameters, operation at lower sample rates, and a tendency to introduce artifacts. On the other hand, signal processing-based noise reduction algorithms offer fine-grained control and operation on a broad range of content, however, they often require manual operation to achieve the best results. To address the limitations of both approaches, in this work we introduce a method that leverages a signal processing-based denoiser that when combined with a neural network controller, enables fully automatic and high-fidelity noise reduction on both speech and music signals. We evaluate our proposed method with objective metrics and a perceptual listening test. Our evaluation reveals that speech enhancement models can be extended to music, however training the model to remove only stationary noise is critical. Furthermore, our proposed approach achieves performance on par with the deep learning models, while being significantly more efficient and introducing fewer artifacts in some cases. Listening examples are available online at https://tape.it/research/denoiser .","sentences":["Noise reduction techniques based on deep learning have demonstrated impressive performance in enhancing the overall quality of recorded speech.","While these approaches are highly performant, their application in audio engineering can be limited due to a number of factors.","These include operation only on speech without support for music, lack of real-time capability, lack of interpretable control parameters, operation at lower sample rates, and a tendency to introduce artifacts.","On the other hand, signal processing-based noise reduction algorithms offer fine-grained control and operation on a broad range of content, however, they often require manual operation to achieve the best results.","To address the limitations of both approaches, in this work we introduce a method that leverages a signal processing-based denoiser that when combined with a neural network controller, enables fully automatic and high-fidelity noise reduction on both speech and music signals.","We evaluate our proposed method with objective metrics and a perceptual listening test.","Our evaluation reveals that speech enhancement models can be extended to music, however training the model to remove only stationary noise is critical.","Furthermore, our proposed approach achieves performance on par with the deep learning models, while being significantly more efficient and introducing fewer artifacts in some cases.","Listening examples are available online at https://tape.it/research/denoiser ."],"url":"http://arxiv.org/abs/2310.11364v1"}
{"created":"2023-10-17 16:00:26","title":"Disentangling the Linguistic Competence of Privacy-Preserving BERT","abstract":"Differential Privacy (DP) has been tailored to address the unique challenges of text-to-text privatization. However, text-to-text privatization is known for degrading the performance of language models when trained on perturbed text. Employing a series of interpretation techniques on the internal representations extracted from BERT trained on perturbed pre-text, we intend to disentangle at the linguistic level the distortion induced by differential privacy. Experimental results from a representational similarity analysis indicate that the overall similarity of internal representations is substantially reduced. Using probing tasks to unpack this dissimilarity, we find evidence that text-to-text privatization affects the linguistic competence across several formalisms, encoding localized properties of words while falling short at encoding the contextual relationships between spans of words.","sentences":["Differential Privacy (DP) has been tailored to address the unique challenges of text-to-text privatization.","However, text-to-text privatization is known for degrading the performance of language models when trained on perturbed text.","Employing a series of interpretation techniques on the internal representations extracted from BERT trained on perturbed pre-text, we intend to disentangle at the linguistic level the distortion induced by differential privacy.","Experimental results from a representational similarity analysis indicate that the overall similarity of internal representations is substantially reduced.","Using probing tasks to unpack this dissimilarity, we find evidence that text-to-text privatization affects the linguistic competence across several formalisms, encoding localized properties of words while falling short at encoding the contextual relationships between spans of words."],"url":"http://arxiv.org/abs/2310.11363v1"}
{"created":"2023-10-17 15:55:31","title":"Enhancing Neural Machine Translation with Semantic Units","abstract":"Conventional neural machine translation (NMT) models typically use subwords and words as the basic units for model input and comprehension. However, complete words and phrases composed of several tokens are often the fundamental units for expressing semantics, referred to as semantic units. To address this issue, we propose a method Semantic Units for Machine Translation (SU4MT) which models the integral meanings of semantic units within a sentence, and then leverages them to provide a new perspective for understanding the sentence. Specifically, we first propose Word Pair Encoding (WPE), a phrase extraction method to help identify the boundaries of semantic units. Next, we design an Attentive Semantic Fusion (ASF) layer to integrate the semantics of multiple subwords into a single vector: the semantic unit representation. Lastly, the semantic-unit-level sentence representation is concatenated to the token-level one, and they are combined as the input of encoder. Experimental results demonstrate that our method effectively models and leverages semantic-unit-level information and outperforms the strong baselines. The code is available at https://github.com/ictnlp/SU4MT.","sentences":["Conventional neural machine translation (NMT) models typically use subwords and words as the basic units for model input and comprehension.","However, complete words and phrases composed of several tokens are often the fundamental units for expressing semantics, referred to as semantic units.","To address this issue, we propose a method Semantic Units for Machine Translation (SU4MT) which models the integral meanings of semantic units within a sentence, and then leverages them to provide a new perspective for understanding the sentence.","Specifically, we first propose Word Pair Encoding (WPE), a phrase extraction method to help identify the boundaries of semantic units.","Next, we design an Attentive Semantic Fusion (ASF) layer to integrate the semantics of multiple subwords into a single vector: the semantic unit representation.","Lastly, the semantic-unit-level sentence representation is concatenated to the token-level one, and they are combined as the input of encoder.","Experimental results demonstrate that our method effectively models and leverages semantic-unit-level information and outperforms the strong baselines.","The code is available at https://github.com/ictnlp/SU4MT."],"url":"http://arxiv.org/abs/2310.11360v1"}
{"created":"2023-10-17 15:40:43","title":"Learning by Teaching: Key Challenges and Design Implications","abstract":"Benefits of learning by teaching (LbT) have been highlighted by previous studies from a pedagogical lens, as well as through computer-supported systems. However, the challenges that university students face in technology-mediated LbT$\\unicode{x2013}$whether it be teaching oneself, teaching a peer, or teaching an agent$\\unicode{x2013}$are not well understood. Furthermore, there is a gap in knowledge on the challenges that students encounter throughout the process of teaching (content selection, preparation, teaching, receiving and giving feedback, and reflection) despite its importance to the design of LbT platforms. Thus, we conducted a study with 24 university students where they taught content they had not fully grasped, without guidance, and participated in a semi-structured interview. Results demonstrate that participants encountered the following challenges: psychological barriers relating to self and others, and lack of know-how. Furthermore, we illuminate design implications required to overcome these challenges and benefit from LbT without requiring prior training in pedagogy.","sentences":["Benefits of learning by teaching (LbT) have been highlighted by previous studies from a pedagogical lens, as well as through computer-supported systems.","However, the challenges that university students face in technology-mediated LbT$\\unicode{x2013}$whether it be teaching oneself, teaching a peer, or teaching an agent$\\unicode{x2013}$are not well understood.","Furthermore, there is a gap in knowledge on the challenges that students encounter throughout the process of teaching (content selection, preparation, teaching, receiving and giving feedback, and reflection) despite its importance to the design of LbT platforms.","Thus, we conducted a study with 24 university students where they taught content they had not fully grasped, without guidance, and participated in a semi-structured interview.","Results demonstrate that participants encountered the following challenges: psychological barriers relating to self and others, and lack of know-how.","Furthermore, we illuminate design implications required to overcome these challenges and benefit from LbT without requiring prior training in pedagogy."],"url":"http://arxiv.org/abs/2310.11354v1"}
{"created":"2023-10-17 15:31:28","title":"Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing","abstract":"Detecting objects in 3D space using multiple cameras, known as Multi-Camera 3D Object Detection (MC3D-Det), has gained prominence with the advent of bird's-eye view (BEV) approaches. However, these methods often struggle when faced with unfamiliar testing environments due to the lack of diverse training data encompassing various viewpoints and environments. To address this, we propose a novel method that aligns 3D detection with 2D camera plane results, ensuring consistent and accurate detections. Our framework, anchored in perspective debiasing, helps the learning of features resilient to domain shifts. In our approach, we render diverse view maps from BEV features and rectify the perspective bias of these maps, leveraging implicit foreground volumes to bridge the camera and BEV planes. This two-step process promotes the learning of perspective- and context-independent features, crucial for accurate object detection across varying viewpoints, camera parameters and environment conditions. Notably, our model-agnostic approach preserves the original network structure without incurring additional inference costs, facilitating seamless integration across various models and simplifying deployment. Furthermore, we also show our approach achieves satisfactory results in real data when trained only with virtual datasets, eliminating the need for real scene annotations. Experimental results on both Domain Generalization (DG) and Unsupervised Domain Adaptation (UDA) clearly demonstrate its effectiveness. Our code will be released.","sentences":["Detecting objects in 3D space using multiple cameras, known as Multi-Camera 3D Object Detection (MC3D-Det), has gained prominence with the advent of bird's-eye view (BEV) approaches.","However, these methods often struggle when faced with unfamiliar testing environments due to the lack of diverse training data encompassing various viewpoints and environments.","To address this, we propose a novel method that aligns 3D detection with 2D camera plane results, ensuring consistent and accurate detections.","Our framework, anchored in perspective debiasing, helps the learning of features resilient to domain shifts.","In our approach, we render diverse view maps from BEV features and rectify the perspective bias of these maps, leveraging implicit foreground volumes to bridge the camera and BEV planes.","This two-step process promotes the learning of perspective- and context-independent features, crucial for accurate object detection across varying viewpoints, camera parameters and environment conditions.","Notably, our model-agnostic approach preserves the original network structure without incurring additional inference costs, facilitating seamless integration across various models and simplifying deployment.","Furthermore, we also show our approach achieves satisfactory results in real data when trained only with virtual datasets, eliminating the need for real scene annotations.","Experimental results on both Domain Generalization (DG) and Unsupervised Domain Adaptation (UDA) clearly demonstrate its effectiveness.","Our code will be released."],"url":"http://arxiv.org/abs/2310.11346v1"}
{"created":"2023-10-17 15:26:40","title":"The effect of stemming and lemmatization on Portuguese fake news text classification","abstract":"With the popularization of the internet, smartphones and social media, information is being spread quickly and easily way, which implies bigger traffic of information in the world, but there is a problem that is harming society with the dissemination of fake news. With a bigger flow of information, some people are trying to disseminate deceptive information and fake news. The automatic detection of fake news is a challenging task because to obtain a good result is necessary to deal with linguistics problems, especially when we are dealing with languages that not have been comprehensively studied yet, besides that, some techniques can help to reach a good result when we are dealing with text data, although, the motivation of detecting this deceptive information it is in the fact that the people need to know which information is true and trustful and which one is not. In this work, we present the effect the pre-processing methods such as lemmatization and stemming have on fake news classification, for that we designed some classifier models applying different pre-processing techniques. The results show that the pre-processing step is important to obtain betters results, the stemming and lemmatization techniques are interesting methods and need to be more studied to develop techniques focused on the Portuguese language so we can reach better results.","sentences":["With the popularization of the internet, smartphones and social media, information is being spread quickly and easily way, which implies bigger traffic of information in the world, but there is a problem that is harming society with the dissemination of fake news.","With a bigger flow of information, some people are trying to disseminate deceptive information and fake news.","The automatic detection of fake news is a challenging task because to obtain a good result is necessary to deal with linguistics problems, especially when we are dealing with languages that not have been comprehensively studied yet, besides that, some techniques can help to reach a good result when we are dealing with text data, although, the motivation of detecting this deceptive information it is in the fact that the people need to know which information is true and trustful and which one is not.","In this work, we present the effect the pre-processing methods such as lemmatization and stemming have on fake news classification, for that we designed some classifier models applying different pre-processing techniques.","The results show that the pre-processing step is important to obtain betters results, the stemming and lemmatization techniques are interesting methods and need to be more studied to develop techniques focused on the Portuguese language so we can reach better results."],"url":"http://arxiv.org/abs/2310.11344v1"}
{"created":"2023-10-17 15:24:02","title":"Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning","abstract":"Artificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data. However, the data in the real world is continuous and dynamic, and ANNs must adapt to novel scenarios while also retaining the learned knowledge to become lifelong learners. The ability of humans to excel at these tasks can be attributed to multiple factors ranging from cognitive computational structures, cognitive biases, and the multi-memory systems in the brain. We incorporate key concepts from each of these to design a novel framework, Dual Cognitive Architecture (DUCA), which includes multiple sub-systems, implicit and explicit knowledge representation dichotomy, inductive bias, and a multi-memory system. The inductive bias learner within DUCA is instrumental in encoding shape information, effectively countering the tendency of ANNs to learn local textures. Simultaneously, the inclusion of a semantic memory submodule facilitates the gradual consolidation of knowledge, replicating the dynamics observed in fast and slow learning systems, reminiscent of the principles underpinning the complementary learning system in human cognition. DUCA shows improvement across different settings and datasets, and it also exhibits reduced task recency bias, without the need for extra information. To further test the versatility of lifelong learning methods on a challenging distribution shift, we introduce a novel domain-incremental dataset DN4IL. In addition to improving performance on existing benchmarks, DUCA also demonstrates superior performance on this complex dataset.","sentences":["Artificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data.","However, the data in the real world is continuous and dynamic, and ANNs must adapt to novel scenarios while also retaining the learned knowledge to become lifelong learners.","The ability of humans to excel at these tasks can be attributed to multiple factors ranging from cognitive computational structures, cognitive biases, and the multi-memory systems in the brain.","We incorporate key concepts from each of these to design a novel framework, Dual Cognitive Architecture (DUCA), which includes multiple sub-systems, implicit and explicit knowledge representation dichotomy, inductive bias, and a multi-memory system.","The inductive bias learner within DUCA is instrumental in encoding shape information, effectively countering the tendency of ANNs to learn local textures.","Simultaneously, the inclusion of a semantic memory submodule facilitates the gradual consolidation of knowledge, replicating the dynamics observed in fast and slow learning systems, reminiscent of the principles underpinning the complementary learning system in human cognition.","DUCA shows improvement across different settings and datasets, and it also exhibits reduced task recency bias, without the need for extra information.","To further test the versatility of lifelong learning methods on a challenging distribution shift, we introduce a novel domain-incremental dataset DN4IL.","In addition to improving performance on existing benchmarks, DUCA also demonstrates superior performance on this complex dataset."],"url":"http://arxiv.org/abs/2310.11341v1"}
{"created":"2023-10-17 15:13:33","title":"Non-ergodicity in reinforcement learning: robustness via ergodicity transformations","abstract":"Envisioned application areas for reinforcement learning (RL) include autonomous driving, precision agriculture, and finance, which all require RL agents to make decisions in the real world. A significant challenge hindering the adoption of RL methods in these domains is the non-robustness of conventional algorithms. In this paper, we argue that a fundamental issue contributing to this lack of robustness lies in the focus on the expected value of the return as the sole \"correct\" optimization objective. The expected value is the average over the statistical ensemble of infinitely many trajectories. For non-ergodic returns, this average differs from the average over a single but infinitely long trajectory. Consequently, optimizing the expected value can lead to policies that yield exceptionally high returns with probability zero but almost surely result in catastrophic outcomes. This problem can be circumvented by transforming the time series of collected returns into one with ergodic increments. This transformation enables learning robust policies by optimizing the long-term return for individual agents rather than the average across infinitely many trajectories. We propose an algorithm for learning ergodicity transformations from data and demonstrate its effectiveness in an instructive, non-ergodic environment and on standard RL benchmarks.","sentences":["Envisioned application areas for reinforcement learning (RL) include autonomous driving, precision agriculture, and finance, which all require RL agents to make decisions in the real world.","A significant challenge hindering the adoption of RL methods in these domains is the non-robustness of conventional algorithms.","In this paper, we argue that a fundamental issue contributing to this lack of robustness lies in the focus on the expected value of the return as the sole \"correct\" optimization objective.","The expected value is the average over the statistical ensemble of infinitely many trajectories.","For non-ergodic returns, this average differs from the average over a single but infinitely long trajectory.","Consequently, optimizing the expected value can lead to policies that yield exceptionally high returns with probability zero but almost surely result in catastrophic outcomes.","This problem can be circumvented by transforming the time series of collected returns into one with ergodic increments.","This transformation enables learning robust policies by optimizing the long-term return for individual agents rather than the average across infinitely many trajectories.","We propose an algorithm for learning ergodicity transformations from data and demonstrate its effectiveness in an instructive, non-ergodic environment and on standard RL benchmarks."],"url":"http://arxiv.org/abs/2310.11335v1"}
{"created":"2023-10-17 15:12:56","title":"Agent-Specific Effects","abstract":"Establishing causal relationships between actions and outcomes is fundamental for accountable multi-agent decision-making. However, interpreting and quantifying agents' contributions to such relationships pose significant challenges. These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how the other agents respond to that action. In this paper, our objective is to present a systematic approach for attributing the causal effects of agents' actions to the influence they exert on other agents. Focusing on multi-agent Markov decision processes, we introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then turn to the counterfactual counterpart of ASE (cf-ASE), provide a sufficient set of conditions for identifying cf-ASE, and propose a practical sampling-based algorithm for estimating it. Finally, we experimentally evaluate the utility of cf-ASE through a simulation-based testbed, which includes a sepsis management environment.","sentences":["Establishing causal relationships between actions and outcomes is fundamental for accountable multi-agent decision-making.","However, interpreting and quantifying agents' contributions to such relationships pose significant challenges.","These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how the other agents respond to that action.","In this paper, our objective is to present a systematic approach for attributing the causal effects of agents' actions to the influence they exert on other agents.","Focusing on multi-agent Markov decision processes, we introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents.","We then turn to the counterfactual counterpart of ASE (cf-ASE), provide a sufficient set of conditions for identifying cf-ASE, and propose a practical sampling-based algorithm for estimating it.","Finally, we experimentally evaluate the utility of cf-ASE through a simulation-based testbed, which includes a sepsis management environment."],"url":"http://arxiv.org/abs/2310.11334v1"}
{"created":"2023-10-17 15:12:11","title":"Key Point-based Orientation Estimation of Strawberries for Robotic Fruit Picking","abstract":"Selective robotic harvesting is a promising technological solution to address labour shortages which are affecting modern agriculture in many parts of the world. For an accurate and efficient picking process, a robotic harvester requires the precise location and orientation of the fruit to effectively plan the trajectory of the end effector. The current methods for estimating fruit orientation employ either complete 3D information which typically requires registration from multiple views or rely on fully-supervised learning techniques, which require difficult-to-obtain manual annotation of the reference orientation. In this paper, we introduce a novel key-point-based fruit orientation estimation method allowing for the prediction of 3D orientation from 2D images directly. The proposed technique can work without full 3D orientation annotations but can also exploit such information for improved accuracy. We evaluate our work on two separate datasets of strawberry images obtained from real-world data collection scenarios. Our proposed method achieves state-of-the-art performance with an average error as low as $8^{\\circ}$, improving predictions by $\\sim30\\%$ compared to previous work presented in~\\cite{wagner2021efficient}. Furthermore, our method is suited for real-time robotic applications with fast inference times of $\\sim30$ms.","sentences":["Selective robotic harvesting is a promising technological solution to address labour shortages which are affecting modern agriculture in many parts of the world.","For an accurate and efficient picking process, a robotic harvester requires the precise location and orientation of the fruit to effectively plan the trajectory of the end effector.","The current methods for estimating fruit orientation employ either complete 3D information which typically requires registration from multiple views or rely on fully-supervised learning techniques, which require difficult-to-obtain manual annotation of the reference orientation.","In this paper, we introduce a novel key-point-based fruit orientation estimation method allowing for the prediction of 3D orientation from 2D images directly.","The proposed technique can work without full 3D orientation annotations but can also exploit such information for improved accuracy.","We evaluate our work on two separate datasets of strawberry images obtained from real-world data collection scenarios.","Our proposed method achieves state-of-the-art performance with an average error as low as $8^{\\circ}$, improving predictions by $\\sim30\\%$ compared to previous work presented in~\\cite{wagner2021efficient}.","Furthermore, our method is suited for real-time robotic applications with fast inference times of $\\sim30$ms."],"url":"http://arxiv.org/abs/2310.11333v1"}
{"created":"2023-10-17 15:12:05","title":"Discovering High-Quality Process Models Despite Data Scarcity","abstract":"Process discovery algorithms learn process models from executed activity sequences, describing concurrency, causality, and conflict. Concurrent activities require observing multiple permutations, increasing data requirements, especially for processes with concurrent subprocesses such as hierarchical, composite, or distributed processes. While process discovery algorithms traditionally use sequences of activities as input, recently introduced object-centric process discovery algorithms can use graphs of activities as input, encoding partial orders between activities. As such, they contain the concurrency information of many sequences in a single graph. In this paper, we address the research question of reducing process discovery data requirements when using object-centric event logs for process discovery. We classify different real-life processes according to the control-flow complexity within and between subprocesses and introduce an evaluation framework to assess process discovery algorithm quality of traditional and object-centric process discovery based on the sample size. We complement this with a large-scale production process case study. Our results show reduced data requirements, enabling the discovery of large, concurrent processes such as manufacturing with little data, previously infeasible with traditional process discovery. Our findings suggest that object-centric process mining could revolutionize process discovery in various sectors, including manufacturing and supply chains.","sentences":["Process discovery algorithms learn process models from executed activity sequences, describing concurrency, causality, and conflict.","Concurrent activities require observing multiple permutations, increasing data requirements, especially for processes with concurrent subprocesses such as hierarchical, composite, or distributed processes.","While process discovery algorithms traditionally use sequences of activities as input, recently introduced object-centric process discovery algorithms can use graphs of activities as input, encoding partial orders between activities.","As such, they contain the concurrency information of many sequences in a single graph.","In this paper, we address the research question of reducing process discovery data requirements when using object-centric event logs for process discovery.","We classify different real-life processes according to the control-flow complexity within and between subprocesses and introduce an evaluation framework to assess process discovery algorithm quality of traditional and object-centric process discovery based on the sample size.","We complement this with a large-scale production process case study.","Our results show reduced data requirements, enabling the discovery of large, concurrent processes such as manufacturing with little data, previously infeasible with traditional process discovery.","Our findings suggest that object-centric process mining could revolutionize process discovery in various sectors, including manufacturing and supply chains."],"url":"http://arxiv.org/abs/2310.11332v1"}
{"created":"2023-10-17 15:11:14","title":"Streamlining Sleepy Consensus: Total-Order Broadcast with Single-Vote Decisions in the Sleepy Model","abstract":"Over the past years, distributed consensus research has shifted its focus towards addressing challenges in large-scale, permissionless systems, such as blockchains. This shift is characterized by the need to accommodate dynamic participation, contrasting the traditional approach of a static set of continuously online participants. Works like Bitcoin and the Sleepy Model have set the stage for this developing framework.   Notable contributions from Momose and Ren (CCS 2022) and subsequent works have introduced Total-Order Broadcast protocols leveraging Graded Agreement primitives and supporting dynamic participation, though often requiring multiple rounds of voting per decision -- a potential bottleneck for real-world large-scale systems.   Addressing this, our paper presents a novel Total-Order Broadcast protocol in the Sleepy Model resilient to up to 1/2 adversarial participants, requiring just a single round of voting per decision. This work paves the way to more practical Total-Order Broadcast protocols to be implemented in real-world systems where a large number of participants are involved simultaneously and their participation level might fluctuate over time.","sentences":["Over the past years, distributed consensus research has shifted its focus towards addressing challenges in large-scale, permissionless systems, such as blockchains.","This shift is characterized by the need to accommodate dynamic participation, contrasting the traditional approach of a static set of continuously online participants.","Works like Bitcoin and the Sleepy Model have set the stage for this developing framework.   ","Notable contributions from Momose and Ren (CCS 2022) and subsequent works have introduced Total-Order Broadcast protocols leveraging Graded Agreement primitives and supporting dynamic participation, though often requiring multiple rounds of voting per decision -- a potential bottleneck for real-world large-scale systems.   ","Addressing this, our paper presents a novel Total-Order Broadcast protocol in the Sleepy Model resilient to up to 1/2 adversarial participants, requiring just a single round of voting per decision.","This work paves the way to more practical Total-Order Broadcast protocols to be implemented in real-world systems where a large number of participants are involved simultaneously and their participation level might fluctuate over time."],"url":"http://arxiv.org/abs/2310.11331v1"}
{"created":"2023-10-17 15:04:40","title":"Integrated Sensing and Channel Estimation by Exploiting Dual Timescales for Delay-Doppler Alignment Modulation","abstract":"For integrated sensing and communication (ISAC) systems, the channel information essential for communication and sensing tasks fluctuates across different timescales. Specifically, wireless sensing primarily focuses on acquiring path state information (PSI) (e.g., delay, angle, and Doppler) of individual multi-path components to sense the environment, which usually evolves much more slowly than the composite channel state information (CSI) required for communications. Typically, the CSI is approximately unchanged during the channel coherence time, which characterizes the statistical properties of wireless communication channels. However, this concept is less appropriate for describing that for wireless sensing. To this end, in this paper, we introduce a new timescale to study the variation of the PSI from a channel geometric perspective, termed path invariant time, during which the PSI largely remains constant. Our analysis indicates that the path invariant time considerably exceeds the channel coherence time. Thus, capitalizing on these dual timescales of the wireless channel, in this paper, we propose a novel ISAC framework exploiting the recently proposed delay-Doppler alignment modulation (DDAM) technique. Different from most existing studies on DDAM that assume the availability of perfect PSI, in this work, we propose a novel algorithm, termed as adaptive simultaneously orthogonal matching pursuit with support refinement (ASOMP-SR), for joint environment sensing and PSI estimation. We also analyze the performance of DDAM with imperfectly sensed PSI.Simulation results unveil that the proposed DDAM-based ISAC can achieve superior spectral efficiency and a reduced peak-to-average power ratio (PAPR) compared to standard orthogonal frequency division multiplexing (OFDM).","sentences":["For integrated sensing and communication (ISAC) systems, the channel information essential for communication and sensing tasks fluctuates across different timescales.","Specifically, wireless sensing primarily focuses on acquiring path state information (PSI) (e.g., delay, angle, and Doppler) of individual multi-path components to sense the environment, which usually evolves much more slowly than the composite channel state information (CSI) required for communications.","Typically, the CSI is approximately unchanged during the channel coherence time, which characterizes the statistical properties of wireless communication channels.","However, this concept is less appropriate for describing that for wireless sensing.","To this end, in this paper, we introduce a new timescale to study the variation of the PSI from a channel geometric perspective, termed path invariant time, during which the PSI largely remains constant.","Our analysis indicates that the path invariant time considerably exceeds the channel coherence time.","Thus, capitalizing on these dual timescales of the wireless channel, in this paper, we propose a novel ISAC framework exploiting the recently proposed delay-Doppler alignment modulation (DDAM) technique.","Different from most existing studies on DDAM that assume the availability of perfect PSI, in this work, we propose a novel algorithm, termed as adaptive simultaneously orthogonal matching pursuit with support refinement (ASOMP-SR), for joint environment sensing and PSI estimation.","We also analyze the performance of DDAM with imperfectly sensed PSI.Simulation results unveil that the proposed DDAM-based ISAC can achieve superior spectral efficiency and a reduced peak-to-average power ratio (PAPR) compared to standard orthogonal frequency division multiplexing (OFDM)."],"url":"http://arxiv.org/abs/2310.11326v1"}
{"created":"2023-10-17 15:03:37","title":"Detection of Malicious DNS-over-HTTPS Traffic: An Anomaly Detection Approach using Autoencoders","abstract":"To maintain the privacy of users' web browsing history, popular browsers encrypt their DNS traffic using the DNS-over-HTTPS (DoH) protocol. Unfortunately, encrypting DNS packets prevents many existing intrusion detection systems from using plaintext domain names to detect malicious traffic. In this paper, we design an autoencoder that is capable of detecting malicious DNS traffic by only observing the encrypted DoH traffic. Compared to previous works, the proposed autoencoder looks for anomalies in DoH traffic, and thus can detect malicious traffic that has not been previously observed, i.e., zero-day attacks. We run extensive experiments to evaluate the performance of our proposed autoencoder and compare it to that of other anomaly detection algorithms, namely, local outlier factor, one-class support vector machine, isolation forest, and variational autoencoders. We find that our proposed autoencoder achieves the highest detection performance, with a median F-1 score of 99\\% over several types of malicious traffic.","sentences":["To maintain the privacy of users' web browsing history, popular browsers encrypt their DNS traffic using the DNS-over-HTTPS (DoH) protocol.","Unfortunately, encrypting DNS packets prevents many existing intrusion detection systems from using plaintext domain names to detect malicious traffic.","In this paper, we design an autoencoder that is capable of detecting malicious DNS traffic by only observing the encrypted DoH traffic.","Compared to previous works, the proposed autoencoder looks for anomalies in DoH traffic, and thus can detect malicious traffic that has not been previously observed, i.e., zero-day attacks.","We run extensive experiments to evaluate the performance of our proposed autoencoder and compare it to that of other anomaly detection algorithms, namely, local outlier factor, one-class support vector machine, isolation forest, and variational autoencoders.","We find that our proposed autoencoder achieves the highest detection performance, with a median F-1 score of 99\\% over several types of malicious traffic."],"url":"http://arxiv.org/abs/2310.11325v1"}
{"created":"2023-10-17 15:03:30","title":"Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting","abstract":"As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.","sentences":["As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance.","Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model.","In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting.","We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning.","Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format.","We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format.","To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights.","Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats."],"url":"http://arxiv.org/abs/2310.11324v1"}
{"created":"2023-10-17 14:52:33","title":"Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue","abstract":"In support of open and reproducible research, there has been a rapidly increasing number of datasets made available for research. As the availability of datasets increases, it becomes more important to have quality metadata for discovering and reusing them. Yet, it is a common issue that datasets often lack quality metadata due to limited resources for data curation. Meanwhile, technologies such as artificial intelligence and large language models (LLMs) are progressing rapidly. Recently, systems based on these technologies, such as ChatGPT, have demonstrated promising capabilities for certain data curation tasks. This paper proposes to leverage LLMs for cost-effective annotation of subject metadata through the LLM-based in-context learning. Our method employs GPT-3.5 with prompts designed for annotating subject metadata, demonstrating promising performance in automatic metadata annotation. However, models based on in-context learning cannot acquire discipline-specific rules, resulting in lower performance in several categories. This limitation arises from the limited contextual information available for subject inference. To the best of our knowledge, we are introducing, for the first time, an in-context learning method that harnesses large language models for automated subject metadata annotation.","sentences":["In support of open and reproducible research, there has been a rapidly increasing number of datasets made available for research.","As the availability of datasets increases, it becomes more important to have quality metadata for discovering and reusing them.","Yet, it is a common issue that datasets often lack quality metadata due to limited resources for data curation.","Meanwhile, technologies such as artificial intelligence and large language models (LLMs) are progressing rapidly.","Recently, systems based on these technologies, such as ChatGPT, have demonstrated promising capabilities for certain data curation tasks.","This paper proposes to leverage LLMs for cost-effective annotation of subject metadata through the LLM-based in-context learning.","Our method employs GPT-3.5 with prompts designed for annotating subject metadata, demonstrating promising performance in automatic metadata annotation.","However, models based on in-context learning cannot acquire discipline-specific rules, resulting in lower performance in several categories.","This limitation arises from the limited contextual information available for subject inference.","To the best of our knowledge, we are introducing, for the first time, an in-context learning method that harnesses large language models for automated subject metadata annotation."],"url":"http://arxiv.org/abs/2310.11318v1"}
{"created":"2023-10-17 14:48:02","title":"MonoSKD: General Distillation Framework for Monocular 3D Object Detection via Spearman Correlation Coefficient","abstract":"Monocular 3D object detection is an inherently ill-posed problem, as it is challenging to predict accurate 3D localization from a single image. Existing monocular 3D detection knowledge distillation methods usually project the LiDAR onto the image plane and train the teacher network accordingly. Transferring LiDAR-based model knowledge to RGB-based models is more complex, so a general distillation strategy is needed. To alleviate cross-modal prob-lem, we propose MonoSKD, a novel Knowledge Distillation framework for Monocular 3D detection based on Spearman correlation coefficient, to learn the relative correlation between cross-modal features. Considering the large gap between these features, strict alignment of features may mislead the training, so we propose a looser Spearman loss. Furthermore, by selecting appropriate distillation locations and removing redundant modules, our scheme saves more GPU resources and trains faster than existing methods. Extensive experiments are performed to verify the effectiveness of our framework on the challenging KITTI 3D object detection benchmark. Our method achieves state-of-the-art performance until submission with no additional inference computational cost. Our codes are available at https://github.com/Senwang98/MonoSKD","sentences":["Monocular 3D object detection is an inherently ill-posed problem, as it is challenging to predict accurate 3D localization from a single image.","Existing monocular 3D detection knowledge distillation methods usually project the LiDAR onto the image plane and train the teacher network accordingly.","Transferring LiDAR-based model knowledge to RGB-based models is more complex, so a general distillation strategy is needed.","To alleviate cross-modal prob-lem, we propose MonoSKD, a novel Knowledge Distillation framework for Monocular 3D detection based on Spearman correlation coefficient, to learn the relative correlation between cross-modal features.","Considering the large gap between these features, strict alignment of features may mislead the training, so we propose a looser Spearman loss.","Furthermore, by selecting appropriate distillation locations and removing redundant modules, our scheme saves more GPU resources and trains faster than existing methods.","Extensive experiments are performed to verify the effectiveness of our framework on the challenging KITTI 3D object detection benchmark.","Our method achieves state-of-the-art performance until submission with no additional inference computational cost.","Our codes are available at https://github.com/Senwang98/MonoSKD"],"url":"http://arxiv.org/abs/2310.11316v1"}
{"created":"2023-10-17 14:34:58","title":"Elucidating The Design Space of Classifier-Guided Diffusion Generation","abstract":"Guidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions. On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance. In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds. Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation. Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost. With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks. The code is available at https://github.com/AlexMaOLS/EluCD/tree/main.","sentences":["Guidance in conditional diffusion generation is of great importance for sample quality and controllability.","However, existing guidance schemes are to be desired.","On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions.","On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance.","In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds.","Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation.","Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost.","With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks.","The code is available at https://github.com/AlexMaOLS/EluCD/tree/main."],"url":"http://arxiv.org/abs/2310.11311v1"}
{"created":"2023-10-17 14:34:18","title":"Intelligent Surfaces Aided High-Mobility Communications: Opportunities and Design Issues","abstract":"Intelligent reflecting/refracting surface (IRS) is envisioned as a promising technology to reconfigure wireless propagation environment for enhancing the communication performance, by smartly controlling the signal reflection/refraction with a large number of tunable passive elements. In particular, the application of IRS in high-mobility scenarios can convert wireless channels from fast fading to slow fading, thus achieving more reliable communications. In this paper, we first provide an overview of the new applications and opportunities of IRS in high-mobility communications. Next, we present two practical strategies for deploying IRS to aid high-mobility communications, namely, roadside IRS versus vehicle-side IRS, and compare their different channel characteristics, handover requirements, and deployment costs. Then, the main issues in designing IRS-aided high-mobility communications, including node discovery, mode switching, beam alignment/tracking, handover, and multiuser scheduling are discussed for both IRS deployment strategies. Moreover, numerical results are presented to demonstrate the potential performance gains of IRSs in vehicular communications. Finally, new research directions are pointed out for future work.","sentences":["Intelligent reflecting/refracting surface (IRS) is envisioned as a promising technology to reconfigure wireless propagation environment for enhancing the communication performance, by smartly controlling the signal reflection/refraction with a large number of tunable passive elements.","In particular, the application of IRS in high-mobility scenarios can convert wireless channels from fast fading to slow fading, thus achieving more reliable communications.","In this paper, we first provide an overview of the new applications and opportunities of IRS in high-mobility communications.","Next, we present two practical strategies for deploying IRS to aid high-mobility communications, namely, roadside IRS versus vehicle-side IRS, and compare their different channel characteristics, handover requirements, and deployment costs.","Then, the main issues in designing IRS-aided high-mobility communications, including node discovery, mode switching, beam alignment/tracking, handover, and multiuser scheduling are discussed for both IRS deployment strategies.","Moreover, numerical results are presented to demonstrate the potential performance gains of IRSs in vehicular communications.","Finally, new research directions are pointed out for future work."],"url":"http://arxiv.org/abs/2310.11309v1"}
{"created":"2023-10-17 14:32:49","title":"Multi Self-supervised Pre-fine-tuned Transformer Fusion for Better Intelligent Transportation Detection","abstract":"Intelligent transportation system combines advanced information technology to provide intelligent services such as monitoring, detection, and early warning for modern transportation. Intelligent transportation detection is the cornerstone of many intelligent traffic services by identifying task targets through object detection methods. However existing detection methods in intelligent transportation are limited by two aspects. First, there is a difference between the model knowledge pre-trained on large-scale datasets and the knowledge required for target task. Second, most detection models follow the pattern of single-source learning, which limits the learning ability. To address these problems, we propose a Multi Self-supervised Pre-fine-tuned Transformer Fusion (MSPTF) network, consisting of two steps: unsupervised pre-fine-tune domain knowledge learning and multi-model fusion target task learning. In the first step, we introduced self-supervised learning methods into transformer model pre-fine-tune which could reduce data costs and alleviate the knowledge gap between pre-trained model and target task. In the second step, we take feature information differences between different model architectures and different pre-fine-tune tasks into account and propose Multi-model Semantic Consistency Cross-attention Fusion (MSCCF) network to combine different transformer model features by considering channel semantic consistency and feature vector semantic consistency, which obtain more complete and proper fusion features for detection task. We experimented the proposed method on vehicle recognition dataset and road disease detection dataset and achieved 1.1%, 5.5%, 4.2% improvement compared with baseline and 0.7%, 1.8%, 1.7% compared with sota, which proved the effectiveness of our method.","sentences":["Intelligent transportation system combines advanced information technology to provide intelligent services such as monitoring, detection, and early warning for modern transportation.","Intelligent transportation detection is the cornerstone of many intelligent traffic services by identifying task targets through object detection methods.","However existing detection methods in intelligent transportation are limited by two aspects.","First, there is a difference between the model knowledge pre-trained on large-scale datasets and the knowledge required for target task.","Second, most detection models follow the pattern of single-source learning, which limits the learning ability.","To address these problems, we propose a Multi Self-supervised Pre-fine-tuned Transformer Fusion (MSPTF) network, consisting of two steps: unsupervised pre-fine-tune domain knowledge learning and multi-model fusion target task learning.","In the first step, we introduced self-supervised learning methods into transformer model pre-fine-tune which could reduce data costs and alleviate the knowledge gap between pre-trained model and target task.","In the second step, we take feature information differences between different model architectures and different pre-fine-tune tasks into account and propose Multi-model Semantic Consistency Cross-attention Fusion (MSCCF) network to combine different transformer model features by considering channel semantic consistency and feature vector semantic consistency, which obtain more complete and proper fusion features for detection task.","We experimented the proposed method on vehicle recognition dataset and road disease detection dataset and achieved 1.1%, 5.5%, 4.2% improvement compared with baseline and 0.7%, 1.8%, 1.7% compared with sota, which proved the effectiveness of our method."],"url":"http://arxiv.org/abs/2310.11307v1"}
{"created":"2023-10-17 14:29:25","title":"MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and Atari Games","abstract":"This paper presents MiniZero, a zero-knowledge learning framework that supports four state-of-the-art algorithms, including AlphaZero, MuZero, Gumbel AlphaZero, and Gumbel MuZero. While these algorithms have demonstrated super-human performance in many games, it remains unclear which among them is most suitable or efficient for specific tasks. Through MiniZero, we systematically evaluate the performance of each algorithm in two board games, 9x9 Go and 8x8 Othello, as well as 57 Atari games. Our empirical findings are summarized as follows. For two board games, using more simulations generally results in higher performance. However, the choice of AlphaZero and MuZero may differ based on game properties. For Atari games, both MuZero and Gumbel MuZero are worth considering. Since each game has unique characteristics, different algorithms and simulations yield varying results. In addition, we introduce an approach, called progressive simulation, which progressively increases the simulation budget during training to allocate computation more efficiently. Our empirical results demonstrate that progressive simulation achieves significantly superior performance in two board games. By making our framework and trained models publicly available, this paper contributes a benchmark for future research on zero-knowledge learning algorithms, assisting researchers in algorithm selection and comparison against these zero-knowledge learning baselines.","sentences":["This paper presents MiniZero, a zero-knowledge learning framework that supports four state-of-the-art algorithms, including AlphaZero, MuZero, Gumbel AlphaZero, and Gumbel MuZero.","While these algorithms have demonstrated super-human performance in many games, it remains unclear which among them is most suitable or efficient for specific tasks.","Through MiniZero, we systematically evaluate the performance of each algorithm in two board games, 9x9 Go and 8x8 Othello, as well as 57 Atari games.","Our empirical findings are summarized as follows.","For two board games, using more simulations generally results in higher performance.","However, the choice of AlphaZero and MuZero may differ based on game properties.","For Atari games, both MuZero and Gumbel MuZero are worth considering.","Since each game has unique characteristics, different algorithms and simulations yield varying results.","In addition, we introduce an approach, called progressive simulation, which progressively increases the simulation budget during training to allocate computation more efficiently.","Our empirical results demonstrate that progressive simulation achieves significantly superior performance in two board games.","By making our framework and trained models publicly available, this paper contributes a benchmark for future research on zero-knowledge learning algorithms, assisting researchers in algorithm selection and comparison against these zero-knowledge learning baselines."],"url":"http://arxiv.org/abs/2310.11305v1"}
{"created":"2023-10-17 14:27:34","title":"QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering","abstract":"Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks. State-of-the-art approaches fine-tune language models on QA pairs constructed from CommonSense Knowledge Bases (CSKBs) to equip the models with more commonsense knowledge in a QA context. However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model's ability to generalize. To address these issues, we propose QADYNAMICS, a training dynamics-driven framework for QA diagnostics and refinement. Our approach analyzes the training dynamics of each QA pair at both the question level and option level, discarding machine-detectable artifacts by removing uninformative QA pairs and mislabeled or false-negative options. Extensive experiments demonstrate the effectiveness of our approach, which outperforms all baselines while using only 33% of the synthetic data, even including LLMs such as ChatGPT. Moreover, expert evaluations confirm that our framework significantly improves the quality of QA synthesis. Our codes and model checkpoints are available at https://github.com/HKUST-KnowComp/QaDynamics.","sentences":["Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks.","State-of-the-art approaches fine-tune language models on QA pairs constructed from CommonSense Knowledge Bases (CSKBs) to equip the models with more commonsense knowledge in a QA context.","However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model's ability to generalize.","To address these issues, we propose QADYNAMICS, a training dynamics-driven framework for QA diagnostics and refinement.","Our approach analyzes the training dynamics of each QA pair at both the question level and option level, discarding machine-detectable artifacts by removing uninformative QA pairs and mislabeled or false-negative options.","Extensive experiments demonstrate the effectiveness of our approach, which outperforms all baselines while using only 33% of the synthetic data, even including LLMs such as ChatGPT.","Moreover, expert evaluations confirm that our framework significantly improves the quality of QA synthesis.","Our codes and model checkpoints are available at https://github.com/HKUST-KnowComp/QaDynamics."],"url":"http://arxiv.org/abs/2310.11303v1"}
{"created":"2023-10-17 14:23:46","title":"Source Code Comprehension: A Contemporary Definition and Conceptual Model for Empirical Investigation","abstract":"Be it in debugging, testing, code review or, more recently, pair programming with AI assistance: in all these activities, software engineers need to understand source code. Accordingly, plenty of research is taking place in the field to find out, for example, what makes code easy to understand and which tools can best support developers in their comprehension process. And while any code comprehension researcher certainly has a rough idea of what they mean when they mention a developer having a good understanding of a piece of code, to date, the research community has not managed to define source code comprehension as a concept. Instead, in primary research on code comprehension, an implicit definition by task prevails, i.e., code comprehension is what the experimental tasks measure. This approach has two negative consequences. First, it makes it difficult to conduct secondary research. Currently, each code comprehension primary study uses different comprehension tasks and measures, and thus it is not clear whether different studies intend to measure the same construct. Second, authors of a primary study run into the difficulty of justifying their design decisions without a definition of what they attempt to measure. An operationalization of an insufficiently described construct occurs, which poses a threat to construct validity.   The task of defining code comprehension considering the theory of the past fifty years is not an easy one. Nor is it a task that every author of a primary study must accomplish on their own. Therefore, this paper constitutes a reference work that defines source code comprehension and presents a conceptual framework in which researchers can anchor their empirical code comprehension research.","sentences":["Be it in debugging, testing, code review or, more recently, pair programming with AI assistance: in all these activities, software engineers need to understand source code.","Accordingly, plenty of research is taking place in the field to find out, for example, what makes code easy to understand and which tools can best support developers in their comprehension process.","And while any code comprehension researcher certainly has a rough idea of what they mean when they mention a developer having a good understanding of a piece of code, to date, the research community has not managed to define source code comprehension as a concept.","Instead, in primary research on code comprehension, an implicit definition by task prevails, i.e., code comprehension is what the experimental tasks measure.","This approach has two negative consequences.","First, it makes it difficult to conduct secondary research.","Currently, each code comprehension primary study uses different comprehension tasks and measures, and thus it is not clear whether different studies intend to measure the same construct.","Second, authors of a primary study run into the difficulty of justifying their design decisions without a definition of what they attempt to measure.","An operationalization of an insufficiently described construct occurs, which poses a threat to construct validity.   ","The task of defining code comprehension considering the theory of the past fifty years is not an easy one.","Nor is it a task that every author of a primary study must accomplish on their own.","Therefore, this paper constitutes a reference work that defines source code comprehension and presents a conceptual framework in which researchers can anchor their empirical code comprehension research."],"url":"http://arxiv.org/abs/2310.11301v1"}
{"created":"2023-10-17 14:16:42","title":"CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation","abstract":"Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing research interest. During speaking activities, the mouth displays strong motions, while the other facial regions typically demonstrate comparatively weak activity levels. Existing approaches often simplify the process by directly mapping single-level speech features to the entire facial animation, which overlook the differences in facial activity intensity leading to overly smoothed facial movements. In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal correlation between hierarchical speech features and facial activities of different intensities across distinct regions. A novel facial activity intensity metric is defined to distinguish between strong and weak facial activity, obtained by computing the short-time Fourier transform of facial vertex displacements. Based on the variances in facial activity, we propose a dual-branch decoding framework to synchronously synthesize strong and weak facial activity, which guarantees wider intensity facial animation synthesis. Furthermore, a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions. Extensive qualitatively and quantitatively experiments as well as a user study indicate that our CorrTalk outperforms existing state-of-the-art methods. The source code and supplementary video are publicly available at: https://zjchu.github.io/projects/CorrTalk/","sentences":["Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing research interest.","During speaking activities, the mouth displays strong motions, while the other facial regions typically demonstrate comparatively weak activity levels.","Existing approaches often simplify the process by directly mapping single-level speech features to the entire facial animation, which overlook the differences in facial activity intensity leading to overly smoothed facial movements.","In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal correlation between hierarchical speech features and facial activities of different intensities across distinct regions.","A novel facial activity intensity metric is defined to distinguish between strong and weak facial activity, obtained by computing the short-time Fourier transform of facial vertex displacements.","Based on the variances in facial activity, we propose a dual-branch decoding framework to synchronously synthesize strong and weak facial activity, which guarantees wider intensity facial animation synthesis.","Furthermore, a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions.","Extensive qualitatively and quantitatively experiments as well as a user study indicate that our CorrTalk outperforms existing state-of-the-art methods.","The source code and supplementary video are publicly available at: https://zjchu.github.io/projects/CorrTalk/"],"url":"http://arxiv.org/abs/2310.11295v1"}
{"created":"2023-10-17 14:16:28","title":"Fair Reward Distribution in Federated Byzantine Agreement Systems","abstract":"Federated Byzantine Agreement Systems (FBASs) offer a solution to consensus in permissionless systems by adapting the well-studied Byzantine agreement model to permissionless consensus. Unlike its counterparts in the context of permissionless consensus, the FBAS system model does not offer validating nodes protocol-level incentives although they are entrusted with safeguarding and ensuring the functionality of the system. Multiple studies have reported on the small number of active validators in these systems leading to some concerns about their resilience. To this end, this paper studies how rewards can be distributed in FBASs and presents a fair reward distribution function for FBASs. The challenge is that, on the one hand, consensus in an FBAS is found jointly between all nodes and, on the other hand, nodes do not all contribute equally to this process. We draw on game-theoretic methods to quantify these contributions bearing the overall health of the FBAS in mind and present a fair reward distribution function which we evaluate based on a set of identified properties.","sentences":["Federated Byzantine Agreement Systems (FBASs) offer a solution to consensus in permissionless systems by adapting the well-studied Byzantine agreement model to permissionless consensus.","Unlike its counterparts in the context of permissionless consensus, the FBAS system model does not offer validating nodes protocol-level incentives although they are entrusted with safeguarding and ensuring the functionality of the system.","Multiple studies have reported on the small number of active validators in these systems leading to some concerns about their resilience.","To this end, this paper studies how rewards can be distributed in FBASs and presents a fair reward distribution function for FBASs.","The challenge is that, on the one hand, consensus in an FBAS is found jointly between all nodes and, on the other hand, nodes do not all contribute equally to this process.","We draw on game-theoretic methods to quantify these contributions bearing the overall health of the FBAS in mind and present a fair reward distribution function which we evaluate based on a set of identified properties."],"url":"http://arxiv.org/abs/2310.11294v1"}
{"created":"2023-10-17 14:15:57","title":"An Automatic Learning Rate Schedule Algorithm for Achieving Faster Convergence and Steeper Descent","abstract":"The delta-bar-delta algorithm is recognized as a learning rate adaptation technique that enhances the convergence speed of the training process in optimization by dynamically scheduling the learning rate based on the difference between the current and previous weight updates. While this algorithm has demonstrated strong competitiveness in full data optimization when compared to other state-of-the-art algorithms like Adam and SGD, it may encounter convergence issues in mini-batch optimization scenarios due to the presence of noisy gradients.   In this study, we thoroughly investigate the convergence behavior of the delta-bar-delta algorithm in real-world neural network optimization. To address any potential convergence challenges, we propose a novel approach called RDBD (Regrettable Delta-Bar-Delta). Our approach allows for prompt correction of biased learning rate adjustments and ensures the convergence of the optimization process. Furthermore, we demonstrate that RDBD can be seamlessly integrated with any optimization algorithm and significantly improve the convergence speed.   By conducting extensive experiments and evaluations, we validate the effectiveness and efficiency of our proposed RDBD approach. The results showcase its capability to overcome convergence issues in mini-batch optimization and its potential to enhance the convergence speed of various optimization algorithms. This research contributes to the advancement of optimization techniques in neural network training, providing practitioners with a reliable automatic learning rate scheduler for achieving faster convergence and improved optimization outcomes.","sentences":["The delta-bar-delta algorithm is recognized as a learning rate adaptation technique that enhances the convergence speed of the training process in optimization by dynamically scheduling the learning rate based on the difference between the current and previous weight updates.","While this algorithm has demonstrated strong competitiveness in full data optimization when compared to other state-of-the-art algorithms like Adam and SGD, it may encounter convergence issues in mini-batch optimization scenarios due to the presence of noisy gradients.   ","In this study, we thoroughly investigate the convergence behavior of the delta-bar-delta algorithm in real-world neural network optimization.","To address any potential convergence challenges, we propose a novel approach called RDBD (Regrettable Delta-Bar-Delta).","Our approach allows for prompt correction of biased learning rate adjustments and ensures the convergence of the optimization process.","Furthermore, we demonstrate that RDBD can be seamlessly integrated with any optimization algorithm and significantly improve the convergence speed.   ","By conducting extensive experiments and evaluations, we validate the effectiveness and efficiency of our proposed RDBD approach.","The results showcase its capability to overcome convergence issues in mini-batch optimization and its potential to enhance the convergence speed of various optimization algorithms.","This research contributes to the advancement of optimization techniques in neural network training, providing practitioners with a reliable automatic learning rate scheduler for achieving faster convergence and improved optimization outcomes."],"url":"http://arxiv.org/abs/2310.11291v1"}
{"created":"2023-10-17 14:14:32","title":"Signal Temporal Logic-Guided Model Predictive Control for Robust Bipedal Locomotion Resilient to Runtime External Perturbations","abstract":"This study investigates formal-method-based trajectory optimization (TO) for bipedal locomotion, focusing on scenarios where the robot encounters external perturbations at unforeseen times. Our key research question centers around the assurance of task specification correctness and the maximization of specification robustness for a bipedal robot in the presence of external perturbations.   Our contribution includes the design of an optimization-based task and motion planning framework that generates optimal control sequences with formal guarantees of external perturbation recovery. As a core component of the framework, a model predictive controller (MPC) encodes signal temporal logic (STL)-based task specifications as a cost function. In particular, we investigate challenging scenarios where the robot is subjected to lateral perturbations that increase the risk of failure due to leg self-collision. To address this, we synthesize agile and safe crossed-leg maneuvers to enhance locomotion stability.   This work marks the first study to incorporate formal guarantees offered by STL into a TO for perturbation recovery of bipedal locomotion. We demonstrate the efficacy of the framework via perturbation experiments in simulations.","sentences":["This study investigates formal-method-based trajectory optimization (TO) for bipedal locomotion, focusing on scenarios where the robot encounters external perturbations at unforeseen times.","Our key research question centers around the assurance of task specification correctness and the maximization of specification robustness for a bipedal robot in the presence of external perturbations.   ","Our contribution includes the design of an optimization-based task and motion planning framework that generates optimal control sequences with formal guarantees of external perturbation recovery.","As a core component of the framework, a model predictive controller (MPC) encodes signal temporal logic (STL)-based task specifications as a cost function.","In particular, we investigate challenging scenarios where the robot is subjected to lateral perturbations that increase the risk of failure due to leg self-collision.","To address this, we synthesize agile and safe crossed-leg maneuvers to enhance locomotion stability.   ","This work marks the first study to incorporate formal guarantees offered by STL into a TO for perturbation recovery of bipedal locomotion.","We demonstrate the efficacy of the framework via perturbation experiments in simulations."],"url":"http://arxiv.org/abs/2310.11290v1"}
{"created":"2023-10-17 14:12:39","title":"Enriching Diagrams with Algebraic Operations","abstract":"In this paper, we extend diagrammatic reasoning in monoidal categories with algebraic operations and equations. We achieve this by considering monoidal categories that are enriched in the category of Eilenberg-Moore algebras for a monad. Under the condition that this monad is monoidal and affine, we construct an adjunction between symmetric monoidal categories and symmetric monoidal categories enriched over algebras for the monad. This allows us to devise an extension, and its semantics, of the ZX-calculus with probabilistic choices by freely enriching over convex algebras, which are the algebras of the finite distribution monad. We show how this construction can be used for diagrammatic reasoning of noise in quantum systems.","sentences":["In this paper, we extend diagrammatic reasoning in monoidal categories with algebraic operations and equations.","We achieve this by considering monoidal categories that are enriched in the category of Eilenberg-Moore algebras for a monad.","Under the condition that this monad is monoidal and affine, we construct an adjunction between symmetric monoidal categories and symmetric monoidal categories enriched over algebras for the monad.","This allows us to devise an extension, and its semantics, of the ZX-calculus with probabilistic choices by freely enriching over convex algebras, which are the algebras of the finite distribution monad.","We show how this construction can be used for diagrammatic reasoning of noise in quantum systems."],"url":"http://arxiv.org/abs/2310.11288v1"}
{"created":"2023-10-17 14:09:45","title":"Evaluating the Impact of Humanitarian Aid on Food Security","abstract":"In the face of climate change-induced droughts, vulnerable regions encounter severe threats to food security, demanding urgent humanitarian assistance. This paper introduces a causal inference framework for the Horn of Africa, aiming to assess the impact of cash-based interventions on food crises. Our contributions encompass identifying causal relationships within the food security system, harmonizing a comprehensive database, and estimating the causal effect of humanitarian interventions on malnutrition. Our results revealed no significant effects, likely due to limited sample size, suboptimal data quality, and an imperfect causal graph resulting from our limited understanding of multidisciplinary systems like food security. This underscores the need to enhance data collection and refine causal models with domain experts for more effective future interventions and policies, improving transparency and accountability in humanitarian aid.","sentences":["In the face of climate change-induced droughts, vulnerable regions encounter severe threats to food security, demanding urgent humanitarian assistance.","This paper introduces a causal inference framework for the Horn of Africa, aiming to assess the impact of cash-based interventions on food crises.","Our contributions encompass identifying causal relationships within the food security system, harmonizing a comprehensive database, and estimating the causal effect of humanitarian interventions on malnutrition.","Our results revealed no significant effects, likely due to limited sample size, suboptimal data quality, and an imperfect causal graph resulting from our limited understanding of multidisciplinary systems like food security.","This underscores the need to enhance data collection and refine causal models with domain experts for more effective future interventions and policies, improving transparency and accountability in humanitarian aid."],"url":"http://arxiv.org/abs/2310.11287v1"}
{"created":"2023-10-17 14:08:12","title":"Construction of optimal optimum distance flag codes by MRD codes","abstract":"Optimum distance flag codes (ODFCs), as special flag codes, have received a lot of attention due to its application in random network coding. In 2021, Alonso-Gonz\\'{a}lez et al. constructed optimal $(n,\\mathcal{A})$-ODFC for $\\mathcal {A}\\subseteq \\{1,2,\\ldots,k,n-k,\\ldots,n-1\\}$ with $k\\in \\mathcal A$ and $k|n$. In this paper, we introduce a new construction of $(n,\\mathcal A)_q$-ODFCs by maximum rank-metric codes. It is proved that there is an $(n,\\mathcal{A})$-ODFC of size $\\frac{q^n-q^{k+r}}{q^k-1}+1$ for any $\\mathcal{A}\\subseteq\\{1,2,\\ldots,k,n-k,\\ldots,n-1\\}$ with $\\mathcal A\\cap \\{k,n-k\\}\\neq\\emptyset$, where $r\\equiv n\\pmod k$ and $0\\leq r<k$. Furthermore, when $k>\\frac{q^r-1}{q-1}$, this $(n,\\mathcal A)_q$-ODFC is optimal. Specially, when $r=0$, Alonso-Gonz\\'{a}lez et al.'s result is also obtained.","sentences":["Optimum distance flag codes (ODFCs), as special flag codes, have received a lot of attention due to its application in random network coding.","In 2021, Alonso-Gonz\\'{a}lez et al. constructed optimal $(n,\\mathcal{A})$-ODFC for $\\mathcal {A}\\subseteq \\{1,2,\\ldots,k,n-k,\\ldots,n-1\\}$ with $k\\in \\mathcal A$ and $k|n$. In this paper, we introduce a new construction of $(n,\\mathcal A)_q$-ODFCs by maximum rank-metric codes.","It is proved that there is an $(n,\\mathcal{A})$-ODFC of size $\\frac{q^n-q^{k+r}}{q^k-1}+1$ for any $\\mathcal{A}\\subseteq\\{1,2,\\ldots,k,n-k,\\ldots,n-1\\}$ with $\\mathcal A\\cap \\{k,n-k\\}\\neq\\emptyset$, where $r\\equiv n\\pmod k$ and $0\\leq r<k$.","Furthermore, when $k>\\frac{q^r-1}{q-1}$, this $(n,\\mathcal A)_q$-ODFC is optimal.","Specially, when $r=0$, Alonso-Gonz\\'{a}lez et al.'s result is also obtained."],"url":"http://arxiv.org/abs/2310.11285v1"}
{"created":"2023-10-17 14:06:55","title":"Separator Theorem and Algorithms for Planar Hyperbolic Graphs","abstract":"The hyperbolicity of a graph, informally, measures how close a graph is (metrically) to a tree. Hence, it is intuitively similar to treewidth, but the measures are formally incomparable. Motivated by the broad study of algorithms and separators on planar graphs and their relation to treewidth, we initiate the study of planar graphs of bounded hyperbolicity.   Our main technical contribution is a novel balanced separator theorem for planar $\\delta$-hyperbolic graphs that is substantially stronger than the classic planar separator theorem. For any fixed $\\delta \\geq 0$, we can find balanced separator that induces either a single geodesic (shortest) path or a single geodesic cycle in the graph.   An important advantage of our separator is that the union of our separator (vertex set $Z$) with any subset of the connected components of $G - Z$ induces again a planar $\\delta$-hyperbolic graph, which would not be guaranteed with an arbitrary separator. Our construction runs in near-linear time and guarantees that size of separator is $\\mathrm{poly}(\\delta) \\cdot \\log n$. As an application of our separator theorem and its strong properties, we obtain two novel approximation schemes on planar $\\delta$-hyperbolic graphs. We prove that Maximum Independent Set and the Traveling Salesperson problem have a near-linear time FPTAS for any constant $\\delta$, running in $n\\, \\mathrm{polylog}(n) \\cdot 2^{\\mathcal{O}(\\delta^2)} \\cdot \\varepsilon^{-\\mathcal{O}(\\delta)}$ time.   We also show that our approximation scheme for Maximum Independent Set has essentially the best possible running time under the Exponential Time Hypothesis (ETH). This immediately follows from our third contribution: we prove that Maximum Independent Set has no $n^{o(\\delta)}$-time algorithm on planar $\\delta$-hyperbolic graphs, unless ETH fails.","sentences":["The hyperbolicity of a graph, informally, measures how close a graph is (metrically) to a tree.","Hence, it is intuitively similar to treewidth, but the measures are formally incomparable.","Motivated by the broad study of algorithms and separators on planar graphs and their relation to treewidth, we initiate the study of planar graphs of bounded hyperbolicity.   ","Our main technical contribution is a novel balanced separator theorem for planar $\\delta$-hyperbolic graphs that is substantially stronger than the classic planar separator theorem.","For any fixed $\\delta \\geq 0$, we can find balanced separator that induces either a single geodesic (shortest) path or a single geodesic cycle in the graph.   ","An important advantage of our separator is that the union of our separator (vertex set $Z$) with any subset of the connected components of $G - Z$ induces again a planar $\\delta$-hyperbolic graph, which would not be guaranteed with an arbitrary separator.","Our construction runs in near-linear time and guarantees that size of separator is $\\mathrm{poly}(\\delta) \\cdot \\log n$. As an application of our separator theorem and its strong properties, we obtain two novel approximation schemes on planar $\\delta$-hyperbolic graphs.","We prove that Maximum Independent Set and the Traveling Salesperson problem have a near-linear time FPTAS for any constant $\\delta$, running in $n\\, \\mathrm{polylog}(n) \\cdot 2^{\\mathcal{O}(\\delta^2)}","\\cdot \\varepsilon^{-\\mathcal{O}(\\delta)}$ time.   ","We also show that our approximation scheme for Maximum Independent Set has essentially the best possible running time under the Exponential Time Hypothesis (ETH).","This immediately follows from our third contribution: we prove that Maximum Independent Set has no $n^{o(\\delta)}$-time algorithm on planar $\\delta$-hyperbolic graphs, unless ETH fails."],"url":"http://arxiv.org/abs/2310.11283v1"}
{"created":"2023-10-17 14:06:55","title":"Self-Supervised 3D Scene Flow Estimation and Motion Prediction using Local Rigidity Prior","abstract":"In this article, we investigate self-supervised 3D scene flow estimation and class-agnostic motion prediction on point clouds. A realistic scene can be well modeled as a collection of rigidly moving parts, therefore its scene flow can be represented as a combination of the rigid motion of these individual parts. Building upon this observation, we propose to generate pseudo scene flow labels for self-supervised learning through piecewise rigid motion estimation, in which the source point cloud is decomposed into local regions and each region is treated as rigid. By rigidly aligning each region with its potential counterpart in the target point cloud, we obtain a region-specific rigid transformation to generate its pseudo flow labels. To mitigate the impact of potential outliers on label generation, when solving the rigid registration for each region, we alternately perform three steps: establishing point correspondences, measuring the confidence for the correspondences, and updating the rigid transformation based on the correspondences and their confidence. As a result, confident correspondences will dominate label generation and a validity mask will be derived for the generated pseudo labels. By using the pseudo labels together with their validity mask for supervision, models can be trained in a self-supervised manner. Extensive experiments on FlyingThings3D and KITTI datasets demonstrate that our method achieves new state-of-the-art performance in self-supervised scene flow learning, without any ground truth scene flow for supervision, even performing better than some supervised counterparts. Additionally, our method is further extended to class-agnostic motion prediction and significantly outperforms previous state-of-the-art self-supervised methods on nuScenes dataset.","sentences":["In this article, we investigate self-supervised 3D scene flow estimation and class-agnostic motion prediction on point clouds.","A realistic scene can be well modeled as a collection of rigidly moving parts, therefore its scene flow can be represented as a combination of the rigid motion of these individual parts.","Building upon this observation, we propose to generate pseudo scene flow labels for self-supervised learning through piecewise rigid motion estimation, in which the source point cloud is decomposed into local regions and each region is treated as rigid.","By rigidly aligning each region with its potential counterpart in the target point cloud, we obtain a region-specific rigid transformation to generate its pseudo flow labels.","To mitigate the impact of potential outliers on label generation, when solving the rigid registration for each region, we alternately perform three steps: establishing point correspondences, measuring the confidence for the correspondences, and updating the rigid transformation based on the correspondences and their confidence.","As a result, confident correspondences will dominate label generation and a validity mask will be derived for the generated pseudo labels.","By using the pseudo labels together with their validity mask for supervision, models can be trained in a self-supervised manner.","Extensive experiments on FlyingThings3D and KITTI datasets demonstrate that our method achieves new state-of-the-art performance in self-supervised scene flow learning, without any ground truth scene flow for supervision, even performing better than some supervised counterparts.","Additionally, our method is further extended to class-agnostic motion prediction and significantly outperforms previous state-of-the-art self-supervised methods on nuScenes dataset."],"url":"http://arxiv.org/abs/2310.11284v1"}
{"created":"2023-10-17 14:06:06","title":"ChapGTP, ILLC's Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation","abstract":"We present the submission of the ILLC at the University of Amsterdam to the BabyLM challenge (Warstadt et al., 2023), in the strict-small track. Our final model, ChapGTP, is a masked language model that was trained for 200 epochs, aided by a novel data augmentation technique called Automatic Task Formation. We discuss in detail the performance of this model on the three evaluation suites: BLiMP, (Super)GLUE, and MSGS. Furthermore, we present a wide range of methods that were ultimately not included in the model, but may serve as inspiration for training LMs in low-resource settings.","sentences":["We present the submission of the ILLC at the University of Amsterdam to the BabyLM challenge (Warstadt et al., 2023), in the strict-small track.","Our final model, ChapGTP, is a masked language model that was trained for 200 epochs, aided by a novel data augmentation technique called Automatic Task Formation.","We discuss in detail the performance of this model on the three evaluation suites: BLiMP, (Super)GLUE, and MSGS.","Furthermore, we present a wide range of methods that were ultimately not included in the model, but may serve as inspiration for training LMs in low-resource settings."],"url":"http://arxiv.org/abs/2310.11282v1"}
{"created":"2023-10-17 14:04:22","title":"Self-supervision meets kernel graph neural models: From architecture to augmentations","abstract":"Graph representation learning has now become the de facto standard when handling graph-structured data, with the framework of message-passing graph neural networks (MPNN) being the most prevailing algorithmic tool. Despite its popularity, the family of MPNNs suffers from several drawbacks such as transparency and expressivity. Recently, the idea of designing neural models on graphs using the theory of graph kernels has emerged as a more transparent as well as sometimes more expressive alternative to MPNNs known as kernel graph neural networks (KGNNs). Developments on KGNNs are currently a nascent field of research, leaving several challenges from algorithmic design and adaptation to other learning paradigms such as self-supervised learning. In this paper, we improve the design and learning of KGNNs. Firstly, we extend the algorithmic formulation of KGNNs by allowing a more flexible graph-level similarity definition that encompasses former proposals like random walk graph kernel, as well as providing a smoother optimization objective that alleviates the need of introducing combinatorial learning procedures. Secondly, we enhance KGNNs through the lens of self-supervision via developing a novel structure-preserving graph data augmentation method called latent graph augmentation (LGA). Finally, we perform extensive empirical evaluations to demonstrate the efficacy of our proposed mechanisms. Experimental results over benchmark datasets suggest that our proposed model achieves competitive performance that is comparable to or sometimes outperforming state-of-the-art graph representation learning frameworks with or without self-supervision on graph classification tasks. Comparisons against other previously established graph data augmentation methods verify that the proposed LGA augmentation scheme captures better semantics of graph-level invariance.","sentences":["Graph representation learning has now become the de facto standard when handling graph-structured data, with the framework of message-passing graph neural networks (MPNN) being the most prevailing algorithmic tool.","Despite its popularity, the family of MPNNs suffers from several drawbacks such as transparency and expressivity.","Recently, the idea of designing neural models on graphs using the theory of graph kernels has emerged as a more transparent as well as sometimes more expressive alternative to MPNNs known as kernel graph neural networks (KGNNs).","Developments on KGNNs are currently a nascent field of research, leaving several challenges from algorithmic design and adaptation to other learning paradigms such as self-supervised learning.","In this paper, we improve the design and learning of KGNNs.","Firstly, we extend the algorithmic formulation of KGNNs by allowing a more flexible graph-level similarity definition that encompasses former proposals like random walk graph kernel, as well as providing a smoother optimization objective that alleviates the need of introducing combinatorial learning procedures.","Secondly, we enhance KGNNs through the lens of self-supervision via developing a novel structure-preserving graph data augmentation method called latent graph augmentation (LGA).","Finally, we perform extensive empirical evaluations to demonstrate the efficacy of our proposed mechanisms.","Experimental results over benchmark datasets suggest that our proposed model achieves competitive performance that is comparable to or sometimes outperforming state-of-the-art graph representation learning frameworks with or without self-supervision on graph classification tasks.","Comparisons against other previously established graph data augmentation methods verify that the proposed LGA augmentation scheme captures better semantics of graph-level invariance."],"url":"http://arxiv.org/abs/2310.11281v1"}
{"created":"2023-10-17 13:53:57","title":"xMEN: A Modular Toolkit for Cross-Lingual Medical Entity Normalization","abstract":"Objective: To improve performance of medical entity normalization across many languages, especially when fewer language resources are available compared to English.   Materials and Methods: We introduce xMEN, a modular system for cross-lingual medical entity normalization, which performs well in both low- and high-resource scenarios. When synonyms in the target language are scarce for a given terminology, we leverage English aliases via cross-lingual candidate generation. For candidate ranking, we incorporate a trainable cross-encoder model if annotations for the target task are available. We also evaluate cross-encoders trained in a weakly supervised manner based on machine-translated datasets from a high resource domain. Our system is publicly available as an extensible Python toolkit.   Results: xMEN improves the state-of-the-art performance across a wide range of multilingual benchmark datasets. Weakly supervised cross-encoders are effective when no training data is available for the target task. Through the compatibility of xMEN with the BigBIO framework, it can be easily used with existing and prospective datasets.   Discussion: Our experiments show the importance of balancing the output of general-purpose candidate generators with subsequent trainable re-rankers, which we achieve through a rank regularization term in the loss function of the cross-encoder. However, error analysis reveals that multi-word expressions and other complex entities are still challenging.   Conclusion: xMEN exhibits strong performance for medical entity normalization in multiple languages, even when no labeled data and few terminology aliases for the target language are available. Its configuration system and evaluation modules enable reproducible benchmarks. Models and code are available online at the following URL: https://github.com/hpi-dhc/xmen","sentences":["Objective: To improve performance of medical entity normalization across many languages, especially when fewer language resources are available compared to English.   ","Materials and Methods: We introduce xMEN, a modular system for cross-lingual medical entity normalization, which performs well in both low- and high-resource scenarios.","When synonyms in the target language are scarce for a given terminology, we leverage English aliases via cross-lingual candidate generation.","For candidate ranking, we incorporate a trainable cross-encoder model if annotations for the target task are available.","We also evaluate cross-encoders trained in a weakly supervised manner based on machine-translated datasets from a high resource domain.","Our system is publicly available as an extensible Python toolkit.   ","Results: xMEN improves the state-of-the-art performance across a wide range of multilingual benchmark datasets.","Weakly supervised cross-encoders are effective when no training data is available for the target task.","Through the compatibility of xMEN with the BigBIO framework, it can be easily used with existing and prospective datasets.   ","Discussion:","Our experiments show the importance of balancing the output of general-purpose candidate generators with subsequent trainable re-rankers, which we achieve through a rank regularization term in the loss function of the cross-encoder.","However, error analysis reveals that multi-word expressions and other complex entities are still challenging.   ","Conclusion: xMEN exhibits strong performance for medical entity normalization in multiple languages, even when no labeled data and few terminology aliases for the target language are available.","Its configuration system and evaluation modules enable reproducible benchmarks.","Models and code are available online at the following URL: https://github.com/hpi-dhc/xmen"],"url":"http://arxiv.org/abs/2310.11275v1"}
{"created":"2023-10-17 13:42:32","title":"Graph Neural Networks for Recommendation: Reproducibility, Graph Topology, and Node Representation","abstract":"Graph neural networks (GNNs) have gained prominence in recommendation systems in recent years. By representing the user-item matrix as a bipartite and undirected graph, GNNs have demonstrated their potential to capture short- and long-distance user-item interactions, thereby learning more accurate preference patterns than traditional recommendation approaches. In contrast to previous tutorials on the same topic, this tutorial aims to present and examine three key aspects that characterize GNNs for recommendation: (i) the reproducibility of state-of-the-art approaches, (ii) the potential impact of graph topological characteristics on the performance of these models, and (iii) strategies for learning node representations when training features from scratch or utilizing pre-trained embeddings as additional item information (e.g., multimodal features). The goal is to provide three novel theoretical and practical perspectives on the field, currently subject to debate in graph learning but long been overlooked in the context of recommendation systems.","sentences":["Graph neural networks (GNNs) have gained prominence in recommendation systems in recent years.","By representing the user-item matrix as a bipartite and undirected graph, GNNs have demonstrated their potential to capture short- and long-distance user-item interactions, thereby learning more accurate preference patterns than traditional recommendation approaches.","In contrast to previous tutorials on the same topic, this tutorial aims to present and examine three key aspects that characterize GNNs for recommendation: (i) the reproducibility of state-of-the-art approaches, (ii) the potential impact of graph topological characteristics on the performance of these models, and (iii) strategies for learning node representations when training features from scratch or utilizing pre-trained embeddings as additional item information (e.g., multimodal features).","The goal is to provide three novel theoretical and practical perspectives on the field, currently subject to debate in graph learning but long been overlooked in the context of recommendation systems."],"url":"http://arxiv.org/abs/2310.11270v1"}
{"created":"2023-10-17 13:39:26","title":"Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering with Large Language Models","abstract":"In response to the pressing need for advanced clinical problem-solving tools in healthcare, we introduce BooksMed, a novel framework based on a Large Language Model (LLM). BooksMed uniquely emulates human cognitive processes to deliver evidence-based and reliable responses, utilizing the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) framework to effectively quantify evidence strength. For clinical decision-making to be appropriately assessed, an evaluation metric that is clinically aligned and validated is required. As a solution, we present ExpertMedQA, a multispecialty clinical benchmark comprised of open-ended, expert-level clinical questions, and validated by a diverse group of medical professionals. By demanding an in-depth understanding and critical appraisal of up-to-date clinical literature, ExpertMedQA rigorously evaluates LLM performance. BooksMed outperforms existing state-of-the-art models Med-PaLM 2, Almanac, and ChatGPT in a variety of medical scenarios. Therefore, a framework that mimics human cognitive stages could be a useful tool for providing reliable and evidence-based responses to clinical inquiries.","sentences":["In response to the pressing need for advanced clinical problem-solving tools in healthcare, we introduce BooksMed, a novel framework based on a Large Language Model (LLM).","BooksMed uniquely emulates human cognitive processes to deliver evidence-based and reliable responses, utilizing the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) framework to effectively quantify evidence strength.","For clinical decision-making to be appropriately assessed, an evaluation metric that is clinically aligned and validated is required.","As a solution, we present ExpertMedQA, a multispecialty clinical benchmark comprised of open-ended, expert-level clinical questions, and validated by a diverse group of medical professionals.","By demanding an in-depth understanding and critical appraisal of up-to-date clinical literature, ExpertMedQA rigorously evaluates LLM performance.","BooksMed outperforms existing state-of-the-art models Med-PaLM 2, Almanac, and ChatGPT in a variety of medical scenarios.","Therefore, a framework that mimics human cognitive stages could be a useful tool for providing reliable and evidence-based responses to clinical inquiries."],"url":"http://arxiv.org/abs/2310.11266v1"}
{"created":"2023-10-17 13:23:18","title":"Utilizing Weak Supervision To Generate Indonesian Conservation Dataset","abstract":"Weak supervision has emerged as a promising approach for rapid and large-scale dataset creation in response to the increasing demand for accelerated NLP development. By leveraging labeling functions, weak supervision allows practitioners to generate datasets quickly by creating learned label models that produce soft-labeled datasets. This paper aims to show how such an approach can be utilized to build an Indonesian NLP dataset from conservation news text. We construct two types of datasets: multi-class classification and sentiment classification. We then provide baseline experiments using various pretrained language models. These baseline results demonstrate test performances of 59.79% accuracy and 55.72% F1-score for sentiment classification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC for multi-class classification. Additionally, we release the datasets and labeling functions used in this work for further research and exploration.","sentences":["Weak supervision has emerged as a promising approach for rapid and large-scale dataset creation in response to the increasing demand for accelerated NLP development.","By leveraging labeling functions, weak supervision allows practitioners to generate datasets quickly by creating learned label models that produce soft-labeled datasets.","This paper aims to show how such an approach can be utilized to build an Indonesian NLP dataset from conservation news text.","We construct two types of datasets: multi-class classification and sentiment classification.","We then provide baseline experiments using various pretrained language models.","These baseline results demonstrate test performances of 59.79% accuracy and 55.72% F1-score for sentiment classification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC for multi-class classification.","Additionally, we release the datasets and labeling functions used in this work for further research and exploration."],"url":"http://arxiv.org/abs/2310.11258v1"}
{"created":"2023-10-17 13:22:59","title":"An empirical study of automatic wildlife detection using drone thermal imaging and object detection","abstract":"Artificial intelligence has the potential to make valuable contributions to wildlife management through cost-effective methods for the collection and interpretation of wildlife data. Recent advances in remotely piloted aircraft systems (RPAS or ``drones'') and thermal imaging technology have created new approaches to collect wildlife data. These emerging technologies could provide promising alternatives to standard labourious field techniques as well as cover much larger areas. In this study, we conduct a comprehensive review and empirical study of drone-based wildlife detection. Specifically, we collect a realistic dataset of drone-derived wildlife thermal detections. Wildlife detections, including arboreal (for instance, koalas, phascolarctos cinereus) and ground dwelling species in our collected data are annotated via bounding boxes by experts. We then benchmark state-of-the-art object detection algorithms on our collected dataset. We use these experimental results to identify issues and discuss future directions in automatic animal monitoring using drones.","sentences":["Artificial intelligence has the potential to make valuable contributions to wildlife management through cost-effective methods for the collection and interpretation of wildlife data.","Recent advances in remotely piloted aircraft systems (RPAS or ``drones'') and thermal imaging technology have created new approaches to collect wildlife data.","These emerging technologies could provide promising alternatives to standard labourious field techniques as well as cover much larger areas.","In this study, we conduct a comprehensive review and empirical study of drone-based wildlife detection.","Specifically, we collect a realistic dataset of drone-derived wildlife thermal detections.","Wildlife detections, including arboreal (for instance, koalas, phascolarctos cinereus) and ground dwelling species in our collected data are annotated via bounding boxes by experts.","We then benchmark state-of-the-art object detection algorithms on our collected dataset.","We use these experimental results to identify issues and discuss future directions in automatic animal monitoring using drones."],"url":"http://arxiv.org/abs/2310.11257v1"}
{"created":"2023-10-17 13:20:16","title":"Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges","abstract":"The growing popularity of generative language models has amplified interest in interactive methods to guide model outputs. Prompt refinement is considered one of the most effective means to influence output among these methods. We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges. A comprehensive examination of model outputs, including runner-up candidates and their corresponding probabilities, is needed to address these issues. The beam search tree, the prevalent algorithm to sample model outputs, can inherently supply this information. Consequently, we introduce an interactive visual method for investigating the beam search tree, facilitating analysis of the decisions made by the model during generation. We quantitatively show the value of exposing the beam search tree and present five detailed analysis scenarios addressing the identified challenges. Our methodology validates existing results and offers additional insights.","sentences":["The growing popularity of generative language models has amplified interest in interactive methods to guide model outputs.","Prompt refinement is considered one of the most effective means to influence output among these methods.","We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges.","A comprehensive examination of model outputs, including runner-up candidates and their corresponding probabilities, is needed to address these issues.","The beam search tree, the prevalent algorithm to sample model outputs, can inherently supply this information.","Consequently, we introduce an interactive visual method for investigating the beam search tree, facilitating analysis of the decisions made by the model during generation.","We quantitatively show the value of exposing the beam search tree and present five detailed analysis scenarios addressing the identified challenges.","Our methodology validates existing results and offers additional insights."],"url":"http://arxiv.org/abs/2310.11252v1"}
