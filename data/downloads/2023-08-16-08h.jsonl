{"created":"2023-08-15 17:59:56","title":"CoDeF: Content Deformation Fields for Temporally Consistent Video Processing","abstract":"We present the content deformation field CoDeF as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis.Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline.We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e.g., the object shape) from the video.With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field.We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training.More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog.Project page can be found at https://qiuyu96.github.io/CoDeF/.","sentences":["We present the content deformation field CoDeF as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis.","Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline.","We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e.g., the object shape) from the video.","With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field.","We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training.","More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog.","Project page can be found at https://qiuyu96.github.io/CoDeF/."],"url":"http://arxiv.org/abs/2308.07926v1"}
{"created":"2023-08-15 17:59:55","title":"Domain-Adaptive Device Fingerprints for Network Access Authentication Through Multifractal Dimension Representation","abstract":"RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a potential solution for automated network access authentication. Traditional approaches are commonly susceptible to the domain adaptation problem where a model trained on data from one domain performs badly when tested on data from a different domain. Some examples of a domain change include varying the device location or environment and varying the time or day of data collection. In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are domain generalizable. We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ signals, and evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices under different locations and at different scales. Our results show that the VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using raw IQ data.","sentences":["RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a potential solution for automated network access authentication.","Traditional approaches are commonly susceptible to the domain adaptation problem where a model trained on data from one domain performs badly when tested on data from a different domain.","Some examples of a domain change include varying the device location or environment and varying the time or day of data collection.","In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are domain generalizable.","We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ signals, and evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices under different locations and at different scales.","Our results show that the VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using raw IQ data."],"url":"http://arxiv.org/abs/2308.07925v1"}
{"created":"2023-08-15 17:59:46","title":"Investigation Toward The Economic Feasibility of Personalized Medicine For Healthcare Service Providers: The Case of Bladder Cancer","abstract":"In today's complex healthcare landscape, the pursuit of delivering optimal patient care while navigating intricate economic dynamics poses a significant challenge for healthcare service providers (HSPs). In this already complex dynamics, the emergence of clinically promising personalized medicine based treatment aims to revolutionize medicine. While personalized medicine holds tremendous potential for enhancing therapeutic outcomes, its integration within resource-constrained HSPs presents formidable challenges. In this study, we investigate the economic feasibility of implementing personalized medicine. The central objective is to strike a balance between catering to individual patient needs and making economically viable decisions. Unlike conventional binary approaches to personalized treatment, we propose a more nuanced perspective by treating personalization as a spectrum. This approach allows for greater flexibility in decision-making and resource allocation. To this end, we propose a mathematical framework to investigate our proposal, focusing on Bladder Cancer (BC) as a case study. Our results show that while it is feasible to introduce personalized medicine, a highly efficient but highly expensive one would be short-lived relative to its less effective but cheaper alternative as the latter can be provided to a larger cohort of patients, optimizing the HSP's objective better.","sentences":["In today's complex healthcare landscape, the pursuit of delivering optimal patient care while navigating intricate economic dynamics poses a significant challenge for healthcare service providers (HSPs).","In this already complex dynamics, the emergence of clinically promising personalized medicine based treatment aims to revolutionize medicine.","While personalized medicine holds tremendous potential for enhancing therapeutic outcomes, its integration within resource-constrained HSPs presents formidable challenges.","In this study, we investigate the economic feasibility of implementing personalized medicine.","The central objective is to strike a balance between catering to individual patient needs and making economically viable decisions.","Unlike conventional binary approaches to personalized treatment, we propose a more nuanced perspective by treating personalization as a spectrum.","This approach allows for greater flexibility in decision-making and resource allocation.","To this end, we propose a mathematical framework to investigate our proposal, focusing on Bladder Cancer (BC) as a case study.","Our results show that while it is feasible to introduce personalized medicine, a highly efficient but highly expensive one would be short-lived relative to its less effective but cheaper alternative as the latter can be provided to a larger cohort of patients, optimizing the HSP's objective better."],"url":"http://arxiv.org/abs/2308.07924v1"}
{"created":"2023-08-15 17:59:33","title":"Enumerating Tarski fixed points on lattices of binary relations","abstract":"We study the problem of enumerating Tarski fixed points, focusing on the relational lattices of equivalences, quasiorders and binary relations. We present a polynomial space enumeration algorithm for Tarski fixed points on these lattices and other lattices of polynomial height. It achieves polynomial delay when enumerating fixed points of increasing isotone maps on all three lattices, as well as decreasing isotone maps on the lattice of binary relations. In those cases in which the enumeration algorithm does not guarantee polynomial delay on the three relational lattices on the other hand, we prove exponential lower bounds for deciding the existence of three fixed points when the isotone map is given as an oracle, and that it is NP-hard to find three or more Tarski fixed points. More generally, we show that any deterministic or bounded-error randomized algorithm must perform a number of queries asymptotically at least as large as the lattice width to decide the existence of three fixed points when the isotone map is given as an oracle. Finally, we demonstrate that our findings yield a polynomial delay and space algorithm for listing bisimulations and instances of some related models of behavioral or role equivalence.","sentences":["We study the problem of enumerating Tarski fixed points, focusing on the relational lattices of equivalences, quasiorders and binary relations.","We present a polynomial space enumeration algorithm for Tarski fixed points on these lattices and other lattices of polynomial height.","It achieves polynomial delay when enumerating fixed points of increasing isotone maps on all three lattices, as well as decreasing isotone maps on the lattice of binary relations.","In those cases in which the enumeration algorithm does not guarantee polynomial delay on the three relational lattices on the other hand, we prove exponential lower bounds for deciding the existence of three fixed points when the isotone map is given as an oracle, and that it is NP-hard to find three or more Tarski fixed points.","More generally, we show that any deterministic or bounded-error randomized algorithm must perform a number of queries asymptotically at least as large as the lattice width to decide the existence of three fixed points when the isotone map is given as an oracle.","Finally, we demonstrate that our findings yield a polynomial delay and space algorithm for listing bisimulations and instances of some related models of behavioral or role equivalence."],"url":"http://arxiv.org/abs/2308.07923v1"}
{"created":"2023-08-15 17:59:18","title":"RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models","abstract":"In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.","sentences":["In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models.","We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length.","To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling.","We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications.","Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters.","Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction."],"url":"http://arxiv.org/abs/2308.07922v1"}
{"created":"2023-08-15 17:58:45","title":"Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification","abstract":"Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}.","sentences":["Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems.","In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets.","In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter.","We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs.","Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter.","This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers.","In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination.","Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting.","With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}."],"url":"http://arxiv.org/abs/2308.07921v1"}
{"created":"2023-08-15 17:58:11","title":"Helping Hands: An Object-Aware Ego-Centric Video Recognition Model","abstract":"We introduce an object-aware decoder for improving the performance of spatio-temporal representations on ego-centric videos. The key idea is to enhance object-awareness during training by tasking the model to predict hand positions, object positions, and the semantic label of the objects using paired captions when available. At inference time the model only requires RGB frames as inputs, and is able to track and ground objects (although it has not been trained explicitly for this). We demonstrate the performance of the object-aware representations learnt by our model, by: (i) evaluating it for strong transfer, i.e. through zero-shot testing, on a number of downstream video-text retrieval and classification benchmarks; and (ii) by using the representations learned as input for long-term video understanding tasks (e.g. Episodic Memory in Ego4D). In all cases the performance improves over the state of the art -- even compared to networks trained with far larger batch sizes. We also show that by using noisy image-level detection as pseudo-labels in training, the model learns to provide better bounding boxes using video consistency, as well as grounding the words in the associated text descriptions. Overall, we show that the model can act as a drop-in replacement for an ego-centric video model to improve performance through visual-text grounding.","sentences":["We introduce an object-aware decoder for improving the performance of spatio-temporal representations on ego-centric videos.","The key idea is to enhance object-awareness during training by tasking the model to predict hand positions, object positions, and the semantic label of the objects using paired captions when available.","At inference time the model only requires RGB frames as inputs, and is able to track and ground objects (although it has not been trained explicitly for this).","We demonstrate the performance of the object-aware representations learnt by our model, by: (i) evaluating it for strong transfer, i.e. through zero-shot testing, on a number of downstream video-text retrieval and classification benchmarks; and (ii) by using the representations learned as input for long-term video understanding tasks (e.g. Episodic Memory in Ego4D).","In all cases the performance improves over the state of the art -- even compared to networks trained with far larger batch sizes.","We also show that by using noisy image-level detection as pseudo-labels in training, the model learns to provide better bounding boxes using video consistency, as well as grounding the words in the associated text descriptions.","Overall, we show that the model can act as a drop-in replacement for an ego-centric video model to improve performance through visual-text grounding."],"url":"http://arxiv.org/abs/2308.07918v1"}
{"created":"2023-08-15 17:45:02","title":"The Role of Early Sampling in Age of Information Minimization in the Presence of ACK Delays","abstract":"We study the structure of the optimal sampling policy to minimize the average age of information when the channel state (i.e., busy or idle) is not immediately perceived by the transmitter upon the delivery of a sample due to random delays in the feedback (ACK) channel. In this setting, we show that it is not always optimal to wait for ACKs before sampling, and thus, early sampling before the arrival of an ACK may be optimal. We show that, under certain conditions on the distribution of the ACK delays, the optimal policy is a mixture of two threshold policies.","sentences":["We study the structure of the optimal sampling policy to minimize the average age of information when the channel state (i.e., busy or idle) is not immediately perceived by the transmitter upon the delivery of a sample due to random delays in the feedback (ACK) channel.","In this setting, we show that it is not always optimal to wait for ACKs before sampling, and thus, early sampling before the arrival of an ACK may be optimal.","We show that, under certain conditions on the distribution of the ACK delays, the optimal policy is a mixture of two threshold policies."],"url":"http://arxiv.org/abs/2308.07905v1"}
{"created":"2023-08-15 17:42:39","title":"Relightable and Animatable Neural Avatar from Sparse-View Video","abstract":"This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination. Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem. Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting. While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering. To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses. Specifically, we estimate coarse distances based on a parametric human model and compute fine distances by exploiting the local deformation invariance of SDF. Based on the HDQ algorithm, we leverage sphere tracing to efficiently estimate the surface intersection and light visibility. This allows us to develop the first system to recover animatable and relightable neural avatars from sparse view (or monocular) inputs. Experiments demonstrate that our approach is able to produce superior results compared to state-of-the-art methods. Our code will be released for reproducibility.","sentences":["This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination.","Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem.","Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting.","While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering.","To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses.","Specifically, we estimate coarse distances based on a parametric human model and compute fine distances by exploiting the local deformation invariance of SDF.","Based on the HDQ algorithm, we leverage sphere tracing to efficiently estimate the surface intersection and light visibility.","This allows us to develop the first system to recover animatable and relightable neural avatars from sparse view (or monocular) inputs.","Experiments demonstrate that our approach is able to produce superior results compared to state-of-the-art methods.","Our code will be released for reproducibility."],"url":"http://arxiv.org/abs/2308.07903v1"}
{"created":"2023-08-15 17:40:34","title":"Through the Lens of Core Competency: Survey on Evaluation of Large Language Models","abstract":"From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses. The evaluation of a research field guides its direction of improvement. However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety. For every competency, we introduce its definition, corresponding benchmarks, and metrics. Under this competency architecture, similar tasks are combined to reflect corresponding ability, while new tasks can also be easily added into the system. Finally, we give our suggestions on the future direction of LLM's evaluation.","sentences":["From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses.","The evaluation of a research field guides its direction of improvement.","However, LLMs are extremely hard to thoroughly evaluate for two reasons.","First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM.","Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios.","To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs.","To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations.","We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety.","For every competency, we introduce its definition, corresponding benchmarks, and metrics.","Under this competency architecture, similar tasks are combined to reflect corresponding ability, while new tasks can also be easily added into the system.","Finally, we give our suggestions on the future direction of LLM's evaluation."],"url":"http://arxiv.org/abs/2308.07902v1"}
{"created":"2023-08-15 17:40:10","title":"The Regular Expression Inference Challenge","abstract":"We propose \\emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\\text{cost}(\\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\\text{cost}(r')<\\text{cost}(r)$.   REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based ML.   Recently, an REI solver was implemented on GPUs, using program synthesis techniques. This enabled, for the first time, fast generation of minimal expressions for complex REI instances. Building on this advance, we generate and publish the first large-scale datasets for REI, and devise and evaluate several initial heuristic and machine learning baselines.   We invite the community to participate and explore ML methods that learn to solve REI problems. We believe that progress in REI directly translates to code/language modelling.","sentences":["We propose \\emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community.","REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\\text{cost}(\\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\\text{cost}(r')<\\text{cost}(r)$.   REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based ML.   ","Recently, an REI solver was implemented on GPUs, using program synthesis techniques.","This enabled, for the first time, fast generation of minimal expressions for complex REI instances.","Building on this advance, we generate and publish the first large-scale datasets for REI, and devise and evaluate several initial heuristic and machine learning baselines.   ","We invite the community to participate and explore ML methods that learn to solve REI problems.","We believe that progress in REI directly translates to code/language modelling."],"url":"http://arxiv.org/abs/2308.07899v1"}
{"created":"2023-08-15 17:39:52","title":"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision","abstract":"Foundation vision-language models are currently transforming computer vision, and are on the rise in medical imaging fueled by their very promising generalization capabilities. However, the initial attempts to transfer this new paradigm to medical imaging have shown less impressive performances than those observed in other domains, due to the significant domain shift and the complex, expert domain knowledge inherent to medical-imaging tasks. Motivated by the need for domain-expert foundation models, we present FLAIR, a pre-trained vision-language model for universal retinal fundus image understanding. To this end, we compiled 37 open-access, mostly categorical fundus imaging datasets from various sources, with up to 97 different target conditions and 284,660 images. We integrate the expert's domain knowledge in the form of descriptive textual prompts, during both pre-training and zero-shot inference, enhancing the less-informative categorical supervision of the data. Such a textual expert's knowledge, which we compiled from the relevant clinical literature and community standards, describes the fine-grained features of the pathologies as well as the hierarchies and dependencies between them. We report comprehensive evaluations, which illustrate the benefit of integrating expert knowledge and the strong generalization capabilities of FLAIR under difficult scenarios with domain shifts or unseen categories. When adapted with a lightweight linear probe, FLAIR outperforms fully-trained, dataset-focused models, more so in the few-shot regimes. Interestingly, FLAIR outperforms by a large margin more generalist, larger-scale image-language models, which emphasizes the potential of embedding experts' domain knowledge and the limitations of generalist models in medical imaging.","sentences":["Foundation vision-language models are currently transforming computer vision, and are on the rise in medical imaging fueled by their very promising generalization capabilities.","However, the initial attempts to transfer this new paradigm to medical imaging have shown less impressive performances than those observed in other domains, due to the significant domain shift and the complex, expert domain knowledge inherent to medical-imaging tasks.","Motivated by the need for domain-expert foundation models, we present FLAIR, a pre-trained vision-language model for universal retinal fundus image understanding.","To this end, we compiled 37 open-access, mostly categorical fundus imaging datasets from various sources, with up to 97 different target conditions and 284,660 images.","We integrate the expert's domain knowledge in the form of descriptive textual prompts, during both pre-training and zero-shot inference, enhancing the less-informative categorical supervision of the data.","Such a textual expert's knowledge, which we compiled from the relevant clinical literature and community standards, describes the fine-grained features of the pathologies as well as the hierarchies and dependencies between them.","We report comprehensive evaluations, which illustrate the benefit of integrating expert knowledge and the strong generalization capabilities of FLAIR under difficult scenarios with domain shifts or unseen categories.","When adapted with a lightweight linear probe, FLAIR outperforms fully-trained, dataset-focused models, more so in the few-shot regimes.","Interestingly, FLAIR outperforms by a large margin more generalist, larger-scale image-language models, which emphasizes the potential of embedding experts' domain knowledge and the limitations of generalist models in medical imaging."],"url":"http://arxiv.org/abs/2308.07898v1"}
{"created":"2023-08-15 17:36:18","title":"Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining","abstract":"Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.","sentences":["Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life.","Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics.","We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data.","Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms.","It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models.","The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery.","We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers.","The results demonstrate that our system effectively supports clinical and symptom research."],"url":"http://arxiv.org/abs/2308.07895v1"}
{"created":"2023-08-15 17:34:54","title":"Memory-and-Anticipation Transformer for Online Action Understanding","abstract":"Most existing forecasting systems are memory-based methods, which attempt to mimic human forecasting ability by employing various memory mechanisms and have progressed in temporal modeling for memory dependency. Nevertheless, an obvious weakness of this paradigm is that it can only model limited historical dependence and can not transcend the past. In this paper, we rethink the temporal dependence of event evolution and propose a novel memory-anticipation-based paradigm to model an entire temporal structure, including the past, present, and future. Based on this idea, we present Memory-and-Anticipation Transformer (MAT), a memory-anticipation-based approach, to address the online action detection and anticipation tasks. In addition, owing to the inherent superiority of MAT, it can process online action detection and anticipation tasks in a unified manner. The proposed MAT model is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, and EPIC-Kitchens-100, for online action detection and anticipation tasks, and it significantly outperforms all existing methods. Code is available at https://github.com/Echo0125/Memory-and-Anticipation-Transformer.","sentences":["Most existing forecasting systems are memory-based methods, which attempt to mimic human forecasting ability by employing various memory mechanisms and have progressed in temporal modeling for memory dependency.","Nevertheless, an obvious weakness of this paradigm is that it can only model limited historical dependence and can not transcend the past.","In this paper, we rethink the temporal dependence of event evolution and propose a novel memory-anticipation-based paradigm to model an entire temporal structure, including the past, present, and future.","Based on this idea, we present Memory-and-Anticipation Transformer (MAT), a memory-anticipation-based approach, to address the online action detection and anticipation tasks.","In addition, owing to the inherent superiority of MAT, it can process online action detection and anticipation tasks in a unified manner.","The proposed MAT model is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, and EPIC-Kitchens-100, for online action detection and anticipation tasks, and it significantly outperforms all existing methods.","Code is available at https://github.com/Echo0125/Memory-and-Anticipation-Transformer."],"url":"http://arxiv.org/abs/2308.07893v1"}
{"created":"2023-08-15 17:33:24","title":"Link-Context Learning for Multimodal LLMs","abstract":"The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.","sentences":["The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations.","Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge.","In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks.","In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs.","LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set.","By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively.","To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning.","Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs.","Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning."],"url":"http://arxiv.org/abs/2308.07891v1"}
{"created":"2023-08-15 17:31:35","title":"EduSAT: A Pedagogical Tool for Theory and Applications of Boolean Satisfiability","abstract":"Boolean Satisfiability (SAT) and Satisfiability Modulo Theories (SMT) are widely used in automated verification, but there is a lack of interactive tools designed for educational purposes in this field. To address this gap, we present EduSAT, a pedagogical tool specifically developed to support learning and understanding of SAT and SMT solving. EduSAT offers implementations of key algorithms such as the Davis-Putnam-Logemann-Loveland (DPLL) algorithm and the Reduced Order Binary Decision Diagram (ROBDD) for SAT solving. Additionally, EduSAT provides solver abstractions for five NP-complete problems beyond SAT and SMT. Users can benefit from EduSAT by experimenting, analyzing, and validating their understanding of SAT and SMT solving techniques. Our tool is accompanied by comprehensive documentation and tutorials, extensive testing, and practical features such as a natural language interface and SAT and SMT formula generators, which also serve as a valuable opportunity for learners to deepen their understanding. Our evaluation of EduSAT demonstrates its high accuracy, achieving 100% correctness across all the implemented SAT and SMT solvers. We release EduSAT as a python package in .whl file, and the source can be identified at https://github.com/zhaoy37/SAT_Solver.","sentences":["Boolean Satisfiability (SAT) and Satisfiability Modulo Theories (SMT) are widely used in automated verification, but there is a lack of interactive tools designed for educational purposes in this field.","To address this gap, we present EduSAT, a pedagogical tool specifically developed to support learning and understanding of SAT and SMT solving.","EduSAT offers implementations of key algorithms such as the Davis-Putnam-Logemann-Loveland (DPLL) algorithm and the Reduced Order Binary Decision Diagram (ROBDD) for SAT solving.","Additionally, EduSAT provides solver abstractions for five NP-complete problems beyond SAT and SMT.","Users can benefit from EduSAT by experimenting, analyzing, and validating their understanding of SAT and SMT solving techniques.","Our tool is accompanied by comprehensive documentation and tutorials, extensive testing, and practical features such as a natural language interface and SAT and SMT formula generators, which also serve as a valuable opportunity for learners to deepen their understanding.","Our evaluation of EduSAT demonstrates its high accuracy, achieving 100% correctness across all the implemented SAT and SMT solvers.","We release EduSAT as a python package in .whl file, and the source can be identified at https://github.com/zhaoy37/SAT_Solver."],"url":"http://arxiv.org/abs/2308.07890v1"}
{"created":"2023-08-15 17:30:57","title":"A Comprehensive Study on Knowledge Graph Embedding over Relational Patterns Based on Rule Learning","abstract":"Knowledge Graph Embedding (KGE) has proven to be an effective approach to solving the Knowledge Graph Completion (KGC) task. Relational patterns which refer to relations with specific semantics exhibiting graph patterns are an important factor in the performance of KGE models. Though KGE models' capabilities are analyzed over different relational patterns in theory and a rough connection between better relational patterns modeling and better performance of KGC has been built, a comprehensive quantitative analysis on KGE models over relational patterns remains absent so it is uncertain how the theoretical support of KGE to a relational pattern contributes to the performance of triples associated to such a relational pattern. To address this challenge, we evaluate the performance of 7 KGE models over 4 common relational patterns on 2 benchmarks, then conduct an analysis in theory, entity frequency, and part-to-whole three aspects and get some counterintuitive conclusions. Finally, we introduce a training-free method Score-based Patterns Adaptation (SPA) to enhance KGE models' performance over various relational patterns. This approach is simple yet effective and can be applied to KGE models without additional training. Our experimental results demonstrate that our method generally enhances performance over specific relational patterns. Our source code is available from GitHub at https://github.com/zjukg/Comprehensive-Study-over-Relational-Patterns.","sentences":["Knowledge Graph Embedding (KGE) has proven to be an effective approach to solving the Knowledge Graph Completion (KGC) task.","Relational patterns which refer to relations with specific semantics exhibiting graph patterns are an important factor in the performance of KGE models.","Though KGE models' capabilities are analyzed over different relational patterns in theory and a rough connection between better relational patterns modeling and better performance of KGC has been built, a comprehensive quantitative analysis on KGE models over relational patterns remains absent so it is uncertain how the theoretical support of KGE to a relational pattern contributes to the performance of triples associated to such a relational pattern.","To address this challenge, we evaluate the performance of 7 KGE models over 4 common relational patterns on 2 benchmarks, then conduct an analysis in theory, entity frequency, and part-to-whole three aspects and get some counterintuitive conclusions.","Finally, we introduce a training-free method Score-based Patterns Adaptation (SPA) to enhance KGE models' performance over various relational patterns.","This approach is simple yet effective and can be applied to KGE models without additional training.","Our experimental results demonstrate that our method generally enhances performance over specific relational patterns.","Our source code is available from GitHub at https://github.com/zjukg/Comprehensive-Study-over-Relational-Patterns."],"url":"http://arxiv.org/abs/2308.07889v1"}
{"created":"2023-08-15 17:23:18","title":"Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms","abstract":"The state-of-the-art in time series classification has come a long way, from the 1NN-DTW algorithm to the ROCKET family of classifiers. However, in the current fast-paced development of new classifiers, taking a step back and performing simple baseline checks is essential. These checks are often overlooked, as researchers are focused on establishing new state-of-the-art results, developing scalable algorithms, and making models explainable. Nevertheless, there are many datasets that look like time series at first glance, but classic algorithms such as tabular methods with no time ordering may perform better on such problems. For example, for spectroscopy datasets, tabular methods tend to significantly outperform recent time series methods. In this study, we compare the performance of tabular models using classic machine learning approaches (e.g., Ridge, LDA, RandomForest) with the ROCKET family of classifiers (e.g., Rocket, MiniRocket, MultiRocket). Tabular models are simple and very efficient, while the ROCKET family of classifiers are more complex and have state-of-the-art accuracy and efficiency among recent time series classifiers. We find that tabular models outperform the ROCKET family of classifiers on approximately 19% of univariate and 28% of multivariate datasets in the UCR/UEA benchmark and achieve accuracy within 10 percentage points on about 50% of datasets. Our results suggest that it is important to consider simple tabular models as baselines when developing time series classifiers. These models are very fast, can be as effective as more complex methods and may be easier to understand and deploy.","sentences":["The state-of-the-art in time series classification has come a long way, from the 1NN-DTW algorithm to the ROCKET family of classifiers.","However, in the current fast-paced development of new classifiers, taking a step back and performing simple baseline checks is essential.","These checks are often overlooked, as researchers are focused on establishing new state-of-the-art results, developing scalable algorithms, and making models explainable.","Nevertheless, there are many datasets that look like time series at first glance, but classic algorithms such as tabular methods with no time ordering may perform better on such problems.","For example, for spectroscopy datasets, tabular methods tend to significantly outperform recent time series methods.","In this study, we compare the performance of tabular models using classic machine learning approaches (e.g., Ridge, LDA, RandomForest) with the ROCKET family of classifiers (e.g., Rocket, MiniRocket, MultiRocket).","Tabular models are simple and very efficient, while the ROCKET family of classifiers are more complex and have state-of-the-art accuracy and efficiency among recent time series classifiers.","We find that tabular models outperform the ROCKET family of classifiers on approximately 19% of univariate and 28% of multivariate datasets in the UCR/UEA benchmark and achieve accuracy within 10 percentage points on about 50% of datasets.","Our results suggest that it is important to consider simple tabular models as baselines when developing time series classifiers.","These models are very fast, can be as effective as more complex methods and may be easier to understand and deploy."],"url":"http://arxiv.org/abs/2308.07886v1"}
{"created":"2023-08-15 17:13:16","title":"Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations","abstract":"Recently, Graph Neural Networks (GNNs) have shown promising performance in tasks on dynamic graphs such as node classification, link prediction and graph regression. However, few work has studied the temporal edge regression task which has important real-world applications. In this paper, we explore the application of GNNs to edge regression tasks in both static and dynamic settings, focusing on predicting food and agriculture trade values between nations. We introduce three simple yet strong baselines and comprehensively evaluate one static and three dynamic GNN models using the UN Trade dataset. Our experimental results reveal that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs. We also find that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks. Moreover, we note that the proportion of negative edges in the training samples significantly affects the test performance. The companion source code can be found at: https://github.com/scylj1/GNN_Edge_Regression.","sentences":["Recently, Graph Neural Networks (GNNs) have shown promising performance in tasks on dynamic graphs such as node classification, link prediction and graph regression.","However, few work has studied the temporal edge regression task which has important real-world applications.","In this paper, we explore the application of GNNs to edge regression tasks in both static and dynamic settings, focusing on predicting food and agriculture trade values between nations.","We introduce three simple yet strong baselines and comprehensively evaluate one static and three dynamic GNN models using the UN Trade dataset.","Our experimental results reveal that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs.","We also find that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks.","Moreover, we note that the proportion of negative edges in the training samples significantly affects the test performance.","The companion source code can be found at: https://github.com/scylj1/GNN_Edge_Regression."],"url":"http://arxiv.org/abs/2308.07883v1"}
{"created":"2023-08-15 16:57:09","title":"The $10 Million ANA Avatar XPRIZE Competition Advanced Immersive Telepresence Systems","abstract":"The $10M ANA Avatar XPRIZE aimed to create avatar systems that can transport human presence to remote locations in real time. The participants of this multi-year competition developed robotic systems that allow operators to see, hear, and interact with a remote environment in a way that feels as if they are truly there. On the other hand, people in the remote environment were given the impression that the operator was present inside the avatar robot. At the competition finals, held in November 2022 in Long Beach, CA, USA, the avatar systems were evaluated on their support for remotely interacting with humans, exploring new environments, and employing specialized skills. This article describes the competition stages with tasks and evaluation procedures, reports the results, presents the winning teams' approaches, and discusses lessons learned.","sentences":["The $10M ANA Avatar XPRIZE aimed to create avatar systems that can transport human presence to remote locations in real time.","The participants of this multi-year competition developed robotic systems that allow operators to see, hear, and interact with a remote environment in a way that feels as if they are truly there.","On the other hand, people in the remote environment were given the impression that the operator was present inside the avatar robot.","At the competition finals, held in November 2022 in Long Beach, CA, USA, the avatar systems were evaluated on their support for remotely interacting with humans, exploring new environments, and employing specialized skills.","This article describes the competition stages with tasks and evaluation procedures, reports the results, presents the winning teams' approaches, and discusses lessons learned."],"url":"http://arxiv.org/abs/2308.07878v1"}
{"created":"2023-08-15 16:41:53","title":"Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT","abstract":"Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classification. ZSP demonstrates competitive performance compared to supervised BERT models, positioning it as a valuable tool for event record validation and ontology development. Our work underscores the potential of leveraging transfer learning and existing expertise to enhance the efficiency and scalability of research in the field.","sentences":["Recent supervised models for event coding vastly outperform pattern-matching methods.","However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification.","To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks.","Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP.","ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels.","This framework improves interpretability, efficiency, and adaptability to schema changes.","By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP.","ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classification.","ZSP demonstrates competitive performance compared to supervised BERT models, positioning it as a valuable tool for event record validation and ontology development.","Our work underscores the potential of leveraging transfer learning and existing expertise to enhance the efficiency and scalability of research in the field."],"url":"http://arxiv.org/abs/2308.07876v1"}
{"created":"2023-08-15 16:40:46","title":"SEDA: Self-Ensembling ViT with Defensive Distillation and Adversarial Training for robust Chest X-rays Classification","abstract":"Deep Learning methods have recently seen increased adoption in medical imaging applications. However, elevated vulnerabilities have been explored in recent Deep Learning solutions, which can hinder future adoption. Particularly, the vulnerability of Vision Transformer (ViT) to adversarial, privacy, and confidentiality attacks raise serious concerns about their reliability in medical settings. This work aims to enhance the robustness of self-ensembling ViTs for the tuberculosis chest x-ray classification task. We propose Self-Ensembling ViT with defensive Distillation and Adversarial training (SEDA). SEDA utilizes efficient CNN blocks to learn spatial features with various levels of abstraction from feature representations extracted from intermediate ViT blocks, that are largely unaffected by adversarial perturbations. Furthermore, SEDA leverages adversarial training in combination with defensive distillation for improved robustness against adversaries. Training using adversarial examples leads to better model generalizability and improves its ability to handle perturbations. Distillation using soft probabilities introduces uncertainty and variation into the output probabilities, making it more difficult for adversarial and privacy attacks. Extensive experiments performed with the proposed architecture and training paradigm on publicly available Tuberculosis x-ray dataset shows SOTA efficacy of SEDA compared to SEViT in terms of computational efficiency with 70x times lighter framework and enhanced robustness of +9%.","sentences":["Deep Learning methods have recently seen increased adoption in medical imaging applications.","However, elevated vulnerabilities have been explored in recent Deep Learning solutions, which can hinder future adoption.","Particularly, the vulnerability of Vision Transformer (ViT) to adversarial, privacy, and confidentiality attacks raise serious concerns about their reliability in medical settings.","This work aims to enhance the robustness of self-ensembling ViTs for the tuberculosis chest x-ray classification task.","We propose Self-Ensembling ViT with defensive Distillation and Adversarial training (SEDA).","SEDA utilizes efficient CNN blocks to learn spatial features with various levels of abstraction from feature representations extracted from intermediate ViT blocks, that are largely unaffected by adversarial perturbations.","Furthermore, SEDA leverages adversarial training in combination with defensive distillation for improved robustness against adversaries.","Training using adversarial examples leads to better model generalizability and improves its ability to handle perturbations.","Distillation using soft probabilities introduces uncertainty and variation into the output probabilities, making it more difficult for adversarial and privacy attacks.","Extensive experiments performed with the proposed architecture and training paradigm on publicly available Tuberculosis x-ray dataset shows SOTA efficacy of SEDA compared to SEViT in terms of computational efficiency with 70x times lighter framework and enhanced robustness of +9%."],"url":"http://arxiv.org/abs/2308.07874v1"}
{"created":"2023-08-15 16:40:10","title":"Near-Optimal Last-iterate Convergence of Policy Optimization in Zero-sum Polymatrix Markov games","abstract":"Computing approximate Nash equilibria in multi-player general-sum Markov games is a computationally intractable task. However, multi-player Markov games with certain cooperative or competitive structures might circumvent this intractability. In this paper, we focus on multi-player zero-sum polymatrix Markov games, where players interact in a pairwise fashion while remain overall competitive. To the best of our knowledge, we propose the first policy optimization algorithm called Entropy-Regularized Optimistic-Multiplicative-Weights-Update (ER-OMWU) for finding approximate Nash equilibria in finite-horizon zero-sum polymatrix Markov games with full information feedback. We provide last-iterate convergence guarantees for finding an $\\epsilon$-approximate Nash equilibrium within $\\tilde{O}(1/\\epsilon)$ iterations, which is near-optimal compared to the optimal $O(1/\\epsilon)$ iteration complexity in two-player zero-sum Markov games, which is a degenerate case of zero-sum polymatrix games with only two players involved. Our algorithm combines the regularized and optimistic learning dynamics with separated smooth value update within a single loop, where players update strategies in a symmetric and almost uncoupled manner. It provides a natural dynamics for finding equilibria and is more probable to be adapted to a sample-efficient and fully decentralized implementation where only partial information feedback is available in the future.","sentences":["Computing approximate Nash equilibria in multi-player general-sum Markov games is a computationally intractable task.","However, multi-player Markov games with certain cooperative or competitive structures might circumvent this intractability.","In this paper, we focus on multi-player zero-sum polymatrix Markov games, where players interact in a pairwise fashion while remain overall competitive.","To the best of our knowledge, we propose the first policy optimization algorithm called Entropy-Regularized Optimistic-Multiplicative-Weights-Update (ER-OMWU) for finding approximate Nash equilibria in finite-horizon zero-sum polymatrix Markov games with full information feedback.","We provide last-iterate convergence guarantees for finding an $\\epsilon$-approximate Nash equilibrium within $\\tilde{O}(1/\\epsilon)$ iterations, which is near-optimal compared to the optimal $O(1/\\epsilon)$ iteration complexity in two-player zero-sum Markov games, which is a degenerate case of zero-sum polymatrix games with only two players involved.","Our algorithm combines the regularized and optimistic learning dynamics with separated smooth value update within a single loop, where players update strategies in a symmetric and almost uncoupled manner.","It provides a natural dynamics for finding equilibria and is more probable to be adapted to a sample-efficient and fully decentralized implementation where only partial information feedback is available in the future."],"url":"http://arxiv.org/abs/2308.07873v1"}
{"created":"2023-08-15 16:39:10","title":"Emotion Embeddings $\\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets","abstract":"Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc. Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis. Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types. This article presents such a unifying computational model. We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label formats, and even disparate model architectures. Experiments on a wide range of heterogeneous affective datasets indicate that this approach yields the desired interoperability for the sake of reusability, interpretability and flexibility, without penalizing prediction quality. Code and data are archived under https://doi.org/10.5281/zenodo.7405327 .","sentences":["Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc.","Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis.","Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types.","This article presents such a unifying computational model.","We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label formats, and even disparate model architectures.","Experiments on a wide range of heterogeneous affective datasets indicate that this approach yields the desired interoperability for the sake of reusability, interpretability and flexibility, without penalizing prediction quality.","Code and data are archived under https://doi.org/10.5281/zenodo.7405327 ."],"url":"http://arxiv.org/abs/2308.07871v1"}
{"created":"2023-08-15 16:37:16","title":"Brain-Inspired Computational Intelligence via Predictive Coding","abstract":"Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a powerful inversion scheme for a specific class of continuous-state generative models. With the hope of foregrounding research in this direction, we survey the literature that has contributed to this perspective, highlighting the many ways that PC might play a role in the future of machine learning and computational intelligence at large.","sentences":["Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century.","The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm.","However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility.","It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories.","One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a powerful inversion scheme for a specific class of continuous-state generative models.","With the hope of foregrounding research in this direction, we survey the literature that has contributed to this perspective, highlighting the many ways that PC might play a role in the future of machine learning and computational intelligence at large."],"url":"http://arxiv.org/abs/2308.07870v1"}
{"created":"2023-08-15 16:35:40","title":"ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces","abstract":"In recent years, neural implicit surface reconstruction has emerged as a popular paradigm for multi-view 3D reconstruction. Unlike traditional multi-view stereo approaches, the neural implicit surface-based methods leverage neural networks to represent 3D scenes as signed distance functions (SDFs). However, they tend to disregard the reconstruction of individual objects within the scene, which limits their performance and practical applications. To address this issue, previous work ObjectSDF introduced a nice framework of object-composition neural implicit surfaces, which utilizes 2D instance masks to supervise individual object SDFs. In this paper, we propose a new framework called ObjectSDF++ to overcome the limitations of ObjectSDF. First, in contrast to ObjectSDF whose performance is primarily restricted by its converted semantic field, the core component of our model is an occlusion-aware object opacity rendering formulation that directly volume-renders object opacity to be supervised with instance masks. Second, we design a novel regularization term for object distinction, which can effectively mitigate the issue that ObjectSDF may result in unexpected reconstruction in invisible regions due to the lack of constraint to prevent collisions. Our extensive experiments demonstrate that our novel framework not only produces superior object reconstruction results but also significantly improves the quality of scene reconstruction. Code and more resources can be found in \\url{https://qianyiwu.github.io/objectsdf++}","sentences":["In recent years, neural implicit surface reconstruction has emerged as a popular paradigm for multi-view 3D reconstruction.","Unlike traditional multi-view stereo approaches, the neural implicit surface-based methods leverage neural networks to represent 3D scenes as signed distance functions (SDFs).","However, they tend to disregard the reconstruction of individual objects within the scene, which limits their performance and practical applications.","To address this issue, previous work ObjectSDF introduced a nice framework of object-composition neural implicit surfaces, which utilizes 2D instance masks to supervise individual object SDFs.","In this paper, we propose a new framework called ObjectSDF++ to overcome the limitations of ObjectSDF.","First, in contrast to ObjectSDF whose performance is primarily restricted by its converted semantic field, the core component of our model is an occlusion-aware object opacity rendering formulation that directly volume-renders object opacity to be supervised with instance masks.","Second, we design a novel regularization term for object distinction, which can effectively mitigate the issue that ObjectSDF may result in unexpected reconstruction in invisible regions due to the lack of constraint to prevent collisions.","Our extensive experiments demonstrate that our novel framework not only produces superior object reconstruction results but also significantly improves the quality of scene reconstruction.","Code and more resources can be found in \\url{https://qianyiwu.github.io/objectsdf++}"],"url":"http://arxiv.org/abs/2308.07868v1"}
{"created":"2023-08-15 16:30:49","title":"StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models","abstract":"Content and style (C-S) disentanglement is a fundamental problem and critical challenge of style transfer. Existing approaches based on explicit definitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a new C-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the complementary style information, yielding interpretable and controllable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coordinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further leveraging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and flexible C-S disentanglement and trade-off control. Our work provides new insights into the C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled C-S characteristics.","sentences":["Content and style (C-S) disentanglement is a fundamental problem and critical challenge of style transfer.","Existing approaches based on explicit definitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results.","In this paper, we propose a new C-S disentangled framework for style transfer without using previous assumptions.","The key insight is to explicitly extract the content information and implicitly learn the complementary style information, yielding interpretable and controllable C-S disentanglement and style transfer.","A simple yet effective CLIP-based style disentanglement loss coordinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space.","By further leveraging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and flexible C-S disentanglement and trade-off control.","Our work provides new insights into the C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled C-S characteristics."],"url":"http://arxiv.org/abs/2308.07863v1"}
{"created":"2023-08-15 16:25:47","title":"SplITS: Split Input-to-State Mapping for Effective Firmware Fuzzing","abstract":"Ability to test firmware on embedded devices is critical to discovering vulnerabilities prior to their adversarial exploitation. State-of-the-art automated testing methods rehost firmware in emulators and attempt to facilitate inputs from a diversity of methods (interrupt driven, status polling) and a plethora of devices (such as modems and GPS units). Despite recent progress to tackle peripheral input generation challenges in rehosting, a firmware's expectation of multi-byte magic values supplied from peripheral inputs for string operations still pose a significant roadblock. We solve the impediment posed by multi-byte magic strings in monolithic firmware. We propose feedback mechanisms for input-to-state mapping and retaining seeds for targeted replacement mutations with an efficient method to solve multi-byte comparisons. The feedback allows an efficient search over a combinatorial solution-space. We evaluate our prototype implementation, SplITS, with a diverse set of 21 real-world monolithic firmware binaries used in prior works, and 3 new binaries from popular open source projects. SplITS automatically solves 497% more multi-byte magic strings guarding further execution to uncover new code and bugs compared to state-of-the-art. In 11 of the 12 real-world firmware binaries with string comparisons, including those extensively analyzed by prior works, SplITS outperformed, statistically significantly. We observed up to 161% increase in blocks covered and discovered 6 new bugs that remained guarded by string comparisons. Significantly, deep and difficult to reproduce bugs guarded by comparisons, identified in prior work, were found consistently. To facilitate future research in the field, we release SplITS, the new firmware data sets, and bug analysis at https://github.com/SplITS-Fuzzer","sentences":["Ability to test firmware on embedded devices is critical to discovering vulnerabilities prior to their adversarial exploitation.","State-of-the-art automated testing methods rehost firmware in emulators and attempt to facilitate inputs from a diversity of methods (interrupt driven, status polling) and a plethora of devices (such as modems and GPS units).","Despite recent progress to tackle peripheral input generation challenges in rehosting, a firmware's expectation of multi-byte magic values supplied from peripheral inputs for string operations still pose a significant roadblock.","We solve the impediment posed by multi-byte magic strings in monolithic firmware.","We propose feedback mechanisms for input-to-state mapping and retaining seeds for targeted replacement mutations with an efficient method to solve multi-byte comparisons.","The feedback allows an efficient search over a combinatorial solution-space.","We evaluate our prototype implementation, SplITS, with a diverse set of 21 real-world monolithic firmware binaries used in prior works, and 3 new binaries from popular open source projects.","SplITS automatically solves 497% more multi-byte magic strings guarding further execution to uncover new code and bugs compared to state-of-the-art.","In 11 of the 12 real-world firmware binaries with string comparisons, including those extensively analyzed by prior works, SplITS outperformed, statistically significantly.","We observed up to 161% increase in blocks covered and discovered 6 new bugs that remained guarded by string comparisons.","Significantly, deep and difficult to reproduce bugs guarded by comparisons, identified in prior work, were found consistently.","To facilitate future research in the field, we release SplITS, the new firmware data sets, and bug analysis at https://github.com/SplITS-Fuzzer"],"url":"http://arxiv.org/abs/2308.07860v1"}
{"created":"2023-08-15 16:16:02","title":"Impression-Aware Recommender Systems","abstract":"Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that can be addressed in future works.","sentences":["Novel data sources bring new opportunities to improve the quality of recommender systems.","Impressions are a novel data source containing past recommendations (shown items) and traditional interactions.","Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research.","The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders.","We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies.","We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies.","Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that can be addressed in future works."],"url":"http://arxiv.org/abs/2308.07857v1"}
{"created":"2023-08-15 15:51:52","title":"Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models","abstract":"Large Language Models (LLMs) have led to significant improvements in many tasks across various domains, such as code interpretation, response generation, and ambiguity handling. These LLMs, however, when upgrading, primarily prioritize enhancing user experience while neglecting security, privacy, and safety implications. Consequently, unintended vulnerabilities or biases can be introduced. Previous studies have predominantly focused on specific versions of the models and disregard the potential emergence of new attack vectors targeting the updated versions. Through the lens of adversarial examples within the in-context learning framework, this longitudinal study addresses this gap by conducting a comprehensive assessment of the robustness of successive versions of LLMs, vis-\\`a-vis GPT-3.5. We conduct extensive experiments to analyze and understand the impact of the robustness in two distinct learning categories: zero-shot learning and few-shot learning. Our findings indicate that, in comparison to earlier versions of LLMs, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks. In addition, our study emphasizes the increased effectiveness of synergized adversarial queries in most zero-shot learning and few-shot learning cases. We hope that our study can lead to a more refined assessment of the robustness of LLMs over time and provide valuable insights of these models for both developers and users.","sentences":["Large Language Models (LLMs) have led to significant improvements in many tasks across various domains, such as code interpretation, response generation, and ambiguity handling.","These LLMs, however, when upgrading, primarily prioritize enhancing user experience while neglecting security, privacy, and safety implications.","Consequently, unintended vulnerabilities or biases can be introduced.","Previous studies have predominantly focused on specific versions of the models and disregard the potential emergence of new attack vectors targeting the updated versions.","Through the lens of adversarial examples within the in-context learning framework, this longitudinal study addresses this gap by conducting a comprehensive assessment of the robustness of successive versions of LLMs, vis-\\`a-vis GPT-3.5.","We conduct extensive experiments to analyze and understand the impact of the robustness in two distinct learning categories: zero-shot learning and few-shot learning.","Our findings indicate that, in comparison to earlier versions of LLMs, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks.","In addition, our study emphasizes the increased effectiveness of synergized adversarial queries in most zero-shot learning and few-shot learning cases.","We hope that our study can lead to a more refined assessment of the robustness of LLMs over time and provide valuable insights of these models for both developers and users."],"url":"http://arxiv.org/abs/2308.07847v1"}
{"created":"2023-08-15 15:43:12","title":"Dyadic Reinforcement Learning","abstract":"Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation studies on both toy scenarios and on a realistic test bed constructed from data collected in a mobile health study.","sentences":["Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life.","The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions.","This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support.","In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner.","Here, multiple sets of interventions impact the dyad across multiple time intervals.","The developed dyadic RL is Bayesian and hierarchical.","We formally introduce the problem setup, develop dyadic RL and establish a regret bound.","We demonstrate dyadic RL's empirical performance through simulation studies on both toy scenarios and on a realistic test bed constructed from data collected in a mobile health study."],"url":"http://arxiv.org/abs/2308.07843v1"}
{"created":"2023-08-15 15:34:19","title":"PoFEL: Energy-efficient Consensus for Blockchain-based Hierarchical Federated Learning","abstract":"Facilitated by mobile edge computing, client-edge-cloud hierarchical federated learning (HFL) enables communication-efficient model training in a widespread area but also incurs additional security and privacy challenges from intermediate model aggregations and remains the single point of failure issue. To tackle these challenges, we propose a blockchain-based HFL (BHFL) system that operates a permissioned blockchain among edge servers for model aggregation without the need for a centralized cloud server. The employment of blockchain, however, introduces additional overhead. To enable a compact and efficient workflow, we design a novel lightweight consensus algorithm, named Proof of Federated Edge Learning (PoFEL), to recycle the energy consumed for local model training. Specifically, the leader node is selected by evaluating the intermediate FEL models from all edge servers instead of other energy-wasting but meaningless calculations. This design thus improves the system efficiency compared with traditional BHFL frameworks. To prevent model plagiarism and bribery voting during the consensus process, we propose Hash-based Commitment and Digital Signature (HCDS) and Bayesian Truth Serum-based Voting (BTSV) schemes. Finally, we devise an incentive mechanism to motivate continuous contributions from clients to the learning task. Experimental results demonstrate that our proposed BHFL system with the corresponding consensus protocol and incentive mechanism achieves effectiveness, low computational cost, and fairness.","sentences":["Facilitated by mobile edge computing, client-edge-cloud hierarchical federated learning (HFL) enables communication-efficient model training in a widespread area but also incurs additional security and privacy challenges from intermediate model aggregations and remains the single point of failure issue.","To tackle these challenges, we propose a blockchain-based HFL (BHFL) system that operates a permissioned blockchain among edge servers for model aggregation without the need for a centralized cloud server.","The employment of blockchain, however, introduces additional overhead.","To enable a compact and efficient workflow, we design a novel lightweight consensus algorithm, named Proof of Federated Edge Learning (PoFEL), to recycle the energy consumed for local model training.","Specifically, the leader node is selected by evaluating the intermediate FEL models from all edge servers instead of other energy-wasting but meaningless calculations.","This design thus improves the system efficiency compared with traditional BHFL frameworks.","To prevent model plagiarism and bribery voting during the consensus process, we propose Hash-based Commitment and Digital Signature (HCDS) and Bayesian Truth Serum-based Voting (BTSV) schemes.","Finally, we devise an incentive mechanism to motivate continuous contributions from clients to the learning task.","Experimental results demonstrate that our proposed BHFL system with the corresponding consensus protocol and incentive mechanism achieves effectiveness, low computational cost, and fairness."],"url":"http://arxiv.org/abs/2308.07840v1"}
{"created":"2023-08-15 15:27:42","title":"CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction","abstract":"In this paper, we present a novel shape reconstruction method leveraging diffusion model to generate 3D sparse point cloud for the object captured in a single RGB image. Recent methods typically leverage global embedding or local projection-based features as the condition to guide the diffusion model. However, such strategies fail to consistently align the denoised point cloud with the given image, leading to unstable conditioning and inferior performance. In this paper, we present CCD-3DR, which exploits a novel centered diffusion probabilistic model for consistent local feature conditioning. We constrain the noise and sampled point cloud from the diffusion model into a subspace where the point cloud center remains unchanged during the forward diffusion process and reverse process. The stable point cloud center further serves as an anchor to align each point with its corresponding local projection-based features. Extensive experiments on synthetic benchmark ShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large margin, with over 40% improvement. We also provide results on real-world dataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world applications. Codes will be released soon","sentences":["In this paper, we present a novel shape reconstruction method leveraging diffusion model to generate 3D sparse point cloud for the object captured in a single RGB image.","Recent methods typically leverage global embedding or local projection-based features as the condition to guide the diffusion model.","However, such strategies fail to consistently align the denoised point cloud with the given image, leading to unstable conditioning and inferior performance.","In this paper, we present CCD-3DR, which exploits a novel centered diffusion probabilistic model for consistent local feature conditioning.","We constrain the noise and sampled point cloud from the diffusion model into a subspace where the point cloud center remains unchanged during the forward diffusion process and reverse process.","The stable point cloud center further serves as an anchor to align each point with its corresponding local projection-based features.","Extensive experiments on synthetic benchmark ShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large margin, with over 40% improvement.","We also provide results on real-world dataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world applications.","Codes will be released soon"],"url":"http://arxiv.org/abs/2308.07837v1"}
{"created":"2023-08-15 15:23:36","title":"Simple and Efficient Partial Graph Adversarial Attack: A New Perspective","abstract":"As the study of graph neural networks becomes more intensive and comprehensive, their robustness and security have received great research interest. The existing global attack methods treat all nodes in the graph as their attack targets. Although existing methods have achieved excellent results, there is still considerable space for improvement. The key problem is that the current approaches rigidly follow the definition of global attacks. They ignore an important issue, i.e., different nodes have different robustness and are not equally resilient to attacks. From a global attacker's view, we should arrange the attack budget wisely, rather than wasting them on highly robust nodes. To this end, we propose a totally new method named partial graph attack (PGA), which selects the vulnerable nodes as attack targets. First, to select the vulnerable items, we propose a hierarchical target selection policy, which allows attackers to only focus on easy-to-attack nodes. Then, we propose a cost-effective anchor-picking policy to pick the most promising anchors for adding or removing edges, and a more aggressive iterative greedy-based attack method to perform more efficient attacks. Extensive experimental results demonstrate that PGA can achieve significant improvements in both attack effect and attack efficiency compared to other existing graph global attack methods.","sentences":["As the study of graph neural networks becomes more intensive and comprehensive, their robustness and security have received great research interest.","The existing global attack methods treat all nodes in the graph as their attack targets.","Although existing methods have achieved excellent results, there is still considerable space for improvement.","The key problem is that the current approaches rigidly follow the definition of global attacks.","They ignore an important issue, i.e., different nodes have different robustness and are not equally resilient to attacks.","From a global attacker's view, we should arrange the attack budget wisely, rather than wasting them on highly robust nodes.","To this end, we propose a totally new method named partial graph attack (PGA), which selects the vulnerable nodes as attack targets.","First, to select the vulnerable items, we propose a hierarchical target selection policy, which allows attackers to only focus on easy-to-attack nodes.","Then, we propose a cost-effective anchor-picking policy to pick the most promising anchors for adding or removing edges, and a more aggressive iterative greedy-based attack method to perform more efficient attacks.","Extensive experimental results demonstrate that PGA can achieve significant improvements in both attack effect and attack efficiency compared to other existing graph global attack methods."],"url":"http://arxiv.org/abs/2308.07834v1"}
{"created":"2023-08-15 15:21:36","title":"REFORMS: Reporting Standards for Machine Learning Based Science","abstract":"Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\\textbf{Re}$porting Standards $\\textbf{For}$ $\\textbf{M}$achine Learning Based $\\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility.","sentences":["Machine learning (ML) methods are proliferating in scientific research.","However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability.","These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science.","ML methods are often applied and fail in similar ways across disciplines.","Motivated by this observation, our goal is to provide clear reporting standards for ML-based science.","Drawing from an extensive review of past literature, we present the REFORMS checklist ($\\textbf{Re}$porting Standards $\\textbf{For}$ $\\textbf{M}$achine Learning Based $\\textbf{S}$cience).","It consists of 32 questions and a paired set of guidelines.","REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences.","REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility."],"url":"http://arxiv.org/abs/2308.07832v1"}
{"created":"2023-08-15 15:13:26","title":"A Genetic Algorithm Meta-Heuristic for a Generalized Quadratic Assignment Problem","abstract":"The generalized quadratic assignment problem (GQAP) is one of the hardest problems to solve in the operations research area. The GQAP addressed in this work is defined as the task of minimizing the assignment and transportation costs of assigning a set of facilities to a set of locations. The facilities have different space requirements, and the locations have different space capacities. Multiple facilities can be assigned to each location if the space capacity is not violated. In this work, three instances of GQAP in different situations are presented. Then, a genetic algorithm is developed to solve the GQAP instances. Finally, the local neighborhood search with the steepest descend strategy is constructed and applied to the final solution obtained by the GA, and the final solution is compared with the best solution found by MPL/CPLEX software and reference papers. The results show that the developed GA heuristic is effective for solving the GQAP.","sentences":["The generalized quadratic assignment problem (GQAP) is one of the hardest problems to solve in the operations research area.","The GQAP addressed in this work is defined as the task of minimizing the assignment and transportation costs of assigning a set of facilities to a set of locations.","The facilities have different space requirements, and the locations have different space capacities.","Multiple facilities can be assigned to each location if the space capacity is not violated.","In this work, three instances of GQAP in different situations are presented.","Then, a genetic algorithm is developed to solve the GQAP instances.","Finally, the local neighborhood search with the steepest descend strategy is constructed and applied to the final solution obtained by the GA, and the final solution is compared with the best solution found by MPL/CPLEX software and reference papers.","The results show that the developed GA heuristic is effective for solving the GQAP."],"url":"http://arxiv.org/abs/2308.07828v1"}
{"created":"2023-08-15 15:11:13","title":"Learning Better Keypoints for Multi-Object 6DoF Pose Estimation","abstract":"We investigate the impact of pre-defined keypoints for pose estimation, and found that accuracy and efficiency can be improved by training a graph network to select a set of disperse keypoints with similarly distributed votes. These votes, learned by a regression network to accumulate evidence for the keypoint locations, can be regressed more accurately compared to previous heuristic keypoint algorithms. The proposed KeyGNet, supervised by a combined loss measuring both Wassserstein distance and dispersion, learns the color and geometry features of the target objects to estimate optimal keypoint locations. Experiments demonstrate the keypoints selected by KeyGNet improved the accuracy for all evaluation metrics of all seven datasets tested, for three keypoint voting methods. The challenging Occlusion LINEMOD dataset notably improved ADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR improvement for all objects, of between +1% and +21.5%. There was also a notable increase in performance when transitioning from single object to multiple object training using KeyGNet keypoints, essentially eliminating the SISO-MIMO gap for Occlusion LINEMOD.","sentences":["We investigate the impact of pre-defined keypoints for pose estimation, and found that accuracy and efficiency can be improved by training a graph network to select a set of disperse keypoints with similarly distributed votes.","These votes, learned by a regression network to accumulate evidence for the keypoint locations, can be regressed more accurately compared to previous heuristic keypoint algorithms.","The proposed KeyGNet, supervised by a combined loss measuring both Wassserstein distance and dispersion, learns the color and geometry features of the target objects to estimate optimal keypoint locations.","Experiments demonstrate the keypoints selected by KeyGNet improved the accuracy for all evaluation metrics of all seven datasets tested, for three keypoint voting methods.","The challenging Occlusion LINEMOD dataset notably improved ADD(S)","by +16.4% on PVN3D, and all core BOP datasets showed an AR improvement for all objects, of between +1% and +21.5%.","There was also a notable increase in performance when transitioning from single object to multiple object training using KeyGNet keypoints, essentially eliminating the SISO-MIMO gap for Occlusion LINEMOD."],"url":"http://arxiv.org/abs/2308.07827v1"}
{"created":"2023-08-15 15:07:32","title":"Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves","abstract":"The degradation process of lithium-ion batteries is intricately linked to their entire lifecycle as power sources and energy storage devices, encompassing aspects such as performance delivery and cycling utilization. Consequently, the accurate and expedient estimation or prediction of the aging state of lithium-ion batteries has garnered extensive attention. Nonetheless, prevailing research predominantly concentrates on either aging estimation or prediction, neglecting the dynamic fusion of both facets. This paper proposes a hybrid model for capacity aging estimation and prediction based on deep learning, wherein salient features highly pertinent to aging are extracted from charge and discharge relaxation processes. By amalgamating historical capacity decay data, the model dynamically furnishes estimations of the present capacity and forecasts of future capacity for lithium-ion batteries. Our approach is validated against a novel dataset involving charge and discharge cycles at varying rates. Specifically, under a charging condition of 0.25C, a mean absolute percentage error (MAPE) of 0.29% is achieved. This outcome underscores the model's adeptness in harnessing relaxation processes commonly encountered in the real world and synergizing with historical capacity records within battery management systems (BMS), thereby affording estimations and prognostications of capacity decline with heightened precision.","sentences":["The degradation process of lithium-ion batteries is intricately linked to their entire lifecycle as power sources and energy storage devices, encompassing aspects such as performance delivery and cycling utilization.","Consequently, the accurate and expedient estimation or prediction of the aging state of lithium-ion batteries has garnered extensive attention.","Nonetheless, prevailing research predominantly concentrates on either aging estimation or prediction, neglecting the dynamic fusion of both facets.","This paper proposes a hybrid model for capacity aging estimation and prediction based on deep learning, wherein salient features highly pertinent to aging are extracted from charge and discharge relaxation processes.","By amalgamating historical capacity decay data, the model dynamically furnishes estimations of the present capacity and forecasts of future capacity for lithium-ion batteries.","Our approach is validated against a novel dataset involving charge and discharge cycles at varying rates.","Specifically, under a charging condition of 0.25C, a mean absolute percentage error (MAPE) of 0.29% is achieved.","This outcome underscores the model's adeptness in harnessing relaxation processes commonly encountered in the real world and synergizing with historical capacity records within battery management systems (BMS), thereby affording estimations and prognostications of capacity decline with heightened precision."],"url":"http://arxiv.org/abs/2308.07824v1"}
{"created":"2023-08-15 14:56:37","title":"Deep reinforcement learning for process design: Review and perspective","abstract":"The transformation towards renewable energy and feedstock supply in the chemical industry requires new conceptual process design approaches. Recently, breakthroughs in artificial intelligence offer opportunities to accelerate this transition. Specifically, deep reinforcement learning, a subclass of machine learning, has shown the potential to solve complex decision-making problems and aid sustainable process design. We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward. Moreover, we discuss perspectives on underlying challenges and promising future works to unfold the full potential of reinforcement learning for process design in chemical engineering.","sentences":["The transformation towards renewable energy and feedstock supply in the chemical industry requires new conceptual process design approaches.","Recently, breakthroughs in artificial intelligence offer opportunities to accelerate this transition.","Specifically, deep reinforcement learning, a subclass of machine learning, has shown the potential to solve complex decision-making problems and aid sustainable process design.","We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward.","Moreover, we discuss perspectives on underlying challenges and promising future works to unfold the full potential of reinforcement learning for process design in chemical engineering."],"url":"http://arxiv.org/abs/2308.07822v1"}
{"created":"2023-08-15 14:54:37","title":"A Nearly Quadratic-Time FPTAS for Knapsack","abstract":"We investigate polynomial-time approximation schemes for the classic 0-1 knapsack problem. The previous algorithm by Deng, Jin, and Mao (SODA'23) has approximation factor $1 + \\eps$ with running time $\\widetilde{O}(n + \\frac{1}{\\eps^{2.2}})$. There is a lower Bound of $(n + \\frac{1}{\\eps})^{2-o(1)}$ conditioned on the hypothesis that $(\\min, +)$ has no truly subquadratic algorithm. We close the gap by proposing an approximation scheme that runs in $\\widetilde{O}(n + \\frac{1}{\\eps^2})$ time.","sentences":["We investigate polynomial-time approximation schemes for the classic 0-1 knapsack problem.","The previous algorithm by Deng, Jin, and Mao (SODA'23) has approximation factor $1 + \\eps$ with running time $\\widetilde{O}(n + \\frac{1}{\\eps^{2.2}})$. There is a lower Bound of $(n + \\frac{1}{\\eps})^{2-o(1)}$ conditioned on the hypothesis that $(\\min, +)$ has no truly subquadratic algorithm.","We close the gap by proposing an approximation scheme that runs in $\\widetilde{O}(n + \\frac{1}{\\eps^2})$ time."],"url":"http://arxiv.org/abs/2308.07821v1"}
{"created":"2023-08-15 14:50:12","title":"Quantifying the Cost of Learning in Queueing Systems","abstract":"Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.   In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues. In establishing our results, we propose a unified analysis framework for CLQ that bridges Lyapunov and bandit analysis, which could be of independent interest.","sentences":["Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc.","Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters.","Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems.","This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.   ","In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage.","Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty.","We characterize the CLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues.","In establishing our results, we propose a unified analysis framework for CLQ that bridges Lyapunov and bandit analysis, which could be of independent interest."],"url":"http://arxiv.org/abs/2308.07817v1"}
{"created":"2023-08-15 14:48:23","title":"FedCache: A Knowledge Cache-driven Federated Learning Architecture for Personalized Edge Intelligence","abstract":"Edge Intelligence (EI) allows Artificial Intelligence (AI) applications to run at the edge, where data analysis and decision-making can be performed in real-time and close to data sources. To protect data privacy and unify data silos among end devices in EI, Federated Learning (FL) is proposed for collaborative training of shared AI models across devices without compromising data privacy. However, the prevailing FL approaches cannot guarantee model generalization and adaptation on heterogeneous clients. Recently, Personalized Federated Learning (PFL) has drawn growing awareness in EI, as it enables a productive balance between local-specific training requirements inherent in devices and global-generalized optimization objectives for satisfactory performance. However, most existing PFL methods are based on the Parameters Interaction-based Architecture (PIA) represented by FedAvg, which causes unaffordable communication burdens due to large-scale parameters transmission between devices and the edge server. In contrast, Logits Interaction-based Architecture (LIA) allows to update model parameters with logits transfer and gains the advantages of communication lightweight and heterogeneous on-device model allowance compared to PIA. Nevertheless, previous LIA methods attempt to achieve satisfactory performance either relying on unrealistic public datasets or increasing communication overhead for additional information transmission other than logits. To tackle this dilemma, we propose a knowledge cache-driven PFL architecture, named FedCache, which reserves a knowledge cache on the server for fetching personalized knowledge from the samples with similar hashes to each given on-device sample. During the training phase, ensemble distillation is applied to on-device models for constructive optimization with personalized knowledge transferred from the server-side knowledge cache.","sentences":["Edge Intelligence (EI) allows Artificial Intelligence (AI) applications to run at the edge, where data analysis and decision-making can be performed in real-time and close to data sources.","To protect data privacy and unify data silos among end devices in EI, Federated Learning (FL) is proposed for collaborative training of shared AI models across devices without compromising data privacy.","However, the prevailing FL approaches cannot guarantee model generalization and adaptation on heterogeneous clients.","Recently, Personalized Federated Learning (PFL) has drawn growing awareness in EI, as it enables a productive balance between local-specific training requirements inherent in devices and global-generalized optimization objectives for satisfactory performance.","However, most existing PFL methods are based on the Parameters Interaction-based Architecture (PIA) represented by FedAvg, which causes unaffordable communication burdens due to large-scale parameters transmission between devices and the edge server.","In contrast, Logits Interaction-based Architecture (LIA) allows to update model parameters with logits transfer and gains the advantages of communication lightweight and heterogeneous on-device model allowance compared to PIA.","Nevertheless, previous LIA methods attempt to achieve satisfactory performance either relying on unrealistic public datasets or increasing communication overhead for additional information transmission other than logits.","To tackle this dilemma, we propose a knowledge cache-driven PFL architecture, named FedCache, which reserves a knowledge cache on the server for fetching personalized knowledge from the samples with similar hashes to each given on-device sample.","During the training phase, ensemble distillation is applied to on-device models for constructive optimization with personalized knowledge transferred from the server-side knowledge cache."],"url":"http://arxiv.org/abs/2308.07816v1"}
{"created":"2023-08-15 14:46:32","title":"ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition","abstract":"Class imbalance is a common challenge in real-world recognition tasks, where the majority of classes have few samples, also known as tail classes. We address this challenge with the perspective of generalization and empirically find that the promising Sharpness-Aware Minimization (SAM) fails to address generalization issues under the class-imbalanced setting. Through investigating this specific type of task, we identify that its generalization bottleneck primarily lies in the severe overfitting for tail classes with limited training data. To overcome this bottleneck, we leverage class priors to restrict the generalization scope of the class-agnostic SAM and propose a class-aware smoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With the guidance of class priors, our ImbSAM specifically improves generalization targeting tail classes. We also verify the efficacy of ImbSAM on two prototypical applications of class-imbalanced recognition: long-tailed classification and semi-supervised anomaly detection, where our ImbSAM demonstrates remarkable performance improvements for tail classes and anomaly. Our code implementation is available at https://github.com/cool-xuan/Imbalanced_SAM.","sentences":["Class imbalance is a common challenge in real-world recognition tasks, where the majority of classes have few samples, also known as tail classes.","We address this challenge with the perspective of generalization and empirically find that the promising Sharpness-Aware Minimization (SAM) fails to address generalization issues under the class-imbalanced setting.","Through investigating this specific type of task, we identify that its generalization bottleneck primarily lies in the severe overfitting for tail classes with limited training data.","To overcome this bottleneck, we leverage class priors to restrict the generalization scope of the class-agnostic SAM and propose a class-aware smoothness optimization algorithm named Imbalanced-SAM (ImbSAM).","With the guidance of class priors, our ImbSAM specifically improves generalization targeting tail classes.","We also verify the efficacy of ImbSAM on two prototypical applications of class-imbalanced recognition: long-tailed classification and semi-supervised anomaly detection, where our ImbSAM demonstrates remarkable performance improvements for tail classes and anomaly.","Our code implementation is available at https://github.com/cool-xuan/Imbalanced_SAM."],"url":"http://arxiv.org/abs/2308.07815v1"}
{"created":"2023-08-15 14:33:52","title":"Another virtue of wavelet forests?","abstract":"A wavelet forest for a text $T [1..n]$ over an alphabet $\\sigma$ takes $n H_0 (T) + o (n \\log \\sigma)$ bits of space and supports access and rank on $T$ in $O (\\log \\sigma)$ time. K\\\"arkk\\\"ainen and Puglisi (2011) implicitly introduced wavelet forests and showed that when $T$ is the Burrows-Wheeler Transform (BWT) of a string $S$, then a wavelet forest for $T$ occupies space bounded in terms of higher-order empirical entropies of $S$ even when the forest is implemented with uncompressed bitvectors. In this paper we show experimentally that wavelet forests also have better access locality than wavelet trees and are thus interesting even when higher-order compression is not effective on $S$, or when $T$ is not a BWT at all.","sentences":["A wavelet forest for a text $T","[1..n]$ over an alphabet $\\sigma$ takes $n H_0 (T)","+","o (n \\log \\sigma)$ bits of space and supports access and rank on $T$ in $O (\\log \\sigma)$ time.","K\\\"arkk\\\"ainen and Puglisi (2011) implicitly introduced wavelet forests and showed that when $T$ is the Burrows-Wheeler Transform (BWT) of a string $S$, then a wavelet forest for $T$ occupies space bounded in terms of higher-order empirical entropies of $S$ even when the forest is implemented with uncompressed bitvectors.","In this paper we show experimentally that wavelet forests also have better access locality than wavelet trees and are thus interesting even when higher-order compression is not effective on $S$, or when $T$ is not a BWT at all."],"url":"http://arxiv.org/abs/2308.07809v1"}
{"created":"2023-08-15 14:33:17","title":"Grasp Transfer based on Self-Aligning Implicit Representations of Local Surfaces","abstract":"Objects we interact with and manipulate often share similar parts, such as handles, that allow us to transfer our actions flexibly due to their shared functionality. This work addresses the problem of transferring a grasp experience or a demonstration to a novel object that shares shape similarities with objects the robot has previously encountered. Existing approaches for solving this problem are typically restricted to a specific object category or a parametric shape. Our approach, however, can transfer grasps associated with implicit models of local surfaces shared across object categories. Specifically, we employ a single expert grasp demonstration to learn an implicit local surface representation model from a small dataset of object meshes. At inference time, this model is used to transfer grasps to novel objects by identifying the most geometrically similar surfaces to the one on which the expert grasp is demonstrated. Our model is trained entirely in simulation and is evaluated on simulated and real-world objects that are not seen during training. Evaluations indicate that grasp transfer to unseen object categories using this approach can be successfully performed both in simulation and real-world experiments. The simulation results also show that the proposed approach leads to better spatial precision and grasp accuracy compared to a baseline approach.","sentences":["Objects we interact with and manipulate often share similar parts, such as handles, that allow us to transfer our actions flexibly due to their shared functionality.","This work addresses the problem of transferring a grasp experience or a demonstration to a novel object that shares shape similarities with objects the robot has previously encountered.","Existing approaches for solving this problem are typically restricted to a specific object category or a parametric shape.","Our approach, however, can transfer grasps associated with implicit models of local surfaces shared across object categories.","Specifically, we employ a single expert grasp demonstration to learn an implicit local surface representation model from a small dataset of object meshes.","At inference time, this model is used to transfer grasps to novel objects by identifying the most geometrically similar surfaces to the one on which the expert grasp is demonstrated.","Our model is trained entirely in simulation and is evaluated on simulated and real-world objects that are not seen during training.","Evaluations indicate that grasp transfer to unseen object categories using this approach can be successfully performed both in simulation and real-world experiments.","The simulation results also show that the proposed approach leads to better spatial precision and grasp accuracy compared to a baseline approach."],"url":"http://arxiv.org/abs/2308.07807v1"}
{"created":"2023-08-15 14:32:16","title":"Fairness and Privacy in Federated Learning and Their Implications in Healthcare","abstract":"Currently, many contexts exist where distributed learning is difficult or otherwise constrained by security and communication limitations. One common domain where this is a consideration is in Healthcare where data is often governed by data-use-ordinances like HIPAA. On the other hand, larger sample sizes and shared data models are necessary to allow models to better generalize on account of the potential for more variability and balancing underrepresented classes. Federated learning is a type of distributed learning model that allows data to be trained in a decentralized manner. This, in turn, addresses data security, privacy, and vulnerability considerations as data itself is not shared across a given learning network nodes. Three main challenges to federated learning include node data is not independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and there is the heterogeneity of different clients within a network with respect to dataset bias and size. As the field has grown, the notion of fairness in federated learning has also been introduced through novel implementations. Fairness approaches differ from the standard form of federated learning and also have distinct challenges and considerations for the healthcare domain. This paper endeavors to outline the typical lifecycle of fair federated learning in research as well as provide an updated taxonomy to account for the current state of fairness in implementations. Lastly, this paper provides added insight into the implications and challenges of implementing and supporting fairness in federated learning in the healthcare domain.","sentences":["Currently, many contexts exist where distributed learning is difficult or otherwise constrained by security and communication limitations.","One common domain where this is a consideration is in Healthcare where data is often governed by data-use-ordinances like HIPAA.","On the other hand, larger sample sizes and shared data models are necessary to allow models to better generalize on account of the potential for more variability and balancing underrepresented classes.","Federated learning is a type of distributed learning model that allows data to be trained in a decentralized manner.","This, in turn, addresses data security, privacy, and vulnerability considerations as data itself is not shared across a given learning network nodes.","Three main challenges to federated learning include node data is not independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and there is the heterogeneity of different clients within a network with respect to dataset bias and size.","As the field has grown, the notion of fairness in federated learning has also been introduced through novel implementations.","Fairness approaches differ from the standard form of federated learning and also have distinct challenges and considerations for the healthcare domain.","This paper endeavors to outline the typical lifecycle of fair federated learning in research as well as provide an updated taxonomy to account for the current state of fairness in implementations.","Lastly, this paper provides added insight into the implications and challenges of implementing and supporting fairness in federated learning in the healthcare domain."],"url":"http://arxiv.org/abs/2308.07805v1"}
{"created":"2023-08-15 14:27:46","title":"Neuromorphic Seatbelt State Detection for In-Cabin Monitoring with Event Cameras","abstract":"Neuromorphic vision sensors, or event cameras, differ from conventional cameras in that they do not capture images at a specified rate. Instead, they asynchronously log local brightness changes at each pixel. As a result, event cameras only record changes in a given scene, and do so with very high temporal resolution, high dynamic range, and low power requirements. Recent research has demonstrated how these characteristics make event cameras extremely practical sensors in driver monitoring systems (DMS), enabling the tracking of high-speed eye motion and blinks. This research provides a proof of concept to expand event-based DMS techniques to include seatbelt state detection. Using an event simulator, a dataset of 108,691 synthetic neuromorphic frames of car occupants was generated from a near-infrared (NIR) dataset, and split into training, validation, and test sets for a seatbelt state detection algorithm based on a recurrent convolutional neural network (CNN). In addition, a smaller set of real event data was collected and reserved for testing. In a binary classification task, the fastened/unfastened frames were identified with an F1 score of 0.989 and 0.944 on the simulated and real test sets respectively. When the problem extended to also classify the action of fastening/unfastening the seatbelt, respective F1 scores of 0.964 and 0.846 were achieved.","sentences":["Neuromorphic vision sensors, or event cameras, differ from conventional cameras in that they do not capture images at a specified rate.","Instead, they asynchronously log local brightness changes at each pixel.","As a result, event cameras only record changes in a given scene, and do so with very high temporal resolution, high dynamic range, and low power requirements.","Recent research has demonstrated how these characteristics make event cameras extremely practical sensors in driver monitoring systems (DMS), enabling the tracking of high-speed eye motion and blinks.","This research provides a proof of concept to expand event-based DMS techniques to include seatbelt state detection.","Using an event simulator, a dataset of 108,691 synthetic neuromorphic frames of car occupants was generated from a near-infrared (NIR) dataset, and split into training, validation, and test sets for a seatbelt state detection algorithm based on a recurrent convolutional neural network (CNN).","In addition, a smaller set of real event data was collected and reserved for testing.","In a binary classification task, the fastened/unfastened frames were identified with an F1 score of 0.989 and 0.944 on the simulated and real test sets respectively.","When the problem extended to also classify the action of fastening/unfastening the seatbelt, respective F1 scores of 0.964 and 0.846 were achieved."],"url":"http://arxiv.org/abs/2308.07802v1"}
{"created":"2023-08-15 14:25:53","title":"Handwritten Stenography Recognition and the LION Dataset","abstract":"Purpose: In this paper, we establish a baseline for handwritten stenography recognition, using the novel LION dataset, and investigate the impact of including selected aspects of stenographic theory into the recognition process. We make the LION dataset publicly available with the aim of encouraging future research in handwritten stenography recognition.   Methods: A state-of-the-art text recognition model is trained to establish a baseline. Stenographic domain knowledge is integrated by applying four different encoding methods that transform the target sequence into representations, which approximate selected aspects of the writing system. Results are further improved by integrating a pre-training scheme, based on synthetic data.   Results: The baseline model achieves an average test character error rate (CER) of 29.81% and a word error rate (WER) of 55.14%. Test error rates are reduced significantly by combining stenography-specific target sequence encodings with pre-training and fine-tuning, yielding CERs in the range of 24.5% - 26% and WERs of 44.8% - 48.2%.   Conclusion: The obtained results demonstrate the challenging nature of stenography recognition. Integrating stenography-specific knowledge, in conjunction with pre-training and fine-tuning on synthetic data, yields considerable improvements. Together with our precursor study on the subject, this is the first work to apply modern handwritten text recognition to stenography. The dataset and our code are publicly available via Zenodo.","sentences":["Purpose: In this paper, we establish a baseline for handwritten stenography recognition, using the novel LION dataset, and investigate the impact of including selected aspects of stenographic theory into the recognition process.","We make the LION dataset publicly available with the aim of encouraging future research in handwritten stenography recognition.   ","Methods: A state-of-the-art text recognition model is trained to establish a baseline.","Stenographic domain knowledge is integrated by applying four different encoding methods that transform the target sequence into representations, which approximate selected aspects of the writing system.","Results are further improved by integrating a pre-training scheme, based on synthetic data.   ","Results:","The baseline model achieves an average test character error rate (CER) of 29.81% and a word error rate (WER) of 55.14%.","Test error rates are reduced significantly by combining stenography-specific target sequence encodings with pre-training and fine-tuning, yielding CERs in the range of 24.5% - 26% and WERs of 44.8% - 48.2%.   ","Conclusion:","The obtained results demonstrate the challenging nature of stenography recognition.","Integrating stenography-specific knowledge, in conjunction with pre-training and fine-tuning on synthetic data, yields considerable improvements.","Together with our precursor study on the subject, this is the first work to apply modern handwritten text recognition to stenography.","The dataset and our code are publicly available via Zenodo."],"url":"http://arxiv.org/abs/2308.07799v1"}
{"created":"2023-08-15 14:21:47","title":"Research Software Engineering in 2030","abstract":"This position paper for an invited talk on the \"Future of eScience\" discusses the Research Software Engineering Movement and where it might be in 2030. Because of the authors' experiences, it is aimed globally but with examples that focus on the United States and United Kingdom.","sentences":["This position paper for an invited talk on the \"Future of eScience\" discusses the Research Software Engineering Movement and where it might be in 2030.","Because of the authors' experiences, it is aimed globally but with examples that focus on the United States and United Kingdom."],"url":"http://arxiv.org/abs/2308.07796v1"}
{"created":"2023-08-15 14:21:24","title":"Learning to Identify Critical States for Reinforcement Learning from Videos","abstract":"Recent work on deep reinforcement learning (DRL) has pointed out that algorithmic information about good policies can be extracted from offline data which lack explicit information about executed actions. For example, videos of humans or robots may convey a lot of implicit information about rewarding action sequences, but a DRL machine that wants to profit from watching such videos must first learn by itself to identify and recognize relevant states/actions/rewards. Without relying on ground-truth annotations, our new method called Deep State Identifier learns to predict returns from episodes encoded as videos. Then it uses a kind of mask-based sensitivity analysis to extract/identify important critical states. Extensive experiments showcase our method's potential for understanding and improving agent behavior. The source code and the generated datasets are available at https://github.com/AI-Initiative-KAUST/VideoRLCS.","sentences":["Recent work on deep reinforcement learning (DRL) has pointed out that algorithmic information about good policies can be extracted from offline data which lack explicit information about executed actions.","For example, videos of humans or robots may convey a lot of implicit information about rewarding action sequences, but a DRL machine that wants to profit from watching such videos must first learn by itself to identify and recognize relevant states/actions/rewards.","Without relying on ground-truth annotations, our new method called Deep State Identifier learns to predict returns from episodes encoded as videos.","Then it uses a kind of mask-based sensitivity analysis to extract/identify important critical states.","Extensive experiments showcase our method's potential for understanding and improving agent behavior.","The source code and the generated datasets are available at https://github.com/AI-Initiative-KAUST/VideoRLCS."],"url":"http://arxiv.org/abs/2308.07795v1"}
{"created":"2023-08-15 14:17:35","title":"Robust Indexing for the Sliced Channel: Almost Optimal Codes for Substitutions and Deletions","abstract":"Encoding data as a set of unordered strings is receiving great attention as it captures one of the basic features of DNA storage systems. However, the challenge of constructing optimal redundancy codes for this channel remained elusive. In this paper, we address this problem and present an order-wise optimal construction of codes that are capable of correcting multiple substitution, deletion, and insertion errors for this channel model. The key ingredient in the code construction is a technique we call robust indexing: simultaneously assigning indices to unordered strings (hence, creating order) and also embedding information in these indices.   The encoded indices are resilient to substitution, deletion, and insertion errors, and therefore, so is the entire code.","sentences":["Encoding data as a set of unordered strings is receiving great attention as it captures one of the basic features of DNA storage systems.","However, the challenge of constructing optimal redundancy codes for this channel remained elusive.","In this paper, we address this problem and present an order-wise optimal construction of codes that are capable of correcting multiple substitution, deletion, and insertion errors for this channel model.","The key ingredient in the code construction is a technique we call robust indexing: simultaneously assigning indices to unordered strings (hence, creating order) and also embedding information in these indices.   ","The encoded indices are resilient to substitution, deletion, and insertion errors, and therefore, so is the entire code."],"url":"http://arxiv.org/abs/2308.07793v1"}
{"created":"2023-08-15 14:16:29","title":"Informed Named Entity Recognition Decoding for Generative Language Models","abstract":"Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, especially in an environment with an unknown entity class set, demonstrating the adaptability of the approach.","sentences":["Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools.","Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models.","Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process.","It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations.","We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, especially in an environment with an unknown entity class set, demonstrating the adaptability of the approach."],"url":"http://arxiv.org/abs/2308.07791v1"}
{"created":"2023-08-15 14:10:56","title":"Infinitary cut-elimination via finite approximations","abstract":"We investigate non-wellfounded proof systems based on parsimonious logic, a weaker variant of linear logic where the exponential modality ! is interpreted as a constructor for streams over finite data. Logical consistency is maintained at a global level by adapting a standard progressing criterion. We present an infinitary version of cut-elimination based on finite approximations, and we prove that, in presence of the progressing criterion, it returns well-defined non-wellfounded proofs at its limit. Furthermore, we show that cut-elimination preserves the progressive criterion and various regularity conditions internalizing degrees of proof-theoretical uniformity. Finally, we provide a denotational semantics for our systems based on the relational model.","sentences":["We investigate non-wellfounded proof systems based on parsimonious logic, a weaker variant of linear logic where the exponential modality !","is interpreted as a constructor for streams over finite data.","Logical consistency is maintained at a global level by adapting a standard progressing criterion.","We present an infinitary version of cut-elimination based on finite approximations, and we prove that, in presence of the progressing criterion, it returns well-defined non-wellfounded proofs at its limit.","Furthermore, we show that cut-elimination preserves the progressive criterion and various regularity conditions internalizing degrees of proof-theoretical uniformity.","Finally, we provide a denotational semantics for our systems based on the relational model."],"url":"http://arxiv.org/abs/2308.07789v1"}
{"created":"2023-08-15 14:07:41","title":"DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding","abstract":"Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations, we further develop a diffusion-based video-to-speech synthesis model, so called DiffV2S, conditioned on those speaker embeddings and the visual representation extracted from the input video. The proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique.","sentences":["Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input.","However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound.","To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information.","Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time.","In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique.","In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time.","Using the extracted vision-guided speaker embedding representations, we further develop a diffusion-based video-to-speech synthesis model, so called DiffV2S, conditioned on those speaker embeddings and the visual representation extracted from the input video.","The proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved.","Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique."],"url":"http://arxiv.org/abs/2308.07787v1"}
{"created":"2023-08-15 14:04:50","title":"Future Video Prediction from a Single Frame for Video Anomaly Detection","abstract":"Video anomaly detection (VAD) is an important but challenging task in computer vision. The main challenge rises due to the rarity of training samples to model all anomaly cases. Hence, semi-supervised anomaly detection methods have gotten more attention, since they focus on modeling normals and they detect anomalies by measuring the deviations from normal patterns. Despite impressive advances of these methods in modeling normal motion and appearance, long-term motion modeling has not been effectively explored so far. Inspired by the abilities of the future frame prediction proxy-task, we introduce the task of future video prediction from a single frame, as a novel proxy-task for video anomaly detection. This proxy-task alleviates the challenges of previous methods in learning longer motion patterns. Moreover, we replace the initial and future raw frames with their corresponding semantic segmentation map, which not only makes the method aware of object class but also makes the prediction task less complex for the model. Extensive experiments on the benchmark datasets (ShanghaiTech, UCSD-Ped1, and UCSD-Ped2) show the effectiveness of the method and the superiority of its performance compared to SOTA prediction-based VAD methods.","sentences":["Video anomaly detection (VAD) is an important but challenging task in computer vision.","The main challenge rises due to the rarity of training samples to model all anomaly cases.","Hence, semi-supervised anomaly detection methods have gotten more attention, since they focus on modeling normals and they detect anomalies by measuring the deviations from normal patterns.","Despite impressive advances of these methods in modeling normal motion and appearance, long-term motion modeling has not been effectively explored so far.","Inspired by the abilities of the future frame prediction proxy-task, we introduce the task of future video prediction from a single frame, as a novel proxy-task for video anomaly detection.","This proxy-task alleviates the challenges of previous methods in learning longer motion patterns.","Moreover, we replace the initial and future raw frames with their corresponding semantic segmentation map, which not only makes the method aware of object class but also makes the prediction task less complex for the model.","Extensive experiments on the benchmark datasets (ShanghaiTech, UCSD-Ped1, and UCSD-Ped2) show the effectiveness of the method and the superiority of its performance compared to SOTA prediction-based VAD methods."],"url":"http://arxiv.org/abs/2308.07783v1"}
{"created":"2023-08-15 13:59:47","title":"Learning Image Deraining Transformer Network with Dynamic Dual Self-Attention","abstract":"Recently, Transformer-based architecture has been introduced into single image deraining task due to its advantage in modeling non-local information. However, existing approaches tend to integrate global features based on a dense self-attention strategy since it tend to uses all similarities of the tokens between the queries and keys. In fact, this strategy leads to ignoring the most relevant information and inducing blurry effect by the irrelevant representations during the feature aggregation. To this end, this paper proposes an effective image deraining Transformer with dynamic dual self-attention (DDSA), which combines both dense and sparse attention strategies to better facilitate clear image reconstruction. Specifically, we only select the most useful similarity values based on top-k approximate calculation to achieve sparse attention. In addition, we also develop a novel spatial-enhanced feed-forward network (SEFN) to further obtain a more accurate representation for achieving high-quality derained results. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed method.","sentences":["Recently, Transformer-based architecture has been introduced into single image deraining task due to its advantage in modeling non-local information.","However, existing approaches tend to integrate global features based on a dense self-attention strategy since it tend to uses all similarities of the tokens between the queries and keys.","In fact, this strategy leads to ignoring the most relevant information and inducing blurry effect by the irrelevant representations during the feature aggregation.","To this end, this paper proposes an effective image deraining Transformer with dynamic dual self-attention (DDSA), which combines both dense and sparse attention strategies to better facilitate clear image reconstruction.","Specifically, we only select the most useful similarity values based on top-k approximate calculation to achieve sparse attention.","In addition, we also develop a novel spatial-enhanced feed-forward network (SEFN) to further obtain a more accurate representation for achieving high-quality derained results.","Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2308.07781v1"}
{"created":"2023-08-15 13:56:29","title":"Do We Fully Understand Students' Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing","abstract":"Knowledge tracing (KT) aims to monitor students' evolving knowledge states through their learning interactions with concept-related questions, and can be indirectly evaluated by predicting how students will perform on future questions. In this paper, we observe that there is a common phenomenon of answer bias, i.e., a highly unbalanced distribution of correct and incorrect answers for each question. Existing models tend to memorize the answer bias as a shortcut for achieving high prediction performance in KT, thereby failing to fully understand students' knowledge states. To address this issue, we approach the KT task from a causality perspective. A causal graph of KT is first established, from which we identify that the impact of answer bias lies in the direct causal effect of questions on students' responses. A novel COunterfactual REasoning (CORE) framework for KT is further proposed, which separately captures the total causal effect and direct causal effect during training, and mitigates answer bias by subtracting the latter from the former in testing. The CORE framework is applicable to various existing KT models, and we implement it based on the prevailing DKT, DKVMN, and AKT models, respectively. Extensive experiments on three benchmark datasets demonstrate the effectiveness of CORE in making the debiased inference for KT.","sentences":["Knowledge tracing (KT) aims to monitor students' evolving knowledge states through their learning interactions with concept-related questions, and can be indirectly evaluated by predicting how students will perform on future questions.","In this paper, we observe that there is a common phenomenon of answer bias, i.e., a highly unbalanced distribution of correct and incorrect answers for each question.","Existing models tend to memorize the answer bias as a shortcut for achieving high prediction performance in KT, thereby failing to fully understand students' knowledge states.","To address this issue, we approach the KT task from a causality perspective.","A causal graph of KT is first established, from which we identify that the impact of answer bias lies in the direct causal effect of questions on students' responses.","A novel COunterfactual REasoning (CORE) framework for KT is further proposed, which separately captures the total causal effect and direct causal effect during training, and mitigates answer bias by subtracting the latter from the former in testing.","The CORE framework is applicable to various existing KT models, and we implement it based on the prevailing DKT, DKVMN, and AKT models, respectively.","Extensive experiments on three benchmark datasets demonstrate the effectiveness of CORE in making the debiased inference for KT."],"url":"http://arxiv.org/abs/2308.07779v1"}
{"created":"2023-08-15 13:53:52","title":"Enhancing Visually-Rich Document Understanding via Layout Structure Modeling","abstract":"In recent years, the use of multi-modal pre-trained Transformers has led to significant advancements in visually-rich document understanding. However, existing models have mainly focused on features such as text and vision while neglecting the importance of layout relationship between text nodes. In this paper, we propose GraphLayoutLM, a novel document understanding model that leverages the modeling of layout structure graph to inject document layout knowledge into the model. GraphLayoutLM utilizes a graph reordering algorithm to adjust the text sequence based on the graph structure. Additionally, our model uses a layout-aware multi-head self-attention layer to learn document layout knowledge. The proposed model enables the understanding of the spatial arrangement of text elements, improving document comprehension. We evaluate our model on various benchmarks, including FUNSD, XFUND and CORD, and achieve state-of-the-art results among these datasets. Our experimental results demonstrate that our proposed method provides a significant improvement over existing approaches and showcases the importance of incorporating layout information into document understanding models. We also conduct an ablation study to investigate the contribution of each component of our model. The results show that both the graph reordering algorithm and the layout-aware multi-head self-attention layer play a crucial role in achieving the best performance.","sentences":["In recent years, the use of multi-modal pre-trained Transformers has led to significant advancements in visually-rich document understanding.","However, existing models have mainly focused on features such as text and vision while neglecting the importance of layout relationship between text nodes.","In this paper, we propose GraphLayoutLM, a novel document understanding model that leverages the modeling of layout structure graph to inject document layout knowledge into the model.","GraphLayoutLM utilizes a graph reordering algorithm to adjust the text sequence based on the graph structure.","Additionally, our model uses a layout-aware multi-head self-attention layer to learn document layout knowledge.","The proposed model enables the understanding of the spatial arrangement of text elements, improving document comprehension.","We evaluate our model on various benchmarks, including FUNSD, XFUND and CORD, and achieve state-of-the-art results among these datasets.","Our experimental results demonstrate that our proposed method provides a significant improvement over existing approaches and showcases the importance of incorporating layout information into document understanding models.","We also conduct an ablation study to investigate the contribution of each component of our model.","The results show that both the graph reordering algorithm and the layout-aware multi-head self-attention layer play a crucial role in achieving the best performance."],"url":"http://arxiv.org/abs/2308.07777v1"}
{"created":"2023-08-15 13:51:03","title":"Hierarchical generative modelling for autonomous robots","abstract":"Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements. We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations. We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control. Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs. This separation of temporal scales also motivates robotics and control. Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs. We use numerical and physical simulation to conduct experiments and to establish the efficacy of this formulation. Using a hierarchical generative model, we show how a humanoid robot can autonomously complete a complex task that necessitates a holistic use of locomotion, manipulation, and grasping. Specifically, we demonstrate the ability of a humanoid robot that can retrieve and transport a box, open and walk through a door to reach the destination, approach and kick a football, while showing robust performance in presence of body damage and ground irregularities. Our findings demonstrated the effectiveness of using human-inspired motor control algorithms, and our method provides a viable hierarchical architecture for the autonomous completion of challenging goal-directed tasks.","sentences":["Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements.","We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations.","We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control.","Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs.","This separation of temporal scales also motivates robotics and control.","Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs.","We use numerical and physical simulation to conduct experiments and to establish the efficacy of this formulation.","Using a hierarchical generative model, we show how a humanoid robot can autonomously complete a complex task that necessitates a holistic use of locomotion, manipulation, and grasping.","Specifically, we demonstrate the ability of a humanoid robot that can retrieve and transport a box, open and walk through a door to reach the destination, approach and kick a football, while showing robust performance in presence of body damage and ground irregularities.","Our findings demonstrated the effectiveness of using human-inspired motor control algorithms, and our method provides a viable hierarchical architecture for the autonomous completion of challenging goal-directed tasks."],"url":"http://arxiv.org/abs/2308.07775v1"}
{"created":"2023-08-15 13:49:12","title":"A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection","abstract":"A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free from learnable parameters, capable of efficiently handling large graphs, and can effectively generate a coarser graph representation while retaining the most significant structural characteristics of the graph. In the decoding stage, we propose an unpooling operation, called LCUnpool, to reconstruct both the structure and nodal features of the original graph. We conduct empirical evaluations of our method on six benchmark datasets using several evaluation metrics, and the results demonstrate its superiority over state-of-the-art anomaly detection approaches.","sentences":["A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information.","However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process.","In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality.","In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term.","By enforcing locality constraints during the coding process, LCPool is designed to be free from learnable parameters, capable of efficiently handling large graphs, and can effectively generate a coarser graph representation while retaining the most significant structural characteristics of the graph.","In the decoding stage, we propose an unpooling operation, called LCUnpool, to reconstruct both the structure and nodal features of the original graph.","We conduct empirical evaluations of our method on six benchmark datasets using several evaluation metrics, and the results demonstrate its superiority over state-of-the-art anomaly detection approaches."],"url":"http://arxiv.org/abs/2308.07774v1"}
{"created":"2023-08-15 13:48:16","title":"MOLE: MOdular Learning FramEwork via Mutual Information Maximization","abstract":"This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.","sentences":["This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE).","This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization.","MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP.","We run experiments on vector-, grid- and graph-type data.","In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data.","Therefore, MOLE has been experimentally proven to be universally applicable to different types of data."],"url":"http://arxiv.org/abs/2308.07772v1"}
{"created":"2023-08-15 13:45:45","title":"Dual-path TokenLearner for Remote Photoplethysmography-based Physiological Measurement with Facial Videos","abstract":"Remote photoplethysmography (rPPG) based physiological measurement is an emerging yet crucial vision task, whose challenge lies in exploring accurate rPPG prediction from facial videos accompanied by noises of illumination variations, facial occlusions, head movements, \\etc, in a non-contact manner. Existing mainstream CNN-based models make efforts to detect physiological signals by capturing subtle color changes in facial regions of interest (ROI) caused by heartbeats. However, such models are constrained by the limited local spatial or temporal receptive fields in the neural units. Unlike them, a native Transformer-based framework called Dual-path TokenLearner (Dual-TL) is proposed in this paper, which utilizes the concept of learnable tokens to integrate both spatial and temporal informative contexts from the global perspective of the video. Specifically, the proposed Dual-TL uses a Spatial TokenLearner (S-TL) to explore associations in different facial ROIs, which promises the rPPG prediction far away from noisy ROI disturbances. Complementarily, a Temporal TokenLearner (T-TL) is designed to infer the quasi-periodic pattern of heartbeats, which eliminates temporal disturbances such as head movements. The two TokenLearners, S-TL and T-TL, are executed in a dual-path mode. This enables the model to reduce noise disturbances for final rPPG signal prediction. Extensive experiments on four physiological measurement benchmark datasets are conducted. The Dual-TL achieves state-of-the-art performances in both intra- and cross-dataset testings, demonstrating its immense potential as a basic backbone for rPPG measurement. The source code is available at \\href{https://github.com/VUT-HFUT/Dual-TL}{https://github.com/VUT-HFUT/Dual-TL}","sentences":["Remote photoplethysmography (rPPG) based physiological measurement is an emerging yet crucial vision task, whose challenge lies in exploring accurate rPPG prediction from facial videos accompanied by noises of illumination variations, facial occlusions, head movements, \\etc, in a non-contact manner.","Existing mainstream CNN-based models make efforts to detect physiological signals by capturing subtle color changes in facial regions of interest (ROI) caused by heartbeats.","However, such models are constrained by the limited local spatial or temporal receptive fields in the neural units.","Unlike them, a native Transformer-based framework called Dual-path TokenLearner (Dual-TL) is proposed in this paper, which utilizes the concept of learnable tokens to integrate both spatial and temporal informative contexts from the global perspective of the video.","Specifically, the proposed Dual-TL uses a Spatial TokenLearner (S-TL) to explore associations in different facial ROIs, which promises the rPPG prediction far away from noisy ROI disturbances.","Complementarily, a Temporal TokenLearner (T-TL) is designed to infer the quasi-periodic pattern of heartbeats, which eliminates temporal disturbances such as head movements.","The two TokenLearners, S-TL and T-TL, are executed in a dual-path mode.","This enables the model to reduce noise disturbances for final rPPG signal prediction.","Extensive experiments on four physiological measurement benchmark datasets are conducted.","The Dual-TL achieves state-of-the-art performances in both intra- and cross-dataset testings, demonstrating its immense potential as a basic backbone for rPPG measurement.","The source code is available at \\href{https://github.com/VUT-HFUT/Dual-TL}{https://github.com/VUT-HFUT/Dual-TL}"],"url":"http://arxiv.org/abs/2308.07771v1"}
{"created":"2023-08-15 13:43:48","title":"Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection","abstract":"Facial Action Unit (AU) detection is a crucial task in affective computing and social robotics as it helps to identify emotions expressed through facial expressions. Anatomically, there are innumerable correlations between AUs, which contain rich information and are vital for AU detection. Previous methods used fixed AU correlations based on expert experience or statistical rules on specific benchmarks, but it is challenging to comprehensively reflect complex correlations between AUs via hand-crafted settings. There are alternative methods that employ a fully connected graph to learn these dependencies exhaustively. However, these approaches can result in a computational explosion and high dependency with a large dataset. To address these challenges, this paper proposes a novel self-adjusting AU-correlation learning (SACL) method with less computation for AU detection. This method adaptively learns and updates AU correlation graphs by efficiently leveraging the characteristics of different levels of AU motion and emotion representation information extracted in different stages of the network. Moreover, this paper explores the role of multi-scale learning in correlation information extraction, and design a simple yet effective multi-scale feature learning (MSFL) method to promote better performance in AU detection. By integrating AU correlation information with multi-scale features, the proposed method obtains a more robust feature representation for the final AU detection. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on widely used AU detection benchmark datasets, with only 28.7\\% and 12.0\\% of the parameters and FLOPs of the best method, respectively. The code for this method is available at \\url{https://github.com/linuxsino/Self-adjusting-AU}.","sentences":["Facial Action Unit (AU) detection is a crucial task in affective computing and social robotics as it helps to identify emotions expressed through facial expressions.","Anatomically, there are innumerable correlations between AUs, which contain rich information and are vital for AU detection.","Previous methods used fixed AU correlations based on expert experience or statistical rules on specific benchmarks, but it is challenging to comprehensively reflect complex correlations between AUs via hand-crafted settings.","There are alternative methods that employ a fully connected graph to learn these dependencies exhaustively.","However, these approaches can result in a computational explosion and high dependency with a large dataset.","To address these challenges, this paper proposes a novel self-adjusting AU-correlation learning (SACL) method with less computation for AU detection.","This method adaptively learns and updates AU correlation graphs by efficiently leveraging the characteristics of different levels of AU motion and emotion representation information extracted in different stages of the network.","Moreover, this paper explores the role of multi-scale learning in correlation information extraction, and design a simple yet effective multi-scale feature learning (MSFL) method to promote better performance in AU detection.","By integrating AU correlation information with multi-scale features, the proposed method obtains a more robust feature representation for the final AU detection.","Extensive experiments show that the proposed method outperforms the state-of-the-art methods on widely used AU detection benchmark datasets, with only 28.7\\% and 12.0\\% of the parameters and FLOPs of the best method, respectively.","The code for this method is available at \\url{https://github.com/linuxsino/Self-adjusting-AU}."],"url":"http://arxiv.org/abs/2308.07770v1"}
{"created":"2023-08-15 13:43:04","title":"The Urban Toolkit: A Grammar-based Framework for Urban Visual Analytics","abstract":"While cities around the world are looking for smart ways to use new advances in data collection, management, and analysis to address their problems, the complex nature of urban issues and the overwhelming amount of available data have posed significant challenges in translating these efforts into actionable insights. In the past few years, urban visual analytics tools have significantly helped tackle these challenges. When analyzing a feature of interest, an urban expert must transform, integrate, and visualize different thematic (e.g., sunlight access, demographic) and physical (e.g., buildings, street networks) data layers, oftentimes across multiple spatial and temporal scales. However, integrating and analyzing these layers require expertise in different fields, increasing development time and effort. This makes the entire visual data exploration and system implementation difficult for programmers and also sets a high entry barrier for urban experts outside of computer science. With this in mind, in this paper, we present the Urban Toolkit (UTK), a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban use cases in mind. In order to facilitate the integration and visualization of different urban data, we also propose the concept of knots to merge thematic and physical urban layers. We evaluate our approach through use cases and a series of interviews with experts and practitioners from different domains, including urban accessibility, urban planning, architecture, and climate science. UTK is available at urbantk.org.","sentences":["While cities around the world are looking for smart ways to use new advances in data collection, management, and analysis to address their problems, the complex nature of urban issues and the overwhelming amount of available data have posed significant challenges in translating these efforts into actionable insights.","In the past few years, urban visual analytics tools have significantly helped tackle these challenges.","When analyzing a feature of interest, an urban expert must transform, integrate, and visualize different thematic (e.g., sunlight access, demographic) and physical (e.g., buildings, street networks) data layers, oftentimes across multiple spatial and temporal scales.","However, integrating and analyzing these layers require expertise in different fields, increasing development time and effort.","This makes the entire visual data exploration and system implementation difficult for programmers and also sets a high entry barrier for urban experts outside of computer science.","With this in mind, in this paper, we present the Urban Toolkit (UTK), a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban use cases in mind.","In order to facilitate the integration and visualization of different urban data, we also propose the concept of knots to merge thematic and physical urban layers.","We evaluate our approach through use cases and a series of interviews with experts and practitioners from different domains, including urban accessibility, urban planning, architecture, and climate science.","UTK is available at urbantk.org."],"url":"http://arxiv.org/abs/2308.07769v1"}
{"created":"2023-08-15 13:35:29","title":"Whale Detection Enhancement through Synthetic Satellite Images","abstract":"With a number of marine populations in rapid decline, collecting and analyzing data about marine populations has become increasingly important to develop effective conservation policies for a wide range of marine animals, including whales. Modern computer vision algorithms allow us to detect whales in images in a wide range of domains, further speeding up and enhancing the monitoring process. However, these algorithms heavily rely on large training datasets, which are challenging and time-consuming to collect particularly in marine or aquatic environments. Recent advances in AI however have made it possible to synthetically create datasets for training machine learning algorithms, thus enabling new solutions that were not possible before. In this work, we present a solution - SeaDroneSim2 benchmark suite, which addresses this challenge by generating aerial, and satellite synthetic image datasets to improve the detection of whales and reduce the effort required for training data collection. We show that we can achieve a 15% performance boost on whale detection compared to using the real data alone for training, by augmenting a 10% real data. We open source both the code of the simulation platform SeaDroneSim2 and the dataset generated through it.","sentences":["With a number of marine populations in rapid decline, collecting and analyzing data about marine populations has become increasingly important to develop effective conservation policies for a wide range of marine animals, including whales.","Modern computer vision algorithms allow us to detect whales in images in a wide range of domains, further speeding up and enhancing the monitoring process.","However, these algorithms heavily rely on large training datasets, which are challenging and time-consuming to collect particularly in marine or aquatic environments.","Recent advances in AI however have made it possible to synthetically create datasets for training machine learning algorithms, thus enabling new solutions that were not possible before.","In this work, we present a solution - SeaDroneSim2 benchmark suite, which addresses this challenge by generating aerial, and satellite synthetic image datasets to improve the detection of whales and reduce the effort required for training data collection.","We show that we can achieve a 15% performance boost on whale detection compared to using the real data alone for training, by augmenting a 10% real data.","We open source both the code of the simulation platform SeaDroneSim2 and the dataset generated through it."],"url":"http://arxiv.org/abs/2308.07766v1"}
{"created":"2023-08-15 13:29:14","title":"NeFL: Nested Federated Learning for Heterogeneous Clients","abstract":"Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.33 improvement on CIFAR-10). Furthermore, we demonstrate NeFL aligns with recent studies in FL.","sentences":["Federated learning (FL) is a promising approach in distributed learning keeping privacy.","However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance.","System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers.","Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture.","We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling.","NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes.","To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters.","NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data.","Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.33 improvement on CIFAR-10).","Furthermore, we demonstrate NeFL aligns with recent studies in FL."],"url":"http://arxiv.org/abs/2308.07761v1"}
{"created":"2023-08-15 13:27:18","title":"Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System","abstract":"With the continuous increase of users and items, conventional recommender systems trained on static datasets can hardly adapt to changing environments. The high-throughput data requires the model to be updated in a timely manner for capturing the user interest dynamics, which leads to the emergence of streaming recommender systems. Due to the prevalence of deep learning-based recommender systems, the embedding layer is widely adopted to represent the characteristics of users, items, and other features in low-dimensional vectors. However, it has been proved that setting an identical and static embedding size is sub-optimal in terms of recommendation performance and memory cost, especially for streaming recommendations. To tackle this problem, we first rethink the streaming model update process and model the dynamic embedding size search as a bandit problem. Then, we analyze and quantify the factors that influence the optimal embedding sizes from the statistics perspective. Based on this, we propose the \\textbf{D}ynamic \\textbf{E}mbedding \\textbf{S}ize \\textbf{S}earch (\\textbf{DESS}) method to minimize the embedding size selection regret on both user and item sides in a non-stationary manner. Theoretically, we obtain a sublinear regret upper bound superior to previous methods. Empirical results across two recommendation tasks on four public datasets also demonstrate that our approach can achieve better streaming recommendation performance with lower memory cost and higher time efficiency.","sentences":["With the continuous increase of users and items, conventional recommender systems trained on static datasets can hardly adapt to changing environments.","The high-throughput data requires the model to be updated in a timely manner for capturing the user interest dynamics, which leads to the emergence of streaming recommender systems.","Due to the prevalence of deep learning-based recommender systems, the embedding layer is widely adopted to represent the characteristics of users, items, and other features in low-dimensional vectors.","However, it has been proved that setting an identical and static embedding size is sub-optimal in terms of recommendation performance and memory cost, especially for streaming recommendations.","To tackle this problem, we first rethink the streaming model update process and model the dynamic embedding size search as a bandit problem.","Then, we analyze and quantify the factors that influence the optimal embedding sizes from the statistics perspective.","Based on this, we propose the \\textbf{D}ynamic \\textbf{E}mbedding \\textbf{S}ize \\textbf{S}earch (\\textbf{DESS}) method to minimize the embedding size selection regret on both user and item sides in a non-stationary manner.","Theoretically, we obtain a sublinear regret upper bound superior to previous methods.","Empirical results across two recommendation tasks on four public datasets also demonstrate that our approach can achieve better streaming recommendation performance with lower memory cost and higher time efficiency."],"url":"http://arxiv.org/abs/2308.07760v1"}
{"created":"2023-08-15 13:19:59","title":"Backward Reasoning in Large Language Models for Verification","abstract":"Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \\citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \\textit{a simple template}, i.e., ``\\textit{\\textbf{If we know the answer of the above question is \\{a candidate answer\\}, what is the value of unknown variable ${\\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three LLMs. Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks.","sentences":["Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks.","Recently, Self-Consistency \\citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected.","In this paper, we propose a novel method to use backward reasoning in verifying candidate answers.","We mask a token in the question by ${\\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \\textit{a simple template}, i.e., ``\\textit{\\textbf{If we know the answer of the above question is \\{a candidate answer\\}, what is the value of unknown variable ${\\bf x}$?}}''","Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct.","We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers.","We conduct extensive experiments on six data sets and three LLMs.","Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks."],"url":"http://arxiv.org/abs/2308.07758v1"}
{"created":"2023-08-15 13:19:17","title":"A Scalable Formal Verification Methodology for Data-Oblivious Hardware","abstract":"The importance of preventing microarchitectural timing side channels in security-critical applications has surged in recent years. Constant-time programming has emerged as a best-practice technique for preventing the leakage of secret information through timing. It is based on the assumption that the timing of certain basic machine instructions is independent of their respective input data. However, whether or not an instruction satisfies this data-independent timing criterion varies between individual processor microarchitectures. In this paper, we propose a novel methodology to formally verify data-oblivious behavior in hardware using standard property checking techniques. The proposed methodology is based on an inductive property that enables scalability even to complex out-of-order cores. We show that proving this inductive property is sufficient to exhaustively verify data-obliviousness at the microarchitectural level. In addition, the paper discusses several techniques that can be used to make the verification process easier and faster. We demonstrate the feasibility of the proposed methodology through case studies on several open-source designs. One case study uncovered a data-dependent timing violation in the extensively verified and highly secure IBEX RISC-V core. In addition to several hardware accelerators and in-order processors, our experiments also include RISC-V BOOM, a complex out-of-order processor, highlighting the scalability of the approach.","sentences":["The importance of preventing microarchitectural timing side channels in security-critical applications has surged in recent years.","Constant-time programming has emerged as a best-practice technique for preventing the leakage of secret information through timing.","It is based on the assumption that the timing of certain basic machine instructions is independent of their respective input data.","However, whether or not an instruction satisfies this data-independent timing criterion varies between individual processor microarchitectures.","In this paper, we propose a novel methodology to formally verify data-oblivious behavior in hardware using standard property checking techniques.","The proposed methodology is based on an inductive property that enables scalability even to complex out-of-order cores.","We show that proving this inductive property is sufficient to exhaustively verify data-obliviousness at the microarchitectural level.","In addition, the paper discusses several techniques that can be used to make the verification process easier and faster.","We demonstrate the feasibility of the proposed methodology through case studies on several open-source designs.","One case study uncovered a data-dependent timing violation in the extensively verified and highly secure IBEX RISC-V core.","In addition to several hardware accelerators and in-order processors, our experiments also include RISC-V BOOM, a complex out-of-order processor, highlighting the scalability of the approach."],"url":"http://arxiv.org/abs/2308.07757v1"}
{"created":"2023-08-15 13:12:19","title":"Self-Supervised Dynamic Hypergraph Recommendation based on Hyper-Relational Knowledge Graph","abstract":"Knowledge graphs (KGs) are commonly used as side information to enhance collaborative signals and improve recommendation quality. In the context of knowledge-aware recommendation (KGR), graph neural networks (GNNs) have emerged as promising solutions for modeling factual and semantic information in KGs. However, the long-tail distribution of entities leads to sparsity in supervision signals, which weakens the quality of item representation when utilizing KG enhancement. Additionally, the binary relation representation of KGs simplifies hyper-relational facts, making it challenging to model complex real-world information. Furthermore, the over-smoothing phenomenon results in indistinguishable representations and information loss. To address these challenges, we propose the SDK (Self-Supervised Dynamic Hypergraph Recommendation based on Hyper-Relational Knowledge Graph) framework. This framework establishes a cross-view hypergraph self-supervised learning mechanism for KG enhancement. Specifically, we model hyper-relational facts in KGs to capture interdependencies between entities under complete semantic conditions. With the refined representation, a hypergraph is dynamically constructed to preserve features in the deep vector space, thereby alleviating the over-smoothing problem. Furthermore, we mine external supervision signals from both the global perspective of the hypergraph and the local perspective of collaborative filtering (CF) to guide the model prediction process. Extensive experiments conducted on different datasets demonstrate the superiority of the SDK framework over state-of-the-art models. The results showcase its ability to alleviate the effects of over-smoothing and supervision signal sparsity.","sentences":["Knowledge graphs (KGs) are commonly used as side information to enhance collaborative signals and improve recommendation quality.","In the context of knowledge-aware recommendation (KGR), graph neural networks (GNNs) have emerged as promising solutions for modeling factual and semantic information in KGs.","However, the long-tail distribution of entities leads to sparsity in supervision signals, which weakens the quality of item representation when utilizing KG enhancement.","Additionally, the binary relation representation of KGs simplifies hyper-relational facts, making it challenging to model complex real-world information.","Furthermore, the over-smoothing phenomenon results in indistinguishable representations and information loss.","To address these challenges, we propose the SDK (Self-Supervised Dynamic Hypergraph Recommendation based on Hyper-Relational Knowledge Graph) framework.","This framework establishes a cross-view hypergraph self-supervised learning mechanism for KG enhancement.","Specifically, we model hyper-relational facts in KGs to capture interdependencies between entities under complete semantic conditions.","With the refined representation, a hypergraph is dynamically constructed to preserve features in the deep vector space, thereby alleviating the over-smoothing problem.","Furthermore, we mine external supervision signals from both the global perspective of the hypergraph and the local perspective of collaborative filtering (CF) to guide the model prediction process.","Extensive experiments conducted on different datasets demonstrate the superiority of the SDK framework over state-of-the-art models.","The results showcase its ability to alleviate the effects of over-smoothing and supervision signal sparsity."],"url":"http://arxiv.org/abs/2308.07752v1"}
{"created":"2023-08-15 13:09:33","title":"CASPNet++: Joint Multi-Agent Motion Prediction","abstract":"The prediction of road users' future motion is a critical task in supporting advanced driver-assistance systems (ADAS). It plays an even more crucial role for autonomous driving (AD) in enabling the planning and execution of safe driving maneuvers. Based on our previous work, Context-Aware Scene Prediction Network (CASPNet), an improved system, CASPNet++, is proposed. In this work, we focus on further enhancing the interaction modeling and scene understanding to support the joint prediction of all road users in a scene using spatiotemporal grids to model future occupancy. Moreover, an instance-based output head is introduced to provide multi-modal trajectories for agents of interest. In extensive quantitative and qualitative analysis, we demonstrate the scalability of CASPNet++ in utilizing and fusing diverse environmental input sources such as HD maps, Radar detection, and Lidar segmentation. Tested on the urban-focused prediction dataset nuScenes, CASPNet++ reaches state-of-the-art performance. The model has been deployed in a testing vehicle, running in real-time with moderate computational resources.","sentences":["The prediction of road users' future motion is a critical task in supporting advanced driver-assistance systems (ADAS).","It plays an even more crucial role for autonomous driving (AD) in enabling the planning and execution of safe driving maneuvers.","Based on our previous work, Context-Aware Scene Prediction Network (CASPNet), an improved system, CASPNet++, is proposed.","In this work, we focus on further enhancing the interaction modeling and scene understanding to support the joint prediction of all road users in a scene using spatiotemporal grids to model future occupancy.","Moreover, an instance-based output head is introduced to provide multi-modal trajectories for agents of interest.","In extensive quantitative and qualitative analysis, we demonstrate the scalability of CASPNet++ in utilizing and fusing diverse environmental input sources such as HD maps, Radar detection, and Lidar segmentation.","Tested on the urban-focused prediction dataset nuScenes, CASPNet++ reaches state-of-the-art performance.","The model has been deployed in a testing vehicle, running in real-time with moderate computational resources."],"url":"http://arxiv.org/abs/2308.07751v1"}
{"created":"2023-08-15 13:00:42","title":"Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model","abstract":"The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT. For preserving background continuity, we put forth a background alignment pipeline, amalgamating insights from segment anything and image inpainting techniques. Furthermore, we propose an inter-frame alignment module that draws inspiration from an auto-regressive pipeline to augment temporal consistency between adjacent frames, where the preceding frame guides the synthesis process of the current frame. Comparisons with state-of-the-art methods demonstrate that Dancing Avatar exhibits the capacity to generate human videos with markedly superior quality, both in terms of human and background fidelity, as well as temporal coherence compared to existing state-of-the-art approaches.","sentences":["The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses.","We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues.","Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion.","The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance.","We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements.","To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module.","This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT.","For preserving background continuity, we put forth a background alignment pipeline, amalgamating insights from segment anything and image inpainting techniques.","Furthermore, we propose an inter-frame alignment module that draws inspiration from an auto-regressive pipeline to augment temporal consistency between adjacent frames, where the preceding frame guides the synthesis process of the current frame.","Comparisons with state-of-the-art methods demonstrate that Dancing Avatar exhibits the capacity to generate human videos with markedly superior quality, both in terms of human and background fidelity, as well as temporal coherence compared to existing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2308.07749v1"}
{"created":"2023-08-15 12:58:06","title":"Exploiting Sparsity in Automotive Radar Object Detection Networks","abstract":"Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems. Radar object detection networks are one fundamental part of such systems. CNN-based object detectors showed good performance in this context, but they require large compute resources. This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources. We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures. We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.","sentences":["Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems.","Radar object detection networks are one fundamental part of such systems.","CNN-based object detectors showed good performance in this context, but they require large compute resources.","This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources.","We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures.","We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0.","Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline."],"url":"http://arxiv.org/abs/2308.07748v1"}
{"created":"2023-08-15 12:53:21","title":"A Tight Competitive Ratio for Online Submodular Welfare Maximization","abstract":"In this paper we consider the online Submodular Welfare (SW) problem. In this problem we are given $n$ bidders each equipped with a general (not necessarily monotone) submodular utility and $m$ items that arrive online. The goal is to assign each item, once it arrives, to a bidder or discard it, while maximizing the sum of utilities. When an adversary determines the items' arrival order we present a simple randomized algorithm that achieves a tight competitive ratio of $\\nicefrac{1}{4}$. The algorithm is a specialization of an algorithm due to [Harshaw-Kazemi-Feldman-Karbasi MOR`22], who presented the previously best known competitive ratio of $3-2\\sqrt{2}\\approx 0.171573 $ to the problem. When the items' arrival order is uniformly random, we present a competitive ratio of $\\approx 0.27493$, improving the previously known $\\nicefrac{1}{4}$ guarantee. Our approach for the latter result is based on a better analysis of the (offline) Residual Random Greedy (RRG) algorithm of [Buchbinder-Feldman-Naor-Schwartz SODA`14], which we believe might be of independent interest.","sentences":["In this paper we consider the online Submodular Welfare (SW) problem.","In this problem we are given $n$ bidders each equipped with a general (not necessarily monotone) submodular utility and $m$ items that arrive online.","The goal is to assign each item, once it arrives, to a bidder or discard it, while maximizing the sum of utilities.","When an adversary determines the items' arrival order we present a simple randomized algorithm that achieves a tight competitive ratio of $\\nicefrac{1}{4}$. The algorithm is a specialization of an algorithm due to [Harshaw-Kazemi-Feldman-Karbasi MOR`22], who presented the previously best known competitive ratio of $3-2\\sqrt{2}\\approx 0.171573 $ to the problem.","When the items' arrival order is uniformly random, we present a competitive ratio of $\\approx 0.27493$, improving the previously known $\\nicefrac{1}{4}$ guarantee.","Our approach for the latter result is based on a better analysis of the (offline) Residual Random Greedy (RRG) algorithm of [Buchbinder-Feldman-Naor-Schwartz SODA`14], which we believe might be of independent interest."],"url":"http://arxiv.org/abs/2308.07746v1"}
{"created":"2023-08-15 12:50:06","title":"ChartDETR: A Multi-shape Detection Network for Visual Chart Recognition","abstract":"Visual chart recognition systems are gaining increasing attention due to the growing demand for automatically identifying table headers and values from chart images. Current methods rely on keypoint detection to estimate data element shapes in charts but suffer from grouping errors in post-processing. To address this issue, we propose ChartDETR, a transformer-based multi-shape detector that localizes keypoints at the corners of regular shapes to reconstruct multiple data elements in a single chart image. Our method predicts all data element shapes at once by introducing query groups in set prediction, eliminating the need for further postprocessing. This property allows ChartDETR to serve as a unified framework capable of representing various chart types without altering the network architecture, effectively detecting data elements of diverse shapes. We evaluated ChartDETR on three datasets, achieving competitive results across all chart types without any additional enhancements. For example, ChartDETR achieved an F1 score of 0.98 on Adobe Synthetic, significantly outperforming the previous best model with a 0.71 F1 score. Additionally, we obtained a new state-of-the-art result of 0.97 on ExcelChart400k. The code will be made publicly available.","sentences":["Visual chart recognition systems are gaining increasing attention due to the growing demand for automatically identifying table headers and values from chart images.","Current methods rely on keypoint detection to estimate data element shapes in charts but suffer from grouping errors in post-processing.","To address this issue, we propose ChartDETR, a transformer-based multi-shape detector that localizes keypoints at the corners of regular shapes to reconstruct multiple data elements in a single chart image.","Our method predicts all data element shapes at once by introducing query groups in set prediction, eliminating the need for further postprocessing.","This property allows ChartDETR to serve as a unified framework capable of representing various chart types without altering the network architecture, effectively detecting data elements of diverse shapes.","We evaluated ChartDETR on three datasets, achieving competitive results across all chart types without any additional enhancements.","For example, ChartDETR achieved an F1 score of 0.98 on Adobe Synthetic, significantly outperforming the previous best model with a 0.71 F1 score.","Additionally, we obtained a new state-of-the-art result of 0.97 on ExcelChart400k.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2308.07743v1"}
{"created":"2023-08-15 12:40:56","title":"Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World","abstract":"Experimentation on real robots is demanding in terms of time and costs. For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms. However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment. The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.   In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions. We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets. An extensive software documentation and an initial stage based on a simulation of the real set-up made the competition particularly accessible. By giving each team plenty of access budget to evaluate their offline-learned policies on a cluster of seven identical real TriFinger platforms, we organized an exciting competition for machine learners and roboticists alike.   In this work we state the rules of the competition, present the methods used by the winning teams and compare their results with a benchmark of state-of-the-art offline RL algorithms on the challenge datasets.","sentences":["Experimentation on real robots is demanding in terms of time and costs.","For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms.","However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment.","The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.   ","In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions.","We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets.","An extensive software documentation and an initial stage based on a simulation of the real set-up made the competition particularly accessible.","By giving each team plenty of access budget to evaluate their offline-learned policies on a cluster of seven identical real TriFinger platforms, we organized an exciting competition for machine learners and roboticists alike.   ","In this work we state the rules of the competition, present the methods used by the winning teams and compare their results with a benchmark of state-of-the-art offline RL algorithms on the challenge datasets."],"url":"http://arxiv.org/abs/2308.07741v1"}
{"created":"2023-08-15 12:33:58","title":"Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods","abstract":"We study how to efficiently combine formal methods, Monte Carlo Tree Search (MCTS), and deep learning in order to produce high-quality receding horizon policies in large Markov Decision processes (MDPs). In particular, we use model-checking techniques to guide the MCTS algorithm in order to generate offline samples of high-quality decisions on a representative set of states of the MDP. Those samples can then be used to train a neural network that imitates the policy used to generate them. This neural network can either be used as a guide on a lower-latency MCTS online search, or alternatively be used as a full-fledged policy when minimal latency is required. We use statistical model checking to detect when additional samples are needed and to focus those additional samples on configurations where the learnt neural network policy differs from the (computationally-expensive) offline policy. We illustrate the use of our method on MDPs that model the Frozen Lake and Pac-Man environments -- two popular benchmarks to evaluate reinforcement-learning algorithms.","sentences":["We study how to efficiently combine formal methods, Monte Carlo Tree Search (MCTS), and deep learning in order to produce high-quality receding horizon policies in large Markov Decision processes (MDPs).","In particular, we use model-checking techniques to guide the MCTS algorithm in order to generate offline samples of high-quality decisions on a representative set of states of the MDP.","Those samples can then be used to train a neural network that imitates the policy used to generate them.","This neural network can either be used as a guide on a lower-latency MCTS online search, or alternatively be used as a full-fledged policy when minimal latency is required.","We use statistical model checking to detect when additional samples are needed and to focus those additional samples on configurations where the learnt neural network policy differs from the (computationally-expensive) offline policy.","We illustrate the use of our method on MDPs that model the Frozen Lake and Pac-Man environments -- two popular benchmarks to evaluate reinforcement-learning algorithms."],"url":"http://arxiv.org/abs/2308.07738v1"}
{"created":"2023-08-15 12:30:22","title":"Identity-Consistent Aggregation for Video Object Detection","abstract":"In Video Object Detection (VID), a common practice is to leverage the rich temporal contexts from the video to enhance the object representations in each frame. Existing methods treat the temporal contexts obtained from different objects indiscriminately and ignore their different identities. While intuitively, aggregating local views of the same object in different frames may facilitate a better understanding of the object. Thus, in this paper, we aim to enable the model to focus on the identity-consistent temporal contexts of each object to obtain more comprehensive object representations and handle the rapid object appearance variations such as occlusion, motion blur, etc. However, realizing this goal on top of existing VID models faces low-efficiency problems due to their redundant region proposals and nonparallel frame-wise prediction manner. To aid this, we propose ClipVID, a VID model equipped with Identity-Consistent Aggregation (ICA) layers specifically designed for mining fine-grained and identity-consistent temporal contexts. It effectively reduces the redundancies through the set prediction strategy, making the ICA layers very efficient and further allowing us to design an architecture that makes parallel clip-wise predictions for the whole video clip. Extensive experimental results demonstrate the superiority of our method: a state-of-the-art (SOTA) performance (84.7% mAP) on the ImageNet VID dataset while running at a speed about 7x faster (39.3 fps) than previous SOTAs.","sentences":["In Video Object Detection (VID), a common practice is to leverage the rich temporal contexts from the video to enhance the object representations in each frame.","Existing methods treat the temporal contexts obtained from different objects indiscriminately and ignore their different identities.","While intuitively, aggregating local views of the same object in different frames may facilitate a better understanding of the object.","Thus, in this paper, we aim to enable the model to focus on the identity-consistent temporal contexts of each object to obtain more comprehensive object representations and handle the rapid object appearance variations such as occlusion, motion blur, etc.","However, realizing this goal on top of existing VID models faces low-efficiency problems due to their redundant region proposals and nonparallel frame-wise prediction manner.","To aid this, we propose ClipVID, a VID model equipped with Identity-Consistent Aggregation (ICA) layers specifically designed for mining fine-grained and identity-consistent temporal contexts.","It effectively reduces the redundancies through the set prediction strategy, making the ICA layers very efficient and further allowing us to design an architecture that makes parallel clip-wise predictions for the whole video clip.","Extensive experimental results demonstrate the superiority of our method: a state-of-the-art (SOTA) performance (84.7% mAP) on the ImageNet VID dataset while running at a speed about 7x faster (39.3 fps) than previous SOTAs."],"url":"http://arxiv.org/abs/2308.07737v1"}
{"created":"2023-08-15 12:30:15","title":"Swarm Bug Algorithms for Path Generation in Unknown Environments","abstract":"In this paper, we consider the problem of a swarm traveling between two points as fast as possible in an unknown environment cluttered with obstacles. Potential applications include search-and-rescue operations where damaged environments are typical. We present swarm generalizations, called SwarmCom, SwarmBug1, and SwarmBug2, of the classical path generation algorithms Com, Bug1, and Bug2. These algorithms were developed for unknown environments and require low computational power and memory storage, thereby freeing up resources for other tasks. We show the upper bound of the worst-case travel time for the first agent in the swarm to reach the target point for SwarmBug1. For SwarmBug2, we show that the algorithm underperforms in terms of worst-case travel time compared to SwarmBug1. For SwarmCom, we show that there exists a trivial scene for which the algorithm will not halt, and it thus has no performance guarantees. Moreover, by comparing the upper bound of the travel time for SwarmBug1 with a universal lower bound for any path generation algorithm, it is shown that in the limit when the number of agents in the swarm approaches infinity, no other algorithm has strictly better worst-case performance than SwarmBug1 and the universal lower bound is tight.","sentences":["In this paper, we consider the problem of a swarm traveling between two points as fast as possible in an unknown environment cluttered with obstacles.","Potential applications include search-and-rescue operations where damaged environments are typical.","We present swarm generalizations, called SwarmCom, SwarmBug1, and SwarmBug2, of the classical path generation algorithms Com, Bug1, and Bug2.","These algorithms were developed for unknown environments and require low computational power and memory storage, thereby freeing up resources for other tasks.","We show the upper bound of the worst-case travel time for the first agent in the swarm to reach the target point for SwarmBug1.","For SwarmBug2, we show that the algorithm underperforms in terms of worst-case travel time compared to SwarmBug1.","For SwarmCom, we show that there exists a trivial scene for which the algorithm will not halt, and it thus has no performance guarantees.","Moreover, by comparing the upper bound of the travel time for SwarmBug1 with a universal lower bound for any path generation algorithm, it is shown that in the limit when the number of agents in the swarm approaches infinity, no other algorithm has strictly better worst-case performance than SwarmBug1 and the universal lower bound is tight."],"url":"http://arxiv.org/abs/2308.07736v1"}
{"created":"2023-08-15 12:13:44","title":"UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation","abstract":"Jointly processing information from multiple sensors is crucial to achieving accurate and robust perception for reliable autonomous driving systems. However, current 3D perception research follows a modality-specific paradigm, leading to additional computation overheads and inefficient collaboration between different sensor data. In this paper, we present an efficient multi-modal backbone for outdoor 3D perception named UniTR, which processes a variety of modalities with unified modeling and shared parameters. Unlike previous works, UniTR introduces a modality-agnostic transformer encoder to handle these view-discrepant sensor data for parallel modal-wise representation learning and automatic cross-modal interaction without additional fusion steps. More importantly, to make full use of these complementary sensor types, we present a novel multi-modal integration strategy by both considering semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood relations. UniTR is also a fundamentally task-agnostic backbone that naturally supports different 3D perception tasks. It sets a new state-of-the-art performance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object detection and +12.0 higher mIoU for BEV map segmentation with lower inference latency. Code will be available at https://github.com/Haiyang-W/UniTR .","sentences":["Jointly processing information from multiple sensors is crucial to achieving accurate and robust perception for reliable autonomous driving systems.","However, current 3D perception research follows a modality-specific paradigm, leading to additional computation overheads and inefficient collaboration between different sensor data.","In this paper, we present an efficient multi-modal backbone for outdoor 3D perception named UniTR, which processes a variety of modalities with unified modeling and shared parameters.","Unlike previous works, UniTR introduces a modality-agnostic transformer encoder to handle these view-discrepant sensor data for parallel modal-wise representation learning and automatic cross-modal interaction without additional fusion steps.","More importantly, to make full use of these complementary sensor types, we present a novel multi-modal integration strategy by both considering semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood relations.","UniTR is also a fundamentally task-agnostic backbone that naturally supports different 3D perception tasks.","It sets a new state-of-the-art performance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object detection and +12.0 higher mIoU for BEV map segmentation with lower inference latency.","Code will be available at https://github.com/Haiyang-W/UniTR ."],"url":"http://arxiv.org/abs/2308.07732v1"}
{"created":"2023-08-15 12:11:33","title":"Context-Aware Pseudo-Label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation","abstract":"In the domain adaptation problem, source data may be unavailable to the target client side due to privacy or intellectual property issues. Source-free unsupervised domain adaptation (SF-UDA) aims at adapting a model trained on the source side to align the target distribution with only the source model and unlabeled target data. The source model usually produces noisy and context-inconsistent pseudo-labels on the target domain, i.e., neighbouring regions that have a similar visual appearance are annotated with different pseudo-labels. This observation motivates us to refine pseudo-labels with context relations. Another observation is that features of the same class tend to form a cluster despite the domain gap, which implies context relations can be readily calculated from feature distances. To this end, we propose a context-aware pseudo-label refinement method for SF-UDA. Specifically, a context-similarity learning module is developed to learn context relations. Next, pseudo-label revision is designed utilizing the learned context relations. Further, we propose calibrating the revised pseudo-labels to compensate for wrong revision caused by inaccurate context relations. Additionally, we adopt a pixel-level and class-level denoising scheme to select reliable pseudo-labels for domain adaptation. Experiments on cross-domain fundus images indicate that our approach yields the state-of-the-art results. Code is available at https://github.com/xmed-lab/CPR.","sentences":["In the domain adaptation problem, source data may be unavailable to the target client side due to privacy or intellectual property issues.","Source-free unsupervised domain adaptation (SF-UDA) aims at adapting a model trained on the source side to align the target distribution with only the source model and unlabeled target data.","The source model usually produces noisy and context-inconsistent pseudo-labels on the target domain, i.e., neighbouring regions that have a similar visual appearance are annotated with different pseudo-labels.","This observation motivates us to refine pseudo-labels with context relations.","Another observation is that features of the same class tend to form a cluster despite the domain gap, which implies context relations can be readily calculated from feature distances.","To this end, we propose a context-aware pseudo-label refinement method for SF-UDA.","Specifically, a context-similarity learning module is developed to learn context relations.","Next, pseudo-label revision is designed utilizing the learned context relations.","Further, we propose calibrating the revised pseudo-labels to compensate for wrong revision caused by inaccurate context relations.","Additionally, we adopt a pixel-level and class-level denoising scheme to select reliable pseudo-labels for domain adaptation.","Experiments on cross-domain fundus images indicate that our approach yields the state-of-the-art results.","Code is available at https://github.com/xmed-lab/CPR."],"url":"http://arxiv.org/abs/2308.07731v1"}
{"created":"2023-08-15 12:08:43","title":"Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability","abstract":"Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor. By leveraging batch normalization layers and integrating linear probing and fine-tuning, our DAFT significantly mitigates feature distortion and achieves improved model performance on both in-distribution and out-of-distribution datasets. Extensive experiments demonstrate that our method outperforms other baseline methods, demonstrating its effectiveness in not only improving performance but also mitigating feature distortion.","sentences":["Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains.","However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities.","Mitigating feature distortion during adaptation to new target domains is crucial.","Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning.","Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance.","In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning.","Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning.","Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor.","By leveraging batch normalization layers and integrating linear probing and fine-tuning, our DAFT significantly mitigates feature distortion and achieves improved model performance on both in-distribution and out-of-distribution datasets.","Extensive experiments demonstrate that our method outperforms other baseline methods, demonstrating its effectiveness in not only improving performance but also mitigating feature distortion."],"url":"http://arxiv.org/abs/2308.07728v1"}
{"created":"2023-08-15 11:55:35","title":"Extended Preintegration for Relative State Estimation of Leader-Follower Platform","abstract":"Relative state estimation using exteroceptive sensors suffers from limitations of the field of view (FOV) and false detection, that the proprioceptive sensor (IMU) data are usually engaged to compensate. Recently ego-motion constraint obtained by Inertial measurement unit (IMU) preintegration has been extensively used in simultaneous localization and mapping (SLAM) to alleviate the computation burden. This paper introduces an extended preintegration incorporating the IMU preintegration of two platforms to formulate the motion constraint of relative state. One merit of this analytic constraint is that it can be seamlessly integrated into the unified graph optimization framework to implement the relative state estimation in a high-performance real-time tracking thread, another point is a full smoother design with this precise constraint to optimize the 3D coordinate and refine the state for the refinement thread. We compare extensively in simulations the proposed algorithms with two existing approaches to confirm our outperformance. In the real virtual reality (VR) application design with the proposed estimator, we properly realize the visual tracking of the six degrees of freedom (6DoF) controller suitable for almost all scenarios, including the challenging environment with missing features, light mutation, dynamic scenes, etc. The demo video is at https://www.youtube.com/watch?v=0idb9Ls2iAM. For the benefit of the community, we make the source code public.","sentences":["Relative state estimation using exteroceptive sensors suffers from limitations of the field of view (FOV) and false detection, that the proprioceptive sensor (IMU) data are usually engaged to compensate.","Recently ego-motion constraint obtained by Inertial measurement unit (IMU) preintegration has been extensively used in simultaneous localization and mapping (SLAM) to alleviate the computation burden.","This paper introduces an extended preintegration incorporating the IMU preintegration of two platforms to formulate the motion constraint of relative state.","One merit of this analytic constraint is that it can be seamlessly integrated into the unified graph optimization framework to implement the relative state estimation in a high-performance real-time tracking thread, another point is a full smoother design with this precise constraint to optimize the 3D coordinate and refine the state for the refinement thread.","We compare extensively in simulations the proposed algorithms with two existing approaches to confirm our outperformance.","In the real virtual reality (VR) application design with the proposed estimator, we properly realize the visual tracking of the six degrees of freedom (6DoF) controller suitable for almost all scenarios, including the challenging environment with missing features, light mutation, dynamic scenes, etc.","The demo video is at https://www.youtube.com/watch?v=0idb9Ls2iAM.","For the benefit of the community, we make the source code public."],"url":"http://arxiv.org/abs/2308.07723v1"}
{"created":"2023-08-15 11:54:08","title":"Improved Lower Bound for Estimating the Number of Defective Items","abstract":"Let $X$ be a set of items of size $n$ that contains some defective items, denoted by $I$, where $I \\subseteq X$. In group testing, a {\\it test} refers to a subset of items $Q \\subset X$. The outcome of a test is $1$ if $Q$ contains at least one defective item, i.e., $Q\\cap I \\neq \\emptyset$, and $0$ otherwise.   We give a novel approach to obtaining lower bounds in non-adaptive randomized group testing. The technique produced lower bounds that are within a factor of $1/{\\log\\log\\stackrel{k}{\\cdots}\\log n}$ of the existing upper bounds for any constant~$k$. Employing this new method, we can prove the following result.   For any fixed constants $k$, any non-adaptive randomized algorithm that, for any set of defective items $I$, with probability at least $2/3$, returns an estimate of the number of defective items $|I|$ to within a constant factor requires at least $$\\Omega\\left(\\frac{\\log n}{\\log\\log\\stackrel{k}{\\cdots}\\log n}\\right)$$ tests.   Our result almost matches the upper bound of $O(\\log n)$ and solves the open problem posed by Damaschke and Sheikh Muhammad [COCOA 2010 and Discrete Math., Alg. and Appl., 2010]. Additionally, it improves upon the lower bound of $\\Omega(\\log n/\\log\\log n)$ previously established by Bshouty [ISAAC 2019].","sentences":["Let $X$ be a set of items of size $n$ that contains some defective items, denoted by $I$, where $I \\subseteq X$.","In group testing, a {\\it test} refers to a subset of items $Q \\subset X$.","The outcome of a test is $1$ if $Q$ contains at least one defective item, i.e., $Q\\cap I \\neq \\emptyset$, and $0$ otherwise.   ","We give a novel approach to obtaining lower bounds in non-adaptive randomized group testing.","The technique produced lower bounds that are within a factor of $1/{\\log\\log\\stackrel{k}{\\cdots}\\log n}$ of the existing upper bounds for any constant~$k$.","Employing this new method, we can prove the following result.   ","For any fixed constants $k$, any non-adaptive randomized algorithm that, for any set of defective items $I$, with probability at least $2/3$, returns an estimate of the number of defective items $|I|$ to within a constant factor requires at least $$\\Omega\\left(\\frac{\\log n}{\\log\\log\\stackrel{k}{\\cdots}\\log n}\\right)$$ tests.   ","Our result almost matches the upper bound of $O(\\log n)$ and solves the open problem posed by Damaschke and","Sheikh Muhammad","[COCOA 2010 and Discrete Math., Alg. and Appl., 2010].","Additionally, it improves upon the lower bound of $\\Omega(\\log n/\\log\\log n)$ previously established by Bshouty [ISAAC 2019]."],"url":"http://arxiv.org/abs/2308.07721v1"}
{"created":"2023-08-15 11:50:57","title":"Real-time Automatic M-mode Echocardiography Measurement with Panel Attention from Local-to-Global Pixels","abstract":"Motion mode (M-mode) recording is an essential part of echocardiography to measure cardiac dimension and function. However, the current diagnosis cannot build an automatic scheme, as there are three fundamental obstructs: Firstly, there is no open dataset available to build the automation for ensuring constant results and bridging M-mode echocardiography with real-time instance segmentation (RIS); Secondly, the examination is involving the time-consuming manual labelling upon M-mode echocardiograms; Thirdly, as objects in echocardiograms occupy a significant portion of pixels, the limited receptive field in existing backbones (e.g., ResNet) composed from multiple convolution layers are inefficient to cover the period of a valve movement. Existing non-local attentions (NL) compromise being unable real-time with a high computation overhead or losing information from a simplified version of the non-local block. Therefore, we proposed RAMEM, a real-time automatic M-mode echocardiography measurement scheme, contributes three aspects to answer the problems: 1) provide MEIS, a dataset of M-mode echocardiograms for instance segmentation, to enable consistent results and support the development of an automatic scheme; 2) propose panel attention, local-to-global efficient attention by pixel-unshuffling, embedding with updated UPANets V2 in a RIS scheme toward big object detection with global receptive field; 3) develop and implement AMEM, an efficient algorithm of automatic M-mode echocardiography measurement enabling fast and accurate automatic labelling among diagnosis. The experimental results show that RAMEM surpasses existing RIS backbones (with non-local attention) in PASCAL 2012 SBD and human performances in real-time MEIS tested. The code of MEIS and dataset are available at https://github.com/hanktseng131415go/RAME.","sentences":["Motion mode (M-mode) recording is an essential part of echocardiography to measure cardiac dimension and function.","However, the current diagnosis cannot build an automatic scheme, as there are three fundamental obstructs: Firstly, there is no open dataset available to build the automation for ensuring constant results and bridging M-mode echocardiography with real-time instance segmentation (RIS); Secondly, the examination is involving the time-consuming manual labelling upon M-mode echocardiograms; Thirdly, as objects in echocardiograms occupy a significant portion of pixels, the limited receptive field in existing backbones (e.g., ResNet) composed from multiple convolution layers are inefficient to cover the period of a valve movement.","Existing non-local attentions (NL) compromise being unable real-time with a high computation overhead or losing information from a simplified version of the non-local block.","Therefore, we proposed RAMEM, a real-time automatic M-mode echocardiography measurement scheme, contributes three aspects to answer the problems: 1) provide MEIS, a dataset of M-mode echocardiograms for instance segmentation, to enable consistent results and support the development of an automatic scheme; 2) propose panel attention, local-to-global efficient attention by pixel-unshuffling, embedding with updated UPANets V2 in a RIS scheme toward big object detection with global receptive field; 3) develop and implement AMEM, an efficient algorithm of automatic M-mode echocardiography measurement enabling fast and accurate automatic labelling among diagnosis.","The experimental results show that RAMEM surpasses existing RIS backbones (with non-local attention) in PASCAL 2012 SBD and human performances in real-time MEIS tested.","The code of MEIS and dataset are available at https://github.com/hanktseng131415go/RAME."],"url":"http://arxiv.org/abs/2308.07717v1"}
