{"created":"2023-06-13","title":"TART: A plug-and-play Transformer module for task-agnostic reasoning","abstract":"Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module. TART trains this reasoning module in a task-agnostic manner using only synthetic logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training. With a single inference module, TART improves performance across different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M - 6B), tasks (14 NLP binary classification tasks), and even across different modalities (audio and vision). Additionally, on the RAFT Benchmark, TART improves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B), and is within 4% of GPT-3 (175B). Our code and models are available at https://github.com/HazyResearch/TART .","sentences":["Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training.","In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task.","In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples.","While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions.","As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks.","This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner?","We answer this in the affirmative and propose TART which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module.","TART trains this reasoning module in a task-agnostic manner using only synthetic logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training.","With a single inference module, TART improves performance across different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M - 6B), tasks (14 NLP binary classification tasks), and even across different modalities (audio and vision).","Additionally, on the RAFT Benchmark, TART improves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B), and is within 4% of GPT-3 (175B).","Our code and models are available at https://github.com/HazyResearch/TART ."],"url":"http://arxiv.org/abs/2306.07536v1"}
{"created":"2023-06-13","title":"Adding guardrails to advanced chatbots","abstract":"Generative AI models continue to become more powerful. The launch of ChatGPT in November 2022 has ushered in a new era of AI. ChatGPT and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art. There are already concerns that humans may be replaced by chatbots for a variety of jobs. Because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them. These biases may cause significant harm and/or inequity toward different subpopulations. To understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of ChatGPT to determine the types of questions that are answered fairly and the types that still need improvement. We find that ChatGPT is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation. We find that ChatGPT is very sensitive to changes in the prompt, where small changes lead to different levels of fairness. This suggests that we need to immediately implement \"corrections\" or mitigation strategies in order to improve fairness of these systems. We suggest different strategies to improve chatbots and also advocate for an impartial review panel that has access to the model parameters to measure the levels of different types of biases and then recommends safeguards that move toward responses that are less discriminatory and more accurate.","sentences":["Generative AI models continue to become more powerful.","The launch of ChatGPT in November 2022 has ushered in a new era of AI.","ChatGPT and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art.","There are already concerns that humans may be replaced by chatbots for a variety of jobs.","Because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them.","These biases may cause significant harm and/or inequity toward different subpopulations.","To understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of ChatGPT to determine the types of questions that are answered fairly and the types that still need improvement.","We find that ChatGPT is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation.","We find that ChatGPT is very sensitive to changes in the prompt, where small changes lead to different levels of fairness.","This suggests that we need to immediately implement \"corrections\" or mitigation strategies in order to improve fairness of these systems.","We suggest different strategies to improve chatbots and also advocate for an impartial review panel that has access to the model parameters to measure the levels of different types of biases and then recommends safeguards that move toward responses that are less discriminatory and more accurate."],"url":"http://arxiv.org/abs/2306.07500v1"}
{"created":"2023-06-13","title":"ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer","abstract":"Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.","sentences":["Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts.","In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal).","Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts.","Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types.","Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style."],"url":"http://arxiv.org/abs/2306.07799v1"}
{"created":"2023-06-13","title":"Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4","abstract":"Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.","sentences":["Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life.","Therefore, it is of great importance to evaluate their emerging abilities.","In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it.","However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner.","For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans.","Moreover, we probe how sturdy the inclination for intuitive-like decision-making is.","Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits."],"url":"http://arxiv.org/abs/2306.07622v1"}
{"created":"2023-06-13","title":"Ethical Aspects of ChatGPT in Software Engineering Research","abstract":"ChatGPT can improve Software Engineering (SE) research practices by offering efficient, accessible information analysis and synthesis based on natural language interactions. However, ChatGPT could bring ethical challenges, encompassing plagiarism, privacy, data security, and the risk of generating biased or potentially detrimental data. This research aims to fill the given gap by elaborating on the key elements: motivators, demotivators, and ethical principles of using ChatGPT in SE research. To achieve this objective, we conducted a literature survey, identified the mentioned elements, and presented their relationships by developing a taxonomy. Further, the identified literature-based elements (motivators, demotivators, and ethical principles) were empirically evaluated by conducting a comprehensive questionnaire-based survey involving SE researchers. Additionally, we employed Interpretive Structure Modeling (ISM) approach to analyze the relationships between the ethical principles of using ChatGPT in SE research and develop a level based decision model. We further conducted a Cross-Impact Matrix Multiplication Applied to Classification (MICMAC) analysis to create a cluster-based decision model. These models aim to help SE researchers devise effective strategies for ethically integrating ChatGPT into SE research by following the identified principles through adopting the motivators and addressing the demotivators. The findings of this study will establish a benchmark for incorporating ChatGPT services in SE research with an emphasis on ethical considerations.","sentences":["ChatGPT can improve Software Engineering (SE) research practices by offering efficient, accessible information analysis and synthesis based on natural language interactions.","However, ChatGPT could bring ethical challenges, encompassing plagiarism, privacy, data security, and the risk of generating biased or potentially detrimental data.","This research aims to fill the given gap by elaborating on the key elements: motivators, demotivators, and ethical principles of using ChatGPT in SE research.","To achieve this objective, we conducted a literature survey, identified the mentioned elements, and presented their relationships by developing a taxonomy.","Further, the identified literature-based elements (motivators, demotivators, and ethical principles) were empirically evaluated by conducting a comprehensive questionnaire-based survey involving SE researchers.","Additionally, we employed Interpretive Structure Modeling (ISM) approach to analyze the relationships between the ethical principles of using ChatGPT in SE research and develop a level based decision model.","We further conducted a Cross-Impact Matrix Multiplication Applied to Classification (MICMAC) analysis to create a cluster-based decision model.","These models aim to help SE researchers devise effective strategies for ethically integrating ChatGPT into SE research by following the identified principles through adopting the motivators and addressing the demotivators.","The findings of this study will establish a benchmark for incorporating ChatGPT services in SE research with an emphasis on ethical considerations."],"url":"http://arxiv.org/abs/2306.07557v1"}
{"created":"2023-06-13","title":"Adding guardrails to advanced chatbots","abstract":"Generative AI models continue to become more powerful. The launch of ChatGPT in November 2022 has ushered in a new era of AI. ChatGPT and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art. There are already concerns that humans may be replaced by chatbots for a variety of jobs. Because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them. These biases may cause significant harm and/or inequity toward different subpopulations. To understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of ChatGPT to determine the types of questions that are answered fairly and the types that still need improvement. We find that ChatGPT is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation. We find that ChatGPT is very sensitive to changes in the prompt, where small changes lead to different levels of fairness. This suggests that we need to immediately implement \"corrections\" or mitigation strategies in order to improve fairness of these systems. We suggest different strategies to improve chatbots and also advocate for an impartial review panel that has access to the model parameters to measure the levels of different types of biases and then recommends safeguards that move toward responses that are less discriminatory and more accurate.","sentences":["Generative AI models continue to become more powerful.","The launch of ChatGPT in November 2022 has ushered in a new era of AI.","ChatGPT and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art.","There are already concerns that humans may be replaced by chatbots for a variety of jobs.","Because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them.","These biases may cause significant harm and/or inequity toward different subpopulations.","To understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of ChatGPT to determine the types of questions that are answered fairly and the types that still need improvement.","We find that ChatGPT is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation.","We find that ChatGPT is very sensitive to changes in the prompt, where small changes lead to different levels of fairness.","This suggests that we need to immediately implement \"corrections\" or mitigation strategies in order to improve fairness of these systems.","We suggest different strategies to improve chatbots and also advocate for an impartial review panel that has access to the model parameters to measure the levels of different types of biases and then recommends safeguards that move toward responses that are less discriminatory and more accurate."],"url":"http://arxiv.org/abs/2306.07500v1"}
{"created":"2023-06-13","title":"XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models","abstract":"The latest breakthroughs in large vision-language models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks. Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation. This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. To enhance the performance of LLMs in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports. These summaries serve to enhance the performance of LLMs through the fine-tuning process. Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. Our open-source demos, models, and instruction sets are available at: https://github.com/mbzuai-oryx/XrayGPT.","sentences":["The latest breakthroughs in large vision-language models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks.","Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks.","However, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images.","On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis.","In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs.","Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation.","This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge.","To enhance the performance of LLMs in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports.","These summaries serve to enhance the performance of LLMs through the fine-tuning process.","Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs.","Our open-source demos, models, and instruction sets are available at: https://github.com/mbzuai-oryx/XrayGPT."],"url":"http://arxiv.org/abs/2306.07971v1"}
{"created":"2023-06-13","title":"GeneCIS: A Benchmark for General Conditional Image Similarity","abstract":"We argue that there are many notions of 'similarity' and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learning methods, supervised or self-supervised, which learn a fixed embedding function and hence implicitly assume a single notion of similarity. For instance, models trained on ImageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or specific elements in the scene. In this paper, we propose the GeneCIS ('genesis') benchmark, which measures models' ability to adapt to a range of similarity conditions. Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We find that baselines from powerful CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet accuracy, suggesting that simply scaling existing methods is not fruitful. We further propose a simple, scalable solution based on automatically mining information from existing image-caption datasets. We find our method offers a substantial boost over the baselines on GeneCIS, and further improves zero-shot performance on related image retrieval benchmarks. In fact, though evaluated zero-shot, our model surpasses state-of-the-art supervised models on MIT-States. Project page at https://sgvaze.github.io/genecis/.","sentences":["We argue that there are many notions of 'similarity' and that models, like humans, should be able to adapt to these dynamically.","This contrasts with most representation learning methods, supervised or self-supervised, which learn a fixed embedding function and hence implicitly assume a single notion of similarity.","For instance, models trained on ImageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or specific elements in the scene.","In this paper, we propose the GeneCIS ('genesis') benchmark, which measures models' ability to adapt to a range of similarity conditions.","Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions.","We find that baselines from powerful CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet accuracy, suggesting that simply scaling existing methods is not fruitful.","We further propose a simple, scalable solution based on automatically mining information from existing image-caption datasets.","We find our method offers a substantial boost over the baselines on GeneCIS, and further improves zero-shot performance on related image retrieval benchmarks.","In fact, though evaluated zero-shot, our model surpasses state-of-the-art supervised models on MIT-States.","Project page at https://sgvaze.github.io/genecis/."],"url":"http://arxiv.org/abs/2306.07969v1"}
{"created":"2023-06-13","title":"Neural Scene Chronology","abstract":"In this work, we aim to reconstruct a time-varying 3D model, capable of rendering photo-realistic renderings with independent control of viewpoint, illumination, and time, from Internet photos of large-scale landmarks. The core challenges are twofold. First, different types of temporal changes, such as illumination and changes to the underlying scene itself (such as replacing one graffiti artwork with another) are entangled together in the imagery. Second, scene-level temporal changes are often discrete and sporadic over time, rather than continuous. To tackle these problems, we propose a new scene representation equipped with a novel temporal step function encoding method that can model discrete scene-level content changes as piece-wise constant functions over time. Specifically, we represent the scene as a space-time radiance field with a per-image illumination embedding, where temporally-varying scene changes are encoded using a set of learned step functions. To facilitate our task of chronology reconstruction from Internet imagery, we also collect a new dataset of four scenes that exhibit various changes over time. We demonstrate that our method exhibits state-of-the-art view synthesis results on this dataset, while achieving independent control of viewpoint, time, and illumination.","sentences":["In this work, we aim to reconstruct a time-varying 3D model, capable of rendering photo-realistic renderings with independent control of viewpoint, illumination, and time, from Internet photos of large-scale landmarks.","The core challenges are twofold.","First, different types of temporal changes, such as illumination and changes to the underlying scene itself (such as replacing one graffiti artwork with another) are entangled together in the imagery.","Second, scene-level temporal changes are often discrete and sporadic over time, rather than continuous.","To tackle these problems, we propose a new scene representation equipped with a novel temporal step function encoding method that can model discrete scene-level content changes as piece-wise constant functions over time.","Specifically, we represent the scene as a space-time radiance field with a per-image illumination embedding, where temporally-varying scene changes are encoded using a set of learned step functions.","To facilitate our task of chronology reconstruction from Internet imagery, we also collect a new dataset of four scenes that exhibit various changes over time.","We demonstrate that our method exhibits state-of-the-art view synthesis results on this dataset, while achieving independent control of viewpoint, time, and illumination."],"url":"http://arxiv.org/abs/2306.07970v1"}
{"created":"2023-06-13","title":"One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning","abstract":"We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code is available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.","sentences":["We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks.","Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets.","Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer.","Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations.","Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets.","Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications.","Code is available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA."],"url":"http://arxiv.org/abs/2306.07967v1"}
{"created":"2023-06-13","title":"Parting with Misconceptions about Learning-based Vehicle Motion Planning","abstract":"The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting. Existing systems struggle to simultaneously meet both requirements. Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently. We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms. More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (\\ie, ignoring all information regarding the map and other agents). Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors, winning the nuPlan planning challenge 2023.","sentences":["The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting.","Existing systems struggle to simultaneously meet both requirements.","Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently.","We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms.","More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (\\ie, ignoring all information regarding the map and other agents).","Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors, winning the nuPlan planning challenge 2023."],"url":"http://arxiv.org/abs/2306.07962v1"}
{"created":"2023-06-13","title":"Supervised-Contrastive Loss Learns Orthogonal Frames and Batching Matters","abstract":"Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy (CE) loss for classification. In this paper we ask: what differences in the learning process occur when the two different loss functions are being optimized? To answer this question, our main finding is that the geometry of embeddings learned by SCL forms an orthogonal frame (OF) regardless of the number of training examples per class. This is in contrast to the CE loss, for which previous work has shown that it learns embeddings geometries that are highly dependent on the class sizes. We arrive at our finding theoretically, by proving that the global minimizers of an unconstrained features model with SCL loss and entry-wise non-negativity constraints form an OF. We then validate the model's prediction by conducting experiments with standard deep-learning models on benchmark vision datasets. Finally, our analysis and experiments reveal that the batching scheme chosen during SCL training plays a critical role in determining the quality of convergence to the OF geometry. This finding motivates a simple algorithm wherein the addition of a few binding examples in each batch significantly speeds up the occurrence of the OF geometry.","sentences":["Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy (CE) loss for classification.","In this paper we ask: what differences in the learning process occur when the two different loss functions are being optimized?","To answer this question, our main finding is that the geometry of embeddings learned by SCL forms an orthogonal frame (OF) regardless of the number of training examples per class.","This is in contrast to the CE loss, for which previous work has shown that it learns embeddings geometries that are highly dependent on the class sizes.","We arrive at our finding theoretically, by proving that the global minimizers of an unconstrained features model with SCL loss and entry-wise non-negativity constraints form an OF.","We then validate the model's prediction by conducting experiments with standard deep-learning models on benchmark vision datasets.","Finally, our analysis and experiments reveal that the batching scheme chosen during SCL training plays a critical role in determining the quality of convergence to the OF geometry.","This finding motivates a simple algorithm wherein the addition of a few binding examples in each batch significantly speeds up the occurrence of the OF geometry."],"url":"http://arxiv.org/abs/2306.07960v1"}
{"created":"2023-06-13","title":"Privacy Preserving Bayesian Federated Learning in Heterogeneous Settings","abstract":"In several practical applications of federated learning (FL), the clients are highly heterogeneous in terms of both their data and compute resources, and therefore enforcing the same model architecture for each client is very limiting. Moreover, the need for uncertainty quantification and data privacy constraints are often particularly amplified for clients that have limited local data. This paper presents a unified FL framework to simultaneously address all these constraints and concerns, based on training customized local Bayesian models that learn well even in the absence of large local datasets. A Bayesian framework provides a natural way of incorporating supervision in the form of prior distributions. We use priors in the functional (output) space of the networks to facilitate collaboration across heterogeneous clients. Moreover, formal differential privacy guarantees are provided for this framework. Experiments on standard FL datasets demonstrate that our approach outperforms strong baselines in both homogeneous and heterogeneous settings and under strict privacy constraints, while also providing characterizations of model uncertainties.","sentences":["In several practical applications of federated learning (FL), the clients are highly heterogeneous in terms of both their data and compute resources, and therefore enforcing the same model architecture for each client is very limiting.","Moreover, the need for uncertainty quantification and data privacy constraints are often particularly amplified for clients that have limited local data.","This paper presents a unified FL framework to simultaneously address all these constraints and concerns, based on training customized local Bayesian models that learn well even in the absence of large local datasets.","A Bayesian framework provides a natural way of incorporating supervision in the form of prior distributions.","We use priors in the functional (output) space of the networks to facilitate collaboration across heterogeneous clients.","Moreover, formal differential privacy guarantees are provided for this framework.","Experiments on standard FL datasets demonstrate that our approach outperforms strong baselines in both homogeneous and heterogeneous settings and under strict privacy constraints, while also providing characterizations of model uncertainties."],"url":"http://arxiv.org/abs/2306.07959v1"}
{"created":"2023-06-13","title":"MOFI: Learning Image Representations from Noisy Entity Annotated Images","abstract":"We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-training, and multi-task learning. For constrastive pre-training, we treat entity names as free-form text, and further enrich them with entity descriptions. Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance. The final MOFI model achieves 86.66% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI's CLIP model. Further experiments on zero-shot and linear probe image classification also show that MOFI outperforms a CLIP model trained on the original image-text data, demonstrating the effectiveness of the I2E dataset in learning strong image representations.","sentences":["We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images.","MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe.","Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs.","Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image.","The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web.","Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild.","Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-training, and multi-task learning.","For constrastive pre-training, we treat entity names as free-form text, and further enrich them with entity descriptions.","Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance.","The final MOFI model achieves 86.66% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI's CLIP model.","Further experiments on zero-shot and linear probe image classification also show that MOFI outperforms a CLIP model trained on the original image-text data, demonstrating the effectiveness of the I2E dataset in learning strong image representations."],"url":"http://arxiv.org/abs/2306.07952v1"}
{"created":"2023-06-13","title":"BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information","abstract":"Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.","sentences":["Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems.","Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning.","However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason.","When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise.","One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference.","In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting.","BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications.","We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs.","While performance can be improved with finetuning, it nevertheless remains poor."],"url":"http://arxiv.org/abs/2306.07934v1"}
{"created":"2023-06-13","title":"Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits","abstract":"We consider policy optimization in contextual bandits, where one is given a fixed dataset of logged interactions. While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are not computationally efficient. We present the first oracle-efficient algorithm for pessimistic policy optimization: it reduces to supervised learning, leading to broad applicability. We also obtain best-effort statistical guarantees analogous to those for pessimistic approaches in prior work. We instantiate our approach for both discrete and continuous actions. We perform extensive experiments in both settings, showing advantage over unregularized policy optimization across a wide range of configurations.","sentences":["We consider policy optimization in contextual bandits, where one is given a fixed dataset of logged interactions.","While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are not computationally efficient.","We present the first oracle-efficient algorithm for pessimistic policy optimization: it reduces to supervised learning, leading to broad applicability.","We also obtain best-effort statistical guarantees analogous to those for pessimistic approaches in prior work.","We instantiate our approach for both discrete and continuous actions.","We perform extensive experiments in both settings, showing advantage over unregularized policy optimization across a wide range of configurations."],"url":"http://arxiv.org/abs/2306.07923v1"}
{"created":"2023-06-13","title":"Robustly Learning a Single Neuron via Sharpness","abstract":"We study the problem of learning a single neuron with respect to the $L_2^2$-loss in the presence of adversarial label noise. We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal $L_2^2$-error within a constant factor. Our algorithm applies under much milder distributional assumptions compared to prior work. The key ingredient enabling our results is a novel connection to local error bounds from optimization theory.","sentences":["We study the problem of learning a single neuron with respect to the $L_2^2$-loss in the presence of adversarial label noise.","We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal $L_2^2$-error within a constant factor.","Our algorithm applies under much milder distributional assumptions compared to prior work.","The key ingredient enabling our results is a novel connection to local error bounds from optimization theory."],"url":"http://arxiv.org/abs/2306.07892v1"}
{"created":"2023-06-13","title":"Scenario Extraction from a Large Real-World Dataset for the Assessment of Automated Vehicles","abstract":"Many players in the automotive field support scenario-based assessment of automated vehicles (AVs), where individual traffic situations can be tested and, thus, facilitate concluding on the performance of AVs in different situations. Since an extremely large number of different scenarios can occur in real-world traffic, the question is how to find a finite set of relevant scenarios. Scenarios extracted from large real-world datasets represent real-world traffic since real driving data is used. Extracting scenarios, however, is challenging because (1) the scenarios to be tested should ensure the AVs behave safely, which conflicts with the fact that the majority of the data contains scenarios that are not interesting from a safety perspective, and (2) extensive data processing is required, which hinders the utilization of large real-world datasets. In this work, we propose a three-step approach for extracting scenarios from real-world driving data. The first step is data preprocessing to tackle the errors and noise in real-world data. The second step performs data tagging to label actors' activities, their interactions with each other, and their interactions with the environment. Finally, the scenarios are extracted by searching for combinations of tags. The proposed approach is evaluated using data simulated with CARLA and applied to a part of a large real-world driving dataset, i.e., the Waymo Open Motion Dataset.","sentences":["Many players in the automotive field support scenario-based assessment of automated vehicles (AVs), where individual traffic situations can be tested and, thus, facilitate concluding on the performance of AVs in different situations.","Since an extremely large number of different scenarios can occur in real-world traffic, the question is how to find a finite set of relevant scenarios.","Scenarios extracted from large real-world datasets represent real-world traffic since real driving data is used.","Extracting scenarios, however, is challenging because (1) the scenarios to be tested should ensure the AVs behave safely, which conflicts with the fact that the majority of the data contains scenarios that are not interesting from a safety perspective, and (2) extensive data processing is required, which hinders the utilization of large real-world datasets.","In this work, we propose a three-step approach for extracting scenarios from real-world driving data.","The first step is data preprocessing to tackle the errors and noise in real-world data.","The second step performs data tagging to label actors' activities, their interactions with each other, and their interactions with the environment.","Finally, the scenarios are extracted by searching for combinations of tags.","The proposed approach is evaluated using data simulated with CARLA and applied to a part of a large real-world driving dataset, i.e., the Waymo Open Motion Dataset."],"url":"http://arxiv.org/abs/2306.07815v1"}
{"created":"2023-06-13","title":"Improving Opinion-based Question Answering Systems Through Label Error Detection and Overwrite","abstract":"Label error is a ubiquitous problem in annotated data. Large amounts of label error substantially degrades the quality of deep learning models. Existing methods to tackle the label error problem largely focus on the classification task, and either rely on task specific architecture or require non-trivial additional computations, which is undesirable or even unattainable for industry usage. In this paper, we propose LEDO: a model-agnostic and computationally efficient framework for Label Error Detection and Overwrite. LEDO is based on Monte Carlo Dropout combined with uncertainty metrics, and can be easily generalized to multiple tasks and data sets. Applying LEDO to an industry opinion-based question answering system demonstrates it is effective at improving accuracy in all the core models. Specifically, LEDO brings 1.1% MRR gain for the retrieval model, 1.5% PR AUC improvement for the machine reading comprehension model, and 0.9% rise in the Average Precision for the ranker, on top of the strong baselines with a large-scale social media dataset. Importantly, LEDO is computationally efficient compared to methods that require loss function change, and cost-effective as the resulting data can be used in the same continuous training pipeline for production. Further analysis shows that these gains come from an improved decision boundary after cleaning the label errors existed in the training data.","sentences":["Label error is a ubiquitous problem in annotated data.","Large amounts of label error substantially degrades the quality of deep learning models.","Existing methods to tackle the label error problem largely focus on the classification task, and either rely on task specific architecture or require non-trivial additional computations, which is undesirable or even unattainable for industry usage.","In this paper, we propose LEDO: a model-agnostic and computationally efficient framework for Label Error Detection and Overwrite.","LEDO is based on Monte Carlo Dropout combined with uncertainty metrics, and can be easily generalized to multiple tasks and data sets.","Applying LEDO to an industry opinion-based question answering system demonstrates it is effective at improving accuracy in all the core models.","Specifically, LEDO brings 1.1% MRR gain for the retrieval model, 1.5% PR AUC improvement for the machine reading comprehension model, and 0.9% rise in the Average Precision for the ranker, on top of the strong baselines with a large-scale social media dataset.","Importantly, LEDO is computationally efficient compared to methods that require loss function change, and cost-effective as the resulting data can be used in the same continuous training pipeline for production.","Further analysis shows that these gains come from an improved decision boundary after cleaning the label errors existed in the training data."],"url":"http://arxiv.org/abs/2306.07499v1"}
{"created":"2023-06-13","title":"Skill Disentanglement for Imitation Learning from Suboptimal Demonstrations","abstract":"Imitation learning has achieved great success in many sequential decision-making tasks, in which a neural agent is learned by imitating collected human demonstrations. However, existing algorithms typically require a large number of high-quality demonstrations that are difficult and expensive to collect. Usually, a trade-off needs to be made between demonstration quality and quantity in practice. Targeting this problem, in this work we consider the imitation of sub-optimal demonstrations, with both a small clean demonstration set and a large noisy set. Some pioneering works have been proposed, but they suffer from many limitations, e.g., assuming a demonstration to be of the same optimality throughout time steps and failing to provide any interpretation w.r.t knowledge learned from the noisy set. Addressing these problems, we propose {\\method} by evaluating and imitating at the sub-demonstration level, encoding action primitives of varying quality into different skills. Concretely, {\\method} consists of a high-level controller to discover skills and a skill-conditioned module to capture action-taking policies, and is trained following a two-phase pipeline by first discovering skills with all demonstrations and then adapting the controller to only the clean set. A mutual-information-based regularization and a dynamic sub-demonstration optimality estimator are designed to promote disentanglement in the skill space. Extensive experiments are conducted over two gym environments and a real-world healthcare dataset to demonstrate the superiority of {\\method} in learning from sub-optimal demonstrations and its improved interpretability by examining learned skills.","sentences":["Imitation learning has achieved great success in many sequential decision-making tasks, in which a neural agent is learned by imitating collected human demonstrations.","However, existing algorithms typically require a large number of high-quality demonstrations that are difficult and expensive to collect.","Usually, a trade-off needs to be made between demonstration quality and quantity in practice.","Targeting this problem, in this work we consider the imitation of sub-optimal demonstrations, with both a small clean demonstration set and a large noisy set.","Some pioneering works have been proposed, but they suffer from many limitations, e.g., assuming a demonstration to be of the same optimality throughout time steps and failing to provide any interpretation w.r.t knowledge learned from the noisy set.","Addressing these problems, we propose {\\method} by evaluating and imitating at the sub-demonstration level, encoding action primitives of varying quality into different skills.","Concretely, {\\method} consists of a high-level controller to discover skills and a skill-conditioned module to capture action-taking policies, and is trained following a two-phase pipeline by first discovering skills with all demonstrations and then adapting the controller to only the clean set.","A mutual-information-based regularization and a dynamic sub-demonstration optimality estimator are designed to promote disentanglement in the skill space.","Extensive experiments are conducted over two gym environments and a real-world healthcare dataset to demonstrate the superiority of {\\method} in learning from sub-optimal demonstrations and its improved interpretability by examining learned skills."],"url":"http://arxiv.org/abs/2306.07919v1"}
{"created":"2023-06-13","title":"Image Captioners Are Scalable Vision Learners Too","abstract":"Contrastive pretraining on image-text pairs from the web is one of the most popular large-scale pretraining strategies for vision backbones, especially in the context of large multimodal models. At the same time, image captioning on this type of data is commonly considered an inferior pretraining strategy. In this paper, we perform a fair comparison of these two pretraining strategies, carefully matching training data, compute, and model capacity. Using a standard encoder-decoder transformer, we find that captioning alone is surprisingly effective: on classification tasks, captioning produces vision encoders competitive with contrastively pretrained encoders, while surpassing them on vision & language tasks. We further analyze the effect of the model architecture and scale, as well as the pretraining data on the representation quality, and find that captioning exhibits the same or better scaling behavior along these axes. Overall our results show that plain image captioning is a more powerful pretraining strategy than was previously believed.","sentences":["Contrastive pretraining on image-text pairs from the web is one of the most popular large-scale pretraining strategies for vision backbones, especially in the context of large multimodal models.","At the same time, image captioning on this type of data is commonly considered an inferior pretraining strategy.","In this paper, we perform a fair comparison of these two pretraining strategies, carefully matching training data, compute, and model capacity.","Using a standard encoder-decoder transformer, we find that captioning alone is surprisingly effective: on classification tasks, captioning produces vision encoders competitive with contrastively pretrained encoders, while surpassing them on vision & language tasks.","We further analyze the effect of the model architecture and scale, as well as the pretraining data on the representation quality, and find that captioning exhibits the same or better scaling behavior along these axes.","Overall our results show that plain image captioning is a more powerful pretraining strategy than was previously believed."],"url":"http://arxiv.org/abs/2306.07915v1"}
{"created":"2023-06-13","title":"VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON","abstract":"Despite progress in vision-based inspection algorithms, real-world industrial challenges -- specifically in data availability, quality, and complex production requirements -- often remain under-addressed. We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges. Unlike previous datasets, VISION brings versatility to defect detection, offering annotation masks across all splits and catering to various detection methodologies. Our datasets also feature instance-segmentation annotation, enabling precise defect identification. With a total of 18k images encompassing 44 defect types, VISION strives to mirror a wide range of real-world production scenarios. By supporting two ongoing challenge competitions on the VISION Datasets, we hope to foster further advancements in vision-based industrial inspection.","sentences":["Despite progress in vision-based inspection algorithms, real-world industrial challenges -- specifically in data availability, quality, and complex production requirements -- often remain under-addressed.","We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges.","Unlike previous datasets, VISION brings versatility to defect detection, offering annotation masks across all splits and catering to various detection methodologies.","Our datasets also feature instance-segmentation annotation, enabling precise defect identification.","With a total of 18k images encompassing 44 defect types, VISION strives to mirror a wide range of real-world production scenarios.","By supporting two ongoing challenge competitions on the VISION Datasets, we hope to foster further advancements in vision-based industrial inspection."],"url":"http://arxiv.org/abs/2306.07890v1"}
{"created":"2023-06-13","title":"From convective stellar dynamo simulations to Zeeman-Doppler images","abstract":"Zeeman-Doppler imaging (ZDI) is used to reconstruct the surface magnetic field of late-type stars from high resolution spectropolarimetric observations. The results are usually described in terms of characteristics of the field topology, i.e. poloidality vs. toroidality and axi-symmetry vs. non-axisymmetry in addition to the field strength. We want to test how well these characteristics are preserved when applying the ZDI method on simulated data, i.e. how accurately the field topology is preserved and to what extent stellar parameters influence the reconstruction. We use published magnetic field data from direct numerical MHD simulations. These have variable rotation rates, and hence represent different levels of activity, of an otherwise Sun-like setup. Our ZDI reconstruction is based on spherical harmonics expansion. By comparing the original values to those of the reconstructed images, we study the ability to reconstruct the surface magnetic field in terms of various characteristics of the field. The main large-scale features are reasonably well recovered, but the strength of the recovered magnetic field is just a fraction of the original input. The quality of the reconstruction shows clear correlations with the data quality. Furthermore, there are some spurious dependencies between stellar parameters and the characteristics of the field. Our study uncovers some limits of ZDI. Firstly, the recovered field strength will generally be lower than the \"real\" value as smaller structures with opposite polarities will be blurred in the inversion. Secondly, the axi-symmetry is overestimated. The poloidality vs. toroidality is better recovered. The reconstruction works better for a stronger field and faster rotation velocity. Still, the ZDI method works surprisingly well even for a weaker field and slow rotation, provided the data has a high signal-to-noise and good rotation phase coverage.","sentences":["Zeeman-Doppler imaging (ZDI) is used to reconstruct the surface magnetic field of late-type stars from high resolution spectropolarimetric observations.","The results are usually described in terms of characteristics of the field topology, i.e. poloidality vs. toroidality and axi-symmetry vs. non-axisymmetry in addition to the field strength.","We want to test how well these characteristics are preserved when applying the ZDI method on simulated data, i.e. how accurately the field topology is preserved and to what extent stellar parameters influence the reconstruction.","We use published magnetic field data from direct numerical MHD simulations.","These have variable rotation rates, and hence represent different levels of activity, of an otherwise Sun-like setup.","Our ZDI reconstruction is based on spherical harmonics expansion.","By comparing the original values to those of the reconstructed images, we study the ability to reconstruct the surface magnetic field in terms of various characteristics of the field.","The main large-scale features are reasonably well recovered, but the strength of the recovered magnetic field is just a fraction of the original input.","The quality of the reconstruction shows clear correlations with the data quality.","Furthermore, there are some spurious dependencies between stellar parameters and the characteristics of the field.","Our study uncovers some limits of ZDI.","Firstly, the recovered field strength will generally be lower than the \"real\" value as smaller structures with opposite polarities will be blurred in the inversion.","Secondly, the axi-symmetry is overestimated.","The poloidality vs. toroidality is better recovered.","The reconstruction works better for a stronger field and faster rotation velocity.","Still, the ZDI method works surprisingly well even for a weaker field and slow rotation, provided the data has a high signal-to-noise and good rotation phase coverage."],"url":"http://arxiv.org/abs/2306.07838v1"}
{"created":"2023-06-13","title":"LMD: Light-weight Prediction Quality Estimation for Object Detection in Lidar Point Clouds","abstract":"Object detection on Lidar point cloud data is a promising technology for autonomous driving and robotics which has seen a significant rise in performance and accuracy during recent years. Particularly uncertainty estimation is a crucial component for down-stream tasks and deep neural networks remain error-prone even for predictions with high confidence. Previously proposed methods for quantifying prediction uncertainty tend to alter the training scheme of the detector or rely on prediction sampling which results in vastly increased inference time. In order to address these two issues, we propose LidarMetaDetect (LMD), a light-weight post-processing scheme for prediction quality estimation. Our method can easily be added to any pre-trained Lidar object detector without altering anything about the base model and is purely based on post-processing, therefore, only leading to a negligible computational overhead. Our experiments show a significant increase of statistical reliability in separating true from false predictions. We propose and evaluate an additional application of our method leading to the detection of annotation errors. Explicit samples and a conservative count of annotation error proposals indicates the viability of our method for large-scale datasets like KITTI and nuScenes. On the widely-used nuScenes test dataset, 43 out of the top 100 proposals of our method indicate, in fact, erroneous annotations.","sentences":["Object detection on Lidar point cloud data is a promising technology for autonomous driving and robotics which has seen a significant rise in performance and accuracy during recent years.","Particularly uncertainty estimation is a crucial component for down-stream tasks and deep neural networks remain error-prone even for predictions with high confidence.","Previously proposed methods for quantifying prediction uncertainty tend to alter the training scheme of the detector or rely on prediction sampling which results in vastly increased inference time.","In order to address these two issues, we propose LidarMetaDetect (LMD), a light-weight post-processing scheme for prediction quality estimation.","Our method can easily be added to any pre-trained Lidar object detector without altering anything about the base model and is purely based on post-processing, therefore, only leading to a negligible computational overhead.","Our experiments show a significant increase of statistical reliability in separating true from false predictions.","We propose and evaluate an additional application of our method leading to the detection of annotation errors.","Explicit samples and a conservative count of annotation error proposals indicates the viability of our method for large-scale datasets like KITTI and nuScenes.","On the widely-used nuScenes test dataset, 43 out of the top 100 proposals of our method indicate, in fact, erroneous annotations."],"url":"http://arxiv.org/abs/2306.07835v1"}
{"created":"2023-06-13","title":"NAVER LABS Europe's Multilingual Speech Translation Systems for the IWSLT 2023 Low-Resource Track","abstract":"This paper presents NAVER LABS Europe's systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-trained models. Our primary submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year's test set, outperforming the second best participant by 7.7 points. For Quechua, we also rank first and achieve 17.7 BLEU, despite having only two hours of translation data. Finally, we show that our proposed multilingual architecture is also competitive for high-resource languages, outperforming the best unconstrained submission to the IWSLT 2021 Multilingual track, despite using much less training data and compute.","sentences":["This paper presents NAVER LABS Europe's systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track.","Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-trained models.","Our primary submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year's test set, outperforming the second best participant by 7.7 points.","For Quechua, we also rank first and achieve 17.7 BLEU, despite having only two hours of translation data.","Finally, we show that our proposed multilingual architecture is also competitive for high-resource languages, outperforming the best unconstrained submission to the IWSLT 2021 Multilingual track, despite using much less training data and compute."],"url":"http://arxiv.org/abs/2306.07763v1"}
{"created":"2023-06-13","title":"Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis","abstract":"Large text-to-image models have shown remarkable performance in synthesizing high-quality images. In particular, the subject-driven model makes it possible to personalize the image synthesis for a specific subject, e.g., a human face or an artistic style, by fine-tuning the generic text-to-image model with a few images from that subject. Nevertheless, misuse of subject-driven image synthesis may violate the authority of subject owners. For example, malicious users may use subject-driven synthesis to mimic specific artistic styles or to create fake facial images without authorization. To protect subject owners against such misuse, recent attempts have commonly relied on adversarial examples to indiscriminately disrupt subject-driven image synthesis. However, this essentially prevents any benign use of subject-driven synthesis based on protected images.   In this paper, we take a different angle and aim at protection without sacrificing the utility of protected images for general synthesis purposes. Specifically, we propose GenWatermark, a novel watermark system based on jointly learning a watermark generator and a detector. In particular, to help the watermark survive the subject-driven synthesis, we incorporate the synthesis process in learning GenWatermark by fine-tuning the detector with synthesized images for a specific subject. This operation is shown to largely improve the watermark detection accuracy and also ensure the uniqueness of the watermark for each individual subject. Extensive experiments validate the effectiveness of GenWatermark, especially in practical scenarios with unknown models and text prompts (74% Acc.), as well as partial data watermarking (80% Acc. for 1/4 watermarking). We also demonstrate the robustness of GenWatermark to two potential countermeasures that substantially degrade the synthesis quality.","sentences":["Large text-to-image models have shown remarkable performance in synthesizing high-quality images.","In particular, the subject-driven model makes it possible to personalize the image synthesis for a specific subject, e.g., a human face or an artistic style, by fine-tuning the generic text-to-image model with a few images from that subject.","Nevertheless, misuse of subject-driven image synthesis may violate the authority of subject owners.","For example, malicious users may use subject-driven synthesis to mimic specific artistic styles or to create fake facial images without authorization.","To protect subject owners against such misuse, recent attempts have commonly relied on adversarial examples to indiscriminately disrupt subject-driven image synthesis.","However, this essentially prevents any benign use of subject-driven synthesis based on protected images.   ","In this paper, we take a different angle and aim at protection without sacrificing the utility of protected images for general synthesis purposes.","Specifically, we propose GenWatermark, a novel watermark system based on jointly learning a watermark generator and a detector.","In particular, to help the watermark survive the subject-driven synthesis, we incorporate the synthesis process in learning GenWatermark by fine-tuning the detector with synthesized images for a specific subject.","This operation is shown to largely improve the watermark detection accuracy and also ensure the uniqueness of the watermark for each individual subject.","Extensive experiments validate the effectiveness of GenWatermark, especially in practical scenarios with unknown models and text prompts (74% Acc.), as well as partial data watermarking (80% Acc.","for 1/4 watermarking).","We also demonstrate the robustness of GenWatermark to two potential countermeasures that substantially degrade the synthesis quality."],"url":"http://arxiv.org/abs/2306.07754v1"}
{"created":"2023-06-13","title":"Dynamically Masked Discriminator for Generative Adversarial Networks","abstract":"Training Generative Adversarial Networks (GANs) remains a challenging problem. The discriminator trains the generator by learning the distribution of real/generated data. However, the distribution of generated data changes throughout the training process, which is difficult for the discriminator to learn. In this paper, we propose a novel method for GANs from the viewpoint of online continual learning. We observe that the discriminator model, trained on historically generated data, often slows down its adaptation to the changes in the new arrival generated data, which accordingly decreases the quality of generated results. By treating the generated data in training as a stream, we propose to detect whether the discriminator slows down the learning of new knowledge in generated data. Therefore, we can explicitly enforce the discriminator to learn new knowledge fast. Particularly, we propose a new discriminator, which automatically detects its retardation and then dynamically masks its features, such that the discriminator can adaptively learn the temporally-vary distribution of generated data. Experimental results show our method outperforms the state-of-the-art approaches.","sentences":["Training Generative Adversarial Networks (GANs) remains a challenging problem.","The discriminator trains the generator by learning the distribution of real/generated data.","However, the distribution of generated data changes throughout the training process, which is difficult for the discriminator to learn.","In this paper, we propose a novel method for GANs from the viewpoint of online continual learning.","We observe that the discriminator model, trained on historically generated data, often slows down its adaptation to the changes in the new arrival generated data, which accordingly decreases the quality of generated results.","By treating the generated data in training as a stream, we propose to detect whether the discriminator slows down the learning of new knowledge in generated data.","Therefore, we can explicitly enforce the discriminator to learn new knowledge fast.","Particularly, we propose a new discriminator, which automatically detects its retardation and then dynamically masks its features, such that the discriminator can adaptively learn the temporally-vary distribution of generated data.","Experimental results show our method outperforms the state-of-the-art approaches."],"url":"http://arxiv.org/abs/2306.07716v1"}
{"created":"2023-06-13","title":"3D-Printed Sheet Jet for Stable Megahertz Liquid Sample Delivery at X-ray Free Electron Lasers","abstract":"X-ray Free Electron Lasers (XFELs) can probe chemical and biological reactions as they unfold with unprecedented spatial and temporal resolution. A principal challenge in this pursuit is the delivery of samples to the X-ray interaction point in a way that produces data of the highest possible quality and efficiency. This is hampered by constraints posed by the light source and operation within a beamline environment. For liquid samples, the solution typically involves a high-speed liquid jet, capable of keeping up with the rate of X-ray pulses. However, conventional jets are not ideal because of radiation-induced explosions of the jet, as well as their cylindrical geometry combined with the X-ray pointing instability of many beamlines causes the interaction volume to differ for every pulse. This complicates data analysis and contributes to measurement errors. An alternative geometry is a liquid sheet jet which, with its constant thickness over large areas, eliminates the X-ray pointing related problems. Since liquid sheets can be made very thin, the radiation-induced explosion is reduced, boosting their stability. They are especially attractive for experiments which benefit from small interaction volumes such as fluctuation X-ray scattering and several types of spectroscopy. Although they have seen increasing use for soft X-ray applications in recent years, there has not yet been wide-scale adoption at XFELs. Here, we demonstrate liquid sheet jet sample injection at the European XFEL. We evaluate several aspects of its performance relative to a conventional liquid jet including thickness profile, stability, and radiation-induced explosion dynamics at high repetition rates. The sheet jet exhibits superior performance across these critical experimental parameters. Its minute thickness also suggests ultrafast single-particle solution scattering is a possibility.","sentences":["X-ray Free Electron Lasers (XFELs) can probe chemical and biological reactions as they unfold with unprecedented spatial and temporal resolution.","A principal challenge in this pursuit is the delivery of samples to the X-ray interaction point in a way that produces data of the highest possible quality and efficiency.","This is hampered by constraints posed by the light source and operation within a beamline environment.","For liquid samples, the solution typically involves a high-speed liquid jet, capable of keeping up with the rate of X-ray pulses.","However, conventional jets are not ideal because of radiation-induced explosions of the jet, as well as their cylindrical geometry combined with the X-ray pointing instability of many beamlines causes the interaction volume to differ for every pulse.","This complicates data analysis and contributes to measurement errors.","An alternative geometry is a liquid sheet jet which, with its constant thickness over large areas, eliminates the X-ray pointing related problems.","Since liquid sheets can be made very thin, the radiation-induced explosion is reduced, boosting their stability.","They are especially attractive for experiments which benefit from small interaction volumes such as fluctuation X-ray scattering and several types of spectroscopy.","Although they have seen increasing use for soft X-ray applications in recent years, there has not yet been wide-scale adoption at XFELs.","Here, we demonstrate liquid sheet jet sample injection at the European XFEL.","We evaluate several aspects of its performance relative to a conventional liquid jet including thickness profile, stability, and radiation-induced explosion dynamics at high repetition rates.","The sheet jet exhibits superior performance across these critical experimental parameters.","Its minute thickness also suggests ultrafast single-particle solution scattering is a possibility."],"url":"http://arxiv.org/abs/2306.07626v1"}
{"created":"2023-06-13","title":"Hyperbolic Graph Diffusion Model for Molecule Generation","abstract":"Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Model (HGDM). The proposed HGDM employs a hyperbolic variational autoencoder to generate the hyperbolic hidden representation of nodes and then a score-based hyperbolic graph neural network is used to learn the distribution in hyperbolic space. Numerical experimental results show that the proposed HGDM achieves higher performance on several molecular datasets, compared with state-of-the-art methods.","sentences":["Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images.","Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably.","Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules.","It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured.","In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Model (HGDM).","The proposed HGDM employs a hyperbolic variational autoencoder to generate the hyperbolic hidden representation of nodes and then a score-based hyperbolic graph neural network is used to learn the distribution in hyperbolic space.","Numerical experimental results show that the proposed HGDM achieves higher performance on several molecular datasets, compared with state-of-the-art methods."],"url":"http://arxiv.org/abs/2306.07618v1"}
