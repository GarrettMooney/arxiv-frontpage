{"created":"2023-06-12","title":"Scalable 3D Captioning with Pretrained Models","abstract":"We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E, and DreamFusion.","sentences":["We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects.","This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation.","We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs.","Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed.","Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset.","Finally, we finetune Text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E, and DreamFusion."],"url":"http://arxiv.org/abs/2306.07279v1"}
{"created":"2023-06-12","title":"MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images","abstract":"In this paper, we present MovieFactory, a powerful framework to generate cinematic-picture (3072$\\times$1280), film-style (multi-scene), and multi-modality (sounding) movies on the demand of natural languages. As the first fully automated movie generation model to the best of our knowledge, our approach empowers users to create captivating movies with smooth transitions using simple text inputs, surpassing existing methods that produce soundless videos limited to a single scene of modest quality. To facilitate this distinctive functionality, we leverage ChatGPT to expand user-provided text into detailed sequential scripts for movie generation. Then we bring scripts to life visually and acoustically through vision generation and audio retrieval. To generate videos, we extend the capabilities of a pretrained text-to-image diffusion model through a two-stage process. Firstly, we employ spatial finetuning to bridge the gap between the pretrained image model and the new video dataset. Subsequently, we introduce temporal learning to capture object motion. In terms of audio, we leverage sophisticated retrieval models to select and align audio elements that correspond to the plot and visual content of the movie. Extensive experiments demonstrate that our MovieFactory produces movies with realistic visuals, diverse scenes, and seamlessly fitting audio, offering users a novel and immersive experience. Generated samples can be found in YouTube or Bilibili (1080P).","sentences":["In this paper, we present MovieFactory, a powerful framework to generate cinematic-picture (3072$\\times$1280), film-style (multi-scene), and multi-modality (sounding) movies on the demand of natural languages.","As the first fully automated movie generation model to the best of our knowledge, our approach empowers users to create captivating movies with smooth transitions using simple text inputs, surpassing existing methods that produce soundless videos limited to a single scene of modest quality.","To facilitate this distinctive functionality, we leverage ChatGPT to expand user-provided text into detailed sequential scripts for movie generation.","Then we bring scripts to life visually and acoustically through vision generation and audio retrieval.","To generate videos, we extend the capabilities of a pretrained text-to-image diffusion model through a two-stage process.","Firstly, we employ spatial finetuning to bridge the gap between the pretrained image model and the new video dataset.","Subsequently, we introduce temporal learning to capture object motion.","In terms of audio, we leverage sophisticated retrieval models to select and align audio elements that correspond to the plot and visual content of the movie.","Extensive experiments demonstrate that our MovieFactory produces movies with realistic visuals, diverse scenes, and seamlessly fitting audio, offering users a novel and immersive experience.","Generated samples can be found in YouTube or Bilibili (1080P)."],"url":"http://arxiv.org/abs/2306.07257v1"}
{"created":"2023-06-12","title":"Valley: Video Assistant with Large Language model Enhanced abilitY","abstract":"Recently, several multi-modal models have been developed for joint image and language understanding, which have demonstrated impressive chat abilities by utilizing advanced large language models (LLMs). The process of developing such models is straightforward yet effective. It involves pre-training an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on the instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of perceiving video, image, and language within a general framework. To achieve this goal, we introduce Valley: Video Assistant with Large Language model Enhanced ability. Specifically, our proposed Valley model is designed with a simple projection module that bridges video, image, and language modalities, and is further unified with a multi-lingual LLM. We also collect multi-source vision-text pairs and adopt a spatio-temporal pooling strategy to obtain a unified vision encoding of video and image input for pre-training. Furthermore, we generate multi-task instruction-following video data, including multi-shot captions, long video descriptions, action recognition, causal relationship inference, etc. To obtain the instruction-following data, we design diverse rounds of task-oriented conversations between humans and videos, facilitated by ChatGPT. Qualitative examples demonstrate that our proposed model has the potential to function as a highly effective multilingual video assistant that can make complex video understanding scenarios easy. Code, data, and models will be available at https://github.com/RupertLuo/Valley.","sentences":["Recently, several multi-modal models have been developed for joint image and language understanding, which have demonstrated impressive chat abilities by utilizing advanced large language models (LLMs).","The process of developing such models is straightforward yet effective.","It involves pre-training an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on the instruction-following data.","However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored.","In this paper, we aim to develop a novel multi-modal foundation model capable of perceiving video, image, and language within a general framework.","To achieve this goal, we introduce Valley: Video Assistant with Large Language model Enhanced ability.","Specifically, our proposed Valley model is designed with a simple projection module that bridges video, image, and language modalities, and is further unified with a multi-lingual LLM.","We also collect multi-source vision-text pairs and adopt a spatio-temporal pooling strategy to obtain a unified vision encoding of video and image input for pre-training.","Furthermore, we generate multi-task instruction-following video data, including multi-shot captions, long video descriptions, action recognition, causal relationship inference, etc.","To obtain the instruction-following data, we design diverse rounds of task-oriented conversations between humans and videos, facilitated by ChatGPT.","Qualitative examples demonstrate that our proposed model has the potential to function as a highly effective multilingual video assistant that can make complex video understanding scenarios easy.","Code, data, and models will be available at https://github.com/RupertLuo/Valley."],"url":"http://arxiv.org/abs/2306.07207v1"}
{"created":"2023-06-12","title":"InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions","abstract":"Enhancing AI systems to perform tasks following human instructions can significantly boost productivity. In this paper, we present InstructP2P, an end-to-end framework for 3D shape editing on point clouds, guided by high-level textual instructions. InstructP2P extends the capabilities of existing methods by synergizing the strengths of a text-conditioned point cloud diffusion model, Point-E, and powerful language models, enabling color and geometry editing using language instructions. To train InstructP2P, we introduce a new shape editing dataset, constructed by integrating a shape segmentation dataset, off-the-shelf shape programs, and diverse edit instructions generated by a large language model, ChatGPT. Our proposed method allows for editing both color and geometry of specific regions in a single forward pass, while leaving other regions unaffected. In our experiments, InstructP2P shows generalization capabilities, adapting to novel shape categories and instructions, despite being trained on a limited amount of data.","sentences":["Enhancing AI systems to perform tasks following human instructions can significantly boost productivity.","In this paper, we present InstructP2P, an end-to-end framework for 3D shape editing on point clouds, guided by high-level textual instructions.","InstructP2P extends the capabilities of existing methods by synergizing the strengths of a text-conditioned point cloud diffusion model, Point-E, and powerful language models, enabling color and geometry editing using language instructions.","To train InstructP2P, we introduce a new shape editing dataset, constructed by integrating a shape segmentation dataset, off-the-shelf shape programs, and diverse edit instructions generated by a large language model, ChatGPT.","Our proposed method allows for editing both color and geometry of specific regions in a single forward pass, while leaving other regions unaffected.","In our experiments, InstructP2P shows generalization capabilities, adapting to novel shape categories and instructions, despite being trained on a limited amount of data."],"url":"http://arxiv.org/abs/2306.07154v1"}
{"created":"2023-06-12","title":"No Free Lunch: The Hazards of Over-Expressive Representations in Anomaly Detection","abstract":"Anomaly detection methods, powered by deep learning, have recently been making significant progress, mostly due to improved representations. It is tempting to hypothesize that anomaly detection can improve indefinitely by increasing the scale of our networks, making their representations more expressive. In this paper, we provide theoretical and empirical evidence to the contrary. In fact, we empirically show cases where very expressive representations fail to detect even simple anomalies when evaluated beyond the well-studied object-centric datasets. To investigate this phenomenon, we begin by introducing a novel theoretical toy model for anomaly detection performance. The model uncovers a fundamental trade-off between representation sufficiency and over-expressivity. It provides evidence for a no-free-lunch theorem in anomaly detection stating that increasing representation expressivity will eventually result in performance degradation. Instead, guidance must be provided to focus the representation on the attributes relevant to the anomalies of interest. We conduct an extensive empirical investigation demonstrating that state-of-the-art representations often suffer from over-expressivity, failing to detect many types of anomalies. Our investigation demonstrates how this over-expressivity impairs image anomaly detection in practical settings. We conclude with future directions for mitigating this issue.","sentences":["Anomaly detection methods, powered by deep learning, have recently been making significant progress, mostly due to improved representations.","It is tempting to hypothesize that anomaly detection can improve indefinitely by increasing the scale of our networks, making their representations more expressive.","In this paper, we provide theoretical and empirical evidence to the contrary.","In fact, we empirically show cases where very expressive representations fail to detect even simple anomalies when evaluated beyond the well-studied object-centric datasets.","To investigate this phenomenon, we begin by introducing a novel theoretical toy model for anomaly detection performance.","The model uncovers a fundamental trade-off between representation sufficiency and over-expressivity.","It provides evidence for a no-free-lunch theorem in anomaly detection stating that increasing representation expressivity will eventually result in performance degradation.","Instead, guidance must be provided to focus the representation on the attributes relevant to the anomalies of interest.","We conduct an extensive empirical investigation demonstrating that state-of-the-art representations often suffer from over-expressivity, failing to detect many types of anomalies.","Our investigation demonstrates how this over-expressivity impairs image anomaly detection in practical settings.","We conclude with future directions for mitigating this issue."],"url":"http://arxiv.org/abs/2306.07284v1"}
{"created":"2023-06-12","title":"Scalable 3D Captioning with Pretrained Models","abstract":"We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E, and DreamFusion.","sentences":["We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects.","This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation.","We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs.","Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed.","Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset.","Finally, we finetune Text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E, and DreamFusion."],"url":"http://arxiv.org/abs/2306.07279v1"}
{"created":"2023-06-12","title":"Reconstructing Heterogeneous Cryo-EM Molecular Structures by Decomposing Them into Polymer Chains","abstract":"Cryogenic electron microscopy (cryo-EM) has transformed structural biology by allowing to reconstruct 3D biomolecular structures up to near-atomic resolution. However, the 3D reconstruction process remains challenging, as the 3D structures may exhibit substantial shape variations, while the 2D image acquisition suffers from a low signal-to-noise ratio, requiring to acquire very large datasets that are time-consuming to process. Current reconstruction methods are precise but computationally expensive, or faster but lack a physically-plausible model of large molecular shape variations. To fill this gap, we propose CryoChains that encodes large deformations of biomolecules via rigid body transformation of their polymer instances (chains), while representing their finer shape variations with the normal mode analysis framework of biophysics. Our synthetic data experiments on the human $\\text{GABA}_{\\text{B}}$ and heat shock protein show that CryoChains gives a biophysically-grounded quantification of the heterogeneous conformations of biomolecules, while reconstructing their 3D molecular structures at an improved resolution compared to the current fastest, interpretable deep learning method.","sentences":["Cryogenic electron microscopy (cryo-EM) has transformed structural biology by allowing to reconstruct 3D biomolecular structures up to near-atomic resolution.","However, the 3D reconstruction process remains challenging, as the 3D structures may exhibit substantial shape variations, while the 2D image acquisition suffers from a low signal-to-noise ratio, requiring to acquire very large datasets that are time-consuming to process.","Current reconstruction methods are precise but computationally expensive, or faster but lack a physically-plausible model of large molecular shape variations.","To fill this gap, we propose CryoChains that encodes large deformations of biomolecules via rigid body transformation of their polymer instances (chains), while representing their finer shape variations with the normal mode analysis framework of biophysics.","Our synthetic data experiments on the human $\\text{GABA}_{\\text{B}}$ and heat shock protein show that CryoChains gives a biophysically-grounded quantification of the heterogeneous conformations of biomolecules, while reconstructing their 3D molecular structures at an improved resolution compared to the current fastest, interpretable deep learning method."],"url":"http://arxiv.org/abs/2306.07274v1"}
{"created":"2023-06-12","title":"Gaussian Membership Inference Privacy","abstract":"We propose a new privacy notion called $f$-Membership Inference Privacy ($f$-MIP), which explicitly considers the capabilities of realistic adversaries under the membership inference attack threat model. By doing so $f$-MIP offers interpretable privacy guarantees and improved utility (e.g., better classification accuracy). Our novel theoretical analysis of likelihood ratio-based membership inference attacks on noisy stochastic gradient descent (SGD) results in a parametric family of $f$-MIP guarantees that we refer to as $\\mu$-Gaussian Membership Inference Privacy ($\\mu$-GMIP). Our analysis additionally yields an analytical membership inference attack that offers distinct advantages over previous approaches. First, unlike existing methods, our attack does not require training hundreds of shadow models to approximate the likelihood ratio. Second, our analytical attack enables straightforward auditing of our privacy notion $f$-MIP. Finally, our analysis emphasizes the importance of various factors, such as hyperparameters (e.g., batch size, number of model parameters) and data specific characteristics in controlling an attacker's success in reliably inferring a given point's membership to the training set. We demonstrate the effectiveness of our method on models trained across vision and tabular datasets.","sentences":["We propose a new privacy notion called $f$-Membership Inference Privacy ($f$-MIP), which explicitly considers the capabilities of realistic adversaries under the membership inference attack threat model.","By doing so $f$-MIP offers interpretable privacy guarantees and improved utility (e.g., better classification accuracy).","Our novel theoretical analysis of likelihood ratio-based membership inference attacks on noisy stochastic gradient descent (SGD) results in a parametric family of $f$-MIP guarantees that we refer to as $\\mu$-Gaussian Membership Inference Privacy ($\\mu$-GMIP).","Our analysis additionally yields an analytical membership inference attack that offers distinct advantages over previous approaches.","First, unlike existing methods, our attack does not require training hundreds of shadow models to approximate the likelihood ratio.","Second, our analytical attack enables straightforward auditing of our privacy notion $f$-MIP.","Finally, our analysis emphasizes the importance of various factors, such as hyperparameters (e.g., batch size, number of model parameters) and data specific characteristics in controlling an attacker's success in reliably inferring a given point's membership to the training set.","We demonstrate the effectiveness of our method on models trained across vision and tabular datasets."],"url":"http://arxiv.org/abs/2306.07273v1"}
{"created":"2023-06-12","title":"Zero-shot Composed Text-Image Retrieval","abstract":"In this paper, we consider the problem of composed image retrieval (CIR), it aims to train a model that can fuse multi-modal information, e.g., text and images, to accurately retrieve images that match the query, extending the user's expression ability. We make the following contributions: (i) we initiate a scalable pipeline to automatically construct datasets for training CIR model, by simply exploiting a large-scale dataset of image-text pairs, e.g., a subset of LAION-5B; (ii) we introduce a transformer-based adaptive aggregation model, TransAgg, which employs a simple yet efficient fusion mechanism, to adaptively combine information from diverse modalities; (iii) we conduct extensive ablation studies to investigate the usefulness of our proposed data construction procedure, and the effectiveness of core components in TransAgg; (iv) when evaluating on the publicly available benckmarks under the zero-shot scenario, i.e., training on the automatically constructed datasets, then directly conduct inference on target downstream datasets, e.g., CIRR and FashionIQ, our proposed approach either performs on par with or significantly outperforms the existing state-of-the-art (SOTA) models. Project page: https://code-kunkun.github.io/ZS-CIR/","sentences":["In this paper, we consider the problem of composed image retrieval (CIR), it aims to train a model that can fuse multi-modal information, e.g., text and images, to accurately retrieve images that match the query, extending the user's expression ability.","We make the following contributions: (i) we initiate a scalable pipeline to automatically construct datasets for training CIR model, by simply exploiting a large-scale dataset of image-text pairs, e.g., a subset of LAION-5B; (ii) we introduce a transformer-based adaptive aggregation model, TransAgg, which employs a simple yet efficient fusion mechanism, to adaptively combine information from diverse modalities; (iii) we conduct extensive ablation studies to investigate the usefulness of our proposed data construction procedure, and the effectiveness of core components in TransAgg; (iv) when evaluating on the publicly available benckmarks under the zero-shot scenario, i.e., training on the automatically constructed datasets, then directly conduct inference on target downstream datasets, e.g., CIRR and FashionIQ, our proposed approach either performs on par with or significantly outperforms the existing state-of-the-art (SOTA) models.","Project page: https://code-kunkun.github.io/ZS-CIR/"],"url":"http://arxiv.org/abs/2306.07272v1"}
{"created":"2023-06-12","title":"Unprocessing Seven Years of Algorithmic Fairness","abstract":"Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Interpreting our findings, we recall a widely overlooked theoretical argument, present seven years ago, that accurately predicted what we observe.","sentences":["Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups.","The work launched hundreds of papers purporting to improve over the postprocessing baseline.","We empirically evaluate these claims through thousands of model evaluations on several tabular datasets.","We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate.","In doing so, we address two common methodological errors that have confounded previous observations.","One relates to the comparison of methods with different unconstrained base models.","The other concerns methods achieving different levels of constraint relaxation.","At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing.","Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation.","Interpreting our findings, we recall a widely overlooked theoretical argument, present seven years ago, that accurately predicted what we observe."],"url":"http://arxiv.org/abs/2306.07261v1"}
{"created":"2023-06-12","title":"MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images","abstract":"In this paper, we present MovieFactory, a powerful framework to generate cinematic-picture (3072$\\times$1280), film-style (multi-scene), and multi-modality (sounding) movies on the demand of natural languages. As the first fully automated movie generation model to the best of our knowledge, our approach empowers users to create captivating movies with smooth transitions using simple text inputs, surpassing existing methods that produce soundless videos limited to a single scene of modest quality. To facilitate this distinctive functionality, we leverage ChatGPT to expand user-provided text into detailed sequential scripts for movie generation. Then we bring scripts to life visually and acoustically through vision generation and audio retrieval. To generate videos, we extend the capabilities of a pretrained text-to-image diffusion model through a two-stage process. Firstly, we employ spatial finetuning to bridge the gap between the pretrained image model and the new video dataset. Subsequently, we introduce temporal learning to capture object motion. In terms of audio, we leverage sophisticated retrieval models to select and align audio elements that correspond to the plot and visual content of the movie. Extensive experiments demonstrate that our MovieFactory produces movies with realistic visuals, diverse scenes, and seamlessly fitting audio, offering users a novel and immersive experience. Generated samples can be found in YouTube or Bilibili (1080P).","sentences":["In this paper, we present MovieFactory, a powerful framework to generate cinematic-picture (3072$\\times$1280), film-style (multi-scene), and multi-modality (sounding) movies on the demand of natural languages.","As the first fully automated movie generation model to the best of our knowledge, our approach empowers users to create captivating movies with smooth transitions using simple text inputs, surpassing existing methods that produce soundless videos limited to a single scene of modest quality.","To facilitate this distinctive functionality, we leverage ChatGPT to expand user-provided text into detailed sequential scripts for movie generation.","Then we bring scripts to life visually and acoustically through vision generation and audio retrieval.","To generate videos, we extend the capabilities of a pretrained text-to-image diffusion model through a two-stage process.","Firstly, we employ spatial finetuning to bridge the gap between the pretrained image model and the new video dataset.","Subsequently, we introduce temporal learning to capture object motion.","In terms of audio, we leverage sophisticated retrieval models to select and align audio elements that correspond to the plot and visual content of the movie.","Extensive experiments demonstrate that our MovieFactory produces movies with realistic visuals, diverse scenes, and seamlessly fitting audio, offering users a novel and immersive experience.","Generated samples can be found in YouTube or Bilibili (1080P)."],"url":"http://arxiv.org/abs/2306.07257v1"}
{"created":"2023-06-12","title":"On the Expected Size of Conformal Prediction Sets","abstract":"While conformal predictors reap the benefits of rigorous statistical guarantees for their error frequency, the size of their corresponding prediction sets is critical to their practical utility. Unfortunately, there is currently a lack of finite-sample analysis and guarantees for their prediction set sizes. To address this shortfall, we theoretically quantify the expected size of the prediction set under the split conformal prediction framework. As this precise formulation cannot usually be calculated directly, we further derive point estimates and high probability intervals that can be easily computed, providing a practical method for characterizing the expected prediction set size across different possible realizations of the test and calibration data. Additionally, we corroborate the efficacy of our results with experiments on real-world datasets, for both regression and classification problems.","sentences":["While conformal predictors reap the benefits of rigorous statistical guarantees for their error frequency, the size of their corresponding prediction sets is critical to their practical utility.","Unfortunately, there is currently a lack of finite-sample analysis and guarantees for their prediction set sizes.","To address this shortfall, we theoretically quantify the expected size of the prediction set under the split conformal prediction framework.","As this precise formulation cannot usually be calculated directly, we further derive point estimates and high probability intervals that can be easily computed, providing a practical method for characterizing the expected prediction set size across different possible realizations of the test and calibration data.","Additionally, we corroborate the efficacy of our results with experiments on real-world datasets, for both regression and classification problems."],"url":"http://arxiv.org/abs/2306.07254v1"}
{"created":"2023-06-12","title":"RB-Dust -- A Reference-based Dataset for Vision-based Dust Removal","abstract":"Dust in the agricultural landscape is a significant challenge and influences, for example, the environmental perception of autonomous agricultural machines. Image enhancement algorithms can be used to reduce dust. However, these require dusty and dust-free images of the same environment for validation. In fact, to date, there is no dataset that we are aware of that addresses this issue. Therefore, we present the agriscapes RB-Dust dataset, which is named after its purpose of reference-based dust removal. It is not possible to take pictures from the cabin during tillage, as this would cause shifts in the images. Because of this, we built a setup from which it is possible to take images from a stationary position close to the passing tractor. The test setup was based on a half-sided gate through which the tractor could drive. The field tests were carried out on a farm in Bavaria, Germany, during tillage. During the field tests, other parameters such as soil moisture and wind speed were controlled, as these significantly affect dust development. We validated our dataset with contrast enhancement and image dehazing algorithms and analyzed the generalizability from recordings from the moving tractor. Finally, we demonstrate the application of dust removal based on a high-level vision task, such as person classification. Our empirical study confirms the validity of RB-Dust for vision-based dust removal in agriculture.","sentences":["Dust in the agricultural landscape is a significant challenge and influences, for example, the environmental perception of autonomous agricultural machines.","Image enhancement algorithms can be used to reduce dust.","However, these require dusty and dust-free images of the same environment for validation.","In fact, to date, there is no dataset that we are aware of that addresses this issue.","Therefore, we present the agriscapes RB-Dust dataset, which is named after its purpose of reference-based dust removal.","It is not possible to take pictures from the cabin during tillage, as this would cause shifts in the images.","Because of this, we built a setup from which it is possible to take images from a stationary position close to the passing tractor.","The test setup was based on a half-sided gate through which the tractor could drive.","The field tests were carried out on a farm in Bavaria, Germany, during tillage.","During the field tests, other parameters such as soil moisture and wind speed were controlled, as these significantly affect dust development.","We validated our dataset with contrast enhancement and image dehazing algorithms and analyzed the generalizability from recordings from the moving tractor.","Finally, we demonstrate the application of dust removal based on a high-level vision task, such as person classification.","Our empirical study confirms the validity of RB-Dust for vision-based dust removal in agriculture."],"url":"http://arxiv.org/abs/2306.07244v1"}
{"created":"2023-06-12","title":"Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches","abstract":"We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural object. Followed by the two clustering models, strokes of each type are further parsed into groups, each representing either a single edge or a single face. Next, groups representing edges are approximated with B-spline curves, followed by a topology-recovering process identifying and fixing desired connectivities between the curves forming a well-connected curve network. Next, groups representing the faces are employed to detect the cycles bounding patches in the curve network, resulting in the final surface mesh geometry of the architectural object. We confirm the usability of Strokes2Surface via a user study and further validate and compare our results against a range of reconstructions computed using alternative methods. We also introduce our manually labeled dataset of 4D architectural design sketches for further use in the community.","sentences":["We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design.","The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design.","The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties.","Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models.","In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural object.","Followed by the two clustering models, strokes of each type are further parsed into groups, each representing either a single edge or a single face.","Next, groups representing edges are approximated with B-spline curves, followed by a topology-recovering process identifying and fixing desired connectivities between the curves forming a well-connected curve network.","Next, groups representing the faces are employed to detect the cycles bounding patches in the curve network, resulting in the final surface mesh geometry of the architectural object.","We confirm the usability of Strokes2Surface via a user study and further validate and compare our results against a range of reconstructions computed using alternative methods.","We also introduce our manually labeled dataset of 4D architectural design sketches for further use in the community."],"url":"http://arxiv.org/abs/2306.07220v1"}
{"created":"2023-06-12","title":"Making Binary Classification from Multiple Unlabeled Datasets Almost Free of Supervision","abstract":"Training a classifier exploiting a huge amount of supervised data is expensive or even prohibited in a situation, where the labeling cost is high. The remarkable progress in working with weaker forms of supervision is binary classification from multiple unlabeled datasets which requires the knowledge of exact class priors for all unlabeled datasets. However, the availability of class priors is restrictive in many real-world scenarios. To address this issue, we propose to solve a new problem setting, i.e., binary classification from multiple unlabeled datasets with only one pairwise numerical relationship of class priors (MU-OPPO), which knows the relative order (which unlabeled dataset has a higher proportion of positive examples) of two class-prior probabilities for two datasets among multiple unlabeled datasets. In MU-OPPO, we do not need the class priors for all unlabeled datasets, but we only require that there exists a pair of unlabeled datasets for which we know which unlabeled dataset has a larger class prior. Clearly, this form of supervision is easier to be obtained, which can make labeling costs almost free. We propose a novel framework to handle the MU-OPPO problem, which consists of four sequential modules: (i) pseudo label assignment; (ii) confident example collection; (iii) class prior estimation; (iv) classifier training with estimated class priors. Theoretically, we analyze the gap between estimated class priors and true class priors under the proposed framework. Empirically, we confirm the superiority of our framework with comprehensive experiments. Experimental results demonstrate that our framework brings smaller estimation errors of class priors and better performance of binary classification.","sentences":["Training a classifier exploiting a huge amount of supervised data is expensive or even prohibited in a situation, where the labeling cost is high.","The remarkable progress in working with weaker forms of supervision is binary classification from multiple unlabeled datasets which requires the knowledge of exact class priors for all unlabeled datasets.","However, the availability of class priors is restrictive in many real-world scenarios.","To address this issue, we propose to solve a new problem setting, i.e., binary classification from multiple unlabeled datasets with only one pairwise numerical relationship of class priors (MU-OPPO), which knows the relative order (which unlabeled dataset has a higher proportion of positive examples) of two class-prior probabilities for two datasets among multiple unlabeled datasets.","In MU-OPPO, we do not need the class priors for all unlabeled datasets, but we only require that there exists a pair of unlabeled datasets for which we know which unlabeled dataset has a larger class prior.","Clearly, this form of supervision is easier to be obtained, which can make labeling costs almost free.","We propose a novel framework to handle the MU-OPPO problem, which consists of four sequential modules: (i) pseudo label assignment; (ii) confident example collection; (iii) class prior estimation; (iv) classifier training with estimated class priors.","Theoretically, we analyze the gap between estimated class priors and true class priors under the proposed framework.","Empirically, we confirm the superiority of our framework with comprehensive experiments.","Experimental results demonstrate that our framework brings smaller estimation errors of class priors and better performance of binary classification."],"url":"http://arxiv.org/abs/2306.07036v1"}
{"created":"2023-06-12","title":"Weakly supervised information extraction from inscrutable handwritten document images","abstract":"State-of-the-art information extraction methods are limited by OCR errors. They work well for printed text in form-like documents, but unstructured, handwritten documents still remain a challenge. Adapting existing models to domain-specific training data is quite expensive, because of two factors, 1) limited availability of the domain-specific documents (such as handwritten prescriptions, lab notes, etc.), and 2) annotations become even more challenging as one needs domain-specific knowledge to decode inscrutable handwritten document images. In this work, we focus on the complex problem of extracting medicine names from handwritten prescriptions using only weakly labeled data. The data consists of images along with the list of medicine names in it, but not their location in the image. We solve the problem by first identifying the regions of interest, i.e., medicine lines from just weak labels and then injecting a domain-specific medicine language model learned using only synthetically generated data. Compared to off-the-shelf state-of-the-art methods, our approach performs >2.5x better in medicine names extraction from prescriptions.","sentences":["State-of-the-art information extraction methods are limited by OCR errors.","They work well for printed text in form-like documents, but unstructured, handwritten documents still remain a challenge.","Adapting existing models to domain-specific training data is quite expensive, because of two factors, 1) limited availability of the domain-specific documents (such as handwritten prescriptions, lab notes, etc.), and 2) annotations become even more challenging as one needs domain-specific knowledge to decode inscrutable handwritten document images.","In this work, we focus on the complex problem of extracting medicine names from handwritten prescriptions using only weakly labeled data.","The data consists of images along with the list of medicine names in it, but not their location in the image.","We solve the problem by first identifying the regions of interest, i.e., medicine lines from just weak labels and then injecting a domain-specific medicine language model learned using only synthetically generated data.","Compared to off-the-shelf state-of-the-art methods, our approach performs >2.5x better in medicine names extraction from prescriptions."],"url":"http://arxiv.org/abs/2306.06823v1"}
{"created":"2023-06-12","title":"Fill-Up: Balancing Long-Tailed Data with Generative Models","abstract":"Modern text-to-image synthesis models have achieved an exceptional level of photorealism, generating high-quality images from arbitrary text descriptions. In light of the impressive synthesis ability, several studies have exhibited promising results in exploiting generated data for image recognition. However, directly supplementing data-hungry situations in the real-world (e.g. few-shot or long-tailed scenarios) with existing approaches result in marginal performance gains, as they suffer to thoroughly reflect the distribution of the real data. Through extensive experiments, this paper proposes a new image synthesis pipeline for long-tailed situations using Textual Inversion. The study demonstrates that generated images from textual-inverted text tokens effectively aligns with the real domain, significantly enhancing the recognition ability of a standard ResNet50 backbone. We also show that real-world data imbalance scenarios can be successfully mitigated by filling up the imbalanced data with synthetic images. In conjunction with techniques in the area of long-tailed recognition, our method achieves state-of-the-art results on standard long-tailed benchmarks when trained from scratch.","sentences":["Modern text-to-image synthesis models have achieved an exceptional level of photorealism, generating high-quality images from arbitrary text descriptions.","In light of the impressive synthesis ability, several studies have exhibited promising results in exploiting generated data for image recognition.","However, directly supplementing data-hungry situations in the real-world (e.g. few-shot or long-tailed scenarios) with existing approaches result in marginal performance gains, as they suffer to thoroughly reflect the distribution of the real data.","Through extensive experiments, this paper proposes a new image synthesis pipeline for long-tailed situations using Textual Inversion.","The study demonstrates that generated images from textual-inverted text tokens effectively aligns with the real domain, significantly enhancing the recognition ability of a standard ResNet50 backbone.","We also show that real-world data imbalance scenarios can be successfully mitigated by filling up the imbalanced data with synthetic images.","In conjunction with techniques in the area of long-tailed recognition, our method achieves state-of-the-art results on standard long-tailed benchmarks when trained from scratch."],"url":"http://arxiv.org/abs/2306.07200v1"}
{"created":"2023-06-12","title":"Latent Dynamical Implicit Diffusion Processes","abstract":"Latent dynamical models are commonly used to learn the distribution of a latent dynamical process that represents a sequence of noisy data samples. However, producing samples from such models with high fidelity is challenging due to the complexity and variability of latent and observation dynamics. Recent advances in diffusion-based generative models, such as DDPM and NCSN, have shown promising alternatives to state-of-the-art latent generative models, such as Neural ODEs, RNNs, and Normalizing flow networks, for generating high-quality sequential samples from a prior distribution. However, their application in modeling sequential data with latent dynamical models is yet to be explored. Here, we propose a novel latent variable model named latent dynamical implicit diffusion processes (LDIDPs), which utilizes implicit diffusion processes to sample from dynamical latent processes and generate sequential observation samples accordingly. We tested LDIDPs on synthetic and simulated neural decoding problems. We demonstrate that LDIDPs can accurately learn the dynamics over latent dimensions. Furthermore, the implicit sampling method allows for the computationally efficient generation of high-quality sequential data samples from the latent and observation spaces.","sentences":["Latent dynamical models are commonly used to learn the distribution of a latent dynamical process that represents a sequence of noisy data samples.","However, producing samples from such models with high fidelity is challenging due to the complexity and variability of latent and observation dynamics.","Recent advances in diffusion-based generative models, such as DDPM and NCSN, have shown promising alternatives to state-of-the-art latent generative models, such as Neural ODEs, RNNs, and Normalizing flow networks, for generating high-quality sequential samples from a prior distribution.","However, their application in modeling sequential data with latent dynamical models is yet to be explored.","Here, we propose a novel latent variable model named latent dynamical implicit diffusion processes (LDIDPs), which utilizes implicit diffusion processes to sample from dynamical latent processes and generate sequential observation samples accordingly.","We tested LDIDPs on synthetic and simulated neural decoding problems.","We demonstrate that LDIDPs can accurately learn the dynamics over latent dimensions.","Furthermore, the implicit sampling method allows for the computationally efficient generation of high-quality sequential data samples from the latent and observation spaces."],"url":"http://arxiv.org/abs/2306.07077v1"}
{"created":"2023-06-12","title":"Dynamic Causal Graph Convolutional Network for Traffic Prediction","abstract":"Modeling complex spatiotemporal dependencies in correlated traffic series is essential for traffic prediction. While recent works have shown improved prediction performance by using neural networks to extract spatiotemporal correlations, their effectiveness depends on the quality of the graph structures used to represent the spatial topology of the traffic network. In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data. We then use graph convolutional networks to generate traffic forecasts. To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs. Our experimental results on a real traffic dataset demonstrate the superior prediction performance of the proposed method.","sentences":["Modeling complex spatiotemporal dependencies in correlated traffic series is essential for traffic prediction.","While recent works have shown improved prediction performance by using neural networks to extract spatiotemporal correlations, their effectiveness depends on the quality of the graph structures used to represent the spatial topology of the traffic network.","In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data.","We then use graph convolutional networks to generate traffic forecasts.","To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs.","Our experimental results on a real traffic dataset demonstrate the superior prediction performance of the proposed method."],"url":"http://arxiv.org/abs/2306.07019v1"}
{"created":"2023-06-12","title":"Decoding Neutron Star Observations: Revealing Composition through Bayesian Neural Networks","abstract":"We exploit the great potential offered by Bayesian Neural Networks (BNNs) to directly decipher the internal composition of neutron stars (NSs) based on their macroscopic properties. By analyzing a set of simulated observations, namely NS radius and tidal deformability, we leverage BNNs as effective tools for inferring the proton fraction and sound speed within NS interiors. To achieve this, several BNNs models were developed upon a dataset of $\\sim$ 25K nuclear EoS within a relativistic mean-field framework, obtained through Bayesian inference that adheres to minimal low-density constraints. Unlike conventional neural networks, BNNs possess an exceptional quality: they provide a prediction uncertainty measure. To simulate the inherent imperfections present in real-world observations, we have generated four distinct training and testing datasets that replicate specific observational uncertainties. Our initial results demonstrate that BNNs successfully recover the composition with reasonable levels of uncertainty. Furthermore, using mock data prepared with the DD2, a different class of relativistic mean-field model utilized during training, the BNN model effectively retrieves the proton fraction and speed of sound for neutron star matter.","sentences":["We exploit the great potential offered by Bayesian Neural Networks (BNNs) to directly decipher the internal composition of neutron stars (NSs) based on their macroscopic properties.","By analyzing a set of simulated observations, namely NS radius and tidal deformability, we leverage BNNs as effective tools for inferring the proton fraction and sound speed within NS interiors.","To achieve this, several BNNs models were developed upon a dataset of $\\sim$ 25K nuclear EoS within a relativistic mean-field framework, obtained through Bayesian inference that adheres to minimal low-density constraints.","Unlike conventional neural networks, BNNs possess an exceptional quality: they provide a prediction uncertainty measure.","To simulate the inherent imperfections present in real-world observations, we have generated four distinct training and testing datasets that replicate specific observational uncertainties.","Our initial results demonstrate that BNNs successfully recover the composition with reasonable levels of uncertainty.","Furthermore, using mock data prepared with the DD2, a different class of relativistic mean-field model utilized during training, the BNN model effectively retrieves the proton fraction and speed of sound for neutron star matter."],"url":"http://arxiv.org/abs/2306.06929v1"}
{"created":"2023-06-12","title":"Sticker820K: Empowering Interactive Retrieval with Stickers","abstract":"Stickers have become a ubiquitous part of modern-day communication, conveying complex emotions through visual imagery. To facilitate the development of more powerful algorithms for analyzing stickers, we propose a large-scale Chinese sticker dataset, namely Sticker820K, which consists of 820k image-text pairs. Each sticker has rich and high-quality textual annotations, including descriptions, optical characters, emotional labels, and style classifications. Although vision-language tasks in the domain of natural images have been well studied, directly applying the those models, such as CLIP, to sticker data is not an optimal solution due to the discrepant nature between natural and emotive image data. Therefore, we propose StickerCLIP as a benchmark model on the Sticker820K dataset. For the text-to-image retrieval task, our StickerCLIP demonstrates strong superiority over the CLIP, which achieves an absolute gain of 66.0\\% in mean recall on the Sticker820K test set. Additionally, we endeavor to extend the recently popularized LLM by means of prompt tuning, integrating its ability for sticker retrieval and allowing users to retrieve stickers through instructions. We validate the feasibility of this method, demonstrating the immense potential of prompt tuning in expanding LLM abilities while not affecting the quality of upstream tasks.","sentences":["Stickers have become a ubiquitous part of modern-day communication, conveying complex emotions through visual imagery.","To facilitate the development of more powerful algorithms for analyzing stickers, we propose a large-scale Chinese sticker dataset, namely Sticker820K, which consists of 820k image-text pairs.","Each sticker has rich and high-quality textual annotations, including descriptions, optical characters, emotional labels, and style classifications.","Although vision-language tasks in the domain of natural images have been well studied, directly applying the those models, such as CLIP, to sticker data is not an optimal solution due to the discrepant nature between natural and emotive image data.","Therefore, we propose StickerCLIP as a benchmark model on the Sticker820K dataset.","For the text-to-image retrieval task, our StickerCLIP demonstrates strong superiority over the CLIP, which achieves an absolute gain of 66.0\\% in mean recall on the Sticker820K test set.","Additionally, we endeavor to extend the recently popularized LLM by means of prompt tuning, integrating its ability for sticker retrieval and allowing users to retrieve stickers through instructions.","We validate the feasibility of this method, demonstrating the immense potential of prompt tuning in expanding LLM abilities while not affecting the quality of upstream tasks."],"url":"http://arxiv.org/abs/2306.06870v1"}
{"created":"2023-06-12","title":"On More than Two Decades of Celestial Reference Frame VLBI Observations in the Deep South: IVS-CRDS (1995-2021)","abstract":"The International VLBI Service for Geodesy & Astrometry (IVS) regularly provides high-quality data to produce Earth Orientation Parameters (EOP), and for the maintenance and realization of the International Terrestrial and Celestial Reference Frames, ITRF and ICRF. The first iteration of the celestial reference frame (CRF) at radio wavelengths, the ICRF1, was adopted by the International Astronomical Union (IAU) in 1997 to replace the FK5 optical frame. Soon after, the IVS began official operations and in 2009 there was a significant increase in data sufficient to warrant a second iteration of the CRF, ICRF2. The most recent ICRF3, was adopted by the IAU in 2018. However, due to the geographic distribution of observing stations being concentrated in the Northern hemisphere, CRFs are generally weaker in the South due to there being fewer Southern Hemisphere observations. To increase the Southern Hemisphere observations, and the density, precision of the sources, a series of deep South observing sessions was initiated in 1995. This initiative in 2004 became the IVS Celestial Reference Frame Deep South (IVS-CRDS) observing program. This paper covers the evolution of the CRDS observing program for the period 1995 to 2021, details the data products and results, and concludes with a summary of upcoming improvements to this ongoing project.","sentences":["The International VLBI Service for Geodesy & Astrometry (IVS) regularly provides high-quality data to produce Earth Orientation Parameters (EOP), and for the maintenance and realization of the International Terrestrial and Celestial Reference Frames, ITRF and ICRF.","The first iteration of the celestial reference frame (CRF) at radio wavelengths, the ICRF1, was adopted by the International Astronomical Union (IAU) in 1997 to replace the FK5 optical frame.","Soon after, the IVS began official operations and in 2009 there was a significant increase in data sufficient to warrant a second iteration of the CRF, ICRF2.","The most recent ICRF3, was adopted by the IAU in 2018.","However, due to the geographic distribution of observing stations being concentrated in the Northern hemisphere, CRFs are generally weaker in the South due to there being fewer Southern Hemisphere observations.","To increase the Southern Hemisphere observations, and the density, precision of the sources, a series of deep South observing sessions was initiated in 1995.","This initiative in 2004 became the IVS Celestial Reference Frame Deep South (IVS-CRDS) observing program.","This paper covers the evolution of the CRDS observing program for the period 1995 to 2021, details the data products and results, and concludes with a summary of upcoming improvements to this ongoing project."],"url":"http://arxiv.org/abs/2306.06830v1"}
{"created":"2023-06-12","title":"HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models","abstract":"Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, the application of diffusion models for synthesizing time-varying audio faces limitations in terms of complexity and controllability, as speech synthesis requires very high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in singing voice synthesis, we propose HiddenSinger, a high-quality singing voice synthesis system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models in terms of audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.","sentences":["Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains.","However, in the speech domain, the application of diffusion models for synthesizing time-varying audio faces limitations in terms of complexity and controllability, as speech synthesis requires very high-dimensional samples with long-term acoustic features.","To alleviate the challenges posed by model complexity in singing voice synthesis, we propose HiddenSinger, a high-quality singing voice synthesis system using a neural audio codec and latent diffusion models.","To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector.","Subsequently, we use the latent diffusion models to sample a latent representation from a musical score.","In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset.","Experimental results demonstrate that our model outperforms previous models in terms of audio quality.","Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data."],"url":"http://arxiv.org/abs/2306.06814v1"}
{"created":"2023-06-12","title":"The Rapid ASKAP Continuum Survey IV: continuum imaging at 1367.5 MHz and the first data release of RACS-mid","abstract":"The Australian SKA Pathfinder (ASKAP) is being used to undertake a campaign to rapidly survey the sky in three frequency bands across its operational spectral range. The first pass of the Rapid ASKAP Continuum Survey (RACS) at 887.5 MHz in the low band has already been completed, with images, visibility datasets, and catalogues made available to the wider astronomical community through the CSIRO ASKAP Science Data Archive (CASDA). This work presents details of the second observing pass in the mid band at 1367.5 MHz, RACS-mid, and associated data release comprising images and visibility datasets covering the whole sky south of declination $+$49$^\\circ$. This data release incorporates selective peeling to reduce artefacts around bright sources, as well as accurately modelled primary beam responses. The Stokes I images reach a median noise of 198 $\\mu$Jy PSF$^{-1}$ with a declination-dependent angular resolution of 8.1 to 47.5 arcsec that fills a niche in the existing ecosystem of large-area astronomical surveys. We also supply Stokes V images after application of a widefield leakage correction, with a median noise of 165 $\\mu$Jy PSF$^{-1}$. We find the residual leakage of Stokes I into V to be $\\lesssim$ 0.9 to 2.4 % over the survey. This initial RACS-mid data release will be complemented by a future release comprising catalogues of the survey region. As with other RACS data releases, data products from this release will be made available through CASDA.","sentences":["The Australian SKA Pathfinder (ASKAP) is being used to undertake a campaign to rapidly survey the sky in three frequency bands across its operational spectral range.","The first pass of the Rapid ASKAP Continuum Survey (RACS) at 887.5 MHz in the low band has already been completed, with images, visibility datasets, and catalogues made available to the wider astronomical community through the CSIRO ASKAP Science Data Archive (CASDA).","This work presents details of the second observing pass in the mid band at 1367.5 MHz, RACS-mid, and associated data release comprising images and visibility datasets covering the whole sky south of declination $+$49$^\\circ$. This data release incorporates selective peeling to reduce artefacts around bright sources, as well as accurately modelled primary beam responses.","The Stokes I images reach a median noise of 198 $\\mu$Jy PSF$^{-1}$ with a declination-dependent angular resolution of 8.1 to 47.5 arcsec that fills a niche in the existing ecosystem of large-area astronomical surveys.","We also supply Stokes V images after application of a widefield leakage correction, with a median noise of 165 $\\mu$Jy PSF$^{-1}$.","We find the residual leakage of Stokes I into V to be $\\lesssim$ 0.9 to 2.4 % over the survey.","This initial RACS-mid data release will be complemented by a future release comprising catalogues of the survey region.","As with other RACS data releases, data products from this release will be made available through CASDA."],"url":"http://arxiv.org/abs/2306.07194v1"}
