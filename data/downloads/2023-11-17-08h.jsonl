{"created":"2023-11-16 18:59:51","title":"The Chosen One: Consistent Characters in Text-to-Image Diffusion Models","abstract":"Recent advances in text-to-image generation models have unlocked vast potential for visual creativity. However, these models struggle with generation of consistent characters, a crucial aspect for numerous real-world applications such as story visualization, game development asset design, advertising, and more. Current methods typically rely on multiple pre-existing images of the target character or involve labor-intensive manual processes. In this work, we propose a fully automated solution for consistent character generation, with the sole input being a text prompt. We introduce an iterative procedure that, at each stage, identifies a coherent set of images sharing a similar identity and extracts a more consistent identity from this set. Our quantitative analysis demonstrates that our method strikes a better balance between prompt alignment and identity consistency compared to the baseline methods, and these findings are reinforced by a user study. To conclude, we showcase several practical applications of our approach. Project page is available at https://omriavrahami.com/the-chosen-one","sentences":["Recent advances in text-to-image generation models have unlocked vast potential for visual creativity.","However, these models struggle with generation of consistent characters, a crucial aspect for numerous real-world applications such as story visualization, game development asset design, advertising, and more.","Current methods typically rely on multiple pre-existing images of the target character or involve labor-intensive manual processes.","In this work, we propose a fully automated solution for consistent character generation, with the sole input being a text prompt.","We introduce an iterative procedure that, at each stage, identifies a coherent set of images sharing a similar identity and extracts a more consistent identity from this set.","Our quantitative analysis demonstrates that our method strikes a better balance between prompt alignment and identity consistency compared to the baseline methods, and these findings are reinforced by a user study.","To conclude, we showcase several practical applications of our approach.","Project page is available at https://omriavrahami.com/the-chosen-one"],"url":"http://arxiv.org/abs/2311.10093v1"}
{"created":"2023-11-16 18:59:46","title":"Traffic Video Object Detection using Motion Prior","abstract":"Traffic videos inherently differ from generic videos in their stationary camera setup, thus providing a strong motion prior where objects often move in a specific direction over a short time interval. Existing works predominantly employ generic video object detection framework for traffic video object detection, which yield certain advantages such as broad applicability and robustness to diverse scenarios. However, they fail to harness the strength of motion prior to enhance detection accuracy. In this work, we propose two innovative methods to exploit the motion prior and boost the performance of both fully-supervised and semi-supervised traffic video object detection. Firstly, we introduce a new self-attention module that leverages the motion prior to guide temporal information integration in the fully-supervised setting. Secondly, we utilise the motion prior to develop a pseudo-labelling mechanism to eliminate noisy pseudo labels for the semi-supervised setting. Both of our motion-prior-centred methods consistently demonstrates superior performance, outperforming existing state-of-the-art approaches by a margin of 2% in terms of mAP.","sentences":["Traffic videos inherently differ from generic videos in their stationary camera setup, thus providing a strong motion prior where objects often move in a specific direction over a short time interval.","Existing works predominantly employ generic video object detection framework for traffic video object detection, which yield certain advantages such as broad applicability and robustness to diverse scenarios.","However, they fail to harness the strength of motion prior to enhance detection accuracy.","In this work, we propose two innovative methods to exploit the motion prior and boost the performance of both fully-supervised and semi-supervised traffic video object detection.","Firstly, we introduce a new self-attention module that leverages the motion prior to guide temporal information integration in the fully-supervised setting.","Secondly, we utilise the motion prior to develop a pseudo-labelling mechanism to eliminate noisy pseudo labels for the semi-supervised setting.","Both of our motion-prior-centred methods consistently demonstrates superior performance, outperforming existing state-of-the-art approaches by a margin of 2% in terms of mAP."],"url":"http://arxiv.org/abs/2311.10092v1"}
{"created":"2023-11-16 18:58:55","title":"Adaptive Shells for Efficient Neural Radiance Field Rendering","abstract":"Neural radiance fields achieve unprecedented quality for novel view synthesis, but their volumetric formulation remains expensive, requiring a huge number of samples to render high-resolution images. Volumetric encodings are essential to represent fuzzy geometry such as foliage and hair, and they are well-suited for stochastic optimization. Yet, many scenes ultimately consist largely of solid surfaces which can be accurately rendered by a single sample per pixel. Based on this insight, we propose a neural radiance formulation that smoothly transitions between volumetric- and surface-based rendering, greatly accelerating rendering speed and even improving visual fidelity. Our method constructs an explicit mesh envelope which spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To this end, we generalize the NeuS formulation with a learned spatially-varying kernel size which encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with width determined by the kernel size, and fine-tune the radiance field within this band. At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Experiments show that our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.","sentences":["Neural radiance fields achieve unprecedented quality for novel view synthesis, but their volumetric formulation remains expensive, requiring a huge number of samples to render high-resolution images.","Volumetric encodings are essential to represent fuzzy geometry such as foliage and hair, and they are well-suited for stochastic optimization.","Yet, many scenes ultimately consist largely of solid surfaces which can be accurately rendered by a single sample per pixel.","Based on this insight, we propose a neural radiance formulation that smoothly transitions between volumetric- and surface-based rendering, greatly accelerating rendering speed and even improving visual fidelity.","Our method constructs an explicit mesh envelope which spatially bounds a neural volumetric representation.","In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample.","To this end, we generalize the NeuS formulation with a learned spatially-varying kernel size which encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions.","We then extract an explicit mesh of a narrow band around the surface, with width determined by the kernel size, and fine-tune the radiance field within this band.","At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required.","Experiments show that our approach enables efficient rendering at very high fidelity.","We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation."],"url":"http://arxiv.org/abs/2311.10091v1"}
{"created":"2023-11-16 18:58:43","title":"JaxMARL: Multi-Agent RL Environments in JAX","abstract":"Benchmarks play an important role in the development of machine learning algorithms. For example, research in reinforcement learning (RL) has been heavily influenced by available environments and benchmarks. However, RL environments are traditionally run on the CPU, limiting their scalability with typical academic compute. Recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles, enabling massively parallel RL training pipelines and environments. This is particularly useful for multi-agent reinforcement learning (MARL) research. First of all, multiple agents must be considered at each environment step, adding computational burden, and secondly, the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges. In this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms. When considering wall clock time, our experiments show that per-run our JAX-based training pipeline is up to 12500x faster than existing approaches. This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. We provide code at https://github.com/flairox/jaxmarl.","sentences":["Benchmarks play an important role in the development of machine learning algorithms.","For example, research in reinforcement learning (RL) has been heavily influenced by available environments and benchmarks.","However, RL environments are traditionally run on the CPU, limiting their scalability with typical academic compute.","Recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles, enabling massively parallel RL training pipelines and environments.","This is particularly useful for multi-agent reinforcement learning (MARL) research.","First of all, multiple agents must be considered at each environment step, adding computational burden, and secondly, the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges.","In this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms.","When considering wall clock time, our experiments show that per-run our JAX-based training pipeline is up to 12500x faster than existing approaches.","This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field.","We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine.","This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL.","We provide code at https://github.com/flairox/jaxmarl."],"url":"http://arxiv.org/abs/2311.10090v1"}
{"created":"2023-11-16 18:55:58","title":"Emu Edit: Precise Image Editing via Recognition and Generation Tasks","abstract":"Instruction-based image editing holds immense potential for a variety of applications, as it enables users to perform any editing operation using a natural language instruction. However, current models in this domain often struggle with accurately executing user instructions. We present Emu Edit, a multi-task image editing model which sets state-of-the-art results in instruction-based image editing. To develop Emu Edit we train it to multi-task across an unprecedented range of tasks, such as region-based editing, free-form editing, and Computer Vision tasks, all of which are formulated as generative tasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we provide it with learned task embeddings which guide the generation process towards the correct edit type. Both these elements are essential for Emu Edit's outstanding performance. Furthermore, we show that Emu Edit can generalize to new tasks, such as image inpainting, super-resolution, and compositions of editing tasks, with just a few labeled examples. This capability offers a significant advantage in scenarios where high-quality samples are scarce. Lastly, to facilitate a more rigorous and informed assessment of instructable image editing models, we release a new challenging and versatile benchmark that includes seven different image editing tasks.","sentences":["Instruction-based image editing holds immense potential for a variety of applications, as it enables users to perform any editing operation using a natural language instruction.","However, current models in this domain often struggle with accurately executing user instructions.","We present Emu Edit, a multi-task image editing model which sets state-of-the-art results in instruction-based image editing.","To develop Emu Edit we train it to multi-task across an unprecedented range of tasks, such as region-based editing, free-form editing, and Computer Vision tasks, all of which are formulated as generative tasks.","Additionally, to enhance Emu Edit's multi-task learning abilities, we provide it with learned task embeddings which guide the generation process towards the correct edit type.","Both these elements are essential for Emu Edit's outstanding performance.","Furthermore, we show that Emu Edit can generalize to new tasks, such as image inpainting, super-resolution, and compositions of editing tasks, with just a few labeled examples.","This capability offers a significant advantage in scenarios where high-quality samples are scarce.","Lastly, to facilitate a more rigorous and informed assessment of instructable image editing models, we release a new challenging and versatile benchmark that includes seven different image editing tasks."],"url":"http://arxiv.org/abs/2311.10089v1"}
{"created":"2023-11-16 18:44:22","title":"A Computationally Efficient Sparsified Online Newton Method","abstract":"Second-order methods hold significant promise for enhancing the convergence of deep neural network training; however, their large memory and computational demands have limited their practicality. Thus there is a need for scalable second-order methods that can efficiently train large models. In this paper, we introduce the Sparsified Online Newton (SONew) method, a memory-efficient second-order algorithm that yields a sparsified yet effective preconditioner. The algorithm emerges from a novel use of the LogDet matrix divergence measure; we combine it with sparsity constraints to minimize regret in the online convex optimization framework. Empirically, we test our method on large scale benchmarks of up to 1B parameters. We achieve up to 30% faster convergence, 3.4% relative improvement in validation performance, and 80% relative improvement in training loss, in comparison to memory efficient optimizers including first order methods. Powering the method is a surprising fact -- imposing structured sparsity patterns, like tridiagonal and banded structure, requires little to no overhead, making it as efficient and parallelizable as first-order methods. In wall-clock time, tridiagonal SONew is only about 3% slower per step than first-order methods but gives overall gains due to much faster convergence. In contrast, one of the state-of-the-art (SOTA) memory-intensive second-order methods, Shampoo, is unable to scale to large benchmarks. Additionally, while Shampoo necessitates significant engineering efforts to scale to large benchmarks, SONew offers a more straightforward implementation, increasing its practical appeal. SONew code is available at: https://github.com/devvrit/SONew","sentences":["Second-order methods hold significant promise for enhancing the convergence of deep neural network training; however, their large memory and computational demands have limited their practicality.","Thus there is a need for scalable second-order methods that can efficiently train large models.","In this paper, we introduce the Sparsified Online Newton (SONew) method, a memory-efficient second-order algorithm that yields a sparsified yet effective preconditioner.","The algorithm emerges from a novel use of the LogDet matrix divergence measure; we combine it with sparsity constraints to minimize regret in the online convex optimization framework.","Empirically, we test our method on large scale benchmarks of up to 1B parameters.","We achieve up to 30% faster convergence, 3.4% relative improvement in validation performance, and 80% relative improvement in training loss, in comparison to memory efficient optimizers including first order methods.","Powering the method is a surprising fact -- imposing structured sparsity patterns, like tridiagonal and banded structure, requires little to no overhead, making it as efficient and parallelizable as first-order methods.","In wall-clock time, tridiagonal SONew is only about 3% slower per step than first-order methods but gives overall gains due to much faster convergence.","In contrast, one of the state-of-the-art (SOTA) memory-intensive second-order methods, Shampoo, is unable to scale to large benchmarks.","Additionally, while Shampoo necessitates significant engineering efforts to scale to large benchmarks, SONew offers a more straightforward implementation, increasing its practical appeal.","SONew code is available at: https://github.com/devvrit/SONew"],"url":"http://arxiv.org/abs/2311.10085v1"}
{"created":"2023-11-16 18:38:25","title":"Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations","abstract":"We propose a theoretical framework for formulating language model decoder algorithms with dynamic programming and information theory. With dynamic programming, we lift the design of decoder algorithms from the logit space to the action-state value function space, and show that the decoding algorithms are consequences of optimizing the action-state value functions. Each component in the action-state value function space has an information theoretical interpretation. With the lifting and interpretation, it becomes evident what the decoder algorithm is optimized for, and hence facilitating the arbitration of the tradeoffs in sensibleness, diversity, and attribution.","sentences":["We propose a theoretical framework for formulating language model decoder algorithms with dynamic programming and information theory.","With dynamic programming, we lift the design of decoder algorithms from the logit space to the action-state value function space, and show that the decoding algorithms are consequences of optimizing the action-state value functions.","Each component in the action-state value function space has an information theoretical interpretation.","With the lifting and interpretation, it becomes evident what the decoder algorithm is optimized for, and hence facilitating the arbitration of the tradeoffs in sensibleness, diversity, and attribution."],"url":"http://arxiv.org/abs/2311.10083v1"}
{"created":"2023-11-16 18:37:29","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback","abstract":"We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs.","sentences":["We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs.","First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences.","Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses.","Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak.","This reduces the capacity for effective multi-turn interactions.","To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement.","The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences.","The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions.","To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training.","Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs."],"url":"http://arxiv.org/abs/2311.10081v1"}
{"created":"2023-11-16 18:30:14","title":"ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond","abstract":"Purpose: Enhanced health literacy has been linked to better health outcomes; however, few interventions have been studied. We investigate whether large language models (LLMs) can serve as a medium to improve health literacy in children and other populations.   Methods: We ran 288 conditions using 26 different prompts through ChatGPT-3.5, Microsoft Bing, and Google Bard. Given constraints imposed by rate limits, we tested a subset of 150 conditions through ChatGPT-4. The primary outcome measurements were the reading grade level (RGL) and word counts of output.   Results: Across all models, output for basic prompts such as \"Explain\" and \"What is (are)\" were at, or exceeded, a 10th-grade RGL. When prompts were specified to explain conditions from the 1st to 12th RGL, we found that LLMs had varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided responses that ranged from the 7th-grade to college freshmen RGL while ChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL. Microsoft Bing provided responses from the 9th to 11th RGL while Google Bard provided responses from the 7th to 10th RGL.   Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade level outputs. Meanwhile Bard and Bing tended to consistently produce an RGL that is at the high school level regardless of prompt. Additionally, Bard's hesitancy in providing certain outputs indicates a cautious approach towards health information. LLMs demonstrate promise in enhancing health communication, but future research should verify the accuracy and effectiveness of such tools in this context.   Implications: LLMs face challenges in crafting outputs below a sixth-grade reading level. However, their capability to modify outputs above this threshold provides a potential mechanism to improve health literacy and communication in a pediatric population and beyond.","sentences":["Purpose: Enhanced health literacy has been linked to better health outcomes; however, few interventions have been studied.","We investigate whether large language models (LLMs) can serve as a medium to improve health literacy in children and other populations.   ","Methods: We ran 288 conditions using 26 different prompts through ChatGPT-3.5, Microsoft Bing, and Google Bard.","Given constraints imposed by rate limits, we tested a subset of 150 conditions through ChatGPT-4.","The primary outcome measurements were the reading grade level (RGL) and word counts of output.   ","Results:","Across all models, output for basic prompts such as \"Explain\" and \"What is (are)\" were at, or exceeded, a 10th-grade RGL.","When prompts were specified to explain conditions from the 1st to 12th RGL, we found that LLMs had varying abilities to tailor responses based on RGL.","ChatGPT-3.5 provided responses that ranged from the 7th-grade to college freshmen RGL while ChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL.","Microsoft Bing provided responses from the 9th to 11th RGL while Google Bard provided responses from the 7th to 10th RGL.   ","Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade level outputs.","Meanwhile Bard and Bing tended to consistently produce an RGL that is at the high school level regardless of prompt.","Additionally, Bard's hesitancy in providing certain outputs indicates a cautious approach towards health information.","LLMs demonstrate promise in enhancing health communication, but future research should verify the accuracy and effectiveness of such tools in this context.   ","Implications: LLMs face challenges in crafting outputs below a sixth-grade reading level.","However, their capability to modify outputs above this threshold provides a potential mechanism to improve health literacy and communication in a pediatric population and beyond."],"url":"http://arxiv.org/abs/2311.10075v1"}
{"created":"2023-11-16 18:02:10","title":"Visual Environment Assessment for Safe Autonomous Quadrotor Landing","abstract":"Autonomous identification and evaluation of safe landing zones are of paramount importance for ensuring the safety and effectiveness of aerial robots in the event of system failures, low battery, or the successful completion of specific tasks. In this paper, we present a novel approach for detection and assessment of potential landing sites for safe quadrotor landing. Our solution efficiently integrates 2D and 3D environmental information, eliminating the need for external aids such as GPS and computationally intensive elevation maps. The proposed pipeline combines semantic data derived from a Neural Network (NN), to extract environmental features, with geometric data obtained from a disparity map, to extract critical geometric attributes such as slope, flatness, and roughness. We define several cost metrics based on these attributes to evaluate safety, stability, and suitability of regions in the environments and identify the most suitable landing area. Our approach runs in real-time on quadrotors equipped with limited computational capabilities. Experimental results conducted in diverse environments demonstrate that the proposed method can effectively assess and identify suitable landing areas, enabling the safe and autonomous landing of a quadrotor.","sentences":["Autonomous identification and evaluation of safe landing zones are of paramount importance for ensuring the safety and effectiveness of aerial robots in the event of system failures, low battery, or the successful completion of specific tasks.","In this paper, we present a novel approach for detection and assessment of potential landing sites for safe quadrotor landing.","Our solution efficiently integrates 2D and 3D environmental information, eliminating the need for external aids such as GPS and computationally intensive elevation maps.","The proposed pipeline combines semantic data derived from a Neural Network (NN), to extract environmental features, with geometric data obtained from a disparity map, to extract critical geometric attributes such as slope, flatness, and roughness.","We define several cost metrics based on these attributes to evaluate safety, stability, and suitability of regions in the environments and identify the most suitable landing area.","Our approach runs in real-time on quadrotors equipped with limited computational capabilities.","Experimental results conducted in diverse environments demonstrate that the proposed method can effectively assess and identify suitable landing areas, enabling the safe and autonomous landing of a quadrotor."],"url":"http://arxiv.org/abs/2311.10065v1"}
{"created":"2023-11-16 18:00:38","title":"Analyzing Deviations of Dyadic Lines in Fast Hough Transform","abstract":"Fast Hough transform is a widely used algorithm in pattern recognition. The algorithm relies on approximating lines using a specific discrete line model called dyadic lines. The worst-case deviation of a dyadic line from the ideal line it used to construct grows as $O(log(n))$, where $n$ is the linear size of the image. But few lines actually reach the worst-case bound. The present paper addresses a statistical analysis of the deviation of a dyadic line from its ideal counterpart. Specifically, our findings show that the mean deviation is zero, and the variance grows as $O(log(n))$. As $n$ increases, the distribution of these (suitably normalized) deviations converges towards a normal distribution with zero mean and a small variance. This limiting result makes an essential use of ergodic theory.","sentences":["Fast Hough transform is a widely used algorithm in pattern recognition.","The algorithm relies on approximating lines using a specific discrete line model called dyadic lines.","The worst-case deviation of a dyadic line from the ideal line it used to construct grows as $O(log(n))$, where $n$ is the linear size of the image.","But few lines actually reach the worst-case bound.","The present paper addresses a statistical analysis of the deviation of a dyadic line from its ideal counterpart.","Specifically, our findings show that the mean deviation is zero, and the variance grows as $O(log(n))$. As $n$ increases, the distribution of these (suitably normalized) deviations converges towards a normal distribution with zero mean and a small variance.","This limiting result makes an essential use of ergodic theory."],"url":"http://arxiv.org/abs/2311.10064v1"}
{"created":"2023-11-16 17:52:21","title":"The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation","abstract":"We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of high-quality audio-caption pairs, designed for the evaluation of music-and-language models. The dataset consists of 1.1k human-written natural language descriptions of 706 music recordings, all publicly accessible and released under Creative Common licenses. To showcase the use of our dataset, we benchmark popular models on three key music-and-language tasks (music captioning, text-to-music generation and music-language retrieval). Our experiments highlight the importance of cross-dataset evaluation and offer insights into how researchers can use SDD to gain a broader understanding of model performance.","sentences":["We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of high-quality audio-caption pairs, designed for the evaluation of music-and-language models.","The dataset consists of 1.1k human-written natural language descriptions of 706 music recordings, all publicly accessible and released under Creative Common licenses.","To showcase the use of our dataset, we benchmark popular models on three key music-and-language tasks (music captioning, text-to-music generation and music-language retrieval).","Our experiments highlight the importance of cross-dataset evaluation and offer insights into how researchers can use SDD to gain a broader understanding of model performance."],"url":"http://arxiv.org/abs/2311.10057v1"}
{"created":"2023-11-16 17:49:00","title":"The Minimum Clique Routing Problem on Cycles","abstract":"In the Minimum Clique Routing Problem on Cycles \\textsc{MCRPC} we are given a cycle together with a set of demands (weighted origin-destination pairs) and the goal is to route all the pairs minimizing the maximum weighted clique of the intersection graph induced by the routing. The vertices of this graph are the demands with their corresponding weights and two demands are adjacent when their routes share at least one arc. In this work we are not only interested in the \\textsc{MCRPC} but also in two natural subproblems. First, we consider the situation where the demands are disjoint, in the sense that every two demands do not share any of their corresponding ends. Second, we analyze the subproblem where the weights of the routes are all equal. We first show that the problem is NP-complete even in the subproblem of disjoint demands. For the case of arbitrary weights, we exhibit a simple combinatorial 2-approximation algorithm and a $\\frac{3}{2}$-approximation algorithm based on rounding a solution of a relaxation of an integer linear programming formulation of our problem. Finally, we give a Fixed Parameter Tractable algorithm for the case of uniform weights, whose parameter is related to the maximum degree of the intersection graph induced by any routing.","sentences":["In the Minimum Clique Routing Problem on Cycles \\textsc{MCRPC} we are given a cycle together with a set of demands (weighted origin-destination pairs) and the goal is to route all the pairs minimizing the maximum weighted clique of the intersection graph induced by the routing.","The vertices of this graph are the demands with their corresponding weights and two demands are adjacent when their routes share at least one arc.","In this work we are not only interested in the \\textsc{MCRPC} but also in two natural subproblems.","First, we consider the situation where the demands are disjoint, in the sense that every two demands do not share any of their corresponding ends.","Second, we analyze the subproblem where the weights of the routes are all equal.","We first show that the problem is NP-complete even in the subproblem of disjoint demands.","For the case of arbitrary weights, we exhibit a simple combinatorial 2-approximation algorithm and a $\\frac{3}{2}$-approximation algorithm based on rounding a solution of a relaxation of an integer linear programming formulation of our problem.","Finally, we give a Fixed Parameter Tractable algorithm for the case of uniform weights, whose parameter is related to the maximum degree of the intersection graph induced by any routing."],"url":"http://arxiv.org/abs/2311.10055v1"}
{"created":"2023-11-16 17:48:55","title":"Is \"A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts","abstract":"Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of the default system prompt. But is \"a helpful assistant\" the best role for LLMs? In this study, we present a systematic evaluation of how social roles in system prompts affect model performance. We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 types of occupations. Through extensive analysis of 3 popular LLMs and 2457 questions, we show that adding interpersonal roles in prompts consistently improves the models' performance over a range of questions. Moreover, while we find that using gender-neutral roles and specifying the role as the audience leads to better performances, predicting which role leads to the best performance remains a challenging task, and that frequency, similarity, and perplexity do not fully explain the effect of social roles on model performances. Our results can help inform the design of system prompts for AI systems. Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.","sentences":["Prompting serves as the major way humans interact with Large Language Models (LLM).","Commercial AI systems commonly define the role of the LLM in system prompts.","For example, ChatGPT uses \"You are a helpful assistant\" as part of the default system prompt.","But is \"a helpful assistant\" the best role for LLMs?","In this study, we present a systematic evaluation of how social roles in system prompts affect model performance.","We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 types of occupations.","Through extensive analysis of 3 popular LLMs and 2457 questions, we show that adding interpersonal roles in prompts consistently improves the models' performance over a range of questions.","Moreover, while we find that using gender-neutral roles and specifying the role as the audience leads to better performances, predicting which role leads to the best performance remains a challenging task, and that frequency, similarity, and perplexity do not fully explain the effect of social roles on model performances.","Our results can help inform the design of system prompts for AI systems.","Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles."],"url":"http://arxiv.org/abs/2311.10054v1"}
{"created":"2023-11-16 17:45:59","title":"Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces","abstract":"Despite the prevalence of tabular datasets, few-shot learning remains under-explored within this domain. Existing few-shot methods are not directly applicable to tabular datasets due to varying column relationships, meanings, and permutational invariance. To address these challenges, we propose FLAT-a novel approach to tabular few-shot learning, encompassing knowledge sharing between datasets with heterogeneous feature spaces. Utilizing an encoder inspired by Dataset2Vec, FLAT learns low-dimensional embeddings of datasets and their individual columns, which facilitate knowledge transfer and generalization to previously unseen datasets. A decoder network parametrizes the predictive target network, implemented as a Graph Attention Network, to accommodate the heterogeneous nature of tabular datasets. Experiments on a diverse collection of 118 UCI datasets demonstrate FLAT's successful generalization to new tabular datasets and a considerable improvement over the baselines.","sentences":["Despite the prevalence of tabular datasets, few-shot learning remains under-explored within this domain.","Existing few-shot methods are not directly applicable to tabular datasets due to varying column relationships, meanings, and permutational invariance.","To address these challenges, we propose FLAT-a novel approach to tabular few-shot learning, encompassing knowledge sharing between datasets with heterogeneous feature spaces.","Utilizing an encoder inspired by Dataset2Vec, FLAT learns low-dimensional embeddings of datasets and their individual columns, which facilitate knowledge transfer and generalization to previously unseen datasets.","A decoder network parametrizes the predictive target network, implemented as a Graph Attention Network, to accommodate the heterogeneous nature of tabular datasets.","Experiments on a diverse collection of 118 UCI datasets demonstrate FLAT's successful generalization to new tabular datasets and a considerable improvement over the baselines."],"url":"http://arxiv.org/abs/2311.10051v1"}
{"created":"2023-11-16 17:45:49","title":"Graph models for Cybersecurity -- A Survey","abstract":"Graph models are helpful means of analyzing computer networks as well as complex system architectures for security. In this paper we evaluate the current state of research for representing and analysing cyber-attack using graph models, i.e. attack graph (AG) formalisms. We propose a taxonomy on attack graph formalisms, based on 70 models, which we analysed with respect to their \\textit{graph semantic}, involved agents and analysis features. Additionally, we adress which formalisms allow for automatic attack graph generation from raw or processes data inputs. Our taxonomy is especially designed to help users and applied researchers identify a suitable AG model for their needs. A summary of the individual AG formalisms is provided as supplementary material.","sentences":["Graph models are helpful means of analyzing computer networks as well as complex system architectures for security.","In this paper we evaluate the current state of research for representing and analysing cyber-attack using graph models, i.e. attack graph (AG) formalisms.","We propose a taxonomy on attack graph formalisms, based on 70 models, which we analysed with respect to their \\textit{graph semantic}, involved agents and analysis features.","Additionally, we adress which formalisms allow for automatic attack graph generation from raw or processes data inputs.","Our taxonomy is especially designed to help users and applied researchers identify a suitable AG model for their needs.","A summary of the individual AG formalisms is provided as supplementary material."],"url":"http://arxiv.org/abs/2311.10050v1"}
{"created":"2023-11-16 17:45:37","title":"Inherently Interpretable Time Series Classification via Multiple Instance Learning","abstract":"Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the first to develop general MIL methods for TSC and apply them to an extensive variety of domains","sentences":["Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes.","In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification.","We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance.","We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation.","On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods.","To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the first to develop general MIL methods for TSC and apply them to an extensive variety of domains"],"url":"http://arxiv.org/abs/2311.10049v1"}
{"created":"2023-11-16 17:43:13","title":"Frozen Set Design for Precoded Polar Codes","abstract":"This paper focuses on the frozen set design for precoded polar codes decoded by the successive cancellation list (SCL) algorithm. We propose a novel frozen set design method, whose computational complexity is low due to the use of analytical bounds and constrained frozen set structure. We derive new bounds based on the recently published complexity analysis of SCL with near maximum-likelihood (ML) performance. To predict the ML performance, we employ the state-of-the-art bounds relying on the code weight distribution. The bounds and constrained frozen set structure are incorporated into the genetic algorithm to generate optimized frozen sets with low complexity. Our simulation results show that the constructed precoded polar codes of length 512 have a superior frame error rate (FER) performance compared to the state-of-the-art codes under SCL decoding with various list sizes.","sentences":["This paper focuses on the frozen set design for precoded polar codes decoded by the successive cancellation list (SCL) algorithm.","We propose a novel frozen set design method, whose computational complexity is low due to the use of analytical bounds and constrained frozen set structure.","We derive new bounds based on the recently published complexity analysis of SCL with near maximum-likelihood (ML) performance.","To predict the ML performance, we employ the state-of-the-art bounds relying on the code weight distribution.","The bounds and constrained frozen set structure are incorporated into the genetic algorithm to generate optimized frozen sets with low complexity.","Our simulation results show that the constructed precoded polar codes of length 512 have a superior frame error rate (FER) performance compared to the state-of-the-art codes under SCL decoding with various list sizes."],"url":"http://arxiv.org/abs/2311.10047v1"}
{"created":"2023-11-16 17:38:21","title":"Depth Insight -- Contribution of Different Features to Indoor Single-image Depth Estimation","abstract":"Depth estimation from a single image is a challenging problem in computer vision because binocular disparity or motion information is absent. Whereas impressive performances have been reported in this area recently using end-to-end trained deep neural architectures, as to what cues in the images that are being exploited by these black box systems is hard to know. To this end, in this work, we quantify the relative contributions of the known cues of depth in a monocular depth estimation setting using an indoor scene data set. Our work uses feature extraction techniques to relate the single features of shape, texture, colour and saturation, taken in isolation, to predict depth. We find that the shape of objects extracted by edge detection substantially contributes more than others in the indoor setting considered, while the other features also have contributions in varying degrees. These insights will help optimise depth estimation models, boosting their accuracy and robustness. They promise to broaden the practical applications of vision-based depth estimation. The project code is attached to the supplementary material and will be published on GitHub.","sentences":["Depth estimation from a single image is a challenging problem in computer vision because binocular disparity or motion information is absent.","Whereas impressive performances have been reported in this area recently using end-to-end trained deep neural architectures, as to what cues in the images that are being exploited by these black box systems is hard to know.","To this end, in this work, we quantify the relative contributions of the known cues of depth in a monocular depth estimation setting using an indoor scene data set.","Our work uses feature extraction techniques to relate the single features of shape, texture, colour and saturation, taken in isolation, to predict depth.","We find that the shape of objects extracted by edge detection substantially contributes more than others in the indoor setting considered, while the other features also have contributions in varying degrees.","These insights will help optimise depth estimation models, boosting their accuracy and robustness.","They promise to broaden the practical applications of vision-based depth estimation.","The project code is attached to the supplementary material and will be published on GitHub."],"url":"http://arxiv.org/abs/2311.10042v1"}
{"created":"2023-11-16 17:37:46","title":"Interpretable Reinforcement Learning for Robotics and Continuous Control","abstract":"Interpretability in machine learning is critical for the safe deployment of learned policies across legally-regulated and safety-critical domains. While gradient-based approaches in reinforcement learning have achieved tremendous success in learning policies for continuous control problems such as robotics and autonomous driving, the lack of interpretability is a fundamental barrier to adoption. We propose Interpretable Continuous Control Trees (ICCTs), a tree-based model that can be optimized via modern, gradient-based, reinforcement learning approaches to produce high-performing, interpretable policies. The key to our approach is a procedure for allowing direct optimization in a sparse decision-tree-like representation. We validate ICCTs against baselines across six domains, showing that ICCTs are capable of learning policies that parity or outperform baselines by up to 33% in autonomous driving scenarios while achieving a 300x-600x reduction in the number of parameters against deep learning baselines. We prove that ICCTs can serve as universal function approximators and display analytically that ICCTs can be verified in linear time. Furthermore, we deploy ICCTs in two realistic driving domains, based on interstate Highway-94 and 280 in the US. Finally, we verify ICCT's utility with end-users and find that ICCTs are rated easier to simulate, quicker to validate, and more interpretable than neural networks.","sentences":["Interpretability in machine learning is critical for the safe deployment of learned policies across legally-regulated and safety-critical domains.","While gradient-based approaches in reinforcement learning have achieved tremendous success in learning policies for continuous control problems such as robotics and autonomous driving, the lack of interpretability is a fundamental barrier to adoption.","We propose Interpretable Continuous Control Trees (ICCTs), a tree-based model that can be optimized via modern, gradient-based, reinforcement learning approaches to produce high-performing, interpretable policies.","The key to our approach is a procedure for allowing direct optimization in a sparse decision-tree-like representation.","We validate ICCTs against baselines across six domains, showing that ICCTs are capable of learning policies that parity or outperform baselines by up to 33% in autonomous driving scenarios while achieving a 300x-600x reduction in the number of parameters against deep learning baselines.","We prove that ICCTs can serve as universal function approximators and display analytically that ICCTs can be verified in linear time.","Furthermore, we deploy ICCTs in two realistic driving domains, based on interstate Highway-94 and 280 in the US.","Finally, we verify ICCT's utility with end-users and find that ICCTs are rated easier to simulate, quicker to validate, and more interpretable than neural networks."],"url":"http://arxiv.org/abs/2311.10041v1"}
{"created":"2023-11-16 17:37:13","title":"A characterization of efficiently compilable constraint languages","abstract":"A central task in knowledge compilation is to compile a CNF-SAT instance into a succinct representation format that allows efficient operations such as testing satisfiability, counting, or enumerating all solutions. Useful representation formats studied in this area range from ordered binary decision diagrams (OBDDs) to circuits in decomposable negation normal form (DNNFs).   While it is known that there exist CNF formulas that require exponential size representations, the situation is less well studied for other types of constraints than Boolean disjunctive clauses. The constraint satisfaction problem (CSP) is a powerful framework that generalizes CNF-SAT by allowing arbitrary sets of constraints over any finite domain. The main goal of our work is to understand for which type of constraints (also called the constraint language) it is possible to efficiently compute representations of polynomial size. We answer this question completely and prove two tight characterizations of efficiently compilable constraint languages, depending on whether target format is structured.   We first identify the combinatorial property of ``strong blockwise decomposability'' and show that if a constraint language has this property, we can compute DNNF representations of linear size. For all other constraint languages we construct families of CSP-instances that provably require DNNFs of exponential size. For a subclass of ``strong uniformly blockwise decomposable'' constraint languages we obtain a similar dichotomy for structured DNNFs. In fact, strong (uniform) blockwise decomposability even allows efficient compilation into multi-valued analogs of OBDDs and FBDDs, respectively. Thus, we get complete characterizations for all knowledge compilation classes between O(B)DDs and DNNFs.","sentences":["A central task in knowledge compilation is to compile a CNF-SAT instance into a succinct representation format that allows efficient operations such as testing satisfiability, counting, or enumerating all solutions.","Useful representation formats studied in this area range from ordered binary decision diagrams (OBDDs) to circuits in decomposable negation normal form (DNNFs).   ","While it is known that there exist CNF formulas that require exponential size representations, the situation is less well studied for other types of constraints than Boolean disjunctive clauses.","The constraint satisfaction problem (CSP) is a powerful framework that generalizes CNF-SAT by allowing arbitrary sets of constraints over any finite domain.","The main goal of our work is to understand for which type of constraints (also called the constraint language)","it is possible to efficiently compute representations of polynomial size.","We answer this question completely and prove two tight characterizations of efficiently compilable constraint languages, depending on whether target format is structured.   ","We first identify the combinatorial property of ``strong blockwise decomposability'' and show that if a constraint language has this property, we can compute DNNF representations of linear size.","For all other constraint languages we construct families of CSP-instances that provably require DNNFs of exponential size.","For a subclass of ``strong uniformly blockwise decomposable'' constraint languages we obtain a similar dichotomy for structured DNNFs.","In fact, strong (uniform) blockwise decomposability even allows efficient compilation into multi-valued analogs of OBDDs and FBDDs, respectively.","Thus, we get complete characterizations for all knowledge compilation classes between O(B)DDs and DNNFs."],"url":"http://arxiv.org/abs/2311.10040v1"}
{"created":"2023-11-16 17:36:28","title":"Software Dependability Measurement at the Age Of 36","abstract":"Thirty-six years after the first edition of IEEE standard 982.1, Measures of the Software Aspects of Dependability, the third edition focuses on the measurement of in-service software dependability. This article explains how this new point of view evolved and shaped the third edition's guidance for software dependability measurement.","sentences":["Thirty-six years after the first edition of IEEE standard 982.1, Measures of the Software Aspects of Dependability, the third edition focuses on the measurement of in-service software dependability.","This article explains how this new point of view evolved and shaped the third edition's guidance for software dependability measurement."],"url":"http://arxiv.org/abs/2311.10039v1"}
{"created":"2023-11-16 17:32:58","title":"Match and Locate: low-frequency monocular odometry based on deep feature matching","abstract":"Accurate and robust pose estimation plays a crucial role in many robotic systems. Popular algorithms for pose estimation typically rely on high-fidelity and high-frequency signals from various sensors. Inclusion of these sensors makes the system less affordable and much more complicated. In this work we introduce a novel approach for the robotic odometry which only requires a single camera and, importantly, can produce reliable estimates given even extremely low-frequency signal of around one frame per second. The approach is based on matching image features between the consecutive frames of the video stream using deep feature matching models. The resulting coarse estimate is then adjusted by a convolutional neural network, which is also responsible for estimating the scale of the transition, otherwise irretrievable using only the feature matching information. We evaluate the performance of the approach in the AISG-SLA Visual Localisation Challenge and find that while being computationally efficient and easy to implement our method shows competitive results with only around $3^{\\circ}$ of orientation estimation error and $2m$ of translation estimation error taking the third place in the challenge.","sentences":["Accurate and robust pose estimation plays a crucial role in many robotic systems.","Popular algorithms for pose estimation typically rely on high-fidelity and high-frequency signals from various sensors.","Inclusion of these sensors makes the system less affordable and much more complicated.","In this work we introduce a novel approach for the robotic odometry which only requires a single camera and, importantly, can produce reliable estimates given even extremely low-frequency signal of around one frame per second.","The approach is based on matching image features between the consecutive frames of the video stream using deep feature matching models.","The resulting coarse estimate is then adjusted by a convolutional neural network, which is also responsible for estimating the scale of the transition, otherwise irretrievable using only the feature matching information.","We evaluate the performance of the approach in the AISG-SLA Visual Localisation Challenge and find that while being computationally efficient and easy to implement our method shows competitive results with only around $3^{\\circ}$ of orientation estimation error and $2m$ of translation estimation error taking the third place in the challenge."],"url":"http://arxiv.org/abs/2311.10034v1"}
{"created":"2023-11-16 17:14:07","title":"A Novel Neural Network-Based Federated Learning System for Imbalanced and Non-IID Data","abstract":"With the growth of machine learning techniques, privacy of data of users has become a major concern. Most of the machine learning algorithms rely heavily on large amount of data which may be collected from various sources. Collecting these data yet maintaining privacy policies has become one of the most challenging tasks for the researchers. To combat this issue, researchers have introduced federated learning, where a prediction model is learnt by ensuring the privacy of data of clients data. However, the prevalent federated learning algorithms possess an accuracy and efficiency trade-off, especially for non-IID data. In this research, we propose a centralized, neural network-based federated learning system. The centralized algorithm incorporates micro-level parallel processing inspired by the traditional mini-batch algorithm where the client devices and the server handle the forward and backward propagation respectively. We also devise a semi-centralized version of our proposed algorithm. This algorithm takes advantage of edge computing for minimizing the load from the central server, where clients handle both the forward and backward propagation while sacrificing the overall train time to some extent. We evaluate our proposed systems on five well-known benchmark datasets and achieve satisfactory performance in a reasonable time across various data distribution settings as compared to some existing benchmark algorithms.","sentences":["With the growth of machine learning techniques, privacy of data of users has become a major concern.","Most of the machine learning algorithms rely heavily on large amount of data which may be collected from various sources.","Collecting these data yet maintaining privacy policies has become one of the most challenging tasks for the researchers.","To combat this issue, researchers have introduced federated learning, where a prediction model is learnt by ensuring the privacy of data of clients data.","However, the prevalent federated learning algorithms possess an accuracy and efficiency trade-off, especially for non-IID data.","In this research, we propose a centralized, neural network-based federated learning system.","The centralized algorithm incorporates micro-level parallel processing inspired by the traditional mini-batch algorithm where the client devices and the server handle the forward and backward propagation respectively.","We also devise a semi-centralized version of our proposed algorithm.","This algorithm takes advantage of edge computing for minimizing the load from the central server, where clients handle both the forward and backward propagation while sacrificing the overall train time to some extent.","We evaluate our proposed systems on five well-known benchmark datasets and achieve satisfactory performance in a reasonable time across various data distribution settings as compared to some existing benchmark algorithms."],"url":"http://arxiv.org/abs/2311.10025v1"}
{"created":"2023-11-16 17:02:34","title":"On the Overconfidence Problem in Semantic 3D Mapping","abstract":"Semantic 3D mapping, the process of fusing depth and image segmentation information between multiple views to build 3D maps annotated with object classes in real-time, is a recent topic of interest. This paper highlights the fusion overconfidence problem, in which conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion on a modular ObjectNav agent improves its success rates. Our code will be provided on Github for reproducibility upon acceptance.","sentences":["Semantic 3D mapping, the process of fusing depth and image segmentation information between multiple views to build 3D maps annotated with object classes in real-time, is a recent topic of interest.","This paper highlights the fusion overconfidence problem, in which conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs.","Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset.","We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability.","We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion on a modular ObjectNav agent improves its success rates.","Our code will be provided on Github for reproducibility upon acceptance."],"url":"http://arxiv.org/abs/2311.10018v1"}
{"created":"2023-11-16 16:53:52","title":"Finding Real-World Orbital Motion Laws from Data","abstract":"A novel approach is presented for discovering PDEs that govern the motion of satellites in space. The method is based on SINDy, a data-driven technique capable of identifying the underlying dynamics of complex physical systems from time series data. SINDy is utilized to uncover PDEs that describe the laws of physics in space, which are non-deterministic and influenced by various factors such as drag or the reference area (related to the attitude of the satellite). In contrast to prior works, the physically interpretable coordinate system is maintained, and no dimensionality reduction technique is applied to the data. By training the model with multiple representative trajectories of LEO - encompassing various inclinations, eccentricities, and altitudes - and testing it with unseen orbital motion patterns, a mean error of around 140 km for the positions and 0.12 km/s for the velocities is achieved. The method offers the advantage of delivering interpretable, accurate, and complex models of orbital motion that can be employed for propagation or as inputs to predictive models for other variables of interest, such as atmospheric drag or the probability of collision in an encounter with a spacecraft or space objects. In conclusion, the work demonstrates the promising potential of using SINDy to discover the equations governing the behaviour of satellites in space. The technique has been successfully applied to uncover PDEs describing the motion of satellites in LEO with high accuracy. The method possesses several advantages over traditional models, including the ability to provide physically interpretable, accurate, and complex models of orbital motion derived from high-entropy datasets. These models can be utilised for propagation or as inputs to predictive models for other variables of interest.","sentences":["A novel approach is presented for discovering PDEs that govern the motion of satellites in space.","The method is based on SINDy, a data-driven technique capable of identifying the underlying dynamics of complex physical systems from time series data.","SINDy is utilized to uncover PDEs that describe the laws of physics in space, which are non-deterministic and influenced by various factors such as drag or the reference area (related to the attitude of the satellite).","In contrast to prior works, the physically interpretable coordinate system is maintained, and no dimensionality reduction technique is applied to the data.","By training the model with multiple representative trajectories of LEO - encompassing various inclinations, eccentricities, and altitudes - and testing it with unseen orbital motion patterns, a mean error of around 140 km for the positions and 0.12 km/s for the velocities is achieved.","The method offers the advantage of delivering interpretable, accurate, and complex models of orbital motion that can be employed for propagation or as inputs to predictive models for other variables of interest, such as atmospheric drag or the probability of collision in an encounter with a spacecraft or space objects.","In conclusion, the work demonstrates the promising potential of using SINDy to discover the equations governing the behaviour of satellites in space.","The technique has been successfully applied to uncover PDEs describing the motion of satellites in LEO with high accuracy.","The method possesses several advantages over traditional models, including the ability to provide physically interpretable, accurate, and complex models of orbital motion derived from high-entropy datasets.","These models can be utilised for propagation or as inputs to predictive models for other variables of interest."],"url":"http://arxiv.org/abs/2311.10012v1"}
{"created":"2023-11-16 16:50:56","title":"SQLNet: Scale-Modulated Query and Localization Network for Few-Shot Class-Agnostic Counting","abstract":"The class-agnostic counting (CAC) task has recently been proposed to solve the problem of counting all objects of an arbitrary class with several exemplars given in the input image. To address this challenging task, existing leading methods all resort to density map regression, which renders them impractical for downstream tasks that require object locations and restricts their ability to well explore the scale information of exemplars for supervision. To address the limitations, we propose a novel localization-based CAC approach, termed Scale-modulated Query and Localization Network (SQLNet). It fully explores the scales of exemplars in both the query and localization stages and achieves effective counting by accurately locating each object and predicting its approximate size. Specifically, during the query stage, rich discriminative representations of the target class are acquired by the Hierarchical Exemplars Collaborative Enhancement (HECE) module from the few exemplars through multi-scale exemplar cooperation with equifrequent size prompt embedding. These representations are then fed into the Exemplars-Unified Query Correlation (EUQC) module to interact with the query features in a unified manner and produce the correlated query tensor. In the localization stage, the Scale-aware Multi-head Localization (SAML) module utilizes the query tensor to predict the confidence, location, and size of each potential object. Moreover, a scale-aware localization loss is introduced, which exploits flexible location associations and exemplar scales for supervision to optimize the model performance. Extensive experiments demonstrate that SQLNet outperforms state-of-the-art methods on popular CAC benchmarks, achieving excellent performance not only in counting accuracy but also in localization and bounding box generation. Our codes will be available at https://github.com/HCPLab-SYSU/SQLNet","sentences":["The class-agnostic counting (CAC) task has recently been proposed to solve the problem of counting all objects of an arbitrary class with several exemplars given in the input image.","To address this challenging task, existing leading methods all resort to density map regression, which renders them impractical for downstream tasks that require object locations and restricts their ability to well explore the scale information of exemplars for supervision.","To address the limitations, we propose a novel localization-based CAC approach, termed Scale-modulated Query and Localization Network (SQLNet).","It fully explores the scales of exemplars in both the query and localization stages and achieves effective counting by accurately locating each object and predicting its approximate size.","Specifically, during the query stage, rich discriminative representations of the target class are acquired by the Hierarchical Exemplars Collaborative Enhancement (HECE) module from the few exemplars through multi-scale exemplar cooperation with equifrequent size prompt embedding.","These representations are then fed into the Exemplars-Unified Query Correlation (EUQC) module to interact with the query features in a unified manner and produce the correlated query tensor.","In the localization stage, the Scale-aware Multi-head Localization (SAML) module utilizes the query tensor to predict the confidence, location, and size of each potential object.","Moreover, a scale-aware localization loss is introduced, which exploits flexible location associations and exemplar scales for supervision to optimize the model performance.","Extensive experiments demonstrate that SQLNet outperforms state-of-the-art methods on popular CAC benchmarks, achieving excellent performance not only in counting accuracy but also in localization and bounding box generation.","Our codes will be available at https://github.com/HCPLab-SYSU/SQLNet"],"url":"http://arxiv.org/abs/2311.10011v1"}
{"created":"2023-11-16 16:36:37","title":"Towards Flexibility and Robustness of LSM Trees","abstract":"Log-Structured Merge trees (LSM trees) are increasingly used as part of the storage engine behind several data systems, and are frequently deployed in the cloud. As the number of applications relying on LSM-based storage backends increases, the problem of performance tuning of LSM trees receives increasing attention. We consider both nominal tunings - where workload and execution environment are accurately known a priori - and robust tunings - which consider uncertainty in the workload knowledge. This type of workload uncertainty is common in modern applications, notably in shared infrastructure environments like the public cloud.   To address this problem, we introduce ENDURE, a new paradigm for tuning LSM trees in the presence of workload uncertainty. Specifically, we focus on the impact of the choice of compaction policy, size ratio, and memory allocation on the overall performance. ENDURE considers a robust formulation of the throughput maximization problem and recommends a tuning that offers near-optimal throughput when the executed workload is not the same, instead in a neighborhood of the expected workload. Additionally, we explore the robustness of flexible LSM designs by proposing a new unified design called K-LSM that encompasses existing designs. We deploy our robust tuning system, ENDURE, on a state-of-the-art key-value store, RocksDB, and demonstrate throughput improvements of up to 5x in the presence of uncertainty. Our results indicate that the tunings obtained by ENDURE are more robust than tunings obtained under our expanded LSM design space. This indicates that robustness may not be inherent to a design, instead, it is an outcome of a tuning process that explicitly accounts for uncertainty.","sentences":["Log-Structured Merge trees (LSM trees) are increasingly used as part of the storage engine behind several data systems, and are frequently deployed in the cloud.","As the number of applications relying on LSM-based storage backends increases, the problem of performance tuning of LSM trees receives increasing attention.","We consider both nominal tunings - where workload and execution environment are accurately known a priori - and robust tunings - which consider uncertainty in the workload knowledge.","This type of workload uncertainty is common in modern applications, notably in shared infrastructure environments like the public cloud.   ","To address this problem, we introduce ENDURE, a new paradigm for tuning LSM trees in the presence of workload uncertainty.","Specifically, we focus on the impact of the choice of compaction policy, size ratio, and memory allocation on the overall performance.","ENDURE considers a robust formulation of the throughput maximization problem and recommends a tuning that offers near-optimal throughput when the executed workload is not the same, instead in a neighborhood of the expected workload.","Additionally, we explore the robustness of flexible LSM designs by proposing a new unified design called K-LSM that encompasses existing designs.","We deploy our robust tuning system, ENDURE, on a state-of-the-art key-value store, RocksDB, and demonstrate throughput improvements of up to 5x in the presence of uncertainty.","Our results indicate that the tunings obtained by ENDURE are more robust than tunings obtained under our expanded LSM design space.","This indicates that robustness may not be inherent to a design, instead, it is an outcome of a tuning process that explicitly accounts for uncertainty."],"url":"http://arxiv.org/abs/2311.10005v1"}
{"created":"2023-11-16 16:30:04","title":"Straggler-resilient Federated Learning: Tackling Computation Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network","abstract":"Federated Learning (FL) enables many resource-limited devices to train a model collaboratively without data sharing. However, many existing works focus on model-homogeneous FL, where the global and local models are the same size, ignoring the inherently heterogeneous computational capabilities of different devices and restricting resource-constrained devices from contributing to FL. In this paper, we consider model-heterogeneous FL and propose Federated Partial Model Training (FedPMT), where devices with smaller computational capabilities work on partial models (subsets of the global model) and contribute to the global model. Different from Dropout-based partial model generation, which removes neurons in hidden layers at random, model training in FedPMT is achieved from the back-propagation perspective. As such, all devices in FedPMT prioritize the most crucial parts of the global model. Theoretical analysis shows that the proposed partial model training design has a similar convergence rate to the widely adopted Federated Averaging (FedAvg) algorithm, $\\mathcal{O}(1/T)$, with the sub-optimality gap enlarged by a constant factor related to the model splitting design in FedPMT. Empirical results show that FedPMT significantly outperforms the existing benchmark FedDrop. Meanwhile, compared to the popular model-homogeneous benchmark, FedAvg, FedPMT reaches the learning target in a shorter completion time, thus achieving a better trade-off between learning accuracy and completion time.","sentences":["Federated Learning (FL) enables many resource-limited devices to train a model collaboratively without data sharing.","However, many existing works focus on model-homogeneous FL, where the global and local models are the same size, ignoring the inherently heterogeneous computational capabilities of different devices and restricting resource-constrained devices from contributing to FL.","In this paper, we consider model-heterogeneous FL and propose Federated Partial Model Training (FedPMT), where devices with smaller computational capabilities work on partial models (subsets of the global model) and contribute to the global model.","Different from Dropout-based partial model generation, which removes neurons in hidden layers at random, model training in FedPMT is achieved from the back-propagation perspective.","As such, all devices in FedPMT prioritize the most crucial parts of the global model.","Theoretical analysis shows that the proposed partial model training design has a similar convergence rate to the widely adopted Federated Averaging (FedAvg) algorithm, $\\mathcal{O}(1/T)$, with the sub-optimality gap enlarged by a constant factor related to the model splitting design in FedPMT.","Empirical results show that FedPMT significantly outperforms the existing benchmark FedDrop.","Meanwhile, compared to the popular model-homogeneous benchmark, FedAvg, FedPMT reaches the learning target in a shorter completion time, thus achieving a better trade-off between learning accuracy and completion time."],"url":"http://arxiv.org/abs/2311.10002v1"}
{"created":"2023-11-16 16:23:11","title":"TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection","abstract":"Surface anomaly detection is a vital component in manufacturing inspection. Reconstructive anomaly detection methods restore the normal appearance of an object, ideally modifying only the anomalous regions. Due to the limitations of commonly used reconstruction architectures, the produced reconstructions are often poor and either still contain anomalies or lack details in anomaly-free regions. Recent reconstructive methods adopt diffusion models, however with the standard diffusion process the problems are not adequately addressed. We propose a novel transparency-based diffusion process, where the transparency of anomalous regions is progressively increased, restoring their normal appearance accurately and maintaining the appearance of anomaly-free regions without loss of detail. We propose TRANSparency DifFUSION (TransFusion), a discriminative anomaly detection method that implements the proposed diffusion process, enabling accurate downstream anomaly detection. TransFusion achieves state-of-the-art performance on both the VisA and the MVTec AD datasets, with an image-level AUROC of 98.5% and 99.2%, respectively.","sentences":["Surface anomaly detection is a vital component in manufacturing inspection.","Reconstructive anomaly detection methods restore the normal appearance of an object, ideally modifying only the anomalous regions.","Due to the limitations of commonly used reconstruction architectures, the produced reconstructions are often poor and either still contain anomalies or lack details in anomaly-free regions.","Recent reconstructive methods adopt diffusion models, however with the standard diffusion process the problems are not adequately addressed.","We propose a novel transparency-based diffusion process, where the transparency of anomalous regions is progressively increased, restoring their normal appearance accurately and maintaining the appearance of anomaly-free regions without loss of detail.","We propose TRANSparency DifFUSION (TransFusion), a discriminative anomaly detection method that implements the proposed diffusion process, enabling accurate downstream anomaly detection.","TransFusion achieves state-of-the-art performance on both the VisA and the MVTec AD datasets, with an image-level AUROC of 98.5% and 99.2%, respectively."],"url":"http://arxiv.org/abs/2311.09999v1"}
{"created":"2023-11-16 16:14:58","title":"DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's Distance","abstract":"The Earth Mover's Distance (EMD) is the measure of choice between point clouds. However the computational cost to compute it makes it prohibitive as a training loss, and the standard approach is to use a surrogate such as the Chamfer distance. We propose an attention-based model to compute an accurate approximation of the EMD that can be used as a training loss for generative models. To get the necessary accurate estimation of the gradients we train our model to explicitly compute the matching between point clouds instead of EMD itself. We cast this new objective as the estimation of an attention matrix that approximates the ground truth matching matrix. Experiments show that this model provides an accurate estimate of the EMD and its gradient with a wall clock speed-up of more than two orders of magnitude with respect to the exact Hungarian matching algorithm and one order of magnitude with respect to the standard approximate Sinkhorn algorithm, allowing in particular to train a point cloud VAE with the EMD itself. Extensive evaluation show the remarkable behaviour of this model when operating out-of-distribution, a key requirement for a distance surrogate. Finally, the model generalizes very well to point clouds during inference several times larger than during training.","sentences":["The Earth Mover's Distance (EMD) is the measure of choice between point clouds.","However the computational cost to compute it makes it prohibitive as a training loss, and the standard approach is to use a surrogate such as the Chamfer distance.","We propose an attention-based model to compute an accurate approximation of the EMD that can be used as a training loss for generative models.","To get the necessary accurate estimation of the gradients we train our model to explicitly compute the matching between point clouds instead of EMD itself.","We cast this new objective as the estimation of an attention matrix that approximates the ground truth matching matrix.","Experiments show that this model provides an accurate estimate of the EMD and its gradient with a wall clock speed-up of more than two orders of magnitude with respect to the exact Hungarian matching algorithm and one order of magnitude with respect to the standard approximate Sinkhorn algorithm, allowing in particular to train a point cloud VAE with the EMD itself.","Extensive evaluation show the remarkable behaviour of this model when operating out-of-distribution, a key requirement for a distance surrogate.","Finally, the model generalizes very well to point clouds during inference several times larger than during training."],"url":"http://arxiv.org/abs/2311.09998v1"}
{"created":"2023-11-16 16:09:44","title":"Towards more Practical Threat Models in Artificial Intelligence Security","abstract":"Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI. For example, while models are often studied in isolation, they form part of larger ML pipelines in practice. Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical. We take a first step towards describing the full extent of this disparity. To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with \\textbf{271} industrial practitioners. On the one hand, we find that all existing threat models are indeed applicable. On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings. Our paper is thus a call for action to study more practical threat models in artificial intelligence security.","sentences":["Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI.","For example, while models are often studied in isolation, they form part of larger ML pipelines in practice.","Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical.","We take a first step towards describing the full extent of this disparity.","To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with \\textbf{271} industrial practitioners.","On the one hand, we find that all existing threat models are indeed applicable.","On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings.","Our paper is thus a call for action to study more practical threat models in artificial intelligence security."],"url":"http://arxiv.org/abs/2311.09994v1"}
{"created":"2023-11-16 16:09:43","title":"Generative AI for Hate Speech Detection: Evaluation and Findings","abstract":"Automatic hate speech detection using deep neural models is hampered by the scarcity of labeled datasets, leading to poor generalization. To mitigate this problem, generative AI has been utilized to generate large amounts of synthetic hate speech sequences from available labeled examples, leveraging the generated data in finetuning large pre-trained language models (LLMs). In this chapter, we provide a review of relevant methods, experimental setups and evaluation of this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT, we apply and evaluate the impact of train set augmentation with generated data using LLMs that have been already adapted for hate detection, including RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical study corroborates our previous findings, showing that this approach improves hate speech generalization, boosting recall performance across data distributions. In addition, we explore and compare the performance of the finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results demonstrate that while better generalization is achieved using the GPT-3.5 model, it achieves mediocre recall and low precision on most datasets. It is an open question whether the sensitivity of models such as GPT-3.5, and onward, can be improved using similar techniques of text generation.","sentences":["Automatic hate speech detection using deep neural models is hampered by the scarcity of labeled datasets, leading to poor generalization.","To mitigate this problem, generative AI has been utilized to generate large amounts of synthetic hate speech sequences from available labeled examples, leveraging the generated data in finetuning large pre-trained language models (LLMs).","In this chapter, we provide a review of relevant methods, experimental setups and evaluation of this approach.","In addition to general LLMs, such as BERT, RoBERTa and ALBERT, we apply and evaluate the impact of train set augmentation with generated data using LLMs that have been already adapted for hate detection, including RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen.","An empirical study corroborates our previous findings, showing that this approach improves hate speech generalization, boosting recall performance across data distributions.","In addition, we explore and compare the performance of the finetuned LLMs with zero-shot hate detection using a GPT-3.5 model.","Our results demonstrate that while better generalization is achieved using the GPT-3.5 model, it achieves mediocre recall and low precision on most datasets.","It is an open question whether the sensitivity of models such as GPT-3.5, and onward, can be improved using similar techniques of text generation."],"url":"http://arxiv.org/abs/2311.09993v1"}
{"created":"2023-11-16 16:08:52","title":"Market Research on IIoT Standard Compliance Monitoring Providers and deriving Attributes for IIoT Compliance Monitoring","abstract":"Adapting security architectures to common standards like IEC 62443 or ISO 27000 in the Industrial Internet of Things (IIoT) involves complex processes and compliance reports. Automatic monitoring of compliance status would enhance this process. Despite limited research, practical applications exist. This paper conducts a market study on providers implementing IEC 62443 in IIoT, aiming to formulate a catalog of monitorable attributes aligned with the standard. The study reveals challenges, such as a lack of formal separation in security architectures, limiting visibility. Despite these challenges, practical implementations share commonalities, providing insights into viable monitoring properties. The research serves as a crucial entry point into developing a comprehensive catalog of monitorable attributes for IEC 62443 standards in IIoT.   Aligned with the IEC 62443 SR catalog of document 3-3, monitorable attributes are derived based on current research about IIoT security and Expert Knowledge. The provided tables serve as an exemplary extract, not exhaustive, defining three types of attributes based on their origin of creation.","sentences":["Adapting security architectures to common standards like IEC 62443 or ISO 27000 in the Industrial Internet of Things (IIoT) involves complex processes and compliance reports.","Automatic monitoring of compliance status would enhance this process.","Despite limited research, practical applications exist.","This paper conducts a market study on providers implementing IEC 62443 in IIoT, aiming to formulate a catalog of monitorable attributes aligned with the standard.","The study reveals challenges, such as a lack of formal separation in security architectures, limiting visibility.","Despite these challenges, practical implementations share commonalities, providing insights into viable monitoring properties.","The research serves as a crucial entry point into developing a comprehensive catalog of monitorable attributes for IEC 62443 standards in IIoT.   Aligned with the IEC 62443 SR catalog of document 3-3, monitorable attributes are derived based on current research about IIoT security and Expert Knowledge.","The provided tables serve as an exemplary extract, not exhaustive, defining three types of attributes based on their origin of creation."],"url":"http://arxiv.org/abs/2311.09991v1"}
{"created":"2023-11-16 16:07:19","title":"Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI Experience","abstract":"The rapid proliferation of data across diverse fields has accentuated the importance of accurate imputation for missing values. This task is crucial for ensuring data integrity and deriving meaningful insights. In response to this challenge, we present Xputer, a novel imputation tool that adeptly integrates Non-negative Matrix Factorization (NMF) with the predictive strengths of XGBoost. One of Xputer's standout features is its versatility: it supports zero imputation, enables hyperparameter optimization through Optuna, and allows users to define the number of iterations. For enhanced user experience and accessibility, we have equipped Xputer with an intuitive Graphical User Interface (GUI) ensuring ease of handling, even for those less familiar with computational tools. In performance benchmarks, Xputer not only rivals the computational speed of established tools such as IterativeImputer but also often outperforms them in terms of imputation accuracy. Furthermore, Xputer autonomously handles a diverse spectrum of data types, including categorical, continuous, and Boolean, eliminating the need for prior preprocessing. Given its blend of performance, flexibility, and user-friendly design, Xputer emerges as a state-of-the-art solution in the realm of data imputation.","sentences":["The rapid proliferation of data across diverse fields has accentuated the importance of accurate imputation for missing values.","This task is crucial for ensuring data integrity and deriving meaningful insights.","In response to this challenge, we present Xputer, a novel imputation tool that adeptly integrates Non-negative Matrix Factorization (NMF) with the predictive strengths of XGBoost.","One of Xputer's standout features is its versatility: it supports zero imputation, enables hyperparameter optimization through Optuna, and allows users to define the number of iterations.","For enhanced user experience and accessibility, we have equipped Xputer with an intuitive Graphical User Interface (GUI) ensuring ease of handling, even for those less familiar with computational tools.","In performance benchmarks, Xputer not only rivals the computational speed of established tools such as IterativeImputer but also often outperforms them in terms of imputation accuracy.","Furthermore, Xputer autonomously handles a diverse spectrum of data types, including categorical, continuous, and Boolean, eliminating the need for prior preprocessing.","Given its blend of performance, flexibility, and user-friendly design, Xputer emerges as a state-of-the-art solution in the realm of data imputation."],"url":"http://arxiv.org/abs/2311.09989v1"}
{"created":"2023-11-16 16:03:37","title":"A Framework for Modeling, Analyzing, and Decision-Making in Disease Spread Dynamics and Medicine/Vaccine Distribution","abstract":"The challenges posed by epidemics and pandemics are immense, especially if the causes are novel. This article introduces a versatile open-source simulation framework designed to model intricate dynamics of infectious diseases across diverse population centres. Taking inspiration from historical precedents such as the Spanish flu and COVID-19, and geographical economic theories such as Central place theory, the simulation integrates agent-based modelling to depict the movement and interactions of individuals within different settlement hierarchies. Additionally, the framework provides a tool for decision-makers to assess and strategize optimal distribution plans for limited resources like vaccines or cures as well as to impose mobility restrictions.","sentences":["The challenges posed by epidemics and pandemics are immense, especially if the causes are novel.","This article introduces a versatile open-source simulation framework designed to model intricate dynamics of infectious diseases across diverse population centres.","Taking inspiration from historical precedents such as the Spanish flu and COVID-19, and geographical economic theories such as Central place theory, the simulation integrates agent-based modelling to depict the movement and interactions of individuals within different settlement hierarchies.","Additionally, the framework provides a tool for decision-makers to assess and strategize optimal distribution plans for limited resources like vaccines or cures as well as to impose mobility restrictions."],"url":"http://arxiv.org/abs/2311.09984v1"}
{"created":"2023-11-16 15:52:24","title":"Unambiguity and Fewness for Nonuniform Families of Polynomial-Size Nondeterministic Finite Automata","abstract":"Nonuniform families of polynomial-size finite automata, which are series of indexed finite automata having polynomially many inner states, are used in the past literature to solve nonuniform families of promise decision problems. Among such nonuniform families of finite automata, we focus our attention, in particular, on the variants of nondeterministic finite automata, which have at most \"one\" (unambiguous), \"polynomially many\" (few) accepting computation paths, or unambiguous/few computation paths leading to each fixed configuration. When such machines are limited to make only one-way head moves, we can prove with no unproven hardness assumptions that some of these variants are different in computational power from each other. As for two-way machines restricted to instances of polynomially-bounded length, families of two-way polynomial-size nondeterministic finite automata are equivalent in power to families of polynomial-size unambiguous finite automata.","sentences":["Nonuniform families of polynomial-size finite automata, which are series of indexed finite automata having polynomially many inner states, are used in the past literature to solve nonuniform families of promise decision problems.","Among such nonuniform families of finite automata, we focus our attention, in particular, on the variants of nondeterministic finite automata, which have at most \"one\" (unambiguous), \"polynomially many\" (few) accepting computation paths, or unambiguous/few computation paths leading to each fixed configuration.","When such machines are limited to make only one-way head moves, we can prove with no unproven hardness assumptions that some of these variants are different in computational power from each other.","As for two-way machines restricted to instances of polynomially-bounded length, families of two-way polynomial-size nondeterministic finite automata are equivalent in power to families of polynomial-size unambiguous finite automata."],"url":"http://arxiv.org/abs/2311.09979v1"}
{"created":"2023-11-16 15:50:02","title":"Revolutionizing Customer Interactions: Insights and Challenges in Deploying ChatGPT and Generative Chatbots for FAQs","abstract":"In the rapidly evolving domain of artificial intelligence, chatbots have emerged as a potent tool for various applications ranging from e-commerce to healthcare. This research delves into the intricacies of chatbot technology, from its foundational concepts to advanced generative models like ChatGPT. We present a comprehensive taxonomy of existing chatbot approaches, distinguishing between rule-based, retrieval-based, generative, and hybrid models. A specific emphasis is placed on ChatGPT, elucidating its merits for frequently asked questions (FAQs)-based chatbots, coupled with an exploration of associated Natural Language Processing (NLP) techniques such as named entity recognition, intent classification, and sentiment analysis. The paper further delves into the customization and fine-tuning of ChatGPT, its integration with knowledge bases, and the consequent challenges and ethical considerations that arise. Through real-world applications in domains such as online shopping, healthcare, and education, we underscore the transformative potential of chatbots. However, we also spotlight open challenges and suggest future research directions, emphasizing the need for optimizing conversational flow, advancing dialogue mechanics, improving domain adaptability, and enhancing ethical considerations. The research culminates in a call for further exploration in ensuring transparent, ethical, and user-centric chatbot systems.","sentences":["In the rapidly evolving domain of artificial intelligence, chatbots have emerged as a potent tool for various applications ranging from e-commerce to healthcare.","This research delves into the intricacies of chatbot technology, from its foundational concepts to advanced generative models like ChatGPT.","We present a comprehensive taxonomy of existing chatbot approaches, distinguishing between rule-based, retrieval-based, generative, and hybrid models.","A specific emphasis is placed on ChatGPT, elucidating its merits for frequently asked questions (FAQs)-based chatbots, coupled with an exploration of associated Natural Language Processing (NLP) techniques such as named entity recognition, intent classification, and sentiment analysis.","The paper further delves into the customization and fine-tuning of ChatGPT, its integration with knowledge bases, and the consequent challenges and ethical considerations that arise.","Through real-world applications in domains such as online shopping, healthcare, and education, we underscore the transformative potential of chatbots.","However, we also spotlight open challenges and suggest future research directions, emphasizing the need for optimizing conversational flow, advancing dialogue mechanics, improving domain adaptability, and enhancing ethical considerations.","The research culminates in a call for further exploration in ensuring transparent, ethical, and user-centric chatbot systems."],"url":"http://arxiv.org/abs/2311.09976v1"}
{"created":"2023-11-16 15:47:51","title":"Version Age of Information Minimization over Fading Broadcast Channels","abstract":"We consider a base station (BS) that receives version update packets from multiple exogenous streams and broadcasts them to corresponding destination users over a fading broadcast channel using a non-orthogonal multiple access (NOMA) scheme. In each stream, packets arrive randomly, with each new packet's index being one higher than that of the immediate previous packet. Moreover, the arrival of a new version update renders previous versions obsolete. In this case, we consider the version age of information (VAoI) at a user, defined as the difference in the version index of the latest available packet at the BS and that at the user, as a metric of freshness of information. Our objective is to minimize a weighted sum of the long-term expected average VAoI across users and average power by optimally scheduling the update packets from different streams for transmission and transmitting them with appropriate powers to guarantee their successful delivery. We obtain the optimal policy within the class of channel-only stationary randomized policies (CO-SRP), which make transmission decisions based only on the channel power gain realizations at each decision time, and the optimal Markov decision process (MDP) based solution, along with its structural properties. Via numerical simulations, we show that the proposed optimal CO-SRP performs close to the MDP-based solution. Additionally, a time division multiple access (TDMA) scheme that allows transmission to at most one user at any given time instant exhibits comparable performance to NOMA when the average power consumed is low. However, as the average power consumed increases, NOMA outperforms TDMA.","sentences":["We consider a base station (BS) that receives version update packets from multiple exogenous streams and broadcasts them to corresponding destination users over a fading broadcast channel using a non-orthogonal multiple access (NOMA) scheme.","In each stream, packets arrive randomly, with each new packet's index being one higher than that of the immediate previous packet.","Moreover, the arrival of a new version update renders previous versions obsolete.","In this case, we consider the version age of information (VAoI) at a user, defined as the difference in the version index of the latest available packet at the BS and that at the user, as a metric of freshness of information.","Our objective is to minimize a weighted sum of the long-term expected average VAoI across users and average power by optimally scheduling the update packets from different streams for transmission and transmitting them with appropriate powers to guarantee their successful delivery.","We obtain the optimal policy within the class of channel-only stationary randomized policies (CO-SRP), which make transmission decisions based only on the channel power gain realizations at each decision time, and the optimal Markov decision process (MDP) based solution, along with its structural properties.","Via numerical simulations, we show that the proposed optimal CO-SRP performs close to the MDP-based solution.","Additionally, a time division multiple access (TDMA) scheme that allows transmission to at most one user at any given time instant exhibits comparable performance to NOMA when the average power consumed is low.","However, as the average power consumed increases, NOMA outperforms TDMA."],"url":"http://arxiv.org/abs/2311.09975v1"}
{"created":"2023-11-16 15:47:49","title":"From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning","abstract":"In recent years, self-supervised contrastive learning has emerged as a distinguished paradigm in the artificial intelligence landscape. It facilitates unsupervised feature learning through contrastive delineations at the instance level. However, crafting an effective self-supervised paradigm remains a pivotal challenge within this field. This paper delves into two crucial factors impacting self-supervised contrastive learning-bach size and pretext tasks, and from a data processing standpoint, proposes an adaptive technique of batch fusion. The proposed method, via dimensionality reduction and reconstruction of batch data, enables formerly isolated individual data to partake in intra-batch communication through the Embedding Layer. Moreover, it adaptively amplifies the self-supervised feature encoding capability as the training progresses. We conducted a linear classification test of this method based on the classic contrastive learning framework on ImageNet-1k. The empirical findings illustrate that our approach achieves state-of-the-art performance under equitable comparisons. Benefiting from its \"plug-and-play\" characteristics, we further explored other contrastive learning methods. On the ImageNet-100, compared to the original performance, the top1 has seen a maximum increase of 1.25%. We suggest that the proposed method may contribute to the advancement of data-driven self-supervised learning research, bringing a fresh perspective to this community.","sentences":["In recent years, self-supervised contrastive learning has emerged as a distinguished paradigm in the artificial intelligence landscape.","It facilitates unsupervised feature learning through contrastive delineations at the instance level.","However, crafting an effective self-supervised paradigm remains a pivotal challenge within this field.","This paper delves into two crucial factors impacting self-supervised contrastive learning-bach size and pretext tasks, and from a data processing standpoint, proposes an adaptive technique of batch fusion.","The proposed method, via dimensionality reduction and reconstruction of batch data, enables formerly isolated individual data to partake in intra-batch communication through the Embedding Layer.","Moreover, it adaptively amplifies the self-supervised feature encoding capability as the training progresses.","We conducted a linear classification test of this method based on the classic contrastive learning framework on ImageNet-1k.","The empirical findings illustrate that our approach achieves state-of-the-art performance under equitable comparisons.","Benefiting from its \"plug-and-play\" characteristics, we further explored other contrastive learning methods.","On the ImageNet-100, compared to the original performance, the top1 has seen a maximum increase of 1.25%.","We suggest that the proposed method may contribute to the advancement of data-driven self-supervised learning research, bringing a fresh perspective to this community."],"url":"http://arxiv.org/abs/2311.09974v1"}
{"created":"2023-11-16 15:43:31","title":"Examining bias perpetuation in academic search engines: an algorithm audit of Google and Semantic Scholar","abstract":"Researchers rely on academic web search engines to find scientific sources, but search engine mechanisms may selectively present content that aligns with biases embedded in the queries. This study examines whether confirmation-biased queries prompted into Google Scholar and Semantic Scholar will yield skewed results. Six queries (topics across health and technology domains such as \"vaccines\" or \"internet use\") were analyzed for disparities in search results. We confirm that biased queries (targeting \"benefits\" or \"risks\") affect search results in line with the bias, with technology-related queries displaying more significant disparities. Overall, Semantic Scholar exhibited fewer disparities than Google Scholar. Topics rated as more polarizing did not consistently show more skewed results. Academic search results that perpetuate confirmation bias have strong implications for both researchers and citizens searching for evidence. More research is needed to explore how scientific inquiry and academic search engines interact.","sentences":["Researchers rely on academic web search engines to find scientific sources, but search engine mechanisms may selectively present content that aligns with biases embedded in the queries.","This study examines whether confirmation-biased queries prompted into Google Scholar and Semantic Scholar will yield skewed results.","Six queries (topics across health and technology domains such as \"vaccines\" or \"internet use\") were analyzed for disparities in search results.","We confirm that biased queries (targeting \"benefits\" or \"risks\") affect search results in line with the bias, with technology-related queries displaying more significant disparities.","Overall, Semantic Scholar exhibited fewer disparities than Google Scholar.","Topics rated as more polarizing did not consistently show more skewed results.","Academic search results that perpetuate confirmation bias have strong implications for both researchers and citizens searching for evidence.","More research is needed to explore how scientific inquiry and academic search engines interact."],"url":"http://arxiv.org/abs/2311.09969v1"}
{"created":"2023-11-16 15:41:00","title":"Scalable Sequential Optimization Under Observability Don't Cares","abstract":"Sequential logic synthesis can provide better Power-Performance-Area (PPA) than combinational logic synthesis since it explores a larger solution space. As the gate cost in advanced technologies keeps rising, sequential logic synthesis provides a powerful alternative that is gaining momentum in the EDA community. In this work, we present a new scalable algorithm for don't-care-based sequential logic synthesis. Our new approach is based on sequential k-step induction and can apply both redundancy removal and resubstitution transformations under Sequential Observability Don't Cares (SODCs). Using SODC-based optimizations with induction is a challenging problem due to dependencies and alignment of don't cares among the base case and the inductive case. We propose a new approach utilizing the full power of SODCs without limiting the solution space. Our algorithm is implemented as part of an industrial tool and achieves 6.9% average area improvement after technology mapping when compared to state-of-the-art sequential synthesis methods. Moreover, all the new sequential optimizations can be verified using state-of-the-art sequential verification tools.","sentences":["Sequential logic synthesis can provide better Power-Performance-Area (PPA) than combinational logic synthesis since it explores a larger solution space.","As the gate cost in advanced technologies keeps rising, sequential logic synthesis provides a powerful alternative that is gaining momentum in the EDA community.","In this work, we present a new scalable algorithm for don't-care-based sequential logic synthesis.","Our new approach is based on sequential k-step induction and can apply both redundancy removal and resubstitution transformations under Sequential Observability Don't Cares (SODCs).","Using SODC-based optimizations with induction is a challenging problem due to dependencies and alignment of don't cares among the base case and the inductive case.","We propose a new approach utilizing the full power of SODCs without limiting the solution space.","Our algorithm is implemented as part of an industrial tool and achieves 6.9% average area improvement after technology mapping when compared to state-of-the-art sequential synthesis methods.","Moreover, all the new sequential optimizations can be verified using state-of-the-art sequential verification tools."],"url":"http://arxiv.org/abs/2311.09967v1"}
{"created":"2023-11-16 15:39:01","title":"SurgPLAN: Surgical Phase Localization Network for Phase Recognition","abstract":"Surgical phase recognition is crucial to providing surgery understanding in smart operating rooms. Despite great progress in automatic surgical phase recognition, most existing methods are still restricted by two problems. First, these methods cannot capture discriminative visual features for each frame and motion information with simple 2D networks. Second, the frame-by-frame recognition paradigm degrades the performance due to unstable predictions within each phase, termed as phase shaking. To address these two challenges, we propose a Surgical Phase LocAlization Network, named SurgPLAN, to facilitate a more accurate and stable surgical phase recognition with the principle of temporal detection. Specifically, we first devise a Pyramid SlowFast (PSF) architecture to serve as the visual backbone to capture multi-scale spatial and temporal features by two branches with different frame sampling rates. Moreover, we propose a Temporal Phase Localization (TPL) module to generate the phase prediction based on temporal region proposals, which ensures accurate and consistent predictions within each surgical phase. Extensive experiments confirm the significant advantages of our SurgPLAN over frame-by-frame approaches in terms of both accuracy and stability.","sentences":["Surgical phase recognition is crucial to providing surgery understanding in smart operating rooms.","Despite great progress in automatic surgical phase recognition, most existing methods are still restricted by two problems.","First, these methods cannot capture discriminative visual features for each frame and motion information with simple 2D networks.","Second, the frame-by-frame recognition paradigm degrades the performance due to unstable predictions within each phase, termed as phase shaking.","To address these two challenges, we propose a Surgical Phase LocAlization Network, named SurgPLAN, to facilitate a more accurate and stable surgical phase recognition with the principle of temporal detection.","Specifically, we first devise a Pyramid SlowFast (PSF) architecture to serve as the visual backbone to capture multi-scale spatial and temporal features by two branches with different frame sampling rates.","Moreover, we propose a Temporal Phase Localization (TPL) module to generate the phase prediction based on temporal region proposals, which ensures accurate and consistent predictions within each surgical phase.","Extensive experiments confirm the significant advantages of our SurgPLAN over frame-by-frame approaches in terms of both accuracy and stability."],"url":"http://arxiv.org/abs/2311.09965v1"}
{"created":"2023-11-16 15:35:47","title":"Dynamic modeling of wing-assisted inclined running with a morphing multi-modal robot","abstract":"Robot designs can take many inspirations from nature, where there are many examples of highly resilient and fault-tolerant locomotion strategies to navigate complex terrains by using multi-functional appendages. For example, Chukar and Hoatzin birds can repurpose their wings for quadrupedal walking and wing-assisted incline running (WAIR) to climb steep surfaces. We took inspiration from nature and designed a morphing robot with multi-functional thruster-wheel appendages that allows the robot to change its mode of locomotion by transforming into a rover, quad-rotor, mobile inverted pendulum (MIP), and other modes. In this work, we derive a dynamic model and formulate a nonlinear model predictive controller to perform WAIR to showcase the unique capabilities of our robot. We implemented the model and controller in a numerical simulation and experiments to show their feasibility and the capabilities of our transforming multi-modal robot.","sentences":["Robot designs can take many inspirations from nature, where there are many examples of highly resilient and fault-tolerant locomotion strategies to navigate complex terrains by using multi-functional appendages.","For example, Chukar and Hoatzin birds can repurpose their wings for quadrupedal walking and wing-assisted incline running (WAIR) to climb steep surfaces.","We took inspiration from nature and designed a morphing robot with multi-functional thruster-wheel appendages that allows the robot to change its mode of locomotion by transforming into a rover, quad-rotor, mobile inverted pendulum (MIP), and other modes.","In this work, we derive a dynamic model and formulate a nonlinear model predictive controller to perform WAIR to showcase the unique capabilities of our robot.","We implemented the model and controller in a numerical simulation and experiments to show their feasibility and the capabilities of our transforming multi-modal robot."],"url":"http://arxiv.org/abs/2311.09963v1"}
{"created":"2023-11-16 15:32:22","title":"Self-supervised learning of multi-omics embeddings in the low-label, high-data regime","abstract":"Contrastive, self-supervised learning (SSL) is used to train a model that predicts cancer type from miRNA, mRNA or RPPA expression data. This model, a pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost, standard benchmarks for tabular data, when labelled samples are scarce but the number of unlabelled samples is high. This is despite the fact that the datasets we use have $\\mathcal{O}(10^{1})$ classes and $\\mathcal{O}(10^{2})-\\mathcal{O}(10^{4})$ features. After demonstrating the efficacy of our chosen method of self-supervised pretraining, we investigate SSL for multi-modal models. A late-fusion model is proposed, where each omics is passed through its own sub-network, the outputs of which are averaged and passed to the pretraining or downstream objective function. Multi-modal pretraining is shown to improve predictions from a single omics, and we argue that this is useful for datasets with many unlabelled multi-modal samples, but few labelled unimodal samples. Additionally, we show that pretraining each omics-specific module individually is highly effective. This enables the application of the proposed model in a variety of contexts where a large amount of unlabelled data is available from each omics, but only a few labelled samples.","sentences":["Contrastive, self-supervised learning (SSL) is used to train a model that predicts cancer type from miRNA, mRNA or RPPA expression data.","This model, a pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost, standard benchmarks for tabular data, when labelled samples are scarce but the number of unlabelled samples is high.","This is despite the fact that the datasets we use have $\\mathcal{O}(10^{1})$ classes and $\\mathcal{O}(10^{2})-\\mathcal{O}(10^{4})$ features.","After demonstrating the efficacy of our chosen method of self-supervised pretraining, we investigate SSL for multi-modal models.","A late-fusion model is proposed, where each omics is passed through its own sub-network, the outputs of which are averaged and passed to the pretraining or downstream objective function.","Multi-modal pretraining is shown to improve predictions from a single omics, and we argue that this is useful for datasets with many unlabelled multi-modal samples, but few labelled unimodal samples.","Additionally, we show that pretraining each omics-specific module individually is highly effective.","This enables the application of the proposed model in a variety of contexts where a large amount of unlabelled data is available from each omics, but only a few labelled samples."],"url":"http://arxiv.org/abs/2311.09962v1"}
{"created":"2023-11-16 15:01:48","title":"Hijacking Large Language Models via Adversarial In-Context Learning","abstract":"In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.","sentences":["In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts.","Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples.","Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL.","However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL.","To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response.","The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations.","Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs."],"url":"http://arxiv.org/abs/2311.09948v1"}
{"created":"2023-11-16 15:01:26","title":"Natural Disaster Analysis using Satellite Imagery and Social-Media Data for Emergency Response Situations","abstract":"Disaster Management is one of the most promising research areas because of its significant economic, environmental and social repercussions. This research focuses on analyzing different types of data (pre and post satellite images and twitter data) related to disaster management for in-depth analysis of location-wise emergency requirements. This research has been divided into two stages, namely, satellite image analysis and twitter data analysis followed by integration using location. The first stage involves pre and post disaster satellite image analysis of the location using multi-class land cover segmentation technique based on U-Net architecture. The second stage focuses on mapping the region with essential information about the disaster situation and immediate requirements for relief operations. The severely affected regions are demarcated and twitter data is extracted using keywords respective to that location. The extraction of situational information from a large corpus of raw tweets adopts Content Word based Tweet Summarization (COWTS) technique. An integration of these modules using real-time location-based mapping and frequency analysis technique gathers multi-dimensional information in the advent of disaster occurrence such as the Kerala and Mississippi floods that were analyzed and validated as test cases. The novelty of this research lies in the application of segmented satellite images for disaster relief using highlighted land cover changes and integration of twitter data by mapping these region-specific filters for obtaining a complete overview of the disaster.","sentences":["Disaster Management is one of the most promising research areas because of its significant economic, environmental and social repercussions.","This research focuses on analyzing different types of data (pre and post satellite images and twitter data) related to disaster management for in-depth analysis of location-wise emergency requirements.","This research has been divided into two stages, namely, satellite image analysis and twitter data analysis followed by integration using location.","The first stage involves pre and post disaster satellite image analysis of the location using multi-class land cover segmentation technique based on U-Net architecture.","The second stage focuses on mapping the region with essential information about the disaster situation and immediate requirements for relief operations.","The severely affected regions are demarcated and twitter data is extracted using keywords respective to that location.","The extraction of situational information from a large corpus of raw tweets adopts Content Word based Tweet Summarization (COWTS) technique.","An integration of these modules using real-time location-based mapping and frequency analysis technique gathers multi-dimensional information in the advent of disaster occurrence such as the Kerala and Mississippi floods that were analyzed and validated as test cases.","The novelty of this research lies in the application of segmented satellite images for disaster relief using highlighted land cover changes and integration of twitter data by mapping these region-specific filters for obtaining a complete overview of the disaster."],"url":"http://arxiv.org/abs/2311.09947v1"}
{"created":"2023-11-16 14:56:09","title":"An Attention-Based Denoising Framework for Personality Detection in Social Media Texts","abstract":"In social media networks, users produce a large amount of text content anytime, providing researchers with a valuable approach to digging for personality-related information. Personality detection based on user-generated texts is a universal method that can be used to build user portraits. The presence of noise in social media texts hinders personality detection. However, previous studies have not fully addressed this challenge. Inspired by the scanning reading technique, we propose an attention-based information extraction mechanism (AIEM) for long texts, which is applied to quickly locate valuable pieces of information, and focus more attention on the deep semantics of key pieces. Then, we provide a novel attention-based denoising framework (ADF) for personality detection tasks and achieve state-of-the-art performance on two commonly used datasets. Notably, we obtain an average accuracy improvement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator (Twitter-MBTI) dataset. We made our code publicly available on GitHub. We shed light on how AIEM works to magnify personality-related signals.","sentences":["In social media networks, users produce a large amount of text content anytime, providing researchers with a valuable approach to digging for personality-related information.","Personality detection based on user-generated texts is a universal method that can be used to build user portraits.","The presence of noise in social media texts hinders personality detection.","However, previous studies have not fully addressed this challenge.","Inspired by the scanning reading technique, we propose an attention-based information extraction mechanism (AIEM) for long texts, which is applied to quickly locate valuable pieces of information, and focus more attention on the deep semantics of key pieces.","Then, we provide a novel attention-based denoising framework (ADF) for personality detection tasks and achieve state-of-the-art performance on two commonly used datasets.","Notably, we obtain an average accuracy improvement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator (Twitter-MBTI) dataset.","We made our code publicly available on GitHub.","We shed light on how AIEM works to magnify personality-related signals."],"url":"http://arxiv.org/abs/2311.09945v1"}
{"created":"2023-11-16 14:45:02","title":"Ghost Value Augmentation for $k$-ECSS and $k$-ECSM","abstract":"We give a poly-time algorithm for the $k$-edge-connected spanning subgraph ($k$-ECSS) problem that returns a solution of cost no greater than the cheapest $(k+10)$-ECSS on the same graph. Our approach enhances the iterative relaxation framework with a new ingredient, which we call ghost values, that allows for high sparsity in intermediate problems.   Our guarantees improve upon the best-known approximation factor of $2$ for $k$-ECSS whenever the optimal value of $(k+10)$-ECSS is close to that of $k$-ECSS. This is a property that holds for the closely related problem $k$-edge-connected spanning multi-subgraph ($k$-ECSM), which is identical to $k$-ECSS except edges can be selected multiple times at the same cost. As a consequence, we obtain a $\\left(1+O\\left(\\frac{1}{k}\\right)\\right)$-approximation for $k$-ECSM, which resolves a conjecture of Pritchard and improves upon a recent $1+O\\left(\\frac{1}{k}\\right)$ approximation of Karlin, Klein, Oveis Gharan, and Zhang. Moreover, we present a matching lower bound for $k$-ECSM, showing that our approximation ratio is tight up to the constant factor in $O\\left(\\frac{1}{k}\\right)$, unless $P=NP$.","sentences":["We give a poly-time algorithm for the $k$-edge-connected spanning subgraph ($k$-ECSS) problem that returns a solution of cost no greater than the cheapest $(k+10)$-ECSS on the same graph.","Our approach enhances the iterative relaxation framework with a new ingredient, which we call ghost values, that allows for high sparsity in intermediate problems.   ","Our guarantees improve upon the best-known approximation factor of $2$ for $k$-ECSS whenever the optimal value of $(k+10)$-ECSS is close to that of $k$-ECSS.","This is a property that holds for the closely related problem $k$-edge-connected spanning multi-subgraph ($k$-ECSM), which is identical to $k$-ECSS except edges can be selected multiple times at the same cost.","As a consequence, we obtain a $\\left(1+O\\left(\\frac{1}{k}\\right)\\right)$-approximation for $k$-ECSM, which resolves a conjecture of Pritchard and improves upon a recent $1+O\\left(\\frac{1}{k}\\right)$ approximation of Karlin, Klein, Oveis Gharan, and Zhang.","Moreover, we present a matching lower bound for $k$-ECSM, showing that our approximation ratio is tight up to the constant factor in $O\\left(\\frac{1}{k}\\right)$, unless $P=NP$."],"url":"http://arxiv.org/abs/2311.09941v1"}
{"created":"2023-11-16 14:43:45","title":"RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection","abstract":"Online misinformation is often multimodal in nature, i.e., it is caused by misleading associations between texts and accompanying images. To support the fact-checking process, researchers have been recently developing automatic multimodal methods that gather and analyze external information, evidence, related to the image-text pairs under examination. However, prior works assumed all collected evidence to be relevant. In this study, we introduce a \"Relevant Evidence Detection\" (RED) module to discern whether each piece of evidence is relevant, to support or refute the claim. Specifically, we develop the \"Relevant Evidence Detection Directed Transformer\" (RED-DOT) and explore multiple architectural variants (e.g., single or dual-stage) and mechanisms (e.g., \"guided attention\"). Extensive ablation and comparative experiments demonstrate that RED-DOT achieves significant improvements over the state-of-the-art on the VERITE benchmark by up to 28.5%. Furthermore, our evidence re-ranking and element-wise modality fusion led to RED-DOT achieving competitive and even improved performance on NewsCLIPings+, without the need for numerous evidence or multiple backbone encoders. Finally, our qualitative analysis demonstrates that the proposed \"guided attention\" module has the potential to enhance the architecture's interpretability. We release our code at: https://github.com/stevejpapad/relevant-evidence-detection","sentences":["Online misinformation is often multimodal in nature, i.e., it is caused by misleading associations between texts and accompanying images.","To support the fact-checking process, researchers have been recently developing automatic multimodal methods that gather and analyze external information, evidence, related to the image-text pairs under examination.","However, prior works assumed all collected evidence to be relevant.","In this study, we introduce a \"Relevant Evidence Detection\" (RED) module to discern whether each piece of evidence is relevant, to support or refute the claim.","Specifically, we develop the \"Relevant Evidence Detection Directed Transformer\" (RED-DOT) and explore multiple architectural variants (e.g., single or dual-stage) and mechanisms (e.g., \"guided attention\").","Extensive ablation and comparative experiments demonstrate that RED-DOT achieves significant improvements over the state-of-the-art on the VERITE benchmark by up to 28.5%.","Furthermore, our evidence re-ranking and element-wise modality fusion led to RED-DOT achieving competitive and even improved performance on NewsCLIPings+, without the need for numerous evidence or multiple backbone encoders.","Finally, our qualitative analysis demonstrates that the proposed \"guided attention\" module has the potential to enhance the architecture's interpretability.","We release our code at: https://github.com/stevejpapad/relevant-evidence-detection"],"url":"http://arxiv.org/abs/2311.09939v1"}
{"created":"2023-11-16 14:39:37","title":"Echo Chambers within the Russo-Ukrainian War: The Role of Bipartisan Users","abstract":"The ongoing Russia-Ukraine war has been extensively discussed on social media. One commonly observed problem in such discussions is the emergence of echo chambers, where users are rarely exposed to opinions outside their worldview. Prior literature on this topic has assumed that such users hold a single consistent view. However, recent work has revealed that complex topics (such as the war) often trigger bipartisanship among certain people. With this in mind, we study the presence of echo chambers on Twitter related to the Russo-Ukrainian war. We measure their presence and identify an important subset of bipartisan users who vary their opinions during the invasion. We explore the role they play in the communications graph and identify features that distinguish them from remaining users. We conclude by discussing their importance and how they can improve the quality of discourse surrounding the war.","sentences":["The ongoing Russia-Ukraine war has been extensively discussed on social media.","One commonly observed problem in such discussions is the emergence of echo chambers, where users are rarely exposed to opinions outside their worldview.","Prior literature on this topic has assumed that such users hold a single consistent view.","However, recent work has revealed that complex topics (such as the war) often trigger bipartisanship among certain people.","With this in mind, we study the presence of echo chambers on Twitter related to the Russo-Ukrainian war.","We measure their presence and identify an important subset of bipartisan users who vary their opinions during the invasion.","We explore the role they play in the communications graph and identify features that distinguish them from remaining users.","We conclude by discussing their importance and how they can improve the quality of discourse surrounding the war."],"url":"http://arxiv.org/abs/2311.09934v1"}
{"created":"2023-11-16 14:36:41","title":"The Communication GSC System with Energy Harvesting Nodes aided by Opportunistic Routing","abstract":"In this paper, a cooperative communication network based on energy-harvesting (EH) decode-and-forward (DF) relays is proposed. For relay nodes, there is harvest-storage-use (HSU) structure in this system. And energy can be obtained from the surrounding environment through energy buffering. In order to improve the performance of the communication system, the opportunistic routing algorithm and the generalized selection combining (GSC) algorithm are adopted in this communication system. In addition, from discrete-time continuous-state space Markov chain model (DCSMC), a theoretical expression of the energy limiting distribution stored in infinite buffers is derived. Through using the probability distribution and state transition matrix, the theoretical expressions of system outage probability, throughput and time cost of per packet are obtained. Through the simulation verification, the theoretical results are in good agreement with the simulation results.","sentences":["In this paper, a cooperative communication network based on energy-harvesting (EH) decode-and-forward (DF) relays is proposed.","For relay nodes, there is harvest-storage-use (HSU) structure in this system.","And energy can be obtained from the surrounding environment through energy buffering.","In order to improve the performance of the communication system, the opportunistic routing algorithm and the generalized selection combining (GSC) algorithm are adopted in this communication system.","In addition, from discrete-time continuous-state space Markov chain model (DCSMC), a theoretical expression of the energy limiting distribution stored in infinite buffers is derived.","Through using the probability distribution and state transition matrix, the theoretical expressions of system outage probability, throughput and time cost of per packet are obtained.","Through the simulation verification, the theoretical results are in good agreement with the simulation results."],"url":"http://arxiv.org/abs/2311.09932v1"}
{"created":"2023-11-16 14:32:18","title":"A Framework for Monitoring and Retraining Language Models in Real-World Applications","abstract":"In the Machine Learning (ML) model development lifecycle, training candidate models using an offline holdout dataset and identifying the best model for the given task is only the first step. After the deployment of the selected model, continuous model monitoring and model retraining is required in many real-world applications. There are multiple reasons for retraining, including data or concept drift, which may be reflected on the model performance as monitored by an appropriate metric. Another motivation for retraining is the acquisition of increasing amounts of data over time, which may be used to retrain and improve the model performance even in the absence of drifts. We examine the impact of various retraining decision points on crucial factors, such as model performance and resource utilization, in the context of Multilabel Classification models. We explain our key decision points and propose a reference framework for designing an effective model retraining strategy.","sentences":["In the Machine Learning (ML) model development lifecycle, training candidate models using an offline holdout dataset and identifying the best model for the given task is only the first step.","After the deployment of the selected model, continuous model monitoring and model retraining is required in many real-world applications.","There are multiple reasons for retraining, including data or concept drift, which may be reflected on the model performance as monitored by an appropriate metric.","Another motivation for retraining is the acquisition of increasing amounts of data over time, which may be used to retrain and improve the model performance even in the absence of drifts.","We examine the impact of various retraining decision points on crucial factors, such as model performance and resource utilization, in the context of Multilabel Classification models.","We explain our key decision points and propose a reference framework for designing an effective model retraining strategy."],"url":"http://arxiv.org/abs/2311.09930v1"}
{"created":"2023-11-16 14:31:16","title":"Mutating etcd Towards Edge Suitability","abstract":"In the edge environment servers are no longer being co-located away from clients, instead they are being co-located with clients away from other servers, focusing on reliable and performant operation. Orchestration platforms, such as Kubernetes, are a key system being transitioned to the edge but they remain unsuited to the environment, stemming primarily from their critical key-value stores. In this work we derive requirements from the edge environment showing that, fundamentally, the design of distributed key-value datastores, such as etcd, is unsuited to meet them. Using these requirements, we explore the design space for distributed key-value datastores and implement two successive mutations of etcd for different points: mergeable-etcd and dismerge, trading linearizability for causal consistency based on CRDTs. mergeable-etcd retains the linear revision history but encounters inherent shortcomings, whilst dismerge embraces the causal model. Both stores are local-first, maintaining reliable performance under network partitions and variability, drastically surpassing etcd's performance, whilst maintaining competitive performance in reliable settings.","sentences":["In the edge environment servers are no longer being co-located away from clients, instead they are being co-located with clients away from other servers, focusing on reliable and performant operation.","Orchestration platforms, such as Kubernetes, are a key system being transitioned to the edge but they remain unsuited to the environment, stemming primarily from their critical key-value stores.","In this work we derive requirements from the edge environment showing that, fundamentally, the design of distributed key-value datastores, such as etcd, is unsuited to meet them.","Using these requirements, we explore the design space for distributed key-value datastores and implement two successive mutations of etcd for different points: mergeable-etcd and dismerge, trading linearizability for causal consistency based on CRDTs.","mergeable-etcd retains the linear revision history but encounters inherent shortcomings, whilst dismerge embraces the causal model.","Both stores are local-first, maintaining reliable performance under network partitions and variability, drastically surpassing etcd's performance, whilst maintaining competitive performance in reliable settings."],"url":"http://arxiv.org/abs/2311.09929v1"}
{"created":"2023-11-16 14:21:13","title":"Fast multiplication by two's complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation","abstract":"We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distributed arithmetic operations such that the 'polynomial integer index multiplication' method overcomes the current limitation of parallel multiplication methods. Ie, the need to share common core memory and common disk for the calculation of results and intermediate results.","sentences":["We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list.","The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code.","We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range.","Also implemented in python code for comparison purposes with the polynomial radix 2 integer method.","We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two.","The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs.","We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture.","We demonstrate fully distributed arithmetic operations such that the 'polynomial integer index multiplication' method overcomes the current limitation of parallel multiplication methods.","Ie, the need to share common core memory and common disk for the calculation of results and intermediate results."],"url":"http://arxiv.org/abs/2311.09922v1"}
{"created":"2023-11-16 14:18:10","title":"DSR-Diff: Depth Map Super-Resolution with Diffusion Model","abstract":"Color-guided depth map super-resolution (CDSR) improve the spatial resolution of a low-quality depth map with the corresponding high-quality color map, benefiting various applications such as 3D reconstruction, virtual reality, and augmented reality. While conventional CDSR methods typically rely on convolutional neural networks or transformers, diffusion models (DMs) have demonstrated notable effectiveness in high-level vision tasks. In this work, we present a novel CDSR paradigm that utilizes a diffusion model within the latent space to generate guidance for depth map super-resolution. The proposed method comprises a guidance generation network (GGN), a depth map super-resolution network (DSRN), and a guidance recovery network (GRN). The GGN is specifically designed to generate the guidance while managing its compactness. Additionally, we integrate a simple but effective feature fusion module and a transformer-style feature extraction module into the DSRN, enabling it to leverage guided priors in the extraction, fusion, and reconstruction of multi-model images. Taking into account both accuracy and efficiency, our proposed method has shown superior performance in extensive experiments when compared to state-of-the-art methods. Our codes will be made available at https://github.com/shiyuan7/DSR-Diff.","sentences":["Color-guided depth map super-resolution (CDSR) improve the spatial resolution of a low-quality depth map with the corresponding high-quality color map, benefiting various applications such as 3D reconstruction, virtual reality, and augmented reality.","While conventional CDSR methods typically rely on convolutional neural networks or transformers, diffusion models (DMs) have demonstrated notable effectiveness in high-level vision tasks.","In this work, we present a novel CDSR paradigm that utilizes a diffusion model within the latent space to generate guidance for depth map super-resolution.","The proposed method comprises a guidance generation network (GGN), a depth map super-resolution network (DSRN), and a guidance recovery network (GRN).","The GGN is specifically designed to generate the guidance while managing its compactness.","Additionally, we integrate a simple but effective feature fusion module and a transformer-style feature extraction module into the DSRN, enabling it to leverage guided priors in the extraction, fusion, and reconstruction of multi-model images.","Taking into account both accuracy and efficiency, our proposed method has shown superior performance in extensive experiments when compared to state-of-the-art methods.","Our codes will be made available at https://github.com/shiyuan7/DSR-Diff."],"url":"http://arxiv.org/abs/2311.09919v1"}
{"created":"2023-11-16 14:16:51","title":"Proceedings of the 18th International Workshop on Logical Frameworks and Meta-Languages: Theory and Practice","abstract":"Logical frameworks and meta-languages form a common substrate for representing, implementing and reasoning about a wide variety of deductive systems of interest in logic and computer science. Their design, implementation and their use in reasoning tasks, ranging from the correctness of software to the properties of formal systems, have been the focus of considerable research over the last two decades. This workshop brings together designers, implementors and practitioners to discuss various aspects impinging on the structure and utility of logical frameworks, including the treatment of variable binding, inductive and co-inductive reasoning techniques and the expressiveness and lucidity of the reasoning process.","sentences":["Logical frameworks and meta-languages form a common substrate for representing, implementing and reasoning about a wide variety of deductive systems of interest in logic and computer science.","Their design, implementation and their use in reasoning tasks, ranging from the correctness of software to the properties of formal systems, have been the focus of considerable research over the last two decades.","This workshop brings together designers, implementors and practitioners to discuss various aspects impinging on the structure and utility of logical frameworks, including the treatment of variable binding, inductive and co-inductive reasoning techniques and the expressiveness and lucidity of the reasoning process."],"url":"http://arxiv.org/abs/2311.09918v1"}
{"created":"2023-11-16 14:03:17","title":"DNA-Correcting Codes in DNA Storage Systems","abstract":"In [1], the authors proposed a new model of DNA storage system that integrates all three steps of retrieval and introduced the concept of DNA-correcting codes, which guarantees that the output of the storage system can be decoded to the original data. They also gave necessary and sufficient conditions for DNA-correcting codes when the data part is free of errors. In this paper, we generalize their results to the general case.","sentences":["In [1], the authors proposed a new model of DNA storage system that integrates all three steps of retrieval and introduced the concept of DNA-correcting codes, which guarantees that the output of the storage system can be decoded to the original data.","They also gave necessary and sufficient conditions for DNA-correcting codes when the data part is free of errors.","In this paper, we generalize their results to the general case."],"url":"http://arxiv.org/abs/2311.09910v1"}
{"created":"2023-11-16 13:56:14","title":"Capacitated Network Bargaining Games: Stability and Structure","abstract":"Capacitated network bargaining games are popular combinatorial games that involve the structure of matchings in graphs. We show that it is always possible to stabilize unweighted instances of this problem (that is, ensure that they admit a stable outcome) via capacity-reduction and edge-removal operations, without decreasing the total value that the players can get.   Furthermore, for general weighted instances, we show that computing a minimum amount of vertex-capacity to reduce to make an instance stable is a polynomial-time solvable problem. We then exploit this to give approximation results for the NP-hard problem of stabilizing a graph via edge-removal operations.   Our work extends and generalizes previous results in the literature that dealt with an uncapacitated version of the problem, using several new arguments. In particular, while previous results mainly used combinatorial techniques, we here rely on polyhedral arguments and, more specifically, on the notion of circuits of a polytope.","sentences":["Capacitated network bargaining games are popular combinatorial games that involve the structure of matchings in graphs.","We show that it is always possible to stabilize unweighted instances of this problem (that is, ensure that they admit a stable outcome) via capacity-reduction and edge-removal operations, without decreasing the total value that the players can get.   ","Furthermore, for general weighted instances, we show that computing a minimum amount of vertex-capacity to reduce to make an instance stable is a polynomial-time solvable problem.","We then exploit this to give approximation results for the NP-hard problem of stabilizing a graph via edge-removal operations.   ","Our work extends and generalizes previous results in the literature that dealt with an uncapacitated version of the problem, using several new arguments.","In particular, while previous results mainly used combinatorial techniques, we here rely on polyhedral arguments and, more specifically, on the notion of circuits of a polytope."],"url":"http://arxiv.org/abs/2311.09904v1"}
{"created":"2023-11-16 13:54:38","title":"Selection of Distinct Morphologies to Divide & Conquer Gigapixel Pathology Images","abstract":"Whole slide images (WSIs) are massive digital pathology files illustrating intricate tissue structures. Selecting a small, representative subset of patches from each WSI is essential yet challenging. Therefore, following the \"Divide & Conquer\" approach becomes essential to facilitate WSI analysis including the classification and the WSI matching in computational pathology. To this end, we propose a novel method termed \"Selection of Distinct Morphologies\" (SDM) to choose a subset of WSI patches. The aim is to encompass all inherent morphological variations within a given WSI while simultaneously minimizing the number of selected patches to represent these variations, ensuring a compact yet comprehensive set of patches. This systematically curated patch set forms what we term a \"montage\". We assess the representativeness of the SDM montage across various public and private histopathology datasets. This is conducted by using the leave-one-out WSI search and matching evaluation method, comparing it with the state-of-the-art Yottixel's mosaic. SDM demonstrates remarkable efficacy across all datasets during its evaluation. Furthermore, SDM eliminates the necessity for empirical parameterization, a crucial aspect of Yottixel's mosaic, by inherently optimizing the selection process to capture the distinct morphological features within the WSI.","sentences":["Whole slide images (WSIs) are massive digital pathology files illustrating intricate tissue structures.","Selecting a small, representative subset of patches from each WSI is essential yet challenging.","Therefore, following the \"Divide & Conquer\" approach becomes essential to facilitate WSI analysis including the classification and the WSI matching in computational pathology.","To this end, we propose a novel method termed \"Selection of Distinct Morphologies\" (SDM) to choose a subset of WSI patches.","The aim is to encompass all inherent morphological variations within a given WSI while simultaneously minimizing the number of selected patches to represent these variations, ensuring a compact yet comprehensive set of patches.","This systematically curated patch set forms what we term a \"montage\".","We assess the representativeness of the SDM montage across various public and private histopathology datasets.","This is conducted by using the leave-one-out WSI search and matching evaluation method, comparing it with the state-of-the-art Yottixel's mosaic.","SDM demonstrates remarkable efficacy across all datasets during its evaluation.","Furthermore, SDM eliminates the necessity for empirical parameterization, a crucial aspect of Yottixel's mosaic, by inherently optimizing the selection process to capture the distinct morphological features within the WSI."],"url":"http://arxiv.org/abs/2311.09902v1"}
{"created":"2023-11-16 13:37:21","title":"Language Generation from Human Brain Activities","abstract":"Generating human language through non-invasive brain-computer interfaces (BCIs) has the potential to unlock many applications, such as serving disabled patients and improving communication. Currently, however, generating language via BCIs has been previously successful only within a classification setup for selecting pre-generated sentence continuation candidates with the most likely cortical semantic representation. Inspired by recent research that revealed associations between the brain and the large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input. The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generated candidates. We compare the language generated from the presented model with a random control, pre-generated language selection approach, and a standard LLM, which generates common coherent text solely based on the next word likelihood according to statistical language training data. The proposed model is found to generate language that is more aligned with semantic stimulus in response to which brain input is sampled. Our findings demonstrate the potential and feasibility of employing BCIs in direct language generation.","sentences":["Generating human language through non-invasive brain-computer interfaces (BCIs) has the potential to unlock many applications, such as serving disabled patients and improving communication.","Currently, however, generating language via BCIs has been previously successful only within a classification setup for selecting pre-generated sentence continuation candidates with the most likely cortical semantic representation.","Inspired by recent research that revealed associations between the brain and the large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input.","The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generated candidates.","We compare the language generated from the presented model with a random control, pre-generated language selection approach, and a standard LLM, which generates common coherent text solely based on the next word likelihood according to statistical language training data.","The proposed model is found to generate language that is more aligned with semantic stimulus in response to which brain input is sampled.","Our findings demonstrate the potential and feasibility of employing BCIs in direct language generation."],"url":"http://arxiv.org/abs/2311.09889v1"}
{"created":"2023-11-16 13:33:41","title":"Near-Field Velocity Sensing and Predictive Beamforming","abstract":"The novel concept of near-field velocity sensing is proposed. In contrast to far-field velocity sensing, near-field velocity sensing enables the simultaneous estimation of both radial and transverse velocities of a moving target. A maximum-likelihood-based method is proposed for jointly estimating the radial and transverse velocities from the echo signals. Assisted by near-field velocity sensing, a predictive beamforming framework is proposed for a moving communication user, which requires no channel estimation but achieves seamless data transmission. Finally, numerical examples validate the proposed approaches.","sentences":["The novel concept of near-field velocity sensing is proposed.","In contrast to far-field velocity sensing, near-field velocity sensing enables the simultaneous estimation of both radial and transverse velocities of a moving target.","A maximum-likelihood-based method is proposed for jointly estimating the radial and transverse velocities from the echo signals.","Assisted by near-field velocity sensing, a predictive beamforming framework is proposed for a moving communication user, which requires no channel estimation but achieves seamless data transmission.","Finally, numerical examples validate the proposed approaches."],"url":"http://arxiv.org/abs/2311.09888v1"}
{"created":"2023-11-16 13:28:19","title":"LIO-EKF: High Frequency LiDAR-Inertial Odometry using Extended Kalman Filters","abstract":"Odometry estimation is a key element for every autonomous system requiring navigation in an unknown environment. In modern mobile robots, 3D LiDAR-inertial systems are often used for this task. By fusing LiDAR scans and IMU measurements, these systems can reduce the accumulated drift caused by sequentially registering individual LiDAR scans and provide a robust pose estimate. Although effective, LiDAR-inertial odometry systems require proper parameter tuning to be deployed. In this paper, we propose LIO-EKF, a tightly-coupled LiDAR-inertial odometry system based on point-to-point registration and the classical extended Kalman filter scheme. We propose an adaptive data association that considers the relative pose uncertainty, the map discretization errors, and the LiDAR noise. In this way, we can substantially reduce the parameters to tune for a given type of environment. The experimental evaluation suggests that the proposed system performs on par with the state-of-the-art LiDAR-inertial odometry pipelines, but is significantly faster in computing the odometry.","sentences":["Odometry estimation is a key element for every autonomous system requiring navigation in an unknown environment.","In modern mobile robots, 3D LiDAR-inertial systems are often used for this task.","By fusing LiDAR scans and IMU measurements, these systems can reduce the accumulated drift caused by sequentially registering individual LiDAR scans and provide a robust pose estimate.","Although effective, LiDAR-inertial odometry systems require proper parameter tuning to be deployed.","In this paper, we propose LIO-EKF, a tightly-coupled LiDAR-inertial odometry system based on point-to-point registration and the classical extended Kalman filter scheme.","We propose an adaptive data association that considers the relative pose uncertainty, the map discretization errors, and the LiDAR noise.","In this way, we can substantially reduce the parameters to tune for a given type of environment.","The experimental evaluation suggests that the proposed system performs on par with the state-of-the-art LiDAR-inertial odometry pipelines, but is significantly faster in computing the odometry."],"url":"http://arxiv.org/abs/2311.09887v1"}
