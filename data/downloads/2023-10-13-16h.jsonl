{"created":"2023-10-12 17:59:58","title":"Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?","abstract":"Rendering scenes observed in a monocular video from novel viewpoints is a challenging problem. For static scenes the community has studied both scene-specific optimization techniques, which optimize on every test scene, and generalized techniques, which only run a deep net forward pass on a test scene. In contrast, for dynamic scenes, scene-specific optimization techniques exist, but, to our best knowledge, there is currently no generalized method for dynamic novel view synthesis from a given monocular video. To answer whether generalized dynamic novel view synthesis from monocular videos is possible today, we establish an analysis framework based on existing techniques and work toward the generalized approach. We find a pseudo-generalized process without scene-specific appearance optimization is possible, but geometrically and temporally consistent depth estimates are needed. Despite no scene-specific appearance optimization, the pseudo-generalized approach improves upon some scene-specific methods.","sentences":["Rendering scenes observed in a monocular video from novel viewpoints is a challenging problem.","For static scenes the community has studied both scene-specific optimization techniques, which optimize on every test scene, and generalized techniques, which only run a deep net forward pass on a test scene.","In contrast, for dynamic scenes, scene-specific optimization techniques exist, but, to our best knowledge, there is currently no generalized method for dynamic novel view synthesis from a given monocular video.","To answer whether generalized dynamic novel view synthesis from monocular videos is possible today, we establish an analysis framework based on existing techniques and work toward the generalized approach.","We find a pseudo-generalized process without scene-specific appearance optimization is possible, but geometrically and temporally consistent depth estimates are needed.","Despite no scene-specific appearance optimization, the pseudo-generalized approach improves upon some scene-specific methods."],"url":"http://arxiv.org/abs/2310.08587v1"}
{"created":"2023-10-12 17:59:58","title":"Octopus: Embodied Vision-Language Programmer from Environmental Feedback","abstract":"Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.","sentences":["Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning.","Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision.","In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code.","Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games.","Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse.","We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF).","Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making.","By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community."],"url":"http://arxiv.org/abs/2310.08588v1"}
{"created":"2023-10-12 17:59:57","title":"Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic Scenes","abstract":"This paper aims to tackle the challenge of dynamic view synthesis from multi-view videos. The key observation is that while previous grid-based methods offer consistent rendering, they fall short in capturing appearance details of a complex dynamic scene, a domain where multi-view image-based rendering methods demonstrate the opposite properties. To combine the best of two worlds, we introduce Im4D, a hybrid scene representation that consists of a grid-based geometry representation and a multi-view image-based appearance representation. Specifically, the dynamic geometry is encoded as a 4D density function composed of spatiotemporal feature planes and a small MLP network, which globally models the scene structure and facilitates the rendering consistency. We represent the scene appearance by the original multi-view videos and a network that learns to predict the color of a 3D point from image features, instead of memorizing detailed appearance totally with networks, thereby naturally making the learning of networks easier. Our method is evaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap, NHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D exhibits state-of-the-art performance in rendering quality and can be trained efficiently, while realizing real-time rendering with a speed of 79.8 FPS for 512x512 images, on a single RTX 3090 GPU.","sentences":["This paper aims to tackle the challenge of dynamic view synthesis from multi-view videos.","The key observation is that while previous grid-based methods offer consistent rendering, they fall short in capturing appearance details of a complex dynamic scene, a domain where multi-view image-based rendering methods demonstrate the opposite properties.","To combine the best of two worlds, we introduce Im4D, a hybrid scene representation that consists of a grid-based geometry representation and a multi-view image-based appearance representation.","Specifically, the dynamic geometry is encoded as a 4D density function composed of spatiotemporal feature planes and a small MLP network, which globally models the scene structure and facilitates the rendering consistency.","We represent the scene appearance by the original multi-view videos and a network that learns to predict the color of a 3D point from image features, instead of memorizing detailed appearance totally with networks, thereby naturally making the learning of networks easier.","Our method is evaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap, NHR, DNA-Rendering and ENeRF-Outdoor datasets.","The results show that Im4D exhibits state-of-the-art performance in rendering quality and can be trained efficiently, while realizing real-time rendering with a speed of 79.8 FPS for 512x512 images, on a single RTX 3090 GPU."],"url":"http://arxiv.org/abs/2310.08585v1"}
{"created":"2023-10-12 17:59:57","title":"PonderV2: Pave the Way for 3D Foundataion Model with A Universal Pre-training Paradigm","abstract":"In contrast to numerous NLP and 2D computer vision foundational models, the learning of a robust and highly generalized 3D foundational model poses considerably greater challenges. This is primarily due to the inherent data variability and the diversity of downstream tasks. In this paper, we introduce a comprehensive 3D pre-training framework designed to facilitate the acquisition of efficient 3D representations, thereby establishing a pathway to 3D foundational models. Motivated by the fact that informative 3D features should be able to encode rich geometry and appearance cues that can be utilized to render realistic images, we propose a novel universal paradigm to learn point cloud representations by differentiable neural rendering, serving as a bridge between 3D and 2D worlds. We train a point cloud encoder within a devised volumetric neural renderer by comparing the rendered images with the real images. Notably, our approach demonstrates the seamless integration of the learned 3D encoder into diverse downstream tasks. These tasks encompass not only high-level challenges such as 3D detection and segmentation but also low-level objectives like 3D reconstruction and image synthesis, spanning both indoor and outdoor scenarios. Besides, we also illustrate the capability of pre-training a 2D backbone using the proposed universal methodology, surpassing conventional pre-training methods by a large margin. For the first time, \\sexyname achieves state-of-the-art performance on 11 indoor and outdoor benchmarks. The consistent improvements in various settings imply the effectiveness of the proposed method. Code and models will be made available at https://github.com/Pointcept/Pointcept.","sentences":["In contrast to numerous NLP and 2D computer vision foundational models, the learning of a robust and highly generalized 3D foundational model poses considerably greater challenges.","This is primarily due to the inherent data variability and the diversity of downstream tasks.","In this paper, we introduce a comprehensive 3D pre-training framework designed to facilitate the acquisition of efficient 3D representations, thereby establishing a pathway to 3D foundational models.","Motivated by the fact that informative 3D features should be able to encode rich geometry and appearance cues that can be utilized to render realistic images, we propose a novel universal paradigm to learn point cloud representations by differentiable neural rendering, serving as a bridge between 3D and 2D worlds.","We train a point cloud encoder within a devised volumetric neural renderer by comparing the rendered images with the real images.","Notably, our approach demonstrates the seamless integration of the learned 3D encoder into diverse downstream tasks.","These tasks encompass not only high-level challenges such as 3D detection and segmentation but also low-level objectives like 3D reconstruction and image synthesis, spanning both indoor and outdoor scenarios.","Besides, we also illustrate the capability of pre-training a 2D backbone using the proposed universal methodology, surpassing conventional pre-training methods by a large margin.","For the first time, \\sexyname achieves state-of-the-art performance on 11 indoor and outdoor benchmarks.","The consistent improvements in various settings imply the effectiveness of the proposed method.","Code and models will be made available at https://github.com/Pointcept/Pointcept."],"url":"http://arxiv.org/abs/2310.08586v1"}
{"created":"2023-10-12 17:59:55","title":"Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video","abstract":"Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a \"Walking Tours\" dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.   Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a \"tracking to learn to recognize\" approach. Our method called DoRA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.","sentences":["Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary.","But are we making the best use of data?","How more economical can we be?","In this work, we attempt to answer this question by making two contributions.","First, we investigate first-person videos and introduce a \"Walking Tours\" dataset.","These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions.","They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.   ","Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos.","Existing methods typically adapt image-based pretraining approaches to incorporate more frames.","Instead, we advocate a \"tracking to learn to recognize\" approach.","Our method called DoRA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention.","We derive multiple views from the tracks and use them in a classical self-supervised distillation loss.","Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks."],"url":"http://arxiv.org/abs/2310.08584v1"}
{"created":"2023-10-12 17:59:51","title":"Discovering Fatigued Movements for Virtual Character Animation","abstract":"Virtual character animation and movement synthesis have advanced rapidly during recent years, especially through a combination of extensive motion capture datasets and machine learning. A remaining challenge is interactively simulating characters that fatigue when performing extended motions, which is indispensable for the realism of generated animations. However, capturing such movements is problematic, as performing movements like backflips with fatigued variations up to exhaustion raises capture cost and risk of injury. Surprisingly, little research has been done on faithful fatigue modeling. To address this, we propose a deep reinforcement learning-based approach, which -- for the first time in literature -- generates control policies for full-body physically simulated agents aware of cumulative fatigue. For this, we first leverage Generative Adversarial Imitation Learning (GAIL) to learn an expert policy for the skill; Second, we learn a fatigue policy by limiting the generated constant torque bounds based on endurance time to non-linear, state- and time-dependent limits in the joint-actuation space using a Three-Compartment Controller (3CC) model. Our results demonstrate that agents can adapt to different fatigue and rest rates interactively, and discover realistic recovery strategies without the need for any captured data of fatigued movement.","sentences":["Virtual character animation and movement synthesis have advanced rapidly during recent years, especially through a combination of extensive motion capture datasets and machine learning.","A remaining challenge is interactively simulating characters that fatigue when performing extended motions, which is indispensable for the realism of generated animations.","However, capturing such movements is problematic, as performing movements like backflips with fatigued variations up to exhaustion raises capture cost and risk of injury.","Surprisingly, little research has been done on faithful fatigue modeling.","To address this, we propose a deep reinforcement learning-based approach, which -- for the first time in literature -- generates control policies for full-body physically simulated agents aware of cumulative fatigue.","For this, we first leverage Generative Adversarial Imitation Learning (GAIL) to learn an expert policy for the skill; Second, we learn a fatigue policy by limiting the generated constant torque bounds based on endurance time to non-linear, state- and time-dependent limits in the joint-actuation space using a Three-Compartment Controller (3CC) model.","Our results demonstrate that agents can adapt to different fatigue and rest rates interactively, and discover realistic recovery strategies without the need for any captured data of fatigued movement."],"url":"http://arxiv.org/abs/2310.08583v1"}
{"created":"2023-10-12 17:59:50","title":"Tree-Planner: Efficient Close-loop Task Planning with Large Language Models","abstract":"This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: https://tree-planner.github.io/","sentences":["This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations.","Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness.","However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications.","To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding.","Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree.","Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information.","Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency.","By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed.","As a result, token consumption is reduced by 92.2% compared to the previously best-performing model.","Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections.","Project page: https://tree-planner.github.io/"],"url":"http://arxiv.org/abs/2310.08582v1"}
{"created":"2023-10-12 17:59:41","title":"Universal Visual Decomposer: Long-Horizon Manipulation Made Easy","abstract":"Real-world robotic tasks stretch over extended horizons and encompass multiple stages. Learning long-horizon manipulation tasks, however, is a long-standing challenge, and demands decomposing the overarching task into several manageable subtasks to facilitate policy learning and generalization to unseen tasks. Prior task decomposition methods require task-specific knowledge, are computationally intensive, and cannot readily be applied to new tasks. To address these shortcomings, we propose Universal Visual Decomposer (UVD), an off-the-shelf task decomposition method for visual long horizon manipulation using pre-trained visual representations designed for robotic control. At a high level, UVD discovers subgoals by detecting phase shifts in the embedding space of the pre-trained representation. Operating purely on visual demonstrations without auxiliary information, UVD can effectively extract visual subgoals embedded in the videos, while incurring zero additional training cost on top of standard visuomotor policy training. Goal-conditioned policies learned with UVD-discovered subgoals exhibit significantly improved compositional generalization at test time to unseen tasks. Furthermore, UVD-discovered subgoals can be used to construct goal-based reward shaping that jump-starts temporally extended exploration for reinforcement learning. We extensively evaluate UVD on both simulation and real-world tasks, and in all cases, UVD substantially outperforms baselines across imitation and reinforcement learning settings on in-domain and out-of-domain task sequences alike, validating the clear advantage of automated visual task decomposition within the simple, compact UVD framework.","sentences":["Real-world robotic tasks stretch over extended horizons and encompass multiple stages.","Learning long-horizon manipulation tasks, however, is a long-standing challenge, and demands decomposing the overarching task into several manageable subtasks to facilitate policy learning and generalization to unseen tasks.","Prior task decomposition methods require task-specific knowledge, are computationally intensive, and cannot readily be applied to new tasks.","To address these shortcomings, we propose Universal Visual Decomposer (UVD), an off-the-shelf task decomposition method for visual long horizon manipulation using pre-trained visual representations designed for robotic control.","At a high level, UVD discovers subgoals by detecting phase shifts in the embedding space of the pre-trained representation.","Operating purely on visual demonstrations without auxiliary information, UVD can effectively extract visual subgoals embedded in the videos, while incurring zero additional training cost on top of standard visuomotor policy training.","Goal-conditioned policies learned with UVD-discovered subgoals exhibit significantly improved compositional generalization at test time to unseen tasks.","Furthermore, UVD-discovered subgoals can be used to construct goal-based reward shaping that jump-starts temporally extended exploration for reinforcement learning.","We extensively evaluate UVD on both simulation and real-world tasks, and in all cases, UVD substantially outperforms baselines across imitation and reinforcement learning settings on in-domain and out-of-domain task sequences alike, validating the clear advantage of automated visual task decomposition within the simple, compact UVD framework."],"url":"http://arxiv.org/abs/2310.08581v1"}
{"created":"2023-10-12 17:59:38","title":"OmniControl: Control Any Joint at Any Time for Human Motion Generation","abstract":"We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.","sentences":["We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process.","Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model.","Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals.","At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion.","Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism.","By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints.","Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints."],"url":"http://arxiv.org/abs/2310.08580v1"}
{"created":"2023-10-12 17:59:34","title":"HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion","abstract":"Despite significant advances in large-scale text-to-image models, achieving hyper-realistic human image generation remains a desirable yet unsolved task. Existing models like Stable Diffusion and DALL-E 2 tend to generate human images with incoherent parts or unnatural poses. To tackle these challenges, our key insight is that human image is inherently structural over multiple granularities, from the coarse-level body skeleton to fine-grained spatial geometry. Therefore, capturing such correlations between the explicit appearance and latent structure in one model is essential to generate coherent and natural human images. To this end, we propose a unified framework, HyperHuman, that generates in-the-wild human images of high realism and diverse layouts. Specifically, 1) we first build a large-scale human-centric dataset, named HumanVerse, which consists of 340M images with comprehensive annotations like human pose, depth, and surface normal. 2) Next, we propose a Latent Structural Diffusion Model that simultaneously denoises the depth and surface normal along with the synthesized RGB image. Our model enforces the joint learning of image appearance, spatial relationship, and geometry in a unified network, where each branch in the model complements to each other with both structural awareness and textural richness. 3) Finally, to further boost the visual quality, we propose a Structure-Guided Refiner to compose the predicted conditions for more detailed generation of higher resolution. Extensive experiments demonstrate that our framework yields the state-of-the-art performance, generating hyper-realistic human images under diverse scenarios. Project Page: https://snap-research.github.io/HyperHuman/","sentences":["Despite significant advances in large-scale text-to-image models, achieving hyper-realistic human image generation remains a desirable yet unsolved task.","Existing models like Stable Diffusion and DALL-E 2 tend to generate human images with incoherent parts or unnatural poses.","To tackle these challenges, our key insight is that human image is inherently structural over multiple granularities, from the coarse-level body skeleton to fine-grained spatial geometry.","Therefore, capturing such correlations between the explicit appearance and latent structure in one model is essential to generate coherent and natural human images.","To this end, we propose a unified framework, HyperHuman, that generates in-the-wild human images of high realism and diverse layouts.","Specifically, 1) we first build a large-scale human-centric dataset, named HumanVerse, which consists of 340M images with comprehensive annotations like human pose, depth, and surface normal.","2) Next, we propose a Latent Structural Diffusion Model that simultaneously denoises the depth and surface normal along with the synthesized RGB image.","Our model enforces the joint learning of image appearance, spatial relationship, and geometry in a unified network, where each branch in the model complements to each other with both structural awareness and textural richness.","3) Finally, to further boost the visual quality, we propose a Structure-Guided Refiner to compose the predicted conditions for more detailed generation of higher resolution.","Extensive experiments demonstrate that our framework yields the state-of-the-art performance, generating hyper-realistic human images under diverse scenarios.","Project Page: https://snap-research.github.io/HyperHuman/"],"url":"http://arxiv.org/abs/2310.08579v1"}
{"created":"2023-10-12 17:59:30","title":"Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models","abstract":"Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of \\textit{Visual Data-Type Identification}, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual \\textit{data-types}, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \\textit{data-types}, such as cartoons and sketches, they struggle with simpler \\textit{data-types} arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual \\textit{data-types} through scaling. By analyzing the pre-training distributions of these models and incorporating \\textit{data-type} information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released \\href{https://github.com/bethgelab/DataTypeIdentification}{here}.","sentences":["Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding.","Here, we introduce the novel task of \\textit{Visual Data-Type Identification}, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining).","We develop two datasets consisting of animal images altered across a diverse set of 27 visual \\textit{data-types}, spanning four broad categories.","An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape.","While VLMs are reasonably good at identifying certain stylistic \\textit{data-types}, such as cartoons and sketches, they struggle with simpler \\textit{data-types} arising from basic manipulations like image rotations or additive noise.","Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo.","This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual \\textit{data-types} through scaling.","By analyzing the pre-training distributions of these models and incorporating \\textit{data-type} information into the captions during fine-tuning, we achieve a significant enhancement in performance.","By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding.","Code and datasets are released \\href{https://github.com/bethgelab/DataTypeIdentification}{here}."],"url":"http://arxiv.org/abs/2310.08577v1"}
{"created":"2023-10-12 17:59:23","title":"Learning to Act from Actionless Videos through Dense Correspondences","abstract":"In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.","sentences":["In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations.","Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals.","By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels.","This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks.","We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks.","Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day."],"url":"http://arxiv.org/abs/2310.08576v1"}
{"created":"2023-10-12 17:57:57","title":"Jigsaw: Supporting Designers in Prototyping Multimodal Applications by Assembling AI Foundation Models","abstract":"Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.","sentences":["Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes.","However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks.","To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models.","Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces.","To inform the design of Jigsaw, we interviewed ten designers and distilled design goals.","In a user study, we showed that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation."],"url":"http://arxiv.org/abs/2310.08574v1"}
{"created":"2023-10-12 17:57:32","title":"PolyTask: Learning Unified Policies through Behavior Distillation","abstract":"Unified models capable of solving a wide variety of tasks have gained traction in vision and NLP due to their ability to share regularities and structures across tasks, which improves individual task performance and reduces computational footprint. However, the impact of such models remains limited in embodied learning problems, which present unique challenges due to interactivity, sample inefficiency, and sequential task presentation. In this work, we present PolyTask, a novel method for learning a single unified model that can solve various embodied tasks through a 'learn then distill' mechanism. In the 'learn' step, PolyTask leverages a few demonstrations for each task to train task-specific policies. Then, in the 'distill' step, task-specific policies are distilled into a single policy using a new distillation method called Behavior Distillation. Given a unified policy, individual task behavior can be extracted through conditioning variables. PolyTask is designed to be conceptually simple while being able to leverage well-established algorithms in RL to enable interactivity, a handful of expert demonstrations to allow for sample efficiency, and preventing interactive access to tasks during distillation to enable lifelong learning. Experiments across three simulated environment suites and a real-robot suite show that PolyTask outperforms prior state-of-the-art approaches in multi-task and lifelong learning settings by significant margins.","sentences":["Unified models capable of solving a wide variety of tasks have gained traction in vision and NLP due to their ability to share regularities and structures across tasks, which improves individual task performance and reduces computational footprint.","However, the impact of such models remains limited in embodied learning problems, which present unique challenges due to interactivity, sample inefficiency, and sequential task presentation.","In this work, we present PolyTask, a novel method for learning a single unified model that can solve various embodied tasks through a 'learn then distill' mechanism.","In the 'learn' step, PolyTask leverages a few demonstrations for each task to train task-specific policies.","Then, in the 'distill' step, task-specific policies are distilled into a single policy using a new distillation method called Behavior Distillation.","Given a unified policy, individual task behavior can be extracted through conditioning variables.","PolyTask is designed to be conceptually simple while being able to leverage well-established algorithms in RL to enable interactivity, a handful of expert demonstrations to allow for sample efficiency, and preventing interactive access to tasks during distillation to enable lifelong learning.","Experiments across three simulated environment suites and a real-robot suite show that PolyTask outperforms prior state-of-the-art approaches in multi-task and lifelong learning settings by significant margins."],"url":"http://arxiv.org/abs/2310.08573v1"}
{"created":"2023-10-12 17:56:53","title":"Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders","abstract":"Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.","sentences":["Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs.","Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs.","We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users.","Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space.","To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations.","This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy.","Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs."],"url":"http://arxiv.org/abs/2310.08571v1"}
{"created":"2023-10-12 17:56:23","title":"A Lightweight Calibrated Simulation Enabling Efficient Offline Learning for Optimal Control of Real Buildings","abstract":"Modern commercial Heating, Ventilation, and Air Conditioning (HVAC) devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) model is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many real world challenges. We propose a novel simulation-based approach, where a customized simulator is used to train the agent for each building. Our open-source simulator (available online: https://github.com/google/sbsim) is lightweight and calibrated via telemetry from the building to reach a higher level of fidelity. On a two-story, 68,000 square foot building, with 127 devices, we were able to calibrate our simulator to have just over half a degree of drift from the real world over a six-hour interval. This approach is an important step toward having a real-world RL control system that can be scaled to many buildings, allowing for greater efficiency and resulting in reduced energy consumption and carbon emissions.","sentences":["Modern commercial Heating, Ventilation, and Air Conditioning (HVAC) devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission.","Given a suitable training environment, a Reinforcement Learning (RL) model is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many real world challenges.","We propose a novel simulation-based approach, where a customized simulator is used to train the agent for each building.","Our open-source simulator (available online: https://github.com/google/sbsim) is lightweight and calibrated via telemetry from the building to reach a higher level of fidelity.","On a two-story, 68,000 square foot building, with 127 devices, we were able to calibrate our simulator to have just over half a degree of drift from the real world over a six-hour interval.","This approach is an important step toward having a real-world RL control system that can be scaled to many buildings, allowing for greater efficiency and resulting in reduced energy consumption and carbon emissions."],"url":"http://arxiv.org/abs/2310.08569v1"}
{"created":"2023-10-12 17:56:12","title":"Placement Optimization of Substitutable Products","abstract":"Strategic product placement can have a strong influence on customer purchase behavior in physical stores as well as online platforms. Motivated by this, we consider the problem of optimizing the placement of substitutable products in designated display locations to maximize the expected revenue of the seller. We model the customer behavior as a two-stage process: first, the customer visits a subset of display locations according to a browsing distribution; second, the customer chooses at most one product from the displayed products at those locations according to a choice model. Our goal is to design a general algorithm that can select and place the products optimally for any browsing distribution and choice model, and we call this the Placement problem. We give a randomized algorithm that utilizes an $\\alpha$-approximate algorithm for cardinality constrained assortment optimization and outputs a $\\frac{\\Theta(\\alpha)}{\\log m}$-approximate solution (in expectation) for Placement with $m$ display locations, i.e., our algorithm outputs a solution with value at least $\\frac{\\Omega(\\alpha)}{\\log m}$ factor of the optimal and this is tight in the worst case. We also give algorithms with stronger guarantees in some special cases. In particular, we give a deterministic $\\frac{\\Omega(1)}{\\log m}$-approximation algorithm for the Markov choice model, and a tight $(1-1/e)$-approximation algorithm for the problem when products have identical prices.","sentences":["Strategic product placement can have a strong influence on customer purchase behavior in physical stores as well as online platforms.","Motivated by this, we consider the problem of optimizing the placement of substitutable products in designated display locations to maximize the expected revenue of the seller.","We model the customer behavior as a two-stage process: first, the customer visits a subset of display locations according to a browsing distribution; second, the customer chooses at most one product from the displayed products at those locations according to a choice model.","Our goal is to design a general algorithm that can select and place the products optimally for any browsing distribution and choice model, and we call this the Placement problem.","We give a randomized algorithm that utilizes an $\\alpha$-approximate algorithm for cardinality constrained assortment optimization and outputs a $\\frac{\\Theta(\\alpha)}{\\log m}$-approximate solution (in expectation) for Placement with $m$ display locations, i.e., our algorithm outputs a solution with value at least $\\frac{\\Omega(\\alpha)}{\\log m}$ factor of the optimal and this is tight in the worst case.","We also give algorithms with stronger guarantees in some special cases.","In particular, we give a deterministic $\\frac{\\Omega(1)}{\\log m}$-approximation algorithm for the Markov choice model, and a tight $(1-1/e)$-approximation algorithm for the problem when products have identical prices."],"url":"http://arxiv.org/abs/2310.08568v1"}
{"created":"2023-10-12 17:55:02","title":"Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining","abstract":"Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.","sentences":["Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments.","However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood.","In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms.","This paper provides a theoretical framework that analyzes supervised pretraining for ICRL.","This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers.","First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory.","The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms.","Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes.","This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories."],"url":"http://arxiv.org/abs/2310.08566v1"}
{"created":"2023-10-12 17:54:20","title":"Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities","abstract":"Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakeholders with a holistic understanding of these areas to enhance the overall AI-Robotics system security. We begin by surveying potential attack surfaces and provide mitigating defensive strategies. We then delve into ethical issues, such as dependency and psychological impact, as well as the legal concerns regarding accountability for these systems. Besides, emerging trends such as HRI are discussed, considering privacy, integrity, safety, trustworthiness, and explainability concerns. Finally, we present our vision for future research directions in this dynamic and promising field.","sentences":["Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception.","Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars.","These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control.","However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks.","The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences.","Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security.","Our goal is to provide users, developers and other stakeholders with a holistic understanding of these areas to enhance the overall AI-Robotics system security.","We begin by surveying potential attack surfaces and provide mitigating defensive strategies.","We then delve into ethical issues, such as dependency and psychological impact, as well as the legal concerns regarding accountability for these systems.","Besides, emerging trends such as HRI are discussed, considering privacy, integrity, safety, trustworthiness, and explainability concerns.","Finally, we present our vision for future research directions in this dynamic and promising field."],"url":"http://arxiv.org/abs/2310.08565v1"}
{"created":"2023-10-12 17:51:32","title":"MemGPT: Towards LLMs as Operating Systems","abstract":"Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.","sentences":["Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis.","To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory.","Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user.","We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users.","We release MemGPT code and data for our experiments at https://memgpt.ai."],"url":"http://arxiv.org/abs/2310.08560v1"}
{"created":"2023-10-12 17:51:10","title":"Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement","abstract":"The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps in rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.","sentences":["The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence.","Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks.","In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting.","Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules.","By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts.","However, they also behave as puzzling inductive reasoners, showing notable performance gaps in rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules.","Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks."],"url":"http://arxiv.org/abs/2310.08559v1"}
{"created":"2023-10-12 17:50:09","title":"Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias","abstract":"It is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when prior offline data does not provide enough state coverage. However, exploration bonuses can bias the learned policy, and our experiments find that naive, yet standard use of such bonuses can fail to recover a performant policy. Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets. Can we leverage offline RL to recover better policies from online interaction? We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation. Specifically, we propose offline retraining, a policy extraction step at the end of online fine-tuning in our Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL). An optimistic (exploration) policy is used to interact with the environment, and a separate pessimistic (exploitation) policy is trained on all the observed data for evaluation. Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation. OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14% to 26% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and improves online RL performance by 165% on two OpenAI gym environments. Further, OOO can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy. Implementation: https://github.com/MaxSobolMark/OOO","sentences":["It is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when prior offline data does not provide enough state coverage.","However, exploration bonuses can bias the learned policy, and our experiments find that naive, yet standard use of such bonuses can fail to recover a performant policy.","Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets.","Can we leverage offline RL to recover better policies from online interaction?","We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation.","Specifically, we propose offline retraining, a policy extraction step at the end of online fine-tuning in our Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL).","An optimistic (exploration) policy is used to interact with the environment, and a separate pessimistic (exploitation) policy is trained on all the observed data for evaluation.","Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation.","OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14% to 26% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and improves online RL performance by 165% on two OpenAI gym environments.","Further, OOO can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy.","Implementation: https://github.com/MaxSobolMark/OOO"],"url":"http://arxiv.org/abs/2310.08558v1"}
{"created":"2023-10-12 17:45:05","title":"Cross-Episodic Curriculum for Transformer Agents","abstract":"We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced at https://cec-agent.github.io/ to facilitate research on Transformer agent learning.","sentences":["We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents.","Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum.","By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes.","Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism.","The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise.","In all instances, policies resulting from CEC exhibit superior performance and strong generalization.","Code is open-sourced at https://cec-agent.github.io/ to facilitate research on Transformer agent learning."],"url":"http://arxiv.org/abs/2310.08549v1"}
{"created":"2023-10-12 17:44:59","title":"Stronger Coreset Bounds for Kernel Density Estimators via Chaining","abstract":"We apply the discrepancy method and a chaining approach to give improved bounds on the coreset complexity of a wide class of kernel functions. Our results give randomized polynomial time algorithms to produce coresets of size $O\\big(\\frac{\\sqrt{d}}{\\varepsilon}\\sqrt{\\log\\log \\frac{1}{\\varepsilon}}\\big)$ for the Gaussian and Laplacian kernels in the case that the data set is uniformly bounded, an improvement that was not possible with previous techniques. We also obtain coresets of size $O\\big(\\frac{1}{\\varepsilon}\\sqrt{\\log\\log \\frac{1}{\\varepsilon}}\\big)$ for the Laplacian kernel for $d$ constant. Finally, we give the best known bounds of $O\\big(\\frac{\\sqrt{d}}{\\varepsilon}\\sqrt{\\log(2\\max\\{1,\\alpha\\})}\\big)$ on the coreset complexity of the exponential, Hellinger, and JS Kernels, where $1/\\alpha$ is the bandwidth parameter of the kernel.","sentences":["We apply the discrepancy method and a chaining approach to give improved bounds on the coreset complexity of a wide class of kernel functions.","Our results give randomized polynomial time algorithms to produce coresets of size $O\\big(\\frac{\\sqrt{d}}{\\varepsilon}\\sqrt{\\log\\log \\frac{1}{\\varepsilon}}\\big)$ for the Gaussian and Laplacian kernels in the case that the data set is uniformly bounded, an improvement that was not possible with previous techniques.","We also obtain coresets of size $O\\big(\\frac{1}{\\varepsilon}\\sqrt{\\log\\log \\frac{1}{\\varepsilon}}\\big)$ for the Laplacian kernel for $d$ constant.","Finally, we give the best known bounds of $O\\big(\\frac{\\sqrt{d}}{\\varepsilon}\\sqrt{\\log(2\\max\\{1,\\alpha\\})}\\big)$ on the coreset complexity of the exponential, Hellinger, and JS Kernels, where $1/\\alpha$ is the bandwidth parameter of the kernel."],"url":"http://arxiv.org/abs/2310.08548v1"}
{"created":"2023-10-12 17:38:15","title":"NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation","abstract":"Datasets of labeled network traces are essential for a multitude of machine learning (ML) tasks in networking, yet their availability is hindered by privacy and maintenance concerns, such as data staleness. To overcome this limitation, synthetic network traces can often augment existing datasets. Unfortunately, current synthetic trace generation methods, which typically produce only aggregated flow statistics or a few selected packet attributes, do not always suffice, especially when model training relies on having features that are only available from packet traces. This shortfall manifests in both insufficient statistical resemblance to real traces and suboptimal performance on ML tasks when employed for data augmentation. In this paper, we apply diffusion models to generate high-resolution synthetic network traffic traces. We present NetDiffusion, a tool that uses a finely-tuned, controlled variant of a Stable Diffusion model to generate synthetic network traffic that is high fidelity and conforms to protocol specifications. Our evaluation demonstrates that packet captures generated from NetDiffusion can achieve higher statistical similarity to real data and improved ML model performance than current state-of-the-art approaches (e.g., GAN-based approaches). Furthermore, our synthetic traces are compatible with common network analysis tools and support a myriad of network tasks, suggesting that NetDiffusion can serve a broader spectrum of network analysis and testing tasks, extending beyond ML-centric applications.","sentences":["Datasets of labeled network traces are essential for a multitude of machine learning (ML) tasks in networking, yet their availability is hindered by privacy and maintenance concerns, such as data staleness.","To overcome this limitation, synthetic network traces can often augment existing datasets.","Unfortunately, current synthetic trace generation methods, which typically produce only aggregated flow statistics or a few selected packet attributes, do not always suffice, especially when model training relies on having features that are only available from packet traces.","This shortfall manifests in both insufficient statistical resemblance to real traces and suboptimal performance on ML tasks when employed for data augmentation.","In this paper, we apply diffusion models to generate high-resolution synthetic network traffic traces.","We present NetDiffusion, a tool that uses a finely-tuned, controlled variant of a Stable Diffusion model to generate synthetic network traffic that is high fidelity and conforms to protocol specifications.","Our evaluation demonstrates that packet captures generated from NetDiffusion can achieve higher statistical similarity to real data and improved ML model performance than current state-of-the-art approaches (e.g., GAN-based approaches).","Furthermore, our synthetic traces are compatible with common network analysis tools and support a myriad of network tasks, suggesting that NetDiffusion can serve a broader spectrum of network analysis and testing tasks, extending beyond ML-centric applications."],"url":"http://arxiv.org/abs/2310.08543v1"}
{"created":"2023-10-12 17:34:20","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation","abstract":"We introduce ``Idea to Image,'' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model's characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation.","sentences":["We introduce ``Idea to Image,'' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation.","Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations.","This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images.","We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries.","Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model's characteristics.","The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models.","Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities.","The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation."],"url":"http://arxiv.org/abs/2310.08541v1"}
{"created":"2023-10-12 17:32:09","title":"Do pretrained Transformers Really Learn In-context by Gradient Descent?","abstract":"Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.   We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.   Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies.","sentences":["Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?","Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models.","However, these works make assumptions far from the realistic natural language setting in which language models are trained.","Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.   ","We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent.","Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.   ","Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting.","We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B).","Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations.","We observe that ICL and GD adapt the output distribution of language models differently.","These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies."],"url":"http://arxiv.org/abs/2310.08540v1"}
{"created":"2023-10-12 17:28:06","title":"Image2PCI -- A Multitask Learning Framework for Estimating Pavement Condition Indices Directly from Images","abstract":"The Pavement Condition Index (PCI) is a widely used metric for evaluating pavement performance based on the type, extent and severity of distresses detected on a pavement surface. In recent times, significant progress has been made in utilizing deep-learning approaches to automate PCI estimation process. However, the current approaches rely on at least two separate models to estimate PCI values -- one model dedicated to determining the type and extent and another for estimating their severity. This approach presents several challenges, including complexities, high computational resource demands, and maintenance burdens that necessitate careful consideration and resolution. To overcome these challenges, the current study develops a unified multi-tasking model that predicts the PCI directly from a top-down pavement image. The proposed architecture is a multi-task model composed of one encoder for feature extraction and four decoders to handle specific tasks: two detection heads, one segmentation head and one PCI estimation head. By multitasking, we are able to extract features from the detection and segmentation heads for automatically estimating the PCI directly from the images. The model performs very well on our benchmarked and open pavement distress dataset that is annotated for multitask learning (the first of its kind). To our best knowledge, this is the first work that can estimate PCI directly from an image at real time speeds while maintaining excellent accuracy on all related tasks for crack detection and segmentation.","sentences":["The Pavement Condition Index (PCI) is a widely used metric for evaluating pavement performance based on the type, extent and severity of distresses detected on a pavement surface.","In recent times, significant progress has been made in utilizing deep-learning approaches to automate PCI estimation process.","However, the current approaches rely on at least two separate models to estimate PCI values -- one model dedicated to determining the type and extent and another for estimating their severity.","This approach presents several challenges, including complexities, high computational resource demands, and maintenance burdens that necessitate careful consideration and resolution.","To overcome these challenges, the current study develops a unified multi-tasking model that predicts the PCI directly from a top-down pavement image.","The proposed architecture is a multi-task model composed of one encoder for feature extraction and four decoders to handle specific tasks: two detection heads, one segmentation head and one PCI estimation head.","By multitasking, we are able to extract features from the detection and segmentation heads for automatically estimating the PCI directly from the images.","The model performs very well on our benchmarked and open pavement distress dataset that is annotated for multitask learning (the first of its kind).","To our best knowledge, this is the first work that can estimate PCI directly from an image at real time speeds while maintaining excellent accuracy on all related tasks for crack detection and segmentation."],"url":"http://arxiv.org/abs/2310.08538v1"}
{"created":"2023-10-12 17:26:16","title":"XAI Benchmark for Visual Explanation","abstract":"The rise of deep learning algorithms has led to significant advancements in computer vision tasks, but their \"black box\" nature has raised concerns regarding interpretability. Explainable AI (XAI) has emerged as a critical area of research aiming to open this \"black box\", and shed light on the decision-making process of AI models. Visual explanations, as a subset of Explainable Artificial Intelligence (XAI), provide intuitive insights into the decision-making processes of AI models handling visual data by highlighting influential areas in an input image. Despite extensive research conducted on visual explanations, most evaluations are model-centered since the availability of corresponding real-world datasets with ground truth explanations is scarce in the context of image data. To bridge this gap, we introduce an XAI Benchmark comprising a dataset collection from diverse topics that provide both class labels and corresponding explanation annotations for images. We have processed data from diverse domains to align with our unified visual explanation framework. We introduce a comprehensive Visual Explanation pipeline, which integrates data loading, preprocessing, experimental setup, and model evaluation processes. This structure enables researchers to conduct fair comparisons of various visual explanation techniques. In addition, we provide a comprehensive review of over 10 evaluation methods for visual explanation to assist researchers in effectively utilizing our dataset collection. To further assess the performance of existing visual explanation methods, we conduct experiments on selected datasets using various model-centered and ground truth-centered evaluation metrics. We envision this benchmark could facilitate the advancement of visual explanation models. The XAI dataset collection and easy-to-use code for evaluation are publicly accessible at https://xaidataset.github.io.","sentences":["The rise of deep learning algorithms has led to significant advancements in computer vision tasks, but their \"black box\" nature has raised concerns regarding interpretability.","Explainable AI (XAI) has emerged as a critical area of research aiming to open this \"black box\", and shed light on the decision-making process of AI models.","Visual explanations, as a subset of Explainable Artificial Intelligence (XAI), provide intuitive insights into the decision-making processes of AI models handling visual data by highlighting influential areas in an input image.","Despite extensive research conducted on visual explanations, most evaluations are model-centered since the availability of corresponding real-world datasets with ground truth explanations is scarce in the context of image data.","To bridge this gap, we introduce an XAI Benchmark comprising a dataset collection from diverse topics that provide both class labels and corresponding explanation annotations for images.","We have processed data from diverse domains to align with our unified visual explanation framework.","We introduce a comprehensive Visual Explanation pipeline, which integrates data loading, preprocessing, experimental setup, and model evaluation processes.","This structure enables researchers to conduct fair comparisons of various visual explanation techniques.","In addition, we provide a comprehensive review of over 10 evaluation methods for visual explanation to assist researchers in effectively utilizing our dataset collection.","To further assess the performance of existing visual explanation methods, we conduct experiments on selected datasets using various model-centered and ground truth-centered evaluation metrics.","We envision this benchmark could facilitate the advancement of visual explanation models.","The XAI dataset collection and easy-to-use code for evaluation are publicly accessible at https://xaidataset.github.io."],"url":"http://arxiv.org/abs/2310.08537v1"}
{"created":"2023-10-12 17:24:15","title":"Formally Specifying the High-Level Behavior of LLM-Based Agents","abstract":"LLM-based agents have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic, high-level generation framework that simplifies the process of building agents. The framework we introduce allows the user to specify desired agent behaviors in Linear Temporal Logic (LTL). The declarative LTL specification is then used to construct a constrained decoder that guarantees the LLM will produce an output exhibiting the desired behavior. By designing our framework in this way, we obtain several benefits, including the ability to enforce complex agent behavior, the ability to formally validate prompt examples, and the ability to seamlessly incorporate content-focused logical constraints into generation. In particular, our declarative approach, in which the desired behavior is simply described without concern for how it should be implemented or enforced, enables rapid design, implementation and experimentation with different LLM-based agents. We demonstrate how the proposed framework can be used to implement recent LLM-based agents, and show how the guardrails our approach provides can lead to improvements in agent performance. In addition, we release our code for general use.","sentences":["LLM-based agents have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure.","Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design.","In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic, high-level generation framework that simplifies the process of building agents.","The framework we introduce allows the user to specify desired agent behaviors in Linear Temporal Logic (LTL).","The declarative LTL specification is then used to construct a constrained decoder that guarantees the LLM will produce an output exhibiting the desired behavior.","By designing our framework in this way, we obtain several benefits, including the ability to enforce complex agent behavior, the ability to formally validate prompt examples, and the ability to seamlessly incorporate content-focused logical constraints into generation.","In particular, our declarative approach, in which the desired behavior is simply described without concern for how it should be implemented or enforced, enables rapid design, implementation and experimentation with different LLM-based agents.","We demonstrate how the proposed framework can be used to implement recent LLM-based agents, and show how the guardrails our approach provides can lead to improvements in agent performance.","In addition, we release our code for general use."],"url":"http://arxiv.org/abs/2310.08535v1"}
{"created":"2023-10-12 17:24:05","title":"Animating Street View","abstract":"We present a system that automatically brings street view imagery to life by populating it with naturally behaving, animated pedestrians and vehicles. Our approach is to remove existing people and vehicles from the input image, insert moving objects with proper scale, angle, motion, and appearance, plan paths and traffic behavior, as well as render the scene with plausible occlusion and shadowing effects. The system achieves these by reconstructing the still image street scene, simulating crowd behavior, and rendering with consistent lighting, visibility, occlusions, and shadows. We demonstrate results on a diverse range of street scenes including regular still images and panoramas.","sentences":["We present a system that automatically brings street view imagery to life by populating it with naturally behaving, animated pedestrians and vehicles.","Our approach is to remove existing people and vehicles from the input image, insert moving objects with proper scale, angle, motion, and appearance, plan paths and traffic behavior, as well as render the scene with plausible occlusion and shadowing effects.","The system achieves these by reconstructing the still image street scene, simulating crowd behavior, and rendering with consistent lighting, visibility, occlusions, and shadows.","We demonstrate results on a diverse range of street scenes including regular still images and panoramas."],"url":"http://arxiv.org/abs/2310.08534v1"}
{"created":"2023-10-12 17:23:52","title":"Platform for generating medical datasets for machine learning in public health","abstract":"Currently, there are many difficulties regarding the interoperability of medical data and related population data sources. These complications get in the way of the generation of high-quality data sets at city, region and national levels. Moreover, the collection of datasets within large medical centers is feasible due to own IT departments whereas the collection of raw medical data from multiple organizations is a more complicated process. In these circumstances, the most appropriate option is to develop digital products based on microservice architecture. Because of this approach, it is possible to ensure the multimodality of the system, the flexibility of the interface and the internal system approach, when interconnected elements behave as a whole, demonstrating behavior different from the behavior when working independently. These conditions allow, in turn, to ensure the maximum number and representativeness of the resulting data sets. This paper demonstrates a concept of the platform for a sustainable generation of quality and reliable sets of multimodal medical data. It collects data from different external sources, harmonizes it using a special service, anonymizes harmonized data, and labels processed data. The proposed system aims to be a promising solution to the improvement of medical data quality for machine learning.","sentences":["Currently, there are many difficulties regarding the interoperability of medical data and related population data sources.","These complications get in the way of the generation of high-quality data sets at city, region and national levels.","Moreover, the collection of datasets within large medical centers is feasible due to own IT departments whereas the collection of raw medical data from multiple organizations is a more complicated process.","In these circumstances, the most appropriate option is to develop digital products based on microservice architecture.","Because of this approach, it is possible to ensure the multimodality of the system, the flexibility of the interface and the internal system approach, when interconnected elements behave as a whole, demonstrating behavior different from the behavior when working independently.","These conditions allow, in turn, to ensure the maximum number and representativeness of the resulting data sets.","This paper demonstrates a concept of the platform for a sustainable generation of quality and reliable sets of multimodal medical data.","It collects data from different external sources, harmonizes it using a special service, anonymizes harmonized data, and labels processed data.","The proposed system aims to be a promising solution to the improvement of medical data quality for machine learning."],"url":"http://arxiv.org/abs/2310.08532v1"}
{"created":"2023-10-12 17:22:58","title":"UniPose: Detecting Any Keypoints","abstract":"This work proposes a unified framework called UniPose to detect keypoints of any articulated (e.g., human and animal), rigid, and soft objects via visual or textual prompts for fine-grained vision understanding and manipulation. Keypoint is a structure-aware, pixel-level, and compact representation of any object, especially articulated objects. Existing fine-grained promptable tasks mainly focus on object instance detection and segmentation but often fail to identify fine-grained granularity and structured information of image and instance, such as eyes, leg, paw, etc. Meanwhile, prompt-based keypoint detection is still under-explored. To bridge the gap, we make the first attempt to develop an end-to-end prompt-based keypoint detection framework called UniPose to detect keypoints of any objects. As keypoint detection tasks are unified in this framework, we can leverage 13 keypoint detection datasets with 338 keypoints across 1,237 categories over 400K instances to train a generic keypoint detection model. UniPose can effectively align text-to-keypoint and image-to-keypoint due to the mutual enhancement of textual and visual prompts based on the cross-modality contrastive learning optimization objectives. Our experimental results show that UniPose has strong fine-grained localization and generalization abilities across image styles, categories, and poses. Based on UniPose as a generalist keypoint detector, we hope it could serve fine-grained visual perception, understanding, and generation.","sentences":["This work proposes a unified framework called UniPose to detect keypoints of any articulated (e.g., human and animal), rigid, and soft objects via visual or textual prompts for fine-grained vision understanding and manipulation.","Keypoint is a structure-aware, pixel-level, and compact representation of any object, especially articulated objects.","Existing fine-grained promptable tasks mainly focus on object instance detection and segmentation but often fail to identify fine-grained granularity and structured information of image and instance, such as eyes, leg, paw, etc.","Meanwhile, prompt-based keypoint detection is still under-explored.","To bridge the gap, we make the first attempt to develop an end-to-end prompt-based keypoint detection framework called UniPose to detect keypoints of any objects.","As keypoint detection tasks are unified in this framework, we can leverage 13 keypoint detection datasets with 338 keypoints across 1,237 categories over 400K instances to train a generic keypoint detection model.","UniPose can effectively align text-to-keypoint and image-to-keypoint due to the mutual enhancement of textual and visual prompts based on the cross-modality contrastive learning optimization objectives.","Our experimental results show that UniPose has strong fine-grained localization and generalization abilities across image styles, categories, and poses.","Based on UniPose as a generalist keypoint detector, we hope it could serve fine-grained visual perception, understanding, and generation."],"url":"http://arxiv.org/abs/2310.08530v1"}
{"created":"2023-10-12 17:22:24","title":"GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors","abstract":"In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but the 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D generation framework, named as \\name, is proposed, where the 3D diffusion model provides point cloud priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our \\name can generate a high-quality 3D instance within 25 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.","sentences":["In recent times, the generation of 3D assets from text prompts has shown impressive results.","Both 2D and 3D diffusion models can generate decent 3D objects based on prompts.","3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain.","2D diffusion models enjoy strong abilities of generalization and fine generation, but the 3D consistency is hard to guarantee.","This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation.","A fast 3D generation framework, named as \\name, is proposed, where the 3D diffusion model provides point cloud priors for initialization and the 2D diffusion model enriches the geometry and appearance.","Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians.","Our \\name can generate a high-quality 3D instance within 25 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time.","Demos and code are available at https://taoranyi.com/gaussiandreamer/."],"url":"http://arxiv.org/abs/2310.08529v1"}
{"created":"2023-10-12 17:21:41","title":"4D Gaussian Splatting for Real-Time Dynamic Scene Rendering","abstract":"Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting (4D-GS) to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency. An efficient deformation field is constructed to model both Gaussian motions and shape deformations. Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations. Our 4D-GS method achieves real-time rendering under high resolutions, 70 FPS at a 800$\\times$800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.","sentences":["Representing and rendering dynamic scenes has been an important but challenging task.","Especially, to accurately model complex motions, high efficiency is usually hard to maintain.","We introduce the 4D Gaussian Splatting (4D-GS) to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency.","An efficient deformation field is constructed to model both Gaussian motions and shape deformations.","Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations.","Our 4D-GS method achieves real-time rendering under high resolutions, 70 FPS at a 800$\\times$800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art methods.","More demos and code are available at https://guanjunwu.github.io/4dgs/."],"url":"http://arxiv.org/abs/2310.08528v1"}
{"created":"2023-10-12 17:17:27","title":"LLM-augmented Preference Learning from Natural Language","abstract":"Finding preferences expressed in natural language is an important but challenging task. State-of-the-art(SotA) methods leverage transformer-based models such as BERT, RoBERTa, etc. and graph neural architectures such as graph attention networks. Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformer-based model, we investigate their ability to classify comparative text directly. This work aims to serve as a first step towards using LLMs for the CPC task. We design and conduct a set of experiments that format the classification task into an input prompt for the LLM and a methodology to get a fixed-format response that can be automatically evaluated. Comparing performances with existing methods, we see that pre-trained LLMs are able to outperform the previous SotA models with no fine-tuning involved. Our results show that the LLMs can consistently outperform the SotA when the target text is large -- i.e. composed of multiple sentences --, and are still comparable to the SotA performance in shorter text. We also find that few-shot learning yields better performance than zero-shot learning.","sentences":["Finding preferences expressed in natural language is an important but challenging task.","State-of-the-art(SotA) methods leverage transformer-based models such as BERT, RoBERTa, etc. and graph neural architectures such as graph attention networks.","Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformer-based model, we investigate their ability to classify comparative text directly.","This work aims to serve as a first step towards using LLMs for the CPC task.","We design and conduct a set of experiments that format the classification task into an input prompt for the LLM and a methodology to get a fixed-format response that can be automatically evaluated.","Comparing performances with existing methods, we see that pre-trained LLMs are able to outperform the previous SotA models with no fine-tuning involved.","Our results show that the LLMs can consistently outperform the SotA when the target text is large -- i.e. composed of multiple sentences --, and are still comparable to the SotA performance in shorter text.","We also find that few-shot learning yields better performance than zero-shot learning."],"url":"http://arxiv.org/abs/2310.08523v1"}
{"created":"2023-10-12 17:11:06","title":"A linear proof language for second-order intuitionistic linear logic","abstract":"We present a polymorphic linear lambda-calculus as a proof language for second-order intuitionistic linear logic. The calculus includes addition and scalar multiplication, enabling the proof of a linearity result at the syntactic level.","sentences":["We present a polymorphic linear lambda-calculus as a proof language for second-order intuitionistic linear logic.","The calculus includes addition and scalar multiplication, enabling the proof of a linearity result at the syntactic level."],"url":"http://arxiv.org/abs/2310.08517v1"}
{"created":"2023-10-12 17:08:45","title":"How connectivity structure shapes rich and lazy learning in neural circuits","abstract":"In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity generally has a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights, in particular their effective rank, influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks. Conversely, low-rank initialization biases learning towards richer learning. Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics. Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting.","sentences":["In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics.","Notably, initial weight distributions with small (resp.","large) variance may yield a rich (resp. lazy) regime, where significant (resp.","minor) changes to network states and representation are observed over the course of learning.","However, in biology, neural circuit connectivity generally has a low-rank structure and therefore differs markedly from the random initializations generally used for these studies.","As such, here we investigate how the structure of the initial weights, in particular their effective rank, influences the network learning regime.","Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks.","Conversely, low-rank initialization biases learning towards richer learning.","Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics.","Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting."],"url":"http://arxiv.org/abs/2310.08513v1"}
{"created":"2023-10-12 17:06:19","title":"HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science","abstract":"We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations. Our code and relevant datasets are publicly available at \\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee}.","sentences":["We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee).","MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science.","In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude).","Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data.","Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models.","Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement.","We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations.","Our code and relevant datasets are publicly available at \\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee}."],"url":"http://arxiv.org/abs/2310.08511v1"}
{"created":"2023-10-12 17:05:03","title":"Yuga: Automatically Detecting Lifetime Annotation Bugs in the Rust Language","abstract":"The Rust programming language is becoming increasingly popular among systems programmers due to its efficient performance and robust memory safety guarantees. Rust employs an ownership model to ensure this guarantee by allowing each value to be owned by only one identifier at a time. Additionally, it introduces the concept of borrowing and lifetimes to enable other variables to borrow the values under certain conditions temporarily. Despite its benefits, security vulnerabilities have been reported in Rust projects, often attributed to the use of \"unsafe\" Rust code. These vulnerabilities, in part, arise from incorrect lifetime annotations on function signatures. However, existing tools fail to detect these bugs, primarily because such bugs are rare, challenging to detect through dynamic analysis, and require explicit memory models. To overcome these limitations, first, we characterize incorrect lifetime annotations as a source of memory safety bugs and leverage this understanding to devise a novel static analysis tool, Yuga, to detect potential lifetime annotation bugs. Yuga uses a multi-phase analysis approach, starting with a quick pattern-matching algorithm to identify potential buggy components and then conducting a flow and field-sensitive alias analysis to confirm the bugs. We also curate new datasets of lifetime annotation bugs. Yuga successfully detects bugs with good precision on these datasets, and we make the code and datasets publicly available for review.","sentences":["The Rust programming language is becoming increasingly popular among systems programmers due to its efficient performance and robust memory safety guarantees.","Rust employs an ownership model to ensure this guarantee by allowing each value to be owned by only one identifier at a time.","Additionally, it introduces the concept of borrowing and lifetimes to enable other variables to borrow the values under certain conditions temporarily.","Despite its benefits, security vulnerabilities have been reported in Rust projects, often attributed to the use of \"unsafe\" Rust code.","These vulnerabilities, in part, arise from incorrect lifetime annotations on function signatures.","However, existing tools fail to detect these bugs, primarily because such bugs are rare, challenging to detect through dynamic analysis, and require explicit memory models.","To overcome these limitations, first, we characterize incorrect lifetime annotations as a source of memory safety bugs and leverage this understanding to devise a novel static analysis tool, Yuga, to detect potential lifetime annotation bugs.","Yuga uses a multi-phase analysis approach, starting with a quick pattern-matching algorithm to identify potential buggy components and then conducting a flow and field-sensitive alias analysis to confirm the bugs.","We also curate new datasets of lifetime annotation bugs.","Yuga successfully detects bugs with good precision on these datasets, and we make the code and datasets publicly available for review."],"url":"http://arxiv.org/abs/2310.08507v1"}
{"created":"2023-10-12 16:59:50","title":"Unsupervised Learning of Object-Centric Embeddings for Cell Instance Segmentation in Microscopy Images","abstract":"Segmentation of objects in microscopy images is required for many biomedical applications. We introduce object-centric embeddings (OCEs), which embed image patches such that the spatial offsets between patches cropped from the same object are preserved. Those learnt embeddings can be used to delineate individual objects and thus obtain instance segmentations. Here, we show theoretically that, under assumptions commonly found in microscopy images, OCEs can be learnt through a self-supervised task that predicts the spatial offset between image patches. Together, this forms an unsupervised cell instance segmentation method which we evaluate on nine diverse large-scale microscopy datasets. Segmentations obtained with our method lead to substantially improved results, compared to state-of-the-art baselines on six out of nine datasets, and perform on par on the remaining three datasets. If ground-truth annotations are available, our method serves as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude, thus substantially increasing the practical applicability of our method. Source code is available at https://github.com/funkelab/cellulus.","sentences":["Segmentation of objects in microscopy images is required for many biomedical applications.","We introduce object-centric embeddings (OCEs), which embed image patches such that the spatial offsets between patches cropped from the same object are preserved.","Those learnt embeddings can be used to delineate individual objects and thus obtain instance segmentations.","Here, we show theoretically that, under assumptions commonly found in microscopy images, OCEs can be learnt through a self-supervised task that predicts the spatial offset between image patches.","Together, this forms an unsupervised cell instance segmentation method which we evaluate on nine diverse large-scale microscopy datasets.","Segmentations obtained with our method lead to substantially improved results, compared to state-of-the-art baselines on six out of nine datasets, and perform on par on the remaining three datasets.","If ground-truth annotations are available, our method serves as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude, thus substantially increasing the practical applicability of our method.","Source code is available at https://github.com/funkelab/cellulus."],"url":"http://arxiv.org/abs/2310.08501v1"}
{"created":"2023-10-12 16:56:37","title":"Impact of time and note duration tokenizations on deep learning symbolic music modeling","abstract":"Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways. As Transformer can struggle at reasoning, but capture more easily explicit information, it is important to study how the way the information is represented for such model impact their performances. In this work, we analyze the common tokenization methods and experiment with time and note duration representations. We compare the performances of these two impactful criteria on several tasks, including composer and emotion classification, music generation, and sequence representation learning. We demonstrate that explicit information leads to better results depending on the task.","sentences":["Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR).","It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens.","Tokenization can be performed in different ways.","As Transformer can struggle at reasoning, but capture more easily explicit information, it is important to study how the way the information is represented for such model impact their performances.","In this work, we analyze the common tokenization methods and experiment with time and note duration representations.","We compare the performances of these two impactful criteria on several tasks, including composer and emotion classification, music generation, and sequence representation learning.","We demonstrate that explicit information leads to better results depending on the task."],"url":"http://arxiv.org/abs/2310.08497v1"}
{"created":"2023-10-12 16:55:44","title":"The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS","abstract":"Automatic analysis for modern Chinese has greatly improved the accuracy of text mining in related fields, but the study of ancient Chinese is still relatively rare. Ancient text division and lexical annotation are important parts of classical literature comprehension, and previous studies have tried to construct auxiliary dictionary and other fused knowledge to improve the performance. In this paper, we propose a framework for ancient Chinese Word Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one hand, we try to capture the wordhood semantics; on the other hand, we re-predict the uncertain samples of baseline model by introducing external knowledge. The performance of our architecture outperforms pre-trained BERT with CRF and existing tools such as Jiayan.","sentences":["Automatic analysis for modern Chinese has greatly improved the accuracy of text mining in related fields, but the study of ancient Chinese is still relatively rare.","Ancient text division and lexical annotation are important parts of classical literature comprehension, and previous studies have tried to construct auxiliary dictionary and other fused knowledge to improve the performance.","In this paper, we propose a framework for ancient Chinese Word Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one hand, we try to capture the wordhood semantics; on the other hand, we re-predict the uncertain samples of baseline model by introducing external knowledge.","The performance of our architecture outperforms pre-trained BERT with CRF and existing tools such as Jiayan."],"url":"http://arxiv.org/abs/2310.08496v1"}
{"created":"2023-10-12 16:52:00","title":"An Experience-based TAMP Framework for Foliated Manifolds","abstract":"Due to their complexity, foliated structure problems often pose intricate challenges to task and motion planning in robotics manipulation. To counter this, our study presents the ``Foliated Repetition Roadmap.'' This roadmap assists task and motion planners by transforming the complex foliated structure problem into a more accessible graph format. By leveraging query experiences from different foliated manifolds, our framework can dynamically and efficiently update this graph. The refined graph can generate distribution sets, optimizing motion planning performance in foliated structure problems. In our paper, we lay down the theoretical groundwork and illustrate its practical applications through real-world examples.","sentences":["Due to their complexity, foliated structure problems often pose intricate challenges to task and motion planning in robotics manipulation.","To counter this, our study presents the ``Foliated Repetition Roadmap.''","This roadmap assists task and motion planners by transforming the complex foliated structure problem into a more accessible graph format.","By leveraging query experiences from different foliated manifolds, our framework can dynamically and efficiently update this graph.","The refined graph can generate distribution sets, optimizing motion planning performance in foliated structure problems.","In our paper, we lay down the theoretical groundwork and illustrate its practical applications through real-world examples."],"url":"http://arxiv.org/abs/2310.08494v1"}
{"created":"2023-10-12 16:50:08","title":"Prometheus: Inducing Fine-grained Evaluation Capability in Language Models","abstract":"Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at https://github.com/kaistAI/Prometheus.","sentences":["Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard.","However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs.","In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied.","We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4.","Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user.","Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).","Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM.","Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model.","We open-source our code, dataset, and model at https://github.com/kaistAI/Prometheus."],"url":"http://arxiv.org/abs/2310.08491v1"}
{"created":"2023-10-12 16:47:15","title":"Community Consensus: Converging Locally despite Adversaries and Heterogeneous Connectivity","abstract":"We introduce the concept of community consensus in the presence of malicious agents using a well-known median-based consensus algorithm. We consider networks that have multiple well-connected regions that we term communities, characterized by specific robustness and minimum degree properties. Prior work derives conditions on properties that are necessary and sufficient for achieving global consensus in a network. This however, requires the minimum degree of the network graph to be proportional to the number of malicious agents in the network, which is not very practical in large networks. In this work we present a natural generalization of this previous result. We characterize cases when although global consensus is not reached, some subsets of agents $V_i$ will still converge to the same values $\\mathcal{M}_i$ among themselves. We define more relaxed requirements for this new type of consensus to be reached in terms of the number $k$ of edges connecting an agent in a community to agents external to the community, and the number of malicious agents in each community.","sentences":["We introduce the concept of community consensus in the presence of malicious agents using a well-known median-based consensus algorithm.","We consider networks that have multiple well-connected regions that we term communities, characterized by specific robustness and minimum degree properties.","Prior work derives conditions on properties that are necessary and sufficient for achieving global consensus in a network.","This however, requires the minimum degree of the network graph to be proportional to the number of malicious agents in the network, which is not very practical in large networks.","In this work we present a natural generalization of this previous result.","We characterize cases when although global consensus is not reached, some subsets of agents $V_i$ will still converge to the same values $\\mathcal{M}_i$ among themselves.","We define more relaxed requirements for this new type of consensus to be reached in terms of the number $k$ of edges connecting an agent in a community to agents external to the community, and the number of malicious agents in each community."],"url":"http://arxiv.org/abs/2310.08488v1"}
{"created":"2023-10-12 16:46:58","title":"GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language Models","abstract":"While multi-modal models have successfully integrated information from image, video, and audio modalities, integrating graph modality into large language models (LLMs) remains unexplored. This discrepancy largely stems from the inherent divergence between structured graph data and unstructured text data. Incorporating graph knowledge provides a reliable source of information, enabling potential solutions to address issues in text generation, e.g., hallucination, and lack of domain knowledge. To evaluate the integration of graph knowledge into language models, a dedicated dataset is needed. However, there is currently no benchmark dataset specifically designed for multimodal graph-language models. To address this gap, we propose GraphextQA, a question answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate the evaluation and future development of graph-language models. Additionally, we introduce a baseline model called CrossGNN, which conditions answer generation on the paired graphs by cross-attending question-aware graph features at decoding. The proposed dataset is designed to evaluate graph-language models' ability to understand graphs and make use of it for answer generation. We perform experiments with language-only models and the proposed graph-language model to validate the usefulness of the paired graphs and to demonstrate the difficulty of the task.","sentences":["While multi-modal models have successfully integrated information from image, video, and audio modalities, integrating graph modality into large language models (LLMs) remains unexplored.","This discrepancy largely stems from the inherent divergence between structured graph data and unstructured text data.","Incorporating graph knowledge provides a reliable source of information, enabling potential solutions to address issues in text generation, e.g., hallucination, and lack of domain knowledge.","To evaluate the integration of graph knowledge into language models, a dedicated dataset is needed.","However, there is currently no benchmark dataset specifically designed for multimodal graph-language models.","To address this gap, we propose GraphextQA, a question answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate the evaluation and future development of graph-language models.","Additionally, we introduce a baseline model called CrossGNN, which conditions answer generation on the paired graphs by cross-attending question-aware graph features at decoding.","The proposed dataset is designed to evaluate graph-language models' ability to understand graphs and make use of it for answer generation.","We perform experiments with language-only models and the proposed graph-language model to validate the usefulness of the paired graphs and to demonstrate the difficulty of the task."],"url":"http://arxiv.org/abs/2310.08487v1"}
{"created":"2023-10-12 16:42:53","title":"Understanding the Humans Behind Online Misinformation: An Observational Study Through the Lens of the COVID-19 Pandemic","abstract":"The proliferation of online misinformation has emerged as one of the biggest threats to society. Considerable efforts have focused on building misinformation detection models, still the perils of misinformation remain abound. Mitigating online misinformation and its ramifications requires a holistic approach that encompasses not only an understanding of its intricate landscape in relation to the complex issue and topic-rich information ecosystem online, but also the psychological drivers of individuals behind it. Adopting a time series analytic technique and robust causal inference-based design, we conduct a large-scale observational study analyzing over 32 million COVID-19 tweets and 16 million historical timeline tweets. We focus on understanding the behavior and psychology of users disseminating misinformation during COVID-19 and its relationship with the historical inclinations towards sharing misinformation on Non-COVID topics before the pandemic. Our analysis underscores the intricacies inherent to cross-topic misinformation, and highlights that users' historical inclination toward sharing misinformation is positively associated with their present behavior pertaining to misinformation sharing on emergent topics and beyond. This work may serve as a valuable foundation for designing user-centric inoculation strategies and ecologically-grounded agile interventions for effectively tackling online misinformation.","sentences":["The proliferation of online misinformation has emerged as one of the biggest threats to society.","Considerable efforts have focused on building misinformation detection models, still the perils of misinformation remain abound.","Mitigating online misinformation and its ramifications requires a holistic approach that encompasses not only an understanding of its intricate landscape in relation to the complex issue and topic-rich information ecosystem online, but also the psychological drivers of individuals behind it.","Adopting a time series analytic technique and robust causal inference-based design, we conduct a large-scale observational study analyzing over 32 million COVID-19 tweets and 16 million historical timeline tweets.","We focus on understanding the behavior and psychology of users disseminating misinformation during COVID-19 and its relationship with the historical inclinations towards sharing misinformation on Non-COVID topics before the pandemic.","Our analysis underscores the intricacies inherent to cross-topic misinformation, and highlights that users' historical inclination toward sharing misinformation is positively associated with their present behavior pertaining to misinformation sharing on emergent topics and beyond.","This work may serve as a valuable foundation for designing user-centric inoculation strategies and ecologically-grounded agile interventions for effectively tackling online misinformation."],"url":"http://arxiv.org/abs/2310.08483v1"}
{"created":"2023-10-12 16:32:44","title":"Can We Edit Multimodal Large Language Models?","abstract":"In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights\\footnote{Code and dataset are available in https://github.com/zjunlp/EasyEdit.","sentences":["In this paper, we focus on editing Multimodal Large Language Models (MLLMs).","Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process.","To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation.","We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs.","Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task.","We hope that our work can provide the NLP community with insights\\footnote{Code and dataset are available in https://github.com/zjunlp/EasyEdit."],"url":"http://arxiv.org/abs/2310.08475v1"}
{"created":"2023-10-12 16:29:56","title":"No-Regret Learning and Equilibrium Computation in Quantum Games","abstract":"As quantum processors advance, the emergence of large-scale decentralized systems involving interacting quantum-enabled agents is on the horizon. Recent research efforts have explored quantum versions of Nash and correlated equilibria as solution concepts of strategic quantum interactions, but these approaches did not directly connect to decentralized adaptive setups where agents possess limited information. This paper delves into the dynamics of quantum-enabled agents within decentralized systems that employ no-regret algorithms to update their behaviors over time. Specifically, we investigate two-player quantum zero-sum games and polymatrix quantum zero-sum games, showing that no-regret algorithms converge to separable quantum Nash equilibria in time-average. In the case of general multi-player quantum games, our work leads to a novel solution concept, (separable) quantum coarse correlated equilibria (QCCE), as the convergent outcome of the time-averaged behavior no-regret algorithms, offering a natural solution concept for decentralized quantum systems. Finally, we show that computing QCCEs can be formulated as a semidefinite program and establish the existence of entangled (i.e., non-separable) QCCEs, which cannot be approached via the current paradigm of no-regret learning.","sentences":["As quantum processors advance, the emergence of large-scale decentralized systems involving interacting quantum-enabled agents is on the horizon.","Recent research efforts have explored quantum versions of Nash and correlated equilibria as solution concepts of strategic quantum interactions, but these approaches did not directly connect to decentralized adaptive setups where agents possess limited information.","This paper delves into the dynamics of quantum-enabled agents within decentralized systems that employ no-regret algorithms to update their behaviors over time.","Specifically, we investigate two-player quantum zero-sum games and polymatrix quantum zero-sum games, showing that no-regret algorithms converge to separable quantum Nash equilibria in time-average.","In the case of general multi-player quantum games, our work leads to a novel solution concept, (separable) quantum coarse correlated equilibria (QCCE), as the convergent outcome of the time-averaged behavior no-regret algorithms, offering a natural solution concept for decentralized quantum systems.","Finally, we show that computing QCCEs can be formulated as a semidefinite program and establish the existence of entangled (i.e., non-separable) QCCEs, which cannot be approached via the current paradigm of no-regret learning."],"url":"http://arxiv.org/abs/2310.08473v1"}
{"created":"2023-10-12 16:28:25","title":"Strategies and impact of learning curve estimation for CNN-based image classification","abstract":"Learning curves are a measure for how the performance of machine learning models improves given a certain volume of training data. Over a wide variety of applications and models it was observed that learning curves follow -- to a large extent -- a power law behavior. This makes the performance of different models for a given task somewhat predictable and opens the opportunity to reduce the training time for practitioners, who are exploring the space of possible models and hyperparameters for the problem at hand. By estimating the learning curve of a model from training on small subsets of data only the best models need to be considered for training on the full dataset. How to choose subset sizes and how often to sample models on these to obtain estimates is however not researched. Given that the goal is to reduce overall training time strategies are needed that sample the performance in a time-efficient way and yet leads to accurate learning curve estimates. In this paper we formulate the framework for these strategies and propose several strategies. Further we evaluate the strategies for simulated learning curves and in experiments with popular datasets and models for image classification tasks.","sentences":["Learning curves are a measure for how the performance of machine learning models improves given a certain volume of training data.","Over a wide variety of applications and models it was observed that learning curves follow -- to a large extent -- a power law behavior.","This makes the performance of different models for a given task somewhat predictable and opens the opportunity to reduce the training time for practitioners, who are exploring the space of possible models and hyperparameters for the problem at hand.","By estimating the learning curve of a model from training on small subsets of data only the best models need to be considered for training on the full dataset.","How to choose subset sizes and how often to sample models on these to obtain estimates is however not researched.","Given that the goal is to reduce overall training time strategies are needed that sample the performance in a time-efficient way and yet leads to accurate learning curve estimates.","In this paper we formulate the framework for these strategies and propose several strategies.","Further we evaluate the strategies for simulated learning curves and in experiments with popular datasets and models for image classification tasks."],"url":"http://arxiv.org/abs/2310.08470v1"}
{"created":"2023-10-12 16:26:18","title":"MotionDirector: Motion Customization of Text-to-Video Diffusion Models","abstract":"Large-scale pre-trained diffusion models have exhibited remarkable capabilities in diverse video generations. Given a set of video clips of the same motion concept, the task of Motion Customization is to adapt existing text-to-video diffusion models to generate videos with this motion. For example, generating a video with a car moving in a prescribed manner under specific camera movements to make a movie, or a video illustrating how a bear would lift weights to inspire creators. Adaptation methods have been developed for customizing appearance like subject or style, yet unexplored for motion. It is straightforward to extend mainstream adaption methods for motion customization, including full model tuning, parameter-efficient tuning of additional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept learned by these methods is often coupled with the limited appearances in the training videos, making it difficult to generalize the customized motion to other appearances. To overcome this challenge, we propose MotionDirector, with a dual-path LoRAs architecture to decouple the learning of appearance and motion. Further, we design a novel appearance-debiased temporal loss to mitigate the influence of appearance on the temporal training objective. Experimental results show the proposed method can generate videos of diverse appearances for the customized motions. Our method also supports various downstream applications, such as the mixing of different videos with their appearance and motion respectively, and animating a single image with customized motions. Our code and model weights will be released.","sentences":["Large-scale pre-trained diffusion models have exhibited remarkable capabilities in diverse video generations.","Given a set of video clips of the same motion concept, the task of Motion Customization is to adapt existing text-to-video diffusion models to generate videos with this motion.","For example, generating a video with a car moving in a prescribed manner under specific camera movements to make a movie, or a video illustrating how a bear would lift weights to inspire creators.","Adaptation methods have been developed for customizing appearance like subject or style, yet unexplored for motion.","It is straightforward to extend mainstream adaption methods for motion customization, including full model tuning, parameter-efficient tuning of additional layers, and Low-Rank Adaptions (LoRAs).","However, the motion concept learned by these methods is often coupled with the limited appearances in the training videos, making it difficult to generalize the customized motion to other appearances.","To overcome this challenge, we propose MotionDirector, with a dual-path LoRAs architecture to decouple the learning of appearance and motion.","Further, we design a novel appearance-debiased temporal loss to mitigate the influence of appearance on the temporal training objective.","Experimental results show the proposed method can generate videos of diverse appearances for the customized motions.","Our method also supports various downstream applications, such as the mixing of different videos with their appearance and motion respectively, and animating a single image with customized motions.","Our code and model weights will be released."],"url":"http://arxiv.org/abs/2310.08465v1"}
{"created":"2023-10-12 16:21:04","title":"DistillSpec: Improving Speculative Decoding via Knowledge Distillation","abstract":"Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.","sentences":["Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution.","However, identifying a compact draft model that is well-aligned with the target model is challenging.","To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD.","DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy.","Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling.","Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off.","Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation."],"url":"http://arxiv.org/abs/2310.08461v1"}
{"created":"2023-10-12 16:19:58","title":"A Survey on Heterogeneous Transfer Learning","abstract":"The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks. Despite the existence of a survey in 2017 on this topic, the fast-paced advances post-2017 necessitate an updated, in-depth review. We therefore present a comprehensive survey of recent developments in heterogeneous transfer learning methods, offering a systematic guide for future research. Our paper reviews methodologies for diverse learning scenarios, discusses the limitations of current studies, and covers various application contexts, including Natural Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster a deeper understanding and spur future research.","sentences":["The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios.","The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies.","These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption.","Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain.","Arbitrary elimination of these differences is not always feasible or optimal.","Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks.","Despite the existence of a survey in 2017 on this topic, the fast-paced advances post-2017 necessitate an updated, in-depth review.","We therefore present a comprehensive survey of recent developments in heterogeneous transfer learning methods, offering a systematic guide for future research.","Our paper reviews methodologies for diverse learning scenarios, discusses the limitations of current studies, and covers various application contexts, including Natural Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster a deeper understanding and spur future research."],"url":"http://arxiv.org/abs/2310.08459v1"}
{"created":"2023-10-12 16:15:30","title":"Metrics for popularity bias in dynamic recommender systems","abstract":"Albeit the widespread application of recommender systems (RecSys) in our daily lives, rather limited research has been done on quantifying unfairness and biases present in such systems. Prior work largely focuses on determining whether a RecSys is discriminating or not but does not compute the amount of bias present in these systems. Biased recommendations may lead to decisions that can potentially have adverse effects on individuals, sensitive user groups, and society. Hence, it is important to quantify these biases for fair and safe commercial applications of these systems. This paper focuses on quantifying popularity bias that stems directly from the output of RecSys models, leading to over recommendation of popular items that are likely to be misaligned with user preferences. Four metrics to quantify popularity bias in RescSys over time in dynamic setting across different sensitive user groups have been proposed. These metrics have been demonstrated for four collaborative filtering based RecSys algorithms trained on two commonly used benchmark datasets in the literature. Results obtained show that the metrics proposed provide a comprehensive understanding of growing disparities in treatment between sensitive groups over time when used conjointly.","sentences":["Albeit the widespread application of recommender systems (RecSys) in our daily lives, rather limited research has been done on quantifying unfairness and biases present in such systems.","Prior work largely focuses on determining whether a RecSys is discriminating or not but does not compute the amount of bias present in these systems.","Biased recommendations may lead to decisions that can potentially have adverse effects on individuals, sensitive user groups, and society.","Hence, it is important to quantify these biases for fair and safe commercial applications of these systems.","This paper focuses on quantifying popularity bias that stems directly from the output of RecSys models, leading to over recommendation of popular items that are likely to be misaligned with user preferences.","Four metrics to quantify popularity bias in RescSys over time in dynamic setting across different sensitive user groups have been proposed.","These metrics have been demonstrated for four collaborative filtering based RecSys algorithms trained on two commonly used benchmark datasets in the literature.","Results obtained show that the metrics proposed provide a comprehensive understanding of growing disparities in treatment between sensitive groups over time when used conjointly."],"url":"http://arxiv.org/abs/2310.08455v1"}
{"created":"2023-10-12 16:15:05","title":"Faster Ascending Auctions via Polymatroid Sum","abstract":"We consider ascending auctions for finding Walrasian equilibria in markets with indivisible items and gross substitutes valuation functions. Each price increase step in the auction algorithm requires finding an inclusion-wise minimal maximal overdemanded set at the current prices. This can be formulated as a submodular function minimization problem. We observe that minimizing this submodular function corresponds to a polymatroid sum problem, and using this viewpoint, we give a fast and simple push-relabel algorithm for finding the minimal maximal overdemanded set. This improves on the previously best running time of Murota, Shioura and Yang (ISAAC 2013). Our algorithm is an adaptation of the push-relabel framework by Frank and Mikl\\'os (JJIAM, 2012) to the particular setting. We obtain a further improvement for the special case of unit-supplies. Furthermore, we show that for gross substitutes valuations, the component-wise minimal competitive prices are the same as the minimal Walrasian prices. This enables us to derive monotonicity properties of the Walrasian prices. Namely, we show that the minimal Walrasian prices can only increase if supply decreases, or demand increases.","sentences":["We consider ascending auctions for finding Walrasian equilibria in markets with indivisible items and gross substitutes valuation functions.","Each price increase step in the auction algorithm requires finding an inclusion-wise minimal maximal overdemanded set at the current prices.","This can be formulated as a submodular function minimization problem.","We observe that minimizing this submodular function corresponds to a polymatroid sum problem, and using this viewpoint, we give a fast and simple push-relabel algorithm for finding the minimal maximal overdemanded set.","This improves on the previously best running time of Murota, Shioura and Yang (ISAAC 2013).","Our algorithm is an adaptation of the push-relabel framework by Frank and Mikl\\'os (JJIAM, 2012) to the particular setting.","We obtain a further improvement for the special case of unit-supplies.","Furthermore, we show that for gross substitutes valuations, the component-wise minimal competitive prices are the same as the minimal Walrasian prices.","This enables us to derive monotonicity properties of the Walrasian prices.","Namely, we show that the minimal Walrasian prices can only increase if supply decreases, or demand increases."],"url":"http://arxiv.org/abs/2310.08454v1"}
{"created":"2023-10-12 16:11:13","title":"Proving the Potential of Skeleton Based Action Recognition to Automate the Analysis of Manual Processes","abstract":"In manufacturing sectors such as textiles and electronics, manual processes are a fundamental part of production. The analysis and monitoring of the processes is necessary for efficient production design. Traditional methods for analyzing manual processes are complex, expensive, and inflexible. Compared to established approaches such as Methods-Time-Measurement (MTM), machine learning (ML) methods promise: Higher flexibility, self-sufficient & permanent use, lower costs. In this work, based on a video stream, the current motion class in a manual assembly process is detected. With information on the current motion, Key-Performance-Indicators (KPIs) can be derived easily. A skeleton-based action recognition approach is taken, as this field recently shows major success in machine vision tasks. For skeleton-based action recognition in manual assembly, no sufficient pre-work could be found. Therefore, a ML pipeline is developed, to enable extensive research on different (pre-) processing methods and neural nets. Suitable well generalizing approaches are found, proving the potential of ML to enhance analyzation of manual processes. Models detect the current motion, performed by an operator in manual assembly, but the results can be transferred to all kinds of manual processes.","sentences":["In manufacturing sectors such as textiles and electronics, manual processes are a fundamental part of production.","The analysis and monitoring of the processes is necessary for efficient production design.","Traditional methods for analyzing manual processes are complex, expensive, and inflexible.","Compared to established approaches such as Methods-Time-Measurement (MTM), machine learning (ML) methods promise: Higher flexibility, self-sufficient & permanent use, lower costs.","In this work, based on a video stream, the current motion class in a manual assembly process is detected.","With information on the current motion, Key-Performance-Indicators (KPIs) can be derived easily.","A skeleton-based action recognition approach is taken, as this field recently shows major success in machine vision tasks.","For skeleton-based action recognition in manual assembly, no sufficient pre-work could be found.","Therefore, a ML pipeline is developed, to enable extensive research on different (pre-) processing methods and neural nets.","Suitable well generalizing approaches are found, proving the potential of ML to enhance analyzation of manual processes.","Models detect the current motion, performed by an operator in manual assembly, but the results can be transferred to all kinds of manual processes."],"url":"http://arxiv.org/abs/2310.08451v1"}
{"created":"2023-10-12 16:06:18","title":"Towards Robust Multi-Modal Reasoning via Model Selection","abstract":"The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the \"brain\" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.   To this end, we identify the key challenges therein and propose the $\\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.","sentences":["The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents.","LLM serves as the \"brain\" of agent, orchestrating multiple tools for collaborative multi-step task solving.","Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges.","However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile.","Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.   ","To this end, we identify the key challenges therein and propose the $\\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time.","This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning.","In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents.","Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process.","Our code and benchmark: https://github.com/LINs-lab/M3."],"url":"http://arxiv.org/abs/2310.08446v1"}
{"created":"2023-10-12 16:05:08","title":"Essentially non-hourglass and non-tensile-instability SPH elastic dynamics","abstract":"Since the tension instability was discovered in updated Lagrangian smoothed particle hydrodynamics (ULSPH) at the end of the 20th century, researchers have made considerable efforts to suppress its occurrence. However, up to the present day, this problem has not been fundamentally resolved. In this paper, the concept of hourglass modes is firstly introduced into ULSPH, and the inherent causes of tension instability in elastic dynamics are clarified based on this brand-new perspective. Specifically, we present an essentially non-hourglass formulation by decomposing the shear acceleration with the Laplacian operator, and a comprehensive set of challenging benchmark cases for elastic dynamics is used to showcase that our method can completely eliminate tensile instability by resolving hourglass modes. The present results reveal the true origin of tension instability and challenge the traditional understanding of its sources, i.e., hourglass modes are the real culprit behind inducing this instability in tension zones rather that the tension itself. Furthermore, a time integration scheme known as dual-criteria time stepping is adopted into the simulation of solids for the first time, to significantly enhance computational efficiency.","sentences":["Since the tension instability was discovered in updated Lagrangian smoothed particle hydrodynamics (ULSPH) at the end of the 20th century, researchers have made considerable efforts to suppress its occurrence.","However, up to the present day, this problem has not been fundamentally resolved.","In this paper, the concept of hourglass modes is firstly introduced into ULSPH, and the inherent causes of tension instability in elastic dynamics are clarified based on this brand-new perspective.","Specifically, we present an essentially non-hourglass formulation by decomposing the shear acceleration with the Laplacian operator, and a comprehensive set of challenging benchmark cases for elastic dynamics is used to showcase that our method can completely eliminate tensile instability by resolving hourglass modes.","The present results reveal the true origin of tension instability and challenge the traditional understanding of its sources, i.e., hourglass modes are the real culprit behind inducing this instability in tension zones rather that the tension itself.","Furthermore, a time integration scheme known as dual-criteria time stepping is adopted into the simulation of solids for the first time, to significantly enhance computational efficiency."],"url":"http://arxiv.org/abs/2310.08444v1"}
{"created":"2023-10-12 16:04:41","title":"Debias the Training of Diffusion Models","abstract":"Diffusion models have demonstrated compelling generation quality by optimizing the variational lower bound through a simple denoising score matching loss. In this paper, we provide theoretical evidence that the prevailing practice of using a constant loss weight strategy in diffusion models leads to biased estimation during the training phase. Simply optimizing the denoising network to predict Gaussian noise with constant weighting may hinder precise estimations of original images. To address the issue, we propose an elegant and effective weighting strategy grounded in the theoretically unbiased principle. Moreover, we conduct a comprehensive and systematic exploration to dissect the inherent bias problem deriving from constant weighting loss from the perspectives of its existence, impact and reasons. These analyses are expected to advance our understanding and demystify the inner workings of diffusion models. Through empirical evaluation, we demonstrate that our proposed debiased estimation method significantly enhances sample quality without the reliance on complex techniques, and exhibits improved efficiency compared to the baseline method both in training and sampling processes.","sentences":["Diffusion models have demonstrated compelling generation quality by optimizing the variational lower bound through a simple denoising score matching loss.","In this paper, we provide theoretical evidence that the prevailing practice of using a constant loss weight strategy in diffusion models leads to biased estimation during the training phase.","Simply optimizing the denoising network to predict Gaussian noise with constant weighting may hinder precise estimations of original images.","To address the issue, we propose an elegant and effective weighting strategy grounded in the theoretically unbiased principle.","Moreover, we conduct a comprehensive and systematic exploration to dissect the inherent bias problem deriving from constant weighting loss from the perspectives of its existence, impact and reasons.","These analyses are expected to advance our understanding and demystify the inner workings of diffusion models.","Through empirical evaluation, we demonstrate that our proposed debiased estimation method significantly enhances sample quality without the reliance on complex techniques, and exhibits improved efficiency compared to the baseline method both in training and sampling processes."],"url":"http://arxiv.org/abs/2310.08442v1"}
{"created":"2023-10-12 16:01:46","title":"Cold Start Latency in Serverless Computing: A Systematic Review, Taxonomy, and Future Directions","abstract":"Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model. In serverless computing, users pay only for the time they actually spend using the resources. Although zero scaling optimises cost and resource utilisation, it is the fundamental reason for the serverless cold start problem. Various academic and corporate sector studies are being conducted to tackle the cold start problem, which has large research challenges. To study the \"cold start\" problem in serverless computing, this article provides a comprehensive literature overview of recent research. In addition, we present a detailed taxonomy of several approaches to addressing the issue of cold start latency in serverless computing. Several academic and industrial organisations have proposed methods for cutting down the cold start time and cold start frequency, and this taxonomy is being used to explore these methods. There are several categories in which a current study on cold start latency is organised: caching and application-level optimization-based solutions, as well as AI/ML-based solutions. We have analysed the current methods and grouped them into categories based on their commonalities and features. Finally, we conclude with a review of current challenges and possible future research directions.","sentences":["Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model.","In serverless computing, users pay only for the time they actually spend using the resources.","Although zero scaling optimises cost and resource utilisation, it is the fundamental reason for the serverless cold start problem.","Various academic and corporate sector studies are being conducted to tackle the cold start problem, which has large research challenges.","To study the \"cold start\" problem in serverless computing, this article provides a comprehensive literature overview of recent research.","In addition, we present a detailed taxonomy of several approaches to addressing the issue of cold start latency in serverless computing.","Several academic and industrial organisations have proposed methods for cutting down the cold start time and cold start frequency, and this taxonomy is being used to explore these methods.","There are several categories in which a current study on cold start latency is organised: caching and application-level optimization-based solutions, as well as AI/ML-based solutions.","We have analysed the current methods and grouped them into categories based on their commonalities and features.","Finally, we conclude with a review of current challenges and possible future research directions."],"url":"http://arxiv.org/abs/2310.08437v1"}
{"created":"2023-10-12 15:57:37","title":"MUN-FRL: A Visual Inertial LiDAR Dataset for Aerial Autonomous Navigation and Mapping","abstract":"This paper presents a unique outdoor aerial visual-inertial-LiDAR dataset captured using a multi-sensor payload to promote the global navigation satellite system (GNSS)-denied navigation research. The dataset features flight distances ranging from 300m to 5km, collected using a DJI M600 hexacopter drone and the National Research Council (NRC) Bell 412 Advanced Systems Research Aircraft (ASRA). The dataset consists of hardware synchronized monocular images, IMU measurements, 3D LiDAR point-clouds, and high-precision real-time kinematic (RTK)-GNSS based ground truth. Ten datasets were collected as ROS bags over 100 mins of outdoor environment footage ranging from urban areas, highways, hillsides, prairies, and waterfronts. The datasets were collected to facilitate the development of visual-inertial-LiDAR odometry and mapping algorithms, visual-inertial navigation algorithms, object detection, segmentation, and landing zone detection algorithms based upon real-world drone and full-scale helicopter data. All the datasets contain raw sensor measurements, hardware timestamps, and spatio-temporally aligned ground truth. The intrinsic and extrinsic calibrations of the sensors are also provided along with raw calibration datasets. A performance summary of state-of-the-art methods applied on the datasets is also provided.","sentences":["This paper presents a unique outdoor aerial visual-inertial-LiDAR dataset captured using a multi-sensor payload to promote the global navigation satellite system (GNSS)-denied navigation research.","The dataset features flight distances ranging from 300m to 5km, collected using a DJI M600 hexacopter drone and the National Research Council (NRC)","Bell 412 Advanced Systems Research Aircraft (ASRA).","The dataset consists of hardware synchronized monocular images, IMU measurements, 3D LiDAR point-clouds, and high-precision real-time kinematic (RTK)-GNSS based ground truth.","Ten datasets were collected as ROS bags over 100 mins of outdoor environment footage ranging from urban areas, highways, hillsides, prairies, and waterfronts.","The datasets were collected to facilitate the development of visual-inertial-LiDAR odometry and mapping algorithms, visual-inertial navigation algorithms, object detection, segmentation, and landing zone detection algorithms based upon real-world drone and full-scale helicopter data.","All the datasets contain raw sensor measurements, hardware timestamps, and spatio-temporally aligned ground truth.","The intrinsic and extrinsic calibrations of the sensors are also provided along with raw calibration datasets.","A performance summary of state-of-the-art methods applied on the datasets is also provided."],"url":"http://arxiv.org/abs/2310.08435v1"}
{"created":"2023-10-12 15:56:24","title":"A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing","abstract":"We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.","sentences":["We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style.","We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile.","We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style.","Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind.","Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it.","We discuss the implications and limitations of our study and suggest directions for future research."],"url":"http://arxiv.org/abs/2310.08433v1"}
{"created":"2023-10-12 15:56:02","title":"Neural Sampling in Hierarchical Exponential-family Energy-based Models","abstract":"Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process. On natural image datasets, our model exhibits representations akin to those observed in the biological visual system. Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation. We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs.","sentences":["Bayesian brain theory suggests that the brain employs generative models to understand the external world.","The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses.","Additionally, the brain continually updates its generative model to approach the true distribution of the external world.","In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning.","In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term.","This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs).","As a result, the learning process is localized both in time and space, and the model is easy to converge.","To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process.","On natural image datasets, our model exhibits representations akin to those observed in the biological visual system.","Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation.","We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs."],"url":"http://arxiv.org/abs/2310.08431v1"}
{"created":"2023-10-12 15:53:47","title":"Assessing of Soil Erosion Risk Through Geoinformation Sciences and Remote Sensing -- A Review","abstract":"During past decades a marked manifestation of widespread erosion phenomena was studied worldwide. Global conservation community has launched campaigns at local, regional and continental level in developing countries for preservation of soil resources in order not only to stop or mitigate human impact on nature but also to improve life in rural areas introducing new approaches for soil cultivation. After the adoption of Sustainable Development Goals of UNs and launching several world initiatives such as the Land Degradation Neutrality (LDN) the world came to realize the very importance of the soil resources on which the biosphere relies for its existence. The main goal of the chapter is to review different types and structures erosion models as well as their applications. Several methods using spatial analysis capabilities of geographic information systems (GIS) are in operation for soil erosion risk assessment, such as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss Equation (RUSLE) in operation worldwide and in the USA and MESALES model. These and more models are being discussed in the present work alongside more experimental models and methods for assessing soil erosion risk such as Artificial Intelligence (AI), Machine and Deep Learning, etc. At the end of this work, a prospectus for the future development of soil erosion risk assessment is drawn.","sentences":["During past decades a marked manifestation of widespread erosion phenomena was studied worldwide.","Global conservation community has launched campaigns at local, regional and continental level in developing countries for preservation of soil resources in order not only to stop or mitigate human impact on nature but also to improve life in rural areas introducing new approaches for soil cultivation.","After the adoption of Sustainable Development Goals of UNs and launching several world initiatives such as the Land Degradation Neutrality (LDN) the world came to realize the very importance of the soil resources on which the biosphere relies for its existence.","The main goal of the chapter is to review different types and structures erosion models as well as their applications.","Several methods using spatial analysis capabilities of geographic information systems (GIS) are in operation for soil erosion risk assessment, such as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss Equation (RUSLE) in operation worldwide and in the USA and MESALES model.","These and more models are being discussed in the present work alongside more experimental models and methods for assessing soil erosion risk such as Artificial Intelligence (AI), Machine and Deep Learning, etc.","At the end of this work, a prospectus for the future development of soil erosion risk assessment is drawn."],"url":"http://arxiv.org/abs/2310.08430v1"}
{"created":"2023-10-12 15:53:24","title":"Revisiting Data Augmentation for Rotational Invariance in Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNN) offer state of the art performance in various computer vision tasks. Many of those tasks require different subtypes of affine invariances (scale, rotational, translational) to image transformations. Convolutional layers are translation equivariant by design, but in their basic form lack invariances. In this work we investigate how best to include rotational invariance in a CNN for image classification. Our experiments show that networks trained with data augmentation alone can classify rotated images nearly as well as in the normal unrotated case; this increase in representational power comes only at the cost of training time. We also compare data augmentation versus two modified CNN models for achieving rotational invariance or equivariance, Spatial Transformer Networks and Group Equivariant CNNs, finding no significant accuracy increase with these specialized methods. In the case of data augmented networks, we also analyze which layers help the network to encode the rotational invariance, which is important for understanding its limitations and how to best retrain a network with data augmentation to achieve invariance to rotation.","sentences":["Convolutional Neural Networks (CNN) offer state of the art performance in various computer vision tasks.","Many of those tasks require different subtypes of affine invariances (scale, rotational, translational) to image transformations.","Convolutional layers are translation equivariant by design, but in their basic form lack invariances.","In this work we investigate how best to include rotational invariance in a CNN for image classification.","Our experiments show that networks trained with data augmentation alone can classify rotated images nearly as well as in the normal unrotated case; this increase in representational power comes only at the cost of training time.","We also compare data augmentation versus two modified CNN models for achieving rotational invariance or equivariance, Spatial Transformer Networks and Group Equivariant CNNs, finding no significant accuracy increase with these specialized methods.","In the case of data augmented networks, we also analyze which layers help the network to encode the rotational invariance, which is important for understanding its limitations and how to best retrain a network with data augmentation to achieve invariance to rotation."],"url":"http://arxiv.org/abs/2310.08429v1"}
{"created":"2023-10-12 15:48:14","title":"Differentially Private Non-convex Learning for Multi-layer Neural Networks","abstract":"This paper focuses on the problem of Differentially Private Stochastic Optimization for (multi-layer) fully connected neural networks with a single output node. In the first part, we examine cases with no hidden nodes, specifically focusing on Generalized Linear Models (GLMs). We investigate the well-specific model where the random noise possesses a zero mean, and the link function is both bounded and Lipschitz continuous. We propose several algorithms and our analysis demonstrates the feasibility of achieving an excess population risk that remains invariant to the data dimension. We also delve into the scenario involving the ReLU link function, and our findings mirror those of the bounded link function. We conclude this section by contrasting well-specified and misspecified models, using ReLU regression as a representative example.   In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model. In the third part, we study the theoretical guarantees of DP-SGD in Abadi et al. (2016) for fully connected multi-layer neural networks. By utilizing recent advances in Neural Tangent Kernel theory, we provide the first excess population risk when both the sample size and the width of the network are sufficiently large. Additionally, we discuss the role of some parameters in DP-SGD regarding their utility, both theoretically and empirically.","sentences":["This paper focuses on the problem of Differentially Private Stochastic Optimization for (multi-layer) fully connected neural networks with a single output node.","In the first part, we examine cases with no hidden nodes, specifically focusing on Generalized Linear Models (GLMs).","We investigate the well-specific model where the random noise possesses a zero mean, and the link function is both bounded and Lipschitz continuous.","We propose several algorithms and our analysis demonstrates the feasibility of achieving an excess population risk that remains invariant to the data dimension.","We also delve into the scenario involving the ReLU link function, and our findings mirror those of the bounded link function.","We conclude this section by contrasting well-specified and misspecified models, using ReLU regression as a representative example.   ","In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model.","In the third part, we study the theoretical guarantees of DP-SGD in Abadi et al. (2016) for fully connected multi-layer neural networks.","By utilizing recent advances in Neural Tangent Kernel theory, we provide the first excess population risk when both the sample size and the width of the network are sufficiently large.","Additionally, we discuss the role of some parameters in DP-SGD regarding their utility, both theoretically and empirically."],"url":"http://arxiv.org/abs/2310.08425v1"}
{"created":"2023-10-12 15:42:17","title":"\"SegLoc\": Study on Novel Visual Self-supervised Learning Scheme (Segment Localization) Tailored for Dense Prediction Tasks of Security Inspection X-ray Images","abstract":"Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance Localization, our model SegLoc has managed to address one of the most challenging downsides of contrastive learning, i.e., false negative pairs of query embeddings. In order to do so, in contrast to baseline model InsLoc, our pretraining dataset is synthesized by cropping, transforming, then pasting already labeled segments from an available labeled dataset, foregrounds, onto instances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray datasets are considered as labeled and unlabeled datasets, respectively. Moreover, we fully harness labels by avoiding false negative pairs through implementing the idea, one queue per class, in MoCo-v2 whereby negative pairs corresponding to each query are extracted from its corresponding queue within the memory bank. Our approach has outperformed random initialization by 3% to 6%, while having underperformed supervised initialization.","sentences":["Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme.","Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively.","Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts.","Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet.","Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization.","Based upon the model Instance Localization, our model SegLoc has managed to address one of the most challenging downsides of contrastive learning, i.e., false negative pairs of query embeddings.","In order to do so, in contrast to baseline model InsLoc, our pretraining dataset is synthesized by cropping, transforming, then pasting already labeled segments from an available labeled dataset, foregrounds, onto instances of an unlabeled dataset, backgrounds.","In our case, PIDray and SIXray datasets are considered as labeled and unlabeled datasets, respectively.","Moreover, we fully harness labels by avoiding false negative pairs through implementing the idea, one queue per class, in MoCo-v2 whereby negative pairs corresponding to each query are extracted from its corresponding queue within the memory bank.","Our approach has outperformed random initialization by 3% to 6%, while having underperformed supervised initialization."],"url":"http://arxiv.org/abs/2310.08421v1"}
{"created":"2023-10-12 15:39:54","title":"Visual Attention-Prompted Prediction and Learning","abstract":"Explanation(attention)-guided learning is a method that enhances a model's predictive power by incorporating human understanding during the training phase. While attention-guided learning has shown promising results, it often involves time-consuming and computationally expensive model retraining. To address this issue, we introduce the attention-prompted prediction technique, which enables direct prediction guided by the attention prompt without the need for model retraining. However, this approach presents several challenges, including: 1) How to incorporate the visual attention prompt into the model's decision-making process and leverage it for future predictions even in the absence of a prompt? and 2) How to handle the incomplete information from the visual attention prompt? To tackle these challenges, we propose a novel framework called Visual Attention-Prompted Prediction and Learning, which seamlessly integrates visual attention prompts into the model's decision-making process and adapts to images both with and without attention prompts for prediction. To address the incomplete information of the visual attention prompt, we introduce a perturbation-based attention map modification method. Additionally, we propose an optimization-based mask aggregation method with a new weight learning function for adaptive perturbed annotation aggregation in the attention map modification process. Our overall framework is designed to learn in an attention-prompt guided multi-task manner to enhance future predictions even for samples without attention prompts and trained in an alternating manner for better convergence. Extensive experiments conducted on two datasets demonstrate the effectiveness of our proposed framework in enhancing predictions for samples, both with and without provided prompts.","sentences":["Explanation(attention)-guided learning is a method that enhances a model's predictive power by incorporating human understanding during the training phase.","While attention-guided learning has shown promising results, it often involves time-consuming and computationally expensive model retraining.","To address this issue, we introduce the attention-prompted prediction technique, which enables direct prediction guided by the attention prompt without the need for model retraining.","However, this approach presents several challenges, including: 1) How to incorporate the visual attention prompt into the model's decision-making process and leverage it for future predictions even in the absence of a prompt?","and 2) How to handle the incomplete information from the visual attention prompt?","To tackle these challenges, we propose a novel framework called Visual Attention-Prompted Prediction and Learning, which seamlessly integrates visual attention prompts into the model's decision-making process and adapts to images both with and without attention prompts for prediction.","To address the incomplete information of the visual attention prompt, we introduce a perturbation-based attention map modification method.","Additionally, we propose an optimization-based mask aggregation method with a new weight learning function for adaptive perturbed annotation aggregation in the attention map modification process.","Our overall framework is designed to learn in an attention-prompt guided multi-task manner to enhance future predictions even for samples without attention prompts and trained in an alternating manner for better convergence.","Extensive experiments conducted on two datasets demonstrate the effectiveness of our proposed framework in enhancing predictions for samples, both with and without provided prompts."],"url":"http://arxiv.org/abs/2310.08420v1"}
{"created":"2023-10-12 15:38:28","title":"Jailbreaking Black Box Large Language Models in Twenty Queries","abstract":"There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.","sentences":["There is growing interest in ensuring that large language models (LLMs) align with human values.","However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails.","The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse.","To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM.","PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention.","In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak.","Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms.","PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2."],"url":"http://arxiv.org/abs/2310.08419v1"}
{"created":"2023-10-12 15:27:12","title":"A Sound and Complete Refinement Relation for Non-reducible Modal Transition Systems","abstract":"Modal Transition Systems (MTS) are a well-known formalism that extend Labelled Transition Systems (LTS) with the possibility of specifying necessary and permitted behaviour. Whenever two MTS are not in modal refinement relationship, it could still be the case that the set of implementations of one MTS is included in the set of implementations of the other. The challenge of devising an alternative notion of modal refinement that is both sound and complete with respect to the set of implementations, without disregarding valuable implementations, remains open. In this paper, we address this challenge. We introduce a subset of MTS called Non-reducible Modal Transition Systems (NMTS), together with a novel refinement relation for NMTS. We show that NMTS refinement is sound and also complete with respect to its set of implementations. We illustrate through examples how the additional constraints imposed by NMTS are necessary for achieving completeness. Furthermore, we discuss a property holding for NMTS whose implementations are non-deterministic. We show that any implementation obtained through modal refinement but disregarded by NMTS refinement is violating this property.","sentences":["Modal Transition Systems (MTS) are a well-known formalism that extend Labelled Transition Systems (LTS) with the possibility of specifying necessary and permitted behaviour.","Whenever two MTS are not in modal refinement relationship, it could still be the case that the set of implementations of one MTS is included in the set of implementations of the other.","The challenge of devising an alternative notion of modal refinement that is both sound and complete with respect to the set of implementations, without disregarding valuable implementations, remains open.","In this paper, we address this challenge.","We introduce a subset of MTS called Non-reducible Modal Transition Systems (NMTS), together with a novel refinement relation for NMTS.","We show that NMTS refinement is sound and also complete with respect to its set of implementations.","We illustrate through examples how the additional constraints imposed by NMTS are necessary for achieving completeness.","Furthermore, we discuss a property holding for NMTS whose implementations are non-deterministic.","We show that any implementation obtained through modal refinement but disregarded by NMTS refinement is violating this property."],"url":"http://arxiv.org/abs/2310.08412v1"}
{"created":"2023-10-12 15:19:15","title":"Tightening Bounds on Probabilities of Causation By Merging Datasets","abstract":"Probabilities of Causation (PoC) play a fundamental role in decision-making in law, health care and public policy. Nevertheless, their point identification is challenging, requiring strong assumptions, in the absence of which only bounds can be derived. Existing work to further tighten these bounds by leveraging extra information either provides numerical bounds, symbolic bounds for fixed dimensionality, or requires access to multiple datasets that contain the same treatment and outcome variables. However, in many clinical, epidemiological and public policy applications, there exist external datasets that examine the effect of different treatments on the same outcome variable, or study the association between covariates and the outcome variable. These external datasets cannot be used in conjunction with the aforementioned bounds, since the former may entail different treatment assignment mechanisms, or even obey different causal structures. Here, we provide symbolic bounds on the PoC for this challenging scenario. We focus on combining either two randomized experiments studying different treatments, or a randomized experiment and an observational study, assuming causal sufficiency. Our symbolic bounds work for arbitrary dimensionality of covariates and treatment, and we discuss the conditions under which these bounds are tighter than existing bounds in literature. Finally, our bounds parameterize the difference in treatment assignment mechanism across datasets, allowing the mechanisms to vary across datasets while still allowing causal information to be transferred from the external dataset to the target dataset.","sentences":["Probabilities of Causation (PoC) play a fundamental role in decision-making in law, health care and public policy.","Nevertheless, their point identification is challenging, requiring strong assumptions, in the absence of which only bounds can be derived.","Existing work to further tighten these bounds by leveraging extra information either provides numerical bounds, symbolic bounds for fixed dimensionality, or requires access to multiple datasets that contain the same treatment and outcome variables.","However, in many clinical, epidemiological and public policy applications, there exist external datasets that examine the effect of different treatments on the same outcome variable, or study the association between covariates and the outcome variable.","These external datasets cannot be used in conjunction with the aforementioned bounds, since the former may entail different treatment assignment mechanisms, or even obey different causal structures.","Here, we provide symbolic bounds on the PoC for this challenging scenario.","We focus on combining either two randomized experiments studying different treatments, or a randomized experiment and an observational study, assuming causal sufficiency.","Our symbolic bounds work for arbitrary dimensionality of covariates and treatment, and we discuss the conditions under which these bounds are tighter than existing bounds in literature.","Finally, our bounds parameterize the difference in treatment assignment mechanism across datasets, allowing the mechanisms to vary across datasets while still allowing causal information to be transferred from the external dataset to the target dataset."],"url":"http://arxiv.org/abs/2310.08406v1"}
{"created":"2023-10-12 15:12:37","title":"Vault: Decentralized Storage Made Durable","abstract":"The lack of centralized control, combined with highly dynamic adversarial behaviors, makes data durability a challenge in decentralized storage systems. In this work, we introduce a new storage system, Vault, that offers strong data durability guarantees in a fully decentralized, permission-less setting. Vault leverages the rateless property of erasure code to encode each data object into an infinite stream of encoding fragments. To ensure durability in the presence of dynamic Byzantine behaviors and targeted attacks, an infinite sequence of storage nodes are randomly selected to store encoding fragments. Encoding generation and candidate selection are fully decentralized: When necessary, Vault nodes use a gossip protocol and a publically verifiable selection proof to determine new fragments. Simulations and large-scale EC2 experiments demonstrate that Vault provides close-to-ideal mean-time-to-data-loss (MTTDL) with low storage redundancy, scales to more than 10,000 nodes, and attains performance comparable to IPFS","sentences":["The lack of centralized control, combined with highly dynamic adversarial behaviors, makes data durability a challenge in decentralized storage systems.","In this work, we introduce a new storage system, Vault, that offers strong data durability guarantees in a fully decentralized, permission-less setting.","Vault leverages the rateless property of erasure code to encode each data object into an infinite stream of encoding fragments.","To ensure durability in the presence of dynamic Byzantine behaviors and targeted attacks, an infinite sequence of storage nodes are randomly selected to store encoding fragments.","Encoding generation and candidate selection are fully decentralized: When necessary, Vault nodes use a gossip protocol and a publically verifiable selection proof to determine new fragments.","Simulations and large-scale EC2 experiments demonstrate that Vault provides close-to-ideal mean-time-to-data-loss (MTTDL) with low storage redundancy, scales to more than 10,000 nodes, and attains performance comparable to IPFS"],"url":"http://arxiv.org/abs/2310.08403v1"}
{"created":"2023-10-12 15:10:55","title":"Performance/power assessment of CNN packages on embedded automotive platforms","abstract":"The rise of power-efficient embedded computers based on highly-parallel accelerators opens a number of opportunities and challenges for researchers and engineers, and paved the way to the era of edge computing. At the same time, advances in embedded AI for object detection and categorization such as YOLO, GoogleNet and AlexNet reached an unprecedented level of accuracy (mean-Average Precision - mAP) and performance (Frames-Per-Second - FPS). Today, edge computers based on heterogeneous many-core systems are a predominant choice to deploy such systems in industry 4.0, wearable devices, and - our focus - autonomous driving systems. In these latter systems, engineers struggle to make reduced automotive power and size budgets co-exist with the accuracy and performance targets requested by autonomous driving. We aim at validating the effectiveness and efficiency of most recent networks on state-of-the-art platforms with embedded commercial-off-the-shelf System-on-Chips, such as Xavier AGX, Tegra X2 and Nano for NVIDIA and XCZU9EG and XCZU3EG of the Zynq UltraScale+ family, for the Xilinx counterpart. Our work aims at supporting engineers in choosing the most appropriate CNN package and computing system for their designs, and deriving guidelines for adequately sizing their systems.","sentences":["The rise of power-efficient embedded computers based on highly-parallel accelerators opens a number of opportunities and challenges for researchers and engineers, and paved the way to the era of edge computing.","At the same time, advances in embedded AI for object detection and categorization such as YOLO, GoogleNet and AlexNet reached an unprecedented level of accuracy (mean-Average Precision - mAP) and performance (Frames-Per-Second - FPS).","Today, edge computers based on heterogeneous many-core systems are a predominant choice to deploy such systems in industry 4.0, wearable devices, and - our focus - autonomous driving systems.","In these latter systems, engineers struggle to make reduced automotive power and size budgets co-exist with the accuracy and performance targets requested by autonomous driving.","We aim at validating the effectiveness and efficiency of most recent networks on state-of-the-art platforms with embedded commercial-off-the-shelf System-on-Chips, such as Xavier AGX, Tegra X2 and Nano for NVIDIA and XCZU9EG and XCZU3EG of the Zynq UltraScale+ family, for the Xilinx counterpart.","Our work aims at supporting engineers in choosing the most appropriate CNN package and computing system for their designs, and deriving guidelines for adequately sizing their systems."],"url":"http://arxiv.org/abs/2310.08401v1"}
{"created":"2023-10-12 15:09:12","title":"Towards Design and Development of an ArUco Markers-Based Quantitative Surface Tactile Sensor","abstract":"In this paper, with the goal of quantifying the qualitative image outputs of a Vision-based Tactile Sensor (VTS), we present the design, fabrication, and characterization of a novel Quantitative Surface Tactile Sensor (called QS-TS). QS-TS directly estimates the sensor's gel layer deformation in real-time enabling safe and autonomous tactile manipulation and servoing of delicate objects using robotic manipulators. The core of the proposed sensor is the utilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner binary patterns and a broad black border, called ArUco Markers. Each ArUco marker can provide real-time camera pose estimation that, in our design, is used as a quantitative measure for obtaining deformation of the QS-TS gel layer. Moreover, thanks to the use of ArUco markers, we propose a unique fabrication procedure that mitigates various challenges associated with the fabrication of the existing marker-based VTSs and offers an intuitive and less-arduous method for the construction of the VTS. Remarkably, the proposed fabrication facilitates the integration and adherence of markers with the gel layer to robustly and reliably obtain a quantitative measure of deformation in real-time regardless of the orientation of ArUco Markers. The performance and efficacy of the proposed QS-TS in estimating the deformation of the sensor's gel layer were experimentally evaluated and verified. Results demonstrate the phenomenal performance of the QS-TS in estimating the deformation of the gel layer with a relative error of <5%.","sentences":["In this paper, with the goal of quantifying the qualitative image outputs of a Vision-based Tactile Sensor (VTS), we present the design, fabrication, and characterization of a novel Quantitative Surface Tactile Sensor (called QS-TS).","QS-TS directly estimates the sensor's gel layer deformation in real-time enabling safe and autonomous tactile manipulation and servoing of delicate objects using robotic manipulators.","The core of the proposed sensor is the utilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner binary patterns and a broad black border, called ArUco Markers.","Each ArUco marker can provide real-time camera pose estimation that, in our design, is used as a quantitative measure for obtaining deformation of the QS-TS gel layer.","Moreover, thanks to the use of ArUco markers, we propose a unique fabrication procedure that mitigates various challenges associated with the fabrication of the existing marker-based VTSs and offers an intuitive and less-arduous method for the construction of the VTS.","Remarkably, the proposed fabrication facilitates the integration and adherence of markers with the gel layer to robustly and reliably obtain a quantitative measure of deformation in real-time regardless of the orientation of ArUco Markers.","The performance and efficacy of the proposed QS-TS in estimating the deformation of the sensor's gel layer were experimentally evaluated and verified.","Results demonstrate the phenomenal performance of the QS-TS in estimating the deformation of the gel layer with a relative error of <5%."],"url":"http://arxiv.org/abs/2310.08398v1"}
{"created":"2023-10-12 15:08:36","title":"Uncertainty-Aware Planning for Heterogeneous Robot Teams using Dynamic Topological Graphs and Mixed-Integer Programming","abstract":"Planning under uncertainty is a fundamental challenge in robotics. For multi-robot teams, the challenge is further exacerbated, since the planning problem can quickly become computationally intractable as the number of robots increase. In this paper, we propose a novel approach for planning under uncertainty using heterogeneous multi-robot teams. In particular, we leverage the notion of a dynamic topological graph and mixed-integer programming to generate multi-robot plans that deploy fast scout team members to reduce uncertainty about the environment. We test our approach in a number of representative scenarios where the robot team must move through an environment while minimizing detection in the presence of uncertain observer positions. We demonstrate that our approach is sufficiently computationally tractable for real-time re-planning in changing environments, can improve performance in the presence of imperfect information, and can be adjusted to accommodate different risk profiles.","sentences":["Planning under uncertainty is a fundamental challenge in robotics.","For multi-robot teams, the challenge is further exacerbated, since the planning problem can quickly become computationally intractable as the number of robots increase.","In this paper, we propose a novel approach for planning under uncertainty using heterogeneous multi-robot teams.","In particular, we leverage the notion of a dynamic topological graph and mixed-integer programming to generate multi-robot plans that deploy fast scout team members to reduce uncertainty about the environment.","We test our approach in a number of representative scenarios where the robot team must move through an environment while minimizing detection in the presence of uncertain observer positions.","We demonstrate that our approach is sufficiently computationally tractable for real-time re-planning in changing environments, can improve performance in the presence of imperfect information, and can be adjusted to accommodate different risk profiles."],"url":"http://arxiv.org/abs/2310.08396v1"}
{"created":"2023-10-12 15:08:14","title":"Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation","abstract":"The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.","sentences":["The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question.","For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed.","However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation.","The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks.","Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation.","Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form.","Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations.","To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity.","We conduct extensive experiments over three public KBQG datasets.","The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets.","Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively."],"url":"http://arxiv.org/abs/2310.08395v1"}
{"created":"2023-10-12 15:07:11","title":"Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization","abstract":"Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing $300$ document-instruction pairs with $3$ answers each. All $900$ answers are rated by $3$ human annotators. Using riSum, we analyze agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on-par with costly reference-based metrics which require high-quality summaries.","sentences":["Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem.","While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted.","In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs.","Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing $300$ document-instruction pairs with $3$ answers each.","All $900$ answers are rated by $3$ human annotators.","Using riSum, we analyze agreement between evaluation methods and human judgment.","Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on-par with costly reference-based metrics which require high-quality summaries."],"url":"http://arxiv.org/abs/2310.08394v1"}
{"created":"2023-10-12 15:00:06","title":"Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric Learning","abstract":"Metric learning plays a critical role in training image retrieval and classification. It is also a key algorithm in representation learning, e.g., for feature learning and its alignment in metric space. Hyperbolic embedding has been recently developed, compared to the conventional Euclidean embedding in most of the previously developed models, and can be more effective in representing the hierarchical data structure. Second, uncertainty estimation/measurement is a long-lasting challenge in artificial intelligence. Successful uncertainty estimation can improve a machine learning model's performance, robustness, and security. In Hyperbolic space, uncertainty measurement is at least with equivalent, if not more, critical importance. In this paper, we develop a Hyperbolic image embedding with uncertainty-aware metric learning for image retrieval. We call our method Hyp-UML: Hyperbolic Uncertainty-aware Metric Learning. Our contribution are threefold: we propose an image embedding algorithm based on Hyperbolic space, with their corresponding uncertainty value; we propose two types of uncertainty-aware metric learning, for the popular Contrastive learning and conventional margin-based metric learning, respectively. We perform extensive experimental validations to prove that the proposed algorithm can achieve state-of-the-art results among related methods. The comprehensive ablation study validates the effectiveness of each component of the proposed algorithm.","sentences":["Metric learning plays a critical role in training image retrieval and classification.","It is also a key algorithm in representation learning, e.g., for feature learning and its alignment in metric space.","Hyperbolic embedding has been recently developed, compared to the conventional Euclidean embedding in most of the previously developed models, and can be more effective in representing the hierarchical data structure.","Second, uncertainty estimation/measurement is a long-lasting challenge in artificial intelligence.","Successful uncertainty estimation can improve a machine learning model's performance, robustness, and security.","In Hyperbolic space, uncertainty measurement is at least with equivalent, if not more, critical importance.","In this paper, we develop a Hyperbolic image embedding with uncertainty-aware metric learning for image retrieval.","We call our method Hyp-UML:","Hyperbolic Uncertainty-aware Metric Learning.","Our contribution are threefold: we propose an image embedding algorithm based on Hyperbolic space, with their corresponding uncertainty value; we propose two types of uncertainty-aware metric learning, for the popular Contrastive learning and conventional margin-based metric learning, respectively.","We perform extensive experimental validations to prove that the proposed algorithm can achieve state-of-the-art results among related methods.","The comprehensive ablation study validates the effectiveness of each component of the proposed algorithm."],"url":"http://arxiv.org/abs/2310.08390v1"}
{"created":"2023-10-12 14:59:22","title":"MeanAP-Guided Reinforced Active Learning for Object Detection","abstract":"Active learning presents a promising avenue for training high-performance models with minimal labeled data, achieved by judiciously selecting the most informative instances to label and incorporating them into the task learner. Despite notable advancements in active learning for image recognition, metrics devised or learned to gauge the information gain of data, crucial for query strategy design, do not consistently align with task model performance metrics, such as Mean Average Precision (MeanAP) in object detection tasks. This paper introduces MeanAP-Guided Reinforced Active Learning for Object Detection (MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task model to devise a sampling strategy employing a reinforcement learning-based sampling agent. Built upon LSTM architecture, the agent efficiently explores and selects subsequent training instances, and optimizes the process through policy gradient with MeanAP serving as reward. Recognizing the time-intensive nature of MeanAP computation at each step, we propose fast look-up tables to expedite agent training. We assess MAGRAL's efficacy across popular benchmarks, PASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical findings substantiate MAGRAL's superiority over recent state-of-the-art methods, showcasing substantial performance gains. MAGRAL establishes a robust baseline for reinforced active object detection, signifying its potential in advancing the field.","sentences":["Active learning presents a promising avenue for training high-performance models with minimal labeled data, achieved by judiciously selecting the most informative instances to label and incorporating them into the task learner.","Despite notable advancements in active learning for image recognition, metrics devised or learned to gauge the information gain of data, crucial for query strategy design, do not consistently align with task model performance metrics, such as Mean Average Precision (MeanAP) in object detection tasks.","This paper introduces MeanAP-Guided Reinforced Active Learning for Object Detection (MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task model to devise a sampling strategy employing a reinforcement learning-based sampling agent.","Built upon LSTM architecture, the agent efficiently explores and selects subsequent training instances, and optimizes the process through policy gradient with MeanAP serving as reward.","Recognizing the time-intensive nature of MeanAP computation at each step, we propose fast look-up tables to expedite agent training.","We assess MAGRAL's efficacy across popular benchmarks, PASCAL VOC and MS COCO, utilizing different backbone architectures.","Empirical findings substantiate MAGRAL's superiority over recent state-of-the-art methods, showcasing substantial performance gains.","MAGRAL establishes a robust baseline for reinforced active object detection, signifying its potential in advancing the field."],"url":"http://arxiv.org/abs/2310.08387v1"}
{"created":"2023-10-12 14:57:47","title":"Towards Running Time Analysis of Interactive Multi-objective Evolutionary Algorithms","abstract":"Evolutionary algorithms (EAs) are widely used for multi-objective optimization due to their population-based nature. Traditional multi-objective EAs (MOEAs) generate a large set of solutions to approximate the Pareto front, leaving a decision maker (DM) with the task of selecting a preferred solution. However, this process can be inefficient and time-consuming, especially when there are many objectives or the subjective preferences of DM is known. To address this issue, interactive MOEAs (iMOEAs) combine decision making into the optimization process, i.e., update the population with the help of the DM. In contrast to their wide applications, there has existed only two pieces of theoretical works on iMOEAs, which only considered interactive variants of the two simple single-objective algorithms, RLS and (1+1)-EA. This paper provides the first running time analysis (the essential theoretical aspect of EAs) for practical iMOEAs. Specifically, we prove that the expected running time of the well-developed interactive NSGA-II (called R-NSGA-II) for solving the OneMinMax and OneJumpZeroJump problems is $O(n \\log n)$ and $O(n^k)$, respectively, which are all asymptotically faster than the traditional NSGA-II. Meanwhile, we present a variant of OneMinMax, and prove that R-NSGA-II can be exponentially slower than NSGA-II. These results provide theoretical justification for the effectiveness of iMOEAs while identifying situations where they may fail. Experiments are also conducted to validate the theoretical results.","sentences":["Evolutionary algorithms (EAs) are widely used for multi-objective optimization due to their population-based nature.","Traditional multi-objective EAs (MOEAs) generate a large set of solutions to approximate the Pareto front, leaving a decision maker (DM) with the task of selecting a preferred solution.","However, this process can be inefficient and time-consuming, especially when there are many objectives or the subjective preferences of DM is known.","To address this issue, interactive MOEAs (iMOEAs) combine decision making into the optimization process, i.e., update the population with the help of the DM.","In contrast to their wide applications, there has existed only two pieces of theoretical works on iMOEAs, which only considered interactive variants of the two simple single-objective algorithms, RLS and (1+1)-EA.","This paper provides the first running time analysis (the essential theoretical aspect of EAs) for practical iMOEAs.","Specifically, we prove that the expected running time of the well-developed interactive NSGA-II (called R-NSGA-II) for solving the OneMinMax and OneJumpZeroJump problems is $O(n \\log n)$ and $O(n^k)$, respectively, which are all asymptotically faster than the traditional NSGA-II.","Meanwhile, we present a variant of OneMinMax, and prove that R-NSGA-II can be exponentially slower than NSGA-II.","These results provide theoretical justification for the effectiveness of iMOEAs while identifying situations where they may fail.","Experiments are also conducted to validate the theoretical results."],"url":"http://arxiv.org/abs/2310.08384v1"}
{"created":"2023-10-12 14:57:24","title":"Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction","abstract":"Discovery of new materials has a documented history of propelling human progress for centuries and more. The behaviour of a material is a function of its composition, structure, and properties, which further depend on its processing and testing conditions. Recent developments in deep learning and natural language processing have enabled information extraction at scale from published literature such as peer-reviewed publications, books, and patents. However, this information is spread in multiple formats, such as tables, text, and images, and with little or no uniformity in reporting style giving rise to several machine learning challenges. Here, we discuss, quantify, and document these outstanding challenges in automated information extraction (IE) from materials science literature towards the creation of a large materials science knowledge base. Specifically, we focus on IE from text and tables and outline several challenges with examples. We hope the present work inspires researchers to address the challenges in a coherent fashion, providing to fillip to IE for the materials knowledge base.","sentences":["Discovery of new materials has a documented history of propelling human progress for centuries and more.","The behaviour of a material is a function of its composition, structure, and properties, which further depend on its processing and testing conditions.","Recent developments in deep learning and natural language processing have enabled information extraction at scale from published literature such as peer-reviewed publications, books, and patents.","However, this information is spread in multiple formats, such as tables, text, and images, and with little or no uniformity in reporting style giving rise to several machine learning challenges.","Here, we discuss, quantify, and document these outstanding challenges in automated information extraction (IE) from materials science literature towards the creation of a large materials science knowledge base.","Specifically, we focus on IE from text and tables and outline several challenges with examples.","We hope the present work inspires researchers to address the challenges in a coherent fashion, providing to fillip to IE for the materials knowledge base."],"url":"http://arxiv.org/abs/2310.08383v1"}
{"created":"2023-10-12 14:55:31","title":"AutoVP: An Automated Visual Prompting Framework and Benchmark","abstract":"Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP's development. The source code is available at https://github.com/IBM/AutoVP.","sentences":["Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks.","However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance.","To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark.","Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping.","Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline.","AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP's development.","The source code is available at https://github.com/IBM/AutoVP."],"url":"http://arxiv.org/abs/2310.08381v1"}
